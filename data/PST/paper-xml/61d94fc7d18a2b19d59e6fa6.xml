<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Efficient Information Extraction from Form-Like Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-07">7 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
							<email>bgunel@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Navneet</forename><surname>Potti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Wendt</surname></persName>
							<email>jwendt@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
							<email>najork@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-Efficient Information Extraction from Form-Like Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-07">7 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.02647v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>information extraction, multi-domain transfer learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automating information extraction from form-like documents at scale is a pressing need due to its potential impact on automating business workflows across many industries like financial services, insurance, and healthcare. The key challenge is that form-like documents in these business workflows can be laid out in virtually infinitely many ways; hence, a good solution to this problem should generalize to documents with unseen layouts and languages. A solution to this problem requires a holistic understanding of both the textual segments and the visual cues within a document, which is non-trivial. While the natural language processing and computer vision communities are starting to tackle this problem, there has not been much focus on (1) data-efficiency, and (2) ability to generalize across different document types and languages.</p><p>In this paper, we show that when we have only a small number of labeled documents for training (∼50), a straightforward transfer learning approach from a considerably structurally-different larger labeled corpus yields up to a 27 F1 point improvement over simply training on the small corpus in the target domain. We improve on this with a simple multi-domain transfer learning approach, that is currently in production use, and show that this yields up to a further 8 F1 point improvement. We make the case that data efficiency is critical to enable information extraction systems to scale to handle hundreds of different document-types, and learning good representations is critical to accomplishing this.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Given a target set of fields for a particular document type, say, invoice date and total amount for invoices, along with a small set of manually-labeled documents, the task at hand is to learn to automatically extract these fields from documents with unseen layouts and languages. Note that even within the documents of the same type and language, say English invoices, same pieces of information may be described in entirely different ways, as each vendor often has their own layout structure. We will refer to this layout structure as template for the rest of the paper. Moreover, this information extraction task requires understanding both the textual segments and the visual cues within a document as it aims to generalize to unseen templates across different document types and languages. Hence, the traditional information extraction techniques from webpages, most of which do integrate visual layout information <ref type="bibr">[2-4, 15, 16]</ref>, do not suffice here.</p><p>Figure <ref type="figure">1</ref>: Given a document image and a target schema as inputs, we perform Optical Character Recognition (OCR) on the document image, generate candidates using candidate generators that leverage the existing domain knowledge of working with structured documents, score these generated candidates using a representation learning-based ML model that is described in Section 2, and assign the best candidates to the target fields to produce the final extraction result.</p><p>There have been several multi-modal ML-based proposals to tackle this task such as BERTGrid <ref type="bibr" target="#b4">[5]</ref>. BERTGrid extends Katti et al. <ref type="bibr" target="#b7">[8]</ref>'s work that previously proposed inputting documents as 2D grids of text tokens to fully convolutional encoder-decoder networks by incorporating pre-trained BERT <ref type="bibr" target="#b5">[6]</ref> text embeddings into the 2D grid representation. Another line of work proposes extending language model pre-training approaches to include the document layout information including <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. Although these approaches are promising, training or pre-training time of their pipelines are not only compute-and data-intensive, but also need to be re-done from scratch for competitive extraction performance while working with a new considerably structurally different document type or a different language. We would like to point out that in order to fully automate this task, we need to tackle 100s of document types -and the main cost is data acquisition and labeling for every new language or document type. If we can get to same extraction performance with 10x less data, we effectively cut the cost of developing new extraction models by 10x.</p><p>In our work, we borrow the basic architecture from Glean <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>, an information extraction system that uses a factored approach. Glean decouples the task into three stages -candidate generation, ML-based scoring, and assigning -as described in Figure <ref type="figure">1</ref>. Glean's design decision of leveraging candidate generators built using standard entity extraction libraries significantly narrows down the search problem for its ML-based scorer model. This way, Glean's ML-based scorer model can fully focus on (1) learning a representation for an extraction candidate that captures its spatial neighborhood on the page, as well as (2) learning the semantics of the target field. We summarize the design of the extraction system and its ML-based scorer model that effectively leverages representation learning <ref type="bibr" target="#b0">[1]</ref> ideas in Section 2.</p><p>In this paper, we explore whether it is possible to transfer knowledge over to (1) a considerably structurally-different document type in the same language or to (2) same document type in a different language in the context of Glean, when we have several orders of magnitude less labeled documents in the new document type or language. It is, again, crucial to note that this is of great practical importance, as it is often prohibitively expensive to gather and label datasets of large size for every new document type or language.</p><p>We build on the hypothesis that form-like documents share a visual design language and that the representation learning approach of Glean naturally enables multi-domain training and fine-tuning across different document types and different languages, by its design. In other words, we postulate that a representation learning approach that exploits that shared visual design language across form-like documents is precisely why we can effectively transfer knowledge across considerably different domains. The core idea we follow is that we first focus on learning a good encoder for the extraction candidates that understands the spatial relationships and semantics of form-like documents, and then we fine-tune the learned candidate encoder and the field-specific encodings on the new document type or language of interest.</p><p>We show that our very simple multi-domain transfer learning approach -that combines extraction candidates from both source and target domains, and use a common vocabulary across both these domains to train the scorer model followed by fine-tuning the model on the target domain -enables remarkable data-efficient generalization both from English Invoices to considerably structurallydifferent new document type Paystubs, and from English Invoices to French Invoices. Our proposed approach consistently improves over both training from scratch and simple transfer learning baselines up to 1k labeled documents. The value of our proposed approach is particularly impressive in the low data regimes. Specifically, we improve on the training from scratch baseline by up to 35 F1 points, and on the simple transfer learning baseline by up to 8 F1 points for the 50 labeled document case while generalizing to a new document type. Similarly, we improve on the training from scratch baseline by up to 23 F1 points, and on the simple transfer learning baseline by up to 7 F1 points for the 10 labeled document in the target domain case while generalizing to a new language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EXTRACTION SYSTEM OVERVIEW</head><p>We build on Glean <ref type="bibr" target="#b12">[13]</ref> extraction system that is described in Figure <ref type="figure">1</ref>. Glean takes in a document image and a target schema -that includes target fields and their corresponding types-as inputs, and performs Optical Character Recognition (OCR). As an example, target schema for Invoice document type could include target fields such as invoice date of type date and total amount of type price. Glean supports numerous field types including integer, numeric, alphanumeric, and currency, address, phone_number, url, and other common entity types. Glean leverages an existing library of entity detectors used in Google's Knowledge Graph and are available through a Cloud API<ref type="foot" target="#foot_0">1</ref> for all the types described. Open-source entity detection libraries can be used for common types like names, dates, currency amounts, numbers, addresses, URLs, etc. <ref type="foot" target="#foot_1">2</ref> The candidate generators are designed to be high-recall -they identify every text span in the document that is likely to be of their type.</p><p>Once extraction candidates have been generated, an ML-based Scorer is used to assign a score for each (field, candidate) pair that estimates the likelihood that the given extraction candidate is the right extraction value for that field. Multiple fields in the target schema may belong to the same type, say invoice date and due date, and may therefore share the same set of extraction candidates. An extraction candidate is represented by the text span identified by the candidate generator along with context such as text in its immediate neighborhood to provide the ML-based scorer model with additional relevant features. Finally, candidates for a target field and document are independently scored, and highest scoring candidate for the field is assigned as the final extraction value. Note that, (1) additional business logic specific to a document type can be specified at the assignment stage such as including constraints like invoice date must precede due date chronologically, (2) precision of the overall extraction system can be adjusted by imposing a minimum score threshold for each field in the target schema.</p><p>A high-level abstraction for the ML-based Scorer is shown in Figure <ref type="figure" target="#fig_0">2</ref>. Modeling specifics of the Scorer architecture is not the focus of this paper, and we refer the reader to Majumder et al. <ref type="bibr" target="#b10">[11]</ref> for details. In this section, we explain the ML-based Scorer model architecture at a high-level in order for us to qualify why the representation learning inspired modeling choices naturally enable multi-domain training and fine-tuning across different document types and different languages. The features of each extraction candidate supplied to the model are its neighboring words and their relative positions, as visualized in Figure <ref type="figure" target="#fig_0">2</ref>. Note that we exclude the candidate's value from the set of features in order to avoid biasing the model towards the distribution of values seen during training, which may not be representative of the entire domain at test time. Model learns a dense representation for each extraction candidate using a simple self-attention based architecture. Separately, in the same embedding space, it learns dense representations for each field in the target schema that capture the semantics of the fields. Based on these learned candidate and field representations, each extraction candidate is scored based on the similarity to its corresponding field embedding. The model is trained as a binary classifier using cross-entropy loss, where the target labels are obtained by comparing the candidate to the ground truth. Please refer to Tata et al. <ref type="bibr" target="#b12">[13]</ref> for how the training data consisting of positive and negative extraction candidates are generated, along with the design decisions to address the data management challenges that arise due to the nature of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA EFFICIENCY 3.1 Experimental Setup</head><p>Datasets and Evaluation Metrics Dataset statistics are summarized in Table <ref type="table">1</ref>. During data curation, we ensured that no two documents in the corpus share the same template as we aim to learn to extract from any template. All of these datasets are proprietary, and hence, are unfortunately not available publicly. Our primary metric is the end-to-end extraction performance measured using Max F1 in the precision-recall curve, which we refer to as the F1 score. We always report macro-average F1 score across fields in the target schema. Note that F1 score is affected by the performance of all the parts within the pipeline including the quality of OCR engine and recall of the candidate generators. All experiments below use a batch size of 256, the Rectified Adam optimizer <ref type="bibr" target="#b8">[9]</ref> with a learning rate of 0.001. These hyperparameters were found to be optimal using a grid-search. We train the models using an 80-20 train-validation split for up to 25 epochs in each stage, and pick the checkpoint with the best validation ROC AUC for the scorer model, which typically occurs in fewer than 10 epochs. Experiments on generalization to new language all use a vocabulary consisting of the 2k most frequent tokens occurring in English and French Invoice documents. Experiments on generalization to new document type all use a vocabulary consisting of the 4k most frequent occurring in Paystub and English Invoice documents. All experiments were repeated using 10 different seeded random initializations of the model, and we report the median performance on a fixed, holdout test set of target domain documents along with error bars that show the variance across these runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Domain Transfer Learning</head><p>Our proposed multi-domain transfer learning approach is a natural extension of the representation learning ideas used in Glean. For the remainder of this paper, we will refer to the document type  <ref type="table">1</ref>: Dataset statistics. # Docs refer to the number of documents in the corpora, and # Fields refer to the number of fields in the schema for the given document type.</p><p>or language that we already have enough labeled examples for as source domain; and the document type or language that we would like to generalize to but not have enough labeled examples as target domain. Note that this is of great practical importance, as it is often simply not feasible to gather and label datasets of large size for every new document type or language.</p><p>We build on the hypothesis of Glean that form-like documents share a visual design language and that the candidate encoder within the ML-based Scorer we use can effectively learn to represent the domain-agnostic spatial relationships -between the candidate and its neighbors -that are critical to understand so that we can have a holistic understanding of form-like documents.</p><p>Another key observation we make is that the candidate encoder learns embeddings for the neighboring tokens, and these word embeddings, while agnostic to the field, are likely to be specific to the domain used for training. Thus, at Stage 1, we ( <ref type="formula">1</ref>) combine the extraction candidates from both the source and target domains, and (2) use a common vocabulary across both these domains while training Glean's ML-based Scorer model. Significant performance benefit we observe over simple transfer learning approaches stems from these two key decisions based on our domain-specific observations, as they make the learned candidate representations more general. Field-specific information continues to be encoded in the field embeddings within the same latent space. At Stage 2, we simply fine-tune both the candidate encodings and field embeddings on the target domain. Note that this framework can easily be extended to an arbitrary number of source and target domains.</p><p>Our results shown in Figure <ref type="figure" target="#fig_1">3</ref> indicate that our proposed multidomain transfer learning approach enables remarkable data-efficient generalization both from English Invoices to considerably structurallydifferent new document type Paystubs, and from English Invoices to French Invoices -consistently improving over both training from scratch and simple transfer learning baselines up to 1k labeled documents. Summary of all compared methods is described in Figure <ref type="figure" target="#fig_2">4</ref>. Note that domain refers to document type in the first described setting, and it refers to language in the second described setting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Katti et al. <ref type="bibr" target="#b7">[8]</ref> propose inputting documents as 2D grids of text tokens to fully convolutional encoder-decoder networks. Denk and Reisswig <ref type="bibr" target="#b4">[5]</ref> incorporate pretrained BERT text embeddings into that 2D grid representation. Xu et al. <ref type="bibr" target="#b13">[14]</ref> propose integrating 2D position embeddings and image embeddings, produced with a Faster R-CNN <ref type="bibr" target="#b11">[12]</ref> model, into the backbone structure of a BERT language model <ref type="bibr" target="#b5">[6]</ref> and using a masked visual-language loss during pre-training. Similarly, Garncarek et al. <ref type="bibr" target="#b6">[7]</ref> propose integrating the 2D layout information into the backbone structure of both BERT and RoBERTa <ref type="bibr" target="#b9">[10]</ref>, where they construct layout embeddings using a graph neural network using a heuristically constructed document graph. In contrast to these pre-training based approaches, Glean extraction system that we build on (1) requires several orders of magnitude less labeled training data (2) an order of magnitude less training and inference time for the parts of the extraction system that use ML, without sacrificing the generalization ability to new document types and languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We argued that data-efficiency will be immensely critical as the information extraction systems in production will increasingly need to perform well across more document types, more languages, and potentially on private customer data -ideally without sacrificing the generalization ability and the training and inference time of the parts of the extraction system that use ML. We hope that our preliminary results will help start the discussion on the importance of data-efficiency while building the next generation information extraction systems tailored to form-like documents for production use. We believe that next big step will be to decrease the labeled document need from ∼1k to ∼100 for each new (n+1)th document type or language we would like to generalize to.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A candidate's score is based on the similarity between its embedding and a field embedding. A date extraction candidate "10/22/18" is shown in blue, along with its neighboring tokens, shown in orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Both learning curves use the backbone architecture described in Section 2 and compare train from scratch and transfer learning baselines to our proposed multi-domain transfer learning approach. Left figure shows generalization ability from English Invoices to French Invoices, and right figure demonstrates generalization ability from English Invoices to Paystubs. All fields included in this analysis have above 80% candidate generation coverage and have at least 40 ground truth label in their corresponding test set. French Invoices document type have 12 and Paystubs have 19 fields in their target schemas. Both figures show median performance at different number of labeled documents for the target domain (new document type or language we are trying to generalize to) along with error bars that show variance across 10 different seeds. Proposed multidomain transfer learning approach consistently improves on both baselines up to 1k labeled documents in the target domain, and the improvement is particularly significant in the low data regime. Source domain for both figures is English Invoices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: All compared methods use the same backbone described in Section 2. Please refer to Figure2for high-level descriptions of Field Embedding and Candidate Encoder, and to<ref type="bibr" target="#b10">[11]</ref> for specifics of the ML-based Scorer architecture.included in this analysis have above 80% candidate generation coverage and have at least 40 ground truth label in their corresponding test set. French Invoices document type have 12 and Paystubs have 19 fields in their target schemas. Both figures show median performance at different number of labeled documents for the target domain along with error bars. Our proposed multi-domain transfer learning approach consistently improves on both baselines up to 1k labeled documents in the target domain, and the improvement is particularly significant in the low data regime. Source domain for both cases is English Invoices.The value of our proposed approach is particularly impressive in the low data regimes. Specifically, we improve on the training from scratch baseline by up to 35 F1 points, and on the simple transfer learning baseline by up to 8 F1 points for the 50 labeled document case while generalizing to a new document type. Similarly, we improve on the training from scratch baseline by up to 23 F1 points, and on the simple transfer learning baseline by up to 7 F1 points for the 10 labeled document case while generalizing to a new language. We also would like to point out that (1) source model training takes</figDesc><graphic url="image-5.png" coords="4,53.80,383.58,252.19,58.13" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://developers.google.com/knowledge-graph</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.50</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2013.50" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting Content Structure for Web Pages Based on Visual Representation</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-36901-5_42</idno>
		<ptr target="https://doi.org/10.1007/3-540-36901-5_42" />
	</analytic>
	<monogr>
		<title level="m">Web Technologies and Applications, APWeb</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="406" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Block-based web search</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/1008992.1009070</idno>
		<ptr target="https://doi.org/10.1145/1008992.1009070" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Template Mining for Information Extraction from Digital Documents</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gobinda</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdhury</surname></persName>
		</author>
		<ptr target="https://www.ideals.illinois.edu/handle/2142/8258" />
	</analytic>
	<monogr>
		<title level="j">Library Trends</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="182" to="208" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><surname>Reisswig</surname></persName>
		</author>
		<idno>ArXiv abs/1909.04948</idno>
		<ptr target="http://arxiv.org/abs/1909.04948" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N19-1423/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LAMBERT: Layout-Aware language Modeling using BERT for information extraction</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Garncarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Stanislawek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Gralinski</surname></persName>
		</author>
		<idno>ArXiv abs/2002.08087</idno>
		<ptr target="https://arxiv.org/abs/2002.08087" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chargrid: Towards Understanding 2D Documents</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordula</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Höhne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faddoul</forename><surname>Baptiste</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1476/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4459" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgz2aEKDr" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Learning Representations</title>
				<meeting>the 9th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv abs/1907.11692</idno>
		<ptr target="https://arxiv.org/abs/1907.11692" />
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation Learning for Information Extraction from Form-like Documents</title>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Potti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Tata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Najork</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.580</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.580" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6495" to="6504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2577031</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2577031" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glean: Structured Extractions from Templatic Documents</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Tata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Potti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauro Beltrao</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<idno type="DOI">10.14778/3447689.3447703</idno>
		<ptr target="https://doi.org/10.14778/3447689.3447703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment. 997-1005</title>
				<meeting>the VLDB Endowment. 997-1005</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LayoutLM: Pre-training of Text and Layout for Document Image Understanding</title>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403172</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403172" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 1192-1200</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 1192-1200</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving pseudorelevance feedback in web information retrieval using web page segmentation</title>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/775152.775155</idno>
		<ptr target="https://doi.org/10.1145/775152.775155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on World Wide Web. 11-18</title>
				<meeting>the 12th International Conference on World Wide Web. 11-18</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous record detection and attribute labeling in web data extraction</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150457</idno>
		<ptr target="https://doi.org/10.1145/1150402.1150457" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="494" to="503" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
