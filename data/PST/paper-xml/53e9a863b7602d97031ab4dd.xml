<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Illustrative visualization of 3D planning models for augmented reality in liver surgery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-06-19">19 June 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Christian</forename><surname>Hansen</surname></persName>
							<email>christian.hansen@mevis.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Wieferich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Ritter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Rieder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heinz-Otto</forename><surname>Peitgen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H.-O</forename><surname>Peitgen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fraunhofer</forename><surname>Mevis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insitute for Medical Image Computing</orgName>
								<address>
									<settlement>Bremen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Illustrative visualization of 3D planning models for augmented reality in liver surgery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-06-19">19 June 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">25EC93F60C40E9B0299E89F97BF018AD</idno>
					<idno type="DOI">10.1007/s11548-009-0365-3</idno>
					<note type="submission">Received: 17 December 2008 / Accepted: 14 May 2009 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Intraoperative visualization</term>
					<term>Augmented reality</term>
					<term>Image-guided surgery</term>
					<term>Illustrative rendering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Purpose Augmented reality (AR) obtains increasing acceptance in the operating room. However, a meaningful augmentation of the surgical view with a 3D visualization of planning data which allows reliable comparisons of distances and spatial relations is still an open request. Methods We introduce methods for intraoperative visualization of 3D planning models which extend illustrative rendering and AR techniques. We aim to reduce visual complexity of 3D planning models and accentuate spatial relations between relevant objects. The main contribution of our work is an advanced silhouette algorithm for 3D planning models (distance-encoding silhouettes) combined with procedural textures (distance-encoding surfaces). In addition, we present a method for illustrative visualization of resection surfaces. Results The developed algorithms have been embedded into a clinical prototype that has been evaluated in the operating room. To verify the expressiveness of our illustration methods, we performed a user study under controlled conditions. The study revealed a clear advantage in distance assessment with the proposed illustrative approach in comparison to classical rendering techniques. Conclusion The presented illustration methods are beneficial for distance assessment in surgical AR. To increase the safety of interventions with the proposed approach, the reduction of inaccuracies in tracking and registration is a subject of our current research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent planning software for liver interventions enables physicians to inspect 3D models of patients' anatomical structures and provides valuable risk analyses and resection plans <ref type="bibr" target="#b0">[1]</ref>. This information allows preoperative assessment of surgical risks and can support the intraoperative navigation of surgical instruments. Although advanced navigation systems for liver surgery have been introduced into clinical practice <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, and surface-based tracking enables the adaptation of planning data to the intraoperative movement of the liver <ref type="bibr" target="#b4">[5]</ref>, the style and display location of 3D planning models is often inappropriate with respect to surgical user requirements.</p><p>In open liver surgery, the planning models are often presented on a display in front of the surgeon (Fig. <ref type="figure">1a</ref>). During laparoscopic interventions, two separate viewports are provided in conventional systems, i.e., a live camera stream and a presentation of the planning models (Fig. <ref type="figure">1b</ref>). Based on our observations during such interventions and many discussions with surgeons, we conclude that a mental fusion of planning models with the current surgical view is error-prone. Furthermore, it frequently results in distracting comparisons during the intervention that consume an unacceptable amount of time.</p><p>Numerous surgical applications for Augmented Reality (AR) apply classical rendering methods (e.g., Gouraud or Phong Shading) to overlay the graphical information. However, opaque planning models such as vascular structures, organ surfaces, and tumors can occlude the surgical view in a way that is inappropriate in surgical routine. The use of transparency, on the other hand, complicates the perception of relative depth distances between surfaces, particularly if manual rotation of the model (perception of motion parallax) is not possible. Moreover, the assessment of spatial relations in static images is difficult even when opaque models are 123 Fig. <ref type="figure">1</ref> a Intraoperative visualization of 3D planning models on the screen of a navigation system for open liver surgery at General Hospital Celle <ref type="bibr" target="#b1">[2]</ref> and b for laparoscopic surgery at University Hospital LÃ¼beck <ref type="bibr" target="#b2">[3]</ref> presented <ref type="bibr" target="#b5">[6]</ref>. To improve the understanding of spatial relations and depth, we introduce illustrative visualization methods for complex 3D planning models that encode relevant distance information. The methods are integrated in an AR application for liver surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>In the context of AR in liver surgery, a variety of concepts have already been developed. Our work is inspired by two research areas: AR and illustrative visualization. Therefore, we review work in the field of AR for liver interventions and previous work on improvement of depth perception in medical AR. Subsequently, we describe related illustrative visualization approaches on which we base our augmentation rather than on classical rendering techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmented reality for liver interventions</head><p>Medical AR has a long history in science; a comprehensive literature review can be found in Sielhorst et al. <ref type="bibr" target="#b6">[7]</ref>. The basic approach to employ AR during liver interventions is described by Ayache et al. <ref type="bibr" target="#b7">[8]</ref>. They proposed to augment intraoperative video images with an associated 3D-reconstruction of the liver surface using alpha compositing. Samset et al. <ref type="bibr" target="#b8">[9]</ref> used AR to educate surgeons in radiofrequency ablations of liver tumors. Using a head-mounted display (HMD), interventional procedures are trained on phantoms without the risk of performing an invasive intervention in reality. Nicolau et al. <ref type="bibr" target="#b9">[10]</ref> introduced a guidance system for liver percutaneous punctures that superimposes planning models on video images of the interventional view. Alpha compositing is used to achieve semi-transparent planning models. However, if AR applications apply transparency to superimpose planning models on the surgical view, visual depth cues can be degraded through lower contrast.</p><p>Feuerstein et al. <ref type="bibr" target="#b10">[11]</ref> applied direct volume visualization of intraoperative retrieved CT data to superimpose laparoscopic video images for trocar placement in liver surgery. A drawback therein is its limitation to the intraoperative processed segmentation result, which does not provide an accentuation of risk structures and spatial relations. Several groups <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> in the field of laparoscopy guidance applied transparency-based superimpositions, similar to <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, in order to achieve a superimposition of laparoscopic video images with planning information, which could also lead to misinterpretations.</p><p>Projector-based AR represents an interesting way to support surgical decisions: Krempien et al. <ref type="bibr" target="#b13">[14]</ref> and Riechmann et al. <ref type="bibr" target="#b14">[15]</ref> showed that a projector can be used not only for intraoperative visualization, but also for the registration of the patient's organ using structured light techniques. However, in preliminary studies in collaboration with the Institute of Process Control and Robotics, University of Karlsruhe, Germany <ref type="bibr" target="#b14">[15]</ref> we found that complex 3D planning models (such as vascular structures) shaded with classical rendering methods are inappropriate for intraoperative projection. The projected image provides insufficient visual contrast which results in a crucial loss of spatial information (Fig. <ref type="figure" target="#fig_0">2</ref>). Glossop et al. <ref type="bibr" target="#b15">[16]</ref> showed that the application of laser projectors is conceivable. The advantage of laser projectors is an unlimited depth of field, although it represents a safety risk for the unprotected eyes of the surgical staff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth perception in medical augmented reality</head><p>The surgical need to assess spatial information of planning models during an intervention has lead to the development of several techniques that attempt to improve depth perception in AR applications. In the context of projector-based AR, Riechmann et al. <ref type="bibr" target="#b14">[15]</ref> proposed to project vascular structures onto an organ surface via projective texture mapping while tracking the surgeons head. Thus, an important depth cue (motion parallax) is provided by taking the observer's position into account. However, a permanent tracking via head-attached tracking applicators could affect the surgical workflow, e.g., by forcing a surgeon to move his head to improve depth perception.</p><p>Multiple viewports have been proposed to enhance depth perception in AR without forcing observers to change their viewing position or to rotate the model. Navab et al. <ref type="bibr" target="#b16">[17]</ref> presented a render-to-texture approach, termed virtual mirror, for monitor-based AR which provides additional views on the planning model. Particularly, the interpretation of partial self-occlusions inside complex planning models is improved.</p><p>One way to add depth cues to AR is to extract depth information from the associated video images and use this data to control the superimposition. Lerotic et al. <ref type="bibr" target="#b17">[18]</ref> utilize photometric stereo to derive the orientation of the organ surface. This information is used to generate a translucent contour layer that preserves sufficient details to aid navigation and depth cueing. Bichlmeier et al. <ref type="bibr" target="#b18">[19]</ref> use surface topology, viewing attributes, and the location of surgical instruments to generate a transparency map that is applied to control the pixel transparency of video images. Thus, a context-preserving focus region is provided that facilitates intuitive depth perception. Moreover, a optical see-through HMD with stereoscopic imagery is used to provide an augmented view. However, head-mounted displays can handicap a surgeon during interventions and need further technological improvement before application in the operating room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Illustrative visualization in medicine</head><p>Based on traditional illustration techniques, a variety of non-photorealistic methods have been proposed to apply silhouettes and hatching strokes in order to increase expressiveness of visualizations. An overview can be found in Strothotte et al. <ref type="bibr" target="#b19">[20]</ref>. Whereas the field of non-photorealistic rendering is concerned with imitating artistic styles in an automated way, illustrative rendering applies these techniques to enhance visual comprehension; for a survey of illustrative rendering techniques we refer to the thesis of Bruckner <ref type="bibr" target="#b20">[21]</ref>.</p><p>We were inspired by the work of Fischer et al. <ref type="bibr" target="#b21">[22]</ref> who developed an illustrative rendering technique that is capable of generating a stylized augmented video stream. Based on an edge-detection algorithm, silhouttes are extracted and applied to both the camera image and the virtual objects. Thus, visual realism of the graphical foreground and the real camera image is reduced. Both modalities become less distinguishable from each other and thus an improved immersion can be achieved.</p><p>Our project is based on prior work in the field of vascular visualization: Ritter et al. <ref type="bibr" target="#b5">[6]</ref> presented vascular visualization methods, which extend illustrative rendering techniques to particularly accentuate spatial depth and to improve the perceptive separation of important vascular properties such as branching level and supply area. In addition to a GPU-based hatching algorithm for tubular structures (distance-encoding surfaces), shadow-like depth indicators (distance-encoding shadows) are introduced, which enable reliable comparisons of depth distances. Important techniques on which our work is based have been described by Freudenberg <ref type="bibr" target="#b22">[23]</ref>, who introduced an algorithm to generate stroke textures procedurally that is further developed by Ritter et al. <ref type="bibr" target="#b5">[6]</ref> to visualize distance-encoding surfaces. Moreover, Isenberg et al. <ref type="bibr" target="#b23">[24]</ref> described techniques for the stylization of silhouettes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>We aim to accentuate spatial relations and reduce visual complexity of 3D planning models via GPU-accelerated illustrative rendering techniques. It is a prerequisite of our approach that planning information is either projected onto the liver surface during open liver surgery using a light projector as described in <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, or rather superimposed with the images from a laparoscopic camera <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Since the surgical view is augmented with planning models, we expect a reduction of cognitive demands for the surgeon, concerning distracting comparisons of spatially separated views during the intervention. However, as mentioned in the related work section, an augmentation of the surgical view by planning models may result in unacceptable occlusions of the operation field or misinterpretation of spatial relations, colors, and contrast. Therefore, our visualization approach is guided by four requirements:</p><p>â¢ spatial depth of planning models must be perceivable, even in static images,</p><p>â¢ occlusion of the surgical view by planning models should be minimal, â¢ transitions in color and brightness must be avoided in order to ensure a maximal contrast, â¢ required technical devices should not handicap the surgeon.</p><p>To test the usability of our methods, we specified three visualization scenarios in collaboration with experienced liver surgeons. These scenarios represent surgical situations wherein expressive visualizations are requested:</p><p>(1) Anatomical overview: This scenario contains all tumors identified preoperatively and their relations to relevant vascular structures. Besides providing an abstract overview of available planning objects, this visualization scenario allows fast assessment of alignment errors between the real world and the virtual world. (2) Focusing the current tumor: During the treatment of a specific tumor, this visualization provides information about surrounding risk structures, such as vessels that are invisible to the surgeon. (3) Focusing the virtual resection surface: In case of a precise prepared resection strategy, this scenario provides spatial information of the virtual resection surface, while enhancing its relation to risk structures.</p><p>Predefined views for each scenario are generated in advance and provided intraoperatively. In the following subsections we focus on the developed illustrative rendering techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance-encoding silhouettes</head><p>Silhouettes play an important role in figure-to-ground distinction and can be applied to reduce the visual complexity of geometric models. Reduction of visual complexity is a basic requirement of our visualization approach. However, the abstraction of a classical shaded object to its silhouette results in the loss of shading information and consequently in a reduction of depth cues. Therefore, we enhanced conventional silhouette algorithms by implementing two optional rendering settings. Our first extension allows for varying the stroke thickness of silhouettes continuously by using the distance to relevant objects (organ surface, adjacent risk structures, or surgical tracked instruments) as input (Fig. <ref type="figure" target="#fig_1">3</ref>). The distance-dependent scaling of silhouttes is similar to the concept described by Isenberg et al. <ref type="bibr" target="#b23">[24]</ref>, although we control the stroke thickness on the GPU using two framebuffer objects. The algorithm is based on a translation of each vertex of the 3D planning model (i.e., vascular tree) in direction of its normal by a vertex shader. Utilizing multiple render targets, we calcu- late the silhouette by subtracting the original planning model from the scaled model in a fragment program. We vary the length of the applied vertex translation by calculating a distance value (e.g., the distance between a vertex and the tip of a tracked surgical instrument) via built-in shader functions or by exploiting a precomputed 3D distance map via texture lookup. Our algorithm controls stroke thickness within a user-defined interval (minimum and maximum stroke thickness). Irrelevant parts of the model can be omitted.</p><p>Our second extension uses different stroke styles (solid, dashed, dotted) to accentuate view-dependent spatial relations (in front, within, behind) of interweaved objects. The developed rendering styles are particularly important for vessels that intersect other planning models such as resection volumes, territories at risk, or tumors. The stroke styles are varied by means of a sawtooth function in a fragment program. Occluded objects are detected using the depth buffer, whereas overlapping objects are identified by a texture lookup in the underlying 3D segmentation masks of planning models. Figure <ref type="figure" target="#fig_2">4</ref> shows an example of a close-up view of a tumor with surrounding vessels rendered with classical shading (Fig. <ref type="figure" target="#fig_2">4a</ref>), conventional silhouettes (Fig. <ref type="figure" target="#fig_2">4b</ref>), and our new approach termed distance-encoding silhouettes (Fig. <ref type="figure" target="#fig_2">4c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance-encoding surfaces</head><p>Distance-encoding surfaces provide the observer with distance information displayed on the surface of geometric objects. This technique was introduced by Ritter et al. <ref type="bibr" target="#b5">[6]</ref> to visualize the distance of vascular structures to the observer by using texture gradients as additional depth cues (Fig. <ref type="figure" target="#fig_3">5</ref>). A procedural stroke texture with varying stroke thickness is used for this purpose.</p><p>Instead of applying a hatching texture on the whole vessel system, we exploit a distance-based transfer function to limit the use of texture to a specific scope. Thus, distances between  arbitrary planning objects can be visualized, e.g., vessels at risk can be accentuated while their spatial relation to other objects (organ surface, vascular territories, tracked surgical instruments) is encoded by distance-encoding silhouettes. In addition, this enables the combination of distance-encoding surfaces with distance-encoding silhouettes. Figure <ref type="figure">6</ref> shows an example for the combination of both techniques: vessels at risk are emphasized using a distance-encoding surface while a distance-encoding silhouette highlights branches close to the organ surface.</p><p>Regarding the tumor scenario, spatial relations between a tumor and surrounding risk structures have to be visualized. Besides the distance between tumors and vessels, the location of a vessel (in front, inside, or behind a specific tumor) has to be clearly perceivable in order to support surgical decisions. Inspired by standardized conventions in technical drawings we propose to encode spatial relations as follows (Fig. <ref type="figure">7a</ref>): Vessels in front of the volume are encoded by the union of the distance-encoding surface and the distance-encoding silhouette. Vessels within the volume are rendered as solid silhouette, while occluded vessels are rendered as dotted or dashed silhouette. In order to achieve corresponding stroke and texture frequencies, the silhouette style for occluded vessels is controlled by the same sawtooth function as the distance-encoding surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of resection surfaces using contour lines</head><p>If a resection plan has been created before surgery, the aim of an intervention is to execute the preoperatively planned resection as accurately as possible. If a 3D model of the resection surface is used for this purpose, it is important to provide the surgeon with reliable information about distances of the virtual resection surface to other relevant objects such as the liver surface, vessels, or surgical instruments. We found that contour lines (also called isolines) are appropriate for this purpose. They provide an efficient representation of continuous data change, which is often used on topographic maps to represent points of equal value.</p><p>For the visualization of virtual resection surfaces, we project contour lines onto the outer shape of resection volumes. The distance between contour lines is controlled by exploiting a precomputed Euclidian distance map. This distance map encodes the shortest distance of each liver voxel to the liver surface. Thus, line thickness can be held constant or linearly varied depending on a distance function in a fragment program. In addition, the distances between lines can be adjusted, e.g., by 5 mm, which facilitates a quantitative assessment of spatial depth. As illustrated in Fig. <ref type="figure">7b,</ref><ref type="figure">c</ref>, the proposed contour lines can also be combined with distanceencoding silhouettes and distance-encoding surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In a previous user study <ref type="bibr" target="#b5">[6]</ref>, we conducted a quantitative evaluation of the effect of stroke hatching, distance-encoding surfaces, and shadows on distance assessment. Using a web-based questionnaire, we were able to test 160 participants. The study clearly indicated the advantages of direct distance-encoding in 3D visualizations of planning models. Compared to classical rendering methods, participants were significantly better and faster in judging distances.</p><p>In the recent study, design considerations discussed in this article were verified under controlled lab conditions using verbal comments as data according to the Think-Aloud protocol <ref type="bibr" target="#b24">[25]</ref>. For this purpose, an electronic questionnaire was created that contained single video frames from laparoscopic and open liver interventions with overlayed planning models as well as photos from a projector-based AR visualization using liver of a pig cadaver. In order to probe the subjects' perception of spatial relations, we designed tasks that required a precise judgment of distances. Three vessel positions in each image were labeled with markers, while each task started with a question about the distance of these markers to a second object (e.g., tumor, resection surface or organ surface). Participants were asked to determine the correct order of marks. Since we wanted to assess the effect of the new techniques, a visualization of the planning model using one of the new techniques had to be compared with a second visualization of the same model that was identical in every aspect except for the rendering algorithms. This requirement has been met by using the same model and viewpoint. Each visualization scenario was evaluated using three paired tests in different sequences: Fig. <ref type="figure">8</ref> shows an example for a paired test for scenario 1 (overview scenario), Fig. <ref type="figure">9</ref> for scenario 2 (focusing the current tumor), and Fig. <ref type="figure">10</ref> for scenario 3 (focusing the resection surface). The first paired test for each scenario was conducted with video images from open liver surgery, the second paired test with video images from laparoscopic liver surgery, and the third paired test with images from planning models projected on a pig's liver using a light projector. Six liver specialists participated in the study: two surgeons, two radiologist, and two medical To evaluate the surgical applicability of the new methods, we also conducted preliminary studies in the operating room. Therefore, the images from a video camera focusing the patient's liver during an open liver intervention were captured. A rigid registration of the planning model to one video frame was carried out while the liver was immobilized. Subsequently, associated planning models were superimposed onto the video stream which itself was presented on a display in front of the surgeon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and conclusion</head><p>New illustration methods for 3D planning models have been described. The methods use silhouettes and texture gradients to efficiently communicate shape and topology without the requirement of a medium able to display color. Hence, color may be used to encode additional information.</p><p>The recent expert study confirmed the results of our previous quantitative study <ref type="bibr" target="#b5">[6]</ref> in the field of explicit distanceencoding. Compared to classical rendering methods, all six participants judged distances faster and better. Since the advantage of explicit distance-encoding had already been stated in the previous study, a statistical analysis of our measurements was not carried out. However, experts have been asked to express their thoughts on the application while executing the tasks using a Think-Aloud protocol. In case of an inaccurate distance judgement, participants were informed and asked to describe their decision in detail. Thus, a number of constructive suggestions were made:</p><p>1. Although we aimed at reducing the complexity of planning models, participants reported that too much information was presented in scenario 3. One surgeon proposed to use the position of a tracked surgical instrument to select a specific depth layer of the resection surface. In addition, vessels that do not supply or drain healthy parenchyma could be omitted. 2. When presenting scenario 3, several experts asked for a stronger emphasis of intersections between vessels and resection surface in order to judge depth. Similar request were made for scenario 2, in which the intersections between tumor and vessels are important. 3. In several tests, we presented two independent parameters. One parameter was encoded using distance-encoding surfaces, whereas the second parameter was visualized using distance-encoding silhouettes. However, a few participants confused the parameters with each other. After being informed of the mistake, subjects proposed to enhance the visualization with captions.</p><p>4. Concerning the projector-based AR approach, the assessment of distance of an object from varying stroke styles (dashed, dotted) was not successful in all cases. Participants complained that the variation in style and frequency is not always perceivable on the liver surface. 5. To assess the spatial relation of tumors and vessels in scenario 2, participants asked for an improved visualization of tumors. Particularly, quantitative distance information and additional shape hints would be necessary.</p><p>Since cognitive loads during surgical interventions put demands on the usability of intraoperative applications, the reduction of information presented to the surgeon is of high importance. The choice of relevant information has to be made in close collaboration with liver surgeons. The proposed approach utilizes innovative visual encoding. Thus, the usability of the visualization correlates with the surgeon's familiarity with these concepts. It seems promising to integrate the illustrative techniques in software assistants for surgical training.</p><p>As mentioned in the previous section, the developed algorithms have been embedded into a clinical prototype that has already been used in the operating room for preliminary evaluations. Subsequent discussions indicated that the presented visualization exhibits considerable advantages compared to traditional intraoperative visualization methods. However, the performed surface-registration was only valid for a few seconds, since we did not track the movement of the liver surface during the test. Therefore, the surgeon could not make reliable statements about the surgical benefit of the augmentation. After analyzing several intraoperative video streams from open liver interventions, we conclude that a permanent augmentation of the surgical view is (to our knowledge) not possible with liver tracking techniques available today. However, the precise placement of initial cuts is important for the success of an intervention, and this surgical task can be supported with the visualization methods proposed here. A surgical workflow analysis as described by Blum et al. <ref type="bibr" target="#b25">[26]</ref> could reveal further areas of application.</p><p>In the near future, we will investigate the use of real-time video analysis. The superimposition (or projection) could be limited to the organ in order to exclude structures such as surgical instruments from the augmentation. In addition, anatomical details can be preserved similar to the approach described by Lerotic et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>The expectations of surgeons on accuracy and stability in AR settings are high. By now, the virtual planning model is transformed onto the patient's liver using a manual registration approach. Thus, the visualization is only accurate if the liver is immobilized. To alleviate this limitation, we plan to derive registration information from an ultrasound-based navigation system to facilitate an image fusion. In addition, a surface-based tracking method has to be developed to update the registration. However, inaccuracies in camera calibration, tracking, and registration have to be considered. Therefore, the visualization of uncertainty (quality of the alignment between virtual and real world) is important for clinical acceptance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 First experiments in the field of projector-based AR for open liver surgery carried out by the Institute of Process Control and Robotics, University of Karlsruhe, Germany and MeVis at Robert Bosch Hospital Stuttgart, Germany</figDesc><graphic coords="3,52.57,56.30,234.28,148.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Silhouette thickness depends on the distance to the observer. A rotation about 180 â¢ of the left model along the vertical axis changes stroke thickness in the right model and thus adapts the accentuation of specific parts. Compare vessel marks a, b, c, d</figDesc><graphic coords="4,307.18,56.63,235.72,130.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 a</head><label>4</label><figDesc>Fig. 4 a Focusing a tumor using classical rendering techniques. b A silhouette representation of the scene results in a loss of depth cues. Occluded vessel branches are not visible. c Distance-encoding</figDesc><graphic coords="5,116.14,56.12,363.40,106.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig.<ref type="bibr" target="#b4">5</ref> The distance to the observer is encoded by the thickness of strokes. A sawtooth function Ï is compared with a distance function Î´. If the value of Ï is greater than the value of Î´, a black fragment is generated, otherwise a white fragment (idea by Freudenberg<ref type="bibr" target="#b22">[23]</ref>)</figDesc><graphic coords="5,52.57,219.86,234.28,118.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 Fig. 7 a</head><label>67</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref> Classical rendering (a) in comparison to the illustrative approach (b). Whereas the spatial relations between vessels and tumors are difficult to perceive in (a), vessels at risk are accentuated in (b) using a varying stroke texture, termed distance-encoding surface. The</figDesc><graphic coords="5,53.65,449.84,487.69,216.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 Fig. 9</head><label>89</label><figDesc>Fig. 8 Scenario 1 for open liver surgery: Superimposition using classical rendering techniques (a) and the proposed illustrative approach (b). Distance-encoding silhouettes are applied to visualize the distances</figDesc><graphic coords="7,98.08,57.05,397.00,161.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,181.24,56.45,362.92,170.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,53.65,56.57,487.69,131.56" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was funded by the Federal Ministry of Education and Research (SOMIT-FUSION project FKZ01-BE03C). The authors express gratitude to all involved physicians of the FUSIONproject, in particular Karl Oldhafer, Gregor Stavrou (General Hospital Celle, Germany), and Hauke Lang (University Hospital Mainz, Germany) for fruitful discussions. We thank Stefan Weber (University of Bern, Switzerland), Mathias Markert (Technical University Munich, Germany), Armin Besirevic, Volker Martens, and Stefan Schlichting (University of LÃ¼beck, Germany) for providing video and image data and their valuable ideas concerning this work. Finally, we would like to thank all anonymous reviewers and the participants of the user study for their constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clinical relevance of model based computer-assisted diagnosis and therapy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zidowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourquain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hindennach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitgen</forename><surname>Ho</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.780270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE Medical Imaging</title>
		<meeting>SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6915</biblScope>
			<biblScope unit="page" from="691502" to="691503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How to operate a liver tumor you cannot see</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Oldhafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Stavrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Peitgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Lueth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00423-009-0469-9</idno>
	</analytic>
	<monogr>
		<title level="j">Langenbecks Arch Surg</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="489" to="494" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prototype of an intraoperative navigation and documentation system for laparoscopic radiofrequency ablations: first experiences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlichting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Besiveric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kleemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Roblick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mirow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schweikard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bruch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejso.2007.04.017</idno>
	</analytic>
	<monogr>
		<title level="j">Eur J Surg Oncol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="418" to="421" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Upgrade of an optical navigation system with a permanent electromagnetic position control: a first step towards &quot;navigated control&quot; for liver surgery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eulenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>HÃ¼nerbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Schlag</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00534-008-0040-z</idno>
	</analytic>
	<monogr>
		<title level="j">J Hepatobiliary Pancreat Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="170" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Concepts and preliminary data toward the realization of image-guided liver surgery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Miga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Glasgow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Galloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Chapman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11605-007-0090-6</idno>
	</analytic>
	<monogr>
		<title level="j">J Gastrointest Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="844" to="859" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time illustration of vascular structures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dicken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitgen</forename><surname>Ho</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2006.172</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Vis Comput Graph J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="884" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advanced Medical Displays: A literature review of augmented reality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sielhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feuerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1109/JDT.2008.2001575</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/OSA J Disp Technol; Special Issue on Medical Displays</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="467" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Epidaure: a research project in medical image analysis, simulation and robotics at INRIA</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2003.812863</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1185" to="1201" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Augmented reality in surgical procedures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Samset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Vander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Freudenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Casciaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã</forename><surname>Rideng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gersak</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.784155</idno>
	</analytic>
	<monogr>
		<title level="j">Proc SPIE Med Imaging</title>
		<imprint>
			<biblScope unit="volume">6806</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68060K" to="68061" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An augmented reality system for liver thermal ablation: design and evaluation on clinical cases</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Buy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2009.02.003</idno>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="494" to="506" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intraoperative laparoscope augmentation for port placement and resection planning in minimally invasive liver resection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feuerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mussack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Heining</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2007.907327</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="355" to="369" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intraoperative augmented reality for minimally invasive liver interventions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scheuering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Greiner</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.480212</idno>
	</analytic>
	<monogr>
		<title level="j">Proc SPIE Med Imaging</title>
		<imprint>
			<biblScope unit="volume">5029</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="407" to="417" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Augmented-reality-assisted laparoscopic adrenalectomy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.292.18.2214-c</idno>
	</analytic>
	<monogr>
		<title level="j">J Am Med Assoc</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2214" to="2215" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Projector-based augmented reality for intuitive intraoperative guidance in imageguided 3D interstitial brachytherapy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krempien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kahrs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daeuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Munter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Debus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Harms</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijrobp.2007.10.048</idno>
	</analytic>
	<monogr>
		<title level="j">Int J Radiat Oncol Biol Phys</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="944" to="952" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualisierungskonzept fÃ¼r die projektorbasierte Erweiterte RealitÃ¤t in der Leberchirurgie</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Kahrs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raczkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lamade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>WÃ¶rn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc BMT</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Laser projection augmented reality system for computer-assisted surgery</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Glossop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0531-5131(03)00515-6</idno>
	</analytic>
	<monogr>
		<title level="j">Int Congr Ser</title>
		<imprint>
			<biblScope unit="volume">1256</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="71" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laparoscopic virtual mirror-new interaction paradigm for monitor based augmented reality</title>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feuerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bichlmeier</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2007.352462</idno>
	</analytic>
	<monogr>
		<title level="j">Virtual Reality Conference IEEE</title>
		<imprint>
			<biblScope unit="page" from="43" to="50" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pq-space based non-photorealistic rendering for augmented reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lerotic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Mylonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-75759-7</idno>
	</analytic>
	<monogr>
		<title level="j">Proc MICCAI</title>
		<imprint>
			<biblScope unit="volume">4792</biblScope>
			<biblScope unit="page" from="102" to="109" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual anatomic mimesis: hybrid in situ visualization method for improving multi-sensory depth perception in medical augmented reality</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bichlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Heining</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISMAR.2007.4538837</idno>
	</analytic>
	<monogr>
		<title level="m">ISMAR &apos;07: Proceedings of the 2007 6th IEEE and ACM international symposium on mixed and augmented reality</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Non-photorealistic computer graphics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Strothotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlechtweg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Interactive illustrative volume visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stylized augmented reality for improved immersion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bartz</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2005.1492774</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Virtual Reality</title>
		<meeting>the IEEE Conference on Virtual Reality</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Real-time stroke-based halftoning. PhD thesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Freudenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Otto-von-Guericke University Magdeburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stylizing silhouettes at interactive rates: from silhouette edges to silhouette strokes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Halper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strothotte</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-8659.00584</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<title level="m">Protocol Analysis: Verbal Reports as Data</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Workflow mining for visualization and analysis of surgeries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>FeuÃner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-008-0239-0</idno>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Assist Radiol Surg</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="386" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
