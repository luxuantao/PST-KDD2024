<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Infrared and visible image fusion methods and applications: A survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-02-12">February 12, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Ma</surname></persName>
							<email>mayong@whu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Electronic Information School</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>0 2 4 6 8 10 12 0 1 2 3 4 5 6 7 MI</addrLine>
									<postCode>8685</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>0 2 4 6 8 10 12 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7</addrLine>
									<postCode>8723</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>4865 0 2 4 6 8 10 12 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<addrLine>4805 0 2 4 6 8 10 12 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Qw</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<addrLine>0 2 4 6 8 10 12 10 20 30 40 50</addrLine>
									<postCode>7729, 60 70 80 SD</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<addrLine>15 20 25 30 35 1 1.5 2 2.5 3 3.5 4 MI</addrLine>
									<postCode>7798 0 5 10</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<addrLine>20 25 30 35 0.4 0.45 0.5 0.55 0.6 0.65 SSIM</addrLine>
									<postCode>5643 0 5 15</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<address>
									<addrLine>0 5 10 15 20 25 30 35 0.25 0.3 0.35 0.4 0.45 0.5 0.55</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<address>
									<addrLine>0 5 10 15 20 25 30 35 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<address>
									<addrLine>0 5 10 15 20 25 30 35 20 25 30 35 40 45 SD</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<address>
									<addrLine>0 5 10 15 20 25 30 35 6 8 10 12 14 16 18 20 22 SF</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<address>
									<postCode>9940 0 5 10 15 20 25 30 35 0.25 0.3 0.35</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<address>
									<postCode>3410 0 5 10 15 20 25 30 35 0.35 0.4 0.45 0.5 0.55 0.6 0.65</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Infrared and visible image fusion methods and applications: A survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-02-12">February 12, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">AEA6CBFDE64820A1FBE99241674EC6D6</idno>
					<idno type="DOI">10.1016/j.inffus.2018.02.004</idno>
					<note type="submission">Received date: 20 December 2017 Revised date: 7 February 2018 Accepted date: 11 February 2018 Preprint submitted to Information Fusion</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Fusion Image fusion</term>
					<term>infrared image</term>
					<term>visible image</term>
					<term>image registration</term>
					<term>evaluation metric LP:0.6040 Wavelet:0.6067 NSCT:0.6095 DTMDCT:0.5423 CBF:0.5898 HMSD:0.5792 GFF:0.5254 ADF:0.6078 ASR:0.6105 LPSR:0.6054 OIPCNN:0.4277 N-S-P:0.5086 DDCTPCA:0.5930 FPDE:0.6084 TSIFVS:0.6162 LEPLC:0.5664 GTF:0.4701 IFEVIP:0.5962</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Infrared images can distinguish targets from their backgrounds based on the radiation difference, which works well in all-weather and all-day/night conditions. By contrast, visible images can provide texture details with high spatial resolution and definition in a manner consistent with the human visual system. Therefore, it is desirable to fuse these two types of images, which can combine the advantages of thermal radiation information in infrared images and detailed texture information in visible images. In this work, we comprehensively survey the existing methods and applications for the fusion of infrared and visible images.</p><p>First, infrared and visible image fusion methods are reviewed in detail. Meanwhile, image registration, as a prerequisite of image fusion, is briefly introduced. Second, we provide an overview of the main applications of infrared and visible image fusion. Third, the evaluation metrics of fusion performance are discussed and summarized. Fourth, we select eighteen representative methods and nine assessment metrics to conduct qualitative and quantitative experiments, which can provide an objective performance reference for different fusion methods and thus support relative engineering with credible and solid evidence. Finally, we conclude with the current status of infrared and visible image fusion and deliver insightful discussions and prospects for future work. This survey can serve as a reference for researchers in infrared and visible image fusion and related fields.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• The IR and VIS image fusion methods and applications are comprehensively reviewed.</p><p>• The IR and VIS image registration, as a prerequisite of fusion, is briefly introduced.</p><p>• The evaluation metrics of fusion performance are discussed and summarized.</p><p>• Performances are evaluated on various real data using 18 methods and 9 metrics.</p><p>• Insightful discussions and prospects for future work are delivered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T 1. Introduction</head><p>Image fusion is an enhancement technique that aims to combine images obtained by different kinds of sensors to generate a robust or informative image that can facilitate subsequent processing or help in decision making. The keys to an excellent fusion method are effective image information extraction and appropriate fusion principles, which allow useful information to be extracted from source images and integrated in the fused image without introducing any artifact in the process <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b4">2]</ref>. In the age of information explosion, sensor techniques are being developed rapidly. The appearance of complicated applications requires comprehensive information about a certain scenario for enhanced understanding of various conditions. Sensors of the same type acquire information from only one aspect and are thus unable to provide all required information. Therefore, fusion techniques play an increasingly important role in modern applications and computer vision.</p><p>Images of different types, such as visible, infrared, computed tomography (CT), and magnetic resonance imaging (MRI), are good source images for fusion. Among the combinations of these types, infrared and visible image fusion is superior in many aspects. First, their signals come from different modalities, thereby providing scene information from different aspects; i.e., visible images capture reflected light, whereas infrared images capture thermal radiation. Therefore, this combination is more informative than that of single-modality signals. Second, infrared and visible images present characteristics that are inherent in nearly all objects. Furthermore, these images can be acquired by relatively simple equipment, as opposed to such imaging techniques as CT and MRI, which have rigorous requirements.</p><p>Finally, infrared and visible images share complementary properties, thus producing robust and informative fused images. Visible images typically have high spatial resolution and considerable detail and chiaroscuro; thus, they are suitable for human visual perception.</p><p>However, these images can be easily influenced by severe conditions, such as poor illumination, fog, and other effects of bad weather. Meanwhile, infrared images, which depict the thermal radiation of objects, are resistant to these disturbances but typically have low resolution and poor texture. Fusion techniques for visible and infrared images can thus be used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>in more fields than other fusion schemes due to the ubiquitous and complementary characteristics of the utilized images. Object recognition <ref type="bibr" target="#b5">[3]</ref>, detection <ref type="bibr" target="#b6">[4]</ref>, image enhancement <ref type="bibr" target="#b7">[5]</ref>, surveillance <ref type="bibr" target="#b8">[6]</ref>, and remote sensing <ref type="bibr" target="#b9">[7]</ref> are typical applications of infrared and visible image fusion.</p><p>Fusion algorithms can be developed with different schemes and generally divided into seven categories according to their adopted theories, i.e., multi-scale transform- <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b12">10]</ref>, sparse representation- <ref type="bibr" target="#b4">[2,</ref><ref type="bibr" target="#b13">11]</ref>, neural network- <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13]</ref>, subspace- <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b17">15]</ref>, and saliencybased <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b19">17]</ref> methods, hybrid models <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b21">19]</ref>, and other methods <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b23">21]</ref>. Multi-scale transform-based methods are the most active field in image fusion and assume images to be represented by various layers in different grains. These methods decompose source images into several levels, fuse corresponding layers with particular rules, and reconstruct the target images accordingly. Popular transforms used for decomposition and reconstruction include wavelet <ref type="bibr" target="#b24">[22]</ref>, pyramid <ref type="bibr" target="#b25">[23]</ref>, curvelet <ref type="bibr" target="#b26">[24]</ref>, and their revised versions. Sparse representation-based methods work on the basis of the possible representation of images with linear combinations of sparse bases in over-complete dictionaries, which are key to their good performance. Neural network-based methods imitate the perception behavior of the human brain to deal with neural information; this procedure has the advantages of good adaptability, fault tolerance, and anti-noise capacity. Subspace-based methods use similar ideas but implement them in spaces with complete bases instead of over-complete ones.</p><p>Principal component analysis (PCA), non-negative matrix factorization (NMF), and independent component analysis (ICA) are the main methods in this category. Saliency-based methods are based on the fact that human visual attention is often captured by objects or pixels that are more significant than their neighbors. In addition, these methods can retain the integrity of salient object regions and improve the visual quality of the fused images.</p><p>The abovementioned infrared and visible image fusion methods have their advantages and disadvantages. Meanwhile, hybrid models combine the advantages of these schemes to improve image fusion performance. The other techniques are based on total variation <ref type="bibr" target="#b22">[20]</ref>,</p><p>fuzzy theory <ref type="bibr" target="#b27">[25]</ref>, and entropy <ref type="bibr" target="#b23">[21]</ref>. These methods can lead to a new vision for infrared and visible image fusion, which can inspire the design of novel models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The research on fusion algorithms in theory and application has been rapidly developing in recent years. However, certain problems require attention. An appropriate information extraction method is key to ensuring good fusion performance. Several current approaches, especially multi-scale transform-based methods, depend on predefined transforms and corresponding levels for decomposition and reconstruction. However, no criteria are used to evaluate these transforms and levels. Consequently, transforms are selected blindly, thereby easily degrading performance <ref type="bibr" target="#b28">[26]</ref>. Moreover, current fusion rules are mostly excessively simple that certain artifacts, such as halo, are introduced in the results. Methods in other categories also suffer from several problems. For example, constructing suitable over-complete dictionaries that have good representation of target data using sparse representation-based methods is still difficult. Additionally, suitable sparse coding techniques are also important for fusion performance. Designing appropriate neural networks and adjusting the corresponding parameters remain challenging. Furthermore, applying state-of-the-art deep learning techniques to infrared and visible image fusion is yet to be addressed. Finding a subspace with powerful expressive ability in subspace-based methods is difficult. Meanwhile, designing niche targeting and anti-noise saliency detection methods in accordance with the human visual system is also challenging, and using the saliency of source images is still an open problem. In addition, combining the advantages of different methods to create enhanced hybrid image fusion models is a prominent issue in infrared and visible image fusion. Designing new and good infrared and visible image fusion methods with novel ideas is also difficult.</p><p>Infrared and visible image fusion is useful, but its current methods still struggle with several problems. Thus, this survey aims to specifically and comprehensively review the existing fusion algorithms for infrared and visible images and hopes to shed light on the improvement of such approaches. This work has certain advantages in comparison with previous reviews. The current study finds that infrared and visible image fusion is considerably superior to other combinations of source images. However, current reviews are mainly about general fusion approaches and pay only a small amount of attention to methods for infrared and visible image fusion, thus lacking pertinence. This survey is comprehensive and specific, tations reviewing and concluding nearly all main approaches for infrared and visible image fusion. Furthermore, this work includes theories and applications in infrared and visible image fusion and discusses corresponding tendency in future study. Finally, extensive experiments are conducted in this survey to provide an objective comparison of the typical approaches and support relative engineering with credible and solid evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Others</head><p>The structure of this survey is schematically illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Section 2 discusses image fusion and registration methods in different categories. Section 3 introduces the main applications of infrared and visible image fusion. Section 4 provides an overview of the metrics used to measure fusion quality. In Section 5, we extensively experiment to compare the typical algorithms in each category and provide an objective performance reference.</p><p>Finally, Section 6 presents the conclusion and discussion on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Infrared and visible image fusion methods</head><p>In this section, we comprehensively review infrared and visible image fusion methods.</p><p>These methods are divided into seven categories according to their corresponding theories,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>and we review each of them in detail. Successful image fusion has a prerequisite; i.e., different source images should be strictly aligned in advance. Therefore, we also provide a brief review on registration techniques, especially in the context of infrared and visible image registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-scale transform-based methods</head><p>Over the past few decades, multi-scale transforms have been effective in such fields as infrared and visible image fusion <ref type="bibr" target="#b29">[27,</ref><ref type="bibr" target="#b30">28]</ref>. Multi-scale transforms can decompose original images into components of different scales, where each component represents the sub-image at each scale and real-world objects typically comprise components at different scales <ref type="bibr" target="#b3">[1]</ref>.</p><p>Several studies have demonstrated that multi-scale transforms are consistent with human visual characteristics, and this property can enable fused images to have good visual effect <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b32">30]</ref>.</p><p>In general, infrared and visible image fusion schemes based on multi-scale transforms comprise three steps <ref type="bibr" target="#b3">[1]</ref>, as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. First, each source image is decomposed into a series of multi-scale representations. Then, the multi-scale representations of the source image are fused according to a given fusion rule. Finally, the fused image is acquired using corresponding inverse multi-scale transforms on the fused representations. The key in multiscale transform-based fusion schemes lies in the selection of the transforms and fusion rules.</p><p>Next, we review the techniques in this category on the basis of these two aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Multi-scale transform (1) Pyramid transform</head><p>The concept of pyramid transform was proposed in the 1980s and aims to decompose original images into sub-images with different scales of spatial frequency band, which have a pyramid data structure <ref type="bibr" target="#b33">[31]</ref>. Since then, various types of pyramid transforms have been proposed for infrared and visible image fusion, et al., Laplacian <ref type="bibr" target="#b34">[32,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b36">34]</ref>, steerable <ref type="bibr" target="#b37">[35,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b39">37]</ref>, and contrast <ref type="bibr" target="#b40">[38,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41]</ref> pyramids.</p><p>The Laplacian pyramid transform requires iteratively performing the following steps: low-pass filtering, sampling, interpolating, and differencing <ref type="bibr" target="#b33">[31,</ref><ref type="bibr" target="#b34">32]</ref>. For example, Bulanon  et al. adopted the Laplacian pyramid transform and fuzzy logic for fusing infrared and visible images of fruits <ref type="bibr" target="#b34">[32]</ref>, and their method could achieve better fruit detection performance than using only the infrared image. The steerable pyramid transform decomposes the source image into a collection of sub-images in different scales and orientations. This transform has the advantages of self-inverting, aliasing-free, translation and rotation-invariant <ref type="bibr" target="#b37">[35,</ref><ref type="bibr" target="#b44">42]</ref> properties. For example, Liu et al. proposed an image fusion method that was based on the steerable pyramid and expectation maximization algorithm, and their proposed method could outperform the traditional steerable pyramid fusion method <ref type="bibr" target="#b38">[36]</ref>. The contrast pyra- (2) Wavelet transform</p><p>The concept of wavelet transform was proposed by Grossman and Morlet in the 1980s.</p><p>Then, Mallet established multi-resolution theory for wavelet transform; this theory was inspired by the tower algorithm for signal decomposition and reconstruction <ref type="bibr" target="#b45">[43]</ref>. The wavelet transform has since been widely applied in image fusion and related fields. Unlike the multiscale representation coefficients of the pyramid transform, those of the wavelet transform are uncorrelated. Typical wavelet transform-based infrared and visible image fusion methods include discrete wavelet transform <ref type="bibr" target="#b46">[44,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b50">48]</ref>, dual-tree discrete wavelet transform <ref type="bibr" target="#b51">[49,</ref><ref type="bibr" target="#b52">50]</ref>, lifting wavelet transform <ref type="bibr" target="#b53">[51]</ref>, lifting stationary wavelet transform <ref type="bibr" target="#b54">[52]</ref>, redundant-lifting non-separable wavelet multi-directional analysis <ref type="bibr" target="#b55">[53]</ref>, spectral graph wavelet transform <ref type="bibr" target="#b56">[54]</ref>, quaternion wavelet transform <ref type="bibr" target="#b57">[55]</ref>, motion-compensated wavelet transform <ref type="bibr" target="#b58">[56]</ref> and multiwavelet <ref type="bibr" target="#b59">[57]</ref>.</p><p>The discrete wavelet transform can decompose the source image into a series of highand low-frequency sub-images, which can be implemented by a bank of filters. However, the discrete wavelet transform has the problems of oscillation, shift variance, aliasing, and lack of directionality <ref type="bibr" target="#b60">[58]</ref>. The dual-tree discrete wavelet transform is based on the separable filter bank, which has the advantages of computational efficiency, near shift-invariance, and directional selectivity over the discrete wavelet transform <ref type="bibr" target="#b51">[49,</ref><ref type="bibr" target="#b60">58]</ref>. The lifting wavelet transform has also been successfully applied to infrared and visible image fusion <ref type="bibr" target="#b53">[51]</ref> and can be seen as a completely spatial domain transform method; it has the advantages of adaptive design, irregular sampling, and integral transform over the traditional wavelet transform. and Martin Vetterli proposed an efficient multi-direction multi-resolution image representation method named contourlet transform <ref type="bibr" target="#b61">[59]</ref>. This model is based on the Laplacian pyramid <ref type="bibr" target="#b34">[32]</ref> and directional filter bank <ref type="bibr" target="#b62">[60]</ref>, which can capture the geometry of image edges well <ref type="bibr" target="#b63">[61,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b65">63]</ref>. However, the contourlet transform suffers from the problems of shift variance due to downsampling and upsampling and redundancy caused by the pyramidal filter bank structure.</p><p>The nonsubsampled contourlet transform (NSCT) <ref type="bibr" target="#b66">[64]</ref> was proposed as a flexible and fully shift-invariant model with fast implementation to overcome the abovementioned problems of the contourlet transform. The NSCT is based on the nonsubsampled pyramid and nonsubsampled directional filter banks. The nonsubsampled pyramid filter bank decomposes each source image into a set of high-and low-frequency sub-images to attain multiresolution decomposition, whereas the nonsubsampled directional filter bank decomposes these high-frequency sub-images to attain multi-direction decomposition <ref type="bibr" target="#b66">[64]</ref>. The NSCT is shift-invariant because it eliminates the downsampling and upsampling operations of the contourlet transform, thereby ensuring good frequency selectivity and regularity. Hence, the nonsubsampled directional filter bank can reduce the pseudo-Gibbs phenomenon and the effect of misregistration <ref type="bibr" target="#b66">[64,</ref><ref type="bibr" target="#b67">65]</ref>. The NSCT has been widely applied in infrared and visible image fusion <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b69">67,</ref><ref type="bibr" target="#b70">68,</ref><ref type="bibr" target="#b71">69,</ref><ref type="bibr" target="#b72">70,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b74">72,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b76">74,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b78">76,</ref><ref type="bibr" target="#b79">77]</ref>  fusion method that was based on the NSCT and gradient characteristics, and their method could not only extract the target from the infrared image but also preserve details in the visible image <ref type="bibr" target="#b69">[67]</ref>.</p><p>(4) Edge-preserving filter</p><p>The edge-preserving filter emerged as an effective tool for computational photography and other image processing applications in the 2000s. This filter decomposes the source image into a smooth base layer and one or more detail layers <ref type="bibr" target="#b80">[78]</ref>. The edge-preserving filter can preserve the spatial consistency of structures and reduce halo artifacts around edges. The base layer is typically obtained by using an edge-preserving filter on the image, thereby potentially capturing big changes in intensity. The detail layers comprise a series of difference images that can preserve details at various progressively fine scales <ref type="bibr" target="#b80">[78]</ref>. The edge-preserving filter has been widely used in various fields due to these advantages. In recent years, this filter has also been applied in infrared and visible image fusion, such as mean filter <ref type="bibr" target="#b81">[79,</ref><ref type="bibr" target="#b82">80]</ref>, nonlocal means <ref type="bibr" target="#b83">[81]</ref>, anisotropic diffusion <ref type="bibr" target="#b84">[82]</ref>, bilateral filter <ref type="bibr" target="#b85">[83,</ref><ref type="bibr" target="#b86">84]</ref>, weighted least squares filter <ref type="bibr" target="#b87">[85,</ref><ref type="bibr" target="#b88">86]</ref>, multi-scale edge-preserving filter <ref type="bibr" target="#b89">[87]</ref>, local edgepreserving filter <ref type="bibr" target="#b18">[16]</ref>, L 1 fidelity with L 0 gradient <ref type="bibr" target="#b90">[88]</ref>, guided filter <ref type="bibr" target="#b91">[89,</ref><ref type="bibr" target="#b92">90]</ref>, fourth order partial differential equation <ref type="bibr" target="#b16">[14]</ref> and hybrid filter <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b93">91]</ref>.</p><p>The mean filter is a classic and simple denoising method that aims to remove noise from each pixel using the mean value of its spatial neighboring pixels. The nonlocal means and the mean filter are similar, except that the former adopts the neighboring pixels according to the similarity of the intensity gray level. Anisotropic diffusion is based on partial differential equations, which can not only smooth an image in homogeneous regions but also preserve edges, shapes, and positions <ref type="bibr" target="#b94">[92]</ref>. The bilateral filter emerged as a fast alternative to the time-consuming anisotropic diffusion model and combines low-pass filtering with an edge-stopping function <ref type="bibr" target="#b95">[93]</ref>. Mathematically, the bilateral filter removes noise from each pixel using the Gaussian weighted average of the neighboring pixels according to spatial and spectral distances. Hence, this filter is local and nonlinear and can thus simultaneously</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>smooth images and preserve details. The weighted least squares filter aims to balance the smoothing and approximation of original images, which can simultaneously reduce ringing and deblur the images <ref type="bibr" target="#b80">[78,</ref><ref type="bibr" target="#b87">85]</ref>. The multi-scale edge-preserving filter is based on the Laplacian pyramid and weighted least squares filter and can take advantage of multi-scale decomposition and edge-preserving filters <ref type="bibr" target="#b89">[87]</ref>. that was based on the Gaussian and rolling guidance filters, and their approach reduced halos and preserved information as specific scales <ref type="bibr" target="#b21">[19]</ref>.</p><p>(5) Other multi-scale transform methods</p><p>Other types of multi-scale transform methods have also been applied in infrared and visible image fusion. These models include curvelet transform <ref type="bibr" target="#b99">[97,</ref><ref type="bibr" target="#b100">98,</ref><ref type="bibr" target="#b101">99,</ref><ref type="bibr" target="#b102">100,</ref><ref type="bibr" target="#b103">101]</ref>, framelet transform <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b104">102,</ref><ref type="bibr" target="#b105">103]</ref>, shearlet transform <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b106">104,</ref><ref type="bibr" target="#b107">105,</ref><ref type="bibr" target="#b108">106,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b110">108,</ref><ref type="bibr" target="#b111">109]</ref>, tetrolet transform <ref type="bibr" target="#b112">[110,</ref><ref type="bibr" target="#b113">111]</ref>, top-hat transform <ref type="bibr" target="#b114">[112,</ref><ref type="bibr" target="#b115">113,</ref><ref type="bibr" target="#b116">114,</ref><ref type="bibr" target="#b117">115]</ref>, discrete cosine transform <ref type="bibr" target="#b118">[116]</ref>,</p><p>directionlets transform <ref type="bibr" target="#b119">[117]</ref>, empirical mode decomposition <ref type="bibr" target="#b120">[118,</ref><ref type="bibr" target="#b121">119,</ref><ref type="bibr" target="#b122">120,</ref><ref type="bibr" target="#b123">121]</ref>, Internal</p><p>Generative Mechanism <ref type="bibr" target="#b124">[122]</ref> and multi-resolution singular value decomposition (SVD) <ref type="bibr" target="#b125">[123]</ref>.</p><p>The concept of the curvelet transform was proposed by Candes and Donoho <ref type="bibr" target="#b126">[124]</ref> and aims to overcome the inherent limitations of the wavelet transform, which is ill-suited to representing the edges and geometric structures of images. The curvelet transform is a multi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T resolution, multi-direction pyramid that can preserve geometric regularity along edges <ref type="bibr" target="#b127">[125]</ref>.</p><p>However, image fusion methods based on the wavelet and curvelet transforms have block artifacts and reduced contrast. The framelet transform can overcome these problems using tight frame filter banks, which are symmetry-and shift-invariant <ref type="bibr" target="#b17">[15]</ref>. The theory of shearlet transform is based on composite wavelets, which are the optimal approximation of 2D functions <ref type="bibr" target="#b128">[126]</ref>. The support of the shearlet filter is smaller than that of the directional filter of the contourlet transform, which has the advantage of directional selectivity and computational efficiency <ref type="bibr" target="#b109">[107]</ref>. The tetrolet transform is an adaptive Haar wavelet transform, which has tetromino support and geometric adaptability <ref type="bibr" target="#b112">[110]</ref>. This transform can capture the geometric features of images well. Hence, the tetrolet transform has been applied in infrared and visible image fusion <ref type="bibr" target="#b112">[110,</ref><ref type="bibr" target="#b113">111]</ref>. The top-hat transform is a multi-scale transform that is based on mathematical morphology. This transform extracts regions of interest while preserving image details <ref type="bibr" target="#b117">[115]</ref>. The directionlet transform was proposed by Velisavljevic et al. as a multi-scale, multi-directional anisotropic transform <ref type="bibr" target="#b129">[127]</ref> and can preserve the geometric features of source images. This transform has also been successfully applied in infrared and visible image fusion <ref type="bibr" target="#b119">[117]</ref>. Empirical mode decomposition was proposed by Huang et al. <ref type="bibr" target="#b130">[128]</ref> and emerged as a non-stationary adaptive signal decomposition method. This model is now applied in image fusion and many other applications. Empirical mode decomposition has the advantages of extracting low-frequency oscillations and fully exploiting the original images <ref type="bibr" target="#b130">[128]</ref>.</p><p>Multi-scale transform methods have also been successfully applied in infrared and visible image fusion due to the abovementioned advantages.  <ref type="bibr" target="#b116">[114]</ref>. Meanwhile, the weighted average scheme combines the multi-scale representation coefficients of the source images by weighting. Obtaining the weight of the weighted average scheme can be done in a number of ways, the simplest of which is the averaging rule <ref type="bibr" target="#b70">[68,</ref><ref type="bibr" target="#b88">86,</ref><ref type="bibr" target="#b101">99,</ref><ref type="bibr" target="#b115">113]</ref>. Recently, saliency analysis has emerged as a powerful tool for obtaining weight maps <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b81">79,</ref><ref type="bibr" target="#b82">80,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b89">87,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b131">129]</ref>. Certain weighted average fusion rules are based on this method <ref type="bibr" target="#b82">[80,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b90">88]</ref>. Furthermore, saliency analysis is typically combined with a filter to obtain weight maps <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b81">79,</ref><ref type="bibr" target="#b89">87,</ref><ref type="bibr" target="#b131">129]</ref>. Fuzzy logic is usually integrated into the weighted average fusion scheme for infrared and visible image fusion <ref type="bibr" target="#b34">[32,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b68">66]</ref>.</p><p>Approaches to obtaining the weight of the weighted average scheme apart from the above are available. For example, The most representative method is based on the salient region <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b81">79,</ref><ref type="bibr" target="#b82">80,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b89">87,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b131">129]</ref>. The saliency method aims to identify regions that are more salient than their neighbors. This model has been used to extract visually salient regions of images, which can be used to obtain saliency maps of multi-scale sub-images. Hence, the saliency method can be used for infrared and visible image fusion. Zhang et al. adopted the super-pixelbased saliency method to extract salient target regions; then, the fused coefficient could be obtained by the extracted target region using a morphological method <ref type="bibr" target="#b109">[107]</ref>.</p><p>Other region-based rules, such as feature region extraction <ref type="bibr" target="#b114">[112,</ref><ref type="bibr" target="#b116">114,</ref><ref type="bibr" target="#b117">115]</ref>, regions of interest <ref type="bibr" target="#b70">[68,</ref><ref type="bibr" target="#b115">113]</ref>, region segmentation <ref type="bibr" target="#b52">[50,</ref><ref type="bibr" target="#b121">119]</ref>, regional uniformity <ref type="bibr" target="#b69">[67,</ref><ref type="bibr" target="#b71">69]</ref>, regional energy <ref type="bibr" target="#b48">[46,</ref><ref type="bibr" target="#b107">105]</ref>, local region gradient <ref type="bibr" target="#b112">[110,</ref><ref type="bibr" target="#b113">111]</ref>, region-based activity measurement <ref type="bibr" target="#b53">[51]</ref>, and multi-judgment fusion rule <ref type="bibr" target="#b72">[70]</ref> have also been proposed for infrared and visible image fusion. Bright and dim region methods extract feature and detail information first. Then, the extracted feature regions can be integrated into the fusion process <ref type="bibr" target="#b114">[112,</ref><ref type="bibr" target="#b116">114,</ref><ref type="bibr" target="#b117">115]</ref>. Regional energy is used to obtain the fused low-frequency coefficients of infrared and visible images <ref type="bibr" target="#b48">[46,</ref><ref type="bibr" target="#b107">105]</ref>. Chen et al. proposed a multi-judgment fusion rule by comparing regional energy, clarity, and variance contrast <ref type="bibr" target="#b72">[70]</ref>.</p><p>(3) Other fusion rules</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>Other types of infrared and visible image fusion rules, such as activity-level measurement <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b74">72]</ref>, coefficient grouping method <ref type="bibr" target="#b133">[131]</ref>, optimization-based rule <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b49">47]</ref>, Otsu method and top-bottom-hat morphological transform <ref type="bibr" target="#b43">[41]</ref>, mean gradient and PCA <ref type="bibr" target="#b63">[61]</ref>, NMF <ref type="bibr" target="#b17">[15]</ref>, and spiking cortical model <ref type="bibr" target="#b108">[106]</ref>, have been proposed.</p><p>The activity level of the multi-scale representation coefficient measures the local energy of corresponding multi-scale sub-images. The activity-level measurement can be roughly categorized into three classes, i.e., coefficient-, window-, and region-based measures <ref type="bibr" target="#b12">[10]</ref>.</p><p>Liu et al. adopted the max absolute values of coefficients as activity-level measurement to obtain fused high-pass coefficients <ref type="bibr" target="#b20">[18]</ref>. Zou et al. adopted a region-based measure to obtain low-frequency coefficients <ref type="bibr" target="#b53">[51]</ref>. The coefficient grouping method associates multiscale representation coefficients with each other and can be classified into three, i.e., no-, single-scale, and multi-scale grouping schemes <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b12">10]</ref>. Xue et al. proposed an infrared and visible image fusion method that was based on the multi-scale grouping scheme <ref type="bibr" target="#b133">[131]</ref> and could detect weapons concealed in clothing.</p><p>Optimization schemes are adopted to optimize fusion coefficients for infrared and visible image fusion <ref type="bibr" target="#b40">[38,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b21">19]</ref>. The basic concept is to formulate a rule for obtaining the fused coefficients as an optimization problem. Jin et al. proposed three infrared and visible image fusion methods that were based on clonal selection <ref type="bibr" target="#b40">[38]</ref> and teaching-learningbased optimization <ref type="bibr" target="#b41">[39]</ref> and a multi-objective evolutionary algorithm <ref type="bibr" target="#b42">[40]</ref>. Moreover, Li et al. adopted the mean gradient and PCA to fuse high-and low-frequency sub-images, respectively <ref type="bibr" target="#b63">[61]</ref>. Kong et al. used fast NMF to fuse high-and low-frequency sub-images; their model could not only preserve image geometry but also achieve computational efficiency higher than that of traditional multi-scale geometry methods <ref type="bibr" target="#b17">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sparse representation-based methods</head><p>Sparse representation is an effective tool for characterizing the human visual system <ref type="bibr" target="#b134">[132]</ref> and has been successfully applied in different fields, such as image analysis, computer vision, pattern recognition, and machine learning <ref type="bibr" target="#b135">[133,</ref><ref type="bibr" target="#b136">134,</ref><ref type="bibr" target="#b137">135]</ref>. Unlike infrared and visible image fusion methods based on multi-scale transform with prefixed basis functions, sparse  representation image fusion methods aim to learn an over-complete dictionary from a large number of high-quality natural images. Then, the source images can be sparsely represented by the learned dictionary, thereby potentially enhancing the representation of meaningful and stable images <ref type="bibr" target="#b28">[26]</ref>. Besides, misregistration or noise can bring bias to fused multi-scale representation coefficients, thus leading to visual artifacts in the fused image. Meanwhile, sparse representation-based fusion methods divide source images into several overlapping patches using a sliding window strategy, thereby potentially reducing visual artifacts and improving robustness to misregistration <ref type="bibr" target="#b20">[18]</ref>. In general, sparse representation-based infrared and visible image fusion schemes comprise four steps <ref type="bibr" target="#b28">[26]</ref>, as shown in Fig. <ref type="figure" target="#fig_4">3</ref>. First, each source image is decomposed into several overlapping patches using a sliding window strategy. Second, an over-complete dictionary is learned from many high-quality natural images, and sparse coding is performed on each patch to obtain the sparse representation coefficient using the learned over-complete dictionary. Third, the sparse representation coefficients are fused according to a given fusion rule. Finally, the fused image is reconstructed using the fused coefficients using the learned over-complete dictionary. The key in sparse representation-based fusion schemes lies in the over-complete dictionary construction, sparse coding, and fusion rule. In the following sections, we review related techniques on the basis of these three aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Over-complete dictionary construction</head><p>Constructing a good over-complete dictionary is important in infrared and visible image fusion and can be roughly categorized into two classes, i.e., fixed-basis and learning-based methods <ref type="bibr" target="#b28">[26]</ref>. Methods in the first category adopt fixed bases <ref type="bibr" target="#b138">[136,</ref><ref type="bibr" target="#b139">137]</ref> are simple and computationally efficient. Yang et al. constructed two over-complete dictionaries using a fixed basis, i.e., an over-complete discrete cosine transform dictionary and a hybrid dictionary consisting of four different types of bases <ref type="bibr" target="#b138">[136]</ref>. Bin et al. adopted a fixed over-complete discrete cosine transform dictionary to represent infrared and visible images <ref type="bibr" target="#b139">[137]</ref>.</p><p>The second category uses learning methods to obtain over-complete dictionaries. These methods are often more flexible and effective than fixed-basis methods <ref type="bibr" target="#b28">[26]</ref>. According to the manner in which they adopt the training set, learning-based dictionary construction methods can be roughly categorized into two classes, i.e., globally and adaptively trained dictionary-based methods <ref type="bibr" target="#b28">[26]</ref>. Methods in the former class use public training sets to construct over-complete dictionaries <ref type="bibr" target="#b140">[138,</ref><ref type="bibr" target="#b141">139,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b143">141,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b145">143,</ref><ref type="bibr" target="#b146">144]</ref>. These public training images typically have high resolution and quality. Chang et al. used a training set that comprised 100, 000 8 × 8 patches from 40 high-quality images to construct an over-complete dictionary <ref type="bibr" target="#b145">[143]</ref>. Meanwhile, adaptively trained dictionary-based methods adopt source images to construct over-complete dictionaries <ref type="bibr" target="#b147">[145,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b153">151,</ref><ref type="bibr" target="#b154">152,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154]</ref>. For instance, Liu et al. used overlapping patches of source infrared and visible images for training to construct the over-complete dictionary <ref type="bibr" target="#b155">[153]</ref>.</p><p>Learning-based dictionary construction methods, such as K-means generalized SVD (K-SVD) <ref type="bibr" target="#b141">[139,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b145">143,</ref><ref type="bibr" target="#b146">144,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b153">151,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154]</ref>, adaptive sparse representation <ref type="bibr" target="#b140">[138]</ref>, optimal directions <ref type="bibr" target="#b147">[145]</ref>, online dictionary learning <ref type="bibr" target="#b149">[147]</ref>, multi-scale dictionary learning <ref type="bibr" target="#b150">[148]</ref>, and clustering and PCA <ref type="bibr" target="#b152">[150,</ref><ref type="bibr" target="#b154">152]</ref>, have been successfully applied in infrared and visible image fusion. The most representative of these dictionary learning methods is K-SVD <ref type="bibr" target="#b157">[155]</ref>, which alternates between sparse coding and dictionary updating. Only one column of the dictionary is updated at each iteration, and this update is completed by the SVD. Liu et al. proposed an adaptive sparse representation model to learn a series of additionally compact sub-dictionaries from high-resolution and high-quality image patches;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>their method could reduce visual artifacts and computational cost <ref type="bibr" target="#b140">[138]</ref>. Zhang et al. proposed a new dictionary learning algorithm with the Landweber technique and adopted the method of optimal directions for initialization; the computational cost of their model was lower than that of K-SVD <ref type="bibr" target="#b147">[145]</ref>. Yao et al. adopted the online dictionary learning algorithm to train their dictionary <ref type="bibr" target="#b149">[147]</ref>, and their method was considerably faster than batch alternatives on millions of training datasets. Yin et al. proposed a multi-scale dictionary learning method by integrating the wavelet into dictionary learning, thus potentially taking advantage of multi-scale representation and dictionary learning <ref type="bibr" target="#b150">[148]</ref>. <ref type="bibr">Kim et al. proposed</ref> a new dictionary learning method that was based on image patch clustering and PCA <ref type="bibr" target="#b152">[150]</ref>; the proposed model could not only remove the redundancy of the learned dictionary but also maintain the infrared and visible image fusion performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Sparse coding for obtaining coefficients</head><p>After constructing an over-complete dictionary, the representation coefficients can be obtained using sparse coding techniques, such as orthogonal matching pursuit <ref type="bibr" target="#b140">[138,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b145">143,</ref><ref type="bibr" target="#b146">144,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b154">152]</ref>, simultaneous orthogonal matching pursuit <ref type="bibr" target="#b138">[136,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b153">151]</ref>, joint sparse representation model <ref type="bibr" target="#b141">[139,</ref><ref type="bibr" target="#b147">145,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154]</ref>, approximate sparse representation with multi-selection strategy <ref type="bibr" target="#b139">[137]</ref>, and convolutional sparse representation <ref type="bibr" target="#b143">[141]</ref>.</p><p>Orthogonal matching pursuit is a greedy pursuit algorithm created to identify the nonzero sparse coding coefficient <ref type="bibr" target="#b158">[156]</ref>. This model has the advantages of computational efficiency and easy implementation and hence has been widely used for sparse coding. For example, Lu et al. adopted orthogonal matching pursuit to obtain the sparse coding of a training sample <ref type="bibr" target="#b148">[146]</ref>. Simultaneous orthogonal matching pursuit is a generalization of orthogonal matching pursuit; it considers the coherent geometric structure of input signals <ref type="bibr" target="#b159">[157]</ref>. For instance, Yang et al. adopted simultaneous orthogonal matching pursuit to obtain the sparse coefficient of source image patches <ref type="bibr" target="#b138">[136]</ref>. Bin et al. proposed an approximate sparse representation with a multi-selection strategy to obtain the possible sparsest coefficients <ref type="bibr" target="#b139">[137]</ref>.</p><p>Liu et al. adopted convolutional sparse representation to obtain the sparse coding coefficient <ref type="bibr" target="#b143">[141]</ref>, this method was a translation and shift-invariant sparse representation that was</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T robust to misregistration.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Fusion rule</head><p>In general, most of the fusion rules of sparse representation-based infrared and visible image fusion methods are coefficient combining models or their variants, such as choosemax <ref type="bibr" target="#b138">[136,</ref><ref type="bibr" target="#b139">137,</ref><ref type="bibr" target="#b140">138,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b143">141,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b145">143,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b153">151,</ref><ref type="bibr" target="#b154">152]</ref> and weighted average <ref type="bibr" target="#b139">[137,</ref><ref type="bibr" target="#b141">139,</ref><ref type="bibr" target="#b143">141,</ref><ref type="bibr" target="#b146">144,</ref><ref type="bibr" target="#b147">145,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154]</ref>. For example, Wang et al. adopted the Max-L 1 fusion rule to obtain the fused sparse coding coefficient <ref type="bibr" target="#b153">[151]</ref>. Alternatively, Liu et al.</p><p>adopted the weighted average fusion rule to achieve the same goal <ref type="bibr" target="#b155">[153]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural network-based methods</head><p>A neural network usually consists of many neurons and can thus imitate the perception behavior mechanism of the human brain to deal with neuron information. The interactions among neurons characterize the transmission and processing of neuron information, and the neural network has the advantages of strong adaptability and fault tolerance and anti-noise capabilities <ref type="bibr" target="#b160">[158]</ref>. These merits have caused neural networks to be successfully applied in various fields, such as infrared and visible image fusion <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b65">63,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b78">76,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b99">97,</ref><ref type="bibr" target="#b106">104,</ref><ref type="bibr" target="#b124">122,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b161">159,</ref><ref type="bibr" target="#b162">160,</ref><ref type="bibr" target="#b163">161]</ref>.</p><p>Most neural network-based infrared and visible image fusion methods adopt the pulsecoupled neural network (PCNN) or its variants. PCNN was proposed by Eckhorn et al. in the late 1980s <ref type="bibr" target="#b164">[162]</ref>; they introduced a neural approach called Eckhorn's model to simulate the synchronous pulse and coupling of neurons in the visual cortices of cats. These discoveries motivated the development and generation of PCNN; i.e., Johnson modified Eckhorn's model and called the modified approach the PCNN <ref type="bibr" target="#b165">[163]</ref>. In 1996, Broussard and Rogers applied PCNN to image fusion on the basis of physiologically inspired theoretical models for the first time, thus demonstrating the feasibility and advantages of PCNN <ref type="bibr" target="#b166">[164]</ref>. Since then, many infrared and visible image fusion methods that are based on PCNN and its variants have been proposed. decomposed into low-and high-frequency subbands. Then, various strategies are utilized to obtain the fused low-and high-frequency subbands. Finally, inverse transform methods are applied to generate the final fused results. Different image fusion methods may adopt different multi-scale transform methods, such as steerable pyramid <ref type="bibr" target="#b39">[37]</ref>, lifting stationary wavelet transform <ref type="bibr" target="#b54">[52]</ref>, redundant-lifting non-separable wavelet multi-directional analysis <ref type="bibr" target="#b55">[53]</ref>, contourlet <ref type="bibr" target="#b64">[62,</ref><ref type="bibr" target="#b65">63]</ref>, NSCT <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b78">76,</ref><ref type="bibr" target="#b79">77]</ref>, curvelet <ref type="bibr" target="#b99">[97]</ref>, shearlet <ref type="bibr" target="#b106">[104]</ref>, nonsubsampled shearlet transform <ref type="bibr" target="#b15">[13]</ref>, and internal generative mechanism <ref type="bibr" target="#b124">[122]</ref>.</p><p>The key in PCNN-based infrared and visible image fusion schemes lies in the use of PCNN in fusion strategies, and many researchers have utilized PCNNs in high-frequency subbands <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b65">63,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b99">97,</ref><ref type="bibr" target="#b106">104,</ref><ref type="bibr" target="#b124">122,</ref><ref type="bibr" target="#b161">159]</ref>. For example, Li et al. presented a modified PCNN model for adjusting high-frequency coefficients in the NSCT domain while adopting the regional variance integration rule to fuse the low-frequency subband coefficients <ref type="bibr" target="#b77">[75]</ref>. Xi et al. adopted the PCNN in the high-frequency subband, which was generated by the fast discrete curvelet transform, to simulate the biological activity of the human visual system <ref type="bibr" target="#b99">[97]</ref>. In addition, the PCNN can also be adopted to fuse the lowand high-frequency subband coefficients <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b59">57,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b106">104,</ref><ref type="bibr" target="#b161">159]</ref>. For example, Chai et al. adopted the lifting stationary wavelet transform to obtain a flexible multi-scale shiftinvariant representation of original images and used the dual-channel PCNN to fuse the high-and low-frequency subband coefficients <ref type="bibr" target="#b54">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Approaches other than the combined multi-scale transform and PCNN-based infrared and visible image fusion scheme can be adopted to apply PCNN in image fusion. For example, Chen et al. adopted the PCNN in the compressed domain <ref type="bibr" target="#b162">[160]</ref>, and Lu et al.</p><p>adopted the PCNN to segment the source image and fuse the target coefficient <ref type="bibr" target="#b148">[146]</ref>. Furthermore, certain fusion methods are based on the modified PCNN model <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b78">76]</ref>. Kong et al. proposed a unit-fast-linking PCNN to fuse high-and low-frequency subband coefficients, thereby exploiting the advantages of unit-and fast-linking PCNNs <ref type="bibr" target="#b78">[76]</ref>; they also proposed a spatial-frequency PCNN to fuse high-and low-frequency subband coefficients, which were decomposed using the nonsubsampled shearlet transform <ref type="bibr" target="#b15">[13]</ref>.</p><p>In recent years, deep learning has demonstrated state-of-the-art performance in various fields, such as computer vision, pattern recognition, and image processing. Deep learning has a strong capability of modeling complicated relationships among data. Moreover, deep learning can automatically extract distinctive features from data without manual intervention <ref type="bibr" target="#b167">[165]</ref>. Thus, deep learning has also been successfully applied to image fusion, such as multi-focus image fusion <ref type="bibr" target="#b168">[166,</ref><ref type="bibr" target="#b169">167]</ref>, multi-modality image fusion <ref type="bibr" target="#b143">[141,</ref><ref type="bibr" target="#b170">168,</ref><ref type="bibr" target="#b171">169]</ref>, remote sensing image fusion <ref type="bibr" target="#b172">[170,</ref><ref type="bibr" target="#b173">171]</ref>, as well as infrared and visible image fusion <ref type="bibr" target="#b174">[172]</ref>. To the best of our knowledge, there is only one deep learning-based fusion method specialized for infrared and visible image fusion. Specifically, Liu et al. proposed a method based on convolutional neural networks, which can deal with activity level measurement and weight assignment in infrared and visible image fusion as a whole to overcome the difficulty of manual design <ref type="bibr" target="#b174">[172]</ref>. Nevertheless, there are several general image fusion methods that can be applied to infrared and visible image fusion. For example, Liu et al. introduced the convolutional sparse representation for image fusion <ref type="bibr" target="#b143">[141]</ref>; they derived the idea from deconvolutional networks <ref type="bibr" target="#b175">[173]</ref>, which aim to build a hierarchy of layers that each comprise an encoder and a decoder. Zhong et al. proposed a joint image fusion and super-resolution method that was based on a convolutional neural network (CNN) trained using 90 selected images from the ImageNet database <ref type="bibr" target="#b170">[168]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Subspace-based methods</head><p>Subspace-based methods aim to project high-dimensional input images into low-dimensional spaces or subspaces. For most natural images, redundant information exists and lowdimensional subspaces can help capture the intrinsic structures of the original images. In addition, processing low-dimensional subspace data consumes less time and memory than high-dimensional input images. Moreover, low-dimensional subspace representation can be used to improve generalization <ref type="bibr" target="#b176">[174]</ref>. Thus, subspace-based methods, including PCA, ICA, and NMF, have been successfully applied in infrared and visible image fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Principal component analysis</head><p>PCA aims to transform possibly correlated variables into uncorrelated variables called principal components, thereby possibly reducing dimensionality and maintaining the information of the original data simultaneously. In addition, PCA can reduce redundant information and highlight similarities and differences <ref type="bibr" target="#b177">[175]</ref>. Thus, PCA has been applied in infrared and visible image fusion <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b178">176,</ref><ref type="bibr" target="#b179">177,</ref><ref type="bibr" target="#b180">178,</ref><ref type="bibr" target="#b181">179]</ref>. For example, Li et al. adopted PCA to fuse low-frequency images decomposed by morphology-hat transform <ref type="bibr" target="#b63">[61]</ref>; energy information exists mainly in low-frequency images, and PCA was adopted to maintain the brightness information of the original images. Bavirisetti et al. used PCA to fuse detail images decomposed by fourth-order partial differential equations <ref type="bibr" target="#b16">[14]</ref>; their method could obtain the optimal weights for transferring the detail information to the final fused image.</p><p>Although PCA has been widely used in different fields, it is sensitive to gross errors, which are ubiquitous in many applications. Robust PCA (RPCA) overcomes the abovementioned problems by decomposing a matrix into low-rank and sparse components <ref type="bibr" target="#b182">[180]</ref>. In image fusion, RPCA can be used to retain useful information and remove sparse noise <ref type="bibr" target="#b183">[181,</ref><ref type="bibr" target="#b184">182,</ref><ref type="bibr" target="#b185">183]</ref>. For example, Fu et al. adopted the RPCA to obtain the sparse components of infrared and visible images, which were used to obtain the fused low-and high-frequency subband coefficients <ref type="bibr" target="#b184">[182]</ref>. Wang et al. used the RPCA to determine the fused low-frequency subband coefficients in their work <ref type="bibr" target="#b185">[183]</ref>.</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T 2.4.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Independent component analysis</head><p>ICA is an extension of PCA and aims to transform possibly correlated variables into uncorrelated and independent variables <ref type="bibr" target="#b176">[174]</ref>; it has also been successfully applied in infrared and visible image fusion <ref type="bibr" target="#b186">[184,</ref><ref type="bibr" target="#b187">185,</ref><ref type="bibr" target="#b188">186,</ref><ref type="bibr" target="#b189">187]</ref>. ICA-based image fusion methods usually train a set of bases using several natural images that have similar contents as the images to be fused. After obtaining a set of pretrained bases, they can be fused with those with similar contents.</p><p>Cvejic et al. proposed a region-based ICA image fusion method <ref type="bibr" target="#b186">[184]</ref> where the source images are segmented into different regions and the ICA coefficients are obtained for each region using pretrained bases; the ICA coefficients can be subsequently weighted with the Piella fusion metric according to the maximizing fused image quality criterion. Mitianoudis et al. adopted ICA and topographical ICA bases for image fusion under pixel-and regionbased fusion schemes <ref type="bibr" target="#b187">[185]</ref>. The topographical ICA bases demonstrated a better directional selectivity than the traditional ICA bases and hence captured image features better. In addition, the authors proposed an image fusion method that was based on self-trained ICA bases; this proposed approach adopted k-means clustering and hierarchical grouping to improve ICA image fusion performance under the region-based fusion scheme <ref type="bibr" target="#b188">[186]</ref>. Omar et al. proposed a new image fusion method by combining ICA and the Chebyshev polynomial approximation <ref type="bibr" target="#b189">[187]</ref>; the authors used ICA to capture the salient feature and the Chebyshev polynomial to remove noise, especially in seriously noise-degraded images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">Non-negative matrix factorization</head><p>NMF aims to decompose the original data matrix into a product of two non-negative matrices. This approach is a part-based object representation model <ref type="bibr" target="#b190">[188]</ref> that is consistent with the human perception mechanism. Given these merits, NMF has been applied in infrared and visible image fusion <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b191">189,</ref><ref type="bibr" target="#b192">190,</ref><ref type="bibr" target="#b193">191]</ref>.</p><p>Mou et al. adopted the NMF to extract the feature bases of source images <ref type="bibr" target="#b193">[191]</ref>, thereby maintaining the details in the visible image, highlighting the features in the infrared image, and removing the noise. However, traditional NMF is time consuming and has low efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>To overcome these problems, Kong et al. adopted the improved NMF, which could overcome the drawback of SVD-based random initialization and hence converge faster than traditional NMF with random initialization <ref type="bibr" target="#b192">[190]</ref>; moreover, they adopted fast NMF to fuse low-and high-frequency sub-images decomposed by the nonsubsampled shearlet transform; fast NMF is less complex and more computationally efficient than traditional NMF <ref type="bibr" target="#b17">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Saliency-based methods</head><p>Saliency attracts human visual attention in a bottom-up manner <ref type="bibr" target="#b194">[192]</ref>, this attention is often captured by objects or pixels that are more significant than their neighbors. According to the mechanism of the human visual system, saliency-based fusion methods can maintain the integrity of the salient object region and improve the visual quality of the fused image.</p><p>Thus, saliency-based methods have been widely used for infrared and visible image fusion.</p><p>In recent years, researchers have adopted saliency-based methods for infrared and visible image fusion in two ways, i.e., weight calculation <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b81">79,</ref><ref type="bibr" target="#b82">80,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b89">87,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b111">109,</ref><ref type="bibr" target="#b131">129]</ref> and salient object extraction <ref type="bibr" target="#b67">[65,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b195">193,</ref><ref type="bibr" target="#b196">194,</ref><ref type="bibr" target="#b197">195]</ref>. In the first case, saliency is adopted for the reconstruction of the fused image. Infrared and visible images are decomposed into a base layer and a detail layer through a multi-scale transform, and saliency extraction models are applied in the base or/and detail layer of the source image to obtain the saliency map.</p><p>Then, the weight map can be obtained by the saliency map to obtain the fused base image or/and the detail image, and the fused image can be subsequently constructed through the fused base image and the detail image. For example, Gan et al. adopted phase congruency to obtain the saliency maps of the base and detail layers of the source images and then applied guided filtering on the saliency maps to obtain the weighting maps <ref type="bibr" target="#b89">[87]</ref>. Ma et al. initially adopted the rolling guidance and Gaussian filters to decompose the infrared and visible images into the base and detail layers and then used the improved visual saliency map and weighted least square optimization to fuse the base and then detail layers, respectively <ref type="bibr" target="#b21">[19]</ref>.</p><p>Saliency can also be used to extract the significant object regions of source images, thereby obtaining good visual effects and preserving image details. Thus, this model is highly suitable for surveillance applications, such as target detection and recognition. For</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula><p>example, certain research works in <ref type="bibr" target="#b67">[65,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b195">193]</ref> adopted saliency models to extract the salient regions of infrared images. Meng et al. adopted the saliency detection method to extract the salient object region, which can be mapped to the region of the final fused image.</p><p>Zhang et al. used super-pixel-based saliency models to obtain the salient regions of infrared images; the models could preserve the target information of the infrared images well <ref type="bibr" target="#b109">[107]</ref>.</p><p>In addition, several research works <ref type="bibr" target="#b155">[153,</ref><ref type="bibr" target="#b196">194,</ref><ref type="bibr" target="#b197">195</ref>] have adopted saliency models to extract the salient objects and information from infrared and visible images. Liu et al. integrated saliency detection into the sparse representation framework for infrared and visible image fusion, which adopted global and local saliency maps to obtain the weight for fused image reconstruction <ref type="bibr" target="#b155">[153]</ref>. Shibata extracted salient information from infrared and visible images, thereby preserving their salient information <ref type="bibr" target="#b197">[195]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Hybrid methods</head><p>The abovementioned infrared and visible image fusion methods all have their advantages and disadvantages, and their advantages should be combined to improve image fusion performance. The most common hybrid infrared and visible image fusion methods integrate other types of methods into the multi-scale transform framework, such as hybrid multi-scale transform and saliency <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b81">79,</ref><ref type="bibr" target="#b82">80,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b89">87,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b103">101,</ref><ref type="bibr" target="#b105">103,</ref><ref type="bibr" target="#b109">107]</ref>, hybrid multi-scale transform and sparse representation <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b198">196,</ref><ref type="bibr" target="#b199">197]</ref>, hybrid multi-scale transform and neural network <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b59">57,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b65">63,</ref><ref type="bibr" target="#b106">104]</ref>, and hybrid multi-scale transform, sparse representation, and neural network methods <ref type="bibr" target="#b76">[74,</ref><ref type="bibr" target="#b110">108,</ref><ref type="bibr" target="#b200">198]</ref>. method, where a novel shift-invariant dual-tree complex shearlet transform is adopted to decompose the source images into multi-scale sub-images, sparse representation is adopted to obtain the fused low-frequency subband coefficients, and the adaptive dual-channel PCNN was adopted to obtain the fused high-frequency subband coefficients <ref type="bibr" target="#b110">[108]</ref>.</p><p>Certain hybrid infrared and visible image fusion methods are classified under the sparse representation framework, such as hybrid sparse representation and saliency methods <ref type="bibr" target="#b141">[139,</ref><ref type="bibr" target="#b155">153]</ref> and hybrid sparse representation and neural network methods <ref type="bibr" target="#b145">[143,</ref><ref type="bibr" target="#b148">146]</ref>. For example, Liu et al. integrated saliency detection into the sparse representation framework <ref type="bibr" target="#b155">[153]</ref>,</p><p>constructed the saliency map on the basis of the sparse representation coefficients, and then obtained the fused sparse representation coefficients through the saliency map. Chang et al. proposed a new image fusion method that was based on a combination of sparse representation and a deep neural network <ref type="bibr" target="#b145">[143]</ref> to combine the external and internal cartoon and texture sparse representation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Other infrared and visible image fusion methods</head><p>Other infrared and visible image fusion methods can inspire new ideas and perspectives for image fusion. Ma et al. proposed a series of infrared and visible image fusion methods that was based on total variation <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b201">199,</ref><ref type="bibr" target="#b202">200]</ref>. The main idea of a previous work <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b201">199]</ref> was to retain the intensity information of infrared images and preserve the appearance information of visible images simultaneously. However, the authors ignored the intensity information of the visible image, which could lead to a low dynamic range and detail loss.</p><p>In <ref type="bibr" target="#b202">[200]</ref>, Guo et al. improved fusion performance by not only keeping the intensity information of both infrared and visible images but also preserving the appearance information of High-quality image fusion is a fuzzy problem <ref type="bibr" target="#b203">[201]</ref>, and fuzzy theory is an effective tool for solving the fuzzy problem through fuzzy logic or fuzzy based methods. Therefore, fuzzy theory has been successfully applied in infrared and visible image fusion <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b204">202,</ref><ref type="bibr" target="#b205">203,</ref><ref type="bibr" target="#b206">204]</ref>. This theory is often adopted to determine the fusion rules for infrared and visible image fusion <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b204">202]</ref>. For example, Yin et al. adopted fuzzy logic to obtain the adaptive weighted average rule <ref type="bibr" target="#b68">[66]</ref>. The fuzzy logic system is also applied to infrared and visible image fusion <ref type="bibr" target="#b205">[203]</ref>, and the key concept of the fuzzy logic system is the membership function that ranges from 0 to 1, which can be adopted to measure the uncertainty of image fusion performance. Bai et al. also proposed a new infrared and visible image fusion rule based on the fuzzy measure weight strategy <ref type="bibr" target="#b206">[204]</ref>.</p><p>Other types of infrared and visible image fusion methods exist, such as entropy <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b207">205]</ref>,</p><p>Markov random field <ref type="bibr" target="#b195">[193,</ref><ref type="bibr" target="#b208">206]</ref>, morphology <ref type="bibr" target="#b209">[207,</ref><ref type="bibr" target="#b210">208]</ref>, and infrared feature extraction and visual information preservation <ref type="bibr" target="#b211">[209]</ref>. Entropy is an objective metric used to measure the amount of information transferred from the source images into the fused image. Zhao et al. adopted the global maximum entropy criterion to transfer as much information from the source images into the fused image <ref type="bibr" target="#b23">[21]</ref>. Markov random field is also an effective tool for infrared and visible image fusion. For example, Han et al. adopted the Markov random field for saliency detection, which could be formulated as a pixel labeling problem <ref type="bibr" target="#b195">[193]</ref>. Shibata et al. used the Markov random field to preserve the visible information and spatial coherence of the source images <ref type="bibr" target="#b208">[206]</ref>. Morphological methods have also been applied to image fusion.</p><p>Specifically, Bai et al. adopted morphological methods to extract multi-scale morphological features for infrared and visible image fusion <ref type="bibr" target="#b209">[207,</ref><ref type="bibr" target="#b210">208]</ref>.</p><p>Most of the above mentioned infrared and visible image fusion methods are performed at pixel-level, which combine each pixel in source images to obtain the fused image directly.</p><p>However, image fusion can also be performed at feature-or decision-level. The feature-level image fusion methods usually extract the features of source images at first, and then obtain the fused image based on the extracted features for some specific purposes, which have the advantages of designing more intelligent semantic fusion rules based on the extracted fea-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>tures compared with pixel-level fusion strategies <ref type="bibr" target="#b212">[210]</ref>. Therefore, feature-level fusion methods have also been adopted for infrared and visible image fusion <ref type="bibr" target="#b74">[72,</ref><ref type="bibr" target="#b193">191,</ref><ref type="bibr" target="#b209">207,</ref><ref type="bibr" target="#b211">209,</ref><ref type="bibr" target="#b213">211,</ref><ref type="bibr" target="#b214">212]</ref>. Decision-level image fusion is the highest level of the above mentioned three levels, which can also be called interpretation-or symbol-level image fusion. In decision-level image fusion, the information is extracted from each source image, and then decision rules are made to fuse the extracted information according to specific criteria <ref type="bibr" target="#b215">[213,</ref><ref type="bibr" target="#b216">214]</ref>, which can help to better understand the objects under observation. The research about decision-level image fusion is the least among the above mentioned three image fusion levels, and the main application of decision-level infrared and visible image fusion is face recognition <ref type="bibr" target="#b217">[215,</ref><ref type="bibr" target="#b218">216,</ref><ref type="bibr" target="#b219">217]</ref>. For example, Neagoe et al. proposed a real-time face recognition system using the decision fusion, which can combine the recognition scores generated from visual channels with a thermal infrared neural classifier <ref type="bibr" target="#b217">[215]</ref>. Zhao et al. proposed a decision-level method for face recognition, in which the principal component analysis and linear discriminant analysis were adopted to extract the face features, and the decision-level fusion was implemented with the infrared and visible image recognition results and their confidence measures <ref type="bibr" target="#b218">[216]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.">Image registration</head><p>The integration of multi-sensor images can provide a complex and detailed scene representation, thereby increasing the accuracy of decision-making in subsequent tasks. However, a successful image fusion requires the images to be fused to be strictly geometrically</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>aligned <ref type="bibr" target="#b220">[218,</ref><ref type="bibr" target="#b221">219]</ref>. This problem is known as image registration <ref type="bibr" target="#b222">[220,</ref><ref type="bibr" target="#b223">221]</ref>, which occurs because the images captured by different sensors, such as infrared and visible sensors, are usually different in size, perspective, and field of view. Such an alignment can be achieved through hardware (e.g., placing a beam-splitter) <ref type="bibr" target="#b224">[222]</ref>; however, a special imaging device for generating aligned image pairs may not be practical due to its high cost and low availability, especially for real-world applications, where large-scale deployments are required <ref type="bibr" target="#b221">[219]</ref>.</p><p>Therefore, using an image registration algorithm based on off-the-shelf low-cost visible and infrared cameras may be appropriate.</p><p>Image registration has been widely used in many fields, including computer vision <ref type="bibr" target="#b225">[223,</ref><ref type="bibr" target="#b226">224]</ref>, medical image analysis <ref type="bibr" target="#b227">[225,</ref><ref type="bibr" target="#b228">226]</ref>, and remote sensing <ref type="bibr" target="#b229">[227,</ref><ref type="bibr" target="#b230">228]</ref>. The images to be aligned may be captured at different times from different perspectives or with different modalities, which typically refer to the problems of temporal, spatial, and multimodal registrations. Specifically, the registration of infrared and visible images refers to the problem of multimodal registration. A broad overview of the registration methods can be found in the literature <ref type="bibr" target="#b222">[220,</ref><ref type="bibr" target="#b226">224,</ref><ref type="bibr" target="#b227">225]</ref>. In general, registration methods can be classified into two categories, i.e., area-based and feature-based methods. Next, we briefly present an overview these two types of techniques, particularly in the context of infrared and visible image registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.1.">Area-based methods</head><p>Area-based methods deal directly with the intensity values of entire original images; for example, minimizing the total distance between the pixel correspondences under a certain metric. These methods are preferable in cases of few prominent details in images where distinctive information is provided by pixel intensities rather than by salient structures, but they suffer from image distortions, illumination changes, and heavy computational complexities. Area-based methods come in three major types, i.e., correlation-like methods <ref type="bibr" target="#b231">[229]</ref>,</p><p>Fourier methods <ref type="bibr" target="#b232">[230]</ref>, and mutual information (MI) methods <ref type="bibr" target="#b233">[231]</ref>.</p><p>Cross-correlation is a classic correlation-like method <ref type="bibr" target="#b231">[229]</ref> that calculates the crosscorrelation between window pairs in two images, and the one with the maximum value</p><formula xml:id="formula_5">A C C E P T E D M A N U S C R I P T</formula><p>is considered a correspondence. To address the multimodal registration problem, rather than using standard image statistics, Roche et al. introduced a similarity measure that was based on probability theory, i.e., the correlation ratio, which had perceptible practical improvements in registration accuracy and robustness <ref type="bibr" target="#b234">[232]</ref>. Correlation-like methods are frequently used in real-time applications because of their easy implementation in hardware.</p><p>However, they have disadvantages when the image contents in windows suffer from complex transformations or contain a relatively smooth area without prominent details.</p><p>Fourier methods exploit the Fourier representation of images in the frequency domain <ref type="bibr" target="#b232">[230]</ref>.</p><p>One presentative of such method is the phase correlation method based on the Fourier shift theorem, which was originally proposed to deal with image translation and later extended to consider rotation and scaling <ref type="bibr" target="#b235">[233]</ref>. The applications in infrared and visible image registration were described in <ref type="bibr" target="#b236">[234]</ref>, where the Fourier-Mellin transform was used to address the multimodal nature together with spatial distortion issues. In comparison with correlationlike methods, Fourier methods have the advantages of computational efficiency and strong robustness against varying imaging conditions and frequency-dependent noise.</p><p>Mutual information methods utilize mutual information as the metric to maximize the statistical dependency between two given images and then establish pixel correspondences accordingly <ref type="bibr" target="#b233">[231]</ref>. These models work well in domains where edge or gradient magnitudebased methods have difficulty and hence are particularly suitable for registering multimodal images. However, for infrared and visible image pairs, the textures are usually significantly different; mutual information may then work well on only a small part rather than on the entire image <ref type="bibr" target="#b237">[235]</ref>. Therefore, some salient regions, such as regions with a similar edge density or a detected foreground, are selected before applying mutual information <ref type="bibr" target="#b238">[236]</ref>. Moreover, mutual information can be operated on extracted features instead of image intensities (e.g., points sampled from shape contours <ref type="bibr" target="#b239">[237]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8.2.">Feature-based methods</head><p>Feature-based methods first extract two sets of salient structures (e.g., feature points) and then determine the correct correspondence between them and estimate the spatial trans- formation accordingly, which is further used to align the given image pair. Compared with area-based methods, feature-based methods are more robust against typical appearance changes and scene movements and are potentially faster if implemented correctly <ref type="bibr" target="#b220">[218]</ref>. In general, feature-based methods comprise two major steps, i.e., feature extraction and feature matching. The former refers to the detection of salient and distinctive objects, whereas the later refers to the establishment of correspondences between the detected features.</p><p>The first step of feature-based methods is to extract robust common features that can represent the original images. For infrared and visible images that manifest two different phenomena, the appearance features with global statistical dependence, including gray levels/colors, textures (e.g., Gabor filters <ref type="bibr" target="#b240">[238]</ref>), and gradient histograms (e.g., SIFT <ref type="bibr" target="#b241">[239]</ref> and HOG <ref type="bibr" target="#b242">[240]</ref>), are unlikely to match. Instead, the features that represent salient structures are preferred, such as corner points <ref type="bibr" target="#b243">[241]</ref>, edge maps <ref type="bibr" target="#b220">[218]</ref>, silhouettes <ref type="bibr" target="#b244">[242]</ref>, trajectories <ref type="bibr" target="#b237">[235]</ref>,</p><p>visual salient features <ref type="bibr" target="#b214">[212]</ref>, and hybrid visual features <ref type="bibr" target="#b245">[243]</ref>. Among these features, edge information is one of the most popular selections for infrared and visible image registration <ref type="bibr" target="#b246">[244,</ref><ref type="bibr" target="#b220">218,</ref><ref type="bibr" target="#b244">242]</ref> as its magnitudes and orientations may be preserved well across different modalities. A typical example of infrared and visible image registration based on edge information is presented in Fig. <ref type="figure" target="#fig_12">5</ref>.</p><p>After obtaining the salient features from two images, we need to establish correct correspondences between them. In general, high-level features, such as edge maps and surfaces,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>can be discretized as point sets. Therefore, the registration problem is lessened to determine the correspondence and estimate the transformation between two sets of point features <ref type="bibr" target="#b220">[218]</ref>.</p><p>A popular strategy for solving the point matching problem involves two steps <ref type="bibr" target="#b247">[245]</ref>: (i) computing a set of putative correspondence, and then (ii) removing the outliers via geometrical constraints. Putative correspondence instances are obtained in the first step by pruning the set of all possible point correspondences. This scenario is achieved by computing the feature descriptors <ref type="bibr" target="#b241">[239,</ref><ref type="bibr" target="#b248">246]</ref> at the points and eliminating the matches between points whose descriptors are excessively dissimilar. However, the use of local feature descriptors alone will inevitably result in numerous false matches. In the second step, robust estimators based on geometrical constraints are used to detect and remove the outliers.</p><p>To remove false matches from putative sets, numerous methods have been proposed over the last decades. One of the most widely used robust estimator is random sample consensus (RANSAC) <ref type="bibr" target="#b249">[247]</ref>, which adopts a hypothesize-and-verify approach and attempts to obtain the smallest possible outlier-free subset to estimate a given parametric model through resampling. The RANSAC algorithm has achieved success in the registration of infrared and visible images <ref type="bibr" target="#b237">[235,</ref><ref type="bibr" target="#b245">243,</ref><ref type="bibr" target="#b214">212]</ref>. However, this model relies on a predefined parametric model, which becomes less efficient when the underlying image transformation is non-rigid; it also tends to degrade severely when the proportion of false matches becomes large. To address these issues, several non-parametric interpolation methods <ref type="bibr" target="#b247">[245,</ref><ref type="bibr" target="#b250">248]</ref> have been introduced recently, especially in the context of infrared and visible image registration <ref type="bibr" target="#b220">[218,</ref><ref type="bibr" target="#b244">242]</ref>. Specifically, to deal with the expression and pose changes in face recognition, Ma et al. proposed a regularized Gaussian fields criterion to align infrared and visible faces in a non-rigid manner <ref type="bibr" target="#b220">[218]</ref>.</p><p>Furthermore, the authors introduced a sparse approximation for non-rigid transformation estimation that could reduce the computational complexity from cubic to linear without sacrificing registration accuracy <ref type="bibr" target="#b251">[249]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Applications</head><p>Fusion images produced by combining infrared and visible source information have many complementary characteristics. For example, these images make the infrared image more</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>vivid through the transfer of certain textures. Therefore, fusion algorithms can be embedded into many applications and improve the original methods. Main applications that use fusion techniques include recognition <ref type="bibr" target="#b120">[118,</ref><ref type="bibr" target="#b252">250,</ref><ref type="bibr" target="#b253">251]</ref>, object detection <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b254">252,</ref><ref type="bibr" target="#b255">253]</ref>, tracking <ref type="bibr" target="#b256">[254,</ref><ref type="bibr" target="#b257">255]</ref>, surveillance <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b258">256]</ref>, color vision <ref type="bibr" target="#b259">[257,</ref><ref type="bibr" target="#b260">258]</ref>, and remote sensing <ref type="bibr" target="#b261">[259,</ref><ref type="bibr" target="#b262">260]</ref>. In this section, the algorithms for applications in these techno-spheres are introduced for a comprehensive understanding of their peculiarities and performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fusion for recognition</head><p>Given an image that contains one or some objects, recognition aims to specify the cat- Other biological recognition applications that are related to infrared and visible image fusion exist. A person identification algorithm based on periocular region <ref type="bibr" target="#b266">[264]</ref> was proposed to solve the sever occlusion problem in surveillance. In this method, an image set consists of infrared and visible images instead of single-band data and feature score fusion is used.</p><formula xml:id="formula_6">egory</formula><p>This algorithm can identify a person with only a periocular region available. An algorithm for iris recognition <ref type="bibr" target="#b267">[265]</ref> was developed with the Haar wavelet method; in this approach, fusion is performed at the matching score level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fusion for detection and tracking</head><p>Detection based on infrared and visible image fusion is widely used in many real-life applications, such as general object detection <ref type="bibr" target="#b268">[266]</ref>, pedestrian detection <ref type="bibr" target="#b269">[267]</ref>, and fruit detection <ref type="bibr" target="#b34">[32]</ref>. Different from recognition, it should determine the accurate position of the objects at the same time. According to the implementation process, detection methods based on fusion algorithms can be mainly divided into two classes: (1) those that fuse before detecting and (2) those that detect before fusing. Many algorithms adopt the former style. Emna et al. proposed a two-stage method that includes multi-scale decomposition, low-level fusion, and background modeling detection <ref type="bibr" target="#b270">[268]</ref>. The main idea of this algorithm is to detect objects through the inter-frame differences in the background model of the fused image. This algorithm is fast because of the simplicity of the fusion process; however, it suffers from degradation under the condition of moving objects, which may cast a shadow and introduce noise from the changes in illumination. He et al. proposed an algorithm with multilevel fusion and enhancement for target detection <ref type="bibr" target="#b215">[213]</ref>. In this method, both pixel-and feature-level fusion are considered and the relationship between low-frequency information and high-frequency component, which is usually ignored in the wavelet transform fusion, is explored to enhance target visibility. This model exhibits superiority in terms of fusion details and detection precision. In addition, weak infrared targets that cannot be found with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>common algorithms can be detected with this method. As for the second class, the people detection algorithm based on the INT3 framework <ref type="bibr" target="#b271">[269]</ref> proposed by Jose et al. is a typical example <ref type="bibr" target="#b272">[270]</ref>. In this framework, the input source images are segmented and then fused at the pixel level, thereby generating possible candidates for the target objects. Following this step, a detection algorithm, such as deformable part-based model, is performed to search the targets. Although detection is behind fusion superficially, the segmentation process for candidates that is implemented before fusion is an important part of detection. Besides detection and recognition, popular vision tasks, such as tracking or event interpretation, can be embedded in the method. Therefore, the framework provides a unified solution for multi-task implementation that is based on the fusion results of infrared and visible images.</p><p>Target tracking algorithms share some similarities with detection, it should determine the relationship between frames and detect the target object in a time sequence. In target tracking, the algorithms should be implemented quickly because tracking is performed with video or images of time sequence in most situations. Therefore, multi-scale decompositionbased methods are usually used in the fusion process. Stephen et al. proposed an enhanced target tracking approach through infrared-visible image fusion by applying a PCA-weighted fusing rule instead of a simple combination, such as average or maximum selection after pyramid decomposition <ref type="bibr" target="#b273">[271]</ref>. A target tracking system based on infrared and visible image fusion and used in unmanned aerial vehicles was designed with wavelet transform and fuzzy inference fusion <ref type="bibr" target="#b274">[272]</ref>. In addition to the multi-scale decomposition-based method, algorithms that use sparse representation <ref type="bibr" target="#b275">[273,</ref><ref type="bibr" target="#b257">255]</ref> to achieve fusion are important in target tracking. The key to this kind of method is to find an appropriate dictionary with a powerful representation capability for the specific task.</p><p>In most algorithms for recognition, detection, and tracking, fusion is a relatively independent part that is designed to improve the visibility or textures of the original images.</p><p>Different from some end-to-end methods, such as CNNs, suitable recognition or detection algorithms should be selected to produce superior fused images in a specific target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Visible image Infrared image Fused image </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion for surveillance</head><p>As shown in Fig. <ref type="figure" target="#fig_14">6</ref>, surveillance need clear, robust, and expressive images or videos. The complementary properties of infrared and visible image make them a good fusion sources for this task. Infrared and visible images provide object information in various respects given their different imagery characteristics. Infrared images capture the thermal radiation of objects that is uninfluenced by the changes in illumination, weather, and other disturbances.</p><p>However, infrared images have low spatial resolution and are therefore hardly distinguish details. By contrast, visible images have high spatial resolution and are rich in color, which make objects in them vivid and easily identified. The fusion of these two images significantly improves visibility and contributes to several real-life applications. Surveillance methods usually deal with videos, and they are required to be implemented fast. Therefore, fusion algorithms designed for surveillance applications are usually developed in a multi-scale fashion. Liu et al. used an infrared and visible image fusion technique to improve the visibility of unmanned aerial vehicles with discrete wavelet transform and region segmentation <ref type="bibr" target="#b47">[45]</ref>.</p><p>Framelet transform was implemented in <ref type="bibr" target="#b276">[274]</ref>, NSCT was applied to the decomposition process for a night-vision surveillance system <ref type="bibr" target="#b277">[275]</ref>, and cascade wavelet transform and Robert operator were used in a surveillance system with multiple sensors using hybrid fusion algorithms <ref type="bibr" target="#b278">[276]</ref>. Fusion models designed from other respects also exist. Ma et al. solved the fusion problem in another perspective and proposed a fusion method <ref type="bibr" target="#b22">[20]</ref> that could transform a low-resolution infrared image to a high-resolution one by employing total vari-</p><formula xml:id="formula_7">A C C E P T E D M A N U S C R I P T</formula><p>ation and gradient transfer. The algorithm could be applied to images without registration and worked efficiently in face image enhancement. An improved version <ref type="bibr" target="#b202">[200]</ref> of this model was subsequently proposed by providing additive information about intensity from visible images. The fused image generated by this algorithm was natural and rich in details. The algorithm could be implemented fast by solving the fusion problem in the framework of alternating direction method of multipliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fusion for color vision</head><p>The human visual system can perceive only visible light. Therefore, information of thermal radiation from target objects or scenes must be converted into images with pseudocolor. No suitable rendering scheme has been used for a long time, and fusion results have been represented by gray-level images. This method degrades surveillance performance because the human visual system can distinguish more than a thousand colors but can only discern approximately 100 levels of gray intensity <ref type="bibr" target="#b279">[277]</ref>. The color transfer <ref type="bibr" target="#b7">[5]</ref> proposed by Reinhard et al. was a breakthrough in this field. In the method, statistics of natural images in the principal component space is transferred to a multi-band image to make the gray image have a natural appearance. The multi-band image is required to show a similar scene to the source image lest it exhibits a strange appearance. The process of this method is presented in Fig. <ref type="figure" target="#fig_16">7</ref>. As for infrared and visible image fusion, the original images used are from the same scene; therefore, such a fusion can generate satisfactory results. Subsequent  results were difficult to obtain in applications without sufficient training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Fusion for other applications</head><p>Infrared and visible image fusion techniques are also widely used in other fields. Typical areas include remote sensing, culture relic analysis, and augmented reality.</p><p>Remote sensing that uses infrared and visible image fusion can be used under numerous conditions. Li et al. proposed a medium-altitude unmanned aerial vehicle remote sensing system using image registration and fusion <ref type="bibr" target="#b262">[260]</ref>. Particle swarm optimization was introduced to improve efficiency, and the fusion method was achieved by a PCNN. An adaptive fusion method for infrared and visible remote sensing images was developed with multi-contourlet transform <ref type="bibr" target="#b283">[281]</ref>. This method was efficient in direction selectivity, and considerable information about details could be captured in the fusion process. A similar method <ref type="bibr" target="#b284">[282]</ref> was proposed to solve the fusion problem in astronomical images. Instead of multi-contourlet undecimated dual-tree complex wavelet transform, which exhibited shiftinvariance, was applied in the fusion process to reduce noise. Lu et al. proposed a synergetic classification method for long-wave infrared hyperspectral and visible images <ref type="bibr" target="#b285">[283]</ref>. In this</p><formula xml:id="formula_8">A C C E P T E D M A N U S C R I P T</formula><p>method, a new feature extraction method named semi-supervised local discriminant analysis was proposed in infrared image classification. Unlike other methods, which fuse at the pixel level, this method fused at the decision level for infrared and visible images. Applications developed on remote sensing with fusion techniques, such as urban object detection <ref type="bibr" target="#b286">[284]</ref> and geostationary meteorological satellite image fusion <ref type="bibr" target="#b287">[285]</ref>, are also presented.</p><p>Infrared and visible image fusion is also used in areas other than remote sensing. Liang et al. proposed a polarimetric dehazing method for visibility improvement that was based on visible and infrared image fusion <ref type="bibr" target="#b288">[286]</ref>. The method used a multi-scale directional nonlocal means filter to achieve high robustness and minimal error points. However, the algorithm implementation consumed considerable time and was hence inappropriate for certain realtime applications. Li et al. proposed a fusion method for deterioration risk analysis of ancient frescoes <ref type="bibr" target="#b275">[273]</ref>. A fusion technique for color and thermal images was developed for augmented reality in rescue robotics <ref type="bibr" target="#b289">[287]</ref>. In the agriculture field, techniques for predicting apple fruit firmness and soluble solid content have been proposed in recent years <ref type="bibr" target="#b290">[288]</ref>.</p><p>Fusion algorithms for infrared and visible videos in the field of medical imaging have helped improve the clinical intraoperative assessment of critical biliary structures <ref type="bibr" target="#b291">[289]</ref>.</p><p>Current infrared and visible image fusion algorithms are mostly developed at the pixel level. Although fused images can provide complementary information about scenes, distortions, such as halos, still occur in many algorithms because of inappropriate fusion rules.</p><p>These undesired changes limit or degrade the performances of applications built on the corresponding fusion method. Methods with good fusion principles and powerful representations that combine pixel-level semantic features will significantly improve current applications and contribute to inventions of advanced systems in different areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Implementations</head><p>Many fusion algorithms involve large computation and require considerable time to be implemented on machines for general computation purposes. However, numerous real-life applications, such as video surveillance, that are based on infrared and visible image fusion require that the algorithm be efficient to achieve a satisfactory performance. Thus, imple- FPGA has many advantages over general purpose computers; it has a low power/dissipation ratio and a massive processing throughput and is therefore energy efficient and can be designed for parallel computing <ref type="bibr" target="#b292">[290]</ref>. The programmable characteristic of a chip also enables the development of a compact and portable fusion system that can be used in such platforms as unmanned aerial vehicles. Only a few attempts in designing fusion systems using FPGA have been documented. Uninhabited airborne vehicles that use the wavelet transform are probably the first systems developed on FPGA <ref type="bibr" target="#b293">[291]</ref>. Currently, a common fusion system developed on FPGA has been designed with three-level pyramid transform and fast and adaptive bidimensional empirical mode decomposition (FABEMD) <ref type="bibr" target="#b292">[290,</ref><ref type="bibr" target="#b294">292]</ref>. This design was pursued because the characteristics of decomposition and fusion in pyramid transform and FABEMD make the model suitable for parallel computing in FPGA. In addition to modules related to decomposition and fusion, a local buffer for image input has been designed to meet the requirement of immediate access to data. A fusion scheme that uses a linear transformation named Cholesky decomposition has also been developed. This method has been implemented in a system developed with ALTERA Cyclone II FPGA in VHSIC hardware description language <ref type="bibr" target="#b295">[293]</ref>. Another typical method for accelerating implementation is through CUDA. Literature about related architecture is limited. A multi-scale method based on discrete wavelet transform was introduced in <ref type="bibr" target="#b296">[294]</ref>. However, similar architectures of other fusion algorithms implemented with graphic processing units can be good references <ref type="bibr" target="#b297">[295,</ref><ref type="bibr" target="#b298">296,</ref><ref type="bibr" target="#b299">297]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance evaluation of image fusion</head><p>Infrared and visible image fusion has been attracting considerable attention in information fusion in the past few decades, and several image fusion methods have been proposed.</p><p>Infrared and visible image fusion techniques have been widely used in different applica-</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T</formula><p>tions, such as object recognition, tracking, and surveillance, due to the progress of image fusion techniques. However, different fusion methods have different characteristics, and the performances of fusion methods in real-life applications greatly rely on the quality of the fused images. Therefore, the fusion performances of different methods should be evaluated qualitatively and quantitatively <ref type="bibr" target="#b300">[298]</ref>.</p><p>Numerous quality evaluation methods have been proposed to evaluate the performances of different infrared and visible image fusion methods, and they can be classified as subjective and objective evaluation methods <ref type="bibr" target="#b301">[299]</ref>. Quality evaluation methods can be used to compare the performances of different fusion methods and adopted as a guide to select fusion methods in real applications. Evaluation methods can also be used to tune the parameters of fusion methods.</p><p>Subjective evaluation methods assess the quality of fused images on the basis of the human visual system and play an important role in fusion quality evaluation. Subjective human inference can consistently compare different fusion methods according to several criterions, such as image details, object completeness, and image distortion; hence, subjective evaluation methods are popular, reliable, and direct in the quality evaluation of infrared and visible image fusion. Nevertheless, the simplest subjective evaluation method is to score fused images by trained observers, which has the drawbacks of human intervention, considerable time consumed, high cost, and irreproducibility <ref type="bibr" target="#b301">[299]</ref>. Objective evaluation methods that can measure the qualities of fused images quantitatively and automatically have been proposed to overcome these problems.</p><p>Unlike subjective evaluation methods, objective evaluation methods can assess fused image quality quantitatively; they are highly consistent with the human visual perception and not easily biased by observers or interpreters. Objective evaluation methods come in different types, which are based on information theory, structure similarity, image gradient, statistics, and human visual system. In this section, we briefly introduce some representative image quality evaluation measures, which are listed in Table <ref type="table">1</ref>. We use A, B, and F to represent the infrared, visible, and fused images, respectively, and X to represent the source image variables that have a size of M × N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The mutual information (MI) metric is a quality index that measures the amount of information that is transferred from source images to the fused image <ref type="bibr" target="#b316">[314]</ref>. MI is a fundamental concept in information theory and measures the dependence of two random variables.</p><p>The MI fusion metric is defined as follows:</p><formula xml:id="formula_10">M I = M I A,F + M I B,F ,<label>(2)</label></formula><p>where M I A,F and M I B,F denote the amount of information that is transferred from infrared and visible images to the fused image, respectively. The MI between two random variables can be calculated by the Kullback-Leibler measure, which is defined as follows:</p><formula xml:id="formula_11">M I X,F = x,f p X,F (x, f ) log p X,F (x, f ) p X (x)p F (f ) ,<label>(3)</label></formula><p>where p X (x) and p F (f ) denote the marginal histograms of source image X and fused image F , respectively. p X,F (x, f ) denotes the joint histogram of source image X and fused image F . A large MI metric means that considerable information is transferred from source images to the fused image, which indicates a good fusion performance.</p><p>(3) Feature mutual information</p><p>In many image processing tasks, an image is often represented by its features and most of the information related to fused image quality comprise certain features, such as edges, details, and contrast. Therefore, measuring the amount of feature information that is transferred from source images to the fused image is reasonable. Haghighat et al. proposed the feature mutual information (FMI) quality index, which is based on MI and feature information, to measure the amount of feature information that is transferred from source images to the fused image <ref type="bibr" target="#b302">[300]</ref>. The definition of the FMI metric is as follows:</p><formula xml:id="formula_12">F M I = M I Á, F + M I B, F ,<label>(4)</label></formula><p>where Á, B, and F denote the feature maps of infrared, visible, and fused images, respectively. A large FMI metric generally indicates that considerable feature information is transferred from source images to the fused image.</p><p>(4) Structural similarity index measure</p><formula xml:id="formula_13">A C C E P T E D M A N U S C R I P T</formula><p>The human visual system is sensitive to structure loss and distortion. Wang et al.</p><p>proposed a universal quality index called structural similarity index measure (SSIM) to model image loss and distortion <ref type="bibr" target="#b317">[315]</ref>. The index mainly consists of three components: loss of correlation and luminance and contrast distortion. The product of the three components is the assessment result of the fused image and defined as follows:</p><formula xml:id="formula_14">SSIM X,F = x,f 2µ x µ f + C 1 µ 2 x + µ 2 f + C 1 • 2σ x σ f + C 2 σ 2 x + σ 2 f + C 2 • σ xf + C 3 σ x σ f + C 3 ,<label>(5)</label></formula><p>where SSIM X,F denotes the structural similarity between source image X and fused image F ; x and f denote the image patches of source and fused images in a sliding window, respectively; σ xf denotes the covariance of source and fused images; σ x and σ f denote the standard deviation (SD); and µ x and µ f denote the mean values of source and fused images, respectively. C 1 , C 2 , and C 3 are the parameters used to make the algorithm stable; when</p><formula xml:id="formula_15">C 1 = C 2 = C 3 = 0,</formula><p>the SSIM is reduced to the universal image quality index <ref type="bibr" target="#b318">[316]</ref>. Thus, the structural similarities between all source images and the fused image can be written as follows:</p><formula xml:id="formula_16">SSIM = SSIM A,F + SSIM B,F ,<label>(6)</label></formula><p>where SSIM A,F and SSIM B,F denote the structural similarities between infrared/visible and fused images.</p><p>(5) Q AB/F Q AB/F measures the amount of edge information that is transferred from source images to the fused image and is based on the assumption that the edge information in the source images is preserved in the fused image. Q AB/F is defined as follows:</p><formula xml:id="formula_17">Q AB/F = N i=1 M j=1 Q AF (i, j)w A (i, j) + Q BF (i, j)w B (i, j) N i=1 M j=1 (w A (i, j) + w B (i, j)) ,<label>(7)</label></formula><p>where Q XF (i, j) = Q XF g (i, j)Q XF a (i, j), Q XF g (i, j) and Q XF a (i, j) denote the edge strength and orientation values at location (i, j), respectively. w X denotes the weight that expresses the importance of each source image to the fused image. A large Q AB/F means that considerable edge information is transferred to the fused image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_18">M A N U S C R I P T (6) Q W Q W can</formula><p>be determined as a variant of the universal image quality index <ref type="bibr" target="#b319">[317]</ref>. This measure assigns high weights to salient visual regions in an image; these weights can consider the locations and magnitudes of distortions. The definition of Q W is as follows: (7) Q E Q E was designed by modifying Q W <ref type="bibr" target="#b319">[317]</ref> and can consider the edge information of the human visual system. The definition is as follows:</p><formula xml:id="formula_19">Q W (A, B, F ) = w∈W c(w)(λ(w)Q 0 (A, F |w) + (1 -λ(w)Q 0 (B, F |w)),<label>(8)</label></formula><formula xml:id="formula_20">Q E = Q W (A, B, F ) • Q W ( Á, B, F ) α ,<label>(9)</label></formula><p>where Á, B, and F denote the edges of images A, B, and F , respectively, and α is a variable that determines the contribution of edge images. Q E is consistent with the human perception due to the use of the original and corresponding edge images. A high Q E indicates a good fusion performance.</p><p>(8) Standard deviation</p><p>The standard deviation (SD) metric is based on the statistical concept that reflects the distribution and contrast of the fused image. SD is mathematically defined as follows:</p><formula xml:id="formula_21">SD = M i=1 N j=1 (F (i, j) -µ) 2 ,<label>(10)</label></formula><p>where µ denotes the mean value of the fused image. Regions with high contrast always attract human attention due to the sensitivity of the human visual system to contrast.</p><p>Therefore, a fused image with high contrast often results in a large SD, which means that the fused image achieves a good visual effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T (9) Spatial frequency Spatial frequency (SF) is an image quality index based on gradients <ref type="bibr" target="#b320">[318]</ref>, i.e., horizontal and vertical gradients, which are also called spatial row frequency (RF) and column frequency (CF), respectively. The SF metric can measure the gradient distribution of an image effectively, thereby revealing the detail and texture of an image. This metric is defined as follows:</p><formula xml:id="formula_22">SF = √ RF 2 + CF 2 ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_23">RF = M i=1 N j=1 (F (i, j) -F (i, j -1)) 2 and CF = M i=1 N j=1 (F (i, j) -F (i -1, j)) 2 .</formula><p>A fused image with a large SF is sensitive to human perception according to the human visual system and has rich edges and textures.</p><p>(10) Average gradient The average gradient (AG) metric quantifies the gradient information of the fused image <ref type="bibr" target="#b90">[88]</ref> and represents its detail and texture. The AG metric is defined as follows:</p><formula xml:id="formula_24">AG = 1 M N M i=1 N j=1 F 2 x (i, j) + F 2 y (i, j) 2 ,<label>(12)</label></formula><p>where F x (i, j) = F (i, j) -F (i + 1, j) and F y (i, j) = F (i, j) -F (i, j + 1). The larger the AG metric, the more gradient information the fused image contains and the better the performance of the fusion algorithm.</p><p>(11) Mean gradient</p><p>The mean gradient (MG) metric is similar to the AG metric and defined as follows:</p><formula xml:id="formula_25">M G = 1 (M -1)(N -1) × M -1 x=1 N -1 y=1 ((F (x, y) -F (x -1, y)) 2 + (F (x, y) -F (x, y -1)) 2 )/2.<label>(13)</label></formula><p>A large MG metric indicates that the fused image contains rich edges and textures and hence has a good fusion performance. image and source images. The MSE is defined as follows:</p><formula xml:id="formula_26">M SE = M SE AF + M SE BF 2 ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_27">M SE XF = 1 M N M -1 i=0 N -1</formula><p>j=0 (X(i, j) -F (i, j)) 2 , M SE AF and M SE BF denote the dissimilarity between the fused and infrared/visible images. A small MSE metric indicates a good fusion performance, which means that the fused image approximates to the source images and minimal error occurs in the fusion process.</p><p>(13) Root mean squared error</p><p>The root mean squared error (RMSE) metric is similar to the MSE metric and defined as follows:</p><formula xml:id="formula_28">RM SE = RM SE AF + RM SE BF 2 ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_29">RM SE = 1 M N M -1 m=0 N -1 n=0 (X(m, n) -F (m, n)) 2</formula><p>, M SE AF and M SE BF denote the dissimilarity between the fused and infrared/visible images. A small RMSE metric indicates that the fused image has a small amount of error and distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(14) Peak signal-to-noise ratio</head><p>The peak signal-to-noise ratio (PSNR) metric is the ratio of peak value power and noise power in the fused image and thus reflects the distortion during the fusion process. The PSNR metric is defined as follows:</p><formula xml:id="formula_30">P SN R = 10 log 10 r 2 M SE ,<label>(16)</label></formula><p>where r denotes the peak value of the fused image. The larger the PSNR, the closer the fused image is to the source images and the less distortion the fusion method produces.</p><p>(15) Visual information fidelity</p><p>The visual information fidelity (VIF) metric measures the information fidelity of the fused image <ref type="bibr" target="#b300">[298]</ref>, which is consistent with the human visual system. VIF aims to build a model to compute the distortion between the fused and source images, and this process can be achieved by four steps. First, the source images and fused image are filtered and divided into different blocks. Second, the visual information of each block with and without</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T distortion is evaluated. Third, the VIF for each subband is calculated. Finally, the overall metric based on VIF is calculated. The VIF metric demonstrated a good performance on Petrovic's subjective test database <ref type="bibr" target="#b300">[298]</ref> and showed higher computational efficiency than several conventional evaluation metrics.</p><p>(16) Correlation coefficient</p><p>The correlation coefficient (CC) measures the degree of linear correlation of the fused image and source images and is defined as follows:</p><formula xml:id="formula_31">CC = (r AF + r BF ) 2 ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_32">r XF = M i=1 N j=1 (X(i,j)-X)(F (i,j)-µ) M i=1 N j=1 (X(i,j)-X) 2 ( M i=1 N j=1 (F (i,j)-µ) 2 )</formula><p>and X denotes the mean value of source image X. The larger the CC, the more similar the fused image is to the source images and the better the fusion performance.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17) Nonlinear correlation coefficient</head><p>The nonlinear correlation coefficient (NCC) denotes the nonlinear correlation between the fused image and source images <ref type="bibr" target="#b70">[68]</ref> and can be computed as follows:</p><formula xml:id="formula_33">N CC(X; F ) = 2 + b 2 i=1 n i N log b n i N ,<label>(18)</label></formula><p>where n i denotes the number of samples distributed in the i-th rank, b denotes the total number of ranks, and N denotes the total number of sample pairs.</p><p>(18) Other metrics Other image fusion metrics include localized MI <ref type="bibr" target="#b321">[319]</ref>, normalized MI <ref type="bibr" target="#b322">[320]</ref>, Weber MI <ref type="bibr" target="#b323">[321]</ref>, Q P <ref type="bibr" target="#b324">[322]</ref>, average pixel intensity <ref type="bibr" target="#b325">[323]</ref>, and sum of correlation differences <ref type="bibr" target="#b326">[324]</ref>. </p><formula xml:id="formula_34">A C C E P T E D M A N U S C R I P T</formula><p>on the correlation among different images and obtained from the differences between source images and the fused image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we select 18 representative methods and We test the 18 representative methods on the surveillance images from TNO Human Factors, which contain multispectral nighttime imagery of different military relevant scenarios, registered with different multiband camera systems 16 . We conduct experiments on 12 representative image infrared/visible pairs and two infrared/visible image sequences for qualitative and quantitative comparisons. We select nine frequently used assessment metrics, i.e., EN <ref type="bibr" target="#b315">[313]</ref>, MI <ref type="bibr" target="#b316">[314]</ref>, SSIM 17 <ref type="bibr" target="#b318">[316]</ref>, Q AB/F 18 <ref type="bibr" target="#b316">[314]</ref>, Q W <ref type="bibr" target="#b319">[317]</ref>, SD <ref type="bibr" target="#b327">[325]</ref>, SF <ref type="bibr" target="#b320">[318]</ref>,</p><p>VIF 19 [298], and CC <ref type="bibr" target="#b303">[301]</ref>, to evaluate the performances of the different infrared and visible image fusion methods. Large assessment metric values indicate good model performances.</p><p>14 Available at http://www.escience.cn/people/jiayima/cxdm.html. 15 Available at https://github.com/uzeful/Infrared-and-Visual-Image-Fusion-via-Infrared-Feature-Extraction 16 Available at https://figshare.com/articles/TNO_Image_Fusion_Dataset/1008029. 17 Available at http://www.cns.nyu.edu/ ~zwang/files/research/quality_index/demo.html. 18 Available at https://cn.mathworks.com/matlabcentral/fileexchange/ 18213-objective-image-fusion-performance-measure. 19 Available at http://hansy.weebly.com/image-fusion-metric-ifm.html.  The run time comparison of the 18 algorithms on the two sequences are given in Table <ref type="table" target="#tab_5">2</ref>,</p><p>where the images all have a size of 270 × 360 and each value denotes the mean and SD of the run time of the corresponding method on a sequence. The results show that the performances of the multi-scale transform-based methods come in a wide range. Most of these models are fast and stable and have small means and standard variances. However, some approaches, such as NSCT and CBF, are slow. ASR, NSCT SF PCNN, and DDCTPCA are also relatively slow and may be unsuitable for surveillance and tracking applications, which   Despite the considerable progress that has been achieved in infrared and visible image fusion, several issues remain for future work.</p><p>• Multi-scale transform-based infrared and visible image fusion methods usually fix the basis functions and decomposition levels. It is still challenging to select flexible basis functions that allow data-driven choice of the best representation of source images, and the adaptive selection of decomposition levels still remains to be solved.</p><p>• Edge-preserving filtering-based methods have been widely used for image fusion, and efficient, robust, and general filters (e.g., extensions of the original filters) can be designed for infrared and visible image fusion.</p><p>• Traditional sparse representation-based fusion methods often adopt patch-based procedures, which ignore the correlation among different patches and lead to the loss of detail information. In the future, the correlation among different patches can be considered to improve fusion performance. Convolutional sparse coding can be used to enhance detail information. This idea is motivated by deconvolutional networks, which aim to build a hierarchy of sparse representations of source images.</p><p>• Few deep learning-based fusion methods have been specialized for infrared and visible image fusion. In the future, state-of-the-art deep learning techniques can be applied • Different infrared and visible image fusion methods have their own strengths and weaknesses, and the advantages of different methods should be combined to obtain efficient hybrid image fusion methods.</p><p>• The spatial resolutions of infrared and visible image sensors sometimes differ; hence, joint image super-resolution and fusion can be performed in the future.</p><p>• Traditional infrared and visible image fusion methods typically ignore noise. Joint image denoising and fusion can be pursued in the future work.</p><p>• Image pairs without strict alignments lead to ghosting in fused images. The inconsistency of salient structures between the original and fused images can provide a guide for image registration. Therefore, spatial transformation can be considered a variable when designing fusion rules, and registration and fusion can be conducted simultaneously to eliminate ghosting.</p><p>• Traditional fusion methods mainly focus on designing appropriate principles to keep useful information in source images as much as possible. In the future, the develop of fusion methods should also be focus on the application. For example, rather than producing an informative fused image, the fusion should highlight the valuable information in a specific application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of this survey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-scale transform based infrared and visible image fusion scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>S C R I P Tmid transform stems from the Laplacian pyramid transform and calculates the ratio of two adjacent low-pass filtering images of the Gaussian pyramid; this model could consider local contrast<ref type="bibr" target="#b40">[38]</ref>. Jin et al. proposed an infrared and visible image fusion method that was based on the contrast pyramid and multi-objective evolutionary algorithm, where the latter was used to optimize fusion coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Yan( 3 )</head><label>3</label><figDesc>et al. proposed a new infrared and visible image fusion that was based on the spectral graph wavelet transform [54], and their method represented the irregular regions of images. Moreover, Chai et al. applied an infrared and visible image fusion model that was based on A C C E P T E D M A N U S C R I P T the quaternion wavelet transform, which could be considered a generalization of the traditional wavelet transform on the basis of quaternion algebra. Xu et al. proposed an infrared and visible video fusion method formed by combining the wavelet transform with motion compensation [56]. Nonsubsampled contourlet transform The wavelet transform is a fast and efficient method of representing 1D piecewise smooth signals and has been successfully applied in many signal processing and communication domains. Extending the 1D signal wavelet transform to a 2D image wavelet transform can isolate discontinuities at edge points. However, the 2D wavelet transform cannot capture the abundant directional information of images. To overcome this problem, Minh N. Do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>because of these advantages. For example, Meng et al. proposed an infrared and visible image fusion that A C C E P T E D M A N U S C R I P T was based on object region detection and the NSCT, and their model could clarify objects and preserve details and visual artifacts in the fused image [65]. Adu et al. proposed a new</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sparse representation based infrared and visible image fusion scheme. (Credit to [27])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PCNN based infrared and visible image fusion scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Hybrid multi-scale transform and saliency methods aim to integrate salient region detection into the multi-scale transform image fusion framework, which can enhance the region of interest and preserve the details. For example, Zhang et al. adopted the local edgepreserving filter to decompose the source images into the base and detail layers and used the detected salient targets of the infrared image to determine the weight of the base layer<ref type="bibr" target="#b18">[16]</ref>. In addition, sparse representation and neural network are often integrated into the multi-scale transform infrared and visible image fusion framework. For example, Liu et al. proposed a general image fusion framework by combining multi-scale transform with sparse represen-A C C E P T E D M A N U S C R I P T tation [18] and adopted sparse representation to obtain the fused low-frequency subband coefficients. Lin et al. proposed a new infrared and visible image fusion method that was based on the contourlet transform and an improved PCNN; the contourlet transform was used to decompose the source images into multi-scale sub-images, and the improved PCNN was utilized to fuse the high-and low-frequency subband coefficients [62]. Yin et al. proposed a new hybrid multi-scale transform, sparse representation, and neural network fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>For example,</head><label></label><figDesc>Li et al.  proposed a fusion scheme based on nonsubsampled contourlet transform and low-level visual features<ref type="bibr" target="#b74">[72]</ref>. Due to the fact that the human visual system perceives image quality mainly according to some low-level features, they designed two activity measures of lowpass and highpass subbands decomposed by nonsubsampled contourlet transform based on the the low-level features. Alternatively, Bai et al. proposed a fusion method through feature extraction by morphological sequential toggle operator<ref type="bibr" target="#b209">[207]</ref>. Sequential toggle operator is used to extract the features of source images, and then multi-scale features of the original infrared and visible images are transferred into the fused image based on multi-scale morphological theory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Infrared and visible image registration based on edge maps.</figDesc><graphic coords="33,65.72,175.59,71.66,53.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>of the target object. Face recognition is one of the major applications in recognition that employs infrared and visible fusion. Recognition algorithms that use infrared and visible image fusion come in two types. The first class is two-stage recognition: fusing first and then recognizing with the fused result. In the second category, the fusion algorithm is embedded into the recognition process in which distinguishing the boundary between the two processes is difficult. The algorithm proposed by Faten et al. is a representative of the first class<ref type="bibr" target="#b263">[261]</ref>. It solves the problem of near infrared and visible image fusion in face recognition, in which identifying a person at a distance rather than at close range is often necessary. This model uses the wavelet transform to decompose the original face images and then applies SVD and PCA in the fusion process. This method can improve recognition precision by using narrow-band images to enhance the fusion result. Other methods of this class usually change the decomposition algorithms or the fusion rules to achieve enhanced visibility for a specific situation. For example, Heo et al. employed weighted average<ref type="bibr" target="#b253">[251]</ref>,and in<ref type="bibr" target="#b120">[118]</ref> intrinsic mode functions were used instead of the discrete wavelet transform to decompose the original images. Similar methods include<ref type="bibr" target="#b252">[250,</ref><ref type="bibr" target="#b264">262,</ref><ref type="bibr" target="#b265">263]</ref>. Meanwhile, Richa et al. proposed a fusion algorithm with classifiers<ref type="bibr" target="#b5">[3]</ref>. In this method, 2v-granular SVMs are used to learn the global and local features of the original images at several resolutions, which are used to decide the relative weights between the infrared and visible images. Then, the fused image is simultaneously processed by 2D log polar Gabor feature matching and local binary pattern feature matching. Finally, the results are judged by the Dezert-SmarandacheA C C E P T E D MA N U S C R I P T match score fusion, which determines whether the candidate is identified successfully. Compared with other algorithms, this method demonstrates significant superiority even when visible and infrared images provide conflicting results under severe interference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of surveillance with infrared and visible image fusion technique.</figDesc><graphic coords="38,363.94,109.93,135.58,106.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>research improving this transfer scheme has been conducted.Toet  et al. converted the RGB tristimulus into XYZ tristimulus and used lαβ transform to achieve color transferring while optimizing luminance contrast [278, 279]. Tsagaris et al. used NMF instead of PCA to find an additive part-based representation in the fusion method [189]. Cheng et al. proposed a pseudo-color rendering algorithm by developing a deep neural network that was totally different from all previous works [280]. The algorithm combined high-level features extracted by the neural network and adaptive image clustering to improve colorization performance and was better than previous algorithms developed with color transferring. However, this method depended on a large training dataset to learn a powerful representation. Good</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: pseudo-color rendering in image fusion.</figDesc><graphic coords="40,69.38,252.02,102.24,84.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>S C R I P T mentation methods involving special purpose computers have been designed to accelerate the process. Implementation schemes come in two main types: field-programmable gate array (FPGA)-based architecture and compute unified device architecture (CUDA)-based implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>where w denotes the local window and |W | denotes the number of windows in an image. Q 0 (A, F |w) and Q 0 (B, F |w)) can be calculated by the universal image quality index in a sliding window w, λ(w) denotes the saliency weight in each window, and c(w) normalizes saliency among all windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>( 12 )</head><label>12</label><figDesc>Mean squared error The mean squared error (MSE) computes the error of the fused image in comparison with those in the source images and hence measures the dissimilarity between the fused A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 8 : 4 ENFigure 10 :</head><label>8410</label><figDesc>Figure 8: Fusion performance of eighteen representative methods on twelve infrared and visible image pairs. From left to right: Athena, Bench, Bunker, Tank, Nato camp, Sandpath, Kaptein, Kayak, Octec, Street, Steamboat and Road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>S C R I P T cedure, and progress. Image registration significantly affects fusion performance; thus, we briefly review extant registration techniques. Infrared and visible image fusion methods have been widely used in different applications, such as object detection and recognition, tracking, image enhancement, surveillance, and remote sensing, due to their advantages and progress. We then summarize several frequently used infrared and visible image fusion evaluation metrics to evaluate the performances of the studied infrared and visible image fusion methods. Furthermore, we conduct extensive experiments to evaluate the performances of different methods to potentially provide an objective performance reference for researchers in the field of infrared and visible image fusion and consequently support relative engineering with credible and solid evidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>•</head><label></label><figDesc>S C R I P Tin infrared and visible image fusion. For example, more effective deep models can be designed to perform joint activity-level measurement and weight assignment. The imaging mechanisms of infrared and visible sensors are different; thus, we should design niche-targeting saliency detection methods for infrared and visible images separately. The thermal information of objects is reflected by the pixel intensity in infrared images, and regions with serious noise are easily regarded as targets when using saliency detection methods. Therefore, anti-noise saliency detection methods should be designed in future work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Hossny et al. proposed the localized MI for image fusion; this metric adopts the MI on each block of source and fused images, and each block is obtained by EN-driven quad-tree decomposition<ref type="bibr" target="#b321">[319]</ref>. Luo et al. proposed the Weber MI for evaluating infrared and visible image fusion; this metric groups the pixels in infrared and visible images according to the local Weber components to capture image structures<ref type="bibr" target="#b323">[321]</ref>. Aslantas et al. proposed the sum of correlation differences for image fusion quality evaluation<ref type="bibr" target="#b326">[324]</ref>. This metric is based</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>9 assessment metrics to conduct experiments, which can provide an objective performance reference for different infrared and visible image fusion methods and hence support relative engineering with credible and solid evidence. The representative infrared and visible image fusion methods are as follows: LP 1 [31], Wavelet 2 [44], NSCT 3 [69], dual-tree multi-resolution discrete cosine transform (DTMDCT) 4 [116], cross bilateral filter (CBF) 5 [84], hybrid multi-scale decom-local edge-preserving LC (LEPLC) [16], gradient transfer fusion (GTF) 14 [20], and IFE-VIP 15 [209]. The LP, Wavelet, NSCT, DTMDCT, CBF, HMSD, GFF, and ADF are representative multi-scale transform-based methods, ASR and LPSR are representative sparse representation-based methods, OI-PCNN and NSCT-SF-PCNN are representative neural network-based methods, DDCTPCA and FPDE are representative subspace-based methods, TSIFVS and LEPLC are representative saliency-based methods, and GTF and IFE-VIP belong to other method classes. Many of these 18 methods, including GFF, LPSR, NSCT-SF-PCNN, DDCTPCA, TSIFVS, and LEPLC, can be considered hybrid models because they combine different approaches. LPSR is a hybrid multi-scale transform and sparse representation method, NSCT-SF-PCNN is a hybrid multi-scale transform and neural network model, DDCTPCA is a hybrid multi-scale transform and subspace approach, and GFF, TSIFVS, and LEPLC are hybrid multi-scale transform and saliency methods. The experiments are conducted on a desktop with 3.3 GHz Intel Core CPU, 8 GB memory, and MATLAB codes. The codes of the 18 representative image fusion methods are all publicly available, and their parameters are all set in accordance with those in the original studies.</figDesc><table><row><cell>M A N U S C R I P T</cell></row><row><cell>A C C E P T E D</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>position (HMSD)</p>6 </p>[91], guided filtering-based fusion (GFF)</p>7 </p>[79], anisotropic diffusionbased fusion (ADF)</p>8 </p>[82], ASR 3</p><ref type="bibr" target="#b140">[138]</ref></p>, LP and sparse representation (LPSR) 3</p><ref type="bibr" target="#b20">[18]</ref></p>, orientation information-motivated PCNN (OI-PCNN)</p>9 </p>[161], SF-motivated PCNNs in NSCT domain (NSCT-SF-PCNN)</p>10 </p>[159], directional discrete cosine transform and PCA (DDCT-PCA)</p>11 </p>[179], FPDE</p>12 [14]</p>, two-scale image fusion based on visual saliency (TSIFVS)</p>13 </p>[80],</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Run time comparison of eighteen algorithms on the Nato camp and Duine sequences, where each value denotes the mean and standard deviation of run time of a certain method on a sequence (unit: second).</figDesc><table><row><cell></cell><cell>Nato camp</cell><cell>Duine</cell></row><row><cell>LP</cell><cell cols="2">0.0043 ± 0.0007 0.0044 ± 0.0002</cell></row><row><cell>Wavelet</cell><cell cols="2">0.1550 ± 0.0382 0.1592 ± 0.0018</cell></row><row><cell>NSCT</cell><cell cols="2">1.4385 ± 0.0092 1.4402 ± 0.0096</cell></row><row><cell>DTMDCT</cell><cell cols="2">0.0347 ± 0.0018 0.0337 ± 0.0019</cell></row><row><cell>CBF</cell><cell cols="2">6.1431 ± 0.0213 6.1211 ± 0.0304</cell></row><row><cell>HMSD</cell><cell cols="2">0.5441 ± 0.0558 0.5492 ± 0.0328</cell></row><row><cell>GFF</cell><cell cols="2">0.0871 ± 0.0067 0.0927 ± 0.0091</cell></row><row><cell>ADF</cell><cell cols="2">0.1769 ± 0.0031 0.1730 ± 0.0075</cell></row><row><cell>ASR</cell><cell cols="2">94.638 ± 0.3782 94.638 ± 0.3199</cell></row><row><cell>LPSR</cell><cell cols="2">0.0111 ± 0.0026 0.0087 ± 0.0005</cell></row><row><cell>OIPCNN</cell><cell cols="2">0.4000 ± 0.0021 0.3995 ± 0.0018</cell></row><row><cell cols="3">NSCT SF PCNN 72.047 ± 0.2027 72.028 ± 0.1884</cell></row><row><cell>DDCTPCA</cell><cell cols="2">36.901 ± 0.1771 37.102 ± 0.1162</cell></row><row><cell>FPDE</cell><cell cols="2">0.0922 ± 0.0040 0.0925 ± 0.0043</cell></row><row><cell>TSIFVS</cell><cell cols="2">0.0102 ± 0.0019 0.0102 ± 0.0014</cell></row><row><cell>LEPLC</cell><cell cols="2">0.1494 ± 0.0085 0.1575 ± 0.0056</cell></row><row><cell>GTF</cell><cell cols="2">0.9917 ± 0.0609 1.3706 ± 0.1052</cell></row><row><cell>IFEVIP</cell><cell cols="2">0.0517 ± 0.0012 0.0542 ± 0.0015</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Available at http://www.metapix.de/toolbox.htm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Available at https://cn.mathworks.com/help/wavelet/ref/wfusimg.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Available at http://www.escience.cn/people/liuyu1/Codes.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Available athttps://cn.mathworks.com/matlabcentral/fileexchange/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the National Natural Science Foundation of China under Grant nos. 61773295 and 61503288.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Table <ref type="table">1</ref>: Statistics of some representative fusion evaluation measures and references. evaluation measure reference entropy (EN) <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b22">20,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b58">56]</ref>, <ref type="bibr" target="#b59">[57,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b69">67,</ref><ref type="bibr" target="#b71">69,</ref><ref type="bibr" target="#b72">70,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b78">76,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b83">81,</ref><ref type="bibr" target="#b87">85,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b91">89,</ref><ref type="bibr" target="#b92">90,</ref><ref type="bibr" target="#b99">97,</ref><ref type="bibr" target="#b100">98,</ref><ref type="bibr" target="#b101">99,</ref><ref type="bibr" target="#b103">101,</ref><ref type="bibr" target="#b104">102,</ref><ref type="bibr" target="#b107">105,</ref><ref type="bibr" target="#b108">106,</ref><ref type="bibr" target="#b112">110,</ref><ref type="bibr" target="#b113">111]</ref>, <ref type="bibr" target="#b114">[112,</ref><ref type="bibr" target="#b117">115,</ref><ref type="bibr" target="#b121">119,</ref><ref type="bibr" target="#b123">121,</ref><ref type="bibr" target="#b133">131,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b143">141,</ref><ref type="bibr" target="#b145">143,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b160">158,</ref><ref type="bibr" target="#b162">160,</ref><ref type="bibr" target="#b171">169,</ref><ref type="bibr" target="#b177">175,</ref><ref type="bibr" target="#b180">178,</ref><ref type="bibr" target="#b192">190,</ref><ref type="bibr" target="#b193">191,</ref><ref type="bibr" target="#b196">194,</ref><ref type="bibr" target="#b200">198,</ref><ref type="bibr" target="#b201">199,</ref><ref type="bibr" target="#b202">200,</ref><ref type="bibr" target="#b206">204]</ref> mutual information (MI) <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b4">2,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b22">20,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b38">36,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b72">70,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b74">72,</ref><ref type="bibr" target="#b78">76,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b83">81,</ref><ref type="bibr" target="#b85">83,</ref><ref type="bibr" target="#b86">84,</ref><ref type="bibr" target="#b88">86]</ref>, <ref type="bibr" target="#b89">[87,</ref><ref type="bibr" target="#b92">90,</ref><ref type="bibr" target="#b100">98,</ref><ref type="bibr" target="#b102">100,</ref><ref type="bibr" target="#b104">102,</ref><ref type="bibr" target="#b106">104,</ref><ref type="bibr" target="#b108">106,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b110">108,</ref><ref type="bibr" target="#b111">109,</ref><ref type="bibr" target="#b117">115,</ref><ref type="bibr" target="#b121">119,</ref><ref type="bibr" target="#b123">121,</ref><ref type="bibr" target="#b124">122,</ref><ref type="bibr" target="#b125">123,</ref><ref type="bibr" target="#b131">129,</ref><ref type="bibr" target="#b138">136,</ref><ref type="bibr" target="#b140">138,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b145">143]</ref>, <ref type="bibr" target="#b146">[144,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b153">151,</ref><ref type="bibr" target="#b154">152,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b160">158,</ref><ref type="bibr" target="#b162">160,</ref><ref type="bibr" target="#b169">167,</ref><ref type="bibr" target="#b177">175,</ref><ref type="bibr" target="#b180">178,</ref><ref type="bibr" target="#b183">181,</ref><ref type="bibr" target="#b184">182,</ref><ref type="bibr" target="#b185">183,</ref><ref type="bibr" target="#b193">191,</ref><ref type="bibr" target="#b200">198,</ref><ref type="bibr" target="#b201">199,</ref><ref type="bibr" target="#b202">200]</ref> feature mutual information (FMI) <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b171">169,</ref><ref type="bibr" target="#b201">199,</ref><ref type="bibr" target="#b302">300,</ref><ref type="bibr" target="#b303">301]</ref> structural similarity index measure (SSIM) <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b92">90,</ref><ref type="bibr" target="#b103">101,</ref><ref type="bibr" target="#b104">102,</ref><ref type="bibr" target="#b107">105,</ref><ref type="bibr" target="#b111">109,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b169">167,</ref><ref type="bibr" target="#b180">178,</ref><ref type="bibr" target="#b184">182,</ref><ref type="bibr" target="#b200">198</ref>]  <ref type="bibr" target="#b147">[145,</ref><ref type="bibr" target="#b148">146,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b153">151,</ref><ref type="bibr" target="#b154">152,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154,</ref><ref type="bibr" target="#b162">160,</ref><ref type="bibr" target="#b169">167,</ref><ref type="bibr" target="#b170">168,</ref><ref type="bibr" target="#b183">181,</ref><ref type="bibr" target="#b184">182,</ref><ref type="bibr" target="#b185">183,</ref><ref type="bibr" target="#b200">198,</ref><ref type="bibr" target="#b202">200,</ref><ref type="bibr" target="#b207">205,</ref><ref type="bibr" target="#b208">206]</ref> Q W <ref type="bibr" target="#b4">[2,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b85">83,</ref><ref type="bibr" target="#b105">103,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b124">122,</ref><ref type="bibr" target="#b138">136,</ref><ref type="bibr" target="#b139">137,</ref><ref type="bibr" target="#b141">139,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b146">144,</ref><ref type="bibr" target="#b147">145,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154,</ref><ref type="bibr" target="#b170">168,</ref><ref type="bibr" target="#b199">197]</ref> Q E <ref type="bibr" target="#b4">[2,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b79">77,</ref><ref type="bibr" target="#b85">83,</ref><ref type="bibr" target="#b105">103,</ref><ref type="bibr" target="#b109">107,</ref><ref type="bibr" target="#b124">122,</ref><ref type="bibr" target="#b138">136,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b146">144,</ref><ref type="bibr" target="#b147">145,</ref><ref type="bibr" target="#b149">147,</ref><ref type="bibr" target="#b150">148,</ref><ref type="bibr" target="#b155">153,</ref><ref type="bibr" target="#b156">154,</ref><ref type="bibr" target="#b171">169,</ref><ref type="bibr" target="#b184">182,</ref><ref type="bibr" target="#b199">197]</ref> standard deviation (SD) <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b14">12,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b34">32,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b59">57,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b78">76,</ref><ref type="bibr" target="#b83">81,</ref><ref type="bibr" target="#b86">84,</ref><ref type="bibr" target="#b92">90,</ref><ref type="bibr" target="#b100">98]</ref>, <ref type="bibr" target="#b101">[99,</ref><ref type="bibr" target="#b107">105,</ref><ref type="bibr" target="#b110">108,</ref><ref type="bibr" target="#b112">110,</ref><ref type="bibr" target="#b113">111,</ref><ref type="bibr" target="#b117">115,</ref><ref type="bibr" target="#b123">121,</ref><ref type="bibr" target="#b133">131,</ref><ref type="bibr" target="#b142">140,</ref><ref type="bibr" target="#b145">143,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b156">154,</ref><ref type="bibr" target="#b180">178,</ref><ref type="bibr" target="#b192">190,</ref><ref type="bibr" target="#b193">191,</ref><ref type="bibr" target="#b200">198,</ref><ref type="bibr" target="#b202">200]</ref> spatial frequency (SF) <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b83">81,</ref><ref type="bibr" target="#b86">84,</ref><ref type="bibr" target="#b88">86,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b100">98,</ref><ref type="bibr" target="#b104">102,</ref><ref type="bibr" target="#b108">106,</ref><ref type="bibr" target="#b115">113,</ref><ref type="bibr" target="#b117">115,</ref><ref type="bibr" target="#b123">121,</ref><ref type="bibr" target="#b160">158,</ref><ref type="bibr" target="#b180">178,</ref><ref type="bibr" target="#b181">179,</ref><ref type="bibr" target="#b206">204,</ref><ref type="bibr" target="#b210">208]</ref> average gradient (AG) <ref type="bibr" target="#b39">[37,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b56">54,</ref><ref type="bibr" target="#b57">55,</ref><ref type="bibr" target="#b64">62,</ref><ref type="bibr" target="#b67">65,</ref><ref type="bibr" target="#b68">66,</ref><ref type="bibr" target="#b69">67,</ref><ref type="bibr" target="#b71">69,</ref><ref type="bibr" target="#b75">73,</ref><ref type="bibr" target="#b77">75,</ref><ref type="bibr" target="#b78">76]</ref>, <ref type="bibr" target="#b83">[81,</ref><ref type="bibr" target="#b86">84,</ref><ref type="bibr" target="#b90">88,</ref><ref type="bibr" target="#b99">97,</ref><ref type="bibr" target="#b101">99,</ref><ref type="bibr" target="#b107">105,</ref><ref type="bibr" target="#b110">108,</ref><ref type="bibr" target="#b112">110,</ref><ref type="bibr" target="#b117">115,</ref><ref type="bibr" target="#b119">117,</ref><ref type="bibr" target="#b123">121,</ref><ref type="bibr" target="#b151">149,</ref><ref type="bibr" target="#b185">183,</ref><ref type="bibr" target="#b192">190,</ref><ref type="bibr" target="#b200">198]</ref> mean gradient (MG) <ref type="bibr" target="#b63">[61,</ref><ref type="bibr" target="#b88">86,</ref><ref type="bibr" target="#b115">113,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b206">204,</ref><ref type="bibr" target="#b210">208,</ref><ref type="bibr" target="#b304">302,</ref><ref type="bibr" target="#b305">303]</ref> mean squared error (MSE) <ref type="bibr" target="#b306">[304,</ref><ref type="bibr" target="#b307">305,</ref><ref type="bibr" target="#b308">306,</ref><ref type="bibr" target="#b309">307]</ref> root mean squared error (RMSE) <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b49">47,</ref><ref type="bibr" target="#b122">120,</ref><ref type="bibr" target="#b133">131,</ref><ref type="bibr" target="#b303">301,</ref><ref type="bibr" target="#b310">308,</ref><ref type="bibr" target="#b311">309]</ref> peak signal to noise ratio (PSNR) <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b63">61,</ref><ref type="bibr" target="#b103">101,</ref><ref type="bibr" target="#b133">131,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b160">158,</ref><ref type="bibr" target="#b177">175,</ref><ref type="bibr" target="#b183">181,</ref><ref type="bibr" target="#b184">182,</ref><ref type="bibr" target="#b185">183,</ref><ref type="bibr" target="#b187">185,</ref><ref type="bibr" target="#b279">277,</ref><ref type="bibr" target="#b306">304,</ref><ref type="bibr" target="#b307">305,</ref><ref type="bibr" target="#b312">310]</ref> visual information fidelity (VIF) <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b73">71,</ref><ref type="bibr" target="#b74">72,</ref><ref type="bibr" target="#b76">74,</ref><ref type="bibr" target="#b82">80,</ref><ref type="bibr" target="#b85">83,</ref><ref type="bibr" target="#b88">86,</ref><ref type="bibr" target="#b111">109,</ref><ref type="bibr" target="#b116">114,</ref><ref type="bibr" target="#b124">122,</ref><ref type="bibr" target="#b143">141,</ref><ref type="bibr" target="#b144">142,</ref><ref type="bibr" target="#b152">150,</ref><ref type="bibr" target="#b153">151,</ref><ref type="bibr" target="#b154">152,</ref><ref type="bibr" target="#b171">169,</ref><ref type="bibr" target="#b279">277,</ref><ref type="bibr" target="#b313">311,</ref><ref type="bibr" target="#b300">298]</ref> correlation coefficient (CC) <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b86">84,</ref><ref type="bibr" target="#b111">109,</ref><ref type="bibr" target="#b185">183,</ref><ref type="bibr" target="#b307">305,</ref><ref type="bibr" target="#b314">312]</ref> nonlinear correlation coefficient (NCC) <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b70">68]</ref> (1) Entropy Entropy (EN) measures the amount of information contained in a fused image on the basis of information theory <ref type="bibr" target="#b315">[313]</ref>. EN is mathematically defined as follows:</p><p>where L denotes the number of gray levels and p l is the normalized histogram of the corresponding gray level in the fused image. The larger the EN, the more information is contained in the fused image and the better the performance of the fusion method. However, EN may be influenced by noise; the more noise the fused image contains, the larger the EN. Therefore, EN is usually used as an auxiliary metric.</p><p>(2) Mutual information    need to achieve fusion in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future trend</head><p>Infrared and visible image fusion has attracted considerable attention and made significant progress in the past few decades. Thus, we comprehensively survey existing infrared and visible image fusion methods and applications. These approaches can be divided into seven categories: multi-scale decomposition-based, sparse representation-based, neural network-based, subspace-based, saliency-based, and hybrid methods and other models.</p><p>Each category is briefly introduced and summarized according to core idea, theory, pro-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>• Fusion performance evaluation methods usually use edge and intensity to compute the contributions of infrared and visible images to the fused image, but these features do not conform to the human visual system significantly. One trend is to design features that represent visual information that conforms to the human visual system. Another trend involves proposing a new model to represent the information transformation between fused and source images.</p><p>• Since the qualitative performance of a fusion algorithm may not always match with its quantitative performance, it is desirable to design new metrics, which can match with the qualitative performance of fused images. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/bitzhouzq/Context-Enhance-via-Fusion" />
		<title level="m">image-fusion-techniques-using-dct. 5 Available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">fusion-of-infrared-and-visible-sensor-images-based-on-anisotropic-diffusion-and-kl-transform</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">multi-sensor-image-fusion-based-on-fourth-order-partial-differential-equations</title>
		<ptr target="https://sites.google.com/view/durgaprasadbavirisetti/publications.References" />
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A general framework for multiresolution image fusion: from pixels to regions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="259" to="280" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fusion method for infrared and visible images by using non-negative sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="477" to="489" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrated multilevel image fusion and match score fusion of visible and infrared face images for robust face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="880" to="893" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fusion of color and infrared video for moving human detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1771" to="1784" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusion of thermal infrared and visible spectrum video for robust surveillance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="528" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image fusion techniques for remote sensing applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Morabito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Serpico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance comparison of different multi-resolution transforms for image fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A wavelet-based image fusion tutorial</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pajares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1855" to="1872" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A categorization of multiscale-decomposition-based image fusion schemes with a performance study for a digital camera application</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1315" to="1326" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Group-sparse representation with dictionary learning for medical image denoising and fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3450" to="3459" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fusion algorithm for infrared and visible images based on adaptive dual-channel unit-linking pcnn in nsct domain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Novel fusion method for visible light and infrared images based on nsstsf-pcnn</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-sensor image fusion based on fourth order partial differential equations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bavirisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive fusion method of visible light and infrared images based on non-subsampled shearlet transform and fast non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="161" to="172" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via saliency analysis and local edge-preserving multi-scale decomposition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1400" to="1410" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infrared image enhancement through saliency feature analysis based on multi-scale decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="86" to="93" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A general framework for image fusion based on multi-scale transform and sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="147" to="164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion based on visual saliency map and weighted least square optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="8" to="17" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion via gradient transfer and total variation minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="100" to="109" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fusion of visible and infrared images using global entropy and gradient constrained regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="201" to="209" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Region level based multi-focus image fusion using quaternion wavelet and normalized cut</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image fusion by a ratio of low-pass pyramid</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fusion of multispectral and panchromatic satellite images using the curvelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="140" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion using entropy and neuro-fuzzy concepts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Mouli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Convention of Computer Society of India</title>
		<meeting>the Annual Convention of Computer Society of India</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pixel-level image fusion: A survey of the state of the art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="100" to="112" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From multi-scale decomposition to non-multi-scale decomposition methods: A comprehensive survey of image fusion techniques and its applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dogra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="16040" to="16067" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can recent innovations in harmonic analysis &apos;explain&apos; key findings in natural image statistics?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Flesia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="393" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Remotely sensing image fusion based on wavelet transform and human vision system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Image Processing and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image fusion of visible and thermal images for fruit detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bulanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alchanatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems Engineering</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A false color image fusion method based on multi-resolution color transfer in normalization ycbcr space</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="6010" to="6016" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visible and nir image fusion using weight-map-guided laplacian-gaussian pyramid for improving scene visibility</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Vanmali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Gadre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sādhanā</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1063" to="1082" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image fusion by using steerable pyramid</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hanasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="929" to="939" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image fusion based on expectation maximization algorithm and steerable pyramid</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Optics Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="386" to="389" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image fusion based on steerable pyramid and pcnn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on the Applications of Digital Information and Web Technologies</title>
		<meeting>the International Conference on the Applications of Digital Information and Web Technologies</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="569" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visual images based on contrast pyramid directional filter banks using clonal selection optimizing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">27002</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A fusion method for visible and infrared images based on contrast pyramid with teaching learning based optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="134" to="142" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fusion of visible and infrared images using multiobjective evolutionary algorithm based on decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="151" to="158" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Infrared and multi-type images fusion algorithm based on contrast pyramid transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="133" to="146" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The steerable pyramid: A flexible architecture for multi-scale derivative computation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="444" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wavelets and image fusion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="248" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Airborne infrared and visible image fusion for target perception based on target region segmentation and discrete wavelet transform, Mathematical Problems in Engineering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Infrared and visible images fusion method based on discrete wavelet transform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computers</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Swarm intelligence based optimisation in thermal image fusion using dual tree discrete wavelet transform</title>
		<author>
			<persName><forename type="first">K</forename><surname>Madheswari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Venkateswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative InfraRed Thermography Journal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="43" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A novel fusion scheme for visible and infrared images based on compressive sensing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="168" to="177" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion using fuzzy logic and population-based optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1041" to="1054" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Airborne infrared and visible image fusion combined with region segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1127</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visible and infrared image fusion using the lifting wavelet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indonesian Journal of Electrical Engineering and Computer Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6290" to="6295" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image fusion scheme using a novel dual-channel pcnn in lifting stationary wavelet domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3591" to="3602" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Image fusion algorithm based on redundant-lifting nswmda and adaptive pcnn</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="6247" to="6255" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion with spectral graph wavelet transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image fusion using quaternion wavelet transform and multiple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="6724" to="6734" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Infrared-visible video fusion based on motion-compensated wavelet transforms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="318" to="328" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A multi-faceted adaptive image fusion algorithm using a multi-wavelet-based matching measure in the pcnn domain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1113" to="1124" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The dual-tree complex wavelet transform</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Selesnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="123" to="151" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The contourlet transform: an efficient directional multiresolution image representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2091" to="2106" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A filter bank for the directional decomposition of images: Theory and design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="882" to="893" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An improved fusion algorithm for infrared and visible images based on multi-scale transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="28" to="37" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Infrared and visible image fusion algorithm based on contourlet transform and pcnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Infrared Materials, Devices, and Applications</publisher>
			<biblScope unit="page">683514</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Research on fusion technology based on low-light visible image and infrared image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tahir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">123104</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The nonsubsampled contourlet transform: theory, design, and applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Da Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3089" to="3101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image fusion based on object region detection and non-subsampled contourlet transform</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="375" to="383" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion based on nsct and fuzzy logic</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Mechatronics and Automation</title>
		<meeting>the International Conference on Mechatronics and Automation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="671" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Infrared image and visible light image fusion based on nonsubsampled contourlet transform and the gradient of uniformity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advancements in Computing Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="114" to="121" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion based on region of interest detection and nonsubsampled contourlet transform</title>
		<author>
			<persName><forename type="first">H.-X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Shanghai Jiaotong University (Science)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="526" to="534" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Image fusion based on nonsubsampled contourlet transform for infrared and visible light image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fusion method of infrared and visible images based on neighborhood characteristic and regionalization in nsct domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>-L. Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="4980" to="4984" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A fast fusion scheme for infrared and visible light images in nsct domain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="266" to="275" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion scheme based on nsct and low-level visual features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="174" to="184" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visible images based on nonsubsampled contourlet transform and sparse k-svd dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A new infrared and visible image fusion algorithm in nsct domain</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligent Computing</title>
		<meeting>the International Conference on Intelligent Computing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="420" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multi-sensor image fusion by nsct-pcnn transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Science and Automation Engineering</title>
		<meeting>the IEEE International Conference on Computer Science and Automation Engineering</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="638" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Image fusion technique based on non-subsampled contourlet transform and adaptive unit-fast-linking pulse-coupled neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="121" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Image fusion scheme based on modified dual pulse coupled neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>El-Taweel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Helmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="407" to="414" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Edge-preserving decompositions for multi-scale tone and detail manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Image fusion with guided filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Two-scale image fusion of visible and infrared images using saliency detection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bavirisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dhuli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion using multiscale directional nonlocal means filter</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-G. Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="4299" to="4308" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visible sensor images based on anisotropic diffusion and karhunen-loeve transform</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bavirisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dhuli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The multiscale directional bilateral filter and its application to multisensor image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="206" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Image fusion based on pixel significance using cross bilateral filter</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal, Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1193" to="1204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fusion of visible and infrared images using saliency A</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li ; C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T analysis and detail preserving based image decomposition</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="93" to="99" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Infrared Physics &amp; Technology</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Image fusion using multiscale edge-preserving decomposition based on weighted least squares filter</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion with the use of multi-scale edge-preserving decomposition and guided image filter</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Detail preserved fusion of visible and infrared images using regional saliency extraction and multi-scale image decomposition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<biblScope unit="page" from="199" to="209" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multiscale image fusion through guided filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hogervorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Security+ Defence</title>
		<meeting>the SPIE Security+ Defence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">99970</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">An adaptive fusion algorithm for visible and infrared videos based on entropy and the cumulative distribution of gray levels</title>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2706" to="2719" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Perceptual fusion of infrared and visible images through a hybrid multi-scale decomposition with gaussian and bilateral filters</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
	<note>Bilateral filtering for gray and color images</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Local edge-preserving multiscale decomposition for high dynamic range image tone mapping</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Edge-preserving image decomposition using l1 fidelity with l0 gradient</title>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGGRAPH Asia</title>
		<meeting>the SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Image fusion algorithm based on adaptive pulse coupled neural networks in curvelet domain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Signal Processing</title>
		<meeting>the International Conference on Signal Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visible images based on focus measure operators in the curvelet domain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1910" to="1921" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Visible and infrared image fusion based on curvelet transform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Systems and Informatics</title>
		<meeting>the International Conference on Systems and Informatics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="828" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Visible and infrared video fusion using uniform discrete curvelet transform and spatial-temporal information</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Journal of Electronics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="766" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion with the target marked based on multi-resolution visual attention mechanisms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Selected Papers of the Chinese Society for Optical Engineering Conferences</title>
		<meeting>the Selected Papers of the Chinese Society for Optical Engineering Conferences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">102552</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">An image fusion framework based on human visual system in framelet domain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Wavelets, Multiresolution and Information Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page">1250002</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A three scale image transformation for infrared and visible image fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Image fusion by pulse couple neural network with shearlet</title>
		<author>
			<persName><forename type="first">P</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">67005</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Technique for gray-scale visual light and infrared image fusion based on non-subsampled shearlet transform</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="110" to="118" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Technique for infrared and visible image fusion based on non-subsampled shearlet transform and spiking cortical model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="87" to="98" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A fusion algorithm for infrared and visible images based on saliency analysis and non-subsampled shearlet transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="286" to="297" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A novel infrared and visible image fusion algorithm based on shift-invariant dual-tree complex shearlet transform and sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page" from="182" to="191" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">A fusion algorithm for infrared and visible based on guided filtering and phase congruency in nsst domain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics and Lasers in Engineering</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="71" to="77" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion based on tetrolet transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Communications, Signal Processing, and Systems</title>
		<meeting>the International Conference on Communications, Signal Processing, and Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="701" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Fusion of visible and infrared image based on stationary tetrolet transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Youth Academic Annual Conference of Chinese Association of Automation</title>
		<meeting>the Youth Academic Annual Conference of Chinese Association of Automation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="854" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visual images through region extraction by using multi scale center-surround top-hat transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="8444" to="8457" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Multiscale top-hat selection transform based infrared and visual image fusion with emphasis on extracting regions of interest</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A fusion method for infrared-visible image and infrared-polarization image based A</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang ; C C E P T E D M A N U S C R I P T On</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">multi-scale center-surround top-hat transform</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="370" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Fusion of infrared-visible images using improved multi-scale top-hat transform and suitable fusion rules</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="282" to="295" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Novel image fusion techniques using dct</title>
		<author>
			<persName><forename type="first">V</forename><surname>Naidu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Business Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion technology based on directionlets transform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Wireless Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Fusion of visible and infrared images using empirical mode decomposition to improve face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gribok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2049" to="2052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Region-based fusion of infrared and visible images using bidimensional empirical mode decomposition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Educational and Information Technology</title>
		<meeting>the International Conference on Educational and Information Technology</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">358</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Hybrid image fusion scheme using self-fractional fourier functions and multivariate empirical mode decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sahula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="146" to="159" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fusion of infrared and visible images based on bemd and nsdfb</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="82" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Image fusion with internal generative mechanism</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2382" to="2391" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Research on mr-svd based visual and infrared image fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Optoelectronic Technology and Application</title>
		<meeting>the International Symposium on Optoelectronic Technology and Application</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">101571</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Curvelets -a surprisingly effective nonadaptive representation for objects with edges</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy &amp; Astrophysics</title>
		<imprint>
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1051" to="1057" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Fast discrete curvelet transforms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Demanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="861" to="899" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Optimally sparse multidimensional representation using shearlets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Labate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="298" to="318" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Space-frequency quantization for image compression with directionlets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Velisavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beferull-Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1761" to="1773" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings Mathematical Physical &amp; Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="903" to="995" />
			<date type="published" when="1971">1971. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Multiple visual features measurement with gradient domain guided filtering for multisensor image fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Que</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and Measurement</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="691" to="703" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">A region-based multiresolution image fusion algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1557" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Fusion of visual and ir images for concealed weapon detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1198" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">A survey of sparse representation: algorithms and applications</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Semi-supervised sparse representation based classification for face recognition with insufficient labeled samples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2545" to="2560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Pixel-level image fusion with simultaneous orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Efficient image fusion with approximate sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guoyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Wavelets, Multiresolution and Information Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">1650024</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Simultaneous image fusion and denoising with adaptive sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="347" to="357" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Visual attention guided image fusion with sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="4881" to="4888" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion based on random projection and sparse representation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1640" to="1652" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Image fusion with convolutional sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1882" to="1886" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">An image fusion framework using novel dictionary based sparse representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aishwarya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="21869" to="21888" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Image decomposition fusion method based on sparse representation and neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="7969" to="7977" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Space target image fusion method based on image clarity criterion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">53102</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Dictionary learning method for joint sparse representation-based image fusion</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">57006</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The infrared and visible image fusion algorithm based on target separation and sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="397" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Image fusion by hierarchical joint sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Sparse representation with learned multiscale dictionary for image fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="600" to="610" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Image fusion via nonlocal sparse k-svd dictionary learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1814" to="1823" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Joint patch clustering-based dictionary learning for multimodal image fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="198" to="214" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A novel geometric dictionary construction approach for sparse representation based image fusion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">306</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A novel multi-modality image fusion method based on image decomposition and sparse representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion method based on saliency detection in sparse domain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="94" to="102" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Texture clear multi-modal image fusion with joint sparsity model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="255" to="265" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Bruckstein, k-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Signal recovery from random measurements via orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4655" to="4666" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Algorithms for simultaneous sparse approximation. part i: Greedy pursuit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="588" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Fuzzy image fusion based on modified self-generating neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="8515" to="8523" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Image fusion algorithm based on spatial frequencymotivated pulse coupled neural networks in nonsubsampled contourlet transform domain</title>
		<author>
			<persName><forename type="first">X.-B</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1508" to="1514" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Pcnn-based image fusion in compressed domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Problems in Engineering 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">536215</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Image fusion algorithm based on orientation information motivated pulse coupled A</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan ; C C E P T E D M A N U S C R I P T Neural</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Congress on Intelligent Control and Automation</title>
		<meeting>the World Congress on Intelligent Control and Automation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2437" to="2441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">A neural network for feature linking via synchronous activity: Results from cat visual cortex and from simulations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eckhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Reitbock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Microbiology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="759" to="763" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Pulse-coupled neural nets: translation, rotation, scale, distortion, and intensity signal invariance for images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="6239" to="6253" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Physiologically motivated image fusion using pulse-coupled neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Broussard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings of the Applications and Science of Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="372" to="384" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Deep learning for pixel-level image fusion: Recent advances and future prospects</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="158" to="173" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Multi-focus image fusion with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="191" to="207" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Image segmentation-based multi-focus image fusion through multi-scale convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="15750" to="15761" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Image fusion and super-resolution with convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Chinese Conference on Pattern Recognition</title>
		<meeting>the Chinese Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">A medical image fusion method based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Multispectral and hyperspectral image fusion using a 3-d-convolutional neural network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Palsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ulfarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="639" to="643" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Pansharpening by convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">594</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Wavelets, Multiresolution and Information Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1850018</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Image fusion: theories, techniques and applications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Principal component analysis-based image fusion routine with application to automotive stamping split detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Omar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in Nondestructive Evaluation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="76" to="91" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">An advanced image fusion algorithm based on wavelet transform: incorporation with pca and morphological processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Essock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="177" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Pca-based image fusion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muttan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XII</title>
		<meeting>the Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XII</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">62331</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Image fusion using hierarchical pca</title>
		<author>
			<persName><forename type="first">U</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mudengudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on image Information Processing</title>
		<meeting>the International Conference on image Information Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Hybrid ddct-pca based multi sensor image fusion</title>
		<author>
			<persName><forename type="first">V</forename><surname>Naidu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Pixel level jointed sparse representation with rpca image fusion algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Babyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Telecommunications and Signal Processing</title>
		<meeting>the International Conference on Telecommunications and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="592" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Infrared and visible images fusion based on rpca and nsct</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="114" to="123" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Pixel-level multisensor image fusion based on matrix completion and robust principal component analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13007</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Region-based multimodal image fusion using ica bases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="743" to="751" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Pixel-based and region-based image fusion schemes using ica bases</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Region-based ica image fusion using textural information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Antonopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Digital Signal Processing</title>
		<meeting>the International Conference on Digital Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Region-based image fusion using a combinatory chebyshevica method</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mitianoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Graph regularized non-negative low-rank matrix factorization for image clustering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3840" to="3853" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Fusion of visible and infrared imagery for night color vision</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tsagaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Anastassopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Displays</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="191" to="196" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Technique for image fusion based on non-subsampled contourlet transform domain improved nmf</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2429" to="2440" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Image fusion based on non-negative matrix factorization and infrared feature A</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song ; C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Congress on Image and Signal Processing</title>
		<meeting>the International Congress on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1046" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Computational versus psychophysical bottom-up image saliency: A comparative evaluation study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2131" to="2146" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Fast saliency-aware multi-modality image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">De</forename><surname>Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="70" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Multi-window visual saliency extraction for fusion of visible and infrared images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="295" to="302" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Visible and near-infrared image fusion based on visually salient area selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Digital Photography</title>
		<meeting>the Digital Photography</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">94040</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">An infrared-visible image fusion scheme based on nsct and compressed sensing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Maldague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE Defense+ Security</title>
		<meeting>the SPIE Defense+ Security</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">94740</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">An infrared and visible image fusion method based on non-subsampled contourlet transform and joint sparse representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber</title>
		<meeting>the IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber</meeting>
		<imprint>
			<publisher>Physical and Social Computing (CPSCom) and IEEE Smart Data</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="492" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">A novel image fusion approach based on compressive sensing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion using total variation model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion based on total variation and augmented lagrangian</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1961" to="1968" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">A novel multi-focus image fusion method based on stationary wavelet transform and local features of fuzzy sets</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Access</publisher>
			<biblScope unit="page" from="20286" to="20302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">An improved visible and infrared image fusion based on local energy and fuzzy logic</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Signal Processing</title>
		<meeting>the International Conference on Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="861" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Ir and color image fusion using interval type 2 fuzzy logic system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kashyap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Cognitive Computing and Information Processing</title>
		<meeting>the International Conference on Cognitive Computing and Information Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Infrared and visual image fusion through fuzzy measure and alternating operators</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="17149" to="17167" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Entropy based multi-resolution visible-infrared image fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Electrical &amp; Electronics Engineering</title>
		<meeting>the IEEE Symposium on Electrical &amp; Electronics Engineering</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Versatile visible and near-infrared image fusion based on high visibility area selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13016" to="013016" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Infrared and visual image fusion through feature extraction by morphological sequential toggle operator</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Morphological center operator based infrared and visible image fusion through correlation coefficient</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="546" to="554" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Infrared and visual image fusion through infrared feature extraction and visual information preservation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="227" to="237" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">Feature level image fusion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shivprasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuriakose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Amruth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CCSO</title>
		<meeting>CCSO</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="42" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Image fusion based on visual salient features and the cross-contrast</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="218" to="224" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Visible and infrared image registration based on visual salient features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">53017</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Multi-level image fusion and enhancement for target detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-International Journal for Light and Electron Optics</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1203" to="1208" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Symbol-based image fusion for object recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multisource-Multisensor Information Fusion</title>
		<meeting>the International Conference on Multisource-Multisensor Information Fusion</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Real time face recognition using decision fusion of neural classifiers in the visible and thermal infrared spectrum</title>
		<author>
			<persName><forename type="first">V.-E</forename><surname>Neagoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-D</forename><surname>Ropot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Mugioiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>the IEEE Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Decision-level fusion of infrared and visible images for face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Chinese Control and Decision Conference</title>
		<meeting>the Chinese Control and Decision Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2411" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Multi-sensor image decision level fusion detection algorithm based on ds evidence theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Instrumentation and Measurement</title>
		<meeting>the International Conference on Instrumentation and Measurement</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="620" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Non-rigid visible and infrared face registration via regularized gaussian fields criterion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="772" to="784" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Multiscale fusion of visible and thermal ir images for illumination-invariant face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Image registration methods: a survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zitova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flusser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="977" to="1000" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Robust l 2 e estimation of transformation for</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu ; A C C E P T E D M A N U S C R I P T Non</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1115" to="1129" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>rigid registration</note>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Color guided thermal image super resolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Visual Communications and Image Processing</title>
		<meeting>the Visual Communications and Image Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">A survey of image registration techniques</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="376" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Deformable medical image registration: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1153" to="1190" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Feature guided gaussian mixture model with semi-supervised em and local geometric constraint for retinal image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page" from="128" to="142" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">ARRSI: Automatic registration of remote-sensing images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1483" to="1493" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Robust feature matching for remote sensing image registration via locally linear transforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6469" to="6481" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Image matching by normalized cross-correlation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="729" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<title level="m">The Fourier transform and its applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Alignment by maximization of mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">The correlation ratio as a new similarity measure for multimodal image registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malandain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">An fft-based technique for translation, rotation, and scale-invariant image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Chatterji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1266" to="1271" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Registration of visible and near infrared unmanned aerial vehicle images based on fourier-mellin transform</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rabatel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Labbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Precision Agriculture</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="587" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Visible and infrared image registration using trajectories and composite foreground images</title>
		<author>
			<persName><forename type="first">G.-A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">Mutual information based registration of multimodal stereo videos for person tracking</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Krotosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="287" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Rigid point feature registration using mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan ; A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="440" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Comparison of texture features based on gabor filters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kruizinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1160" to="1167" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Infrared-visual image registration based on corners and hausdorff distance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hrkać</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalafatić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Scandinavian Conference on Image Analysis</title>
		<meeting>the Scandinavian Conference on Image Analysis</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Automatic visible and infrared face registration based on silhouette matching and robust transformation estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Visible and infrared image registration in man-made environments employing hybrid visual features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">De</forename><surname>Zeeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Efficient registration of optical and infrared images via modified sobel edging for plant canopy temperature estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wheaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cooley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1213" to="1221" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Robust point matching via vector field consensus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1706" to="1721" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Robust estimation of nonrigid transformation for point set registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Regularized vector field learning with sparse approximation for mismatch removal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3519" to="3532" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Recent advances in visual and infrared face recognitiona review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="135" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Fusion of visual and thermal signatures with eyeglass removal for robust face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">New method for the fusion of complementary information from infrared and</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuruk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T visual images for object detection</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="48" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>IET Image Processing</note>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Method of visual and infrared fusion for moving object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1981" to="1983" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Fusion tracking in color and infrared images using joint sparse representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="590" to="599" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Approaches to multisensor data fusion in target tracking: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1696" to="1710" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Comparison of fusion methods for thermovisual surveillance tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>O'conaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">One color contrast enhanced infrared and visible image fusion method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="150" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<monogr>
		<title level="m" type="main">Image fusion: algorithms and applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visible images based on the second generation curvelet transform</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Infrared &amp; Millimeter Waves</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="254" to="258" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Image registration and fusion of visible and infrared integrated camera for medium-altitude unmanned aerial vehicle remote sensing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">441</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Nir and visible image fusion for improving face recognition at long distance</title>
		<author>
			<persName><forename type="first">F</forename><surname>Omri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Foufou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image and Signal Processing</title>
		<meeting>the International Conference on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="549" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gyaourova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of SPIE</title>
		<imprint>
			<biblScope unit="volume">5404</biblScope>
			<biblScope unit="page" from="585" to="596" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Face recognition by fusing thermal infrared and visible imagery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gyaourova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="727" to="742" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Periocular region-based person identification in the visible, infrared and hyperspectral imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uzair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="854" to="867" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Fusing the information in visible light and nearinfrared images for iris recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shamsafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seyedarabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aghagolzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="881" to="899" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Finite asymmetric generalized gaussian mixture models learning for infrared object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elguebaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouguila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1659" to="1671" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Pedestrian detection at day/night time with visible and fir cameras: A comparison</title>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Socarras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">820</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Fusion of thermal infrared and visible spectra for robust moving object detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fendri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Boukhriss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hammami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="907" to="926" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Int 3-horus framework for multispectrum activity interpretation in intelligent environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández-Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrano-Cuerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Sokolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="6715" to="6727" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Smart environment architecture for robust people detection by infrared and visible video fusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández-Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serrano-Cuerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martínez-Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Humanized Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="237" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Enhanced target tracking through infrared-visible image fusion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Schnelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Fusion</title>
		<meeting>the International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<monogr>
		<title level="m" type="main">Visual and thermal image fusion for uav based target tracking, in: MATLAB-A Ubiquitous Tool for the Practical Engineer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kavitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>InTech</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">A visual analytics approach for deterioration risk analysis of ancient frescoes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="542" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Navigation and surveillance using night vision and image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Industrial Electronics and Applications</title>
		<meeting>the IEEE Symposium on Industrial Electronics and Applications</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">A novel image fusion framework for night-vision navigation and surveillance, Signal</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<monogr>
		<title level="m" type="main">Multi sensor image fusion for surveillance applications using hybrid image fusion algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paramanandham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajendiran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Multimedia Tools and Applications</publisher>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">A survey of infrared and visual image fusion methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="478" to="501" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Natural colour mapping for multiband nightvision imagery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of different image fusion schemes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Franken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Displays</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="37" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Deep colorization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Multicontourlet-based adaptive fusion of infrared and visible remote sensing images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="553" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">An improved infrared/visible fusion for astronomical images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zaidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Astronomy</title>
		<imprint>
			<biblScope unit="page">203872</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Synergetic classification of long-wave infrared hyperspectral and visible images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3546" to="3557" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Developing a spectral-based strategy for urban object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammadzadeh ; A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1808" to="1816" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>P T from airborne hyperspectral tir and visible data</note>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Direct fusion of geostationary meteorological satellite visible and infrared images based on thermal physical properties</title>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wulie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="703" to="714" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Polarimetric dehazing method for visibility improvement based on visible and infrared image fusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="8221" to="8226" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">Color and thermal image fusion for augmented reality in rescue robotics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zalud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kocmanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Burian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jilek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Robotic, Vision, Signal Processing &amp; Power Applications</title>
		<meeting>the International Conference on Robotic, Vision, Signal Processing &amp; Power Applications</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Comparison and fusion of four nondestructive sensors for predicting apple fruit firmness and soluble solids content</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Postharvest Biology and Technology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="89" to="98" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">Intraoperative assessment of critical biliary structures with visible range/infrared image fusion</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gorbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Gilfillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Elster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Surgeons</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1227" to="1231" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Real-time single fpga-based multimodal image fusion system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartyś</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Putz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zbrzezny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Imaging Systems and Techniques</title>
		<meeting>the IEEE International Conference on Imaging Systems and Techniques</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="460" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Image fusion for uninhabited airborne vehicles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Jasiunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kearney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Wigley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Field-Programmable Technology</title>
		<meeting>the IEEE International Conference on Field-Programmable Technology</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="348" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Fpga implementation of decomposition methods for real-time image fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing and Communications Challenges</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="163" to="170" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">An fpga-based hardware implementation of configurable pixel-level color image fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Besiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tsagaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fragoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theoharatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Accelerating multi-scale image fusion algorithms using cuda</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Soft Computing and Pattern Recognition</title>
		<meeting>the International Conference of Soft Computing and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Fusion of infrared and visible images based on nonsubsampled contourlet transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Engineering and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="198" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">Multi-modal cardiac image fusion and visualization on the gpu</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Asen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bogaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amundsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Torp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Ultrasonics Symposium</title>
		<meeting>the IEEE International Ultrasonics Symposium</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="254" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">The high-pass filtering fusion based on gpu</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang ; C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T International Symposium on Computer Science and Society</title>
		<imprint>
			<biblScope unit="page" from="122" to="125" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">A new image fusion performance metric based on visual information fidelity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="135" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">A new automated quality assessment algorithm for image fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1421" to="1432" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">A non-reference image fusion metric based on mutual information of image features</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B A</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aghagolzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seyedarabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="744" to="756" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Image fusion and image quality assessment of fused images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bhosale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="484" to="508" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<monogr>
		<title level="m" type="main">Concepts of image fusion in remote sensing applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pradham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Younan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Academic Press</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform, Signal</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1125" to="1143" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">A comparative analysis of visual and thermal face image fusion based on different wavelet family</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Innovations in Electronics, Signal Processing and Communication</title>
		<meeting>the International Conference on Innovations in Electronics, Signal Processing and Communication</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">A local correlation and directive contrast based image fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision and Image Processing</title>
		<meeting>the International Conference on Computer Vision and Image Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="419" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">A comparison of criterion functions for fusion of multi-focus noisy images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aslantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3231" to="3242" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Implementation and comparative study of image fusion algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Soman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">A new metric based on extended spatial frequency and its application to dwt based fusion algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Essock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Haun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="192" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Subjective tests for image fusion evaluation and objective metric validation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="216" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">A novel architecture for wavelet based image fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vekkot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shukla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Academy of Science, Engineering and Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="372" to="377" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Wavelet-based image fusion and quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="251" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Assessment of image fusion procedures using entropy, image quality, and multispectral classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Aardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">23522</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Information measure for performance of image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="313" to="315" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">A new quality metric for image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heijmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Image quality measures and their performance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Eskicioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2959" to="2965" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Image fusion performance metric based on mutual information and entropy driven quadtree decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Creighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1266" to="1268" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">Comments on &apos;information measure for performance of image fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Creighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1066" to="1067" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main">Weber-aware weighted mutual information evaluation for infrared-visible image fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45004</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">A feature-based metric for the quantitative evaluation of pixel-level image fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganière</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Objective image fusion performance characterisation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xydeas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1866" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">A new image quality metric for image fusion: The sum of the correlations of differences</title>
		<author>
			<persName><forename type="first">V</forename><surname>Aslantas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEU-International Journal of Electronics and Communications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1890" to="1896" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">In-fibre bragg grating sensors</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">355</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
