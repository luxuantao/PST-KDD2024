<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prefetching Using Principles of Hippocampal-Neocortical Interaction</title>
				<funder ref="#_3K4pneC #_tzg9dTR #_pjGemjh #_YPTGfDU #_CupvVFN">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ketaki</forename><surname>Joshi</surname></persName>
							<email>ketaki.joshi@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Sheinberg</surname></persName>
							<email>andrew.sheinberg@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Anurag</forename><surname>Khandelwal</surname></persName>
							<email>anurag.khandelwal@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Raghavendra</forename><forename type="middle">Pradyumna</forename><surname>Pothukuchi</surname></persName>
							<email>raghav.pothukuchi@yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
							<email>abhishek@cs.yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
							<email>gcox@nvidia.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prefetching Using Principles of Hippocampal-Neocortical Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3593856.3595901</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prefetching</term>
					<term>Memory Organization</term>
					<term>Brain-Inspired Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Memory prefetching improves performance across many systems layers. However, achieving high prefetch accuracy with low overhead is challenging, as memory hierarchies and application memory access patterns become more complicated. Furthermore, a prefetcher's ability to adapt to new access patterns as they emerge is becoming more crucial than ever. Recent work has demonstrated the use of deep learning techniques to improve prefetching accuracy, albeit with impractical compute and storage overheads. This paper suggests taking inspiration from the learning mechanisms and memory architecture of the human brain-specifically, the hippocampus and neocortex-to build resource-efficient, accurate, and adaptable prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>? Computer systems organization ? Architectures; Neural networks; Heterogeneous (hybrid) systems; ? Computing methodologies ? Bio-inspired approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Memory prefetching is a performance optimization used widely across several hardware and software layers of modern computer systems. Prefetching proactively brings data from slower levels of memory to faster ones, anticipating its future use. Although well-studied, prefetching continues to be explored, especially as emerging memory hierarchies embrace heterogeneity <ref type="bibr" target="#b21">[22]</ref>, disaggregation <ref type="bibr" target="#b26">[27]</ref>, vertical/horizontal tiering <ref type="bibr" target="#b30">[31]</ref>, and compute in memory <ref type="bibr" target="#b47">[48]</ref>.</p><p>Early prefetchers targeted patterns that were easy to capture, such as strides, and were sufficient for well-understood applications, such as those in SPEC <ref type="bibr" target="#b3">[4]</ref>. However, systems and applications today are far more complex and dynamic, rendering simple approaches ineffective. There is a growing interest in developing prefetchers that can adapt to the dynamic execution by learning memory access patterns instead of detecting pre-programmed rules <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Recent studies have begun exploring the viability of deep learning (DL) for prefetching <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>. In theory, DL should improve prefetching because it is inherently datadriven, and should naturally adapt to applications and their environments. Indeed, these studies show that, in idealized simulations, DL outperforms non-learning prefetch methods in accuracy. However, all of these approaches have three main shortcomings that impede their real-world adoption.</p><p>First, the deep neural networks (DNNs) in prior work use impractically high compute and storage resources, even as much as entire applications ( ?2.1). A more efficient and lightweight learning structure is crucial for real-world systems.</p><p>Second, prior approaches train their models offline, without learning continuously from the system execution. This is partly necessary due to the overhead of DNN training. However, such models can then fail to adapt to evolving application phases, configurations, inputs, and other system dynamics. Offline training also requires labeled datasets, which can be impractical to collect at scale.</p><p>Third, even if we were to optimize resource usage, online DNNs still face the challenge of catastrophic interference. This issue is well known in machine learning (ML) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>, and describes the tendency of DNNs to drastically forget previously learned information when learning new information.</p><p>We consider the fundamental challenge of accurately learning memory access patterns online without interference. We argue that we can develop a robust solution by incorporating principles of learning found in the human brain. We note that the problem of learning access patterns is similar to a task that the human brain encounters continuously, which is to discover generalizable structure from its experiences.</p><p>In particular, we take inspiration from the cognitive theory of Complementary Learning Systems (CLS) <ref type="bibr" target="#b31">[32]</ref>, which models the human brain as an online learning system. CLS theory posits that the brain avoids catastrophic interference using interleaved replay, a process where it interleaves the learning of new and old information. Combining this insight with bio-inspired Hebbian networks, which use far less resources than DNNs, helps us build efficient, online prefetchers.</p><p>Many principles from CLS have already seen use in the ML community <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>. Their objectives and constraints, however, differ from ours. In addition to maximizing model accuracy, we must further account for metrics like training and inference latency, prefetch timeliness, and usage of memory/network bandwidth. The combination of these constraints motivates a more domain-specific solution.</p><p>As we discuss how CLS principles can improve upon DL approaches, we also present expected implementation challenges with two real contexts: disaggregated systems <ref type="bibr" target="#b26">[27]</ref> and CPU-GPU systems <ref type="bibr" target="#b2">[3]</ref>. We specifically choose these because they experience high cross-node communication latency and operate in relatively resource-constrained settings. Thus, they stand to greatly benefit from intelligent memory prefetching. They also provide the opportunity to implement CLS principles in software, offering a faster path to immediate impact in production systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DL for Prefetching and Limitations</head><p>Prior DL techniques for cache or memory prefetching used transformers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b44">45]</ref> or Long Short-Term Memories (LSTMs) <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b39">40]</ref>. These DNNs use an application's recent cache/memory accesses and instructions to predict its next accesses, and have been shown to achieve high accuracy. However, they have unreasonable resource overheads and are only trained offline, thus remaining only simulated ideals.</p><p>Certain works have explored more lightweight learning algorithms for systems, such as reinforcement learning <ref type="bibr" target="#b9">[11]</ref>, gradient boosting <ref type="bibr" target="#b40">[41]</ref>, and small DNNs <ref type="bibr" target="#b23">[24]</ref>. Unfortunately, these approaches too, were only evaluated in simulation, and we find in our testing that the lightweight models fail to match the prefetching accuracy of larger DNNs. Our goal is to develop online, accurate and resource-efficient prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overheads of DL-based Prefetching</head><p>DL-based prefetching incurs untenable compute and storage overheads. A state-of-the-art LSTM-based cache prefetcher <ref type="bibr" target="#b39">[40]</ref> requires over 1 GB of storage using 32-bit parameters. This exceeds the memory capacity of some nodes in our target systems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>. We model this DNN for memory-page prefetching in a disaggregated system <ref type="bibr" target="#b26">[27]</ref>, and aggressively compress it to nearly 1 MB by reducing its input-embedding dimension, and the number of output classes. We compile this model on an Intel i7-8700 CPU, and measure its performance. We evaluate CPU performance because, for the inference times we target, which are around 1-10 ?s <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>, accelerator offloading is not clearly beneficial. Figure <ref type="figure" target="#fig_0">1</ref> shows the prefetcher's deployment.     Figure <ref type="figure" target="#fig_3">2</ref> shows the LSTM latencies with one and two threads, and with parameter quantization (e.g., FP32 to INT8) during inference <ref type="bibr" target="#b28">[29]</ref>. The LSTM takes &gt;150 ?s per inference and &gt;1 ms per example for training, which are orders of magnitude higher than our target. Multi-threading is ineffective because LSTMs have poor parallelism <ref type="bibr" target="#b41">[42]</ref>. Even after quantization, LSTM inference still takes &gt;60 ?s. Pseudorandom accesses and adjacent data. E.g. transform on a list/tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Difficulty of Online Prefetch Learning</head><p>The scale of modern systems and dynamism of their applications makes it impractical to collect representative datasets with which DNN prefetchers can be trained offline. Instead, it is ideal if the model can adapt to changing memory access behavior by learning online. Unfortunately, learning online makes the DNN vulnerable to catastrophic interference <ref type="bibr" target="#b31">[32]</ref>. Catastrophic interference occurs when a DNN trained on one pattern begins learning a different, unrelated pattern. The weight updates made when learning the new pattern overwrite the values that were critical in learning the older pattern, causing the DNN to completely forget the older one. Such interference is avoided during offline training by mixing training data and learning over it in multiple passes. This is not the case when learning online.</p><p>We use the LSTM from ?2.1 to illustrate catastrophic interference with online prefetch learning. We first train the LSTM on a single memory access pattern (e.g., a constant stride) until it achieves perfect accuracy, simulating learning over a single application phase. Next, we present the LSTM with another access pattern (e.g., pointer chasing) to learn. We monitor the LSTM's confidence on the current and previous patterns, which is the probability it assigns to the correct prediction. We select different pairs of patterns from those in Table <ref type="table" target="#tab_0">1</ref>, adapted from prior work <ref type="bibr">[10]</ref>. For a pair of patterns, we generate a trace of 1000 accesses with each pattern. We use these data structure-level prefetching patterns to avoid confounding effects possible in page-level prefetching.</p><p>Figures <ref type="figure" target="#fig_4">3a-3c</ref> show our results. As the LSTM learns the current task, its confidence on the older task drops abruptly, demonstrating catastrophic interference. In some cases, the confidence on the first pattern recovers partially, indicating some knowledge transfer. Nonetheless, the final confidence is poor. Such a prefetcher will be ineffective when patterns repeat, which is the case with many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hippocampal-Neocortical Inspired Prefetching</head><p>Figure <ref type="figure" target="#fig_5">4</ref> shows the brain's learning architecture in CLS theory. Learning occurs through a complementary relationship between two regions of the brain: the neocortex and the hippocampus. The neocortex, similar to DNNs, slowly learns the structure underlying the information it encounters; i.e., the rules behind a memory access pattern. Meanwhile, the hippocampus quickly memorizes the information it encounters -i.e., the exact memory accesses -in a compressed format, likely by separating each access and storing them in an associative memory <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Over time, this information is decompressed and replayed in the neocortex, interleaving the learning of old and new tasks, thus mitigating interference <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>. Furthermore, CLS theory models the networks in the brain using biologically-inspired Hebbian networks, which have much lower resource needs compared to standard DNNs. We show how each of these ideas help overcome the limitations of standard DNNs for prefetching. </p><note type="other">Structure Learner (Neocortex) Pattern Completion</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overcoming Resource Overheads</head><p>Brain-inspired prefetch networks are resource efficient for two reasons. The first reason is that they use a Hebbian update rule, which is much simpler than that of standard DNN learning. When learning with Hebbian networks, a network weight ? ? ? -connecting an input neuron ? ? and output neuron ? ? -is increased if both neurons are non-zero, and decreased if the input neuron (? ? ) is inactive while the output (? ? ) is active. The simplified update rule <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b35">36]</ref> is:</p><formula xml:id="formula_0">?? ? ? = (? ? ? 0) [(? ? ? 0) -(? ? == 0)]<label>(1)</label></formula><p>This update scheme requires far fewer operations than conventional training of DNNs.</p><p>The second advantage of brain-inspired networks comes from their use of sparsity. These networks are sparse in their connectivity, meaning a node connects to only 1-25% of the nodes in adjacent layers unlike all-to-all connections in DNNs. Additionally, they are sparse in their representations, in that only 1-25% of the network's hidden layer neurons are activated on an input. As a result, the storage and compute needs of these networks are a fraction of those for DNNs.</p><p>We prototype a sparse Hebbian neural network for prefetching. The network has a single hidden layer of 1000 neurons, with 12.5% connectivity between layers, and 10% sparsity in the hidden layer. Like an LSTM, our network also uses a recurrent state to capture sequence memory.</p><p>Table <ref type="table" target="#tab_1">2</ref> compares the resource needs of our Hebbian network with that of the LSTM from ?2.2. We list the lower bound for the number of operations (Ops) in the LSTM, as its exact value varies with the implementation of transcendental functions (e.g., sigmoid and tanh). The Hebbian network is nearly 3? smaller than the LSTM with nearly an order of magnitude fewer Ops. Hence, the Hebbian training and inference times, shown in Figure <ref type="figure" target="#fig_3">2</ref>, are proportionately lower. We also compare the networks' prefetching accuracy with multiple applications that have various memory access patterns: TensorFlow <ref type="bibr" target="#b4">[6]</ref> training ResNet-50 <ref type="bibr" target="#b17">[19]</ref>, PageRank <ref type="bibr" target="#b33">[34]</ref> using GraphChi <ref type="bibr" target="#b25">[26]</ref>, mcf <ref type="bibr" target="#b3">[4]</ref>, and graph500 <ref type="bibr" target="#b0">[1]</ref>. For each application, we collect a trace of 2 billion memory accesses and simulate them with a memory sized at 50% of the trace's footprint. We deploy both prefetchers as shown in Figure <ref type="figure" target="#fig_0">1</ref>, with a miss history length of 1 input to the networks, and measure the percentage of misses removed compared to a baseline without prefetching. Figure <ref type="figure" target="#fig_6">5</ref> shows the results. Our network has comparable accuracy to the LSTM, even while consuming far fewer resources, showing the effectiveness of Hebbian learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overcoming Catastrophic Interference</head><p>In CLS theory, catastrophic interference is mitigated by replaying past memories stored in the hippocampus. Building a full hippocampal-like storage is an open problem requiring consideration on selectively storing and sampling accesses into it. Therefore, in this work, we will focus on showing the benefits of replay for online prefetch learning without resource limitations on the hippocampal storage.</p><p>We perform the experiments in ?2.2 that showed catastrophic interference again, but with replay. We implement replay by retraining the network on the first pattern using a 0.1? smaller learning rate after each training/inference of the second. Figures <ref type="figure" target="#fig_4">3d-3f</ref> shows the new results. While the models without replay experienced catastrophic interference, performing replay lets the network learn the new pattern without forgetting the old one. Even if the old pattern were to repeat, the network would maintain prediction accuracy without needing to re-learn it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Target Systems for Online Prefetching</head><p>We target two environments for an initial deployment of our brain-inspired prefetcher: disaggregated systems <ref type="bibr" target="#b26">[27]</ref> and CPU-GPU unified virtual memory (UVM) systems <ref type="bibr" target="#b2">[3]</ref>. Both systems, shown in Figure <ref type="figure" target="#fig_8">6</ref>, have data movement that incurs high communication latencies <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b26">27]</ref>, and stand to benefit from intelligent prefetching. However, the systems differ in a few critical ways that lead to requiring different types of prefetch strategies. One difference is the impact of a page miss. In disaggregated systems, CPU cores fault  only on one page at a time, indicating that the prefetcher should be optimized to hide latency. In GPUs, the SIMT (single instruction multiple thread) execution can produce many concurrent faults, and the lockstep execution model means that a single fault can stall many threads. This suggests that a throughput-optimized prefetcher would be more appropriate for this system.</p><p>Our target systems also differ in the location from which the prefetchers can obtain information about memory accesses, and where the compute and memory resources to run prefetching are available. These parameters determine where the prefetcher should be placed. In the UVM system, software-level information on memory accesses is only available in the CPU-located driver. Therefore, all prefetching decisions must ultimately be made from this centralized location. This contrasts with the disaggregated system, where scarce resources on the switch necessitate a decentralized approach with a separate prefetcher per node.</p><p>The different placement of the prefetcher in each system, in turn, results in unique design spaces. For example, the prefetcher in the UVM system can take advantage of its global view to make better-informed decisions, but may require more processing to ensure that it can isolate the individual access patterns in the combined access streams. However, such interleaving of access streams may naturally offer more resistance to catastrophic interference, reducing replay costs.</p><p>In contrast, the prefetcher in the disaggregated system is less likely to see interleaved access streams since it has a separate prefetcher per CPU. Therefore, the prefetcher network could be smaller to learn the access patterns. With the decentralized architecture, it is easier to integrate applicationspecific, profile-guided prefetch optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Research Challenges</head><p>Our initials results with prefetchers based on the hippocampalneocortical interaction are promising. Harnessing the full benefits of such a prefetcher requires solving important challenges to replicate the CLS architecture in Figure <ref type="figure" target="#fig_5">4</ref> in computer systems. We present these issues, beyond the target system-specific ones discussed in ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Instances</head><p>Training on every prefetch inference, as done in our experiments ( ?3.1), can be unnecessary and resource-consuming, especially because training is more expensive than inference. Possible alternatives are to train on a batch of samples at once, and/or to only train on certain misses. Training only on some misses reduces overall training costs, but requires care. Simple approaches, such as randomly deciding which samples to train on, may miss cases that are critical for the model to understand the application. A more intelligent sampling process could use confidence measures from the model to filter less-information carrying samples, or to avoid training on well-learned cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prefetch Output and Miss History</head><p>There are two main parameters for the prefetcher's output: length, which is the number steps predicted into the future; and width, which is the number of predictions made at each step. The ideal values for these parameters are determined by the system architecture and application behavior. Consider our design from ?3.1, where the network is trained only to predict the next miss. If the time between misses is less than the inference latency, even a perfect model will always prefetch too late. In that case, a more effective method is to predict a sequence of misses further into the future.</p><p>Regarding prefetch width, throughput-bound environments like the UVM system might benefit more from predicting multiple prefetches at a time, even if they have slightly less accuracy. The same could be said for read-heavy workloads. On the other hand, systems where the network is the bottleneck require a prefetcher that is highly selective and confident about bringing in data to minimize communication.</p><p>In order to learn how to predict with a given length and width, a prefetcher must maintain a miss history. For example, when prefetching multiple steps into the future, a window of past misses is required to construct appropriate training examples. Thus, the prefetch length determines a minimum history size. Beyond this, the ideal history size depends on the reuse distances in the access patterns of the application. If the current pattern has short reuse distances, then only a few entries in the miss history are necessary, while it is the opposite for patterns with longer reuse distances. Thus, configuring the prefetch length, width, and the access history will require intelligent co-design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Encoding Data for Prefetching</head><p>Most prior work on ML-based prefetching encodes addresses and strides as one-hot vectors, which are then indexed into an embedding table to obtain a dense representation that is input to the prefetcher <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>. This aligns with approaches used for words in natural language processing, but storing embeddings can become expensive (e.g., &gt;500 MB <ref type="bibr" target="#b39">[40]</ref>). It also inflates compute costs, as the output layer grows linearly with the number of embedding vectors.</p><p>A more fundamental issue is that addresses and strides can be a poor proxy for understanding the inherent behavior of an application. We found that, as of now, neither the LSTM nor the Hebbian network perform well on caching applications like memcached <ref type="bibr" target="#b15">[17]</ref> and cachebench <ref type="bibr" target="#b10">[12]</ref>. This is because these applications are almost entirely pointerbased, and the access patterns are difficult to learn from addresses or strides.</p><p>Ideally, the representation of the input data should more closely resemble how addresses "flow" at the data structure level (e.g. tree nodes, pointer chains). We have found that insights from the cognitive theories can provide inspiration here as well. There is evidence that the hippocampus encodes locations optimizing for vector-based navigation <ref type="bibr" target="#b32">[33]</ref>. A similar encoding for addresses could better represent paths through data structures. Other brain-inspired work has explored ways of representing symbols that allow the efficient detection of relations in neural networks <ref type="bibr" target="#b6">[8]</ref>. An analog of such an approach for prefetching would be an address embedding optimized for detecting pointers that are logically (as opposed to numerically) close to one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementing Replay</head><p>In our experiments to demonstrate the utility of replay, we assumed that we could store all past examples, and interleave them later. A full implementation must trade the storage/compute costs of replay with its benefits for learning. One simple approach is to use a fixed-size buffer. This, however, could lose important information as entries are evicted. A more principled approach could save space by filtering less important examples, perhaps using confidence as a measure, or freeing entries that have already been consolidated due to replay, thus not needed further learning <ref type="bibr" target="#b31">[32]</ref>. Yet another alternative is to average similar examples, producing single representative cases.</p><p>Another challenge in incorporating replay is to define application phases so that they can be replayed. However, phase characteristics can vary significantly between applications, making it difficult to manage replay with a single parameter setting (buffer size, time limits, miss counts, etc.). This could motivate an interface for application developers to directly tune replay parameters, or to indirectly indicate phase behavior and timings. Another approach, also inspired by cognitive theories, is to identify contexts or phases using clustering of abstract representations learned by the network <ref type="bibr" target="#b12">[14]</ref>.</p><p>Finally, our preliminary studies also show only one form of replay. Cognitive literature describes many forms of replay <ref type="bibr" target="#b45">[46]</ref>, each with their own benefits and challenges. Some methods such as hindsight or simulation replay, where the prefetcher artificially generates memory sequences and learns them, can be helpful in avoiding replay storage costs altogether. Such sequences could be generated by rules that are determined with profile-guided techniques, or through generative networks i.e., trading off compute for memory. Another alternative could generate hidden layer values, complete the forward pass, and train on the output to reinforce existing behavior. We leave such ideas for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Availability</head><p>Since training actively changes the weights of a neural network, it may be important to block inference during training. This is an availability issue, which motivates a protocol where training is applied to a separate model copy, which is later redeployed when the live model's confidence/accuracy decreases. However, it is possible that simpler approaches could also suffice for two reasons. One, because prefetching is not correctness-critical, inference requests are safe to drop. Second, neural networks can have noise-robust or noise-smoothing effects, meaning that small perturbations to weights do not cause large changes in the network's output, especially since our training method explicitly seeks to preserve the network's prior performance <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b34">35]</ref>. This could allow inference to remain accurate even when concurrent with training. We expect this property to require further study, as it could significantly optimize a real prefetcher deployment, but may be sensitive to the many details of practical implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Recent deep learning approaches to prefetching have shown promising results, but only in idealized simulations. Their real implementation is impeded due to resource overheads and learning limitations. This paper explores how one might address these issues using principles and models of the hippocampus and neocortex, as well as some challenges we should expect in implementing them. We ultimately expect more challenges to appear in designing and deploying such models, but we hope this paper serves as a starting point for implementing efficient and intelligent learning algorithms for all relevant systems contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A DNN memory prefetcher. The gray area and dashed lines indicate steps during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Inference time for various number of future predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Training time for various batch sizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Inference and training latency of a state-ofthe-art LSTM prefetcher on an Intel i7-8700 CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Catastrophic interference (a-c) and the effect of replay (d-f) during online prefetch learning. Confidence on the older pattern is shown in red, while the current one is in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Memory architecture of the brain in CLS theory. Each block is modeled with a Hebbian neural network. Solid lines show information storage paths and dashed lines show recall and replay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Online memory prefetching performance of Hebbian and LSTM networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architectures of our target systems. Black lines indicate page miss notifications and prefetch requests. Red lines indicate data movement paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Memory Access Patterns</figDesc><table><row><cell>Pattern</cell><cell>Code</cell><cell>Behavior</cell></row><row><cell>Stride</cell><cell>a[i]</cell><cell>Accessing data at regular delta such as streaming</cell></row><row><cell></cell><cell></cell><cell>patterns or array traversal.</cell></row><row><cell>Pointer chase</cell><cell>*ptr</cell><cell>Pseudorandom accesses to different parts of the</cell></row><row><cell></cell><cell></cell><cell>memory. E.g. linked list traversal</cell></row><row><cell>Indirect stride</cell><cell>*(a[i])</cell><cell>Accessing data at regular delta from a base address.</cell></row><row><cell></cell><cell></cell><cell>E.g. array of object pointers.</cell></row><row><cell>Indirect index</cell><cell>b[a[i]]</cell><cell>Accessing data at indices that are at regular delta</cell></row><row><cell></cell><cell></cell><cell>from a constant base address.</cell></row><row><cell>Pointer offset</cell><cell>*ptr</cell><cell></cell></row><row><cell></cell><cell>*(ptr+i)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Resource Needs of Hebbian vs LSTM networks</figDesc><table><row><cell>Model</cell><cell cols="3">Parameters #Ops (inference) #Ops (training)</cell></row><row><cell>LSTM</cell><cell>170 k</cell><cell>&gt; 170k FP</cell><cell>&gt; 400k FP</cell></row><row><cell cols="2">Hebbian 49 k</cell><cell>14k INT</cell><cell>64k INT</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This work is licensed under a Creative Commons Attribution International 4.0 License.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We thank <rs type="person">Jonathan D. Cohen</rs> for his insights on complementary learning systems. We also thank <rs type="person">Seung-seob Lee</rs> for his help with generating the traces used in this work. This work was made possible in part via funding from the <rs type="funder">National Science Foundation</rs> awards <rs type="grantNumber">2118851</rs>, <rs type="grantNumber">2040682</rs>, <rs type="grantNumber">2112562</rs>, <rs type="grantNumber">2047220</rs> and a <rs type="grantName">Computing Innovation Fellowship for Raghavendra Pradyumna Pothukuchi</rs> (under <rs type="funder">NSF</rs> grant <rs type="grantNumber">2127309</rs> to the <rs type="institution">Computing Research Association</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3K4pneC">
					<idno type="grant-number">2118851</idno>
				</org>
				<org type="funding" xml:id="_tzg9dTR">
					<idno type="grant-number">2040682</idno>
				</org>
				<org type="funding" xml:id="_pjGemjh">
					<idno type="grant-number">2112562</idno>
				</org>
				<org type="funding" xml:id="_YPTGfDU">
					<idno type="grant-number">2047220</idno>
					<orgName type="grant-name">Computing Innovation Fellowship for Raghavendra Pradyumna Pothukuchi</orgName>
				</org>
				<org type="funding" xml:id="_CupvVFN">
					<idno type="grant-number">2127309</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Graph 500 | large-scale benchmarks</title>
		<ptr target="https://graph500.org/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to Cache Allocation</title>
		<ptr target="https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-to-cache-allocation-technology.html" />
	</analytic>
	<monogr>
		<title level="m">Technology in the Intel? Xeon? Processor E5 v4 Family</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/" />
	</analytic>
	<monogr>
		<title level="j">Nvidia Unified Memory</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://www.spec.org/benchmarks.html" />
		<title level="m">SPEC Benchmarks and Tools</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS49936.2021.00023</idno>
		<ptr target="https://doi.org/10.1109/IPDPS49936.2021.00023" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Awni</forename><surname>Altabaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.00195</idno>
		<title level="m">Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Workload Analysis of a Large-Scale Key-Value Store</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
		<idno type="DOI">10.1145/2254756.2254766</idno>
		<ptr target="https://doi.org/10.1145/2254756.2254766" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems<address><addrLine>London, England, UK; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
	<note>SIGMETRICS &apos;12)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifying Memory Access Patterns for Prefetching</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378498</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Lausanne, Switzerland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="513" to="526" />
		</imprint>
	</monogr>
	<note>ASPLOS &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taha</forename><surname>Shahroodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3466752.3480114</idno>
		<ptr target="https://doi.org/10.1145/3466752.3480114" />
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO &apos;21)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1121" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The CacheLib Caching Engine: Design and Experiences at Scale</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Grosof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sathya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Uhlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Carrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi20/presentation/berg" />
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="753" to="768" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BINGO: braininspired learning memory</title>
		<author>
			<persName><forename type="first">Prabuddha</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swarup</forename><surname>Bhunia</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-021-06484-8</idno>
		<ptr target="https://doi.org/10.1007/s00521-021-06484-8" />
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2022-02">2022. 02 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cognitive control over learning: creating, clustering, and generalizing task-set structure</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Anne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">190</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive Hebbian learning with random feedback weights</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Detorakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Bartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Neftci</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2019.01.008</idno>
		<ptr target="https://doi.org/10.1016/j.neunet.2019.01.008" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting Fundamentals of Experience Replay</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML&apos;20)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed caching with memcached</title>
		<author>
			<persName><forename type="first">Brad</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux Journal</title>
		<imprint>
			<date type="published" when="2004-08">2004. August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Memory Access Patterns</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1919" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">1997. nov 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco_a_01181</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="m">Sparse Associative Memory</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="998" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<idno type="DOI">10.1162/neco_a_01181</idno>
		<ptr target="https://direct.mit.edu/neco/article-pdf/31/5/998/1052590/neco_a_01181.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HeteroOS: OS Design for Heterogeneous Memory Management in Datacenter</title>
		<author>
			<persName><forename type="first">Sudarsun</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Schwan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3140659.3080245</idno>
		<ptr target="https://doi.org/10.1145/3140659.3080245" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="521" to="534" />
			<date type="published" when="2017-06">2017. jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measuring catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Abitino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Case for Learned Index Structures</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neoklis</forename><surname>Polyzotis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3183713.3196909</idno>
		<ptr target="https://doi.org/10.1145/3183713.3196909" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Management of Data</title>
		<meeting>the 2018 International Conference on Management of Data<address><addrLine>Houston, TX, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
	<note>) (SIGMOD &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated</title>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2016.05.004</idno>
		<ptr target="https://doi.org/10.1016/j.tics.2016.05.004" />
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="512" to="524" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GraphChi: Large-Scale Graph Computation on Just a PC</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 10th USENIX Conference on Operating Systems Design and Implementation<address><addrLine>Hollywood, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>OSDI&apos;12). USENIX Association, USA</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MIND: In-Network Memory Management for Disaggregated Data Centers</title>
		<author>
			<persName><forename type="first">Seung-Seob</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477132.3483561</idno>
		<ptr target="https://doi.org/10.1145/3477132.3483561" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (Virtual Event, Germany) (SOSP &apos;21)</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles (Virtual Event, Germany) (SOSP &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="488" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training Quantized Nets: A Deeper Understanding</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2017/file/1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>c303b0eed3133200cf715285011b4e4-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning based data prefetching in CPU-GPU unified virtual memory</title>
		<author>
			<persName><forename type="first">Xinjian</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpdc.2022.12.004</idno>
		<ptr target="https://doi.org/10.1016/j.jpdc.2022.12.004" />
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2023-04">2023. Apr 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Dhanotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallab</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakash</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName><surname>Chauhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02878</idno>
		<title level="m">Tpp: Transparent page placement for cxl-enabled tiered memory</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</title>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">L</forename><surname>James L Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randall C O'</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName><surname>Reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">419</biblScope>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deciphering The Hippocampal Polyglot: the Hippocampus as a Path Integration System</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gerrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gothard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Knierim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kudrimoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Skaggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<idno type="DOI">10.1242/jeb.199.1.173</idno>
		<ptr target="https://journals.biologists.com/jeb/article-pdf/199/1/173/2532105/jexbio_199_1_173.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Biology</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="185" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The storage and recall of memories in the hippocampo-cortical system</title>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Rolls</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00441-017-2744-3</idno>
		<ptr target="https://doi.org/10.1007/s00441-017-2744-3" />
	</analytic>
	<monogr>
		<title level="j">Cell and Tissue Research</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">09</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pattern separation, completion, and categorisation in the hippocampus and neocortex</title>
		<author>
			<persName><forename type="first">Edmund</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.nlm.2015.07.008</idno>
		<ptr target="https://doi.org/10.1016/j.nlm.2015.07.008" />
	</analytic>
	<monogr>
		<title level="j">Neurobiology of Learning and Memory</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="4" to="28" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Pattern Separation and Pattern Completion in the Hippocampal System</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation</title>
		<author>
			<persName><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Trippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3503222.3507777</idno>
		<ptr target="https://doi.org/10.1145/3503222.3507777" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<title level="s">ASPLOS &apos;22</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Lausanne, Switzerland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="344" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shahrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">??igo</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gohar</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Batum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Laureano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colby</forename><surname>Tresness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC&apos;20)</title>
		<meeting>the 2020 USENIX Conference on Usenix Annual Technical Conference (USENIX ATC&apos;20)</meeting>
		<imprint>
			<publisher>USENIX Association, USA</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Article 14, 14 pages</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Legoos: A disseminated, distributed {OS} for hardware resource disaggregation</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="69" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Hierarchical Neural Model of Data Prefetching</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3445814.3446752</idno>
		<ptr target="https://doi.org/10.1145/3445814.3446752" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (Virtual, USA) (ASPLOS &apos;21)</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (Virtual, USA) (ASPLOS &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="861" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Relaxed Belady for Content Distribution Network Caching</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wyatt</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Usenix Conference on Networked Systems Design and Implementation</title>
		<title level="s">NSDI&apos;20</title>
		<meeting>the 17th Usenix Conference on Networked Systems Design and Implementation<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association, USA</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="529" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parallel Multi-Dimensional LSTM, with Application to Fast Biomedical Volumetric Image Segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Marijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonmin</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<title level="s">NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, Canada; Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2998" to="3006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Perceptron learning for reuse prediction</title>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2016.7783705</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2016.7783705" />
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Thomann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Genssler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussam</forename><surname>Amrouch</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.04789</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2202.04789" />
		<title level="m">HW/SW Co-design for Reliable In-memory Brain-inspired Hyperdimensional Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Replay in minds and machines</title>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Wittkuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Hall-Mcmaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">W</forename><surname>Schuck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2021.08.002</idno>
		<ptr target="https://doi.org/10.1016/j.neubiorev.2021.08.002" />
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="367" to="388" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Large-Scale Analysis of Hundreds of In-Memory Key-Value Cache Clusters at Twitter</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Rashmi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3468521</idno>
		<ptr target="https://doi.org/10.1145/3468521" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2021-08">2021. aug 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">RRAM for compute-in-memory: From inference to training</title>
		<author>
			<persName><forename type="first">Shimeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonbo</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="2753" to="2765" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xuejiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2012.13529</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2012.13529" />
	</analytic>
	<monogr>
		<title level="j">Brain-inspired Search Engine Assistant based on Knowledge Graph</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
