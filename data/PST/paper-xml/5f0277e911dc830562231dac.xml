<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiongnan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
							<email>dou@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
							<email>xiaojiew94@gmail.com</email>
							<affiliation key="aff4">
								<orgName type="department">School of Computing and Information System</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Computing and Information System</orgName>
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jirong.wen@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<settlement>MOE</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3397271.3401084</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>search result diversification, generative adversarial network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search result diversification aims to retrieve diverse results to cover as many subtopics related to the query as possible. Recent studies showed that supervised diversification models are able to outperform the heuristic approaches, by automatically learning a diversification function other than using manually designed score functions. The main challenge of training a diversification model is the lack of high-quality training samples. Due to the involvement of dependence between documents in the ranker, it is very hard for training algorithms to select effective positive and negative ranking lists to train a reliable ranking model, given a large number of candidate documents within which different documents are relevant to different subtopics. To tackle this problem, we propose a supervised diversification framework based on Generative Adversarial Network (GAN). It consists of a generator and a discriminator interacting with each other in a minimax game. Specifically, the generator generates more confusing negative samples for the discriminator, and the discriminator sends back complementary ranking signals to the generator. Furthermore, we explicitly exploit subtopics in the generator, whereas focusing on modeling document similarity in the discriminator. Through such a minimax game, we are able to obtain better ranking models by combining ranking signals learned by the generator and the discriminator. Experimental results on the TREC Web Track dataset show that the proposed method can significantly outperform existing diversification methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In search engines, the queries issued by users may have different meanings. For example, when users issue "java", the meaning of the query could be the programming language or the Java Island in Indonesia. The retrieved documents should cover both topics to satisfy users' information need as much as possible. Thus, the goal of search result diversification is to generate a ranked list of documents that cover different user intents underlying an ambiguous or a broad query. In recent years, many search result diversification approaches have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>.</p><p>Most previous methods of diversification can be described as the following procedure. At each iteration, the document with the highest score graded by the diversification score function is selected and added to the end of the existing document ranking. The task of diversification is to design the diversification score function that combines both relevance to the query and novelty of the documents using information of the query and the selected documents. According to the score function, approaches of diversification can be divided into explicit approaches and implicit approaches. The implicit approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> emphasize the novelty of the documents, which infers that the selected document should be different from the previously selected documents. The diversification score function of implicit approaches can be handcrafted rules such as MMR <ref type="bibr" target="#b0">[1]</ref> or a supervised measure such as R-LTR <ref type="bibr" target="#b25">[26]</ref>, PAMM <ref type="bibr" target="#b22">[23]</ref>, and NTN <ref type="bibr" target="#b23">[24]</ref>. The explicit approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> stress the relevance between the documents and the subtopics of the query, which infers that the selected document should cover the subtopics which the previously selected documents do not cover. The diversification score function in explicit approaches is usually performed by subtopic distribution measures and document-subtopic relevance measures. Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and PM2 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> and supervised approaches such as DSSA <ref type="bibr" target="#b11">[12]</ref>.</p><p>Studies have shown that supervised approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26</ref>] are able to outperform the heuristic approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref> by learning an optimized ranking function. However, the large number of candidate documents with only few subtopic-relevant documents in it may cause the problem that the space of training samples is too large (for example the quantity of ways of sampling 20 documents from 50 document is about 10 14 ) and the sampled data may only contain few documents relevant to the subtopic. This may cause the model hard to train. So the main challenge in training a supervised diversification model is how to sample enough high-quality training data that contain an appropriate number of relevant documents from the candidate document set. Some methods such as R-LTR <ref type="bibr" target="#b25">[26]</ref> only use the top documents in the ideal rankings while other methods such as PAMM <ref type="bibr" target="#b22">[23]</ref> sample the training rankings by judging it through diversification evaluation metrics. However, none of the existing approaches solve this problem completely. The quality of training data used by R-LTR <ref type="bibr" target="#b25">[26]</ref> is high but its quantity is too small to train the model which may lead to underfit. The quantity of the training dataset used by PAMM <ref type="bibr" target="#b22">[23]</ref> is large enough but the quality of it depends on some hyper-parameters such as the range of ğ›¼-nDCG <ref type="bibr" target="#b3">[4]</ref> of negative ranking samples, which may cause the model hard to tune. How to generate training samples effectively is still a challenge for training diversification models.</p><p>To tackle this problem, inspired by IRGAN <ref type="bibr" target="#b18">[19]</ref>, we introduce Generative Adversarial Network (GAN) <ref type="bibr" target="#b9">[10]</ref> into search result diversification. Generator generates negative training samples instead of using handcrafted rules for discriminator to train and discriminator provides reward for generator for better sampling. This positive feedback mechanism may improve sampling performance. Furthermore, there are two main components in GAN, so it is natural to use two different approaches in generator and discriminator. This feature of GAN provides a simple way to combine explicit document-subtopic relevance features and implicit documentdocument similarity features to improve the performance of search result diversification. Specifically, we use explicit approaches in generator and implicit approaches in discriminator in this paper, and it is easy to switch to different settings in real applications. We call this framework DVGAN(search result DiVersification using Generative Adversarial Network). In our framework, we also propose two training methods, namely document selection method DVGAN-doc and ranking selection method DVGAN-rank. Different from the traditional GAN, generator needs input data (for example, the selected document ranking) to generate the negative samples. In our framework, the input data sampling is a part of the data sampling problem. To better generate the input data for better sampling training data, in these two methods, we added a new component sampler to the GAN framework and proposed several sampling algorithms to reduce the high dimension of the sampling space. Experimental results on TREC Web Track data show our methods outperform the existing methods significantly.</p><p>The main contribution of this paper is threefold: (1) To the best of our knowledge, this is the first method adapting generative adversarial network to search result diversification;(2) We combine explicit features and implicit features in GAN to improve diversification quality;(3) We proposed several sampling algorithms considering both quality and quantity of the training data to solve the problem of training data sampling.</p><p>The rest of the paper is organized as follows. We introduce related works in Section 2. Following this we introduce the GAN framework for search result diversification in Section 3. In Section 4, we elaborate each component of our proposed model. We describe experimental settings in Section 5 and analyze experimental results in Section 6. We conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Search Result Diversification</head><p>As search result diversification is an effective way to solve the problem of query ambiguity, many models have been proposed to solve this problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Depending on whether the subtopics of query are explicitly modeled and the form of score function, existing diversification approaches can be categorized into implicit and explicit approaches. We will introduce these two approaches respectively in the following part.</p><p>Implicit Diversification Approaches. The implicit approaches emphasize the document's relevance to the query and novelty to the selected documents. In the early years' research on diversification, implicit methods are most unsupervised. MMR <ref type="bibr" target="#b0">[1]</ref> can be regarded as the foundation of implicit methods. Its diversification score function is as follows.</p><formula xml:id="formula_0">ğ‘“ (ğ‘‘ |ğ‘, ğ‘†) = (1 âˆ’ ğœ†)ğ‘† rel (ğ‘‘, ğ‘) + ğœ†Î› ğ‘‘ ğ‘— âˆˆğ‘† ğ‘† div (ğ‘‘, ğ‘‘ ğ‘— ).<label>(1)</label></formula><p>The ğ‘† rel function reflects the ğ‘‘'s relevance to the query ğ‘ and the ğ‘† div function reflects the ğ‘‘'s novelty to the list of documents ğ‘† that are already ranked before the current document ğ‘‘. The Î› function is to aggregate the novelty between document ğ‘‘ and ğ‘†. Most implicit methods replace the ğ‘† rel and ğ‘† div with more complex function and design loss function to use the machine learning method to improve the performance. The relational learning-to-rank (R-LTR) <ref type="bibr" target="#b25">[26]</ref> replaces the ğ‘† div score by using the relationship matrix between document ğ‘‘ and selected documents ğ‘†. And the loss function is inspired by the learning to rank which is aiming to maximize the probability of optimal rankings. Based on the same score function of R-LTR, Xia et al. proposed PAMM <ref type="bibr" target="#b22">[23]</ref> in which loss function is designed to directly maximize the score margin of positive and negative rankings. Furthermore, Neural Tensor Network (NTN) <ref type="bibr" target="#b23">[24]</ref> was introduced into search result diversification to measure document similarity automatically. In our framework, we use the score function of the R-LTR in discriminator.</p><p>Explicit Diversification Approaches. Different from implicit approaches which mainly model document novelty based on similarities between documents, explicit approaches regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Most explicit approaches focus on the subtopic coverage of results, by calculating subtopic distribution based on ranked documents. The score function for explicit methods can be described as the following form:</p><formula xml:id="formula_1">ğ‘“ (ğ‘‘ |ğ‘, ğ‘†) = (1 âˆ’ ğœ†)ğ‘† rel (ğ‘‘, ğ‘) + ğœ† ğ‘– âˆˆğ¼ ğ‘ ğ´(ğ‘– |ğ‘†) * ğ‘† sub (ğ‘‘, ğ‘–), (2)</formula><p>where ğ‘– denotes one subtopic of the query subtopics, ğ¼ ğ‘ denotes all the subtopics of query ğ‘. ğ´(ğ‘– |ğ‘†) denotes the distribution of the subtopic given the selected documents rankings ğ‘† which contains the document-subtopic information of previous documents. The ğ‘† rel function reflects the ğ‘‘'s relevance to the query ğ‘ and the ğ‘† sub function reflects the ğ‘‘'s relevance to the subtopics. xQuAD <ref type="bibr" target="#b17">[18]</ref> is one of the representative methods of unsupervised explicit approaches. It defines the subtopic distribution by calculating the probability of the selected documents not covering the subtopics. PM2 <ref type="bibr" target="#b5">[6]</ref> is another unsupervised explicit method calculating the distribution by counting the relevant document of the subtopic. DSSA <ref type="bibr" target="#b11">[12]</ref> introduces the machine learning method into explicit approaches. It calculates the distribution using the RNN and attention mechanism <ref type="bibr" target="#b15">[16]</ref>. In our framework, we mainly use the score function of the DSSA in our generator.</p><p>Discussion. As described above, existing implicit and explicit approaches considered different information for learning diversification functions and had different goals. The implicit approaches mainly use the dissimilarity between documents and emphasize the novelty of the documents, whereas the explicit approaches exploit subtopics and stress the coverage. So it is intuitive that using both kinds of features may lead to potential performance improvement compared to the sole use of explicit or implicit approaches. In this paper, we will have a preliminary study on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Adversarial Network</head><p>Generative Adversarial Network(GAN) <ref type="bibr" target="#b9">[10]</ref> is initially used in the area of computer vision to generate pictures that are similar to realistic. There are two models in the Generative Adversarial Network, which are called generator and discriminator. These two models are trained in an adversarial minimax game. This process aims at erasing the unnecessary noise in the contiguous dataset, which is a semi-supervised model. In recent years, after overcoming the problem of passing gradients from the discriminator to the generator, GAN has just been introduced into a discrete area. For example, SeqGAN <ref type="bibr" target="#b12">[13]</ref> introduces the GAN to the text sequence generation area combined with Monte Carlo search. It is also used in the traditional information retrieval area, Wang proposed IR-GAN <ref type="bibr" target="#b18">[19]</ref> which consists of two information retrieval models in it. Comparing to other information retrieval models, IRGAN's generator can provide negative training samples with higher quality. In the personalized search area, Lu proposed PSGAN <ref type="bibr" target="#b14">[15]</ref> inspired by IRGAN. Our framework is inspired by the former two models. However, we combine the idea of minimax game with the method in diversification search. In this paper we will discuss how to apply GAN in our framework to generate more relative and well-covered document ranking answering the query. We also apply GAN to combine explicit and implicit approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DVGAN-A GAN FRAMEWORK FOR SEARCH RESULT DIVERSIFICATION 3.1 Problem Formulation and Framework</head><p>As we introduced in Section 1, we want to combine both explicit approach and implicit approach to improve the search result diversification performance via generative adversarial network. We will use explicit approach's score function in generator because its form is close to the diversification evaluation metrics. We use implicit approach's score function in discriminator as its form is strong to distinguish the positive samples and negative samples which are closed by directly comparing documents. Through the minimax game training process, we expect that discriminator can provide rewards including the information reflecting implicit features for generator for optimizing the calculation of subtopic distribution in it, and generator can generate high-quality negative samples to discriminator to produce more useful feedback. In this way, the </p><formula xml:id="formula_2">= {ğ‘‘ ğ‘™ 1 , â€¢ â€¢ â€¢ , ğ‘‘ ğ‘™ ğ‘– , â€¢ â€¢ â€¢ , ğ‘‘ ğ‘™ |ğ‘™ | } (i.e.</formula><p>, ğœ‰ is ğ‘‘ or ğ‘™). Î“ could be selected document ranking ğ‘† or candidate document set ğ¶ (i.e., Î“ is ğ‘† or ğ¶). In DVGAN-doc method, generator generates negative document set ğ· â€² given the query ğ‘ and selected document ranking ğ‘†. In DVGAN-rank method, generator generates complete negative document ranking set ğ¿ â€² given the query ğ‘ and candidate document set ğ¶. In both methods, there are three components in the adversarial framework: a generator, a discriminator, and a sampler.</p><p>Suppose ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†) and ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†) are the diversification score functions of a document ğ‘‘ in generator and discriminator. In DVGANdoc method, the score function of ğœ‰ (particular ğ‘‘) is the same as the diversification score function ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†). In DVGAN-rank method, we calculate the score function of ğœ‰ (particular ğ‘™), i.e., D ğœƒ (ğ‘™ |ğ‘, ğ¶) and G ğœ™ (ğ‘™ |ğ‘, ğ¶), using Plackett-Luce model <ref type="bibr" target="#b6">[7]</ref>. Specifically, we have:</p><formula xml:id="formula_3">DVGAN-doc D ğœ™ (ğ‘‘ |ğ‘, ğ‘†) = ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†) G ğœƒ (ğ‘‘ |ğ‘, ğ‘†) = ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†), DVGAN-rank ï£± ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£´ ï£³ D ğœ™ (ğ‘™ |ğ‘, ğ¶) = |ğ‘™ | ğ‘–=1 ğ‘“ ğœ™ (ğ‘‘ ğ‘™ ğ‘– |ğ‘, {ğ‘‘ ğ‘™ 1 , ..., ğ‘‘ ğ‘™ ğ‘–âˆ’1 }) |ğ‘™ | ğ‘—=ğ‘– ğ‘“ ğœ™ (ğ‘‘ ğ‘™ ğ‘— |ğ‘, {ğ‘‘ ğ‘™ 1 , ..., ğ‘‘ ğ‘™ ğ‘–âˆ’1 }) G ğœƒ (ğ‘™ |ğ‘, ğ¶) = |ğ‘™ | ğ‘–=1 ğ‘“ ğœƒ (ğ‘‘ ğ‘™ ğ‘– |ğ‘, {ğ‘‘ ğ‘™ 1 , ..., ğ‘‘ ğ‘™ ğ‘–âˆ’1 }) |ğ‘™ | ğ‘—=ğ‘– ğ‘“ ğœƒ (ğ‘‘ ğ‘™ ğ‘— |ğ‘, {ğ‘‘ ğ‘™ 1 , ..., ğ‘‘ ğ‘™ ğ‘–âˆ’1 })<label>(3)</label></formula><p>Suppose ğ‘ true (ğœ‰ |ğ‘, Î“) is the true distribution of samples which generator tries to fit and ğ‘ ğœƒ (ğœ‰ |ğ‘, Î“) is the distribution of generated samples. ğ‘ true (ğœ‰ |ğ‘, Î“) = ğ‘ true (ğ‘‘ |ğ‘, ğ‘†) is the true distribution of documents in DVGAN-doc (i.e., Î“ = ğ‘†), and ğ‘ true (ğœ‰ |ğ‘, Î“) = ğ‘ true (ğ‘™ |ğ‘, ğ¶) is the true distribution of rankings in DVGAN-rank (i.e., Î“ = ğ¶). The form of ğ‘ ğœƒ (ğœ‰ |ğ‘, Î“) for both methods is the same, and it is calculated by the softmax function:</p><formula xml:id="formula_4">ğ‘ ğœƒ (ğœ‰ |ğ‘, Î“) = exp(G ğœƒ (ğœ‰ |ğ‘, Î“)) ğœ‰ exp(G ğœƒ (ğœ‰ |ğ‘, Î“)) = exp(G ğœƒ (ğœ‰ |ğ‘, Î“)) ğœ‰ âˆˆÎ â€² exp(G ğœƒ (ğœ‰ |ğ‘, Î“)) + exp(G ğœƒ (ğœ‰ true |ğ‘, Î“)) ,<label>(4)</label></formula><p>where ğœ‰ true denotes the positive sample in discriminator. In different methods, we can simply replace the ğœ‰,Î â€² by the document ğ‘‘, document set ğ· â€² or ranking ğ‘™, ranking set ğ¿ â€² . The discriminator aims to learn a score distribution D ğœ™ (ğœ‰ |ğ‘, Î“), which is to distinguish the samples which satisfy the demand of diversification (positive sample) from the generated samples (negative samples). The documents or document rankings with the high metric score are treated as positive samples and what the generator generates are treated as negative samples. According to the Eq. ( <ref type="formula" target="#formula_3">3</ref>), we can infer that two forms of D ğœ™ are both related to ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†). So the problem is simplified to learn the ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†) diversification score function. In our method, ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†) is implemented as the implicit diversification score function in order to better distinguish positive and negative samples as the implicit approaches directly model the dissimilarity between documents.</p><p>The generator aims to learn a distribution ğ‘ ğœƒ (ğœ‰ |ğ‘, Î“) to fit the real distribution ğ‘ true (ğœ‰ |ğ‘, Î“) through the score function G ğœƒ (ğœ‰ |ğ‘, Î“). The generator also generates negative samples according to the ğ‘ ğœƒ to confuse the discriminator. According to the Eq. ( <ref type="formula" target="#formula_3">3</ref>), we can infer that two forms of G ğœ™ are both related to ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†). So the problem is simplified to learn the ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†) diversification score function. In our method, ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†) is implemented as the explicit diversification score function in order to generate more confusing samples as the explicit approaches directly model the coverage of subtopics.</p><p>The sampler aims to generate the input data Î“ to generator. In DVGAN-doc, it is a list of documents which are already ranked, i.e., ğ‘†. In DVGAN-rank, it is a set of candidate documents ğ¶. The generator will generate a corresponding object ğœ‰ (a document for DVGAN-doc or a ranking list for DVGAN-rank) based on Î“, hence the sampler is also a critical component that helps reduce the sampling space.</p><p>The minimax game in DVGAN framework can be described as follows: given a query ğ‘, its subtopics, and the input data Î“ given by the sampler, the generator tries to generate the best samples set Î â€² (set of documents ğ· â€² in DVGAN-doc and set of ranking lists ğ¿ â€² in DVGAN-rank) that satisfies the diversification demand with high relevance to the query and coverage of the subtopics. The discriminator tries to distinguish the true document or rankings ğœ‰ true from the negative samples ğœ‰ ğœƒ generated by the generator. Formally, given query set ğ‘„ and document, we have:</p><formula xml:id="formula_5">ğ½ G * ,D * = min ğœƒ max ğœ™ â„‘(ğ‘ true , ğ‘ ğœƒ ),<label>(5)</label></formula><p>We will introduce the specific form of â„‘ in DVGAN-doc and DVGAN-rank respectively in the following part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DVGAN-doc: Document Selection Method</head><p>DVGAN-doc is a natural extension of IRGAN considering diversification features. Generator tries to select documents that resemble the positive documents from the candidate document set to fool the discriminator, whereas discriminator tries to distinguish the positive and negative documents. The â„‘ in Eq. ( <ref type="formula" target="#formula_5">5</ref>) in DVGAN-doc is as follows:</p><formula xml:id="formula_6">â„‘ (ğ‘ true , ğ‘ ğœƒ ) = ğ‘ âˆˆğ‘„,ğ‘† âˆˆS ğ‘ E ğ‘‘âˆ¼ğ‘ true (ğ‘‘ |ğ‘,ğ‘†) log ğ· ğœ™ (ğ‘‘ |ğ‘, ğ‘†) + E ğ‘‘âˆ¼ğ‘ ğœƒ (ğ‘‘ |ğ‘,ğ‘†) log 1 âˆ’ ğ· ğœ™ (ğ‘‘ |ğ‘, ğ‘†) ,<label>(6)</label></formula><p>where generator ğº is written as ğ‘ ğœƒ (ğ‘‘ |ğ‘, ğ‘†). Eq. ( <ref type="formula" target="#formula_4">4</ref>) and the discriminator ğ· is the estimated probability calculated by:</p><formula xml:id="formula_7">ğ· ğœ™ (ğ‘‘ |ğ‘, ğ‘†) = ğœ ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†) = exp(ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†)) 1 + exp(ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†)) .<label>(7)</label></formula><p>Please note that different from IRGAN <ref type="bibr" target="#b18">[19]</ref>, DVGAN-doc has an additional component ğ‘† to represent the former selected documents.</p><p>As ğ‘† contain order information, it is required to be ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Optimizing Discriminator.</head><p>According to the Eq. ( <ref type="formula" target="#formula_6">6</ref>) optimizing the discriminator is to optimize ğœ™ to maximize the whole result given the true documents and generated documents, i.e.,</p><formula xml:id="formula_8">ğœ™ * = arg max ğœ™ ğ‘ âˆˆğ‘„,ğ‘† âˆˆğ‘† ğ‘ (E ğ‘‘âˆ¼ğ‘ true (ğ‘‘ |ğ‘,ğ‘†) log ğ· ğœ™ (ğ‘‘ |ğ‘, ğ‘†)+ E ğ‘‘âˆ¼ğ‘ ğœƒ (ğ‘‘ |ğ‘,ğ‘†) log(1 âˆ’ ğ· ğœ™ (ğ‘‘ |ğ‘, ğ‘†))).<label>(8)</label></formula><p>3.2.2 Optimizing Generator. As GAN is put into practice in the contiguous area firstly, it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN <ref type="bibr" target="#b18">[19]</ref>, we generate negative document set ğ· â€² by selecting the documents from the candidate document set with the highest scores. Formally, the gradient of the generator is:</p><formula xml:id="formula_9">âˆ‡ ğœƒ ğ½ G (ğ‘, ğ‘†) â‰ƒ 1 |ğ· â€² | ğ‘‘ âˆˆğ· â€² âˆ‡ ğœƒ log ğ‘ ğœƒ (ğ‘‘ |ğ‘, ğ‘†) log(1 + exp(ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†))).</formula><p>(9) Note that the feedback from the discriminator log(1+exp(ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†))) can be regarded as the reward to the generator according to the reinforcement learning which contains the implicit information calculated in discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Sampler.</head><p>In the DVGAN-doc method, the sampler sample a list of selected documents ğ‘† which is sent to the generator. As we mentioned in Section 1, training dataset sampling is the main challenge in search result diversification. In this method, we design two methods for sampling. Firstly, it is necessary to sample the ideal ranking but it is not enough. Recall that our method is not an ideal method, so it may produce some documents which may not be as good as the ideal ones. Hence we also sample in the second way: we randomly select some documents from the document set to form the selected document ranking ğ‘†. However, recall that in the DVGAN-doc method, we require that ğ‘† is ranked. So the sampler also needs to re-rank ğ‘† by the diversification metric like ğ›¼-nDCG <ref type="bibr" target="#b3">[4]</ref>. In practice, half of the selected document ranking ğ‘† is sampled from the ideal ranking, and the other half of the ğ‘† is sampled in the second random way. In both ways, the positive document is the best next document maximizing ğ›¼-nDCG given ğ‘†.The sampling method is described in Algorithm 1.</p><p>Algorithm 1 Sampling algorithm used by DVGAN-doc Sampler S ğ‘ â† âˆ… //the first method, sampling from ideal ranking 5:</p><p>ğ‘™ â† len(ğ‘Ÿ ğ‘ ) 6:</p><p>for ğ‘– = 1 to ğ‘™ do end for 18: end for 19: return S ğ‘</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DVGAN-rank: Ranking Selection Method</head><p>In the former DVGAN-doc method, the difference between the positive samples and negative samples is only one document which may not be enough for discriminator to learn. Thus, we put forward the DVGAN-rank method to differ the positive and negative samples at ranking level.</p><p>The â„‘ in the Eq. ( <ref type="formula" target="#formula_5">5</ref>) in DVGAN-rank method is:</p><formula xml:id="formula_10">â„‘ (ğ‘ true , ğ‘ ğœƒ ) = ğ‘ âˆˆğ‘„,ğ¶ âˆˆ C ğ‘ E ğ‘™ + âˆ¼ğ‘ true (ğ‘™ |ğ‘,ğ¶),ğ‘™ âˆ’ âˆ¼ğ‘ ğœƒ (ğ‘™ |ğ‘,ğ¶) [D ğœ™ (ğ‘™ + |ğ‘, ğ¶) âˆ’ D ğœ™ (ğ‘™ âˆ’ |ğ‘, ğ¶)) â‰¤ ğ¸ (ğ‘™ + |ğ‘, ğ¶) âˆ’ ğ¸ (ğ‘™ âˆ’ |ğ‘, ğ¶)] ,<label>(10)</label></formula><p>where the generator G is written as ğ‘ ğœƒ (ğ‘™ |ğ‘, ğ¶) and D ğœ™ is the diversification score for a whole document ranking in discriminator calculated by Eq. ( <ref type="formula" target="#formula_4">4</ref>) using Plackett-Luce model and the ğ¸ is the diversification metrics such as ğ›¼-NDCG and ERR-IA <ref type="bibr" target="#b1">[2]</ref>. The form of â„‘ is inspired by PAMM <ref type="bibr" target="#b22">[23]</ref> method which aims to maximize the margin between positive and negative rankings instead of directly judging the rankings respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Optimizing Discriminator.</head><p>According to the Eq. ( <ref type="formula" target="#formula_10">10</ref>), optimizing the discriminator is to optimize ğœ™ to maximize the whole result given the true rankings and generated rankings, i.e.,</p><formula xml:id="formula_11">ğœ™ * = arg max ğœ™ ğ‘ âˆˆğ‘„,ğ¶ âˆˆ C ğ‘ E ğ‘™ + âˆ¼ğ‘ true (ğ‘™ + |ğ‘,ğ¶),ğ‘™ âˆ’ âˆ¼ğ‘ ğœƒ (ğ‘™ âˆ’ |ğ‘,ğ¶) [D ğœ™ (ğ‘™ + |ğ‘, ğ¶) âˆ’ D ğœ™ (ğ‘™ âˆ’ |ğ‘, ğ¶)) â‰¤ ğ¸ (ğ‘™ + |ğ‘, ğ¶) âˆ’ ğ¸ (ğ‘™ âˆ’ |ğ‘, ğ¶)] .<label>(11)</label></formula><p>The loss function of the discriminator is inspired by PAMM <ref type="bibr" target="#b22">[23]</ref> method which is aiming to maximize the margin between the positive and negative rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Optimizing</head><p>Generator. Similar to the DVGAN-doc method, the gradient of generator is also calculated by sampling technique. We select the rankings with the highest scores to form the negative ranking set ğ¿ â€² . Formally, the gradient is:</p><formula xml:id="formula_12">âˆ‡ ğœƒ ğ½ G (ğ‘, ğ¶) â‰ƒ 1 |ğ¿ â€² | ğ‘™ âˆˆğ¿ â€² âˆ‡ ğœƒ log ğ‘ ğœƒ (ğ‘™ |ğ‘, ğ¶) log 1 + exp(D ğœ™ (ğ‘™ |ğ‘, ğ¶)) . (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>The feedback from the discriminator log(1 + exp(ğ‘‘ ğœ™ (ğ‘™ |ğ‘, ğ¶))) can be described as the reward to the generator according to the reinforcement learning. And the generator is written as ğ‘ ğœƒ (ğ‘™ |ğ‘, ğ¶) and is calculated by Eq. ( <ref type="formula" target="#formula_3">3</ref>). As G ğœƒ , D ğœ™ is a multiply function, the score may be extremely small, so we use normalization and clipping technique in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Sampler.</head><p>Similar to the sampler in the DVGAN-doc method, the sampler in DVGAN-rank method also needs to generate the candidate document set ğ¶. The procedure is simple: we randomly select documents from the document set and then put it together to form the candidate document set ğ¶ for generator to rank it. But there is another problem with our DVGAN-rank method. From the Eq. ( <ref type="formula" target="#formula_12">12</ref>), we notice that the generator needs to generate several rankings ğ‘™. However, the way diversification method generates a ranking is as follows:</p><p>1. Initially we make the selected ranking ğ‘† as an empty set Ã˜. 2. Select the document ğ‘‘ with the highest score given ğ‘† and ğ‘.</p><p>3. Add the document ğ‘‘ to the end ğ‘† and back to step 2 or stop and output ğ‘† as the search result if the length of ğ‘† is enough.</p><p>In this process, generator can generate only one ranking. In optimizing generator as Eq. ( <ref type="formula" target="#formula_12">12</ref>), it needs to generate several ğ‘™ to form the negative rankings set ğ¿ â€² to calculate the gradient, which is difficult to implement in programming. To solve this problem, we make the sampler do the work of generating negative ranking list as we mentioned before.</p><p>DVGAN-rank sampler receives the selected document set ğ¶ from the sampler and re-ranks it by diversification metrics to get the positive list ğ‘™ + . To generate the negative samples ğ‘™ âˆ’ , we shuffle the positive list by swapping the former and latter document randomly in a hyper-parameter ğ‘˜ times. Due to the fact that the quality of the positive rankings ğ‘™ + and negative rankings ğ‘™ âˆ’ has great effect on the performance as it does in the PAMM method, the performance of DVGAN-rank highly depends on how the selected document set ğ¶ is sampled and how the negative rankings are shuffled. Therefore, the training dataset sampling still remains a problem in DVGANrank which we will try to solve in future work.</p><p>The sampling method is described as Algorithm 2. Since we introduced the DVGAN-rank sampler, we can review the former DVGAN-rank method. It is easy to find that in fact the discriminator is just the PAMM method and what the generator does is to score the document rankings DVGAN-rank sampler sampled. Thus it also makes a great challenge for DVGAN-rank sampler to sample the proper document rankings which can imitate the generator. As a result, DVGAN-rank method is hard to tune.</p><p>So far we have introduced two different DVGAN methods. In the next section, we will introduce specific forms of the two diversification score function ğ‘“ ğœ™ (ğ‘‘ |ğ‘, ğ‘†) and ğ‘“ ğœƒ (ğ‘‘ |ğ‘, ğ‘†). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIVERSIFICATION USING DVGAN</head><p>In this section, we instantiate DVGAN-doc and DVGAN-rank to a concrete form and articulate the training algorithms. The main idea of DVGAN is to introduce GAN into diversification to improve the quality of the training dataset. Furthermore, to use more information, we use explicit approach's score function in generator and implicit approach's score function in discriminator. We will introduce the diversification score functions in discriminator and generator in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Discriminator</head><p>As we mentioned in Section 3, the discriminator tries to distinguish the positive document or document rankings from the negative ones. Thus, the discriminator needs a score function to calculate score for document ğ‘‘ given the query ğ‘ and selected document ranking ğ‘†. In our method, we adapt the R-LTR <ref type="bibr" target="#b25">[26]</ref> score function for discriminator. We introduce the score function ğ‘“ ğœ™ (ğ‘‘ ğ‘– |ğ‘, ğ‘†) in the form of Eq. ( <ref type="formula" target="#formula_0">1</ref>):</p><formula xml:id="formula_14">ğ‘“ ğœ™ (ğ‘‘ ğ‘– |ğ‘, ğ‘†) = ğ‘† rel (ğ‘‘ ğ‘– , ğ‘) + Î› ğ‘‘ ğ‘— âˆˆğ‘† ğ‘† div (ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ), ğ‘† rel (ğ‘‘ ğ‘– , ğ‘) = ğ‘¤ ğ‘‡ ğ‘Ÿ (ğ‘‘)ğ‘¥ ğ‘‘ ğ‘– ,ğ‘ , ğ‘† div (ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ) = ğ‘… ğ‘– ğ‘— , Î› = ğ‘¤ ğ‘‡ ğ‘‘ (ğ‘‘)â„(ğ‘… ğ‘– , ğ‘†),<label>(13)</label></formula><p>where ğ‘¥ ğ‘‘ ğ‘– ,ğ‘ denotes the relevance feature vector of the document ğ‘‘ ğ‘– and query ğ‘. ğ‘… ğ‘– denotes the relationship matrix between the document ğ‘‘ ğ‘– and document ğ‘‘ ğ‘— in the selected document list ğ‘†. ğ‘… ğ‘– ğ‘— denotes the relationship vector between ğ‘‘ ğ‘– and ğ‘‘ ğ‘— . â„ denotes the relational function. ğ‘¤ ğ‘Ÿ (ğ‘‘) and ğ‘¤ ğ‘‘ (ğ‘‘) denotes the parameters in discriminator. The vector ğ‘… ğ‘– ğ‘— usually captures different diversity features which store the implicit information. Here we notice that in discriminator, the model judges the document's diversity by calculating its diversity features with the selected ones and these features are helpful for distinguishing positive and negative samples. The relational function â„(ğ‘… ğ‘– , ğ‘†) is to aggregate the diversity features between the current document ğ‘‘ ğ‘– and selected documents, which is usually defined in three ways: max, min, and average. In our method, the max way gets the best performance, i.e., â„(ğ‘… ğ‘– , ğ‘†) = (max</p><formula xml:id="formula_15">ğ‘‘ ğ‘— âˆˆğ‘† ğ‘… ğ‘– ğ‘—1 , ..., max ğ‘‘ ğ‘— âˆˆğ‘† ğ‘… ğ‘– ğ‘—ğ‘˜ ).</formula><p>As relevance features ğ‘¥ ğ‘‘ ğ‘– ,ğ‘ are necessary in information retrieval to model the relevance between document and query, the definition of the relationship vector ğ‘… ğ‘–,ğ‘— is crucial to the performance for the R-LTR. Imagine when we compare two documents, we usually compare their titles, texts , etc., which means that we capture their features using several different components. In our method, we use four different diversity features to construct the relationship vector as we show in Table <ref type="table" target="#tab_3">2</ref>.</p><p>Subtopic Diversity: Here the subtopic is different from the subtopic in explicit method, we construct the subtopic information from the documents instead of the queries. We use SVD to capture the implicit subtopics of the documents and euclidean distance based on it to calculate the dissimilarity between two documents. We define the subtopic diversity feature as follows:</p><formula xml:id="formula_16">ğ‘… ğ‘– ğ‘—1 = ğ‘š ğ‘˜=1 (ğ‘ (ğ‘§ ğ‘˜ |ğ‘‘ ğ‘– ) âˆ’ ğ‘ (ğ‘§ ğ‘˜ |ğ‘‘ ğ‘— )) 2 .</formula><p>Text Diversity: The dissimilarity of text is also useful for diversification. Here we use the traditional ğ‘¡ ğ‘“ * ğ‘–ğ‘‘ ğ‘“ vector to calculate the cosine distance to represent text diversity, i.e.,</p><formula xml:id="formula_17">ğ‘… ğ‘– ğ‘—2 = 1 âˆ’ ğ‘¡ ğ‘– â€¢ ğ‘¡ ğ‘— ||ğ‘¡ ğ‘– || â€¢ ||ğ‘¡ ğ‘— || ,</formula><p>where ğ‘¡ ğ‘– , ğ‘¡ ğ‘— denotes the weighted document vectors based on the traditional TF-IDF model. Title Diversity Title is the precise and brief abstract of the document which contains lots of information. The way of computing title diversity is similar to the text diversity.</p><p>Anchor Diversity Anchor can precisely describe the content of a document. The way of computing title diversity is similar to that of the text diversity using the text in anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Generator</head><p>As we mentioned in Section 3, the generator tries to select highquality documents from the available documents based on the query ğ‘ and the selected document ranking ğ‘† or the candidate document ğ¶ to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA <ref type="bibr" target="#b11">[12]</ref> score function for the generator. We introduce the score function in the form of Eq. ( <ref type="formula">2</ref>):</p><formula xml:id="formula_18">ğ‘“ ğœƒ (ğ‘‘ ğ‘¡ |ğ‘, ğ‘†) = (1 âˆ’ ğœ†)ğ‘† rel (ğ‘‘ ğ‘¡ , ğ‘) + ğœ† ğ‘– âˆˆğ¼ ğ‘ ğ´(ğ‘– |ğ‘†) * ğ‘† sub (ğ‘‘ ğ‘¡ , ğ‘–), ğ‘† rel (ğ‘‘ ğ‘¡ , ğ‘) = S(ğ‘’ ğ‘‘ ğ‘¡ , ğ‘’ ğ‘ ) + ğ‘¤ ğ‘‡ ğ‘Ÿ (ğ‘”) * ğ‘¥ ğ‘‘ ğ‘¡ ,ğ‘ , ğ‘† sub (ğ‘‘ ğ‘¡ , ğ‘– ğ‘˜ ) = S(ğ‘’ ğ‘‘ ğ‘¡ , ğ‘’ ğ‘– ğ‘˜ ) + ğ‘¤ ğ‘‡ ğ‘Ÿ (ğ‘”) * ğ‘¥ ğ‘‘ ğ‘¡ ,ğ‘– ğ‘˜ ,<label>(14)</label></formula><p>where ğ‘¥ ğ‘‘ ğ‘¡ ,ğ‘ , ğ‘¥ ğ‘‘ ğ‘¡ ,ğ‘– ğ‘™ denotes the relevance feature vectors between document ğ‘‘ ğ‘¡ and query ğ‘ or subtopic ğ‘– ğ‘˜ . ğ‘’ ğ‘‘ ğ‘¡ , ğ‘’ ğ‘ , and ğ‘’ ğ‘– ğ‘˜ denotes the embedding vectors for document ğ‘‘ ğ‘¡ ,query ğ‘ and subtopic ğ‘– ğ‘˜ .</p><p>ğ‘¤ ğ‘Ÿ (ğ‘”) denotes parameters in generator. ğ‘† rel and ğ‘† sub both use the S function to calculate the similarity between document and query or subtopic based on the embedding vector:</p><formula xml:id="formula_19">S(ğ‘’ ğ‘‘ , ğ‘’ ğ‘ ) = ğ‘’ ğ‘‡ ğ‘‘ * ğ‘¤ ğ‘  (ğ‘”) * ğ‘’ ğ‘ ,<label>(15)</label></formula><p>where ğ‘¤ ğ‘  (ğ‘”) denotes parameters in generator. Noticed that the subtopic distribution is actually the most important component in explicit approach. The part of calculating the relevance between documents and queries or subtopics is easy to understand. As DSSA, we will introduce the distribution of subtopic ğ´(ğ‘– |ğ‘†). DSSA uses both RNN and attention <ref type="bibr" target="#b15">[16]</ref> mechanism to calculate it. Noticed that the selected document ranking ğ‘† contains order information, it is natural to use RNN to encode the previous document information. We denote that the documents in ğ‘† is ğ‘‘ 1 , â€¢ â€¢ â€¢ , ğ‘‘ ğ‘¡ âˆ’1 in convenience of representation. In spite of the kinds of RNN(in our model, we use LSTM <ref type="bibr" target="#b7">[8]</ref>), we use ğ» to denote the RNN cell and â„ ğ‘¡ to denote the hidden state of RNN, which stores the information of previous ğ‘¡ documents. Thus the previous document information at ğ‘¡-th position can be derived from the ğ‘¡ âˆ’1-th position and document embedding vector ğ‘’ ğ‘‘ ğ‘¡ using RNN method, i.e.,</p><formula xml:id="formula_20">â„ ğ‘¡ = ğ» (â„ ğ‘¡ âˆ’1 , ğ‘’ ğ‘‘ ğ‘¡ ).</formula><p>Thus we get the document information â„ ğ‘¡ âˆ’1 given selected document ranking ğ‘†. Similar to the S function, we calculate the similarity between the document information and subtopic:</p><formula xml:id="formula_21">ğ´ â€² (â„ ğ‘¡ âˆ’1 , ğ‘’ ğ‘– ğ‘˜ ) = â„ ğ‘‡ ğ‘¡ âˆ’1 ğ‘¤ ğ‘ (ğ‘”)ğ‘’ ğ‘– ğ‘˜ ,<label>(16)</label></formula><p>where ğ‘¤ ğ‘ (ğ‘”) denotes parameters in generator. In the way above, we mainly use the distributed embedding representation, which may not be effective and accurate, especially under limited data. So we further use relevance feature vector to improve the subtopic distribution calculation. The following equation can be regarded as the max-pooling:</p><formula xml:id="formula_22">ğ´ â€²â€² (ğ‘¥ ğ‘‘ 1 ,ğ‘– ğ‘˜ , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘‘ ğ‘¡ âˆ’1 ,ğ‘– ğ‘˜ ) = max [ğ‘¤ ğ‘‡ ğ‘ (ğ‘”)ğ‘¥ ğ‘‘ 1 ,ğ‘– ğ‘˜ , â€¢ â€¢ â€¢ , ğ‘¤ ğ‘‡ ğ‘ (ğ‘”)ğ‘¥ ğ‘‘ ğ‘¡ âˆ’1 ,ğ‘– ğ‘˜ ] ,</formula><p>where ğ‘¤ ğ‘ (ğ‘”) denotes parameters in generator.</p><p>We directly adapt an addictive way to aggregate the two subtopic distribution and then use softmax function to normalize to get the final distribution:</p><formula xml:id="formula_23">ğ‘ ğ‘– ğ‘— |ğ‘† = ğ´ â€² (â„ ğ‘¡ âˆ’1 , ğ‘’ ğ‘– ğ‘— ) + ğ´ â€²â€² (ğ‘¥ ğ‘‘ 1 ,ğ‘– ğ‘— , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘‘ ğ‘¡ âˆ’1 ,ğ‘– ğ‘˜ ), ğ´(ğ‘– ğ‘— |ğ‘†) = exp(ğ‘ ğ‘– ğ‘— ) ğ¾ ğ‘˜=1 exp(ğ‘ ğ‘– ğ‘˜ )</formula><p>, where K denotes the number of subtopics of query ğ‘.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Vector</head><p>In this part, we will briefly introduce some feature vectors we used. ğ‘’ ğ‘‘ : Embedding vector for document ğ‘‘, which is the distributed representation of document. It can be constructed in different ways, In this paper, we use doc2vec <ref type="bibr" target="#b13">[14]</ref> to get document embeddings.</p><p>ğ‘¥ ğ‘‘,ğ‘ and ğ‘¥ ğ‘‘,ğ‘– : Relevance feature vectors between the document ğ‘‘ and query ğ‘ and subtopic ğ‘–. We adapt some traditional IR features such as ğ‘¡ ğ‘“ * ğ‘–ğ‘‘ ğ‘“ and ğµğ‘€25 to construct the relevance features.</p><p>ğ‘’ ğ‘ and ğ‘’ ğ‘– : Embedding vectors for query ğ‘ and subtopic ğ‘–. It is obvious that the text of query or subtopic is too short to calculate distributed representation in doc2vec method. To solve this problem, we firstly retrieve ğ‘Š documents using the text of subtopic or query by basic retrieval model (such as BM25). Then we concatenated them to form a pseudo document to calculate the corresponding embedding vector via the doc2vec method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>It is easy to use DVGAN to generate the diversified search result as we show in the former section. We use the generator as the model to generate the final document ranking result.</p><p>In the training process, we first train R-LTR <ref type="bibr" target="#b25">[26]</ref> and DSSA <ref type="bibr" target="#b11">[12]</ref> respectively using MLE loss in both ways. It is because our framework needs a warm start to avoid the deviation in the training process. Then we train them by DVGAN-doc or DVGAN-rank respectively to get the corresponding model.</p><p>In implementation of the models, as the list score G ğœƒ (ğ‘™ |ğ‘, ğ¶) and D ğœ™ (ğ‘™ |ğ‘, ğ¶) and contains a successive multiplication as shown in Eq. ( <ref type="formula" target="#formula_3">3</ref>), it may be extremely small and its gradient after a log function may be extremely huge. So we use clipping technique to control the training process of both positive and negative rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Review of Our Models</head><p>Our model attempts to solve the lack of high-quality sampled training data problems in search result diversification by introducing the generative adversarial network. In order to combine explicit and implicit information to improve the performance, the generator and discriminator in the DVGAN framework use the explicit and implicit features respectively. In the training process, the generator can receive implicit information which it cannot obtain from the discriminator via the reward and the discriminator can receive samples in high-quality from the generator. Inspired by IRGAN, we convert the document generation into document selection. The DVGANdoc is a natural extension of IRGAN, which adds the selected document ranking ğ‘† into the score function. The DVGAN-rank method combines loss function of the GAN loss and the PAMM method's loss, which converts maximizing the likelihood estimation into the margin between positive and negative rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETTINGS 5.1 Data Collection</head><p>We experiment with the Web Track dataset <ref type="bibr" target="#b10">[11]</ref> from 2009 to 2012. There are 198 queries (2 queries are dropped because they have no subtopic judgment) in this dataset. There are 3 to 8 subtopics for each query. The relevance rating is given at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. <ref type="bibr" target="#b8">[9]</ref> on their website 1 . we only use the first level subtopics and will adapt the hierarchical structure in future work. The weights of these subtopics are assumed to be uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>Among all the evaluation metrics <ref type="bibr">[2-4, 20, 21]</ref>, we use ERR-IA <ref type="bibr" target="#b1">[2]</ref>, ğ›¼-NDCG <ref type="bibr" target="#b3">[4]</ref>, and NRBP <ref type="bibr" target="#b2">[3]</ref> as our diversity evaluation metrics. They measure the document ranking by calculating the coverage of each subtopic of the query. Consistent with existing work and TREC Web Track, all these metrics are computed on top 20 results of a ranking. We use two-tailed paired t-test to conduct significance testing with p-value &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Models</head><p>We compare DVGAN with several existing diversification methods. We use Lemur as our non-diversified baseline method. We use xQuAD, TxQuAD, HxQuAD <ref type="bibr" target="#b17">[18]</ref>, PM2 <ref type="bibr" target="#b5">[6]</ref>, TPM2 <ref type="bibr" target="#b4">[5]</ref>, and HPM2 <ref type="bibr" target="#b8">[9]</ref> as our unsupervised baseline methods. We use ListMLE <ref type="bibr" target="#b21">[22]</ref>, R-LTR <ref type="bibr" target="#b25">[26]</ref>, PAMM <ref type="bibr" target="#b5">[6]</ref>, R-LTR-NTN, PAMM-NTN <ref type="bibr" target="#b23">[24]</ref>, and DSSA <ref type="bibr" target="#b11">[12]</ref> as supervised baseline methods. Top 20 results of Lemur are used to train the supervised methods. Top 50(ğ‘ ) results of Lemur are used for diversity re-ranking. To generate enough data for the sampler to sample the selected document ranking ğ‘† or candidate document set ğ¶, we use top 100 results returned by Lemur as the sampler input. In order to prove that the combination of explicit and implicit information is effective, we design a simple method using explicit and implicit features called DSSA+R-LTR, which also uses the whole results of Lemur to train. We use 5-fold cross validation to tune the parameters in all experiments based on ğ›¼-nDCG@20 <ref type="bibr" target="#b3">[4]</ref>. A brief introduction to these baselines is as follows.</p><p>Lemur. We use the non-diversified results as our baseline. They are produced by the Indri engine based on the Lemur 2 .</p><p>ListMLE. ListMLE is a learning-to-rank method, which is similar to the R-LTR method without considering diversity.</p><p>xQuAD, TxQuAD, HxQuAD, PM2, TPM2, and HPM2. These methods are the representative unsupervised explicit methods whose diversification score functions are similar to that of the Eq. ( <ref type="formula" target="#formula_0">1</ref>). HxQuAD and HPM2 use the hierarchical structure by adding new parameters. These methods require prior relevance rankings to fulfill the re-ranking. In our experiment, we use ListMLE.</p><p>R-LTR,PAMM, and NTN. These methods are the representative supervised implicit methods. For the diversity feature, we use the same four features in Table <ref type="table" target="#tab_3">2</ref> with two more features: linkbased diversity and URL-based diversity in <ref type="bibr" target="#b25">[26]</ref>, for PAMM, we use ğ›¼-nDCG@20 as the optimization metrics and tune the number of positive rankings ğ‘™ + and negative rankings ğ‘™ âˆ’ per query. We tune the function â„ ğ‘† (ğ‘…) from minimal, maximal, and average for the best performance. The feature vector is the same as DVGAN. We optimize NTN based on both R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN respectively. For these two methods, the number of tensor slices is tuned from 1 to 10.</p><p>DSSA. DSSA is the supervised explicit method. We use LSTM <ref type="bibr" target="#b7">[8]</ref> as the RNN cell for comparison. In our experiments, we conduct the list-pairwise loss <ref type="bibr" target="#b11">[12]</ref> to train DSSA method. The feature vector 1 http://playbigdata.ruc.edu.cn/dou/hdiv/ 2 Lemur service: http://boston.lti.cs.cmu.edu/Services/clueweb09 batch/ is the same as DVGAN. The result of DSSA(pre-train) in Table <ref type="table" target="#tab_5">4</ref> is the result of the DSSA model after the pre-train process. DSSA + R-LTR. This method is a linear combination of explicit and implicit approach. The diversification score function of this method is:</p><formula xml:id="formula_24">ğ‘“ (ğ‘‘ |ğ‘, ğ‘†) = (1 âˆ’ ğœ† âˆ’ ğœ‡)ğ‘† rel (ğ‘‘, ğ‘) + ğœ†Î› ğ‘‘ ğ‘— âˆˆğ‘† ğ‘† div (ğ‘‘ ğ‘– , ğ‘‘) + ğœ‡ ğ‘– âˆˆğ¼ ğ‘ ğ´(ğ‘– |ğ‘†) * ğ‘† sub (ğ‘‘, ğ‘–). (17)</formula><p>The function ğ‘† rel , ğ‘† div , and Î› are the same as R-LTR, ğ‘† sub is the same as DSSA, the subtopic distribution ğ´(ğ‘–) is the same as DSSA.</p><p>DVGAN. We train DVGAN in two methods respectively to get DVGAN-doc and DVGAN-rank. We use generator as the model to diversify search results. For the feature vector, the 18-dimension relevance feature vector ğ‘¥ ğ‘‘,ğ‘ is listed in Table <ref type="table" target="#tab_4">3</ref>. ğ‘’ ğ‘‘ is the embedding vector via doc2vec method. The query and subtopic embedding vectors ğ‘’ ğ‘ , ğ‘’ ğ‘– are constructed by the top 20 (ğ‘Š ) documents' distribution representation. For the DVGAN-doc method, we sampled about 40 samples of 20-document-length using the random sampling way for each query. For the DVGAN-rank, we sampled about 10 samples of 10 negative rankings of 20-document-length for each query. Besides, we also show the results using top 100(ğ‘ ) results of Lemur for diversity re-ranking. The results using top 50 documents are shown as DVGAN-doc(50). The results using top 100 documents are shown as DVGAN-doc(100).</p><p>For all of the supervised methods, we tune the learning rate ğ‘Ÿ from 10 âˆ’7 to 10 âˆ’1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overall Results</head><p>The overall results are shown in Table <ref type="table" target="#tab_5">4</ref>. We find both DVGAN-doc methods outperform all explicit and implicit baselines including the naive method DSSA + R-LTR and DVGAN-rank methods outperforms all except DSSA. We find that:</p><p>(1) The relative improvement over DSSA, the best explicit method, is up to 2.0% in terms of ğ›¼-nDCG for ğ‘ = 50 and up to 3.5% for ğ‘ = 100. The relative improvement over PAMM-NTN, the best implicit method, is up to 11.5% in terms of ğ›¼-nDCG and up to 13.2% for ğ‘ = 100. These results show the advantage of combining the explicit and implicit approaches using DVGAN instead of using only one kind of features.</p><p>(2) The relative improvement over DSSA + R-LTR method is up to 8.1% in terms of ğ›¼-nDCG for ğ‘ = 50 and up to 9.8% for ğ‘ = 100. This comparison shows that the naive way of combining explicit and implicit approaches is not effective. One possible reason for the bad performance of the naive method is that the diversification score functions of two different methods DSSA and R-LTR are not on the same scale, which may cause the learning process slow and easy to trap into local minimal.</p><p>(3) The relative improvement over the pre-train model(DSSA(pretrain)) is up to 5.4% in terms of ğ›¼-nDCG for ğ‘ = 50 and up to 5.8% for ğ‘ = 100. This comparison shows the great advantage of using loss function of generative adversarial network instead of traditional loss function such as MLE, PAMM and list-pairwise loss in DSSA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effects of Different Generators and Discriminators</head><p>In this part, we compare the different configuraions of generator and discriminator and the result is shown in Table <ref type="table" target="#tab_6">5</ref>. Firstly we can infer that using DSSA's score function in generator and R-LTR's score function in discriminator is the best configuration to introduce generative adversarial network into search result diversification as we expected. Only considering the generator, we can infer that using DSSA's score function outperforms using R-LTR's.</p><p>The reason is that as we use generator to diversify search results, DSSA's score function considering subtopic coverage is close to diversification evaluation metrics such as ğ›¼-nDCG. Only considering the discriminator, we can infer that using R-LTR's score function outperforms using DSSA's, The reason is that R-LTR's score function directly modeling dissimilarity between documents is useful in distinguishing negative and positive samples that are closed. Thus, discriminator can provide better rewards for generators to improve the performance of diversification. We also do the same study on ğ‘ = 100, the result is the same.</p><p>proposed two methods in this framework, DVGAN-doc method and DVGAN-rank method. The DVGAN-doc method is a natural and effective extension of IRGAN. The DVGAN-rank is a combination of PAMM loss function and generative adversarial network. We also proposed several sampling algorithms for the input data to generator to better solve the problem of the lack of high-quality training data. During the the training process, discriminator can provide the implicit information which cannot be obtained in generator via the reward for generator and generator can provide more useful negative samples for discriminator. Experimental results confirm the effectiveness of the proposed methods. The adaption of generative adversarial network also solves the problem of the lack of high-quality data in training process. In future work, we plan to improve the DVGAN-rank and to adapt the hierarchical structure to our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 :S</head><label>7</label><figDesc>ğ‘ â† S ğ‘ âˆª ğ‘Ÿ ğ‘ [: ğ‘–] for ğ‘— = 1 to ğ‘‘ ğ‘  do 13: ğ‘† â† ğ‘† âˆª random_in(ğ· ğ‘ ) 14: end for 15: ğ‘† â† re âˆ’ rank(S) 16: S ğ‘ â† S ğ‘ âˆª ğ‘† 17:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations in our framework ğ‘“ ğœƒ , ğ‘“ ğœ™ diversification function in Gğ‘ğ‘›ğ‘‘ D G ğœƒ (ğœ‰),D ğœ™ (ğœ‰) score function of sample ğœ‰ in G and D generator can produce a high-quality document ranking as diversification search results. The notations used in this paper are listed in Table 1. We propose two different training methods, namely DVGAN-doc and DVGANrank. They mainly differ in what ğœ‰ the generator generates and input data Î“ are given to the generator. The item ğœ‰ generated by the generator could be a document ğ‘‘ or a ranking ğ‘™</figDesc><table><row><cell>Name</cell><cell>Description</cell></row><row><cell>ğ‘„, ğ‘</cell><cell>the query set, a query in the set, ğ‘ âˆˆ ğ‘„</cell></row><row><cell>ğ‘†</cell><cell>a list of documents that are already ranked</cell></row><row><cell>S ğ‘</cell><cell>a set of document list ğ‘† for query ğ‘, ğ‘† âˆˆ S ğ‘</cell></row><row><cell>ğ¶</cell><cell>a set of candidate documents</cell></row><row><cell>C ğ‘</cell><cell>a collection of document sets, ğ¶ âˆˆ C ğ‘</cell></row><row><cell>Î“</cell><cell>data given to the generator</cell></row><row><cell>Î â€² , ğœ‰</cell><cell>set of generated samples, ğœ‰ is a sample</cell></row><row><cell>ğ‘ true (ğœ‰ |ğ‘, Î“)</cell><cell>true distribution of samples</cell></row><row><cell>ğ‘ ğœƒ (ğœ‰ |ğ‘, Î“)</cell><cell>distribution of generated samples</cell></row><row><cell>ğ· â€² , ğ‘‘</cell><cell>set of generated documents, ğ‘‘ is a document</cell></row><row><cell>ğ¿ â€² , ğ‘™</cell><cell>set of generated ranking lists, ğ‘™ is a ranking list</cell></row><row><cell>G, D</cell><cell>generator, discriminator</cell></row><row><cell>ğœ™, ğœƒ</cell><cell>parameters in G and D</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 :</head><label>1</label><figDesc>input: query set ğ‘„, document set ğ· ğ‘ and ideal document ranking ğ‘Ÿ ğ‘ for each query ğ‘, number of random sample ğ‘› ğ‘  , number of selected documents in random sample ğ‘‘ ğ‘  . 2: output: set of selected ranking lists S ğ‘ for each query ğ‘. 3: for query ğ‘ âˆˆ ğ‘„ do</figDesc><table><row><cell>4:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 2 Sampling algorithm used by DVGAN-rank 1: input: query set ğ‘„, document set ğ· ğ‘ for each query ğ‘, number of sample ğ‘› ğ‘  , number of candidate documents in a sample ğ‘‘ ğ‘ and number of negative ranking of a sample ğ‘› ğ‘™ 2: output: the candidate document ranking set C ğ‘ for each query ğ‘ and positive and negative ranking set ğ‘™ + ğ‘ and ğ‘™ âˆ’ ğ‘ for each sample document set ğ¶ 3: for query ğ‘ âˆˆ ğ‘„ do</figDesc><table><row><cell>4:</cell><cell>C ğ‘ â† âˆ…</cell></row><row><cell>5:</cell><cell>for ğ‘– = 1 to ğ‘› ğ‘  do</cell></row><row><cell>6:</cell><cell>ğ¶ â† âˆ…</cell></row><row><cell>7:</cell><cell>for ğ‘— = 1 to ğ‘‘ ğ‘ do</cell></row><row><cell>8:</cell><cell>ğ¶ â† ğ¶ âˆª random_in(D q )</cell></row><row><cell>9:</cell><cell>end for</cell></row><row><cell>10:</cell><cell>C ğ‘ â† C ğ‘ âˆª ğ¶</cell></row><row><cell>11:</cell><cell>end for</cell></row><row><cell>12:</cell><cell>for sample ğ¶ âˆˆ C ğ‘ do</cell></row><row><cell>13:</cell><cell>ğ‘™ + ğ‘ â† re âˆ’ rank(C)</cell></row><row><cell>14:</cell><cell>for ğ‘– = 1 to ğ‘› ğ‘™ do</cell></row><row><cell>15:</cell><cell>ğ‘™ âˆ’ ğ‘ â† shuffle(ğ‘™ + ğ‘ )</cell></row><row><cell>16:</cell><cell>end for</cell></row><row><cell>17:</cell><cell>end for</cell></row><row><cell cols="2">18: end for</cell></row></table><note>19: return C ğ‘ , ğ‘™ + ğ‘ , ğ‘™ âˆ’ ğ‘</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Diversity features for R-LTR</figDesc><table><row><cell>Name</cell><cell>Description</cell></row><row><cell>subtopic diversity</cell><cell>euclidean distance based on SVD model</cell></row><row><cell>text diversity</cell><cell>cosine-based distance on term vector</cell></row><row><cell>title diversity</cell><cell>text diversity on title</cell></row><row><cell cols="2">anchor text diversity text diversity on anchor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Relevance features for both R-LTR and DSSA</figDesc><table><row><cell>Name</cell><cell>Description</cell><cell>#Features</cell></row><row><cell>TF-IDF</cell><cell>the TF-IDF model</cell><cell>5</cell></row><row><cell>BM25</cell><cell>BM25 with default parameters</cell><cell>5</cell></row><row><cell>LMIR</cell><cell cols="2">LMIR with Dirichlet smoothing 5</cell></row><row><cell cols="2">PageRank PageRank score</cell><cell>1</cell></row><row><cell>#inlinks</cell><cell>number of inlinks</cell><cell>1</cell></row><row><cell cols="2">#outlinks number of outlinks</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of all methods. The best result is in bold. â€  indicates significant improvement over the pre-train model and all baselines except DSSA with p-value&lt;0.05. â˜… indicates significant improvement over the pre-train model and all baselines with p-value&lt;0.05.</figDesc><table><row><cell>Methods</cell><cell cols="2">ERR-IA ğ›¼-nDCG</cell><cell>NRBP</cell></row><row><cell>Lemur</cell><cell>.271</cell><cell>.369</cell><cell>.232</cell></row><row><cell>ListMLE</cell><cell>.287</cell><cell>.387</cell><cell>.249</cell></row><row><cell>xQuAD</cell><cell>.317</cell><cell>.413</cell><cell>.284</cell></row><row><cell>TxQuAD</cell><cell>.308</cell><cell>.410</cell><cell>.272</cell></row><row><cell>HxQuAD</cell><cell>.326</cell><cell>.421</cell><cell>.294</cell></row><row><cell>PM2</cell><cell>.306</cell><cell>.411</cell><cell>.267</cell></row><row><cell>TPM2</cell><cell>.291</cell><cell>.399</cell><cell>.250</cell></row><row><cell>HPM2</cell><cell>.317</cell><cell>.420</cell><cell>.279</cell></row><row><cell>R-LTR</cell><cell>.303</cell><cell>.403</cell><cell>.267</cell></row><row><cell>PAMM</cell><cell>.309</cell><cell>.411</cell><cell>.271</cell></row><row><cell>R-LTR-NTN</cell><cell>.312</cell><cell>.415</cell><cell>.275</cell></row><row><cell>PAMM-NTN</cell><cell>.311</cell><cell>.417</cell><cell>.272</cell></row><row><cell>DSSA+R-LTR</cell><cell>.328</cell><cell>.430</cell><cell>.302</cell></row><row><cell>DSSA</cell><cell>.356</cell><cell>.456</cell><cell>.326</cell></row><row><cell>DSSA(pre-train)(50)</cell><cell>.339</cell><cell>.441</cell><cell>.304</cell></row><row><cell>DVGAN-rank(50)</cell><cell>.340</cell><cell>.442</cell><cell>.303</cell></row><row><cell>DVGAN-doc(50)</cell><cell>.367  â€ </cell><cell>.465  â€ </cell><cell>.334  â€ </cell></row><row><cell>DSSA(pre-train)(100)</cell><cell>.342</cell><cell>.446</cell><cell>.306</cell></row><row><cell>DVGAN-rank(100)</cell><cell>.343</cell><cell>.448</cell><cell>.305</cell></row><row><cell>DVGAN-doc(100)</cell><cell>.369  â€ </cell><cell>.472 â˜…</cell><cell>.334  â€ </cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance for DVGAN-doc with different score function in generators and discriminators. (50/100) indicates the number of documents used for re-ranking(ğ‘ )</figDesc><table><row><cell>Gen, Dis</cell><cell>ERR-IA</cell><cell>ğ›¼-nDCG</cell><cell>NRBP</cell></row><row><cell></cell><cell>(50/100)</cell><cell>(50/100)</cell><cell>(50/100)</cell></row><row><cell>DSSA, R-LTR</cell><cell>.367/.369</cell><cell>.465/.472</cell><cell>.334/.334</cell></row><row><cell>DSSA, DSSA</cell><cell>.355/.363</cell><cell>.455/.465</cell><cell>.332/.330</cell></row><row><cell>R-LTR, R-LTR</cell><cell>.341/.353</cell><cell>.441/.454</cell><cell>.305/.318</cell></row><row><cell>R-LTR, DSSA</cell><cell>.336/.349</cell><cell>.437/.452</cell><cell>.298/.313</cell></row></table><note>"Gen, Dis" infers "Generator, Discriminator"</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Different sampling strategies in DVGAN-doc. (50/100) indicates the number of documents used for reranking(ğ‘ )</figDesc><table><row><cell cols="2">Sampling Strategy ERR-IA</cell><cell>ğ›¼-nDCG</cell><cell>NRBP</cell></row><row><cell></cell><cell>(50/100)</cell><cell>(50/100)</cell><cell>(50/100)</cell></row><row><cell>Ideal sampling</cell><cell>.355/.360</cell><cell>.457/.468</cell><cell>.321/.326</cell></row><row><cell cols="2">Random sampling .353/.352</cell><cell>.454/.458</cell><cell>.319/.316</cell></row><row><cell>Both</cell><cell>.367/.369</cell><cell>.465/.472</cell><cell>.334/.334</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Zhicheng Dou is the corresponding author. This work was supported by National Key R&amp;D Program of China No. 2018YFC0830703, National Natural Science Foundation of China No. 61872370 and No. 61832017, Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sampling Study in DVGAN</head><p>In this part, we compare different strategies of sampling in our DVGAN-doc method and the result is shown in Table <ref type="table">6</ref>. The first way is only sampled by the ideal sampling algorithm which is to select the first ğ‘˜ documents in the ideal ranking as our selected document list. The second way is random sampling algorithm which is described in Algorithm 1. The result shows that combining both sampling algorithms is better than using only one. The reason is that (1). ideal sampling is helpful for discriminator to distinguish the positive and negative rankings but makes it hard for generator to imitate the real distribution of data because it is too ideal <ref type="bibr" target="#b1">(2)</ref>. random sampling makes it easy to imitate for generator but can be confusing for discriminator to distinguish the positive and negative rankings and may provide wrong rewards for generator. The reason why the ideal sampling outperforms the random sampling is that the quality of randomly sampled data is under no guarantee and only using it may cause deviation in training. We also do the same study on ğ‘ = 100, the result is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we proposed DVGAN -a framework for search result diversification adversarial training combining both explicit and implicit information to improve the diversification performance and to solve the problem that high-quality dataset is hard to capture. We </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/290941.291025</idno>
		<ptr target="https://doi.org/10.1145/290941.291025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98)</title>
				<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Expected Reciprocal Rank for Graded Relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1645953.1646033</idno>
		<ptr target="https://doi.org/10.1145/1645953.1646033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM &apos;09)</title>
				<meeting>the 18th ACM Conference on Information and Knowledge Management (CIKM &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Effectiveness Measure for Ambiguous and Underspecified Queries</title>
		<author>
			<persName><forename type="first">Kolla</forename><surname>Maheedhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><surname>Vechtomova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04417-5_17</idno>
		<idno>ICTIR 2009. 188-199</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-04417-5_17" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory</title>
				<meeting>the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Novelty and Diversity in Information Retrieval Evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390334.1390446</idno>
		<ptr target="https://doi.org/10.1145/1390334.1390446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;08)</title>
				<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Effectiveness Measure for Ambiguous and Underspecified Queries</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04417-5_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-04417-5_17" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory (ICTIR &apos;09)</title>
				<meeting>the 2nd International Conference on Theory of Information Retrieval: Advances in Information Retrieval Theory (ICTIR &apos;09)<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversity by Proportionality: An Election-Based Approach to Search Result Diversification</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348296</idno>
		<ptr target="https://doi.org/10.1145/2348283.2348296" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)</title>
				<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing and Modeling Rank Data</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Groggel</surname></persName>
		</author>
		<idno type="DOI">10.1080/00401706.1996.10484555</idno>
		<ptr target="https://www.tandfonline.com/doi/pdf/10.1080/00401706.1996.10484555" />
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="403" to="403" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">1997. Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search Result Diversification Based on Hierarchical Intents</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2806416.2806455</idno>
		<ptr target="https://doi.org/10.1145/2806416.2806455" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM &apos;15)</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management (CIKM &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<idno type="DOI">10.5555/2969033.2969125</idno>
		<ptr target="https://doi.org/10.5555/2969033.2969125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bing</forename><forename type="middle">Xu</forename><surname>David</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Warde-Farley</forename><surname>Sherjil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ozair</forename><surname>Aaron Courville</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Changkuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://boston.lti.cs.cmu.edu/Data/clueweb09/" />
		<title level="m">Clueweb09 data set</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Diversify Search Results via Subtopic Attention</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yue</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080805</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data</title>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331218</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331218" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explicit Web Search Result Diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><surname>Santos</surname></persName>
		</author>
		<idno type="DOI">10.1145/2492189.2492205</idno>
		<ptr target="https://doi.org/10.1145/2492189.2492205" />
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="68" />
			<date type="published" when="2012-06">2012. June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting Query Reformulations for Web Search Result Diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772780</idno>
		<ptr target="https://doi.org/10.1145/1772690.1772780" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web (WWW &apos;10)</title>
				<meeting>the 19th International Conference on World Wide Web (WWW &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080786</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080786" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating Search Result Diversity Using Intent Hierarchies</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911497</idno>
		<ptr target="https://doi.org/10.1145/2911451.2911497" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</title>
				<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Search Result Diversity Evaluation Based on Intent Hierarchies</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2729559</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2017.2729559" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Listwise Approach to Learning to Rank: Theory and Algorithm</title>
		<author>
			<persName><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390306</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390306" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1192" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767710</idno>
		<ptr target="https://doi.org/10.1145/2766462.2767710" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;15)</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling Document Novelty with Neural Tensor Network for Search Result Diversification</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911498</idno>
		<ptr target="https://doi.org/10.1145/2911451.2911498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)</title>
				<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting Diverse Subsets Using Structural SVMs</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390310</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390310" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML &apos;08)</title>
				<meeting>the 25th International Conference on Machine Learning (ICML &apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning for Search Result Diversification</title>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609634</idno>
		<ptr target="https://doi.org/10.1145/2600428.2609634" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;14)</title>
				<meeting>the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
