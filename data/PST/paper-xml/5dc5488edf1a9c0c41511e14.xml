<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analytical Cache Modeling and Tilesize Optimization for Tensor Contractions</title>
				<funder ref="#_yDB6z4Z">
					<orgName type="full">Louisiana Board of Regents</orgName>
				</funder>
				<funder ref="#_3fbaGpW #_yFHgVNP">
					<orgName type="full">U.S. National Science Foundation</orgName>
				</funder>
				<funder ref="#_qdWR85y">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<email>lirui@cs.utah.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
							<email>sukumaranrajam.1@osu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Veras</surname></persName>
							<email>rveras@lsu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tze</forename><forename type="middle">Meng</forename><surname>Low</surname></persName>
							<email>lowt@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Fabrice</forename><surname>Rastello</surname></persName>
							<email>fabrice.rastello@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Atanas</forename><surname>Rountev</surname></persName>
							<email>rountev@cse.ohio-state.edu</email>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<settlement>Inria</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Grenoble INP</settlement>
									<region>LIG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Analytical Cache Modeling and Tilesize Optimization for Tensor Contractions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3295500.3356218</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>tensor contraction</term>
					<term>domain-specific compiler optimization</term>
					<term>performance modeling</term>
					<term>model-driven design-space exploration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data movement between processor and memory hierarchy is a fundamental bottleneck that limits the performance of many applications on modern computer architectures. Tiling and loop permutation are key techniques for improving data locality. However, selecting effective tile-sizes and loop permutations is particularly challenging for tensor contractions due to the large number of loops. Even state-of-the-art compilers usually produce sub-optimal tile-sizes and loop permutations, as they rely on na?ve cost models. In this paper we provide an analytical model based approach to multi-level tile size optimization and permutation selection for tensor contractions. Our experimental results show that this approach achieves comparable or better performance than state-of-the-art frameworks and libraries for tensor contractions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>computational science and machine learning. Consider the following contraction from the CCSD(T) <ref type="bibr" target="#b4">[5]</ref> method in computational chemistry, where two 4D tensors are contracted to produce a 6D tensor:</p><p>C[a, b, c, i, j, k] = A[i, b, a, l] * B[l, c, j, k]</p><p>It represents the computation:</p><formula xml:id="formula_1">C[a, b, c, d, i, j, k] = l A[i, b, a, l] * B[l, c, j, k]</formula><p>Typically, the sizes of tensors in large-scale calculations vastly exceed cache capacity, thus tiling is a critical loop transformation for efficient implementation of tensor contractions. However, the number of tiling loops is often very large, and much larger is the number of possible permutations of the tiling loops. Further, multilevel tiling must be considered, in order to optimize across a multilevel cache hierarchy. Finally, a challenging modeling aspect that has generally been ignored in prior attempts at analytical tile-size optimization is that of inter-tile data reuse -thus for simplicity the assumption is often made that no reuse of data occurs across successive tiles. However, this assumption is avoided in the well known panel-panel scheme <ref type="bibr" target="#b6">[7]</ref> for optimal tiling of matrix-matrix multiplication <ref type="bibr" target="#b11">[12]</ref>, which makes full use of inter-tile data reuse by keeping a slice of the result matrix stationary across execution of successive tiles.</p><p>In this paper we address the above challenges and develop an effective analytical approach for selection of tile permutation and tile-size for multi-level tiled execution of tensor contractions.We will show that this approach is broadly applicable, but our primary focus is that of effective tiling of arbitrary tensor contractions, a fundamentally important primitive for many applications in computational and data science. In this section we provide a high-level sketch of the key ideas behind the developed approach to tile optimization.</p><p>The computation for the tensor contraction in Eq. 1 can be expressed as a 7-dimensional loop nest, with one loop per unique index. Allowing for any order of accumulation of additive contributions for each result tensor element, all loops of an arbitrary tensor contraction are fully permutable and hence fully tileable with hyper-rectangular tiles. Considering a three-level memory hierarchy, up to three levels of tiling may be appropriate, leading to an explosively large search space with three groups of 7 tiling loops, with 7! possible permutations of the tiling loops within each group, i.e., 1.28 ? 10 11 possible configurations.</p><p>However, this huge space of permuted orders for the tiling loops can be drastically pruned by showing that only the innermost tiling loop within each band can have a significant effect on performance. This reduces the number of evaluated configurations from (7!) 3  (1.28 ? 10 11 ) to 7 3 , i.e. only 343 cases. We will elaborate later in the paper that this is a consequence of the fact that each tensor dimension of any tensor is indexed by a distinct loop index in a tensor contraction.</p><p>For a given permutation of tiling loops we develop an analytical formulation for the volume of data movement as a set of conditional expressions in terms of parametric tile sizes. A constrained optimization solver is then used to find optimal solutions to the formulated minimization problem of finding multi-level tile sizes that minimize the effective time to transmit the transferred volume of data at the different levels of the storage hierarchy.</p><p>This paper makes the following key contributions:</p><p>? It presents the first practically effective analytical formulation (to our knowledge) for multi-level tile-size optimization for arbitrary dimensional tensor contractions; ? It provides a solution for the multi-level tile-size optimization problem that uses a standard constrained optimization solver; ? It presents experimental validation of the proposed approach using 36 benchmarks in the TCCG benchmark suite . The rest of the paper is organized as follows. Section 2 presents an overview of our approach. Section 3 details our data movement model and loop permutation/tile-size selection strategy. Section 4 describes the micro kernel design and Section 5 describes buffering/packing to reduce data movement. Extensive experimental evaluation is shown in Section 6. Related works are presented in Section 7 and Sections 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW OF MODELING APPROACH</head><p>Similar to the manner in which standard matrix-matrix multiplication can be expressed as a 3-dimensional loop nest, the computation for the tensor contraction in Eq. 1 can be expressed as a 7-dimensional loop nest, with one loop for each of the indices {a,b,c,i,j,k,l}. Since any order of accumulation of additive contributions for each result tensor element is generally considered to be acceptable by application scientists, all loops of an arbitrary tensor contraction are considered fully permutable and hence fully tileable with hyper-rectangular tiles. Considering a three-level memory hierarchy, up to three levels of tiling may be appropriate, leading to an explosively large search space with three groups of 7 tiling loops and 7! possible permutations of the tiling loops within each group, i.e., 1.28 ? 10 11 possible configurations. Zero/Full Inter-Tile Reuse The following key observation is used to drastically prune the huge configuration search space: For any arbitrary tensor contraction, each dimension of any tensor is indexed by a distinct index from the surrounding perfectly nested loops. For example, the four dimensions of tensor A in the contraction in Eq. 1 are respectively indexed by the four distinct loop indices i, b, a, and l. Hence, a loop index is either an explicit index in a given tensor or is unused in indexing that tensor. For example, the loop index a is an explicit index for A, and C, but is not used to access elements of B. For a tiled code, we call the loops that iterate over tiles as tiling loops. For example, for the matrix multiplication in Listing 2 with tiles "i1,j1,k1", the tiling loops are the ones indexed by i2, j2, and k2. The innermost tiling loop is the one indexed by k2. Consider again the CCSD(T) example of Eq. 1. If the innermost tiling loop index is a, successive tiles along that tiled index would repeatedly access exactly the same slice of data for B (because a does not at all affect the addressing of B), while completely distinct slices of data would be accessed by successive tiles for A and C (because a is an explicit index for A and C, causing each tile to access a distinct and disjoint range of values for the tensor dimension indexed by it). Assuming that the combined data-footprint of a tile just fills the cache/ scratchpad, we will have full inter-tile data reuse for elements of B, but no data reuse for A and C. This observation will always hold as soon as the available space in cache or scratchpad memory is disjointly partitioned (thus avoiding conflicts) to hold the slices of data accessed in a tile from the three tensors. Only Innermost Tiling Loop Matters: Tile sizes at each level are generally chosen to be large enough such that the data-footprint of a tile is close to the cache/scratchpad capacity but does not exceed it. In that case, as successive tiles of the innermost tiling loop are executed, the data for tensors not indexed by that loop stays invariant, while the data slices for other tensors will be completely disjoint from those used in the previous tiles. In the example considered, if a is the index corresponding to the innermost tiling loop, the data slices for B would be invariant for successive tiles, while complete replacement of data slices for A and C would occur. A direct consequence is that no inter-tile reuse is possible for A and C, irrespective of the permutation of the outer six tiling loops. Further, any additional reuse for B through outer tiling loops would only have a marginal effect on total data volume. Indeed, a significant degree of reuse is already achieved for B through the innermost tiling loop, implying that the total data movement for B is already much lower than that for A and C.</p><p>The significant implication of the above observation is the following: Consider a given level in the memory hierarchy and its corresponding tiling level. Only the choice of the innermost tiling loop affects the total data volume (ignoring second order effects) for all tensors from/to that memory level. In other words, among all possible tiling loop permutations, we only need to consider the different possible choices for innermost tiling loop, and choose any single arbitrary permutation for all surrounding tiling loops. For the tensor contraction example, this reduces the number of evaluated configurations from (7!) 3 (1.28 ? 10 11 ) to 7 3 , that is, to only 343 cases. Conditional Analytical Expressions for Data Volume: In the next section, we develop an approach to analytical modeling of the impact of tile sizes on data volume using the example of matrixmatrix multiplication. The key idea here is that for a restricted but important class of dense tensor computations, including arbitrary tensor contractions, all tensor dimensions are indexed by distinct loop iterators. With such computations, the data footprint of a tile with respect to any operand tensor is simply the product of tile extents along indices that appear in the indexing of the tensor. An f o r ( i n t i = 0 ; i &lt; Ni ; i + +) f o r ( i n t j = 0 ; j &lt; Nj ; j + + ) f o r ( i n t k = 0 ; k &lt; Nk ; k + + )</p><formula xml:id="formula_2">C[ i ] [ j ] += A[ i ] [ k ] * B [ k ] [ j ]</formula><p>Listing 1: Matrix Multiplication // Tile sizes are assumed to be perfect multiples of problem sizes f o r ( i n t i 2 = 0 ; i 2 &lt; Ni ; i 2 += T i 1 ) f o r ( i n t j 2 = 0 ; j 2 &lt; Nj ; j 2 += T j 1 ) f o r ( i n t k2 = 0 ; k2 &lt; Nk ; k2 +=Tk1 ) f o r ( i n t i 1 = 0 ; i 1 &lt; T i 1 ; i 1 + +) f o r ( i n t j 1 = 0 ; j 1 &lt; T j 1 ; j 1 + +) f o r ( i n t k1 = 0 ; k1 &lt; Tk1 ; k1 + +)</p><formula xml:id="formula_3">C[ i 1 + i 2 ] [ j 1 + j 2 ]+= A[ i 1 + i 2 ] [ k1+k2 ] * B [ k1+k2 ] [ j 1 + j 2 ]</formula><p>Listing 2: Tiled Matrix Multiplication inner-to-outer traversal of the loop structure enables the development of conditional symbolic expressions for total data movement as a function of parametric tile sizes. The conditional analytical expressions are then optimized by use of a non-convex optimization solver to determine optimal tile sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYTICAL CACHE MODELING</head><p>This section presents a new model based approach for predicting the volume of data movement for tiled tensor contraction. For simplicity, we begin by assuming that the caches are programmable (scratchpad) and that the cache-line size is one word. We also assume that the performance is only limited by the cache bandwidth. Later we will address issues that reflect a real cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single level cache modeling</head><p>Loop tiling (loop blocking) is a widely used technique to improve data locality. Tiling chunks the iteration space into multi-dimensional blocks, which enables better reuse of data in hyper-rectangular slices. Tiled loop iterators corresponding to a loop i are represented by an ordered list of iterators i 1 , i 2 , ..., i l +1 , where l represents the tiling level. Iterator i l +1 represents the outermost loop and the i 1 represents the innermost loop. l == 0 denotes the statement level. The tile sizes corresponding to each iterator are represented using Ti 1 ,Ti 2 , ...,Ti l +1 . Listings 1 and 2 illustrates this notation using matrix multiplication as an example. Listing 2 corresponds to one level tiling of the i, j, k loops in Listing 1. The tiled i loop is represented using i1 and i2. i2 (inter tile iterator) iterates over different blocks of i and i1 (intra tile iterator) iterates within a block.</p><p>For a given tensor contraction code with fixed loop structure (loop permutation), and parametric tile size variables, our objective is to model the data movement between the cache and main memory. The cost modelling is illustrated using Listing 2, which shows pseudo code for matrix multiplication. The number of elements in each array is assumed to be much larger than the cache capacity. Let DF (A, i) represent the data footprint of array A corresponding loop to i (number of unique elements of the array A accessed by loop nest starting at i). Let DM(A, i) represent the data movement between cache and the main memory corresponding to array A at loop i. Listing 3 shows the pseudo-code to compute the data movement. At the statement level, only a single element of an array is accessed (DM(A, 0) == DF (A, 0) == 1). If an array is indexed by a given loop iterator i then its data footprint (data movement) corresponding to the i loop is equal to the product of data footprint (data movement) corresponding to the immediate inner loop and the number of i loop iterations. For example, array A is indexed by k1. Hence, for each k1 loop iteration, a distinct element of the array A is accessed. Thus the data footprint for A at k1 is the product of DF (A, 0) and Tk1 which is equal to 1 ?Tk1. Similarly, the DF (B, k1) is k1. Since the array C is not indexed by k1, multiple k1 iterations accesses the same C element (DF (C, k1) == DF (C, 0) == 1). The total data movement for an array for loop i is dependent on the data movement for inner loops and the cache capacity. For example, the data movement cost of the j1 loop is dependent on the data movement cost of k1. Thus DF (B, j1) == DM(B, j1) == Tk1 * T j1 and DF (C, j1) == DM(C, j1) == 1 * T j1. Since array A is not indexed by j1, the data footprint of A at j1 is the equal to the data footprint at k1 (DF (A, j1) == DF (A, k1)). However, data movement for A at j1 depends on whether the cache capacity has already been exceeded or not. If the data footprint corresponding to all arrays at k1 (immediate inner loop) is less than cache capacity (DF (A, k1) + DF (B, k1) + DF (C, k1) &lt;= CacheCapacity), we can load A once and reuse it at j1 level (DM(A, j1) == DM(A, k1)). However, if the data footprint corresponding to all arrays at k1 exceeds cache capacity, A has to be loaded multiple times (DM(A, j1) == DM(A, k1) * T j1).</p><p>Table <ref type="table">1</ref> shows the method to traverse all the small dimension size branches. It is built from a one level cache hierarchy with one tiling group GEMM. The table lists all possible combinations whether each of the dimension problem sizes can fit into cache. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x 1 Table 1: Table for traversing combinations of dimension size</head><p>Therefore, for a GEMM problem on one tiling group with three levels of tiling loops, there would be 2 3 = 8 different combinations to be considered, which are already in the row of the top tiling loop i2.</p><p>The reason to consider data movement of each combination separately is, if some of the dimension can fully fit in cache, the footprint for tensors using this index would not change. That means it would not start to swap out other data at this level. For example, if Nk can fully fit in cache, Tk1 == N k , then the Ti1 ?T k1 amount of data footprint of A would not be swapped out. As a result, this part of A will starts to get reuse in the loop level j2. However in the normal case where N K is very large, Tk1 &lt; N k, accesses of A will starts to go over the whole dimension of Nk, and because of LRU replacement policy, the beginning segment of A will be replaced, which makes the reuse in loop j2 impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Level cache modeling</head><p>Most modern processors have multiple levels of cache. The fastest cache (L1-cache) is designed to have high bandwidth but has low capacity. Higher caches such as L2 and L3 have higher capacity than L1 but lower bandwidth. Multi-level tiling is used to take advantage of multiple levels of cache. The data movement model presented in Section 3.1 can be extended to support multiple cache levels. We assume that each loop is tiled one for each cache level. Listing 4 shows 2-level tiled matrix multiplication code for a machine with 2 levels of cache. Similar to Listing 2, the loop iterators i1, i2, and i3 represents the tiled i loop. The DF () function presented in Section 3.1 is not dependent on number of cache levels, hence it can be directly used. The DM() function is modified to include cache level as a parameter -DM(A, i, l) represents the data movement between memory hierarchy l and l + 1 for array A corresponding to i loop. Listing 3 can be adapted for multi-level tiling by 1 // Tile sizes are assumed to be perfect multiples of problem sizes 2 f o r ( i n t i 3 = 0 ; i 3 &lt; Ni ; i 3 += T i 2 ) 3 f o r ( i n t j 3 = 0 ; j 3 &lt; Nj ; j 3 += T j 2 ) 4 f o r ( i n t k3 = 0 ; k3 &lt; Nk ; k3 +=Tk2 ) 5 f o r ( i n t i 2 = 0 ; i 2 &lt; T i 2 ; i 2 += T i 1 ) 6 f o r ( i n t j 2 = 0 ; j 2 &lt; T j 2 ; j 2 += T j 1 ) 7 f o r ( i n t k2 = 0 ; k2 &lt; Tk2 ; k2 +=Tk1 ) 8 f o r ( i n t i 1 = 0 ; i 1 &lt; T i 1 ; i 1 + +) 9 f o r ( i n t j 1 = 0 ; j 1 &lt; T j 1 ; j 1 + +) 10 f o r ( i n t k1 = 0 ; k1 &lt; Tk1 ; k1 + + )</p><formula xml:id="formula_4">11 C[ i 1 + i 2 + i 3 ] [ j 1 + j 2 + j 3 ] += 12 A[ i 1 + i 2 + i 3 ] [ k1+k2 + j 3 ] * 13 B [ k1+k2+k3 ] [ j 1 + j 2 + j 3 ]</formula><p>Listing 4: Multi-Level tiling for Matrix Multiplication changing DM(A, i) to DM(A, i, l). Line 13 should be modified to 'if A DF (A, i -1) &lt; CacheCapacity(l)'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Predicting execution time based on data movement</head><p>Our model predicts the execution time of a program as the maximum time required to transfer data between different cache levels. This prediction is based on the assumption that the memory/cache bandwidth is the main performance bottleneck. Memory/cache latency could also affect the execution time; however, they can be hidden using prefetching. Let L denote the number of cache levels, C l | l ? 1 to L denote the cache at level l, C 0 denote the compute unit, and C L+1 denote the main-memory. Let BW l | l ? 1 to L denote the maximum bandwidth of cache at level l and BW L+1 denote the maximum main-memory bandwidth. Let C_DM(l) denote the volume of data transferred between C l and C l -1 . Let C_time(l) denote the time required to move C_DM(l) elements between C l and C l -1 . For a given loop permutation P, C_time can be computed as follows</p><formula xml:id="formula_5">C_time(P, l) = C_DM(l)/BW l (2)</formula><p>The predicted execution time is:</p><formula xml:id="formula_6">TotTime(P) = max l ?1 t o L+1 (C_time(P, l))<label>(3)</label></formula><p>Note that the above equation is predicting the time for a fixed loop structure with fixed tile sizes. Next we present how to select the tile sizes and loop permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Tile size and loop permutation selection</head><p>Finding efficient tile-sizes for a fixed loop permutation can be formulated as a constrained optimization problem. Our objective is to find the tile sizes such that minimizes the total execution time. </p><formula xml:id="formula_7">l ?1 t o L+1 (C_time(P, l)))<label>(4)</label></formula><p>In order to reduce the search space, the sum of data movement for all arrays at each cache level l (C_DM(l)) is constrained to be less than or equal to cache capacity at that level. Let group_outer(l) denote the outermost tiling loop corresponding to cache level l. The capacity constraint can be expressed as ?l ? 1 to L A?t ensor s DM(A, group_outer(l)) ? CacheCapacity(l) <ref type="bibr" target="#b4">(5)</ref> where L is the number of cache levels. The tile selection model in Equation (4) relies on an optimistic assumption that reducing the data movement cost corresponding to the most constrained cache level will achieve the best performance. However, the tile sizes obtained by solving this optimization problem only reduces the data movement of the most constrained cache level; the tile-sizes of other cache levels may not be optimal. In real machines, even though the performance is mostly limited by the most constrained cache, the data movement cost of other cache levels also impact the performance. Hence, we modify the previous single level optimization problem to a multi-level optimization problem.</p><p>Let T be set of all tile sizes. Let T l be set of tile sizes such that all tile sizes in T l affect the data movement at cache level l (C_DM(l)). In other words, varying any t ? T i will change C_DM(l) and changing any t T i wont affect C_DM(l).</p><p>Let j be the most constrained cache level. In other words ?i ? 1 to L + 1, (C_DM(j)/C_BW (j)) ? (C_DM(i)/C_BW (i)</p><p>After fixing the tile sizes for j-th cache level, the next constrained cache level can be found using arg min</p><formula xml:id="formula_8">T -T j ( max l ?(1 t o L+1)-T j (C_time(P, l)))<label>(6)</label></formula><p>The solution to Equation ( <ref type="formula" target="#formula_8">6</ref>) can be used to identify the second most constraining cache level. This processes can then be repeated for each level of cache.</p><p>In order to compute the best permutation, we could iterate over all possible permutations and select the one with best-predicted execution time (Equation ( <ref type="formula" target="#formula_7">4</ref>)). However, this search space grows exponentially with the degree of the tensor/array. Even for a simple example such as 2-level tiled matrix multiplication (Listing 4), there are 362880 (9!) possible permutations. The search space can be reduced by relying on the fact that interleaving tiling loops corresponding to different cache levels are not beneficial. In other words, we only need to consider permutations of tiling loops which correspond to the same cache level. For the matrix multiplication example, this property reduces the search space from 9! to 216 (3! ? 3! ? 3!). As explained in the overview section, the data reuse at any cache level is dominantly determined solely based on the innermost loop within a set of tiling loops which correspond to the same cache level determines reuse. For the matrix multiplication example, this property further reduce the search space from 216 to 9 (3 ? 3 ? 3). Let R represent reduced search space. The final solution is given by f inal_solution = arg min </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Solver</head><p>The optimization problem presented in Equation ( <ref type="formula" target="#formula_9">7</ref>) is a non-convex, constrained optimization problem. We use a non-convex, nonlinear programming problem from Couenne <ref type="bibr" target="#b1">[2]</ref> (https://projects.coinor.org/Couenne), released by the COIN-OR (Computational Infrastructure for Operations Research) to solve Equation <ref type="bibr" target="#b6">(7)</ref>. Couenne (Convex Over and Under Envelope's for Nonlinear Estimation) is a branch and bound algorithm to solve Mixed-Integer Nonlinear Programming (MINLP) problems of the form:</p><formula xml:id="formula_10">min f 0 (x, y), f i (x, y) ? 0, i = 1, 2, ..., m x ? R n , y ? Z p (8)</formula><p>where all f i (x, y) are nonlinear functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MICRO KERNEL: MAXIMIZE SIMD INSTRUCTION UTILIZATION</head><p>Many modern processors include SIMD (vector) instructions to improve parallelism. In order to achieve peak machine throughput, it is important to keep the functional unit busy. Functional units can be kept busy if i) sufficient Instruction Level Parallelism (ILP) is maintained and ii) the memory stalls are avoided/minimized by effectively using the cache.</p><p>Let MaxIssue be the maximum number of SIMD instructions that can be issued per clock cycle. Let WordPerVec be the width of vector instructions. Let Latency be the number of clock cycles needed for the instruction to finish all pipeline stages. During each of the clock cycles corresponding to the Latency, MaxIssue independent instructions have to be issued to keep the pipeline full. Thus MaxIssue * Latency is the minimum number of independent instructions to keep the pipeline full. Since each of these instructions should be independent, the results of these instructions should be kept in distinct registers. Thus the minimum register capacity required is MaxIssue * Latency * WordPerVec. BLIS micro-kernel <ref type="bibr" target="#b7">[8]</ref> for matrix multiplication follows this design. Our micro-kernels for tensor contraction are based on the BLIS micro-kernel and follows this design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PACKING</head><p>A packing routine is a transformation that copies a tile of a tensor into a contiguous buffer. The elements in the buffer are ordered based on the order in which the elements are accessed by the kernel. Thus, consecutive accesses to the tensor are guaranteed to be unitstride apart. Unit-stride accesses enables usage of efficient load and store SIMD instructions. In addition, packing also provides a tunable mechanism to reduce conflict misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Contiguous Loads and Stores</head><p>Efficient use of SIMD instructions in our kernel requires that the input data is stored contiguously in memory in the order that is accessed by the microkernel. Consider the tensor contraction</p><formula xml:id="formula_11">C[i, j, l] = A[j, i, k] * B[l, k].</formula><p>Assume that the innermost loops correspond to dimensions i and l. In the original tensor A and B, the unit-stride access corresponds to dimension k. However, since the innermost loops correspond to dimensions i and l, the data should be packed such that the unit-stride for A is along i and B is along l.</p><p>Figure <ref type="figure" target="#fig_4">1</ref> illustrates a simplified version of packing for dense matrix-matrix multiplication (GeMM) micro-kernels. Typical highperformance GeMM micro-kernels perform a set of outer products corresponding to a column vector of A and a row vector of B. Assuming row-major layout, the elements corresponding to B vector are laid out contiguously in memory. However, the elements of A are not contiguous and hence packing is required. During packing, the columns of A are transposed and placed in packedA. The microkernel can then use vector loads to load elements of A using the packedA buffer. In addition to efficient loads and stores, packing also helps to reduce TLB misses.</p><p>1 p a c k C o u n t e r = 0 2 f o r ( i n t i 3 = 0 ; i 3 &lt; Ni ; i 3 += T i 2 ) 3 f o r ( i n t k3 = 0 ; k3 &lt; Nk ; k3 +=Tk2 ) 4 f o r ( i n t i 2 = 0 ; i 2 &lt; T i 2 ; i 2 += T i 1 ) 5 f o r ( i n t k2 = 0 ; k2 &lt; Tk2 ; k2 +=Tk1 ) 6 f o r ( i n t i 1 = 0 ; i 1 &lt; T i 1 ; i 1 + +) Listing 5 shows the pseudo-code for packing the elements of A corresponding to the 2-level tiled matrix multiplication example (Listing 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reducing Conflict Misses</head><p>One of the major advantages of packing is reduced conflict misses. Typical caches in modern architecture are set-associative. The entire cache is divided into sets and the sets are further sub-divided into lines/ways. A mapping function determines the memory address to set mapping. Within each set, a given memory address can occupy any cache line. In such a design, a memory access can produce conflict misses, where a line in cache is swapped out and replaced even if that line was not the Least Recently Used (LRU) element. By carefully choosing the tile sizes and rely on the fact that the packing routine is designed such that the order in which data elements are arranged is same as the order that they will be accessed, conflict misses can be avoided.</p><p>Note that the packed buffers occupy contiguous regions of memory. Hence, the packed buffers are distributed along all the sets in the cache. Since most caches are not programmable, loading elements of one tensor could evict elements of other tensors. In order to prevent this the number of cache lines each tensor occupies is carefully controlled. For the matrix-multiplication example the number of lines dedicated for A, B and C at cache level l can be computed as</p><formula xml:id="formula_12">Line A = ?DF (A, l)/(NumO f Sets(l) * lineSize(l))? Line B = ?DF (B, l)/(NumO f Sets(l) * lineSize(l))? Line C = ?DF (C, l)/(NumO f Sets(l) * lineSize(l))? (9) Line A (l), Line B (l)andLine C (l) satisfy the constraint Line A (l) + Line B (l) + Line C (l) ? Associativity(l).</formula><p>In order understand how packing helps to buffer a particular array in a particular cache level, consider a simplified version of matrix multiplication where arrays A and B are only accessed (accesses to C are ignored). Consider an l level tiling loop corresponding to j. Since A is not indexed by j, A should be buffered at cache level l. During the execution of the j loop, the lines corresponding to A are accessed multiple times. Assuming LRU policy the lines corresponding to A are expected to remain in the cache as they are accessed multiple times. When the cache is full lines corresponding to B have a higher probability of being evicted as they have a lower time stamp. Thus the B elements will be streamed through the cache and A elements will remain stationary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Packing Data Movement Model</head><p>The packing routine adds additional data movement and computations which could increase the time cost. In tiled execution of a code, the same tile may be packed multiple times. In order to reduce the cost of packing, the packed data must be reused. Due to cache constraints, full reuse of all packed arrays is not possible. The packing cost can be modeled as follows. Assume that A is the only tensor that needs to be packed. Let the IS represent the iteration space (the set of all loops). Let IS A be a subset of IS which contains all indices used to access A. Assume that the packing is done at the last level of cache (ll). The cost for packing includes the cost to load the data from the main memory and the cost to store the elements to the ll cache.</p><formula xml:id="formula_13">PackCost A,buf ?mem mem?l 3 = idx ?I S A idx (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>Assume the loop order of L3 tiling group is i L3</p><formula xml:id="formula_15">1 , i L3 2 , ..., i L3 l -1, i L3 l</formula><p>, and assume i 2 , i l are the only reuse index of A, that is i 2 , i l IS a . If a buffer for packing A is created at L3 level then we have the following: Note that the A packing buffer must be filled at each iteration of loop i 2 , even if i 2 is a reuse index for A. This means that the total data movement for inside-cache-packing of a given tensor is the product of the tensor size and the ranges of all level of reuse loops above of the packing buffer's resident level. We can describe this scenario as follows:</p><formula xml:id="formula_16">f o r l o o p i L3 1 f o r l o o p i L3</formula><formula xml:id="formula_17">RDX L3 = {i L3 ? |i ? IS A ? (?i h ? IS A )[i L3 ? &gt; i L3 h ]} (11) PackCost A,buf ?L3 mem?L3 = idx ?I S A idx * r dx ?RDX L3 N Iter (rdx) (12)</formula><p>Where we define i L3 p &gt; i L3 q to mean that loop i L3 p is above loop i L3 q in L3 tiling group. Additionally, let N Iter (i L3 p ) be the number of iterations of loop i L3 p . Finally, let Tile(i L3 p ) be the tile size at L3 level for index p. Let N p be the problem size or global range of index p.</p><p>In the previous case the packing buffer is an explicitly allocated block in memory. If the buffer reside in an inner level cache, say L2, then it may not get reuse in the L3 level. This is because in the L2 level, the packing buffer is continually rewriting data to itself and those rewrite may also pollute data in L3.</p><p>Therefore, for arbitrary cache L c ,</p><formula xml:id="formula_18">RDX L c = {i L c ? |i ? IS A ? (?i h ? IS A )[i L c ? &gt; i L c h ]} (13) PackCost A,buf ?L c mem?L c = idx ?I S A idx * r dx ?RDX Lc N Iter (rdx) = idx ?I S A idx * i p ?RDX Lc (N p /Tile(i L c p ))<label>(14)</label></formula><p>A simple combination of the packing model and the computation model stated in Section 4 is added to the packing cost computed here to the DM i of the computation model. However, from the packing model it is clear that moving the buffer to inner cache will significantly increase the data movement and the number of instructions to be executed. Leaving the packing buffer in memory level will multiply the total required memory. Therefore, it would be the best option to leave the buffers at the L3 level.</p><p>Packing in L3 vs. lower levels of cache. Packing at inner levels increases the number of times each data element is packed, which in turn increases the data movement. The number of times each element is packed depends on the tile-level at which the packedbuffer is placed. Since the tile-sizes corresponding to the L3 cache are the highest, our model correctly predicts that the data movement will be lowest for L3 packing. In addition to the data movement cost, packing also requires expensive modulo and division operations. For example, on the Broadwell processor, for the "abcd-aebf-dfce (all 72)" Tensor contraction, L3 packing achieved 43.5 GFLOPS whereas packing at L2 only achieved 19.0 GFLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cache Line Reuse</head><p>For our machine model the cache line is the basic unit for moving data between memory hierarchy. Maximum cache line reuse can be achieved by accessing the tensor along the fastest accessing index (the unit stride dimension). We can extend our model to incorporate this as follows.</p><p>The packed data automatically obtain the maximum cache line reuse, because the packing order is exactly the order accessed by loop iterations. However the original data do not have this property. To obtain cache line reuse for loading original data, a tiling loop for the fastest index of original tensor layout can be added under the innermost level of packing routine, where tile size is equal to the cache line size. When the original packing routine is not accessing original data layout continuously in fastest index, this added tiling loop will always reduce the total cache lines to be removed, to 1/cacheLineSize of original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>The modeling approach imposes a constraint that tile sizes for later levels of the cache must be greater than or equal to the corresponding tile sizes for earlier level caches. If the per-core capacity of an L2 cache is less than the capacity of the private L1 cache, then the generated solution will satisfy the capacity constraints at all levels of cache, and may leave some L1 capacity unused. In such a situation, we do not see any way of fully utilizing L1 capacity while not exceeding L2 capacity. We note that the modeling approach assumes an inclusive multi-level cache -exclusive caches can also be handled by using the sum of L1+L2 capacities as the modeled L2 capacity in the modeling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>This section presents experimental results. We conducted experiments on two target platforms: an Intel Core i7-6700K dual socket 28-core Broadwell processor and an Intel Xeon CPU E5-2680 v4 single socket quad core Skylake processor. We compared our implementation with two tensor contraction libraries, TBLIS <ref type="bibr" target="#b8">[9]</ref> and TCL <ref type="bibr" target="#b12">[13]</ref>. TBLIS is a BLIS <ref type="bibr" target="#b14">[15]</ref> based library to perform tensor contraction without explicit transpose. TCL is a library for computing tensor contractions using explicit transpose and high-performance GEMM. TCL uses the HPTT <ref type="bibr" target="#b13">[14]</ref> library to perform the transposition, and either the Intel MKL library <ref type="bibr" target="#b16">[17]</ref> or BLIS for GEMM. ACMTC denotes our approach. We used the GNU GCC 7.3.0 compiler with -O3 and -std=c99 flags. For TCL-MKL, we used MKL 2018 to perform BLAS operations. We perform comprison with both versions of TCL: with the TCL-MKL version because it is the higher performing version, as well as TCL-BLIS because it reprrsents a better "apples-to-apples" comparison with ACMTC since it also uses the same BLIS micro-kernel as TCL-BLIS. Table <ref type="table" target="#tab_2">4</ref> lists all the information of contraction examples we used, from the TCCG benchmarks <ref type="bibr" target="#b12">[13]</ref>. This paper focuses on modeling data movement at the different levels of the memory hierarchy for sequential multi-level tiled execution of tensor contractions. The model can be extended for parallel multicore execution of a tensor contraction, where different cores execute adjacent tiles along a parallelizable dimension of the iteration space. The handling of shared levels of cache will depend on whether the tile data footprints of the arrays are the same across the cores or disjoint: for the disjoint data slices the capacity must be partitioned. The development of a model-driven tiled code generation strategy for parallel execution of a tensor contraction is still under development. However, we carried out experiments for parallel execution in the simpler scenario of "batched" tensor contractions, where a batch of independent tensor contractions on disjoint data needs to be performed. With this scenario, since all data processed by the different cores is completely disjoint, we simply model the shared-level L3 cache as having a capacity of 1   14   the per-socket L3 caches in the i7 processor and 1  14 of the L3 cache capacity for the quad-core processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Assessment of Data Movement Model</head><p>In this section, we assess the accuracy of the data movement prediction model by comparing the predicted volume of data movement with measured cache misses obtained using PAPI on the Intel i7-4770K Broadwell processor. We select four tensor contractions as the test cases. The label for each test case specifies the order of indices in the output and input tensors. For example, the label abcdef-degb-gfac represents the contraction C[a,b,c,d,e,f] = A[d,e,g,b]*B[g,f,a,c]. The number of tensor dimension varies from four to seven. Each example maps to one of the cases in (small A, B, large C), (small A, large B, C), (small C, large A, B), (large A, B, C). The table <ref type="table" target="#tab_0">2</ref> shows the measured cache line misses and the predict data movement in cache lines during the computation phase. Our predicted data movement is close to the actual data movement.</p><p>Figure <ref type="figure" target="#fig_7">2</ref> compares cache misses for ACMTC to TBLIS and TCL for the four representative tensor contraction expressions (one each from CCSD, CCSD(T), contractions involving tensor multiplication and two-electron integrals transform). In order to obtain accurate cache miss data, we disabled the hardware prefetcher. The combined data movement of our approach is consistently lower than all the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance Evaluation</head><p>We created a set of micro-benchmarks to measure the bandwidths of the machines at the different levels. Each micro-benchmark consists of a sequence of Load-FMA-Store instructions on a continuous memory block of a given size with no reuse. We start running the micro-benchmark from a small memory block whose size is less than half of the L1 cache and increase the size of memory to be accessed exponentially, till it is close to two times of the size of L3 cache. We recorded the total accessed data amount and time needed, and the bandwidth for that size of data is computed by dividing time by the volume of the accessed data. When the amount of data accessed in micro-benchmark is closest but smaller than some level of cache, it can fit into that level of cache, and the bandwidth of this amount of data could be seen as the bandwidth of that level of cache. We did not use STREAM benchmark or PMBW benchmark as the bandwidth reported by these benchmarks reflects the maximum achievable bandwidth under the assumption that a load/store instruction can be issued every clock cycle. Since tensor contractions require other instructions such as FMA, it is not feasible to issue load/store instructions every clock cycle. Using the PMBW bandwidth reduced the quality of our model. For example, for abcdef-dega-gfbc TC, the performance achieved using our microbenchmark bandwidth was 29 GFLOPS; using bandwidths reported by the PMBW benchmark reduced the performance to 25 GFLOPS.</p><p>The measured bandwidths for the Intel Core i7-6700K processor and the Intel Xeon E5-2680 v4 processor measured by the microbenchmark is listed in the Table <ref type="table" target="#tab_1">3</ref> As shown in 3 ACMTC achieves higher performance for single core on all benchmarks when compared to TBLIS. We outperformed TCL in most cases.</p><p>For Broadwell architecture, as shown in Figure <ref type="figure" target="#fig_8">3</ref>, the geometric mean of the speedup is 1.25x versus TBLIS, 1.41x versus TCL-MKL, and 1.51x versus TCL-TBLIS. On Skylake architecture, as shown in Figure <ref type="figure" target="#fig_8">3</ref>, the geometric mean speedup is 1.34x versus TBLIS, 1.27x versus TCL-MKL, and 1.34x versus TCL-BLIS respectively.</p><p>We also conducted experiments for the multi-core environment for a "batched" contraction scenario where a number of identical contractions are performed on different operands. We modeled the shared L3 cache as logically divided into equal-sized parts for each core on a socket. For each core, the same tensor contraction benchmark was launched on each core simultaneously on independent data. An MPI barrier was set at the beginning and end of the computation. The average performance per core is shown in (b) and (d) of figure <ref type="figure" target="#fig_8">3</ref> in GFLOPS. Overall, on the Broadwell CPU, the geometric mean of speed up is 1.25x versus TBLIS, 1.21x versus TCL-MKL, and 1.38x versus TCL-BLIS. On the Skylake CPU, the geometric mean of speedup is 1.23x versus TBLIS, 1.31 versus TCL-MKL, and 1.47 versus TCL-BLIS. We observe that ACMTC as well as TBLIS and TCL achieve lower per-core performance for the multi-core scenario than the single-core case. A significant reason for this is the lower per-core capacity available in the shared L3 cache. Further, we note that the speedup of ACMTC over TBLIS and TCL for the multi-core case is lower for some of the benchmarks and higher for others. A significant reason appears to be differences in the cross-thread interference in the shared L3 cache. For , where ACMTC suffers the greatest loss of the performance relative to TCL for the multi-core scenario, the L3 miss count increases from 6.2 million misses per core to 41 million misses per core on 28 threads for ACMTC, but only rises from around 9 million to 15 million misses for the TCL-MKL and TCL-BLIS versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Time prediction. Even though the primary focus of our modeling is to aid the choice of tile-loop permutation and tile sizes, our model can also be used to predict the execution time. The error rate of our time prediction model was less than 10% in most cases. This time prediction model can be used to evaluate different architectural choices. As an example, consider the contraction: abcdef-degb-gfac (problem size a to ?: 24, <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">24)</ref>. On an Intel Xeon E5-2680  v4 processor (Broadwell), with 128 GB RAM, our data movement model shows that the performance of the above TC is bottlenecked by the Memory to L3 bandwidth. From an architectural standpoint, there are two main ways to alleviate this bottleneck: i) increase Memory bandwidth ii) increase L3 cache size. Our time prediction model predicts that if the Memory bandwidth is increased by 5%, the performance (GFLOPS) will also increase by 5%. It also predicts that increasing memory bandwidth beyond 21% will change the bottleneck to L1-to-Register bandwidth. On the other hand, if we increase the L3 cache size, our model predicts that the performance will not improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>There has been extensive prior work on loop optimization. Polyhedral compilers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> have developed very powerful loop transformation strategies for tiling complex imperfectly nested affine loop computations. Tensor contractions are special cases of affine loop computations and therefore polyhedral compilers can tile code for arbitrary tensor contractions. However the cost models used for guiding choice of loop transformations in polyhedral compilers are constrained to be linear functions, while the tile size optimization problem is inherently a nonlinear optimization problem, as discussed in detail in this paper. The linear cost models used internally in polyhedral compilers are too imprecise to effectively choose the best among the exponential number of permutations of the tiled loops.</p><p>All previously proposed performance modeling approaches in compilers either suffer from imprecision or an exponential blow-up in the number of cases that have to be evaluated in optimizing the tiling configurations (permutations of the tiling loops) and tile size selection.</p><p>The topic of analytical modeling for tile size optimization has been addressed by a number of prior research efforts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. However, previous modeling approaches suffer from one or more of the following shortcomings: a) they use a model of nested tiles that are optimized in some fixed sequence (in contrast to our approach of solving the multi-level tile size selection problem in a coupled fashion); b) they do not model inter-tile reuse. Finally, prior efforts on tile size optimization generally compare performance or speedup of the optimized tiled code with untiled baseline codes; comparisons with the best available manually optimized code or code from state-of-the-art libraries are rarely done. In contrast, we demonstrate the effectiveness of the modeling approach over an extensive public benchmark suite for tensor contractions, by comparing performance with the best-known implementations for those contractions from state-of-the-art libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper we have presented a new methodology for multi-level tile-size optimization for a class of nested loop tensor computations. It is based on observations that enable significant reduction of the search space and an approach to analytical characterization of data volume at each level of a multi-level storage hierarchy along with solution using a constrained optimization solver. The effectiveness of the modeling and optimization approach was demonstrated over a large set of tensor contractions. The approach is more broadly applicable and is being extended to optimize machine learning kernels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>9 DF 12 DF 15 e l s e 16 DMListing 3 :</head><label>91215163</label><figDesc>1 f o r e a c h l o o p i from bottom t o t o p2 i f ( i == 0 ) { // statement level 3 f o r e a c h t e n s o r A 4 DM(A, i) = DF (A, i) e a c h t e n s o r A i f i ? i n d i c e s o f A { 8 DM(A, i) = DM(A, i -1) * ran?e(i) (A, i) = DF (A, i -1) * ran?e(i) (A, i) = DF (A, i -1) 13 i f A DF (A, i -1) &lt; CacheCapacity 14 DM(A, i) = DM(A, i -1) (A, i) = DM(A, i-1) * ran?e(i) Algorithm for computing data movement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Tj1 &lt;Nj, Tk1 &lt;Nk Ni x Nk * Nj/Tj1 Nj x Nk * Ni/Ti1 Ni x Nj Ti1 &lt;Ni, Tj1 &lt;Nj, Tk1 ==Nk Ni x Tk1 Nj x Tk1 * Ni/Ti1 Ni x Nj Ti1 &lt;Ni, Tj1 == Nj, Tk1 &lt;Nk Ni x Nk * Nj/Tj1 Tj1 x Nk * Ni/Ti1 Ni x Tj1 Ti1 &lt;Ni, Tj1 == Nj, Tk1== Nk Ni x Tk1 Tj1 x Tk1 Ni x Tj1 Ti1 == Ni, Tj1 &lt;Nj, Tk1 &lt;Nk Ti1 x Nk * Nj/Tj1 Nj x Nk * Ni/Ti1 Ti1 x Nj Ti1 == Ni, Tj1 &lt;Nj, Tk1 ==Nk Ti1 x Tk1 Nj x Tk1 * Ni/Ti1 Ti1 x Nj Ti1 == Ni, Tj1 == Nj, Tk1 &lt;Nk Ti1 x Nk * Nj/Tj1 Tj1 x Nk * Ni/Ti1 Ti1 x Tj1 Ti1 == Ni, Tj1 == Nj, Tk1== Nk Ti1 x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>arg min t il e-sizes (TotTime(P)) = arg min t il e-sizes ( max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data packing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 f 8 A 9 =</head><label>789</label><figDesc>o r ( i n t k1 = 0 ; k1 &lt; Tk1 ; k1 + + ) _ B u f f e r [ p a c k C o u n t e r ++] A[ i 1 + i 2 + i 3 ] [ k1+k2+k3 ] Listing 5: Psuedocode for packing elements of A corresponding to code in Listing 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>k i n g b u f f e r r e s i d e s h e r e ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Measured Cache Misses for ACMTC, TBLIS, and TCL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Tensor Contraction Performance Comparison on TCCG benchmarks (a) Broadwell single core, (b) Broadwell multicore, (c) Skylake single core, (d) Skylake multi-core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>b:312 c:312 d:312 ab-cad-dcb a:312 b:312 c:312 d:312 abc-acd-db a:312 b:312 c:312 d:312 abc-ad-bdc a:312 b:312 c:312 d:312 abc-adc-bd a:312 b:312 c:312 d:312 abc-adc-db a:312 b:312 c:312 d:312 abc-bda-dc a:312 b:312 c:24 d:312 abcd-aebf-dfce a:72 b:72 c:72 d:72 e:72 f:72 abcd-aebf-fdec a:72 b:72 c:72 d:72 e:72 f:72 abcd-aecf-bfde a:72 b:72 c:72 d:72 e:72 f:72 abcd-aecf-fbed a:72 b:72 c:72 d:72 e:72 f:72 abcd-aedf-bfce a:72 b:72 c:72 d:72 e:72 f:72 abcd-aedf-fbec a:72 b:72 c:72 d:72 e:72 f:72 abcd-aefb-fdce a:72 b:72 c:72 d:72 e:72 f:72 abcd-aefc-fbed a:72 b:72 c:72 d:72 e:72 f:72 abcd-dbea-ec a:72 b:72 c:72 d:72 e:72 f:72 abcd-deca-be a:72 b:72 c:72 d:72 e:72 f:72 abcd-ea-ebcd a:72 b:72 c:72 d:72 e:72 abcd-eafb-fdec a:72 b:72 c:72 d:72 e:72 f:72 abcd-eafc-bfde a:72 b:72 c:72 d:72 e:72 f:72 abcd-eafd-fbec a:72 b:72 c:72 d:72 e:72 f:72 abcd-eb-aecd a:72 b:72 c:72 d:72 e:72 abcd-ebad-ce a:72 b:72 c:24 d:72 e:72 abcd-ec-abed a:72 b:72 c:72 d:72 e:72 abcde-ecbfa-fd a:48 b:32 c:32 d:24 e:48 f:48 abcde-efbad-cf a:48 b:32 c:24 d:32 e:48 f:32 abcde-efcad-bf a:48 b:24 c:32 d:32 e:48 f:32 abcdef-dega-gfbc a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-degb-gfac a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-degc-gfab a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-dfga-gebc a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-dfgb-geac a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-dfgc-geab a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-efga-gdbc a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-efgb-gdac a:24 b:16 c:16 d:24 e:16 f:16 g:24 abcdef-efgc-gdab a:24 b:16 c:16 d:24 e:16 f:16 g:24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Measured Cache Misses and Model Predicted Data Movement</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Computation, Total</cell><cell cols="2">Cache Misses, Computation</cell><cell>Operation Intensity, Total</cell><cell>Operation Intensity, Computation</cell></row><row><cell cols="2">Benchmark</cell><cell cols="2">Cache Actual</cell><cell>Predict</cell><cell>Actual</cell><cell cols="2">Predict</cell><cell>Actual Predict</cell><cell>Actual Predict</cell></row><row><cell cols="3">abcdef-degb-gfac L1</cell><cell cols="2">8.26E+06 8.13E+06</cell><cell cols="3">8.06E+06 7.84E+06 219.43</cell><cell>222.84</cell><cell>224.79</cell><cell>231.23</cell></row><row><cell></cell><cell></cell><cell>L2</cell><cell cols="2">7.08E+06 6.80E+06</cell><cell cols="3">6.96E+06 6.51E+06 256.02</cell><cell>266.41</cell><cell>260.48</cell><cell>278.48</cell></row><row><cell></cell><cell></cell><cell>L3</cell><cell cols="2">4.80E+06 5.05E+06</cell><cell cols="3">4.78E+06 4.76E+06 377.38</cell><cell>358.77</cell><cell>379.18</cell><cell>381.02</cell></row><row><cell cols="2">abcd-aebf-dfce</cell><cell>L1</cell><cell cols="2">2.64E+09 2.47E+09</cell><cell cols="3">2.50E+09 2.44E+09 105.68</cell><cell>112.73</cell><cell>111.37</cell><cell>114.28</cell></row><row><cell></cell><cell></cell><cell>L2</cell><cell cols="2">3.29E+08 3.01E+08</cell><cell cols="3">2.94E+08 2.68E+08 846.04</cell><cell>924.52</cell><cell>949.12</cell><cell>1039.8</cell></row><row><cell></cell><cell></cell><cell>L3</cell><cell cols="2">1.32E+08 1.34E+08</cell><cell cols="3">1.04E+08 1.01E+08 2103.2</cell><cell>2079.22</cell><cell>2676.06 2769.79</cell></row><row><cell cols="2">abcde-efbca-fd</cell><cell>L1</cell><cell cols="2">7.67E+07 3.30E+07</cell><cell cols="3">2.35E+07 2.36E+07 89.66</cell><cell>208.28</cell><cell>293.36</cell><cell>291.6</cell></row><row><cell></cell><cell></cell><cell>L2</cell><cell cols="2">7.33E+07 3.10E+07</cell><cell cols="3">2.17E+07 2.16E+07 93.87</cell><cell>221.86</cell><cell>316.64</cell><cell>318.94</cell></row><row><cell></cell><cell></cell><cell>L3</cell><cell cols="2">2.86E+07 3.07E+07</cell><cell cols="3">2.15E+07 2.12E+07 240.55</cell><cell>224.3</cell><cell>319.42</cell><cell>324</cell></row><row><cell cols="2">abcd-ea-ebcd</cell><cell>L1</cell><cell cols="2">4.36E+07 7.85E+07</cell><cell cols="3">3.49E+07 1.34E+07 88.80</cell><cell>49.32</cell><cell>110.92</cell><cell>288.00</cell></row><row><cell></cell><cell></cell><cell>L2</cell><cell cols="2">1.48E+07 2.18E+07</cell><cell cols="3">7.06E+06 6.83E+06 261.70</cell><cell>177.16</cell><cell>548.39</cell><cell>566.96</cell></row><row><cell></cell><cell></cell><cell>L3</cell><cell cols="2">6.78E+06 1.35E+07</cell><cell cols="3">6.77E+06 6.72E+06 571.15</cell><cell>285.73</cell><cell>571.76</cell><cell>575.94</cell></row><row><cell></cell><cell></cell><cell cols="2">abcd-aebf-fdec</cell><cell></cell><cell></cell><cell></cell><cell>abcdef-degb-gfac</cell></row><row><cell></cell><cell>4.00E+09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7.00E+07</cell></row><row><cell></cell><cell>3.50E+09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.00E+07</cell></row><row><cell>Cache Misses</cell><cell>1.00E+09 3.00E+09 1.50E+09 2.00E+09 2.50E+09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cache Misses</cell><cell>2.00E+07 3.00E+07 4.00E+07 5.00E+07</cell></row><row><cell></cell><cell>5.00E+08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.00E+07</cell></row><row><cell></cell><cell>0.00E+00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00E+00</cell></row><row><cell></cell><cell>acmtc</cell><cell></cell><cell>tblis</cell><cell>tcl</cell><cell></cell><cell></cell><cell>acmtc</cell><cell>tblis</cell><cell>tcl</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L1 L2 L3</cell><cell></cell><cell></cell><cell></cell><cell>L1 L2 L3</cell></row><row><cell></cell><cell></cell><cell cols="2">abcde-ecbfa-fd</cell><cell></cell><cell></cell><cell></cell><cell>abcd-ea-ebcd</cell></row><row><cell></cell><cell>5.00E+08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.00E+08</cell></row><row><cell></cell><cell>4.50E+08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">9.00E+07</cell></row><row><cell></cell><cell>4.00E+08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">8.00E+07</cell></row><row><cell>Cache Misses</cell><cell>1.50E+08 2.00E+08 2.50E+08 3.00E+08 3.50E+08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3.00E+07 4.00E+07 5.00E+07 6.00E+07 7.00E+07</cell></row><row><cell></cell><cell>1.00E+08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2.00E+07</cell></row><row><cell></cell><cell>5.00E+07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.00E+07</cell></row><row><cell></cell><cell>0.00E+00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.00E+00</cell></row><row><cell></cell><cell>acmtc</cell><cell></cell><cell>tblis</cell><cell>tcl</cell><cell></cell><cell></cell><cell>acmtc</cell><cell>tblis</cell><cell>tcl</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L1 L2 L3</cell><cell></cell><cell></cell><cell></cell><cell>L1 L2 L3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>. Measured Bandwidth on Skylake and Broadwell</figDesc><table><row><cell>Measured Bandwidth (byte/cycle)</cell><cell>L1</cell><cell>L2</cell><cell>L3</cell><cell>Memory</cell></row><row><cell>i7-6700K Skylake</cell><cell cols="3">19.36 18.32 12.8</cell><cell>6.4</cell></row><row><cell cols="5">Xeon E5-2680 Broadwell 25.28 19.68 11.44 6.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Tensor Contraction Benchmarks for Performance Evaluation</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the reviewers for their valuable feedback. This work was supported in part by the <rs type="funder">U.S. National Science Foundation</rs> through awards <rs type="grantNumber">1816793</rs>, <rs type="grantNumber">1513120</rs>, and <rs type="grantNumber">CCF-1619303</rs>, and by the <rs type="funder">Louisiana Board of Regents</rs> through the award LEQSF (<rs type="grantNumber">2016-19)-RD-B-03</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3fbaGpW">
					<idno type="grant-number">1816793</idno>
				</org>
				<org type="funding" xml:id="_yFHgVNP">
					<idno type="grant-number">1513120</idno>
				</org>
				<org type="funding" xml:id="_yDB6z4Z">
					<idno type="grant-number">CCF-1619303</idno>
				</org>
				<org type="funding" xml:id="_qdWR85y">
					<idno type="grant-number">2016-19)-RD-B-03</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Artifact Description/Artifact Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUMMARY OF THE EXPERIMENTS REPORTED</head><p>The experiments were performed on a single core of and Intel Core i7-6700K processor, and an Intel Xeon CPU E5-2680 v4 processor. The used GNU GCC 7.3.0 compiler with flag -O3 -std=c99. For TCL we are using MKL 2018 for performing BLAS operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTIFACT AVAILABILITY</head><p>Software Artifact Availability: Some author-created software artifacts are NOT maintained in a public repository or are NOT available under an OSI-approved license.</p><p>Hardware Artifact Availability: There are no author-created hardware artifacts.</p><p>Data Artifact Availability: There are no author-created data artifacts.</p><p>Proprietary Artifacts: None of the associated artifacts, authorcreated or otherwise, are proprietary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTIFACT EVALUATION</head><p>Verification and validation studies: Experiments were performed using the TCCG benchmarks on two systems, a Broadwell processor with Xeon E5-2680 v4, and a Skylake processor with Core i7-6700K. The performance achieved with the new modeling approach was compared with TBLIS and TCL (using both BLIS and MKL kernels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy and precision of timings:</head><p>The timing measurements used omp_get_wtime. The omp_get_wtime function returns a double-precision floating-point value equal to the elapsed wall clock time in seconds since some "time in the past".</p><p>Used manufactured solutions or spectral properties: N/A.</p><p>Quantified the sensitivity of results to initial conditions and/or parameters of the computational environment: N/A Controls, statistics, or other steps taken to make the measurements and analyses robust to variability and unknowns in the system. Each experiment was repeated five times and the average was reported. The variance was under 5%.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Code generation in the polyhedral model is easier than you think</title>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Bastoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>of the 13th International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Couenne: a user&apos;s manual</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Belotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Lehigh University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PLUTO: A Practical and Fully Automatic Polyhedral Program Optimization System</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation</title>
		<meeting>ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>PLDI 08</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tile Size Selection Using Cache Organization and Data Layout</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation (PLDI &apos;95)</title>
		<meeting>the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation (PLDI &apos;95)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An introduction to coupled cluster theory for computational chemists</title>
		<author>
			<persName><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crawford</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">F</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews in computational chemistry</title>
		<imprint>
			<biblScope unit="page" from="33" to="136" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some efficient solutions to the affine scheduling problem. I. One-dimensional time</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Feautrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of parallel programming</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="313" to="347" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anatomy of high-performance matrix multiplication</title>
		<author>
			<persName><forename type="first">Kazushige</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analytical modeling is enough for high-performance BLIS</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">D</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><forename type="middle">M</forename><surname>Igual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Quintana-Orti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-performance tensor contraction without transposition</title>
		<author>
			<persName><forename type="first">Devin</forename><forename type="middle">A</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="C24" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Positivity, Posynomials and Tile Size Selection</title>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM/IEEE Conference on Supercomputing (SC &apos;08)</title>
		<meeting>the 2008 ACM/IEEE Conference on Supercomputing (SC &apos;08)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Article 55, 12 pages</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analytical bounds for optimal tile size selection</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naznin</forename><surname>Fauzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-No?l</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Compiler Construction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Anatomy of high-performance many-threaded matrix multiplication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Van De Geijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">R</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><surname>Van Zee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 28th International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Design of a High-Performance GEMMlike Tensor-Tensor Multiplication</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Bientinesi</surname></persName>
		</author>
		<idno>arXiv:cs.MS, cs.PF/1607.00145</idno>
		<ptr target="http://arxiv.org/abs/1607.00145" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HPTT: A High-Performance Tensor Transposition C++ Library</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Bientinesi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3091966.3091968</idno>
		<ptr target="https://doi.org/10.1145/3091966.3091968" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming</title>
		<meeting>the 4th ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BLIS: A framework for rapidly instantiating BLAS functionality</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Zee</surname></persName>
		</author>
		<author>
			<persName><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Polyhedral parallel code generation for CUDA</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Juega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francky</forename><surname>Tenllado</surname></persName>
		</author>
		<author>
			<persName><surname>Catthoor</surname></persName>
		</author>
		<idno type="DOI">10.1145/2400682.2400713</idno>
		<ptr target="https://doi.org/10.1145/2400682.2400713" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2013-01">2013. Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intel math kernel library</title>
		<author>
			<persName><forename type="first">Endong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computing on the Intel? Xeon Phi?</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="167" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic Creation of Tile Size Selection Models</title>
		<author>
			<persName><forename type="first">Tomofumi</forename><surname>Yuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><forename type="middle">E</forename><surname>Eichenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin O'</forename><surname>Brien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO &apos;10)</title>
		<meeting>the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization (CGO &apos;10)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
