<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep learning for visual understanding: A review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanming</forename><surname>Guo</surname></persName>
							<email>y.guo@liacs.leidenuniv.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIACS Media Lab</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<addrLine>Niels Bohrweg 1</addrLine>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<email>y.liu@liacs.leidenuniv.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIACS Media Lab</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<addrLine>Niels Bohrweg 1</addrLine>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ard</forename><surname>Oerlemans</surname></persName>
							<email>a.oerlemans@vdgsecurity.com</email>
							<affiliation key="aff1">
								<orgName type="institution">VDG Security BV</orgName>
								<address>
									<settlement>Zoetermeer</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Songyang</forename><surname>Lao</surname></persName>
							<email>laosongyang@vip.sina.com</email>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Wu</surname></persName>
							<email>s.wu@liacs.leidenuniv.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIACS Media Lab</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<addrLine>Niels Bohrweg 1</addrLine>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIACS Media Lab</orgName>
								<orgName type="institution">Leiden University</orgName>
								<address>
									<addrLine>Niels Bohrweg 1</addrLine>
									<settlement>Leiden</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Information Systems and Management</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep learning for visual understanding: A review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.neucom.2015.09.116</idno>
					<note type="submission">Received date: 30 April 2015 Revised date: 18 September 2015 Accepted date: 22 September 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>computer vision</term>
					<term>developments</term>
					<term>applications</term>
					<term>trends</term>
					<term>challenges</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning algorithms are a subset of the machine learning algorithms, which aim at discovering multiple levels of distributed representations. Recently, numerous deep learning algorithms have been proposed to solve traditional artificial intelligence problems. This work aims to review the state-of-the-art in deep learning algorithms in computer vision by highlighting the contributions and challenges from over 220 recent research papers. It first gives an overview of various deep learning approaches and their recent developments, and then briefly describes their applications in diverse vision tasks, such as image classification, object detection, image retrieval, semantic segmentation and human pose estimation. Finally, the paper summarizes the future trends and challenges in designing and training deep neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning is a subfield of machine learning which attempts to learn high-level abstractions in data by utilizing hierarchical architectures. It is an emerging approach and has been widely applied in traditional artificial intelligence domains, such as semantic parsing <ref type="bibr" target="#b0">[1]</ref>, transfer learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, natural language processing <ref type="bibr" target="#b3">[4]</ref>, computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and many more. There are mainly three important reasons for the booming of deep learning today: the dramatically increased chip processing abilities (e.g. GPU units), the significantly lowered cost of computing hardware, and the considerable advances in the machine learning algorithms <ref type="bibr" target="#b7">[9]</ref>.</p><p>Various deep learning approaches have been extensively reviewed and discussed in recent years <ref type="bibr" target="#b6">[8]</ref><ref type="bibr" target="#b7">[9]</ref><ref type="bibr" target="#b8">[10]</ref><ref type="bibr" target="#b9">[11]</ref><ref type="bibr" target="#b10">[12]</ref>. Among those Schmidhuber et al. <ref type="bibr" target="#b8">[10]</ref> emphasized the important inspirations and technical contributions in a historical timeline format, while Bengio <ref type="bibr" target="#b9">[11]</ref> examined the challenges of deep learning research and proposed a few forward-looking research directions. Deep networks have been shown to be successful for computer vision tasks because they can extract appropriate features while jointly performing discrimination <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b11">13]</ref>. In recent ImageNet Large Scale Visual Recognition Challenge (ILSVRC) competitions <ref type="bibr" target="#b191">[192]</ref>, deep learning methods have been widely adopted by different researchers and achieved top accuracy scores <ref type="bibr">[7]</ref>. This survey is intended to be useful to general neural computing, computer vision and multimedia researchers who are interested in the state-of-the-art in deep learning in computer vision. It provides an overview of various deep learning algorithms and their applications, especially those that can be applied in the computer vision domain.</p><p>The remainder of this paper is organized as follows:</p><p>In Section 2, we divide the deep learning algorithms into four categories: Convolutional Neural Networks, Restricted Boltzmann Machines, Autoencoder and Sparse Coding. Some well-known models in these categories as well as their developments are listed. We also describe the contributions and limitations for these models in this section. In Section 3, we describe the achievements of deep learning schemes in various computer vision applications, i.e. image classification, object detection, image retrieval, semantic segmentation and human pose estimation. The results on these applications are shown and compared in the pipeline of their commonly used datasets. In Section 4, along with the success deep learning methods have achieved, we also face several challenges when designing and training the deep networks. In this section, we summarize some major challenges for deep learning, together with the inherent trends that might be developed in the future. In Section 5, we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and recent developments</head><p>In recent years, deep learning has been extensively studied in the field of computer vision and as a consequence, a large number of related approaches have emerged. Generally, these methods can be divided into four categories according to the basic method they are derived from: Convolutional Neural Networks (CNNs), Restricted Boltzmann Machines (RBMs), Autoencoder and Sparse Coding.</p><p>The categorization of deep learning methods along with some representative works is shown in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. A categorization of the deep learning methods and their representative works</head><p>In the next four parts, we will briefly review each of these deep learning methods and their most recent developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Neural Networks (CNNs)</head><p>The Convolutional Neural Networks (CNN) is one of the most notable deep learning approaches where multiple layers are trained in a robust manner <ref type="bibr" target="#b15">[17]</ref>. It has been found highly effective and is also the most commonly used in diverse computer vision applications.</p><p>The pipeline of the general CNN architecture is shown in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN-based Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RBM-based Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autoencoder-based Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Coding-based Methods</head><p>Deep Belief Networks <ref type="bibr" target="#b47">[49]</ref> Deep Boltzmann Machines <ref type="bibr" target="#b64">[65]</ref> Deep Energy Models <ref type="bibr" target="#b72">[73]</ref> Sparse Autoencoder <ref type="bibr" target="#b48">[50]</ref> Denoising Autoencoder <ref type="bibr" target="#b84">[85]</ref> Contractive Autoencoder <ref type="bibr" target="#b86">[87]</ref> Sparse Coding SPM <ref type="bibr" target="#b97">[98]</ref> Laplacian Sparse Coding <ref type="bibr" target="#b112">[113]</ref> Local Coordinate Coding <ref type="bibr" target="#b94">[95]</ref> Super-Vector Coding <ref type="bibr" target="#b117">[118]</ref> AlexNet <ref type="bibr" target="#b5">[6]</ref> Clarifai <ref type="bibr" target="#b50">[52]</ref> VGG <ref type="bibr" target="#b29">[31]</ref> GoogLeNet <ref type="bibr" target="#b18">[20]</ref> SPP <ref type="bibr" target="#b24">[26]</ref> Fig. <ref type="figure">2</ref>. The pipeline of the general CNN architecture Generally, a CNN consists of three main neural layers, which are convolutional layers, pooling layers, and fully connected layers. Different kinds of layers play different roles. In Fig. <ref type="figure">2</ref>, a general CNN architecture for image classification <ref type="bibr" target="#b5">[6]</ref> is shown layer-by-layer. There are two stages for training the network: a forward stage and a backward stage. First, the main goal of the forward stage is to represent the input image with the current parameters (weights and bias) in each layer. Then the prediction output is used to compute the loss cost with the ground truth labels. Second, based on the loss cost, the backward stage computes the gradients of each parameter with chain rules. All the parameters are updated based on the gradients, and are prepared for the next forward computation. After sufficient iterations of the forward and backward stages, the network learning can be stopped.</p><p>Next, we will first introduce the functions along with the recent developments of each layer, and then summarize the commonly used training strategies of the networks. Finally, we present several well-known CNN models, derived models, and conclude with the current tendency for using these models in real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Types of layers</head><p>Generally, a CNN is a hierarchical neural network whose convolutional layers alternate with pooling layers, followed by some fully connected layers (see Fig. <ref type="figure">2</ref>). In this section, we will present the function of the three layers and briefly review the recent advances that have appeared in research on those layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Convolutional layers</head><p>In the convolutional layers, a CNN utilizes various kernels to convolve the whole image as well as the intermediate feature maps, generating various feature maps, as shown in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. The operation of the convolutional layer</head><p>There are three main advantages of the convolution operation <ref type="bibr" target="#b17">[19]</ref>: 1) the weight sharing mechanism in the same feature map reduces the number of parameters 2) local connectivity learns correlations among neighboring pixels 3) invariance to the location of the object.</p><p>Due to the benefits introduced by the convolution operation, some well-known research papers use it as a replacement for the fully connected layers to accelerate the learning process <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b173">174]</ref>. One interesting approach of handling the convolutional layers is the Network in Network (NIN) <ref type="bibr" target="#b19">[21]</ref> method, where the main idea is to substitute the conventional convolutional layer with a small multilayer perceptron consisting of multiple fully connected layers with nonlinear activation functions, thereby replacing the linear filters with nonlinear neural networks. This method achieves good results in image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Pooling layers</head><p>Generally, a pooling layer follows a convolutional layer, and can be used to reduce the dimensions of feature maps and network parameters. Similar to convolutional layers, pooling layers are also translation invariant, because their computations take neighboring pixels into account. Average pooling and max pooling are the most commonly used strategies. Fig. <ref type="figure" target="#fig_0">4</ref> gives an example for a max pooling process. For 8x8 feature maps, the output maps reduce to 4x4 dimensions, with a max pooling operator which has size 2x2 and stride 2. For max pooling and average pooling, Boureau et al. <ref type="bibr" target="#b20">[22]</ref> provided a detailed theoretical analysis of their performances. Scherer et al. <ref type="bibr" target="#b21">[23]</ref> further conducted a comparison between the two pooling operations and found that max-pooling can lead to faster convergence, select superior invariant features and improve generalization. In recent years, various fast GPU implementations of CNN variants were presented, most of them utilize max-pooling strategy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">24]</ref>.</p><p>The pooling layers are the most extensively studied among the three layers. There are three wellknown approaches related to the pooling layers, each having different purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Stochastic Pooling</head><p>A drawback of max pooling is that it is sensitive to overfit the training set, making it hard to generalize well to test samples <ref type="bibr" target="#b17">[19]</ref>. Aiming to solve this problem, Zeiler et al. <ref type="bibr" target="#b23">[25]</ref> proposed a stochastic pooling approach which replaces the conventional deterministic pooling operations with a stochastic procedure, by randomly picking the activation within each pooling region according to a multinomial distribution. It is equivalent to standard max pooling but with many copies of the input image, each having small local deformations. This stochastic nature is helpful to prevent the overfitting problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Spatial Pyramid Pooling (SPP)</head><p>Normally, the CNN-based methods require a fixed-size input image. This restriction may reduce the recognition accuracy for images of an arbitrary size. To eliminate this limitation, He et al. <ref type="bibr" target="#b24">[26]</ref> utilized the general CNN architecture but replaced the last pooling layer with a spatial pyramid pooling layer. The spatial pyramid pooling can extract fixed-length representations from arbitrary images (or regions), generating a flexible solution for handling different scales, sizes, aspect ratios, and can be applied in any CNN structure to boost the performance of this structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Def-Pooling</head><p>Handling deformation is a fundamental challenge in computer vision, especially for the object recognition task. Max pooling and average pooling are useful in handling deformation but they are not able to learn the deformation constraint and geometric model of object parts. To deal with deformation more efficiently, Ouyang et al. <ref type="bibr" target="#b25">[27]</ref> introduced a new deformation constrained pooling layer, called def-pooling layer, Because of the different purposes and procedures the pooling strategies are designed for, various pooling strategies could be combined to boost the performance of a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Fully-connected Layers</head><p>Following the last pooling layer in the network as seen in Fig. <ref type="figure">2</ref>, there are several fully-connected layers converting the 2D feature maps into a 1D feature vector, for further feature representation, as seen in Fig. <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. The operation of the fully-connected layer</head><p>Fully-connected layers perform like a traditional neural network and contain about 90% of the parameters in a CNN. It enables us to feed forward the neural network into a vector with a pre-defined length. We could either feed forward the vector into certain number categories for image classification <ref type="bibr" target="#b5">[6]</ref> or take it as a feature vector for follow-up processing <ref type="bibr" target="#b27">[29]</ref>.</p><p>Changing the structure of the fully-connected layer is uncommon, however an example came in the transferred learning approach <ref type="bibr" target="#b28">[30]</ref>, which preserved the parameters learned by ImageNet <ref type="bibr" target="#b5">[6]</ref>, but replaced the last fully-connected layer with two new fully-connected layers to adapt to the new visual recognition tasks.</p><p>The drawback of these layers is that they contain many parameters, which results in a large computational effort for training them. Therefore, a promising and commonly applied direction is to remove these layers or decrease the connections with a certain method. For example, GoogLeNet <ref type="bibr" target="#b18">[20]</ref> designed a deep and wide network while keeping the computational budget constant, by switching from fully connected to sparsely connected architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Training Strategy</head><p>Compared to shallow learning, the advantage of deep learning is that it can build deep architectures to learn more abstract information. However, the large amount of parameters introduced may also lead to another problem: overfitting. Recently, numerous regularization methods have emerged in defense of overfitting, including the stochastic pooling mentioned above. In this section, we will introduce several other regularization techniques that may influence the training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Dropout and DropConnect</head><p>Dropout was proposed by Hinton et al. <ref type="bibr" target="#b36">[38]</ref> and explained in-depth by Baldi et al. <ref type="bibr" target="#b37">[39]</ref>. During each training case, the algorithm will randomly omit half of the feature detectors in order to prevent complex coadaptations on the training data and enhance the generalization ability. This method was further improved in <ref type="bibr" target="#b38">[40]</ref><ref type="bibr" target="#b39">[41]</ref><ref type="bibr" target="#b40">[42]</ref><ref type="bibr" target="#b41">[43]</ref><ref type="bibr" target="#b42">[44]</ref><ref type="bibr" target="#b43">[45]</ref>. Specifically, research by Warde-Farley et al. <ref type="bibr" target="#b43">[45]</ref> analyzed the efficacy of dropouts and suggested that dropout is an extremely effective ensemble learning method.</p><p>One well-known generalization derived from Dropout is called DropConnect <ref type="bibr" target="#b44">[46]</ref>, which randomly drops weights rather than the activations. Experiments showed that it can achieve competitive or even bet-  Data Augmentation When a CNN is applied to visual object recognition, data augmentation is often utilized to generate additional data without introducing extra labeling costs. The well-known AlexNet <ref type="bibr" target="#b5">[6]</ref> employed two distinct forms of data augmentation: the first form of data augmentation consists of generating image translations and horizontal reflections, and the second form consists of altering the intensities of the RGB channels in training images. Howard et al. <ref type="bibr" target="#b45">[47]</ref> took AlexNet as the base model and added additional transformations that improved the translation invariance and color invariance by extending image crops with extra pixels and adding additional color manipulations. This data augmentation method was widely utilized by some of the more recent studies <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b24">26]</ref>. Dosovitskiy et al. <ref type="bibr" target="#b46">[48]</ref> proposed an unsupervised feature learning approach based on data augmentation: it first randomly sampled a set of image patches and declares each of them as a surrogate class, then extended these classes by applying transformations corresponding to translation, scale, color and contrast. Finally, it trained a CNN to discriminate between these surrogate classes. The features learnt by the network showed good results on a variety of classification tasks. Aside from the classical methods such as scaling, rotating and cropping, Wu et al. <ref type="bibr" target="#b159">[160]</ref> further adopted color casting, vignetting and lens distortion techniques, which produced more training examples with broad coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Pre-training and fine-tuning</head><p>Pre-training means to initialize the networks with pre-trained parameters rather than randomly set parameters. It is quite popular in models based on CNNs, due to the advantages that it can accelerate the learning process as well as improve the generalization ability. Erhan et al. <ref type="bibr" target="#b14">[16]</ref> has conducted extensive simulations on the existing algorithms to find why pre-trained networks work better than networks trained in a traditional way. As AlexNet <ref type="bibr" target="#b5">[6]</ref> achieved excellent performance and is released to the public, numerous approaches choose AlexNet trained on ImageNet2012 as their baseline deep model <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b28">30]</ref>, and use fine-tuning of the parameters according to their specific tasks. Nevertheless, there are approaches <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b146">147]</ref> that deliver better performance by training on other models, e.g. Clarifai <ref type="bibr" target="#b50">[52]</ref>, GoogLeNet <ref type="bibr" target="#b18">[20]</ref>, and VGG <ref type="bibr" target="#b29">[31]</ref>.</p><p>Fine-tuning is a crucial stage for refining models to adapt to specific tasks and datasets. In general, fine-tuning requires class labels for the new training dataset, which are used for computing the loss functions. In this case, all layers of the new model will be initialized based on the pre-trained model, such as AlexNet <ref type="bibr" target="#b5">[6]</ref>, except for the last output layer that depends on the number of class labels of the new dataset and will therefore be randomly initialized. However, in some occasions, it is very difficult to obtain the class labels for any new dataset. To address this problem, a similarity learning objective function was proposed to be used as the loss functions without class labels <ref type="bibr" target="#b166">[167]</ref>, so the back-propagation can work normally and allow the model to be refined layer by layer. There are also many research results describing how to transfer the pre-trained model efficiently. A new way is defined to quantify the degree to which a particular layer is general or specific <ref type="bibr" target="#b167">[168]</ref>, namely how well features at that layer transfers from one task to another. They concluded that initializing a network with transferred features from almost any number of layers can give a boost to generalization performance after fine-tuning to a new dataset. In addition to the regularization methods described above, there are also other common methods such as weight decay, weight tying and many more <ref type="bibr" target="#b8">[10]</ref>. Weight decay works by adding an extra term to the cost function to penalize the parameters, preventing them from exactly modeling the training data and therefore helping to generalize to new examples <ref type="bibr" target="#b5">[6]</ref>. Weight tying allows models to learn good representations of the input data by reducing the number of parameters in Convolutional Neural Networks <ref type="bibr" target="#b160">[161]</ref>.</p><p>Another interesting thing to note is that these regularization techniques for training are not mutually exclusive and they can be combined to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">CNN architecture</head><p>With the recent developments of CNN schemes in the computer vision domain, some well-known CNN models have emerged. In this section, we first describe the commonly used CNN models, and then summarize their characteristics in their applications.</p><p>The configurations and the primary contributions of several typical CNN models are listed in Table <ref type="table" target="#tab_0">1</ref>. AlexNet <ref type="bibr" target="#b5">[6]</ref> is a significant CNN architecture, which consists of five convolutional layers and three fully connected layers. After inputting one fixed-size (224x224) image, the network would repeatedly convolve and pool the activations, then forward the results into the fully-connected layers. The network was trained on ImageNet and integrated various regularization techniques, such as data augmentation, dropout, etc. AlexNet won the ILSVRC2012 competition <ref type="bibr" target="#b191">[192]</ref>, and set the tone for the surge of interest in deep convolutional neural network architectures. Nevertheless, there are two major drawbacks of this model: 1) it requires a fixed resolution of the image; 2) there is no clear understanding of why it performs so well.</p><p>In 2013, Zeiler et al. <ref type="bibr" target="#b50">[52]</ref> introduced a novel visualization technique to give insight into the inner workings of the intermediate feature layers. These visualizations enabled them to find architectures that outperform AlexNet <ref type="bibr" target="#b5">[6]</ref> on the ImageNet classification benchmark, and the resulting model, Clarifai, received top performance in the ILSVRC2013 competition.</p><p>As for the requirement of a fixed resolution, He et al. <ref type="bibr" target="#b24">[26]</ref> proposed a new pooling strategy, i.e. spatial pyramid pooling, to eliminate the restriction of the image size. The resulting SPP-net could boost the accuracy of a variety of published CNN architectures despite their different designs.</p><p>In addition to the commonly used configuration of the CNN structure (five convolutional layers plus three fully connected layers), there are also approaches trying to explore deeper networks. In contrast to AlexNet, VGG <ref type="bibr" target="#b29">[31]</ref> increased the depth of the network by adding more convolutional layers and taking advantage of very small convolutional filters in all layers. Similarly, Szegedy et al. <ref type="bibr" target="#b18">[20]</ref> proposed a model, GoogLeNet, which also has quite a deep structure (22 layers) and has achieved leading performance in the ILSVRC2014 competition <ref type="bibr" target="#b191">[192]</ref>.</p><p>Despite the top-tier classification performances that have been achieved by various models, CNNrelated models and applications are not limited to only image classification. Based on these models, new frameworks have been derived to address other challenging tasks, such as object detection, semantic segmentation, etc.</p><p>There are two well-known derived frameworks: RCNN (Regions with CNN features) <ref type="bibr" target="#b27">[29]</ref> and FCN (fully convolutional network) <ref type="bibr" target="#b146">[147]</ref>, mainly designed for object detection and semantic segmentation respectively, as shown in Fig. <ref type="figure" target="#fig_4">7</ref>. The core idea of RCNN is to generate multiple object proposals, extract features from each proposal using a CNN, and then classify each candidate window with a category-specific linear SVM. The "recognition using regions" paradigm received encouraging performance in object detection and has gradually become the general pipeline for recent promising object detection algorithms <ref type="bibr" target="#b177">[178,</ref><ref type="bibr" target="#b178">179,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b185">186]</ref>. However, the performance of RCNN relies too much on the precision of the object location, which may limit its robustness. Besides, the generation and processing of large number of proposals would also decrease its efficiency. Recent developments <ref type="bibr" target="#b177">[178,</ref><ref type="bibr" target="#b178">179,</ref><ref type="bibr" target="#b183">184,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b186">187]</ref> are mainly focused on these two aspects.</p><p>RCNN takes the CNN models as feature extractor and does not make any change to the networks. In contrast, FCN proposes a technique to recast the CNN models as fully convolutional nets. The recasting technique removes the restriction of image resolution and could produce correspondingly-sized output efficiently. Although FCN is proposed mainly for semantic segmentation, the technique could also be utilized in other applications, e.g. image classification <ref type="bibr" target="#b193">[194]</ref>, edge detection <ref type="bibr" target="#b190">[191]</ref> etc.</p><p>Aside from creating various models, the usage of these models also demonstrates several characteristics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Large Networks</head><p>One intuitive idea is to improve the performance of CNNs by increasing their size, which includes increasing the depth (the number of levels) and the width (the number of units at each level) <ref type="bibr" target="#b18">[20]</ref>. Both GoogLeNet <ref type="bibr" target="#b18">[20]</ref> and VGG <ref type="bibr" target="#b29">[31]</ref>, described above, adopted quite large networks, 22 layers and 19 layers respectively, demonstrating that increasing the size is beneficial for image recognition accuracy. AlexNet <ref type="bibr" target="#b5">[6]</ref> Clarifai <ref type="bibr" target="#b50">[52]</ref> SPP-net <ref type="bibr" target="#b24">[26]</ref> VGG <ref type="bibr" target="#b29">[31]</ref> GoogLeNet <ref type="bibr" target="#b18">[20]</ref> Image Classification RCNN <ref type="bibr" target="#b27">[29]</ref> FCN <ref type="bibr" target="#b146">[147]</ref> Object Detection Semantic Segmentation Jointly training multiple networks could lead to better performance than a single one. There are also many researchers <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b192">193]</ref> who designed large networks by combining different deep structures in cascade mode, where the output of the former networks is utilized by the latter ones, as shown in Fig. <ref type="figure" target="#fig_5">8</ref>.</p><p>The cascade architecture can be utilized to handle different tasks, and the function of the prior networks (i.e. the output) may vary with the tasks. For example, Wang et al. <ref type="bibr" target="#b192">[193]</ref> connected two networks for object extraction, and the first network is used for object localization. Therefore, the output is the corresponding coordinates of the object. Sun et al. <ref type="bibr" target="#b31">[33]</ref> proposed three-level carefully designed convolutional networks to detect facial keypoints. The first level provides highly robust initial estimations, while the following two levels fine-tune the initial prediction. Similarly, Ouyang et al. <ref type="bibr" target="#b25">[27]</ref> adopted a multi-stage training scheme proposed by Zeng et al. <ref type="bibr" target="#b30">[32]</ref>, i.e. classifiers at the previous stages jointly work with the classifiers at the current stage to deal with misclassified samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Multiple Networks</head><p>Another tendency in current applications is to combine the results of multiple networks, where each of the networks can execute the task independently, instead of designing a single architecture and jointly training the networks inside to execute the task, as seen in Fig. <ref type="figure">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 9. Combining the results of multiple networks</head><p>Miclut et al. <ref type="bibr" target="#b32">[34]</ref> gave some insight into how we should generate the final results when we have received a set of scores. Prior to the AlexNet <ref type="bibr" target="#b5">[6]</ref>, Ciresan et al. <ref type="bibr" target="#b4">[5]</ref> proposed a method called Multi-Column DNN (MCDNN) which combines several DNN columns and averages their predictions. This model achieved human-competitive results on tasks such as the recognition of handwritten digits or traffic signs. Recently, Ouyang et al. <ref type="bibr" target="#b25">[27]</ref> also conducted an experiment to evaluate the performance of model combination strategies. It learnt 10 models with different settings and combined them in an averaging scheme. Results show that models generated in this way have high diversity and are complementary to each other in improving the detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> Diverse Networks</head><p>Aside from altering the CNN structure, some researchers also attempt to introduce information from other sources, e.g. combining them with shallow structures, integrating contextual information, as illustrated in Fig. <ref type="figure" target="#fig_7">10</ref>. Shallow methods can give additional insight into the problem. In the literature, examples can be found about combining shallow methods and deep learning frameworks <ref type="bibr" target="#b33">[35]</ref>, i.e. take a deep learning method to extract features and input these features to the shallow learning method, e.g. an SVM. One of the most representative and successful algorithms is the RCNN method <ref type="bibr" target="#b27">[29]</ref>, which feeds the highly distinctive CNN features into a SVM for the final object detection task. Besides that, deep CNNs and Fisher Vectors (FV) are complementary <ref type="bibr" target="#b34">[36]</ref> and can also be combined to significantly improve the accuracy of image classification.</p><formula xml:id="formula_0">… Network 1 … Network 2</formula><p>Contextual information is sometimes available for an object detection task, and it is possible to integrate global context information with the information from the bounding box. In the ImageNet Large Scale Visual Recognition Challenge 2014 (ILSVRC 2014), the winning team NUS concatenated all the raw detection scores and combined them with the outputs from a traditional classification framework by context refinement <ref type="bibr" target="#b35">[37]</ref>. Similarly, Ouyang et al. <ref type="bibr" target="#b25">[27]</ref> also took the 1000-class image classification scores as the contextual features for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Restricted Boltzmann Machines (RBMs)</head><p>A Restricted Boltzmann Machine (RBM) is a generative stochastic neural network, and was proposed by <ref type="bibr">Hinton et al. in 1986 [53]</ref>. An RBM is a variant of the Boltzmann Machine, with the restriction that the visible units and hidden units must form a bipartite graph. This restriction allows for more efficient training algorithms, in particular the gradient-based contrastive divergence algorithm <ref type="bibr" target="#b52">[54]</ref>.</p><p>Since the model is a bipartite graph, the hidden units Hand the visible units are conditionally independent. Therefore,</p><formula xml:id="formula_1">P(H| ) = P(𝐻 | )P(𝐻 | ) … P(𝐻 𝑛 | ) (1)</formula><p>In the equation, both H and satisfy Boltzmann distribution. Given input , we can get Hthrough P(H| ). Similarly, we can figure out through P( |H). By adjusting the parameters, we can minimize the difference between and , and the resulting Hwill act as a good feature of .</p><p>Hinton <ref type="bibr" target="#b53">[55]</ref> gave a detailed explanation and provided a practical way to train RBMs. Further work in <ref type="bibr" target="#b54">[56]</ref> discussed the main difficulties of training RBMs, their underlying reasons and proposed a new algorithm, which consists of an adaptive learning rate and an enhanced gradient, to address those difficulties.</p><p>A well-known development of RBM can be found in <ref type="bibr" target="#b55">[57]</ref>: the model approximates the binary units with noisy rectified linear units to preserve information about relative intensities as information travels through multiple layers of feature detectors. The refinement not only functions well in this model, but is also widely employed in various CNN-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">25]</ref>. DBNs have undirected connections at the top two layers which form an RBM and directed connections to the lower layers. DBMs have undirected connections between all layers of the network. DEMs have deterministic hidden units for the lower layers and stochastic hidden units at the top hidden layer <ref type="bibr" target="#b72">[73]</ref>. A summary of these three deep models along with related representative references is provided in Table 2. In the next sections, we will explain these models and describe their applications to computer vision tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Deep Belief Networks (DBNs)</head><p>The Deep Belief Network (DBN), proposed by Hinton <ref type="bibr" target="#b47">[49]</ref>, was a significant advance in deep learning. It is a probabilistic generative model which provides a joint probability distribution over observable data and labels. A DBN first takes advantage of an efficient layer-by-layer greedy learning strategy to initialize the deep network, and then fine-tunes all of the weights jointly with the desired outputs. The greedy learning procedure has two main advantages <ref type="bibr" target="#b56">[58]</ref>: (1) it generates a proper initialization of the network, addressing the difficulty in parameter selection which may result in poor local optima to some extent; (2) the learning procedure is unsupervised and requires no class labels, so it removes the necessity of labeled data for training. However, creating a DBN model is a computationally expensive task that involves training several</p><formula xml:id="formula_2">… … … … v h … … … … v h … … … … v h</formula><p>RBMs, and it is not clear how to approximate maximum-likelihood training to further optimize the model <ref type="bibr" target="#b10">[12]</ref>.</p><p>DBNs successfully focused researchers" attention on deep learning and as a consequence, many variants were created <ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref>. Nair et al. <ref type="bibr" target="#b59">[60]</ref> developed a modified DBN where the top-layer model utilizes a third-order Boltzmann machine for object recognition. The model in <ref type="bibr" target="#b58">[59]</ref> learned a two-layer model of natural images using sparse RBMs, in which the first layer learns local, oriented, edge filters, and the second layer captures a variety of contour features as well as corners and junctions. To improve the robustness against occlusion and random noise, Lee et al. <ref type="bibr" target="#b62">[63]</ref> applied two strategies: one is to take advantage of sparse connections in the first layer of the DBN to regularize the model, and the other is to develop a probabilistic de-noising algorithm.</p><p>When applied to computer vision tasks, a drawback of DBNs is that they do not consider the 2D structure of an input image. To address this problem, the Convolutional Deep Belief Network (CDBN) was introduced <ref type="bibr" target="#b60">[61]</ref>. CDBN utilized the spatial information of neighboring pixels by introducing convolutional RBMs, generating a translation invariant generative model that scales well with high dimensional images. The algorithm was further extended in <ref type="bibr" target="#b63">[64]</ref> and achieved excellent performance in face verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Deep Boltzmann Machines (DBMs)</head><p>The Deep Boltzmann Machine (DBM), proposed by Salakhutdinov el.al <ref type="bibr" target="#b64">[65]</ref>, is another deep learning algorithm where the units are again arranged in layers. Compared to DBNs, whose top two layers form an undirected graphical model and whose lower layers form a directed generative model, the DBM has undirected connections across its structure.</p><p>Like the RBM, the DBM is also a subset of the Boltzmann family. The difference is that the DBM possesses multiple layers of hidden units, with units in odd-numbered layers being conditionally independent of even-numbered layers, and vice versa. Given the visible units, calculating the posterior distribution over the hidden units is no longer tractable, resulting from the interactions between the hidden units. When training the network, a DBM would jointly train all layers of a specific unsupervised model, and instead of maximizing the likelihood directly, the DBM uses a stochastic maximum likelihood (SML) <ref type="bibr" target="#b161">[162]</ref> based algorithm to maximize the lower bound on the likelihood, i.e. performing only one or a few updates using a Markov chain Monte Carlo (MCMC) method between each parameter update. To avoid ending up in poor local minima which leave many hidden units effectively dead, a greedy layer-wise training strategy is also added into the layers when pre-training the DBM network, much in the same way as the DBN <ref type="bibr" target="#b10">[12]</ref>.</p><p>This joint learning has brought promising improvements, both in terms of likelihood and the classification performance of the deep feature learner. However, a crucial disadvantage of DBMs is the time complexity of approximate inference is considerably higher than DBNs, which makes the joint optimization of DBM parameters impractical for large datasets. To increase the efficiency of DBMs, some researchers introduced an approximate inference algorithm <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref>, which utilizes a separate "recognition" model to initialize the values of the latent variables in all layers, thus effectively accelerating the inference.</p><p>There are also many other approaches that aim to improve the effectiveness of DBMs. The improvements can either take place at the pre-training stage <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref> or at the training stage <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>. For example, Montavon et al. <ref type="bibr" target="#b69">[70]</ref> introduced the centering trick to improve the stability of a DBM and made it to be more discriminative and generative. The multi-prediction training scheme <ref type="bibr" target="#b71">[72]</ref> was utilized to jointly train the DBM which outperforms the previous methods in image classification proposed in <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Deep Energy Models (DEMs)</head><p>The Deep Energy Model (DEM), introduced by Ngiam et al. <ref type="bibr" target="#b72">[73]</ref>, is a more recent approach to train deep architectures. Unlike DBNs and DBMs which share the property of having multiple stochastic hidden layers, the DEM just has a single layer of stochastic hidden units for efficient training and inference.</p><p>The model utilizes deep feed forward neural networks to model the energy landscape and is able to train all layers simultaneously. By evaluating the performance on natural images, it demonstrated the joint training of multiple layers yields qualitative and quantitative improvements over greedy layer-wise training.</p><p>Ngiam et al. <ref type="bibr" target="#b72">[73]</ref> used Hybrid Monte Carlo (HMC) to train the model. There are also other options including contrastive divergence, score matching, and others. Similar work can be found in <ref type="bibr" target="#b73">[74]</ref>.</p><p>Although RBMs are not as suitable as CNNs for vision applications, there are also some good examples adopting RBMs to vision tasks. The Shape Boltzmann Machine was proposed by Eslami et al. <ref type="bibr" target="#b168">[169]</ref> to handle the task of modeling binary shape images, which learns high quality probability distributions over object shapes, for both realism of samples from the distribution and generalization to new examples of the same shape class. Kae et al. <ref type="bibr" target="#b169">[170]</ref> combined the CRF and the RBM to model both local and global structure in face segmentation, which has consistently reduced the error in face labeling. A new deep architecture has been presented for phone recognition <ref type="bibr" target="#b170">[171]</ref> that combines a Mean-Covariance RBM feature extraction module with a standard DBN. This approach attacks both the representational inefficiency issues of GMMs and an important limitation of previous work applying DBNs to phone recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Autoencoder</head><p>The autoencoder is a special type of artificial neural network used for learning efficient encodings <ref type="bibr" target="#b74">[75]</ref>. Instead of training the network to predict some target value Y given inputs X, an autoencoder is trained to reconstruct its own inputs X, therefore, the output vectors have the same dimensionality as the input vector. The general process of an autoencoder is shown in Fig. <ref type="figure" target="#fig_9">12</ref>: Generally, a single layer is not able to get the discriminative and representative features of raw data. Researchers now utilize the deep autoencoder, which forwards the code learnt from the previous autoencoder to the next, to accomplish their task. Adds a sparsity penalty to force the representation to be sparse 1. Make the categories to be more separable 2. Make the complex data more meaningful 3. In line with biological vision system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoising Autoencoder [85,86]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovers the correct input from a corrupted version</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More robust to noise</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contractive Autoencoder[87]</head><p>Adds an analytic contractive penalty to the reconstruction error function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Better captures the local directions of variation dictated by the data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saturating autoencoder[14]</head><p>Raises reconstruction error for inputs not near the data manifold Limits the ability to reconstruct inputs which are not near the data manifold</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional autoencoder[90-92]</head><p>Shares weights among all locations in the input, preserving spatial locality Utilizes the 2D image structure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-bias autoencoder [93]</head><p>Utilizes proper shrinkage function to train autoencoders without additional regularization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More powerful in learning representations on data with very high intrinsic dimensionality</head><p>The deep autoencoder was first proposed by Hinton et al. <ref type="bibr" target="#b75">[76]</ref>, and is still extensively studied in recent papers <ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref>. A deep autoencoder is often trained with a variant of back-propagation, e.g. the conjugate gradient method. Though often reasonably effective, this model could become quite ineffective if errors are present in the first few layers. This may cause the network to learn to reconstruct the average of the training encoder code decoder reconstruction error input data. A proper approach to remove this problem is to pre-train the network with initial weights that approximate the final solution <ref type="bibr" target="#b75">[76]</ref>. There are also variants of autoencoder proposed to make the representation as "constant" as possible with respect to the changes in input.</p><p>In Table <ref type="table" target="#tab_2">3</ref>, we list some well-known variants of the autoencoder, and briefly summarize their characteristics and advantages. In the next sections, we describe three important variants: sparse autoencoder, denoising autoencoder and contractive autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Sparse Autoencoder</head><p>A Sparse autoencoder aims to extract sparse features from raw data. The sparsity of the representation can either be achieved by penalizing the hidden unit biases <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b79">80]</ref> or by directly penalizing the output of hidden unit activations <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>.</p><p>Sparse representations have several potential advantages <ref type="bibr" target="#b48">[50]</ref>: 1) using high-dimensional representations increases the likelihood that different categories will be easily separable, just as in the theory of SVMs; 2) sparse representations provide us with a simple interpretation of the complex input data in terms of a number of "parts"; 3) biological vision uses sparse representations in early visual areas <ref type="bibr" target="#b82">[83]</ref>.</p><p>A quite well-known variant of the sparse autoencoder is a nine-layer locally connected sparse autoencoder with pooling and local contrast normalization <ref type="bibr" target="#b83">[84]</ref>. This model allows the system to train a face detector without having to label images as containing a face or not. The resulting feature detector is robust to translation, scaling and out-of-plane rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Denoising Autoencoder</head><p>In order to increase the robustness of the model, Vincent proposed a model called denoising autoencoder (DAE) <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref>, which can recover the correct input from a corrupted version, thus forcing the model to capture the structure of the input distribution. The process of a DAE is shown in Fig. <ref type="figure" target="#fig_10">13</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Contractive Autoencoder</head><p>Contractive Autoencoder (CAE), proposed by Rifai et al. <ref type="bibr" target="#b86">[87]</ref>, followed after the DAE and shared a similar motivation of learning robust representations <ref type="bibr" target="#b10">[12]</ref>. While a DAE makes the whole mapping robust by injecting noise in the training set, a CAE achieves robustness by adding an analytic contractive penalty to the reconstruction error function.</p><p>Although the notable differences between DAE and CAE are stated by Bengio et al. <ref type="bibr" target="#b10">[12]</ref>, Alain et al. <ref type="bibr" target="#b87">[88]</ref> suggested DAE and a form of CAE are closely related to each other: a DAE with small corruption noise can be valued as a type of CAE where the contractive penalty is on the whole reconstruction function rather than just on the encoder. Both DAE and CAE have been successfully used in the Unsupervised and Transfer Learning Challenge <ref type="bibr" target="#b88">[89]</ref>. input corrupted input reconstruction hidden node reconstruct error</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sparse Coding</head><p>The purpose of sparse coding is to learn an over-complete set of basic functions to describe the input data <ref type="bibr" target="#b93">[94]</ref>. There are numerous advantages of sparse coding <ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref>: (1) It can reconstruct the descriptor better by using multiple bases and capturing the correlations between similar descriptors which share bases;</p><p>(2) the sparsity allows the representation to capture salient properties of images; (3) it is in line with the biological visual system, which argues that sparse features of signals are useful for learning; (4) image statistics study shows that image patches are sparse signals; <ref type="bibr" target="#b4">(5)</ref> patterns with sparse features are more linearly separable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Solving the sparse coding equation</head><p>In this subsection, we will briefly describe how to solve the sparse coding problem, i.e. how to get the sparse representation. The general objective function of sparse coding is as below.</p><formula xml:id="formula_3">min 𝐷 𝑇 ∑ min ℎ (𝑡) ( ‖𝑥 (𝑡) − 𝐷ℎ (𝑡) ‖ + 𝜆‖ℎ (𝑡) ‖ ) 𝑇 𝑡=<label>(2)</label></formula><p>The first term of the function is the reconstruction error (𝐷ℎ (𝑡) is the reconstruction of𝑥 (𝑡) ), while the second L1 regularization term is the sparsity penalty. The L1 norm regularization has been verified to lead to sparse representations <ref type="bibr" target="#b98">[99]</ref>. Equation 2 can be solved with a regression method called LASSO (Least Absolute Shrinkage and Selection Operator). It cannot get the analytic solution of the sparse representation. Therefore, solving of the problem normally results in an intractable computation.</p><p>To optimize the sparse coding model, there is an alternating procedure between updating the weights D and inferring the feature activations h of the input given the current setting of the weights. 1. Weight update One commonly used algorithm for updating the weights is called projected gradient algorithm <ref type="bibr" target="#b99">[100]</ref>, which renormalizes each column of the weight matrix right after each update of the traditional Gradient descent algorithm <ref type="bibr" target="#b100">[101]</ref>. The normalization is necessary for the sparsity penalty to have any effect. However, gradient descent using iterative projections often shows slow convergence. In 2007, Lee et al. <ref type="bibr" target="#b101">[102]</ref> derived a Lagrange dual method, which is much more efficient than gradient-based methods. Given a dictionary, the paper further proposed a feature-sign search algorithm to learn the sparse representation. The combination of these two algorithms enabled the performance to be significantly better than the previous ones. However, it cannot efficiently handle very large training sets, or dynamic training data that is changing over time. Thus it inherently accesses the whole training set at each iteration. To address this issue, an online approach <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b103">104]</ref> was proposed for learning dictionaries that processes one element (or a small subset) of the training set at a time. The algorithm then updates the dictionary using block-coordinate descent <ref type="bibr" target="#b104">[105]</ref> with warm restarts, which does not require any learning rate tuning.</p><p>Gregor et al. <ref type="bibr" target="#b105">[106]</ref> tried to accelerate the dictionary learning in another way: it imports the idea of Coordinate Descent algorithm (CoD) which only updates the "most promising" hidden units and therefore leads to dramatic reduction in the number of iterations to reach a given code prediction error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Activation inference</head><p>Given a set of the weights, we need to infer the feature activations. A popular algorithm for sparse coding inference is the Iterative Shrinkage-Thresholding Algorithm (ISTA) <ref type="bibr" target="#b106">[107]</ref>, which takes a gradient step to optimize the reconstruction term, followed by a sparsity term which has a closed form shrinkage operation. Although simple and effective, the algorithm suffers from a severe problem that it converges quite slowly. The problem is partly solved by the Fast Iterative shrinkage-Thresholding Algorithm (FISTA) approach <ref type="bibr" target="#b107">[108]</ref>, which preserves the computational simplicity of ISTA, but converges more quickly due to the introduction of a "momentum" term in the dynamics (the convergence complexity changed from Ot ). Both the ISTA and FISTA inference involve some sort of iterative optimization (i.e. LASSO), which is of high computational complexity. In contrast, Kavukcuoglu et al. <ref type="bibr" target="#b108">[109]</ref> utilized a feedforward network to approximate the sparse codes, which dramatically accelerated the inference process.</p><p>Furthermore, the LASSO optimization stage was replaced by marginal regression in <ref type="bibr" target="#b109">[110]</ref>, effectively scaling up the sparse coding framework to large dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Developments</head><p>As we have briefly stated how to generate the sparse representation given the objective function, in this subsection, we will introduce some well-known algorithms related to sparse coding, in particular those that are used in computer vision tasks. The well-known sparse coding algorithms and relations, along with their contributions and drawbacks are shown in Fig. <ref type="figure" target="#fig_0">14</ref>.</p><p>One representative algorithm for sparse coding is called Sparse coding SPM (ScSPM) <ref type="bibr" target="#b97">[98]</ref>, which is an extension of the Spatial Pyramid Matching (SPM) method <ref type="bibr" target="#b110">[111]</ref>. Unlike the SPM, which uses vector quantization (VQ) for the image representation, ScSPM utilizes sparse coding (SC) followed by multi-scale spatial max pooling. The codebook of SC is an over-complete basis and each feature can activate a small number of them. Compared to VQ, SC receives a much lower reconstruction error due to the less restrictive constraint on the assignment. Coates et al. <ref type="bibr" target="#b111">[112]</ref> further investigated the reasons for the success of SC over VQ in detail. A drawback of ScSPM is that it deals with local features separately, thus ignores the mutual dependence among them, which makes it too sensitive to feature variance, i.e. the sparse codes may vary a lot, even for similar features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 14. The well-known sparse coding algorithms, relations, contributions and drawbacks</head><p>To address this problem, Gao et al. <ref type="bibr" target="#b112">[113]</ref> proposed a Laplacian Sparse Coding (LSC) approach, in which similar features are not only assigned to optimally-selected cluster centers, but that also guarantees the selected cluster centers to be similar. The difference between K-means, Sparse Coding and Laplacian Sparse Coding is shown in Fig. <ref type="figure" target="#fig_12">15</ref>.</p><p>By adding the locality preserving constraint to the objective of sparse coding, the LSC can keep the mutual dependency in the sparse coding procedure. Gao et al. <ref type="bibr" target="#b113">[114]</ref> further raised a Hyper-graph Laplacian Sparse Coding (HLSC) method, which extends the LSC to the case where the similarity among the instances is defined by a hyper graph. Both LSC and HLSC enhance the robustness of sparse coding.</p><p>ScSPM <ref type="bibr" target="#b97">[98]</ref> SPM <ref type="bibr" target="#b9">[11]</ref> SC has less restrictive constraint on the assignment than VQ Ignore the mutual dependence of the local features LSC <ref type="bibr" target="#b112">[113]</ref> Enhance similar features to keep the mutual dependency in the sparse coding HLSC <ref type="bibr" target="#b113">[114]</ref> Define the similarity among the instances by a hyper graph LCC <ref type="bibr" target="#b94">[95]</ref> Enhance the locality by explicitly encouraging the coding to be local LLC <ref type="bibr" target="#b96">[97]</ref> Accelerate the process Time consuming SVC <ref type="bibr" target="#b117">[118]</ref> Enhance the locality by adopting a smoother coding scheme ASGD <ref type="bibr" target="#b118">[119]</ref> State-of-the-art on ImageNet prior to CNNs Another way to address the sensitivity problem is the hierarchical sparse coding method proposed by Yu et al. <ref type="bibr" target="#b114">[115]</ref>. It introduced a two-layer sparse coding model: the first layer encodes individual patches, and the second layer jointly encodes the set of patches that belong to the same group. Therefore, the model leverages the spatial neighborhood structure by modeling the higher-order dependency of patches in the same local region of an image. Besides that, it is a fully automatic method to learn features from the pixel level, rather than for example the hand-designed SIFT feature. The hierarchical sparse coding is utilized in another research <ref type="bibr" target="#b115">[116]</ref> to learn features for images in an unsupervised fashion. The model is further improved by Zeiler et al. <ref type="bibr" target="#b116">[117]</ref>.</p><p>In addition to the sensitivity, another method exists for improving the ScSPM algorithm, by considering the locality. Yu et al. <ref type="bibr" target="#b94">[95]</ref> observed that the ScSPM results tend to be local, i.e. nonzero coefficients are often assigned to bases nearby. As a result of these observations, they suggested a modification to ScSPM, called Local Coordinate Coding (LCC), which explicitly encourages the coding to be local. They also theoretically showed that locality is more important than sparsity. Experiments have shown that locality can enhance sparsity and that sparse coding is helpful for learning only when the codes are local, so it is preferred to let similar data have similar non-zero dimensions in their codes. Although LCC has a computational advantage over classical sparse coding, it still needs to solve the L1-norm optimization problem, which is time-consuming. To accelerate the learning process, a practical coding method called Locality-Constrained Linear Coding (LLC) was introduced <ref type="bibr" target="#b96">[97]</ref>, which can be seen as a fast implementation of LCC that replaces the L1-norm regularization with L2-norm regularization.</p><p>A comparison between VQ, ScSPM and LLC <ref type="bibr" target="#b96">[97]</ref> are shown in Fig. <ref type="figure" target="#fig_13">16</ref>. Besides LLC, there is another model, called super-vector coding (SVC) <ref type="bibr" target="#b117">[118]</ref>, which can also guarantee local sparse coding. Given x, SVC will activate those coordinates associated to the neighborhood of x to achieve the sparse representation. SVC is a simple extension of VQ by expanding VQ in local tangent directions, and is thus a smoother coding scheme.</p><p>A remarkable result is shown in <ref type="bibr" target="#b118">[119]</ref>, in which the proposed averaging stochastic gradient descent (ASGD) scheme combined LCC and SVC algorithm to scale the image classification to large-scale dataset, : Features to be quantized</p><formula xml:id="formula_4">: Visual words #1 #2 #1 #2 #1 #2 node assigned to #1 node assigned to #2 node assigned to #1 &amp; #2</formula><p>and produced state-of-the-art results on ImageNet object recognition tasks prior to the rise of CNN architectures.</p><p>Another well-known smooth coding method is presented in <ref type="bibr" target="#b109">[110]</ref>, called Smooth Sparse Coding (SSC). The method incorporates the neighborhood similarity and temporal information into sparse coding, leading to codes that represent a neighborhood rather than an individual sample and that have lower mean square reconstruction error.</p><p>More recently, He et al. <ref type="bibr" target="#b119">[120]</ref> proposed a new unsupervised feature learning framework, called Deep Sparse Coding (DeepSC), which extends sparse coding to a multi-layer architecture and has the best performance among the sparse coding schemes described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussion</head><p>In order to compare and understand the above four categories of deep learning, we summarize their advantages and disadvantages with respect to diverse properties, as listed in Table <ref type="table" target="#tab_3">4</ref>. There are nine properties in total. In details, 'Generalization' refers to whether the approach has been shown to be effective in diverse media (e.g. text, images, audio) and applications, including speech recognition, visual recognition and so on. "Unsupervised learning" refers to the ability to learn a deep model without supervisory annotation. "Feature learning" is the ability to automatically learn features based on a data set. "Real-time training" and "Real-time prediction" refer to the efficiency of the learning and inferring processes, respectively. "Biological understanding" and "Theoretical justification" represent whether the approach has significant biological underpinnings or theoretical foundations, respectively. "Invariance" refers to whether the approach has been shown to be robust to transformations such as rotation, scale and translation. "Small training set" refers to the ability to learn a deep model using a small number of examples. It is important to note that the table only represents the general current findings and not future possibilities nor specialized niche cases. Note: "Yes" indicates that the category does well in the property; otherwise, they will be marked by "No". The "Yes * " refers to a preliminary or weak ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications and Results</head><p>Deep learning has been widely adopted in various directions of computer vision, such as image classification, object detection, image retrieval and semantic segmentation, and human pose estimation, which are key tasks for image understanding. In this part, we will briefly summarize the developments of deep learning (all of the results are referred from the original papers), especially the CNN based algorithms, in these five areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Classification</head><p>The image classification task consists of labeling input images with a probability of the presence of a particular visual object class <ref type="bibr" target="#b128">[129]</ref>, as is shown in Fig. <ref type="figure" target="#fig_14">17</ref>.</p><p>Prior to deep learning, perhaps the most commonly used methods in image classification were methods based on bags of visual words (BoW) <ref type="bibr" target="#b129">[130]</ref>, which first describes the image as a histogram of quantized visual words, and then feeds the histogram into a classifier (typically an SVM <ref type="bibr" target="#b130">[131]</ref>). This pipeline was based on the orderless statistics, to incorporate spatial geometry into the BoW descriptors. Lazebnik et al. <ref type="bibr" target="#b110">[111]</ref> integrated a spatial pyramid approach into the pipeline, which counts the number of visual words inside a set of image sub-regions instead of the whole region. Thereafter, this pipeline was further improved by importing sparse coding optimization problems to the building of codebooks <ref type="bibr" target="#b131">[132]</ref>, which receives the best performance on the ImageNet 1000-class classification in 2010. Sparse coding is one of the basic algorithms in deep learning, and it is more discriminative than the original hand-designed ones, i.e. HOG <ref type="bibr" target="#b132">[133]</ref> and LBP <ref type="bibr" target="#b133">[134]</ref>.  The approaches based on BoW just concern the zero order statistics (i.e. counts of visual words), discarding a lot of valuable information of the image <ref type="bibr" target="#b128">[129]</ref>. The method introduced by Perronnin et al. <ref type="bibr" target="#b134">[135]</ref> overcame this issue and extracted higher order statistics by employing the Fisher Kernel <ref type="bibr" target="#b135">[136]</ref>, achieving the state-of-the-art image classification result in 2011. In this phase, researchers tend to focus on the higher order statistics, which is the core idea of deep learning. Krizhevsky et al. <ref type="bibr" target="#b5">[6]</ref> represented a turning point for large-scale object recognition when a large CNN was trained on the ImageNet database <ref type="bibr" target="#b136">[137]</ref>, thus proving that CNN could, in addition to handwritten digit recognition <ref type="bibr" target="#b15">[17]</ref>, perform well on natural image classification. The proposed AlexNet won the ILSVRC 2012, with a top-5 error rate of 15.3%, which sparked significant additional activity in CNN research. In Fig. <ref type="figure" target="#fig_15">18</ref>, we present the state-of-the-art results on the ImageNet test dataset since 2012, along with the pipeline of ILSVRC.</p><p>OverFeat <ref type="bibr" target="#b145">[146]</ref> proposed a multiscale and sliding window approach, which could find the optimal scale of the image and fulfill different tasks simultaneously, i.e. classification, localization and detection. Specifically, the algorithm decreased the top-5 test error to 13.6%. Zeiler et al. <ref type="bibr" target="#b50">[52]</ref> introduced a novel visualization technique to give insight into the function of intermediate feature layers and further adjusted a new model, which outperformed AlexNet, reaching 11.7% top-5 error rate, and had top performance at ILSVRC 2013.</p><p>ILSVRC 2014 witnessed the steep growth of deep learning, as most participants utilized CNNs as the basis for their models. Again significant progress had been made in image classification, as the error was almost halved since ILSVRC2013. The SPP-net <ref type="bibr" target="#b24">[26]</ref> model eliminated the restriction of the fixed input image size and could boost the accuracy of a variety of published CNN architectures despite their different designs. Multiple SPP-nets further reduced the top-5 error rate to 8.06% and ranked third in the image classification challenge of ILSVRC 2014. Along with the improvements of the classical CNN model, another characteristic shared by the top-performing models is that the architectures became deeper, as shown by GoogLeNet <ref type="bibr" target="#b18">[20]</ref> (rank 1 in ILSVRC 2014) and VGG <ref type="bibr" target="#b29">[31]</ref> (rank 2 in ILSVRC 2014), which achieved 6.67% and 7.32% respectively.</p><p>Despite the potential capacity possessed by larger models, they also suffered from overfitting and underfitting problems when there is little training data or little training time. To avoid this shortcoming, Wu et al. <ref type="bibr" target="#b159">[160]</ref> developed new strategies, i.e. DeepImage, for data augmentation and usage of multi-scale images. They also built a large supercomputer for deep neural networks and developed a highly optimized parallel algorithm, and the classification result achieved a relative 20% improvement over the previous one with a top-5 error rate of 5.33%. More Recently, He et al. <ref type="bibr" target="#b162">[163]</ref> proposed the Parametric Rectified Linear Unit to generate the traditional rectified activation units and derived a robust initialization method. This scheme led to 4.94% top-5 test error and surpassed human-level performance (5.1%) for the first time. Similar results were achieved by Ioffe et al. <ref type="bibr" target="#b163">[164]</ref>, whose method reached a 4.8% test error by utilizing an ensemble of batch-normalized networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Detection</head><p>Object detection is different from but closely related to an image classification task. For image classification, the whole image is utilized as the input and the class label of objects within the image are estimated. For object detection, besides outputting the information of the presence of a given class, we also need to estimate the position of the instance (or instances), as shown in Fig. <ref type="figure" target="#fig_16">19</ref>. A detection window is regarded as correct if the outputted bounding box has sufficiently large overlap with the ground truth object (usually more than 50%).  <ref type="table" target="#tab_4">5</ref>.</p><p>Before the surge of deep learning, the Deformable Part Model (DPM) <ref type="bibr" target="#b176">[177]</ref> was the most effective method for object detection. It takes advantage of deformable part models and detects objects across all scales and locations on the image in an exhaustive manner. After integrating with some post-processing techniques, i.e. bounding box prediction and context rescoring, the model achieved 29.09% average precision for VOC 2007 test set.</p><p>As deep learning methods (especially the CNN-based methods) had achieved top tier performance on image classification tasks, researchers started to transfer it to the object detection problem. An early deep learning approach for object detection was introduced by Szegedy et al. <ref type="bibr" target="#b120">[121]</ref>. The paper proposed an algorithm, called DetectorNet, which replaced the last layer of AlexNet <ref type="bibr" target="#b5">[6]</ref> with a regression layer. The algorithm captured object location well and achieved competitive results on the VOC2007 test set with the most advanced algorithms at that time. To handle multiple instances of the same object in the image, Deep-MultiBox <ref type="bibr" target="#b147">[148]</ref> also showed a saliency-inspired neural network model. Note: Training data: "07": VOC07 trainval; "12": VOC2 trainval; "07+12": VOC07 trainval union with VOC12 trainval; "07++12": VOC07 trainval and test union with VOC12 trainval; BB: bounding box regression; A/C-Net: approaches based on AlexNet <ref type="bibr" target="#b5">[6]</ref> or Clarifai <ref type="bibr" target="#b50">[52]</ref>; VGG-Net: approaches based on VGG-Net <ref type="bibr" target="#b29">[31]</ref> A general pattern for current successful object detection systems is to generate a large pool of candidate boxes and classify those using CNN features. The most representative approach is the RCNN scheme proposed by Girshick et al. <ref type="bibr" target="#b27">[29]</ref>. It utilizes selective search <ref type="bibr" target="#b151">[152]</ref> to generate object proposals, and extracts the CNN features for each proposal. The features are then fed into an SVM classifier to decide whether the related candidate windows contain the object or not. RCNNs improved the benchmark by a large margin, and became the base model for many other promising algorithms <ref type="bibr" target="#b177">[178,</ref><ref type="bibr" target="#b178">179,</ref><ref type="bibr" target="#b183">184,</ref><ref type="bibr" target="#b184">185,</ref><ref type="bibr" target="#b186">187]</ref>.</p><p>The algorithms derived from RCNNs are mainly divided into two categories: the first category aims to accelerate the training and testing process. Although an RCNN has excellent object detection accuracy, it is computationally intensive because it first warps and then processes each object proposal independently. Consequently, some well-known algorithms which aim to improve its efficiency appeared, such as SPP-net <ref type="bibr" target="#b24">[26]</ref>, FRCN <ref type="bibr" target="#b177">[178]</ref>, RPN <ref type="bibr" target="#b178">[179]</ref>, YOLO <ref type="bibr" target="#b179">[180]</ref> etc. These algorithms detect objects faster, while achieving comparable mAP with state-of-the-art benchmarks.</p><p>The second category is mainly intended to improve the accuracy of RCNNs. The performance of the "recognition using regions" paradigm is highly dependent on the quality of object hypotheses. Currently, there are many object proposal algorithms, such as objectness <ref type="bibr" target="#b150">[151]</ref>, selective search <ref type="bibr" target="#b151">[152]</ref>, categoryindependent object proposals <ref type="bibr" target="#b152">[153]</ref>, BING <ref type="bibr" target="#b153">[154]</ref>, and edge boxes <ref type="bibr" target="#b154">[155]</ref> et al. These schemes are exhaustively evaluated in <ref type="bibr" target="#b155">[156]</ref>. Although those schemes are good at finding rough object positions, they normally could not accurately localize the whole object via a tight bounding box, which forms the largest source of detection error <ref type="bibr" target="#b180">[181,</ref><ref type="bibr" target="#b181">182]</ref>. Therefore, many approaches have emerged that try to correct the poor localizations.</p><p>One important direction of these methods is to combine them with semantic segmentation techniques <ref type="bibr" target="#b182">[183]</ref><ref type="bibr" target="#b183">[184]</ref><ref type="bibr" target="#b184">[185]</ref>. For example, the SDS scheme proposed by Hariharan et al. <ref type="bibr" target="#b183">[184]</ref> utilizes segmentation to maskout the background inside the detection, resulting in improved performance for object detection (from 49.6% to 50.7%, both without bounding box regression). On the other hand, the UDS method <ref type="bibr" target="#b182">[183]</ref> unified the object detection and semantic segmentation process in one framework, by enforcing their consistency and integrating context information, the model demonstrated encouraging performance on both tasks. Similar works come with segDeepM proposed by Zhu et al. <ref type="bibr" target="#b184">[185]</ref> and MR_CNN in <ref type="bibr" target="#b185">[186]</ref>, which also incorporate the segmentation along with additional evidence to boost the accuracy of object detection.</p><p>There are also approaches which attempt to precisely locate the object in other ways. For instance, FGS <ref type="bibr" target="#b186">[187]</ref> addresses the localization problem via two methods: 1) develop a fine-grained search algorithm to iteratively optimize the location; 2) train a CNN classifier with a structured SVM objective to balance between classification and localization. The combination of these methods demonstrates promising performance on two challenging datasets.</p><p>Aside from the efforts in object localization, the NoC framework in <ref type="bibr" target="#b187">[188]</ref> tries to evolve efforts in the object classification step. In place of the commonly used multi-layer perceptron (MLP), it explored different NoC structures to implement the object classifiers.</p><p>It is much cheaper and easier to collect a large amount of image-level labels than it is to collect detection data and label it with precise bounding boxes. Therefore, a major challenge in scaling the object detection is the difficulty of obtaining labeled images for large numbers of categories <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b143">144]</ref>. Hoffman et al. <ref type="bibr" target="#b142">[143]</ref> proposed a Deep Detection Adaption (DDA) algorithm to learn the difference between image classification and object detection, transferring classifiers for categories into detectors, without bounding box annotated data. The method has the potential to enable the detection for thousands of categories which lack bounding box annotations.</p><p>Two other promising, scalable approaches are ConceptLeaner <ref type="bibr" target="#b188">[189]</ref> and BabyLearning <ref type="bibr" target="#b189">[190]</ref>.Both of them can learn accurate concept detectors but without the massive annotation of visual concepts. As collecting weakly labeled images is cheap, ConceptLeaner develops a max-margin hard instance learning algorithm to automatically discover visual concepts from noisy labeled image collections. As a result, it has the potential to learn concepts directly from the web. On the other hand, the BabyLearning <ref type="bibr" target="#b189">[190]</ref> approach simulates a baby"s interaction with the physical world, and can achieve comparable results with state-ofthe-art full-training based approaches with only few samples for each object category, along with large amounts of online unlabeled videos.</p><p>From Table <ref type="table" target="#tab_3">4</ref>, we can also observe several factors that could improve the performance, in addition to the algorithm itself: 1) larger training set; 2) deeper base model; 3) Bounding Box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Retrieval</head><p>Image retrieval aims to find images containing a similar object or scene as in the query image, as illustrated in Fig. <ref type="figure" target="#fig_17">20</ref>.</p><p>The success of AlexNet <ref type="bibr" target="#b5">[6]</ref> suggests that the features emerging in the upper layers of the CNN learned to classify images can serve as good descriptors for image classification. Motivated by this, many recent studies use CNN models for image retrieval tasks <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b165">166,</ref><ref type="bibr" target="#b166">167,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b172">173]</ref>. These studies achieved com-petitive results compared with the traditional methods, such as VLAD and Fisher Vector. In the following paragraphs, we will introduce the main ideas of these CNN based methods. Inspired by Spatial Pyramid Matching, Gong et al. <ref type="bibr" target="#b26">[28]</ref> proposed a kind of "reverse SPM" idea that extracts patches at multiple scales, starting with the whole image, and then pool each scale without regard to spatial information. Then it aggregates local patch responses at the finer scales via VLAD encoding. The orderless nature of VLAD helps to build a more invariant representation. Finally, the original global deep activations are concatenated with the VLAD features for the finer scales to form the new image representation.</p><p>Razavian et al. <ref type="bibr" target="#b165">[166]</ref> used features extracted from the OverFeat network as a generic image representation to tackle the diverse range of vision tasks, including recognition and retrieval. First, it augments the training set by adding cropped and rotated samples. Then for each image, it extracts multiple sub-patches of different sizes at different locations. Each sub-patch is computed for its CNN presentation. The distance between the reference and the query image is set to the average distance of each query sub-patch to the reference image.</p><p>Given the recent successes that deep learning techniques have achieved, the research presented in <ref type="bibr" target="#b166">[167]</ref> attempts to evaluate if deep learning can bridge the semantic gap in content-based image retrieval (CBIR). Their encouraging results reveal that deep CNN models pre-trained on large datasets can be directly used for feature extraction in new CBIR tasks. When being applied for feature representation in a new domain, it was found that similarity learning can further boost the retrieval performance. Further, by retraining the deep models with a classification or similarity learning objective on the new domain, the accuracy can be improved significantly.</p><p>A different approach shown in <ref type="bibr" target="#b171">[172]</ref> is to first extract object-like image patches with a general object detector. Then, one CNN feature is extracted in each object patch with the pre-trained AlexNet model. With many results from their experiments, it is concluded that their method can achieve a significant accuracy improvement with the same space consumption, and with the same time cost it still obtains a higher accuracy.</p><p>Finally, without sliding windows or multiple-scale patches, Babenko et al. <ref type="bibr" target="#b172">[173]</ref> focus on holistic descriptors where the whole image is mapped to a single vector with a CNN model. It found that the best performance is observed not at the very top of the network, but rather at the layer that is two levels below the outputs. An important result is that PCA affects the performance of the CNN much less than the performance of VLADs or Fisher Vectors. Therefore PCA compression works better for CNN features. In Table 6, we show the retrieval results in several public datasets. There is one more interesting problem in CNN features: which layer has the highest impact on the final performance? Some methods extract features in the second fully connected layer <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b171">172]</ref>. In contrast to them, other methods use the first fully connected layer in their CNN model for image representation <ref type="bibr" target="#b165">[166,</ref><ref type="bibr" target="#b172">173]</ref>. Moreover, these choices may change for different datasets <ref type="bibr" target="#b166">[167]</ref>. Thus, we think investigating the characteristics of each layer is still an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semantic Segmentation</head><p>In the past half year, a large number of studies focus on the semantic segmentation task, and yield promising progress. The main reason of their success comes from CNN models, which are capable of tackling the pixel-level predictions with the pre-trained networks on large-scale datasets. Different from imagelevel classification and object-level detection, semantic segmentation requires output masks that have a 2D spatial distribution. As for semantic segmentation, recent and advanced CNN based methods can be summarized as follows:</p><p>(1) Detection-based segmentation. The approach segments images based on the candidate windows outputted from object detection <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b215">216]</ref>. RCNN <ref type="bibr" target="#b27">[29]</ref> and SDS <ref type="bibr" target="#b164">[165]</ref> first generated region proposals for object detection, and then utilized traditional approaches to segment the region and to assign the pixels with the class label from detection. on SDS <ref type="bibr" target="#b164">[165]</ref>, Hariharan et al. <ref type="bibr" target="#b215">[216]</ref> proposed the hypercolumn at each pixel as the vector of activations, and gained large improvement. One disadvantage of detection-based segmentation is the largely additional expense for object detection. Without extracting regions from raw images, Dai et al. <ref type="bibr" target="#b148">[149]</ref> designed a convolutional feature masking (CFM) method to extract proposals directly from the feature maps, which is efficient as the convolutional feature maps only need to be computed once. Even though, the errors caused by proposals and object detection tend to be propagated to the segmentation stage.</p><p>(2) FCN-CRFs based segmentation. In the second one, fully convolutional networks(FCN), replacing the fully connected layers with more convolutional layers, has been a popular strategy and baseline for semantic segmentation <ref type="bibr" target="#b144">[145,</ref><ref type="bibr" target="#b146">147]</ref>.Long et al. <ref type="bibr" target="#b146">[147]</ref> defined a novel architecture that combined semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. DeepLab <ref type="bibr" target="#b144">[145]</ref> proposed a similar FCN model, but also integrated the strength of conditional random fields (CRFs) into FCN for detailed boundary recovery. Instead of using CRFs as a post-processing step, Lin et al. <ref type="bibr" target="#b216">[217]</ref> jointly trains the FCN and CRFs by efficient piecewise training. Likewise, the work in <ref type="bibr" target="#b217">[218]</ref> converted the CRFs as a recurrent neural network (RNN), which can be plugged in as a part of FCN model.</p><p>(3) Weakly supervised annotations. Apart from the advancements in segmentation models, some works are focused on weakly supervised segmentation. Papandreou et al. <ref type="bibr" target="#b218">[219]</ref> studied the more challenging segmentation with weakly annotated training data such as bounding boxes or image-level labels. Likewise, the BoxSup method in <ref type="bibr" target="#b219">[220]</ref> made use of bounding box annotations to estimate segmentation masks, which are used to update network iteratively. These works both showed excellent performance when combining a small number of pixel-level annotated images with a large number of bounding box annotated images.</p><p>We describe the main properties of the above methods and compare their results on PASCAL VOC 2012 val and test set, as listed in Table <ref type="table" target="#tab_6">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Human Pose Estimation</head><p>Human pose estimation aims to estimate the localization of human joints from still images or image sequences, as shown in Fig. <ref type="figure" target="#fig_18">21</ref>. It is very important for a wide range of potential applications, such as video surveillance, human behavior analysis, human-computer interaction (HCI), and is being extensively studied recently <ref type="bibr" target="#b194">[195]</ref><ref type="bibr" target="#b195">[196]</ref><ref type="bibr" target="#b196">[197]</ref><ref type="bibr" target="#b197">[198]</ref><ref type="bibr" target="#b207">[208]</ref><ref type="bibr" target="#b208">[209]</ref><ref type="bibr" target="#b209">[210]</ref><ref type="bibr" target="#b210">[211]</ref><ref type="bibr" target="#b211">[212]</ref><ref type="bibr" target="#b212">[213]</ref><ref type="bibr" target="#b213">[214]</ref>. However, this task is also very challenging because of the great variation of human appearances, complicated backgrounds, as well as many other nuisance factors, such as illumination, viewpoint, scale, etc. In this part, we mainly summarize deep learning schemes to estimate the human articulation from still images, although these schemes could be incorporated with motion features to further boost their performance in videos <ref type="bibr" target="#b194">[195]</ref><ref type="bibr" target="#b195">[196]</ref><ref type="bibr" target="#b196">[197]</ref>. Normally, human pose estimation involves multiple problems such as recognizing people in images, detecting and describing human body parts, and modeling their spatial configuration. Prior to deep learning, the best performing human pose estimation methods were based on body part detectors, i.e. detect and describe the human body part first, and then impose the contextual relations between local parts. One typical part-based approach is pictorial structures <ref type="bibr" target="#b198">[199]</ref>, which takes advantage of a tree model to capture the geometric relations between adjacent parts and has been developed by various well-known part-based methods <ref type="bibr" target="#b199">[200]</ref><ref type="bibr" target="#b200">[201]</ref><ref type="bibr" target="#b201">[202]</ref><ref type="bibr" target="#b202">[203]</ref>.</p><p>As deep learning algorithms can learn high-level features which are more tolerant to the variations of nuisance factors, and have achieved success in various computer vision tasks, they have recently received significant attention from the research community.</p><p>We have summarized the performance of related deep learning algorithms on two commonly used datasets: Frames Labeled In Cinema (FLIC) <ref type="bibr" target="#b203">[204]</ref> and Leeds Sports Pose (LSP) <ref type="bibr" target="#b204">[205]</ref>. FLIC consists of 3987 training images and 1016 test images obtained from popular Hollywood movies, containing people in diverse poses, annotated with upper-body joint labels. LSP and its extension contains 11000 training and 1000 testing images of sports people gathered from Flickr with 14 full body joints annotated. There are two widely accepted evaluation metrics for the evaluation: Percentage of Correct Parts (PCP) <ref type="bibr" target="#b205">[206]</ref>, which measures the rate of correct limb detection, and Percent of Detected Joints (PDJ) <ref type="bibr" target="#b203">[204]</ref>, which measures the rate of correct limb detection.</p><p>In the following, Table <ref type="table" target="#tab_7">8</ref> illustrates the PDJ comparison of various deep learning methods on FLIC dataset, with a normalized distance of 0.05, and Table <ref type="table" target="#tab_8">9</ref> lists out the PCP comparison on LSP dataset.</p><p>In general, deep learning schemes in human pose estimation can be categorized according to the handling manner of input images: holistic processing or part-based processing.</p><p>The holistic processing methods tend to accomplish their task in a global manner, and do not explicitly define a model for each individual part and their spatial relationships. One typical model is called Deep-Pose proposed by Toshev et al. <ref type="bibr" target="#b206">[207]</ref>. This model formulates the human pose estimation method as a joint regression problem and does not explicitly define the graphical model or part detectors for the human pose estimation. More specifically, it utilizes a two-layer architecture: the first layer addresses the ambiguity between body parts in a holistic way and generates the initial pose estimation. The second layer refines the joint locations for the estimation. This model achieved advances on several challenging datasets. However, the holistic-based method suffers from inaccuracy in the high-precision region, since it is difficult to learn direct regression of complex pose vectors from images. The part-based processing methods propose to detect the human body parts individually, followed with a graphic model to incorporate the spatial information. Instead of training the network using the whole image, Chen et al. <ref type="bibr" target="#b207">[208]</ref> utilized the local part patches and background patches to train a DCNN, in order to learn conditional probabilities of the part presence and spatial relationships. By incorporating with graphic models, the algorithm gained promising performance. Moreover, Jain et al. <ref type="bibr" target="#b208">[209]</ref> trained multiple smaller convnets to perform independent binary body-part classification, followed with a higher-level weak spatial model to remove strong outliers and to enforce global pose consistency. Similarly, Tompson et al. <ref type="bibr" target="#b209">[210]</ref> designed multi-resolution ConvNet architectures to perform heat-map likelihood regression for each body part, followed with an implicit graphic model to further promote joint consistency. The model was further extended in <ref type="bibr" target="#b210">[211]</ref>, which argues that the pooling layers in the CNNs would limit spatial localization accuracy and try to recover the precision loss of the pooling process. They especially improve the method from <ref type="bibr" target="#b209">[210]</ref> by adding a carefully designed Spatial Dropout layer, and present a novel network which reuses hidden-layer convolutional features to improve the precision of the spatial locality.</p><p>There are also approaches which suggesting combining both the local part appearance and the holistic view of the parts for more accurate human pose estimation. For example, Ouyang et al. <ref type="bibr" target="#b211">[212]</ref> derived a multi-source deep model from a Deep Belief Net (DBN), which attempts to take advantage of three information sources of human articulation, i.e. mixture type, appearance score and deformation, and combine their high-level representations to learn holistic, high-order human body articulation patterns. On the other hand, Fan et al. <ref type="bibr" target="#b212">[213]</ref> proposed a dual-source convolutional neutral network (DS-CNN) to integrate the holistic and partial view in the CNN framework. It takes part patches and body patches as inputs to combine both local and contextual information for more accurate pose estimation.</p><p>As most of the schemes tend to design new feed-forward architectures, Carreira et al. <ref type="bibr" target="#b213">[214]</ref> introduced a self-correcting model, called Iterative Error Feedback (IEF). This model can encompass rich structure in both input and output spaces by incorporating top-down feedback, and shows promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Trends and Challenges</head><p>Along with the promising performance deep learning has achieved, the research literature has indicated several important challenges as well as the inherent trends, which are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Theoretical Understanding</head><p>Although promising results in addressing computer vision tasks have been achieved by deep learning methods, the underlying theory is not well understood, and there is no clear understanding of which archi-tectures should perform better than others. It is difficult to determine which structure, how many layers, or how many nodes in each layer are proper for a certain task, and it also need specific knowledge to choose sensible values such as the learning rate, the strength of the regularizer, etc. The design of the architecture has historically been determined on an ad-hoc basis. Chu et al. <ref type="bibr" target="#b140">[141]</ref> proposed a theoretical method for determining the optimal number of feature maps. However, this theoretical method only worked for extremely small receptive fields. To better understand the behavior of the well-known CNN architectures, Zeiler et al. <ref type="bibr" target="#b50">[52]</ref> developed a visualization technique that gave insight into the function of intermediate feature layers. By revealing the features in interpretable patterns, it brought further possibilities for better architecture designs. Similar visualization was also studied by Yu et al. <ref type="bibr" target="#b141">[142]</ref>.</p><p>Apart from visualizing the features, RCNN <ref type="bibr" target="#b27">[29]</ref> attempted to discover the learning pattern of CNN. It tested the performance in a layer-by-layer pattern during the training process, and found that the convolutional layers can learn more general features and convey most of the CNN representational capacity, while the top fully-connected layers are domain-specific. In addition to analyzing the CNN features, Agrawal et al. <ref type="bibr" target="#b121">[122]</ref> further investigated the effects of some commonly used strategies on CNN performance, such as finetuning and pre-training, and provided evidence-backed intuitions to apply CNN models to computer vision problems.</p><p>Despite the progress achieved in the theory of deep learning, there is significant room for better understanding in evolving and optimizing the CNN architectures toward improving desirable properties such as invariance and class discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human-level Vision</head><p>Human vision has a remarkable proficiency in computer vision tasks, even in simple visual representations or under changes to geometric transformations, background variation, and occlusion. Human-level vision can refer to either bridging the semantic gap in terms of accuracy or in bringing new insights from studies of the human brain to be integrated into machine learning architectures. Compared with the traditional low-level features, a CNN mimics human brain structure and builds multi-layers activations for midlevel or high-level features. The study in <ref type="bibr" target="#b166">[167]</ref> aimed to evaluate how much retrieval improvement can be achieved by developing deep learning techniques, and whether deep features are a desirable key to bridge the semantic gap in the long term. As seen in Fig. <ref type="figure" target="#fig_15">18</ref>, the image classification error on the ImageNet test set decreases 10%, from 15.3% <ref type="bibr" target="#b5">[6]</ref> in 2012 to 4.82% <ref type="bibr" target="#b163">[164]</ref> in 2015. This promising improvement verifies the efficiency of CNNs. In particular, the result in <ref type="bibr" target="#b163">[164]</ref> has exceeded the accuracy of human raters. However, we cannot conclude that the representational performance of a CNN rivals that of the brain <ref type="bibr" target="#b122">[123]</ref>. For example, it is easy to produce images that are completely unrecognizable to humans, but one state-of-the-art CNN believes it to contain recognizable objects with 99.99% confidence <ref type="bibr" target="#b123">[124]</ref>. This result highlights the difference between human vision and current CNN models, and raises questions about the generality of CNNs in computer vision. The study in <ref type="bibr" target="#b122">[123]</ref> found that, like the IT cortex, recent CNNs could generate similar feature spaces for the same category, and distinct ones for images with different categories. This result indicates that CNNs may provide insight into understanding primate visual processing. In another study <ref type="bibr" target="#b124">[125]</ref>, the authors considered a novel approach for brain decoding for fMRI data by leveraging unlabeled data and multi-layer temporal CNNs, which learned multiple layers of temporal filters and trained powerful brain decoding models. Whether CNN models that rely on computational mechanisms are similar to the primate visual system is yet to be determined, but it has the potential for further improvements by mimicking and incorporating the primate visual system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training with limited data</head><p>Larger models demonstrate more potential capacity and have become the tendency of recent developments. However, the shortage of training data may limit the size and learning ability of such models, especially when it is expensive to obtain fully labeled data. How to overcome the need for enormous amounts of training data and how to train large networks effectively remains to be addressed.</p><p>Currently, there are two commonly used solutions to obtain more training data. The first solution is to generalize more training data from existing data based on various data augmentation schemes, such as scaling, rotating and cropping. On top of these, Wu et al. <ref type="bibr" target="#b159">[160]</ref> further adopted color casting, vignetting and lens distortion techniques, which could produce much more converted training examples with broad cover-age. The second solution is to collect more training data with weak learning algorithms. Recently, there has been a range of articles on learning visual concepts from image search engines <ref type="bibr" target="#b125">[126,</ref><ref type="bibr" target="#b126">127]</ref>. In order to scale up computer vision recognition systems, Zhou et al. <ref type="bibr" target="#b127">[128]</ref> proposed the ConceptLearner approach, which could automatically learn thousands of visual concept detectors from weakly labeled image collections. Besides that, to reduce laborious bounding box annotation costs for object detection, many weaklysupervised approaches have emerged with image-level object-presence labeling <ref type="bibr" target="#b49">[51]</ref>. Nevertheless, it is promising to further develop techniques for generating or collecting more comprehensive training data, which could make the networks learn better features that are robust under various changes, such as geometric transformations, and occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Time complexity</head><p>The early CNNs were seen as a method that required a lot of computational resources and were not candidates for real-time applications. One of the trends is towards developing new architectures which allow running a CNN in real-time. The study in <ref type="bibr" target="#b16">[18]</ref> conducted a series of experiments under constrained time cost, and proposed models that are fast for real-world applications, yet are competitive with existing CNN models. In addition, fixing the time complexity also helps to understand the impacts of factors such as depth, numbers of filters, filter sizes, etc. Another study <ref type="bibr" target="#b13">[15]</ref> eliminated all the redundant computations in the forward and backward propagation in CNNs, which resulted in a speedup of over 1500 times. It has robust flexibility for various CNN models with different designs and structures, and reaches high efficiency because of its GPU implementation. Ren et al. <ref type="bibr" target="#b2">[3]</ref> converted the key operators in deep CNNs to vectorized forms, so that high parallelism can be achieved given basic parallelized matrix-vector operators. They further provided a unified framework for both high-level and low-level vision applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">More Powerful Models</head><p>As deep learning related algorithms have moved forward the-state-of-the-art results of various computer vision tasks by a large margin, it becomes more challenging to make progress on top of that. There might be several directions for more powerful models:</p><p>The first direction is to increase the generalization ability by increasing the size of the networks <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b29">31]</ref>. Larger networks could normally bring higher quality performance, but care should be taken to address the issues this may cause, such as overfitting and the need for a lot of computational resources.</p><p>A second direction is to combine the information from multiple sources. Feature fusion has long been popular and appealing, and this fusion can be categorized in two types. 1) Combine the features of each layer in the network. Different layers may learn different features <ref type="bibr" target="#b27">[29]</ref>. It is promising if we could develop an algorithm to make the features from each layer to be complementary. For example, DeepIndex <ref type="bibr" target="#b156">[157]</ref> proposed to integrate multiple CNN features by multiple inverted indices, including different layers in one model or several layers from distinct models. 2) Combine the features of different types. We can obtain more comprehensive models by integrating with other type of features, such as SIFT. To improve the image retrieval performance, DeepEmbedding <ref type="bibr" target="#b157">[158]</ref> used the SIFT features to build an inverted index structure, and extracted the CNN features from the local patches to enhance the matching strength.</p><p>A third direction towards more powerful models is to design more specific deep networks. Currently, almost all of the CNN-based schemes adopt a shared network for their predictions, which may not be distinctive enough. A promising direction is to train a more specific deep network, i.e. we should focus more on type of object we are interested in. The study in <ref type="bibr" target="#b25">[27]</ref> has verified that object-level annotation is more useful than image-level annotation for object detection. This can be viewed as a kind of specific deep network which just focuses on the object rather than the whole image. Another possible solution is to train different networks for different categories. For instance, <ref type="bibr" target="#b158">[159]</ref> built on the intuition that not all classes are equally difficult to distinguish from a true class label, and designed an initial coarse classifier CNN as well as several fine CNNs. By adopting a coarse-to-fine classification strategy, it achieves state-of-the-art performance on CIFAR100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper presents a comprehensive review of deep learning and develops a categorization scheme to analyze the existing deep learning literature. It divides the deep learning algorithms into four categories according to the basic model they derived from: Convolutional Neural Networks, Restricted Boltzmann Machines, Autoencoder and Sparse Coding. The state-of-the-art approaches of the four classes are discussed and analyzed in detail. For the applications in the computer vision domain, the paper mainly reports the advancements of CNN based schemes, as it is the most extensively utilized and most suitable for images. Most notably, some recent articles have reported inspiring advances showing that some CNN-based algorithms have already exceeded the accuracy of human raters.</p><p>Despite the promising results reported so far, there is significant room for further advances. For example, the underlying theoretical foundation does not yet explain under what conditions they will perform well or outperform other approaches, and how to determine the optimal structure for a certain task. This paper describes these challenges and summarizes the new trends in designing and training deep neural networks, along with several directions that may be further explored in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The operation of the max pooling layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>deep model by learning the deformation of visual patterns. It can substitute the traditional max-pooling layer at any information abstraction level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. A comparison of No-Drop, Dropout and DropConnect networks<ref type="bibr" target="#b44">[46]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. CNN basic models &amp; derived models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Combining deep structures in cascade mode</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Combining a deep network with information from other sources</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>UtilizingFig. 11 .</head><label>11</label><figDesc>Fig.11. The comparison of the three models<ref type="bibr" target="#b72">[73]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The pipeline of an autoencoderDuring the process, the autoencoder is optimized by minimizing the reconstruction error, and the corresponding code is the learned feature.</figDesc><graphic url="image-35.png" coords="14,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Denoising Autoencoder<ref type="bibr" target="#b84">[85]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.15. The difference between K-means, Sparse Coding and Laplacian Sparse Coding<ref type="bibr" target="#b112">[113]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.16. A comparison between VQ, ScSPM, LLC<ref type="bibr" target="#b96">[97]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Image classification examples from AlexNet<ref type="bibr" target="#b5">[6]</ref>. Each image has one ground truth label, followed by the top 5 guesses with probabilities.</figDesc><graphic url="image-55.png" coords="20,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. ImageNet classification results on test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Object detection examples from RCNN [29]. The red box extracts the salient objects contains, the green box contains the prediction score</figDesc><graphic url="image-57.png" coords="21,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Image retrieval examples using CNN features. The left images are querying ones, and the images with green frames in the right represent the positive retrieval candidates</figDesc><graphic url="image-62.png" coords="24,95.00,53.26,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Human pose estimation<ref type="bibr" target="#b214">[215]</ref> </figDesc><graphic url="image-77.png" coords="26,192.91,198.47,209.30,133.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>CNN models and their achievements in ILSVRC classification competitions</figDesc><table><row><cell>Method</cell><cell>Year</cell><cell>Place</cell><cell>Configuration</cell><cell>Contribution</cell></row><row><cell>AlexNet[6]</cell><cell>2012</cell><cell>1 st</cell><cell>Five convolutional layers +</cell><cell>an important CNN architecture</cell></row><row><cell></cell><cell></cell><cell></cell><cell>three fully connected layers</cell><cell>which set the tone for many</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>computer vision researchers</cell></row><row><cell>Clarifai[52]</cell><cell>2013</cell><cell>1 st</cell><cell>Five convolutional layers +</cell><cell>insight into the function of in-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>three fully connected layers</cell><cell>termediate feature layers</cell></row><row><cell>SPP[26]</cell><cell>2014</cell><cell>3 rd</cell><cell>Five convolutional layers +</cell><cell>proposed the "spatial pyramid</cell></row><row><cell></cell><cell></cell><cell></cell><cell>three fully connected layers</cell><cell>pooling" to remove the require-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ment of image resolution</cell></row><row><cell>VGG[31]</cell><cell>2014</cell><cell>2 nd</cell><cell>Thirteen/Fifteen convolu-</cell><cell>a thorough evaluation of net-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tional layers + three fully</cell><cell>works of increasing depth</cell></row><row><cell></cell><cell></cell><cell></cell><cell>connected layers</cell><cell></cell></row><row><cell cols="2">GoogLeNet[20] 2014</cell><cell>1 st</cell><cell>Twenty-one convolutional</cell><cell>increased the depth and width</cell></row><row><cell></cell><cell></cell><cell></cell><cell>layers + one fully connect-</cell><cell>without raising the computational</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ed layer</cell><cell>requirements</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>An overview of representative RBM-based methods</figDesc><table><row><cell>Method</cell><cell>characteristics</cell><cell>advantages</cell><cell>drawbacks</cell><cell>references</cell></row><row><cell>DBN[49]</cell><cell>undirected connec-</cell><cell>1. Properly initializes the network,</cell><cell>Due to the initializa-</cell><cell cols="2">Lee et al. [59, 61-62]</cell></row><row><cell></cell><cell>tions at the top two</cell><cell>which prevents poor local opti-</cell><cell>tion process, it is</cell><cell cols="2">Nair et al. [60]</cell></row><row><cell></cell><cell>layers and directed</cell><cell>ma to some extent</cell><cell>computationally ex-</cell><cell></cell></row><row><cell></cell><cell>connections at the</cell><cell>2. Training is unsupervised, which</cell><cell>pensive to create a</cell><cell></cell></row><row><cell></cell><cell>lower layers</cell><cell>removes the necessity of labeled</cell><cell>DBN model</cell><cell></cell></row><row><cell></cell><cell></cell><cell>data for training</cell><cell></cell><cell></cell></row><row><cell cols="2">DBM[65] undirected connec-</cell><cell>Deals more robustly with ambiguous</cell><cell>The joint optimization</cell><cell cols="2">Hinton et al.[68]</cell></row><row><cell></cell><cell>tions between all</cell><cell>inputs by incorporating top-down</cell><cell>is time-consuming</cell><cell>Cho et al.[69]</cell></row><row><cell></cell><cell>layers of the net-</cell><cell>feedback</cell><cell></cell><cell cols="2">Montavon et al. [70]</cell></row><row><cell></cell><cell>work</cell><cell></cell><cell></cell><cell cols="2">Goodfellow [71-72]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Srivastava</cell><cell>et al.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[175]</cell></row><row><cell cols="2">DEM[73] deterministic hidden</cell><cell>Produces better generative models by</cell><cell>The learnt initial</cell><cell cols="2">Carreira et al. [176]</cell></row><row><cell></cell><cell>units for the lower</cell><cell>allowing the lower layers to adapt to</cell><cell>weight may not have</cell><cell cols="2">Elfwing et al. [74]</cell></row><row><cell></cell><cell>layers and stochas-</cell><cell>the training of higher layers</cell><cell>good convergence</cell><cell></cell></row><row><cell></cell><cell>tic hidden units at</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>the top hidden layer</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Variants of the autoencoder Method Characteristics Advantages Sparse Autoencoder [50,78]</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparisons among four categories of deep learning</figDesc><table><row><cell>Properties</cell><cell>CNNs</cell><cell>RBMs</cell><cell>AutoEncoder</cell><cell>Sparse Coding</cell></row><row><cell>Generalization</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Unsupervised learning</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Feature learning</cell><cell>Yes</cell><cell>Yes *</cell><cell>Yes *</cell><cell>No</cell></row><row><cell>Real-time training</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Real-time prediction</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Biological understanding</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Theoretical justification</cell><cell>Yes *</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Invariance</cell><cell>Yes *</cell><cell>No</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Small training set</cell><cell>Yes *</cell><cell>Yes *</cell><cell>Yes</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Object detection results of the VOC 2007 and VOC 2012 challenges</figDesc><table><row><cell>Methods</cell><cell>Training data</cell><cell cols="4">VOC 2007 mAP(A/C-Net) mAP(VGG-Net) mAP(Alex-Net) mAP(VGG-Net) VOC 2012</cell></row><row><cell>DPM[177]</cell><cell>07</cell><cell>29.09%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DetectorNet[121]</cell><cell>12</cell><cell>30.41%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepMultiBox[148]</cell><cell>12</cell><cell>29.22%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RCNN[29]</cell><cell>07</cell><cell>54.2%</cell><cell>62.2%</cell><cell>-</cell><cell>-</cell></row><row><cell>RCNN[29]+BB</cell><cell>07</cell><cell>58.5%</cell><cell>66%</cell><cell>-</cell><cell>-</cell></row><row><cell>RCNN[29]</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>49.6%</cell><cell>59.2%</cell></row><row><cell>RCNN[29]+BB</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>53.3%</cell><cell>62.4%</cell></row><row><cell>SPP-Net[26]</cell><cell>07</cell><cell>55.2%</cell><cell>60.4%</cell><cell></cell><cell></cell></row><row><cell>SPP-Net[26]</cell><cell>07+12</cell><cell></cell><cell>64.6%</cell><cell></cell><cell></cell></row><row><cell>SPP-Net[26]+BB</cell><cell>07</cell><cell>59.2%</cell><cell>63.1%</cell><cell></cell><cell></cell></row><row><cell>FRCN[178]</cell><cell>07</cell><cell>-</cell><cell>66.9%</cell><cell>-</cell><cell>65.7%</cell></row><row><cell>FRCN[178]</cell><cell>07++12</cell><cell>-</cell><cell>70.0%</cell><cell>-</cell><cell>68.4%</cell></row><row><cell>RPN[179]</cell><cell>07</cell><cell>59.9%</cell><cell>69.9%</cell><cell></cell><cell></cell></row><row><cell>RPN[179]</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell>67%</cell></row><row><cell>RPN[179]</cell><cell>07+12</cell><cell>-</cell><cell>73.2%</cell><cell></cell><cell></cell></row><row><cell>RPN[179]</cell><cell>07++12</cell><cell>-</cell><cell></cell><cell></cell><cell>70.4%</cell></row><row><cell>MR_CNN[186]</cell><cell>07</cell><cell>-</cell><cell>74.9%</cell><cell>-</cell><cell>69.1%</cell></row><row><cell>MR_CNN[186]</cell><cell>12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.7%</cell></row><row><cell>FGS[187]</cell><cell>07</cell><cell></cell><cell>66.5%</cell><cell></cell><cell>-</cell></row><row><cell>FGS[187]+BB</cell><cell>07</cell><cell></cell><cell>68.5%</cell><cell></cell><cell>66.4%</cell></row><row><cell>NoC[188]</cell><cell>07+12</cell><cell>62.9%</cell><cell>71.8%</cell><cell></cell><cell>67.6%</cell></row><row><cell>NoC[188]+BB</cell><cell>07+12</cell><cell></cell><cell>73.3%</cell><cell></cell><cell>68.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Image retrieval results on several datasets</figDesc><table><row><cell>Methods</cell><cell cols="2">Holidays Paris6K</cell><cell>Oxford5K</cell><cell>UKB</cell></row><row><cell>Babenko et al.[173]</cell><cell>74.7</cell><cell>-</cell><cell>55.7</cell><cell>3.43</cell></row><row><cell>Sun et al. [172]</cell><cell>79.0</cell><cell>-</cell><cell>-</cell><cell>3.61</cell></row><row><cell>Gong et al. [28]</cell><cell>80.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Razavian et al. [166]</cell><cell>84.3</cell><cell>79.50</cell><cell>68.0</cell><cell>-(91.1)</cell></row><row><cell>Wan et al. [167]</cell><cell>-</cell><cell>94.7</cell><cell>78.3</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Semantic segmentation results on PASCAL VOC 2012 val and test set</figDesc><table><row><cell>Methods</cell><cell>Train</cell><cell>Val 2012</cell><cell>Test 2012</cell><cell>Descriptions</cell></row><row><cell>SDS[165]</cell><cell>VOC extra</cell><cell>53.9</cell><cell>51.6</cell><cell>region proposals on input images</cell></row><row><cell>CFM[149]</cell><cell>VOC extra</cell><cell>60.9</cell><cell>61.8</cell><cell>region proposals on feature maps</cell></row><row><cell>FCN-8s[147]</cell><cell>VOC extra</cell><cell>-</cell><cell>62.2</cell><cell>one model; three strides</cell></row><row><cell>Hypercolumn[216]</cell><cell>VOC extra</cell><cell>59.0</cell><cell>62.6</cell><cell>region proposals on input images</cell></row><row><cell>DeepLab[145]</cell><cell>VOC extra</cell><cell>63.7</cell><cell>66.4</cell><cell>one model; one stride</cell></row><row><cell>DeepLab-MSc-LargeFOV[145]</cell><cell>VOC extra</cell><cell>68.7</cell><cell>71.6</cell><cell>multi-scale; Field of view</cell></row><row><cell>Piecewise-DCRFs[217]</cell><cell>VOC extra</cell><cell>70.3</cell><cell>70.7</cell><cell>3 scales of image; 5 models</cell></row><row><cell>CRF-RNN[219]</cell><cell>VOC extra</cell><cell>69.6</cell><cell>72.0</cell><cell>Recurrent Neural Networks</cell></row><row><cell>BoxSup[219]</cell><cell>VOC extra+COCO</cell><cell>68.2</cell><cell>71.0</cell><cell>weakly supervised annotations</cell></row><row><cell>Cross-Joint[220]</cell><cell>VOC extra+COCO</cell><cell>71.7</cell><cell>73.9</cell><cell>weakly supervised annotations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>The PDJ comparison on FLIC dataset</figDesc><table><row><cell>PDJ (PCK)</cell><cell>Head</cell><cell>Shoulder</cell><cell>Elbow</cell><cell>Wrist</cell></row><row><cell>Jain et al. [209]</cell><cell>-</cell><cell>42.6</cell><cell>24.1</cell><cell>22.3</cell></row><row><cell>DeepPose[207]</cell><cell>-</cell><cell>-</cell><cell>25.2</cell><cell>26.4</cell></row><row><cell>Chen et al. [208]</cell><cell>-</cell><cell>-</cell><cell>36.5</cell><cell>41.2</cell></row><row><cell>DS-CNN[213]</cell><cell>-</cell><cell>-</cell><cell>30.5</cell><cell>36.5</cell></row><row><cell>Tompson et al. [210]</cell><cell>90.7</cell><cell>70.4</cell><cell>50.2</cell><cell>55.4</cell></row><row><cell>Tompsonet al. [211]</cell><cell>92.6</cell><cell>73</cell><cell>57.1</cell><cell>60.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>The PCP comparison on LSP dataset</figDesc><table><row><cell></cell><cell>Torso</cell><cell>Head</cell><cell>U.arms</cell><cell>L.arms</cell><cell>U.legs</cell><cell>L.legs</cell><cell>Mean</cell></row><row><cell>Ouyang et al.[212]</cell><cell>85.8</cell><cell>83.1</cell><cell>63.3</cell><cell>46.6</cell><cell>76.5</cell><cell>72.2</cell><cell>68.6</cell></row><row><cell>DeepPose[207]</cell><cell>-</cell><cell>-</cell><cell>56</cell><cell>38</cell><cell>77</cell><cell>71</cell><cell>-</cell></row><row><cell>Chen et al. [208]</cell><cell>92.7</cell><cell>87.8</cell><cell>69.2</cell><cell>55.4</cell><cell>82.9</cell><cell>77</cell><cell>75</cell></row><row><cell>DS-CNN[213]</cell><cell>98</cell><cell>85</cell><cell>80</cell><cell>63</cell><cell>90</cell><cell>88</cell><cell>84</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Leiden University, National University of Defense Technology, NWO, Nvidia and the China Scholarship Council (CSC) for their support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yanming Guo received the B.S. degree in information system engineering, the M.S degree in operational research from the National University of Defense Technology, Changsha, China, in 2011 and 2013, respectively. He is currently a Ph.D. in Leiden Institute of Advanced Computer Science (LIACS), Leiden University. His current research interests include image classification, object detection and image retrieval. Ard Oerlemans received the M.S degree and Ph.D. degree from Leiden University in 2004 and 2011, respectively. He has ever been the software engineer, senior software engineer and lead software engineer at VDG Security B.V, as well as the computer vision engineer at Prime Vision. Currently, he is a computer vision engineer at VDG Security B.V. His current research interests include real-time video analysis, video and image retrieval, interest point detection and visual concept detection. He is also interested in biologically motivated computer vision techniques and optimization approaches for large scale image and video analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yu</head><p>Songyang Lao received the B.S. degree in information system engineering and the Ph.D. degree in system engineering from the National University of Defense Technology, Changsha, China, in 1990 and 1996, respectively. He is currently a professor in School of Information System and Management. He was a Visiting Scholar with the Dublin City University, Irish, from 2004 to 2005. His current research interests include image processing and video analysis and human-computer interaction.</p><p>Song Wu received the B.S. degree and the M.S degree of computer science from the Southwest University, Chongqing, China, in 2009 and 2012, respectively. He is currently a Ph.D. in Leiden Institute of Advanced Computer Science (LIACS), Leiden University, Netherlands. His current research interests include image matching, image retrieval and classification.</p><p>Michael S. Lew is co-head of the Imagery and Media Research Cluster at LIACS and director of the LIACS Media Lab. He received his doctorate from University of Illinois at Urbana-Champaign and then became a postdoctoral researcher at Leiden University. One year later he became the first Leiden University Fellow which was a pilot program for tenure track professors. In 2003, he became a tenured associate professor at Leiden University and was invited to serve as a chair full professor in computer science at Tsinghua University (the MIT of China). He has published over 100 peer reviewed papers with three best paper citations in the areas of computer vision, content-based retrieval, and machine learning. Currently (September 2014), he has the most cited paper in the history of the ACM Transactions on Multimedia. In addition, he has the most cited paper from the ACM International Conference on Multimedia Information Retrieval (MIR) 2008 and also from ACM MIR 2010. He has served on the organizing committees for over a dozen ACM and IEEE conferences. He served as the founding the chair of the ACM ICMR steering committee and had served as chair for both the ACM MIR and ACM CIVR steering committees. In addition he is the Editor-in-Chief of the International Journal of Multimedia Information Retrieval (Springer) and a member of the ACM SIGMM Executive Board which is the highest and most influential committee of the SIGMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yanming Guo Yu Liu Ard Oerlemans</head><p>Songyang Lao Song Wu Michael S. Lew</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transfer learning for Latin and Chinese characters with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IJCNN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On Vectorization of Deep Convolutional Neural Networks for Vision Tasks</title>
		<author>
			<persName><forename type="first">J S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI, Foundations and trends® in Machine Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial survey of architectures, algorithms, and applications for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning of representations: Looking forward, Statistical Language and Speech Processing</title>
				<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning invariant feature hierarchies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Saturating auto-encoders</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Convolutional Neural Networks at Constrained Time Cost</title>
				<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchical convolutional deep learning in computer vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
		<respStmt>
			<orgName>New York University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">High-performance neural networks for visual object classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>in: technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-scale Orderless Pooling of Deep Convolutional Activation Features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Committees of deep feedforward networks trained with few data, Pattern Recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Miclut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="736" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep Fisher networks for large-scale image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Contextualizing Object Detection and Classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Technique report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Understanding dropout</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A PAC-Bayesian tutorial with a dropout bound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technique report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fast dropout training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">An empirical analysis of dropout in piecewise linear networks</title>
		<author>
			<persName><forename type="first">Warde-Farley D</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I J</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Some Improvements on Deep Convolutional Neural Network Based Image Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technique report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning by augmenting single images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technique report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName><forename type="first">H O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning and relearning in Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="4" to="6" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On contrastive divergence learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth international workshop on artificial intelligence and statistics</title>
				<meeting>the tenth international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">926</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Enhanced gradient and adaptive learning rate for training restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Ihler</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Rectified linear units improve restricted boltzmann machines</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deep machine learning-a new frontier in artificial intelligence research</title>
		<author>
			<persName><forename type="first">I</forename><surname>Arel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Karnowski</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>research frontier</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="13" to="18" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Sparse deep belief net model for visual area V2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">3D object recognition with deep belief nets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep networks for robust visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Learned-</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep boltzmann machines</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Efficient learning of deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An efficient learning procedure for deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1967" to="2006" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A better way to pretrain deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A two-stage pretraining algorithm for deep boltzmann machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICANN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines and the centering trick</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="621" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Joint Training Deep Boltzmann Machines for Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technique report</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Multi-prediction deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning deep energy models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Expected energy-based restricted Boltzmann machine for classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elfwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Uchibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Autoencoder for words</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W C</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="84" to="96" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A novel sparse auto-encoder for deep unsupervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICACI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Learning Deep Autoencoders without Layer-wise Training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Measuring invariances in deep networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">On optimization methods for deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lahiri</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual invariance with temporal coherence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m">Simoncelli E P. 4.7 Statistical Modeling of Photographic Images</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Contractive auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">What regularized auto-encoders learn from the data generating distribution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Unsupervised and Transfer Learning Challenge: a Deep Learning Approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICANN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Spatio-Temporal Convolutional Sparse Auto-Encoder for Sequence Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">3D object retrieval with stacked local convolutional autoencoder</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Zero-bias autoencoders and the benefits of co-adapting features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by V1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision research</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal 𝓁1-norm solution is also the sparsest solution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Parallel optimization: Theory, algorithms, and applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Censor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Learning representations by back-propagating errors. Cognitive modeling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Pathwise coordinate optimization. The Annals of Applied Statistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Höfling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Nonlinear wavelet image processing: variational problems, compression, and noise removal through wavelet shrinkage. Image Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Vore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="335" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">A fast iterative shrinkage-thresholding algorithm with application to wavelet-based image deblurring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Fast inference in sparse coding algorithms with applications to object recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Smooth sparse coding via marginal regression for learning sparse representations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Local features are not lonely-Laplacian sparse coding for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L T</forename><surname>Chia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Laplacian sparse coding, hypergraph laplacian sparse coding, and applications. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Learning image representations from the pixel level via hierarchical sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Image classification using super-vector coding of local image descriptors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Large-scale image classification: fast feature extraction and svm training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Unsupervised Feature Learning by Deep Sparse Coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>SDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Deep neural networks for object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Deep neural networks rival the representation of primate IT cortex for core visual object recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D L K</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e1003963</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<title level="m">Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Learning Deep Temporal Representations for Brain Decoding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oztekin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Neil</surname></persName>
		</author>
		<title level="m">Extracting visual knowledge from web data</title>
				<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Discovering Visual Concepts from Weakly Labeled Image Collections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><surname>Conceptlearner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Large scale object detection. Department of Cybernetics Faculty of Electrical Engineering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Master</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Czech Technical University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory</title>
				<meeting>the fifth annual workshop on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Large-scale image classification: fast feature extraction and svm training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Analysis of feature maps selection in supervised learning using convolutional neural networks. Advances in Artificial Intelligence</title>
		<author>
			<persName><forename type="first">J L</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyżak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Visualizing and Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">LSDA: Large Scale Detection Through Adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">From Large-Scale Object Classifiers to Large-Scale Object Detectors: An Adaptation Approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName><forename type="first">L C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Scalable Object Detection using Deep Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Recognition using regions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K E A</forename><surname>R R, Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<title level="m" type="main">Category independent object proposals</title>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">How good are detection proposals, really?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">DeepIndex for Accurate and Efficient Image Retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Guoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICMR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Seeing the Big Picture</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Embedding with Contextual Evidences, in: technical report</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<title level="m">Hierarchical Deep Convolutional Neural Network for Image Classification</title>
				<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<title level="m">Deep Image: Scaling up Image Recognition, in: technical report</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates. Stochastics: An International Journal of Probability and Stochastic Processes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="177" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">CNN Features off-the-shelf an Astounding Baseline for Recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Deep Learning for Content-Based Image Retrieval: A Comprehensive Study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">The Shape Boltzmann Machine: a Strong Model of Object Shape</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winn</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Search by Detection-Object-Level Feature for Image Retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICIMCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Neural Codes for Image Retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Is object localization for free? -Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Distributed optimization of deeply nested systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<monogr>
		<title level="m" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Learning to localize detected objects</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Towards unified object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<title level="m" type="main">Object detection via a multi-region &amp; semantic segmentation-aware CNN model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<monogr>
		<title level="m" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Object Detection Networks on Convolutional Feature Maps, in: technical report</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Discovering Visual Concepts from Weakly Labeled Image Collections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><surname>Conceptlearner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Computational baby learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Holistically-Nested Edge Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<editor>ICCV</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">Deep joint task learning for generic object extraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">Multi-Scale Pyramid Pooling for Deep Convolutional Representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Flowing ConvNets for Human Pose Estimation in Videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Human pose recovery by supervised spectral embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Exploring the spatial hierarchy of mixture models for human pose estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">2d articulated human pose estimation and retrieval in (almost) unconstrained still images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<monogr>
		<title level="m" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">Combining Local Appearance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Human Pose Estimation with Iterative Error Feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b214">
	<monogr>
		<title level="m" type="main">Robust human body shape and pose tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3D" to="3D" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">technical report</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><forename type="middle">J</forename><surname>Boxsup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
