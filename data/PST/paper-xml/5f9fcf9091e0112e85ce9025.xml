<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-11-07">7 Nov 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
							<email>tshin1@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
							<email>yrazeghi@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
							<email>rlogan@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
							<email>ericwallace@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
							<email>sameer@uci.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-11-07">7 Nov 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.15980v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fillin-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AUTOPROMPT, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTO-PROMPT, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (LMs) have had exceptional success when adapted to downstream tasks via finetuning <ref type="bibr" target="#b18">(Peters et al., 2018;</ref><ref type="bibr" target="#b5">Devlin et al., 2019)</ref>. Although it is clear that pretraining improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning * First three authors contributed equally.</p><p>process. How can we directly evaluate the knowledge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific?</p><p>Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs' internal representations. A common strategy is to use probing classifiers-shallow classifiers that predict certain attributes using an LMs' representations as features <ref type="bibr" target="#b3">(Conneau et al., 2018;</ref><ref type="bibr" target="#b14">Liu et al., 2019)</ref>. However, probing classifiers require additional learned parameters and are thus susceptible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge <ref type="bibr" target="#b8">(Hewitt and Liang, 2019;</ref><ref type="bibr" target="#b26">Voita and Titov, 2020)</ref>. Attention visualization, another common technique, has a similar failure mode: attention scores may be correlated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations <ref type="bibr" target="#b10">(Jain and Wallace, 2019;</ref><ref type="bibr" target="#b28">Wiegreffe and Pinter, 2019)</ref>. Both probing and attention visualizations also struggle to evaluate knowledge that cannot be represented as simple token-or sequencelevel classification tasks.</p><p>A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, <ref type="bibr" target="#b20">Radford et al. (2019)</ref> frame summarization as a language modeling task by appending "TL;DR:" to the end of an article and then generating from an LM. Similarly, <ref type="bibr" target="#b19">Petroni et al. (2019)</ref> manually reformulate a knowledge base completion task as a cloze test (i.e., a fill-in-the-blank problem). Compared to existing model analysis methods, prompting is noninvasive: it does not introduce large amounts of additional parameters or require direct inspection of a model's representations. Thus prompting pro- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template {sentence}[T][T][T][T][T][P].</head><p>Trigger Tokens atmosphere, alot, dialogue, Clone... Figure <ref type="figure">1</ref>: Illustration of AUTOPROMPT applied to probe a masked language model's (MLM's) ability to perform sentiment analysis. Each input, x inp , is placed into a natural language prompt, x prompt , which contains a single <ref type="bibr">[MASK]</ref> token. The prompt is created using a template, Î», which combines the original input with a set of trigger tokens, x trig . The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|x prompt ), over sets of automatically detected label tokens (Section 2.3).</p><p>vides a lower bound on what the model "knows", and is therefore a more useful analysis tool. However, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more importantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance <ref type="bibr" target="#b11">(Jiang et al., 2020)</ref>. Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool.</p><p>In this paper, we introduce AUTOPROMPT-an automated method for generating prompts for any task, illustrated in Figure <ref type="figure">1</ref>. Given a task, e.g., sentiment analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in <ref type="bibr" target="#b10">Wallace et al. (2019)</ref>. The LM predictions for the prompt are converted to class probabilities by marginalizing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier.</p><p>We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO-PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks-a properly-prompted RoBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model <ref type="bibr" target="#b18">(Peters et al., 2018)</ref>), and 69% accuracy on a balanced variant of the SICK-E dataset <ref type="bibr" target="#b16">(Marelli et al., 2014)</ref>. Next, we apply AUTOPROMPT to the fact retrieval tasks of LAMA <ref type="bibr" target="#b19">(Petroni et al., 2019)</ref>, where we are able to construct prompts that more effectively elicit MLM's factual knowledge than existing prompts generated using manual and corpusmining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best singleprompt result of 34.1% <ref type="bibr" target="#b11">(Jiang et al., 2020)</ref>. We also introduce a variant of this task, similar to relation extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified.</p><p>Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU-TOPROMPT achieves higher average-and worstcase accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of AUTOPROMPT</head><p>A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.</p><p>However, writing prompts is not only time consuming, but it is not clear that the same phrasing will be effective for every model, nor is it clear what criteria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge. 1 An illustration of AUTOPROMPT is provided in Figure <ref type="figure">1</ref>. The prompt is constructed by taking the original task inputs-a collection of one or more sequences of tokens (e.g., the review in Figure <ref type="figure">1</ref>)-and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled training data to construct prompts, and how it uses the output of the MLM as a prediction for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background and Notation</head><p>For the purpose of prompt construction, we distinguish the original task inputs x inp (e.g., the review in Figure <ref type="figure">1</ref>, "a real joy.") from the prompt x prompt (e.g., "a real joy. atmosphere alot dialogue Clone totally <ref type="bibr">[MASK]</ref>.") that is fed into the MLM. The mapping from x inp to x prompt is performed using a template, Î». This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (denoted by <ref type="bibr">[P]</ref> in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|x prompt ) describing which tokens most likely fill in the blank.</p><p>If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class labels. However, for tasks such as sentiment analysis, there may be a set of label tokens V y that correspond to a particular label y. For example, in Figure <ref type="figure">1</ref>, "Cris", "marvelous", and "philanthrop" all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1 Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens:</p><formula xml:id="formula_0">p(y|x prompt ) = wâVy p([MASK] = w|x prompt ) (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gradient-Based Prompt Search</head><p>So far, we have shown how to reformulate a classification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on <ref type="bibr" target="#b10">Wallace et al. (2019)</ref>. The idea is to add a number of "trigger" tokens that are shared across all prompts (denoted by <ref type="bibr">[T]</ref> in the example template in Figure <ref type="figure">1</ref>). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equation (1)) over batches of examples.</p><p>Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token x (j) trig with another token w â V. Then we identify a candidate set V cand of the top-k tokens estimated to cause the greatest increase:</p><formula xml:id="formula_1">V cand = top-k wâV w T in â log p(y|x prompt )<label>(2)</label></formula><p>where w in is the input embedding of w, and the gradient is taken with respect to the input embedding of x (j)</p><p>trig . Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as computing the LM output projection). For each candidate in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step-this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automating Label Token Selection</head><p>While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involving more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens V y . In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input:</p><formula xml:id="formula_2">h = Transformer enc ( x)<label>(3)</label></formula><p>We write the output of this classifier as:</p><formula xml:id="formula_3">p(y|h (i) ) â exp(h (i) â¢ y + Î² y )<label>(4)</label></formula><p>where y and Î² y are the learned weight and bias terms for the label y, and i represents the index of the [MASK] token.</p><p>In the second step, we substitute h (i) with the MLM's output word embeddings w out to obtain a score s(y, w) = p(y|w out ). Intuitively, because w out â¢ h and y â¢ h are large for words and labels that are relevant to a particular context, s w â exp(w out â¢ y + Î² y ) should be large for words that are typically associated with a given label. The sets of label tokens are then constructed from the k-highest scoring words:</p><formula xml:id="formula_4">V y = top-k wâV [s(y, w)]</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Relation to Other Prompting Methods</head><p>Our work fits into a body of work that probes language model's knowledge via prompts. Previous works have used manually defined prompts to study an LM's ability to perform: commonsense reasoning <ref type="bibr" target="#b25">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b12">Kwon et al., 2019;</ref><ref type="bibr" target="#b22">Shwartz et al., 2020)</ref>, question answering <ref type="bibr" target="#b13">(Lewis et al., 2019)</ref>, fact recall <ref type="bibr" target="#b19">(Petroni et al., 2019;</ref><ref type="bibr" target="#b11">Jiang et al., 2020;</ref><ref type="bibr" target="#b0">Bouraoui et al., 2019)</ref>, summarization <ref type="bibr" target="#b20">(Radford et al., 2019)</ref>, and other supervised tasks <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>. Schick and SchÃ¼tze (2020) use manually constructed prompts in conjunction with semi-supervised learning for fewshot learning. We instead automatically create prompts for any task, which leads to higher accuracy and opens up new phenomena to analyze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Setup</head><p>In the following sections, we apply AUTOPROMPT to probe BERT BASE<ref type="foot" target="#foot_0">2</ref> (110M parameters) and RoBERTa LARGE 's (355M parameters) knowledge of the following tasks: sentiment analysis, natural language inference (NLI), fact retrieval, and relation extraction. We use the PyTorch implementations and pretrained weights provided by the transformers Python library <ref type="bibr" target="#b29">(Wolf et al., 2019)</ref>. For sentiment analysis and NLI, we find label tokens using the logistic-regression-based heuristic described in Section 2.3. For fact retrieval and relation extraction, we skip this step as the labels (entities) directly correspond to tokens in the vocabulary. For all tasks, we perform the prompt search described in Section 2.2 for multiple iterations. In each iteration, we use a batch of training data to identify the candidate set V cand of replacement trigger tokens. We then evaluate the label likelihoods of the updated prompts on a separate batch of data, and we retain the best trigger token in the next iteration of the search. At the end of every iteration, we measure the label likelihood on withheld development data, and return the best prompt found during the entire search as the final output. Performance is evaluated using the appropriate task-specific metrics-e.g., accuracy for sentiment analysis and NLI, and precision@k for fact retrieval-on a separate withheld test set.</p><p>Our AUTOPROMPT implementation is publicly available at http://ucinlp.github.io/autoprompt, and supports prompt generation for pretrained models in the HuggingFace transformers library <ref type="bibr" target="#b29">(Wolf et al., 2019)</ref> on arbitrary datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentiment Analysis</head><p>Sentiment analysis is a fundamental task in NLP, both for natural language understanding research and real-world applications. It is also difficult to probe the extent to which MLMs understand sentiment without finetuning.</p><p>Setup We apply our method to convert instances from the binary Stanford Sentiment Treebank <ref type="bibr" target="#b23">(Socher et al., 2013</ref>, SST-2) into prompts, using the standard train/test splits. We find label tokens using a prompt based on the template in Table 3. For our gradient-based prompt search, we perform a grid search over the following hyperparameters: <ref type="foot" target="#foot_1">3</ref> All prompts are initialized with the same template used to find the label set.</p><formula xml:id="formula_5">|V cand | â {10, 100}, |V y | â {1, 3, 5}, |x trig | â [3, 6].</formula><p>We also construct a prompt manually (before automated prompts are generated, to avoid bias) based on the intuition that SST-2 is comprised of movie reviews. We use "{sentence} this movie was <ref type="bibr">[P]</ref>." as the template, and use "terrible" and "fantastic" for the negative and positive label tokens, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We show results in Table <ref type="table" target="#tab_0">1</ref>, along with reference scores from the GLUE <ref type="bibr" target="#b27">(Wang et al., 2019)</ref> SST-2 leaderboard, and scores for a linear probe trained over the elementwise average of the LM token representations. Prompts generated by AUTOPROMPT reveal that both BERT and RoBERTa have a strong knowledge of sentiment analysis: without any finetuning, BERT performs comparably to a supervised BiLSTM, and RoBERTa achieves an accuracy on-par with finetuned BERT and ELMo models. In addition, we observe that our automatically constructed prompts are more effective than manual prompts, and that they are difficult to construct using human intuition: the best template for RoBERTa is "{sentence} atmosphere alot dialogue Clone totally <ref type="bibr">[P]</ref>." We include results on the effect of the AUTOPROMPT hyperparameters in Appendix A.</p><p>Accuracy in Low-Data Settings Although the goal of AUTOPROMPT is to probe a model's knowledge, we also find that it may be a viable alternative to finetuning in the low-data regime. To show this, we measure the development set accuracy of AU-TOPROMPT prompts when using random subsets of 10, 100, and 1000 instances from the training data. We run our prompt  We observe that while finetuning outperforms AUTOPROMPT on sentiment analysis, AUTO-PROMPT can perform better than finetuning on NLI. Notably, AUTOPROMPT elicits better average performance from both BERT and RoBERTa given only 10 training examples. Furthermore, results for RoBERTa are more stable across all sample sizes whereas finetuning can result in "failed runs" (consistent with <ref type="bibr" target="#b6">Dodge et al. 2020)</ref>. This behavior in the low-data regime is an interesting phenomenon, and suggests that there are barriers that MLMs must surmount when they are converted to finetuned classifiers that are not encountered when the task is presented as masked language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Natural Language Inference</head><p>To evaluate the semantic understanding of MLMs, we experiment on Natural Language Inference Results Table <ref type="table">2</ref> shows that AUTOPROMPT considerably outperforms the majority baseline in all experiments. For example, on the 2-way SICK-E dataset, AUTOPROMPT is comparable to a supervised finetuned BERT. We also test linear probeslinear classifiers trained on top of frozen MLM representations with average pooling -and find AUTOPROMPT has comparable or higher accuracy, despite linear probes being susceptible to false positives. Overall, these results demonstrate that both BERT and RoBERTa have some inherent knowledge of natural language inference. We also examine the efficacy of AUTOPROMPT in the low-data regime (using the same procedure as SST-2) on the unbiased 3-way SICK-E data. The results in Figure <ref type="figure" target="#fig_2">2</ref> show that AUTOPROMPT performs on par with finetuned BERT and significantly better than finetuned RoBERTa in low data settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLMs Excel on Contradiction</head><p>We find that the label tokens are more interpretable for con-tradiction compared to entailment or neutral (examples in Table <ref type="table">3</ref>). We investigate if this hurts the model performance on entailment and neutral classes. We measure the precision for each label in the 3-way balanced SICK-E dataset. BERT achieves 74.9%, 54.4%, and 36.8% precision for contradiction, entailment, and neutral cases, respectively, while RoBERTa obtains 84.9%, 65.1%, and 57.3%. These results suggest that AUTOPROMPT may be more accurate for concepts that can be easily expressed using natural label tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Fact Retrieval</head><p>An important question is whether pretrained MLMs know facts about real-world entities. The LAMA dataset <ref type="bibr" target="#b19">(Petroni et al., 2019)</ref> evaluates this using cloze tests that consist of (sub, rel, obj) triples, e.g. <ref type="bibr">(Obama, bornIn, Hawaii)</ref>, and manually created prompts with missing objects, e.g. "Obama was born in <ref type="bibr">[MASK]</ref>.". LPAQA <ref type="bibr" target="#b11">(Jiang et al., 2020)</ref> extends this idea by systematically creating prompts that are generated by mining Wikipedia, paraphrasing, and crowdsourcing. In this section, we use the same cloze-style setup but automatically generate prompts in order to better evaluate the factual knowledge of MLMs. We compare our approach against LAMA and LPAQA, which are explicitly designed for the task of fact retrieval.</p><p>Setup We reformulate fact retrieval by mapping (sub,rel,obj) triples to a prompt using the template "{sub}[T]. . . <ref type="bibr">[T][P]</ref>.", where the trigger tokens are specific to the relation rel and the correct object obj is the label token. We use the original test set from LAMA <ref type="bibr" target="#b19">(Petroni et al., 2019)</ref>, henceforth Original. To collect training data for AUTOPROMPT, we gather at most 1000 facts for each of the 41 relations in LAMA from the T-REx dataset <ref type="bibr" target="#b7">(ElSahar et al., 2018)</ref>. For the relations that still have less than 1000 samples, we gather extra facts straight from Wikidata. We ensure that none of the T-REx triples are present in the test set, and we split the data 80-20 into train and development sets. Moreover, because the collected T-REx data is from a slightly different distribution than the LAMA test set, we also consider a separate evaluation where we split the T-REx triples into a 60-20-20 train/dev/test split and evaluate on the test set. This T-REx dataset is used to measure the performance of our prompts when the train and test data is from the same distribution. Table <ref type="table">3</ref>: Example Prompts by AUTOPROMPT for each task. On the left, we show the prompt template, which combines the input, a number of trigger tokens [T], and a prediction token [P]. For classification tasks (sentiment analysis and NLI), we make predictions by summing the model's probability for a number of automatically selected label tokens. For fact retrieval and relation extraction, we take the most likely token predicted by the model.</p><p>We use AUTOPROMPT with 5 or 7 tokens, and select the search parameters using the T-REx development set. We prevent proper nouns and tokens that appear as gold objects in the training data from being selected as trigger tokens. This is done to prevent AUTOPROMPT from "cheating" by embedding common answers inside the prompt. To evaluate, we observe the rank of the true object in label token distribution of the MLM, and use standard ranking metrics: mean reciprocal rank (MRR), precision-at-1 (P@1), and precision-at-10 (P@10).</p><p>Results Table <ref type="table">4</ref> shows the performance of MLMs with different prompting methods, and we show qualitative examples in Table <ref type="table">3</ref> and in Appendix C. Prompts generated using AUTOPROMPT can extract factual knowledge from BERT more effectively than their manual and mined counterparts: we improve P@1 by up to 12 points. Moreover, despite AUTOPROMPT using only one prompt per relation, it still outperforms LPAQA's ensemble method (which averages predictions for up to 30 prompts) by approximately 4 points. Using 7 trigger tokens achieves slightly higher scores than 5 trigger tokens, although the difference is not substantial. This indicates that our approach is stable to the choice of trigger length, which is consistent with our sentiment analysis results. Overall, these results show that AUTOPROMPT can retrieve facts more effectively than past prompting methods, thus demonstrating that BERT contains more factual knowledge than previously estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Breakdown</head><p>We also provide a detailed breakdown of the prompts found by <ref type="bibr" target="#b19">Petroni et al. (2019)</ref> and AUTOPROMPT, and their associated accuracies in Appendix C, Table <ref type="table">7</ref>. Manual prompts are competitive when the prompt is easy to specify, e.g., the prompt "was born in" for the PLACE OF BIRTH relation. On the other hand, AUTOPROMPT performs especially well for relations that are difficult to specify in a natural language prompt. For example, <ref type="bibr" target="#b19">Petroni et al. (2019)</ref>'s prompt for the PO-SITION PLAYED ON TEAM relation is "{sub} plays in <ref type="bibr">[MASK]</ref> position", which is not as specific as the relation requires. Although the prompt from AU-TOPROMPT is not grammatical ("{sub} ediatric striker ice baseman defensive {obj}"), it does contain tokens that are directly related to sports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT outperforms RoBERTa</head><p>We finally directly compare BERT and RoBERTa. To do so, we subsample the LAMA test set to consist of examples where the object is a single token for both BERT and RoBERTa (Original-RoBERTa). <ref type="foot" target="#foot_2">4</ref> BERT actually slightly outperforms RoBERTa, and we find that the prompts generated for RoBERTa tend to contain more irrelevant words (see Appendix C, Table <ref type="table">7</ref>). For example, the prompt generated by RoBERTa for the PLAYS INSTRUMENT relation contains words such as "Trump" and symbols such as "," ()," for the <ref type="bibr">POSITION</ref>   <ref type="table">4</ref>: Factual Retrieval: On the left, we evaluate BERT on fact retrieval using the Original LAMA dataset from <ref type="bibr" target="#b19">Petroni et al. (2019)</ref>. For all three metrics (mean reciprocal rank, mean precision-at-10 (P@10), and mean precision-at-1(P@1)), AUTOPROMPT significantly outperforms past prompting methods. We also report results on a T-REx version of the data (see text for details). On the right, we compare BERT versus RoBERTa on a subset of the LAMA data using AUTOPROMPT with 5 tokens. perform better than BERT, and it is worthy of investigating this further in future work. Additionally, recall that prompting is a lower bound on a model's knowledge: the lower relative performance does not mean that the model actually knows less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Relation Extraction</head><p>Apart from evaluating whether MLMs know facts, it is also important to evaluate whether they can extract knowledge from text. In this section, we use the task of relation extraction (RE)-to identify how entities are related in a given sentence-an important task in information extraction. We create RE prompts in a similar fashion as fact retrieval: for a given triple (subj,rel,obj) and sentence that expresses this relation, we construct a prompt as "{sent}{sub}[T]. . . <ref type="bibr">[T][P]</ref>.", where the trigger tokens are specific to the relation, and label token is the correct object obj (see Table <ref type="table">3</ref> for an example).</p><p>Setup We use the T-Rex dataset for RE because each T-REx fact comes with context sentences that mention the subject and object surface forms. We compare AUTOPROMPT to LAMA and LPAQA (their prompts are still useful here), as well as a recent supervised relation extraction model <ref type="bibr" target="#b24">(Sorokin and Gurevych, 2017</ref>) that was also used by <ref type="bibr" target="#b19">Petroni et al. (2019)</ref>. To make the evaluation fair for the supervised RE model, we modify the standard RE evaluation. We give the model credit as long as it does not predict a different relation for the subject and object, i.e. we ignore the "no relation" prediction and all other relations. We also drop all sentences from evaluation for which the model's named entity extractor failed to identify the subject and the object as entities. See Appendix B for further details. For the evaluation of all systems, we treat a prediction as correct if it is either the canonical version of the object (e.g., "USA") or the rendered surface form (e.g., "American") for any of the context sentences in a given triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Table <ref type="table" target="#tab_4">5</ref> shows the results for BERT and RoBERTa. MLMs can extract relational information more effectively than the supervised RE model, providing up to a 33% increase on the task when using AUTOPROMPT. RoBERTa also outperforms the supervised RE model, although it is worse than BERT (likely for similar reasons as we outline in Section 5). For both BERT and RoBERTa, we notice that the trigger tokens consist of words related to their corresponding relations (see Appendix D, Table <ref type="table">8</ref> for full list), e.g. RoBERTa selects "defy trademarks of namesake manufacturer" for relation MANUFACTURER/PRODUCER OF PRODUCT.</p><p>Perturbed Sentence Evaluation A possible explanation for the strong results of MLMs in the RE setting is that they may already know many of the relations. Thus, they may directly predict the objects instead of extracting them. To separate this effect, we synthetically perturb the relation extraction dataset by replacing each object in the test data with a random other object and making the same change to the prompt. For example, "Ryo Kase (born November 9, 1974 in YokohamaâYorkshire) is a Japanese actor" where Ryo Kase is the subject, Yokohama is the original object, and Yorkshire is the new object. We regenerate the prompts using the perturbed version of the data.</p><p>The accuracy of the RE model does not change significantly on the perturbed data (Table <ref type="table" target="#tab_4">5</ref>), however, the accuracy of the MLMs decreases significantly. This indicates that a significant portion of MLM accuracy comes from background information rather than relation extraction. Nevertheless, our prompts for BERT outperform their LAMA and LPAQA counterparts, which provides further evidence that AUTOPROMPT produces better probes. higher mean precision-at-1 (P@1), especially when using prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Prompting as an Alternative to Finetuning</p><p>The goal of prompting a language model is to probe the knowledge that the model acquired from pretraining. Nevertheless, prompting has some practical advantages over finetuning for solving realworld tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime.</p><p>Moreover, prompting has advantages over finetuning when trying to solve many different tasks (e.g., the many users of the OpenAI GPT-3 API <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>). In particular, finetuning requires storing large language model checkpoints for each individual task, and drastically increases system cost and complexity because it requires deploying many different models at the same time. Prompting alleviates both of these issues. Only prompts are stored for each individual task, while the same pretrained model is used across all of the tasks.</p><p>Limitations of Prompting There are certain phenomena that are difficult to elicit from pretrained language models via prompts. In our preliminary evaluation on datasets such as QQP <ref type="bibr" target="#b9">(Iyer et al., 2017)</ref> and RTE <ref type="bibr" target="#b4">(Dagan et al., 2005)</ref>, prompts generated manually and with AUTOPROMPT did not perform considerably better than chance. However, we cannot conclude that BERT does not know paraphrasing or entailment from these results. In general, different probing methods have different tasks and phenomena they are suitable for: AUTO-PROMPT makes prompt-based probes more generally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher.</p><p>Limitations of AUTOPROMPT One downside of AUTOPROMPT is that it requires labeled training data. Although this is also required for other probing techniques (e.g., linear probing classifiers), manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT generated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers. Another limitation of AUTOPROMPT is that it can sometimes struggle when the training data is highly imbalanced. For example, in Sections 4 and 5 we show that the prompts often just increase the likelihood of the majority label. Rebalancing the training data can help to mitigate this problem. Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requiring less human effort. Furthermore, the results for sentiment analysis and textual entailment suggest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we focus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for constructing inputs for models like <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>. Source code and datasets to reproduce the results in this paper is available at http://ucinlp.github.io/autoprompt.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. atmosphere alot dialogue Clone totally[MASK].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>search with |x trig | = 10, |V y | = 3, and |V cand | = 10. We compare to the performance of BERT and RoBERTa finetuned on the same data. For fair comparison between AU-TOPROMPT and finetuning, we use Mosbach et al.(2020)'s recommended parameters for finetuning on small datasets: trained for 20 epochs, using AdamW<ref type="bibr" target="#b15">(Loshchilov and Hutter, 2018)</ref> with bias correction and a learning rate that linearly increases to 2 Ã 10 â5 in the first 10% of iterations, and linearly decreases to 0 afterwards. Experiments are repeated 10 times on random subsets of data (and seeds for the finetuned models). Best-case, worstcase, and average performance are shown in Figure 2. Note that results in the EMNLP version had a bug that has since been fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of Training Data on sentiment analysis and NLI for AUTOPROMPT vs. finetuning. X-axis is the number of data points used during training. Error bars plot the max. and min. accuracies observed over 10 independent runs. (revised since EMNLP version).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Sentiment Analysis performance on the SST-2 test set of supervised classifiers (top) and fill-in-theblank MLMs (bottom). Scores marked with â  are from the GLUE leaderboard: http://gluebenchmark.com/ leaderboard.</figDesc><table><row><cell>Model</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>BiLSTM</cell><cell>-</cell><cell>82.8  â </cell></row><row><cell>BiLSTM + ELMo BERT (linear probing)</cell><cell>-85.2</cell><cell>89.3  â  83.4</cell></row><row><cell>BERT (finetuned) RoBERTa (linear probing)</cell><cell>-87.9</cell><cell>93.5  â  88.8</cell></row><row><cell>RoBERTa (finetuned)</cell><cell>-</cell><cell>96.7  â </cell></row><row><cell cols="2">BERT (manual) BERT (AUTOPROMPT) RoBERTa (manual) RoBERTa (AUTOPROMPT) 91.2 63.2 80.9 85.3</cell><cell>63.2 82.3 85.2 91.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PLAYED ON TEAM relation. It is surprising that RoBERTa does not</figDesc><table><row><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell>T-REx</cell><cell></cell></row><row><cell>Prompt Type</cell><cell cols="2">MRR P@10</cell><cell cols="2">P@1 MRR P@10</cell><cell>P@1</cell><cell>Model</cell><cell>MRR P@10 P@1</cell></row><row><cell cols="2">LAMA LPAQA (Top1) AUTOPROMPT 5 Tokens 53.06 40.27 43.57 AUTOPROMPT 7 Tokens 53.89</cell><cell cols="2">59.49 31.10 35.79 62.03 34.10 39.86 72.17 42.94 54.42 73.93 43.34 54.89</cell><cell cols="2">54.29 26.38 57.27 31.16 72.02 45.57 70.80 45.40</cell><cell>BERT RoBERTa 49.90 68.34 40.01 55.22 74.01 45.23</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have</figDesc><table><row><cell>Model</cell><cell cols="2">Original Perturbed</cell></row><row><cell>Supervised RE LSTM BERT (LAMA) BERT (LPAQA) BERT (AUTOPROMPT) RoBERTa (AUTOPROMPT)</cell><cell>57.95 69.06 76.55 90.73 60.33</cell><cell>58.81 28.02 30.79 56.43 28.95</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For brevity, we will omit subscripts in the model names.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Required 2 days to run with 8 NVIDIA 2080Ti GPUs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">The original dataset consists of examples where the object is a single token for BERT.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gardner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Varying the number of trigger tokens generally has little effect. On the other hand, there is a substantial increase in accuracy when increasing the label set size from 1 to 3 (approximately +5% for BERT, and +10% for RoBERTa). After analyzing the label sets, we find that our method generally produces intuitive results-"marvelous" and "philanthrop" are associated with positive sentiment, whereas "worse" and "incompetence" are associated with negative sentiment for RoBERTa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Relation Extraction Details</head><p>Following <ref type="bibr" target="#b19">Petroni et al. (2019)</ref>, we use the pretrained RE model from <ref type="bibr" target="#b24">Sorokin and Gurevych (2017)</ref> as our baseline. To encode the sentence, this model uses a combination of an LSTM-based relation encoder and an attention mechanism. To make predictions, the model constructs a knowledge graph whose edges are the extracted relation triples. The standard RE evaluation measures how well the model predicts the relation types of entity pairs on the sentence level.</p><p>Since our goal is to extract the object of relation triplets, rather than the relation itself, we tweak the standard RE evaluation. We feed the RE model sentences from test facts and we query the resulting graph for all edges that contain the given subject and relation. Then we select the triple with the highest confidence and compare it's object to the gold object. We do this for every fact and take the average across all relations to get the overall precision. The RE model is not trained to predict two of the original T-REx relations. For fair comparison, we exclude these two relations for our evaluation. Table <ref type="table">6</ref>: A breakdown of all relations for fact retrieval on the original dataset from <ref type="bibr" target="#b19">Petroni et al. (2019)</ref>. We compare P@1 of prompts generated by LAMA, LPAQA, and our approach using five prompt tokens. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Fact Retrieval Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Relation Extraction Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inducing relational knowledge from BERT</title>
		<author>
			<persName><forename type="first">Zied</forename><surname>Bouraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What you can cram into a single vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GermÃ¡n</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LoÃ¯c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FrÃ©dÃ©rique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Designing and interpreting probes with control tasks</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornel</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is not explanation</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How can we know what language models know</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
				<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Why do masked neural language models still need common sense knowledge?</title>
		<author>
			<persName><forename type="first">Sunjae</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheongwoong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyeon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03024</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised question answering by cloze translation</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Nelson F Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>RocktÃ¤schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised commonsense question answering with selftalk</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05483</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextaware representations for knowledge base relation extraction</title>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">A simple method for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal adversarial triggers for attacking and analyzing NLP</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<editor>
			<persName><forename type="middle">Eric</forename><surname>Emnlp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shi</forename><surname>Wallace</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikhil</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matt</forename><surname>Kandpal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sameer</forename><surname>Gardner</surname></persName>
		</editor>
		<editor>
			<persName><surname>Singh</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note>Informationtheoretic probing with minimum description length</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">AUTOPROMPT RoBERTa [X] 1830 dissertation applying mathsucci [Y] 0.17 P103 Manual The native language of</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">RÃ©mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<idno>46.13</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>HuggingFace&apos;s Transformers: State-of-theart natural language processing. X] is [Y] 74.54 AUTOPROMPT BERT [X]PA communerug speaks proper [Y] 84.87 AUTOPROMPT RoBERTa [X]neau optionally fluent!? áºraditional [Y] 81.61 P106 Manual [X] is a [Y] by profession 0.73 AUTOPROMPT BERT [X] supporters studied politicians musician turned. P136 Manual [X] plays [Y] music 0.7 AUTOPROMPT BERT [X] freaking genre orchestra fiction acid [Y] 59.95 AUTOPROMPT RoBERTa [X] blends postwar hostage drama sax [Y] 52.97 P1376 Manual [X] is the capital of [Y] 81.11 AUTOPROMPT BERT [X] boasts native territory traditionally called. reorganizationotype photographic studio in [Y] 33.53 AUTOPROMPT RoBERTa [X].. enigmatic twentieth nowadays near [Y] 31.33 P27 Manual [X] is [Y] citizen 0.0 AUTOPROMPT BERT [X] mÂ³ badminton pieces internationally representing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">AUTOPROMPT RoBERTa [X]onen tribes descending speak mainly</title>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Autoprompt</surname></persName>
		</author>
		<idno>53.67</idno>
		<imprint/>
	</monogr>
	<note>P37 Manual The official language of. P407 Manual [X] was written in [Y] 60.21 AUTOPROMPT BERT [X] playediÄ every dialect but [Y] 69.31 AUTOPROMPT RoBERTa [X] scaven pronunciation.*Wikipedia speaks [Y] 72.0 P413 Manual [X] plays in [Y] position 0.53 AUTOPROMPT BERT [X] played colors skier â defensive [Y] 41.71 AUTOPROMPT RoBERTa [X],&quot; (. Liverpool [Y] 23.21 Table 7: Examples of manual prompts (first line, shown with BERT&apos;s P@1) and prompts generated via AUTO-PROMPT for Fact Retrieval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
