<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models</title>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-29">29 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Korawat</forename><surname>Tanwisuth</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shujian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huangjie</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>&lt;mingyuan.zhou@mccombs.utexas.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-29">29 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.00350v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 languagerelated ones, the proposed approach achieves consistent improvements over the baselines. PyTorch code is available at https://github.com/ korawat-tanwisuth/POUF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The pre-train and fine-tune paradigm has become a standard approach to solve many machine learning applications <ref type="bibr" target="#b36">(Qiu et al., 2020;</ref><ref type="bibr" target="#b7">Du et al., 2022)</ref>. In this paradigm, a model is first pre-trained on an extensive collection of datasets. While the model may see many examples during pre-training, the target dataset may contain unseen examples with new variations <ref type="bibr" target="#b56">(Wenzel et al., 2022)</ref>. To address the problem of distribution shift between the source and target domains, practitioners fine-tune the pre-trained model on different downstream tasks with task-specific parameters and objective functions. However, this fine-tuning stage generally needs labeled examples, which are expensive to acquire, to adapt the pre-trained model to a specific task.</p><p>Without introducing task-specific parameters and objective functions for fine-tuning, recent foundation models, such as CLIP <ref type="bibr" target="#b37">(Radford et al., 2021)</ref>, ALIGN <ref type="bibr" target="#b14">(Jia et al., 2021)</ref>, and GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref>, leverage the power of language supervision during pre-training to perform zero-shot predictions through language prompting. For example, to make predictions on a new dataset, users only need to convert class names into textual prompts such as "a photo of a {CLASS}.". A prediction is then made by obtaining the prompt yielding the highest similarity score with a given image. Despite the capability to perform zero-shot predictions, the distribution shift problem still persists.</p><p>Recent methods focus on prompt engineering to adapt these models to downstream tasks. Prompt engineering refers to the process of finding the most appropriate prompt to allow a language model to solve the task at hand <ref type="bibr">(Liu et al., 2021a)</ref>. Many works <ref type="bibr" target="#b52">(Wallace et al., 2019;</ref><ref type="bibr" target="#b43">Shin et al., 2020)</ref> focus on discrete prompt search (finding the prompt in the space of the vocabularies of the language model). Since the interpretability of the prompt is often not as important as the performance of the model, various works <ref type="bibr" target="#b24">(Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b21">Lester et al., 2021;</ref><ref type="bibr">Zhou et al., 2022b;</ref><ref type="bibr">a)</ref> propose using continuous prompts that perform prompting in the embedding space of the model. Notably, <ref type="bibr" target="#b21">Lester et al. (2021)</ref> and <ref type="bibr">Zhou et al. (2022b)</ref> introduce continuous parameters in the word-embedding space and optimize them by maximizing the likelihood of the model on the labeled target data. While these approaches successfully adapt the pre-trained models, they still require a few annotated samples.</p><p>To overcome this limitation, we propose prompt-oriented unsupervised fine-tuning (POUF), a simple yet effective framework for fine-tuning pre-trained prompt-based large models with zero-shot capabilities, directly on the unlabeled target data. We formulate unsupervised fine-tuning as a process of minimizing the statistical distance between the empirical distribution of the textual prototypes and that of the target features. Our framework relies on language prompts to construct class prototypes or target features depending on the model type. For language-augmented vision models, we align the representations of class-specific language prompts, which are class prototypes, with the target image features in the latent space. For masked-language models, we extract the masked-token representations from language prompts and align them with the textual prototypes from the decoder head of the language model. By aligning these distributions, the pre-trained model can better capture the variations in the target data. To this end, we utilize transport-based alignment and mutual-information maximization objective functions to align the latent representations. Our proposed method is compatible with both full model tuning and prompt tuning.</p><p>Our contributions include the following: 1) We propose a prompt-oriented framework for fine-tuning pre-trained models with zero-shot capabilities, directly on the unlabeled target data. 2) We illustrate how to formulate POUF under both language-augmented vision models and masked-language models and demonstrate its effectiveness in practical tasks, such as image classification, sentiment analysis, and natural language inference. 3) We perform extensive ablation studies to justify the design decisions of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prompt-oriented unsupervised fine-tuning</head><p>Below we provide a simple recipe for fine-tuning zero-shot models on unlabeled target data for classification tasks. Our aim is to reduce the distribution shift between the data used to pre-train zero-shot models and the target data. Motivated by the observation that the class prototypes represent source domain information (data for pre-training), we propose to align them with the target representations in the latent space. We demonstrate how to formulate POUF for both languageaugmented vision and masked-language models. Figure <ref type="figure">1</ref> provides motivating examples of the latent features learned using our method on different models. It is evident that before applying our method, the textual prototypes are not well aligned with target features, whereas after applying POUF, the prototypes and target features become well aligned. A schematic diagram of POUF is shown in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">POUF for language-augmented vision models</head><p>Given an input image x from a target dataset X , the CLIP model encodes it through an image encoder to obtain a latent representation as f = F (x). Each class name is included in a prompt template as an input, x k prompt , to the text encoder, G(?), to obtain textual representation G(x k prompt ) = w k for class k. <ref type="bibr" target="#b37">Radford et al. (2021)</ref> suggest using the template as "a photo of a {CLASS}." in CLIP as it leads to better performance than using the class names alone. The prediction probabilities over classes are then given as</p><formula xml:id="formula_0">P (y = k | x) = exp (cos (f , w k ) /T ) K k =1 exp (cos (f , w k ) /T ) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where T is a temperature parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">TEXTUAL-PROTOTYPE CONSTRUCTION</head><p>While the model has seen a large collection of data during pre-training, it may not capture the variation in the target data, leading to a sub-optimal performance at the test time without any adaptation. For example, the model may see an image of a polar bear on a white background during pre-training but fail to correctly identify another photo of a polar bear on an unseen background. If we regard the pre-training data as the source domain data, we can apply domain adaptation methods to bridge the domain gap.</p><p>Existing methods in domain adaptation align the distribution of source with that of the target data <ref type="bibr" target="#b47">(Tzeng et al., 2014;</ref><ref type="bibr" target="#b9">Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b29">Long et al., 2015;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b48">Tzeng et al., 2017;</ref><ref type="bibr" target="#b31">Long et al., 2018;</ref><ref type="bibr" target="#b60">Zhang et al., 2021b)</ref>. However, this approach presents three challenges. 1) Computational inefficiency: The amount of pre-training data is enormous. As an illustration, the CLIP model is trained with 400 million image and text pairs <ref type="bibr" target="#b37">(Radford et al., 2021)</ref>.</p><p>Figure <ref type="figure">2</ref>: A schematic diagram of POUF for language-augmented vision models and masked-language models. Our method addresses the distribution shift problem by aligning the class prototypes with the target features. To this end, POUF minimizes the transport cost between the prototypes and target features while maximizing the mutual information between them. For language-augmented vision models, as shown on the left, the textual prototypes are the representations of the prompts. For masked language models, as shown on the right, the textual prototypes are extracted from the decoder head of the underlying language model.</p><p>2) Privacy issue: Access to the pre-training data can be restricted. 3) Class mismatch: Pre-training data is likely to contain classes that are not present in the target data <ref type="bibr" target="#b2">(Cao et al., 2018)</ref>.</p><p>To overcome these challenges, we propose constructing a prototype for each class in the latent space to represent the source data (pre-training data). By using these prototypes to represent source images, the computation is now feasible as there are as many textual prototypes as classes, whose number is considerably smaller than the number of pretraining data. This strategy also solves the access issue of the pre-training data because we only need to come up with the class names to obtain these textual prototypes. Finally, the target users have control over which classes they want to classify, meaning that there will not be class mismatching.</p><p>Prior works use average latent feature <ref type="bibr" target="#b44">(Snell et al., 2017;</ref><ref type="bibr" target="#b34">Pan et al., 2019;</ref><ref type="bibr" target="#b58">Yue et al., 2021)</ref> or learnable weight vectors <ref type="bibr" target="#b39">(Saito et al., 2019;</ref><ref type="bibr" target="#b45">Tanwisuth et al., 2021;</ref><ref type="bibr" target="#b46">2023)</ref> to construct prototypes. The former is computationally expensive as we need to perform multiple forward passings to construct reliable prototypes, while the latter is not applicable since we do not have labeled data. Unlike these approaches, we utilize the textual representations G(x k prompt ) = w k , which represent each class in the latent space, as our prototypes. The textual representations will be close to the images of their respective categories. For example, the textual representation of the prompt "A photo of a cat." will be close to cat images. Thus, they represent the prototypes of the classes. To align the distribution of textual prototypes and target data, we first discuss how to construct these distri-butions and then present different options for distribution alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">ALIGNING PROTOTYPES WITH TARGET DATA</head><p>Denoting ? as the Dirac delta function, we express the distributions over textual representations and target data as</p><formula xml:id="formula_2">P = K k=1 u k ? w k , Q = N i=1 v i ? f i ,<label>(2)</label></formula><p>where both u ? R K + and v ? R N + have non-negative elements that sum to one. Unless specified otherwise, we assume u k = 1/K for k = 1, . . . , K and v i = 1/N for i = 1, . . . , N in what follows. To align distributions P and Q, we present two alternative methods and discuss the benefits and drawbacks of each.</p><p>Comparing two distributions. Our objective is to align the distribution of textual prototypes and that of the target data. To achieve this, we need to quantify the difference between two discrete distributions.</p><p>One principled approach is to consider Optimal Transport (OT) <ref type="bibr" target="#b50">(Villani, 2008)</ref>. Given two discrete distributions P and Q shown in (2), the OT between them is defined as</p><formula xml:id="formula_3">OT(P, Q) := min T??(u,v) Tr T C ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">T ? R K?N + is a doubly stochastic transport matrix such that ?(u, v) = T | T1 N = u, T 1 K = v , T ij</formula><p>is the transport probability between x i and y j , C ? R K?N + is the transport cost matrix with C ki = c (w k , f i ), and Tr(?) denotes the matrix trace. However, OT can be sensitive to outliers due to the two marginal constraints <ref type="bibr" target="#b4">(Chizat et al., 2018)</ref>. Moreover, solving (3) without any relaxation is a linear programming problem, which has a complexity of O(M 3 log M ), where M denotes the size of the minibatch of the target examples <ref type="bibr" target="#b32">(M?rigot &amp; Oudet, 2016)</ref>. This complexity makes it unfitting for deep learning applications. <ref type="bibr" target="#b5">Cuturi (2013)</ref> introduces Sinkhorn divergence, an entropyregularized OT, to speed up the optimization. While it significantly lowers the computation, it still relies on a twostage optimization strategy to apply to deep learning models: solving for the transport plan and then updating the network.</p><p>To overcome this computational issue while being deeplearning friendly, <ref type="bibr">Zheng &amp; Zhou (2021)</ref> have introduced the Conditional Transport (CT) framework. Instead of solving for the OT plan, CT defines a bi-directional transport plan, whose two directions correspond to softmax probabilities normalized across the classes and across the data, respectively. This choice makes it easy to integrate it into the deep-learning framework while also effectively aligning the two distributions <ref type="bibr" target="#b45">(Tanwisuth et al., 2021;</ref><ref type="bibr" target="#b55">Wang et al., 2022)</ref>. The CT between two discrete distributions can be written as</p><formula xml:id="formula_5">CT(P, Q) := L t?w + L w?t ,<label>(4)</label></formula><p>where L t?w is the transport cost from target features to textual prototypes and L w?t is the transport cost in the opposite direction. The transport cost from target features to prototypes can be expressed as</p><formula xml:id="formula_6">L t?w = E xi?X E w k ?? ? (w k | f i ) [c(w k , f i )] = E xi?X K k=1 c(w k , f i )? ? (w k | f i ) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">c(w k , f i ) = 1 -cos(w k , f i ) is the point-wise transport cost, ? ? (w k | f i ) = p(w k ) exp(w T k f i ) K k =1 p(w k ) exp(w T k f i )</formula><p>, and p(w k ) is the prior probability of prototype k.</p><p>If we regard the prototypes as the modes of the target distribution, this transport direction has a mode seeking effect, meaning that target features will be close to some prototypes. This direction alone, however, could lead to a degenerate solution where target features are close to only a few prototypes (mode collapse). To counteract this potential issue, CT introduces the transport cost in the opposite direction.</p><formula xml:id="formula_8">L w?t = E {xi} N i=1 ?X E w k ?p(w k ) E f i ?? ? (f i | w k ) [c(w k , f i )] = E {xi} N i=1 ?X K k=1 p(w k ) N i=1 c(w k , f i )? ? (f i | w k ) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">? ? (f i | w k ) = exp(w T k f i ) N i =1 exp(w T k f i )</formula><p>. In contrast to L t?w which has a mode-seeking effect, this transport direction has a mode-covering effect, meaning that each prototype will get target features assigned close to it. Thus, we can avoid the possible mode collapse. We note the definition of modes differs from that in <ref type="bibr">Zheng &amp; Zhou (2021)</ref>, in which one can find a more detailed discussion regarding the mode-covering and mode-seeking behaviors of CT.</p><p>The prior term, p(w k ), can be set to 1 K (a uniform distribution over the classes). However, this approach is not ideal if the class proportions are not balanced. To address the potential class-imbalanced problem, we can learn the prior term from the target data. We discuss an estimation strategy in Appendix G.</p><p>Compared to OT, CT has a lower complexity of O(d f M K), where d f is the dimension of the target feature, M denotes the mini-batch size, and K refers to the number of categories. The optimization of CT is also end-to-end as both the cost and transport plan are parameterized by deep neural networks.</p><p>While POUF is compatible with both OT and CT, we empirically find that CT leads to better performance. We show this result in the ablation study in Section 4.3. The CT-based transport cost is thus expressed as:</p><formula xml:id="formula_10">L transport (F, G; X ) = CT (P, Q). (7) 2.1.3. DECISION-BOUNDARY REFINEMENT</formula><p>Because of the distribution shift problem, some samples may lie close to decision boundaries, meaning that they are far away from the prototypes. Motivated by the cluster assumption <ref type="bibr" target="#b11">(Grandvalet et al., 2005)</ref>, which states that decision boundaries should not cross high-density data regions, we propose incorporating another module to refine the predictions of the model on the target data. If the violation of the cluster assumption is minimized, the input examples will be close to the textual prototypes in the latent space.</p><p>Mutual-information maximization. Many existing works in domain adaptation minimize the conditional entropy of the target predictions <ref type="bibr" target="#b51">(Vu et al., 2019;</ref><ref type="bibr" target="#b39">Saito et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b54">Wang et al., 2020)</ref>. However, this has a major shortcoming since this objective alone could lead to a degenerate solution (all samples are clustered around one prototype) <ref type="bibr" target="#b33">(Morerio et al., 2017;</ref><ref type="bibr" target="#b57">Wu et al., 2020)</ref>. To overcome this issue, we utilize the information maximization objective <ref type="bibr" target="#b17">(Krause et al., 2010;</ref><ref type="bibr" target="#b42">Shi &amp; Sha, 2012;</ref><ref type="bibr" target="#b25">Liang et al., 2020)</ref>. This objective adds a regularization term to conditional entropy minimization. This regularization encourages the model to maximize the marginal entropy of the label distribution <ref type="bibr">(Zhang et al., 2021c)</ref>, making the predictions globally diverse while individually certain. The mutual information objective has the following form:</p><formula xml:id="formula_11">L mi (F, G; X ) = -[H (Y) -H (Y | X )] = -[h (E x?X p(y|x)) -E x?X h (p(y|x))],<label>(8)</label></formula><p>where H (Y) and H (Y | X ) denote the marginal entropy and conditional entropy of the text categories Y, respectively, and h(p) =i p i log p i .</p><p>Putting it all together, we write the final loss function as</p><formula xml:id="formula_12">L LV M (F, G; X ) = L transport (F, G; X ) + ?L mi (F, G; X ) , (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where ? is a hyper-parameter controlling the weight of the mutual-information objective. We then update both the image encoder, F , and the text encoder, G, through gradient backpropagation of this loss function. Alternatively, we can introduce soft-prompt parameters in the word embedding space and update only these parameters while keeping the model parameters fixed. We discuss this alternative design consideration in more detail in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">POUF for masked-language models</head><p>In a conventional setting, to fine-tune masked-language models such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr" target="#b28">(Liu et al., 2019)</ref> on a task, one first converts input x 1 to a sequence of token x. The language model, M, then maps x to a sequence of hidden vectors {h k ? R d }. One then introduces a task-specific head W 0 ? R |Y|?d , which is randomly initialized, to classify the hidden representations through softmax(W 0 h [CLS] ) into a class in Y. Since W 0 does not capture any semantic meaning, it is trained together with the parameter of the pre-trained model by minimizing the negative log probability over the labels y ? Y. The drawbacks of this approach are that one needs to not only introduce additional task-specific parameters but also utilize labeled examples to fine-tune the model.</p><p>An alternative way to fine-tune the language model on this task is to formulate it as a masked-language modeling problem, mimicking the pre-training process <ref type="bibr" target="#b41">(Schick &amp; Sch?tze, 2020;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref>. During this process, one first maps the labels to the words in the vocabulary. As an example, for a sentiment classification task, a positive label (+) can be mapped to the word "great" whereas a negative label (-) can be mapped to the word "terrible". The language model is then tasked to fill the mask token with the label words ("great" or "terrible"). Given an input x 1 , one can then construct a prompt from the text input as:</p><formula xml:id="formula_14">x prompt = [CLS] x 1 It was [MASK]. [SEP].</formula><p>The prompt can be manually or automatically generated, so long as the input contains a mask token. Having a mask token in the input allows one to treat the problem as a masked language modeling task, i.e., directly modeling the probability of each label as that of the corresponding word in the language model's vocabulary as</p><formula xml:id="formula_15">P (y | x in ) = p ([ MASK ] = M(y) | x prompt ) (10) = exp w M(y) ? h [MASK ] y ?Y exp w M(y ) ? h [MASK ] ,<label>(11)</label></formula><p>where w v corresponds to the weight vector of the vocabulary v ? V and h [MASK] denotes the hidden representation of the mask token, and M : Y ? V represents the mapping from the set of labels to the words in the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">TEXTUAL-PROTOTYPE CONSTRUCTION</head><p>To apply POUF to masked-language models, we first construct the textual prototypes, which contain information about the classes. Unlike language-augmented vision models' prompts which encode class information, Maskedlanguage models' prompts contain information about the input examples. Thus, we cannot use them as our class prototypes. To construct textual prototypes, we propose using the weight of the last layer of the decoder head, which is tasked to predict the mask tokens during pre-training. These weights capture the meanings of the individual words in the vocabulary of the language model. For example, w great will be close to the masked-token representations of the sentences which contain the word "great". We then use the alignment module to align</p><formula xml:id="formula_16">P = y?Y 1 |Y| ? w M(y) and Q = N i=1 1 N h i [MASK] . (12)</formula><p>The information maximization objective is directly computed via the predictive distribution of the language model.</p><p>Putting it all together, we write the final loss function as</p><formula xml:id="formula_17">L M LM (M ; X ) = L transport (M ; X ) + ?L mi (M ; X ) , (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where ? is a hyper-parameter controlling the weight of the mutual-information objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Design considerations</head><p>As briefly discussed in the previous section, we could perform full model tuning or prompt tuning. 1) Full-model tuning refers to updating the parameters of both the text and image encoders for language-augmented vision models and the text encoder for masked-language models. This design generally leads to better performance but requires more resources for tuning. 2) Prompt tuning <ref type="bibr" target="#b21">(Lester et al., 2021)</ref> refers to introducing additional tunable parameters in the input embedding space. Although using fewer resources for fine-tuning, it may not necessarily lead to optimal performance.</p><p>We now give a more formal description of prompt tuning. Given a sequence of n tokens, {x 1 , x 2 , . . . , x n }, a transformer-based language model maps this sequence to Instead of tuning the model parameter, prior works <ref type="bibr" target="#b21">(Lester et al., 2021;</ref><ref type="bibr">Zhou et al., 2022b)</ref> introduce soft-prompt parameter P h ? R p?h where p is the prompt length. The embedded prompt and input are then concatenated to form a matrix [P h ; X h ] ? R (p+n)?h . This matrix then goes through the encoder, but only the parameters of the softprompts P h are updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>We summarize how POUF differs from related work in Table <ref type="table" target="#tab_0">1</ref> and provide more details below.</p><p>Learning under distribution shift. Traditional domain adaptation methods jointly optimize on labeled source data and unlabeled target data <ref type="bibr" target="#b47">(Tzeng et al., 2014;</ref><ref type="bibr" target="#b9">Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b29">Long et al., 2015;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b48">Tzeng et al., 2017;</ref><ref type="bibr" target="#b31">Long et al., 2018;</ref><ref type="bibr" target="#b60">Zhang et al., 2021b;</ref><ref type="bibr">d)</ref>. The requirement for labeled source data limits their use in many applications. POUF, however, can directly adapt to the target domain data, greatly increasing its general applicability. This also means that the performance of domain adaptation methods heavily depends on the source dataset, whereas POUF is source domain agnostic. Another related line of work is "source-free" domain adaptation <ref type="bibr" target="#b25">(Liang et al., 2020;</ref><ref type="bibr">Li et al., 2020b;</ref><ref type="bibr">Kundu et al., 2020b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b20">Kurmi et al., 2021)</ref>. In this paradigm, the pre-trained model is adapted in two steps. First, it is fine-tuned on labeled source data. Then, it is adapted to the target data without accessing the source data.</p><p>In this sense, they are not completely source-free. Different from this line of work, POUF leverages textual prototypes in models with zero-shot capabilities to adapt to the target data directly, bypassing the fine-tuning step on the labeled source data.</p><p>Prompt-based learning. For a full review of this topic, we refer the readers to the survey by <ref type="bibr">Liu et al. (2021a)</ref>. Several works that focus on prompt tuning can be broadly classified into two categories: hard and soft prompt tuning. Soft prompt tuning <ref type="bibr" target="#b24">(Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b21">Lester et al., 2021;</ref><ref type="bibr">Zhou et al., 2022b;</ref><ref type="bibr">a)</ref> introduces continuous parameters in the word embedding space and tunes these parameters instead of the full model, whereas hard prompt tuning <ref type="bibr" target="#b52">(Wallace et al., 2019;</ref><ref type="bibr" target="#b43">Shin et al., 2020;</ref><ref type="bibr" target="#b63">Zhang et al., 2022)</ref> searches for discrete tokens in the vocabulary of the language model to optimize the performance of the model. These works focus on prompt design and engineering and rely on labeled data. Unlike these methods, ours is unsupervised, making it more generally applicable. However, as we show in the experiments, our work is complementary to these approaches since we can apply these methods to obtain better prototypes and then further fine-tune using our framework. Recently, <ref type="bibr" target="#b13">Huang et al. (2022)</ref> propose a method for unsupervised prompt tuning for language-augmented vision models. Different from this work, our work provides a general framework for both language-augmented vision and masked-language models.</p><p>Prototype-based learning. The idea of using prototypes has been studied in few-shot classification <ref type="bibr" target="#b44">(Snell et al., 2017;</ref><ref type="bibr" target="#b12">Guo et al., 2022)</ref>, self-supervised learning <ref type="bibr" target="#b0">(Asano et al., 2019;</ref><ref type="bibr" target="#b3">Caron et al., 2020;</ref><ref type="bibr">Li et al., 2020a)</ref>, and domain adaptation <ref type="bibr" target="#b34">(Pan et al., 2019;</ref><ref type="bibr" target="#b16">Kang et al., 2019;</ref><ref type="bibr" target="#b25">Liang et al., 2020;</ref><ref type="bibr" target="#b45">Tanwisuth et al., 2021)</ref>. In these works, the prototypes are defined as the average latent features or learnable weight vectors. Different from them, we propose using textual representations to represent our prototypes. This is the key idea of our method as it allows us to adapt directly to the target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate POUF on both language-augmented vision and masked-language models and compare it to a rich set of baselines in various vision and language modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">POUF for language-augmented vision models</head><p>Datasets. 1) Office-31 <ref type="bibr" target="#b38">(Saenko et al., 2010)</ref> contains 4,652 images with 31 classes from three domains: Amazon (A), Webcam (W), and DSLR (D). 2) Office-Home <ref type="bibr" target="#b49">(Venkateswara et al., 2017)</ref>  Baselines. 1) Clip (zero-shot) <ref type="bibr" target="#b37">(Radford et al., 2021)</ref> refers to using the CLIP model to perform zero-shot predictions on the target dataset. 2) Tent <ref type="bibr" target="#b54">(Wang et al., 2020)</ref> refers to adapting the model with entropy minimization before predicting on the target dataset. We note that we have slightly modified Tent since the vision transformer (ViT) architecture does not have modulation parameters (batch normalization). Thus, we introduce soft-prompt parameters and update them instead. 3) Unsupervised prompt learning (UPL) <ref type="bibr" target="#b13">(Huang et al., 2022)</ref> refers to using the top-K confident predictions for each class as pseudo labels. After obtaining the pseudo labels, we train the soft-prompt parameters with the crossentropy loss. For a fair comparison, we do not use model ensemble as done in <ref type="bibr" target="#b13">Huang et al. (2022)</ref>. To help understand the gap between unsupervised methods and supervised ones, we also provide the results of a representative few-shot learning method, 4) Context Optimization <ref type="bibr">(Zhou et al., 2022b</ref>) (CoOp). We note that this baseline should not be directly compared to the unsupervised ones, but rather serves as a reference to understand how much labeled data can help boost performance.</p><p>Implementation details. We build our method using the open-source CLIP codebase <ref type="bibr" target="#b37">(Radford et al., 2021)</ref> and TLlib transfer learning library <ref type="bibr" target="#b15">(Jiang et al., 2022)</ref>. For all experiments, we adopt the ViTB-16 for the image encoder and the default transformer from the CLIP paper for the text encoder.</p><p>All the unlabeled target samples are used for fine-tuning.</p><p>The learning rate schedule is set to ? iter = ? 0 (1 + ?iter) -? , where ? 0 is the initial learning rate. We adopt the following default hyper-parameters: ? = 2e-4, and ? = 0.75. We set ? 0 = 5e-7 for all experiments except for prompt tuning on Office-31 where ? 0 = 1e-3. We use a mini-batch SGD with a momentum of 0.9 and a batch size of 96 for Office-31 and Office-Home and 16 for DomainNet. The weight of the mutual-information objective, ?, is set to 0.3 for all experiments.</p><p>Main results for language-augmented vision models.</p><p>We conduct systematic experiments on 13 image tasks with varying data sizes. In each experiment, we fine-tune the CLIP model with unlabeled target images. Table <ref type="table" target="#tab_2">2</ref> exhibits the results of POUF and the baselines. Tent and UPL both achieve consistent gains over the zero-shot CLIP model on both the Office-31 and Office-Home datasets. However, on the more challenging DomainNet, both baselines suffer from negative transfers. By contrast, POUF improve the CLIP model by 11.8%, 3.7%, and 3.6% on the Office-31, Office-Home, and DomainNet datasets, respectively. The consistent improvements illustrate that POUF is an effective strategy to fine-tune the model with unlabeled target data. Moreover, POUF is compatible with both prompt tuning and model tuning as evident in the performance gains. Prompt tuning yields slightly worse performance than model tuning, as we expected. However, its memory footprint is significantly lower. We provide parameter and run-time analyses <ref type="bibr" target="#b8">(Fan et al., 2020;</ref><ref type="bibr">Zhang et al., 2021a)</ref> of the two approaches in Appendix B.</p><p>POUF with model tuning outperforms the few-shot learning method (CoOp) in 5 out of 13 tasks. This result illustrates that there is still a gap between the supervised and unsupervised methods. However, the benefit of a large amount of unlabeled data can sometimes outweigh the benefit of a small amount of labeled data.</p><p>To understand the generalization ability of the model after adapting with POUF, we further conduct an unseen class experiment in Appendix E. POUF yields improved performance for the seen class but slightly weaker generalization for the unseen class. This outcome highlights a minor tradeoff between the model's specialization and generalization. However, if target users have access to unseen class names, training POUF with both seen and unseen class text prototypes results in superior performance compared to the zero-shot model for both seen and unseen classes. See Appendix E for a detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">POUF for masked-language models</head><p>Datasets. We perform experiments on 15 English-language tasks, 8 of which are single-sentence tasks and 7 of which are sentence-pair tasks. These datasets include popular classification tasks such as SST-5, MR, CR, MPQA, Subj, and TREC as well as the GLUE benchmark <ref type="bibr" target="#b53">(Wang et al., 2018)</ref>. We provide a table of different tasks in Appendix D.</p><p>Given an input sentence, the goal of the single-sentence task is to predict the label. E.g., given a movie review, we need to predict if it is positive or negative. Similarly, the aim of the sentence-pair task is to predict the relationship between a pair of input sentences. As an illustration, given a premise and a hypothesis, we need to predict whether the hypothesis is an entailment, neutral, or contradiction of the premise.</p><p>Baselines. We directly take the baselines and results from <ref type="bibr" target="#b10">Gao et al. (2020)</ref>. 1) Majority denotes the proportion of the majority class in the data. 2) RoBERTa-large (zero-shot) refers to using prompt-based prediction by RoBERTa-large for zero-shot predictions. 3) Few-shot fine-tuning refers to standard fine-tuning with few-shot examples. 4)"GPT-3" in-context learning refers to prompt-based predictions with augmented context (32 randomly sampled demonstrations).</p><p>The model is still RoBERTa-large (not GPT-3). 5) LM-BFF (man) refers to the complete method by <ref type="bibr" target="#b10">Gao et al. (2020)</ref>, where the prompt is manually selected using the template.</p><p>Implementation details. We follow the experimental protocols in <ref type="bibr" target="#b10">Gao et al. (2020)</ref>. Specifically, for each task, the data is split into D train , D dev , and D test . The authors tune the hyper-parameters on D dev and report the performance of the model on D test . We take the same hyper-parameters as the original paper: batch size = 8, learning rate = 1e -5, training steps = 1, 000, and few-shot examples per class = 16. The weight of the mutual-information objective, ?, is set as 0.6 and the weight for the transport cost is set as one for all experiments except for the unsupervised setting on the MRPC and QQP tasks. In these two tasks, the weights are set to 1e-4 for the mutual-information objective and to 1e-2 for the transport cost. We validate the model's performance every 100 steps on D dev and take the best validated checkpoint for the final evaluation on D test .</p><p>Main results for masked-language model. To validate the effectiveness of POUF on language tasks, we fine-tune the RoBERTa-large model with POUF on the unlabeled text data. We also incorporate POUF into LM-BFF to show the compatibility of our method with few-shot learning frameworks. We report the results on the 15 language tasks in Table <ref type="table" target="#tab_3">3</ref>. Under the unsupervised category, zero-shot RoBERTa model yields consistently higher performance than majority class. This result indicates that the model contains knowledge that can be exploited by prompt-based prediction. After applying POUF, the model performance significantly improves on 14 tasks. Under the few-shot category, few-shot fine-tuning does not always lead to a better performance than prompt-based prediction. Similarly, "GPT-3" in-context learning sometimes hurts the promptbased prediction performance. We speculate that the size of the language model may have an impact on the performance. Since <ref type="bibr" target="#b10">Gao et al. (2020)</ref> use RoBERTa-large instead of the original GPT-3 model, the smaller size of RoBERTa may have a significant impact on the performance of in-context learning. While POUF does not rely on labeled data, it outperforms both few-shot fine-tuning and "GPT-3" in-context learning on 10 and 14 tasks, respectively. For the few-shot setting, we incorporate POUF into LM-BFF by using the few-shot examples without labels to compute POUF's loss and add it to LM-BFF's objective. The results show that POUF also consistently boosts the performance of LM-BFF across 14 out of 15 tasks, illustrating the compatibility of our method with few-shot learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of results</head><p>Ablation studies. To verify the effect of each component of our method, we conduct ablation studies on both the Office-31 (image) and RTE (language) datasets and report the results in Table <ref type="table" target="#tab_4">4</ref>. 1) Distribution-matching options.</p><p>We provide two alternatives for CT: OT and OT-Sinkhorn.</p><p>OT solves the exact linear program to obtain the optimal couplings, while OT-Sinkhorn solves a relaxation of the OT problem with the Sinkhorn algorithm <ref type="bibr" target="#b5">(Cuturi, 2013)</ref>. In  each variant, we replace CT in the L transport while keeping L mi . In both image and language tasks, POUF with CT outperforms POUF with OT and OT-Sinkhorn. 2) Significance of each loss. We remove each loss from the framework to understand the effect of each part. Without the transport cost, the performance drops by 4% and 7% on the image and text tasks, respectively. Similarly, after removing the mutual-information objective, the accuracy decreases by 4% and 6% on the image and text tasks, respectively. These results illustrate the significance of each loss and the synergistic effect of the two losses. 3) Cost function. In the transport framework, we utilize the cosine distance as the cost function. Alternatively, we could use other cost functions. We experiment with another cost function inspired by the radial basis kernel. The result shows that the cosine distance function leads to better performance in both modalities, justifying our design decisions.</p><p>Visualization. Q: Does POUF learn more meaningful prototypes? A: Yes In Figure <ref type="figure" target="#fig_1">3</ref>, we visualize the K-nearest neighbors (K = 5) of the prototypes to understand what the model learns. Before applying POUF, the prototypes of the zero-shot CLIP model contain images that do not semantically represent the classes. The fourth and fifth images among the nearest neighbors of the "Speaker" prototype include a picture of a projector and a ring binder, respectively, while the five nearest neighbors of the "Puncher" prototype do not have a single image of a puncher. The "Puncher" prototype seems to be close to the images of Scissors and Staplers. After applying POUF, we observe significant im- provement qualitatively. Both the "Speaker" and "Puncher" prototypes are now close to the images of their respective classes, demonstrating that POUF helps learn more meaningful prototypes. Q: Does POUF reduce the violation of the cluster assumption? A: Yes. In Figure <ref type="figure">1</ref>, we visualize the t-SNE plots of the prototypes and target features. After applying POUF, the prototypes are assigned to their respective clusters of target features. To further quantify this point, for each example, we compute the cosine similarity between the target feature and the prototype of its label. We then visualize the histograms of the similarity scores in Figure <ref type="figure" target="#fig_2">4</ref>. It is evident that the cosine similarity scores of POUF are higher than those of the zero-shot CLIP model, meaning that the target examples are moved closer to the prototypes of their corresponding classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present POUF, a simple yet effective framework for directly adapting prompt-based zero-shot models to the unlabeled target data. We propose aligning the prototypes and target data in the latent space through transportbased distribution alignment and mutual information maximization. Across 13 image and 15 language tasks, POUF achieves consistent performance gains over the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitation and societal impact</head><p>POUF leverages the power of language representations to adapt to the unlabeled target data directly. While the language model may have a large vocabulary size, it is possible that it does not capture some exotic words. This means that if the categories that we want to predict are outside the vocabulary of the language model, we cannot use our method to fine-tune the model, as we cannot even make predictions. Future research can focus on how to extend POUF to adapt to unknown classes beyond the vocabulary of the model.</p><p>POUF relies on large-scale pre-trained models to adapt to the target data. As with any computationally intensive venture, practitioners should consider using sustainable energy resources. On the positive side, POUF enables practitioners to efficiently fine-tune on the target data, saving the computation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Unseen Class Experiments</head><p>Testing POUF under an open class scenario is an interesting setting. POUF is developed to address domain shift (changes in feature distribution) but does not specifically target label space shift. To investigate the impact of POUF on generalization to unseen classes, we designed two experiments: in-domain generalization and out-of-domain generalization. The setup of each experiment is explained below. POUF (text prototypes = seen classes): we adapt POUF with the textual prototypes from only the seen classes. POUF (text prototypes = seen + unseen classes): in many cases, the target users may not have image data of the unseen classes but know the class names in advance. In this version, we adapt POUF with the textual prototypes from both the seen and unseen classes. 1) Experiment 1: In-domain generalization. In this experiment, our goal is to investigate the impact of POUF on both seen and unseen image classes within the same domain. To do so, we randomly divide the classes in one of the Office-31 dataset's domains into two groups: half as seen and the other half as unseen.</p><p>We employ images from the seen classes for adaptation using POUF. To evaluate the model, we assess its performance on both seen and unseen classes within this domain. The findings are presented below. 2) Experiment 2: Out-of-domain generalization. In this experiment, we aim to examine the impact of POUF on seen and unseen image classes across different domains. To achieve this, we randomly split the classes into two groups: half as seen and the other half as unseen. We apply POUF to adapt the model using the seen classes from one domain and then evaluate the adapted model on the seen and unseen classes of another domain. The findings are presented in Tables <ref type="table" target="#tab_7">8</ref> and<ref type="table" target="#tab_8">9</ref>.</p><p>In both tables, POUF enhances the performance of zero-shot models for seen classes. However, when trained exclusively with seen-class prototypes, POUF yields improved performance for the seen class but slightly weaker generalization for the unseen class when evaluated on the Amazon domain, in both in-domain and out-of-domain generalization settings. This outcome highlights a minor trade-off between the model's specialization and generalization. If target users have access to unseen class names, training POUF with both seen and unseen class text prototypes results in superior performance compared to the zero-shot model for both seen and unseen classes in both scenarios. The outcome implies that the essential concept is to maintain the semantics in the text space for improved generalization. A possible explanation for this observation is that, during pre-training, both text and image features are plentiful. However, during fine-tuning, we continue to have abundant image features but limited text prompts, which come from the target user. If we fine-tune both text and image encoders, we might end up with representations that are particularly tailored to the seen classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Class proportion estimation</head><p>The transport framework provides a nice way of handling the class-imbalanced issue. If we know the proportions beforehand, we can adjust the marginal distribution, p(w k ), by setting it to the corresponding proportions. Classes with higher proportions will then receive higher transport costs. However, in practice, the proportions are unknown so we need to estimate them. To estimate the proportion of class, we can marginalize the predictive distribution of the classes given the data as follows</p><formula xml:id="formula_19">p(w k ) = i ? ? (w k | f i ) = i p(w k ) exp(w T k f i ) K k =1 p(w k ) exp(w T k f i )</formula><p>Since the marginalization is done over the data, mini-batch update is often required to overcome the computational expense. Thus, the above estimation can be noisy. To overcome this issue and estimate the global proportions from a local mini-batch of data, we can iteratively learn the global proportions over multiple iterations. We can then learn the proportions as follows</p><formula xml:id="formula_20">p(w k ) l+1 = 1 M i ? ? (w k | f i )</formula><p>POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models p(w k ) l+1 = ? l p(w k ) l+1 + (1 -? l )p(w k ) l , where ? l is the learning rate of iteration l and follows a cosine learning rate schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Pseudo code</head><p>Algorithm 1 POUF Pseudocode for language-augmented vision models, PyTorch-like </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: t-SNE visualizations of the embeddings of both the CLIP and RoBERTa models before and after applying POUF, where ( ) indicates textual prototypes while (?) represents target features. Different colors correspond to different classes. (a) shows the features and prototypes of the pre-trained zero-shot CLIP model on the Office-31 (Amazon) dataset, while (b) shows those obtained after fine-tuning CLIP with POUF. (c) shows the features and prototypes of the pre-trained zero-shot RoBERTa model on the Subjectivity dataset, while (d) shows those obtained after fine-tuning RoBERTa with POUF. The prototypes of the zero-shot models are not well aligned with the target features as they are clustered around a single point far away from the target features for both CLIP and RoBERTa models. After adapting with POUF, the prototypes and features of their respective classes are well aligned.</figDesc><graphic url="image-1.png" coords="2,94.31,77.03,102.06,102.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: K-nearest neighbors of the prototypes of two different categories in Office-31 -Webcam: "Speaker" and "Puncher". For each category, the top row exhibits the top-5 neighbors of the prototype of the CLIP model before applying POUF, whereas the bottom row shows the top-5 neighbors of the prototype of the CLIP model after applying POUF.</figDesc><graphic url="image-6.png" coords="9,55.44,139.48,242.99,136.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histograms of cosine similarity scores between the correct classes' prototypes and target features in Office-31 -Webcam.</figDesc><graphic url="image-7.png" coords="9,315.09,139.48,218.70,153.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>textual inputs (i.e. "A photo of {CLASS}") for x in loader: # Load a minibatch x with M samples f, prototypes = F(x), G(prompts) # Compute embeddings f = normalize(f, dim=1) prototypes = normalize(prototypes, dim=1) sim_mat = f @ prototypes.T # M-by-K L_transport = compute_transport_loss(sim_mat, T) L_mi = compute_mi_loss(sim_mat/T) L = L_transport + lambda_mi * L_mi # Loss L.backward() # Back-propagate update(F, G) # SGD update def compute_transport_loss(sim_mat, T): # Compute transport cost cost = 1 -(sim_mat) # Compute transport plans forward_plan = softmax(sim_mat/T, dim=0) backward_plan = softmax(sim_mat/T, dim=1) # Compute final loss forward_cost = (cost * forward_plan).sum(0).mean() backward_cost = (cost * backward_plan).sum(1).mean() return forward_cost + backward_cost def compute_mi_loss(sim_mat): # Compute conditional entropy softmax_out = softmax(sim_mat, dim=1) entropy_loss = sum(-softmax_out * log(softmax_out + eps), dim=1).mean() # Compute marginal entropy mean_softmax = softmax_out.mean(0) regularization = sum(-mean_softmax * log(mean_softmax + eps)) # Compute final loss return entropy_loss -regularization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="3,116.19,67.06,364.49,205.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Data and model requirements for different transfer learning settings. POUF requires neither source data nor target labels.</figDesc><table><row><cell>Setup</cell><cell cols="3">Source data Target data Parameter update</cell></row><row><cell>Fine-tuning</cell><cell>N/A</cell><cell>Labeled</cell><cell>Full model</cell></row><row><cell>Domain adaptation</cell><cell>Labeled</cell><cell>Unlabeled</cell><cell>Full model</cell></row><row><cell>Prompt tuning</cell><cell>N/A</cell><cell>Labeled</cell><cell>Prompt</cell></row><row><cell>POUF</cell><cell>N/A</cell><cell cols="2">Unlabeled Prompt/full model</cell></row><row><cell>a matrix X</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>h ? R n?h where h is the hidden dimension.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) on three different datasets for methods based on CLIP.</figDesc><table><row><cell>(a)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on various datasets for methods based on RoBERTa-large.</figDesc><table><row><cell>Category</cell><cell>Methods</cell><cell>SST-2 (acc)</cell><cell>SST-5 (acc)</cell><cell>MR (acc)</cell><cell>CR (acc)</cell><cell>MPQA (acc)</cell><cell>Subj (acc)</cell><cell cols="2">TREC (acc) CoLA (Matt.)</cell></row><row><cell></cell><cell>Majority</cell><cell>50.9</cell><cell>23.1</cell><cell>50.0</cell><cell>50.0</cell><cell>50.0</cell><cell>50.0</cell><cell>18.8</cell><cell>0.0</cell></row><row><cell cols="2">Unsupervised RoBERTa-large (zero-shot)</cell><cell>83.6</cell><cell>35.0</cell><cell>80.8</cell><cell>79.5</cell><cell>67.6</cell><cell>51.4</cell><cell>32.0</cell><cell>2.0</cell></row><row><cell></cell><cell>POUF</cell><cell>89.4(0.0)</cell><cell>44.9(1.6)</cell><cell>86.4(0.9)</cell><cell>86.0(5.6)</cell><cell>78.1(7.8)</cell><cell cols="2">80.8(0.1) 36.9(15.1)</cell><cell>1.5(0.1)</cell></row><row><cell></cell><cell>Few-shot fine-tuning</cell><cell>81.4(3.8)</cell><cell>43.9(2.0)</cell><cell>76.9(5.9)</cell><cell>75.8(3.2)</cell><cell>72.0(3.8)</cell><cell>90.8(1.8)</cell><cell>88.8(2.1)</cell><cell>33.9(14.3)</cell></row><row><cell></cell><cell>"GPT-3" in-context learning</cell><cell>84.8(1.3)</cell><cell>30.6(0.9)</cell><cell>80.5(1.7)</cell><cell>87.4(0.8)</cell><cell>63.8(2.1)</cell><cell>53.6(1.0)</cell><cell>26.2(2.4)</cell><cell>-1.5(2.4)</cell></row><row><cell>Few-shot</cell><cell>LM-BFF (man)</cell><cell>92.6(0.5)</cell><cell>50.6(1.4)</cell><cell>86.6(2.2)</cell><cell>90.2(1.2)</cell><cell>87.0(1.1)</cell><cell>92.3(0.8)</cell><cell>87.5(3.2)</cell><cell>18.7(8.8)</cell></row><row><cell></cell><cell>LM-BFF (man) + POUF</cell><cell>93.2(0.0)</cell><cell>53.6(0.2)</cell><cell>87.6(0.2)</cell><cell>91.8(0.2)</cell><cell>88.0(0.2)</cell><cell>92.9(0.1)</cell><cell>90.1(0.1)</cell><cell>20.6(0.6)</cell></row><row><cell></cell><cell></cell><cell cols="4">MNLI (acc) MNLI-mm (acc) SNLI (acc) QNLI (acc)</cell><cell>RTE (acc)</cell><cell>MRPC (F1)</cell><cell>QQP (F1)</cell><cell>STS-B (Pear.)</cell></row><row><cell></cell><cell>Majority</cell><cell>32.7</cell><cell>33.0</cell><cell>33.8</cell><cell>49.5</cell><cell>52.7</cell><cell>81.2</cell><cell>0.0</cell><cell>-</cell></row><row><cell cols="2">Unsupervised RoBERTa-large (zero-shot)</cell><cell>50.8</cell><cell>51.7</cell><cell>49.5</cell><cell>50.8</cell><cell>51.3</cell><cell>61.9</cell><cell>49.7</cell><cell>-3.2</cell></row><row><cell></cell><cell>POUF</cell><cell>55.0(2.3)</cell><cell>56.3(3.4)</cell><cell>64.0(0.2)</cell><cell>68.2(4.0)</cell><cell>64.5(2.5)</cell><cell>81.2(0.1)</cell><cell>53.8(0.1)</cell><cell>20.9(9.3)</cell></row><row><cell></cell><cell>Few-shot fine-tuning</cell><cell>45.8(6.4)</cell><cell>47.8(6.8)</cell><cell>48.4(4.8)</cell><cell>60.2(6.5)</cell><cell>54.4(3.9)</cell><cell>76.6(2.5)</cell><cell>60.7(4.3)</cell><cell>53.5(8.5)</cell></row><row><cell></cell><cell>"GPT-3" in-context learning</cell><cell>52.0(0.7)</cell><cell>53.4(0.6)</cell><cell>47.1(0.6)</cell><cell>53.8(0.4)</cell><cell>60.4(1.4)</cell><cell>45.7(6.0)</cell><cell>36.1(5.2)</cell><cell>14.3(2.8)</cell></row><row><cell>Few-shot</cell><cell>LM-BFF (man)</cell><cell>70.7(1.3)</cell><cell>72.0(1.2)</cell><cell>79.7(1.5)</cell><cell>69.2(1.9)</cell><cell>68.7(2.3)</cell><cell>77.8(2.0)</cell><cell>69.8(1.8)</cell><cell>73.5(5.1)</cell></row><row><cell></cell><cell>LM-BFF (man) + POUF</cell><cell>73.8(1.1)</cell><cell>74.9(0.1)</cell><cell>80.0(0.2)</cell><cell>76.3(0.3)</cell><cell>70.5(0.6)</cell><cell>81.8(0.7)</cell><cell>69.0(1.0)</cell><cell>82.1(0.1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average accuracy (%) of POUF on Office-31 (CLIP) and RTE (RoBERTa) under different variants. Datasets OT for L transport OT-Sinkhorn for L transport POUF w/o L transport POUF w/o L mi c(? k , f t j ) = exp(-? T k f t j ) POUF (default setting)</figDesc><table><row><cell>Office-31</cell><cell>86.1 ? 0.2</cell><cell>86.5 ? 0.2</cell><cell>83.3 ? 0.4</cell><cell>84.2 ? 0.2</cell><cell>87.6 ? 0.5</cell><cell>88.0 ? 0.9</cell></row><row><cell>RTE</cell><cell>58.0 ? 5.5</cell><cell>63.3 ? 6.0</cell><cell>56.9 ? 1.5</cell><cell>58.4 ? 1.3</cell><cell>61.3 ? 6.3</cell><cell>64.5 ? 2.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Dataset information for image experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Number of images Number of categories</cell><cell>Domains</cell></row><row><cell>Office-31 (Saenko et al., 2010)</cell><cell>4, 652</cell><cell>31</cell><cell>Amazon, DSLR, Webcam</cell></row><row><cell>Office-Home (Venkateswara et al., 2017)</cell><cell>15, 500</cell><cell>65</cell><cell>Artistic images, Clip Art, Product images, Real-World</cell></row><row><cell>DomainNet (Peng et al., 2019)</cell><cell>569, 010</cell><cell>345</cell><cell>Clipart, Infograph, Painting, Quickdraw, Real, Sketch</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Dataset information for language experiments from<ref type="bibr" target="#b10">Gao et al. (2020)</ref>. |Y| denotes the number of categories. L refers to the average number of words in a sentence. The license of the GLUE benchmark is cc-by-4.0.</figDesc><table><row><cell>Category</cell><cell cols="2">Dataset |Y|</cell><cell>L</cell><cell>#Train</cell><cell>#Test</cell><cell>Type</cell><cell>Labels (classification tasks)</cell></row><row><cell></cell><cell>SST-2</cell><cell>2</cell><cell>19</cell><cell>6, 920</cell><cell>872</cell><cell>sentiment</cell><cell>positive, negative</cell></row><row><cell></cell><cell>SST-5</cell><cell>5</cell><cell>18</cell><cell>8, 544</cell><cell>2, 210</cell><cell>sentiment</cell><cell>v. pos., positive, neutral, negative, v. neg.</cell></row><row><cell></cell><cell>MR</cell><cell>2</cell><cell>20</cell><cell>8, 662</cell><cell>2, 000</cell><cell>sentiment</cell><cell>positive, negative</cell></row><row><cell>single-</cell><cell>CR</cell><cell>2</cell><cell>19</cell><cell>1, 775</cell><cell>2, 000</cell><cell>sentiment</cell><cell>positive, negative</cell></row><row><cell>sentence</cell><cell>MPQA</cell><cell>2</cell><cell>3</cell><cell>8, 606</cell><cell cols="3">2, 000 opinion polarity positive, negative</cell></row><row><cell></cell><cell>Subj</cell><cell>2</cell><cell>23</cell><cell>8, 000</cell><cell>2, 000</cell><cell>subjectivity</cell><cell>subjective, objective</cell></row><row><cell></cell><cell>TREC</cell><cell>6</cell><cell>10</cell><cell>5, 452</cell><cell>500</cell><cell>question cls.</cell><cell>abbr., entity, description, human, loc., num.</cell></row><row><cell></cell><cell>CoLA</cell><cell>2</cell><cell>8</cell><cell>8, 551</cell><cell>1, 042</cell><cell>acceptability</cell><cell>grammatical, not grammatical</cell></row><row><cell></cell><cell>QNLI</cell><cell cols="3">2 11/30 104, 743</cell><cell>5, 463</cell><cell>NLI</cell><cell>entailment, not entailment</cell></row><row><cell></cell><cell>MNLI</cell><cell cols="3">3 22/11 392, 702</cell><cell>9, 815</cell><cell>NLI</cell><cell>entailment, neutral, contradiction</cell></row><row><cell cols="2">sentence-RTE</cell><cell cols="2">2 49/10</cell><cell>2, 490</cell><cell>277</cell><cell>NLI</cell><cell>entailment, not entailment</cell></row><row><cell>pair</cell><cell>MRPC</cell><cell cols="2">2 22/21</cell><cell>3, 668</cell><cell>408</cell><cell>paraphrase</cell><cell>equivalent, not equivalent</cell></row><row><cell></cell><cell>QQP</cell><cell cols="4">2 12/12 363, 846 40, 431</cell><cell>paraphrase</cell><cell>equivalent, not equivalent</cell></row><row><cell></cell><cell>STS-B</cell><cell cols="2">R 11/11</cell><cell>5, 749</cell><cell cols="3">1, 500 sent. similarity -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>POUF on unseen classes for Webcam (W).</figDesc><table><row><cell></cell><cell cols="2">In-domain Setting</cell><cell cols="2">Out-of-domain Setting</cell></row><row><cell>Adaptation</cell><cell cols="4">seen class of Webcam (W) seen class of Webcam (W) seen class of Webcam (W) seen class of Webcam (W)</cell></row><row><cell>Testing</cell><cell>seen class of W</cell><cell>unseen class of W</cell><cell>seen class of A</cell><cell>unseen class of A</cell></row><row><cell>CLIP (zero-shot)</cell><cell>65.2</cell><cell>87.5</cell><cell>76.5</cell><cell>83.0</cell></row><row><cell>POUF (text prototypes = seen classes)</cell><cell>85.1</cell><cell>92.8</cell><cell>83.6</cell><cell>82.7</cell></row><row><cell>POUF (text prototypes = seen + unseen classes)</cell><cell>75.5</cell><cell>95.7</cell><cell>79.6</cell><cell>85.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>POUF on unseen classes for Amazon (A).In-domain SettingOut-of-domain Setting Adaptation seen class of Amazon (A) seen class of Amazon (A) seen class of Amazon (A) seen class of Amazon (A)</figDesc><table><row><cell>Testing</cell><cell></cell><cell></cell><cell>seen class of A</cell><cell></cell><cell cols="2">unseen class of A</cell><cell></cell><cell cols="2">seen class of W</cell><cell>unseen class of W</cell></row><row><cell>CLIP (zero-shot)</cell><cell></cell><cell></cell><cell>76.5</cell><cell></cell><cell></cell><cell>83.0</cell><cell></cell><cell></cell><cell>65.2</cell><cell>87.5</cell></row><row><cell cols="2">POUF (text prototypes = seen classes)</cell><cell></cell><cell>84.1</cell><cell></cell><cell></cell><cell>81.7</cell><cell></cell><cell></cell><cell>81.1</cell><cell>89.4</cell></row><row><cell cols="2">POUF (text prototypes = seen + unseen classes)</cell><cell></cell><cell>80.6</cell><cell></cell><cell></cell><cell>87.2</cell><cell></cell><cell></cell><cell>72.2</cell><cell>93.1</cell></row><row><cell cols="7">F. Additional ablation study on the hyper-parameter ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell cols="11">Accuracy 87.5 88.3 90.6 91.7 90.8 89.2 90.6 89.8 88.9 91.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Accuracy of POUF on the Webcam domain of the Office-31 dataset for different ? values.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Camillia Smith Barnes</rs> for providing valuable feedback on the paper. <rs type="person">K. Tanwisuth</rs>, <rs type="person">S. Zhang</rs>, <rs type="person">H. Zheng</rs>, and <rs type="person">M. Zhou</rs> acknowledge the support of <rs type="funder">NSF</rs>-IIS 2212418 and <rs type="institution">Texas Advanced Computing Center</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models: Appendix A. Detailed implementation details A.1. Language-augmented Vision models</p><p>We build our method using the open-source CLIP codebase <ref type="bibr" target="#b37">(Radford et al., 2021;</ref><ref type="bibr">Liu et al., 2021b)</ref> and TLlib transfer learning library <ref type="bibr" target="#b15">(Jiang et al., 2022)</ref>. For all experiments, we adopt the ViTB-16 for the image encoder and the default transformer from the CLIP paper for the text encoder. All the unlabeled target samples are used for fine-tuning. The learning rate shcedule is set to ? iter = ? 0 (1 + ?iter) -? , where ? 0 is the initial learning rate. We adopt the following default hyper-parameters: ? = 0.0002, and ? = 0.75. We set ? 0 = 5e-7 for all experiments except for prompt tuning on Office-31 where ? 0 = 1e-3. For prompt tuning, the context length, p, is set to 4. We then initialize the soft-prompt parameters with the embeddings of the words "A photo of a". We use a mini-batch SGD with a momentum of 0.9 and a batch size of 96 for Office-31 and Office-Home and 16 for DomainNet. The weight of the mutual-information objective, ?, is set to 0.3 for all experiments. We run all experiments for 5, 000 iterations using the seeds {0, 1, 2} and report the average accuracy. All experiments are conducted using a single Nvidia Tesla V100 GPU.</p><p>For all the baselines, we use the same hyper-parameters as our method for prompt tuning. For Tent, we set the weight of the entropy objective to 0.3, the same as the weight of the mutual-information objective, ?. For UPL, we follow the original paper to set the number of pseudo labels as 16 examples per class. For CoOp, we set the number of few-shot examples per class as 16 per the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Masked-language models</head><p>We follow the experimental protocols in <ref type="bibr" target="#b10">Gao et al. (2020)</ref>. Specifically, for each task, the data is split into D train , D dev , and D test . The authors tune the hyper-parameters on D dev and report the performance of the model on D test . We take the same hyper-parameters as the original paper: batch size = 8, learning rate = 1e -5, training steps = 1, 000, and few-shot examples per class = 16. The weight of the mutual-information objective, ?, is set to 0.6 for all experiments except for the unsupervised setting on the MRPC and QQP tasks. In these two tasks, the weights are set to 1e-4 for the mutual-information objective and to 1e-2 for the transport cost. We validate the performance of the model every 100 steps on the development set and take the best validated checkpoint for the final evaluation on the test set. For the unsupervised setting, we use the dataset provided by <ref type="bibr" target="#b10">Gao et al. (2020)</ref> without the labels. For the few-shot setting, we incorporate POUF into LM-BFF by using the few-shot examples without labels to compute POUF's loss and add it to LM-BFF's objective. We run all experiments using the seeds {42, 21, 87, 13, 100} and report the average accuracy. All experiments are conducted on four Nvidia Tesla V100 GPUs. In Table <ref type="table">5</ref>, we present parameter and run-time analyses of prompt and model tuning for the CLIP model. Prompt tuning introduces h (Embedding dimension) ? p (Context length) = 512 ? 4 = 2048 number of parameters to the model, increasing the total number of parameters slightly. However, given the size of the CLIP model, the increase is negligible. We note that the number of tunable parameters is 2, 049 since we also adjust the temperature parameter, a scalar, in the CLIP model. Since the number of tunable parameters is significantly lower in prompt tuning than in model tuning, it is not surprising that the training time per iteration for prompt tuning is one-quarter that of model tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter size and runtime analysis</head><p>Algorithm 2 POUF Pseudocode for masked-language models, PyTorch-like </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2724" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unbalanced optimal transport: Dynamic and kantorovich formulations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schmitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-X</forename><surname>Vialard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3090" to="3123" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A survey of vision-language pre-trained models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10936</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian attention modules</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16362" to="16376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<title level="s">JMLR Workshop and Conference Proceedings</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="281" to="296" />
		</imprint>
		<respStmt>
			<orgName>CAP</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning prototype-oriented set representations for metalearning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>id=WH6u2SvlLp4</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised prompt learning for vision-language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03649</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Transferability in deep learning: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Discriminative clustering by regularized information maximization. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal source-free domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4544" to="4553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Class-incremental domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Revanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII 16</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain impression: A source data free domain adaptation method</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="615" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Model adaptation: Unsupervised domain adaptation without source data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9641" to="9650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6028" to="6039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fusedream: Training-free text-to-image generation with improved clip+ gan space optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01573</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discrete optimal transport: complexity, geometry and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>M?rigot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="283" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10288</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transferrable prototypical networks for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8050" to="8058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Universal domain adaptation through self supervision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07953</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07676</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Information-theoretical learning of discriminative clusters for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6438</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Autoprompt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<title level="m">Eliciting knowledge from language models with automatically generated prompts</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A prototype-oriented framework for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tanwisuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17194" to="17208" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A prototypeoriented clustering for domain shift with source privacy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tanwisuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03807</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Universal adversarial triggers for attacking and analyzing nlp</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07125</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10726</idno>
		<title level="m">Tent: Fully test-time adaptation by entropy minimization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Representing mixtures of word embeddings with mixtures of topic embeddings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanwisuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=IYMuTbGzjFU" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Assaying out-of-distribution generalization in transfer learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dittadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kernert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.09239</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Entropy minimization vs. diversity maximization for domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01690</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Prototypical cross-domain selfsupervised learning for few-shot unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13834" to="13844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bayesian attention belief networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Alignment attention by matching key and query distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanwisuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021-12">Dec. 2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Capturing label distribution: A case study in nli</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06859</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Learning with different amounts of annotation: From zero to many labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04408</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Allsh: Active learning guided by local sensitivity and hardness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.04980</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploiting chain rule and Bayes&apos; theorem to compare probability distributions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14993" to="15006" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Conditional prompt learning for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16816" to="16825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
