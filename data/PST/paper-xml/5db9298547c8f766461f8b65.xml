<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FRAME ATTENTION NETWORKS FOR FACIAL EXPRESSION RECOGNITION IN VIDEOS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-12">12 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<email>xj.peng@siat.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<email>kai.wang@siat.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Key</orgName>
								<orgName type="laboratory">Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Shenzhen, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FRAME ATTENTION NETWORKS FOR FACIAL EXPRESSION RECOGNITION IN VIDEOS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-12">12 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1907.00193v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>facial expression recognition</term>
					<term>audiovideo emotion recognition</term>
					<term>frame attention networks</term>
					<term>CNN</term>
					<term>AFEW</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The video-based facial expression recognition aims to classify a given video into several basic emotions. How to integrate facial features of individual frames is crucial for this task. In this paper, we propose the Frame Attention Networks (FAN) 1 , to automatically highlight some discriminative frames in an end-to-end framework. The network takes a video with a variable number of face images as its input and produces a fixed-dimension representation. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which embeds face images into feature vectors. The frame attention module learns multiple attention weights which are used to adaptively aggregate the feature vectors to form a single discriminative video representation. We conduct extensive experiments on CK+ and AFEW8.0 datasets. Our proposed FAN shows superior performance compared to other CNN based methods and achieves state-of-the-art performance on CK+.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic facial expression recognition (FER) has recently attracted increasing attention in academia and industry due to its wide range of applications such as affective computing, intelligent environments, and multimodal human-computer interface (HCI). Though great progress have been made recently, facial expression recognition in the wild remains a challenging problem due to large head pose, illumination variance, occlusion, motion blur, etc.</p><p>Video-based facial expression recognition aims to classify a video into several basic emotions, such as happy, angry, dis-gust, fear, sad, neutral, and surprise. Given a video, the popular FER pipeline with a visual clue (FER with an audio clue is out of the scope of this paper) mainly includes three steps, namely frame preprocessing, feature extraction, and classification. Especially, frame preprocessing refers to face detection, alignment, illumination normalizing and so on. Feature extraction or video representation is the key part for FER which encodes frames or sequences into compact feature vectors. These feature vectors are subsequently fed into a classifier for prediction.</p><p>Feature extraction methods for video-based FER can be roughly divided into three types: static-based methods, spatial-temporal methods, and geometry-based methods.</p><p>Static-based feature extraction methods mainly inherit those methods from static image emotion recognition which can be both hand-crafted <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and learned <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. For the hand-crafted features, Littlewort et al. <ref type="bibr" target="#b0">[1]</ref> propose to use a bank of 2D Gabor filters to extract facial features for videobased FER. Shan et al. <ref type="bibr" target="#b1">[2]</ref> use local binary patterns (LBP) and LBP histogram for facial feature extraction. For the learned features, Tang <ref type="bibr" target="#b2">[3]</ref> utilizes deep CNNs for feature extraction, and win the FER2013. Some winners in audio-video emotion recognition task of EmotiW2016 and EmotiW2017 only use static facial features from deep CNNs trained on large face datasets or trained with multi-level supervision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Spatial-temporal methods aim to model the temporal or motion information in videos. The Long Short-Term Memory (LSTM) <ref type="bibr" target="#b5">[6]</ref>, and C3D <ref type="bibr" target="#b6">[7]</ref> are two widely-used spatialtemporal methods for video-based FER. LSTM derives information from sequences by exploiting the fact that feature vectors are connected semantically for successive data. This pipeline is widely-used in the EmotiW challenge, e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. C3D, which is originally developed for video action recognition, is also popular in the EmotiW challenge.</p><p>Geometry based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> aim to model the motions of key points in faces which only leverage the geometry locations of facial landmarks in every video frames. Jung et al. <ref type="bibr" target="#b11">[12]</ref> propose a deep temporal appearance-geometry network (DTAGN) which first alternately concatenates the xcoordinates and y-coordinates of the facial landmark points from each frame after normalization and then concatenates these normalized points over time for a one-dimensional trajectory signal of each sequence. Yan et al. <ref type="bibr" target="#b10">[11]</ref> construct an image-like map by stretching all the normalized facial point trajectories in a sequence together as the input of a CNN. Among all the above methods, static-based methods are superior to the others according to several winner solutions in EmotiW challenges. To obtain a video-level result with varied frames, a frame aggregation operation is necessary for staticbased methods. For frame aggregation, Kahou et al. <ref type="bibr" target="#b12">[13]</ref> concatenate the n-class probability vectors of 10 segments to form a fixed-length video representation by frame averaging or frame expansion. Bargal et al. <ref type="bibr" target="#b3">[4]</ref> propose a statistical encoding module (STAT) to aggregate frame features which compute the mean, variance, minimum, and maximum of the frame feature vectors.</p><p>One limitation of these existing aggregation methods is that they ignore the importance of frames for FER. For example, some faces in Figure <ref type="figure" target="#fig_0">1</ref> are representative for the 'happy' category while the others not. In this paper, inspired by the attention mechanism <ref type="bibr" target="#b13">[14]</ref> of machine translation and the neural aggregation networks <ref type="bibr" target="#b14">[15]</ref> of video face recognition, we propose the Frame Attention Networks (FAN) to adaptively aggregate frame features. The FAN is designed to learn self-attention kernels and relation-attention kernels for frame importance reasoning in an end-to-end fashion. The self-attention kernels are directly learned from frame features while the relation-attention kernels are learned from the concatenated features of a video-level anchor feature and frame features. We conduct extensive experiments on CK+ and AFEW8.0 (EmotiW2018) datasets. Our proposed FAN shows superior performance compared to other CNN based methods with only facial features and achieves state-of-theart performance on CK+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FRAME ATTENTION NETWORKS</head><p>We propose Frame Attention Networks (FAN) for videobased facial expression recognition (FER). Figure <ref type="figure" target="#fig_0">1</ref> illustrates the framework of our proposed FAN. It takes a facial video with a variable number of face images as its input and produces a fixed-dimension feature representation for FER. The whole network consists of two modules: feature embedding module and frame attention module. The feature embedding module is a deep CNN which embeds each face image into a feature vector. The frame attention module learns two-level attention weights, i.e. self-attention weights and relation-attention weights, which are used to adaptively aggregate the feature vectors to form a single discriminative video representation.</p><p>Formally, we denote a video with n frames as V, and its frames as I 1 , I 2 , • • • , I n , and the facial frame features are</p><formula xml:id="formula_0">{f 1 , • • • , f n }.</formula><p>Self-attention weights. With individual frame features, our FAN first applies a FC layer and a sigmoid function to assign coarse self-attention weights. Mathematically, the selfattention weight of the i-th frame is defined by:</p><formula xml:id="formula_1">α i = σ(f T i q 0 )<label>(1)</label></formula><p>where q 0 is the parameter of FC, σ denotes the sigmoid function. With these self-attention weights, we aggregate all the input frame features into a global representation f v as follows,</p><formula xml:id="formula_2">f v = n i=1 α i f i n i=1 α i .<label>(2)</label></formula><p>We use f v as a video-level global anchor for learning further accurate relation-attention weights.</p><p>Relation-attention weights. We believe that learning weights from both a global feature and local features is more reliable. The self-attention weights are learned with individual frame features and non-linear mapping, which are rather coarse. Since f v inherently contains the contents of the whole video, the attention weights can be further refined by modeling the relation between frame features and this global representation f v .</p><p>Inspired by the relation-Net in low-shot learning <ref type="bibr" target="#b15">[16]</ref>, we use the sample concatenation and another FC layer to estimate new relation-attention weights for frame features. The relation-attention weight of the i-th frame is formulated as,</p><formula xml:id="formula_3">β i = σ([f i : f v ] T q 1 ),<label>(3)</label></formula><p>where q 1 is the parameter of FC, σ denotes the sigmoid function.</p><p>Finally, with self-attention and relation-attention weights, our FAN aggregates all the frame features into a new compact feature as,</p><formula xml:id="formula_4">f v = n i=0 α i β i [f i : f v ] n i=0 α i β i . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>3. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Implementation Details</head><p>CK+ <ref type="bibr" target="#b16">[17]</ref> contains 593 video sequences from 123 subjects. Among these videos, 327 sequences from 118 subjects are labeled with seven basic expression labels, i.e. anger, contempt, disgust, fear, happiness, sadness, and surprise. Since CK+ does not provide training/testing splits, most of the algorithms evaluated on this database with 10-fold person-independence cross-validation experiments. We constructed 10 subsets by sampling ID in ascending order with a step size of 10 as in several previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and report the overall accuracy over 10 folds. AFEW 8.0 <ref type="bibr" target="#b19">[20]</ref> served as an evaluation platform for the annual EmotiW since 2013. Seven emotion labels are included in AFEW, i.e. anger, disgust, fear, happiness, sadness, surprise and neutral. AFEW contains video clips collected from different movies and TV serials with spontaneous expressions, various head poses, occlusions, illuminations. AFEW 8.0 is divided into three splits: Train (773 samples), Val (383 samples) and Test (653 samples), which ensures data in the three sets belong to mutually exclusive movies and actors. Since the test split is not publicly available, we train our model on training split and report results on validation split.</p><p>Implementation details. We preprocess video frames by face detection and alignment in the Dlib toolbox We extend the face bounding box with a ratio of 25% and then resize the cropped faces to scale of 224×224. We implement our method by the Pytorch toolbox. By default, for feature embedding, we use the ResNet18 which is pre-trained on MS-Celeb-1M <ref type="bibr" target="#b20">[21]</ref> face recognition dataset and FER Plus expression dataset <ref type="bibr" target="#b21">[22]</ref>. For training, on both CK+ and AFEW 8.0, we set a batch to have 48 instances with K frames in each instance. For frame sampling in a video, we first split the video into K segments and then randomly select one frame from each segment. By default, we set K to 3. We use the SGD method for optimization with a momentum of 0.9 and a weight decay of 10 −4 . On CK+, we initialize the learning rate (lr) to 0.1, and modify it to 0.02 at 30 epochs, and stop training after 60 epochs. On AFEW 8.0, we initialize the lr to 4e-6, and modify it to 8e-7 at 60 epochs and 1.6e-7 at 120 epochs, and stop training after 180 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation on CK+</head><p>We evaluate our FAN on CK+ with comparisons to several state-of-the-art methods in Table <ref type="table">1</ref>. On CK+, due to the fact that the videos show a shift from a neutral facial expression to the peak expression, most of the methods conduct data selection manually. Zhang et al <ref type="bibr" target="#b22">[23]</ref> propose to combine a spatial CNN model and a temporal network, where the spatial CNN model only uses the last peak frame. Jung et al <ref type="bibr" target="#b11">[12]</ref> select a fixed length sequence for each video with a lipreading method <ref type="bibr" target="#b25">[26]</ref>, and jointly fine-tune a deep temporal Table <ref type="table">1</ref>. Evaluation of our FAN with a comparison to stateof-the-art methods on CK+ database. Note that only those methods evaluated with 7 classes are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training data Test data Acc.</p><p>ST network <ref type="bibr" target="#b22">[23]</ref> S: the last frame T: all frames S: the last frame T: all frames 98.50 DTAGN <ref type="bibr" target="#b11">[12]</ref> Fixed length Fixed length 97.25</p><p>CNN+Island loss <ref type="bibr" target="#b23">[24]</ref> The last three frames and the first frame</p><p>The last three frames and the first frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>94.35</head><p>LOMo <ref type="bibr" target="#b24">[25]</ref> All appearance-geometry network. Cai et al <ref type="bibr" target="#b23">[24]</ref> select the last three frames and the first frame for each video, and train CNN models with a new Island loss function. We argue that manual data selection is an ad-hoc operation on CK+ and it is impractical since we can not know which is the peak frame beforeahead. Sikka et al <ref type="bibr" target="#b24">[25]</ref> use all frames with a new latent ordinal model which extracts CNN/LBP/SIFT features for sub-event detection and uses multi-instance SVM for expression classification. Our baseline method uses ResNet18 to generate scores for individual frame and applies score fusion (summation) for all frames. It achieves 94.8% which is 2.8% better than <ref type="bibr" target="#b24">[25]</ref>. Our proposed FAN with only selfattention gets 99.08% which significantly boosts the baseline by 4.28%. Adding relation-attention weights further improves the accuracy to 99.69% which sets up a new state of the art on CK+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation on AFEW 8.0</head><p>From the view of performance, AFEW is one of the most challenging videos FER dataset. The EmotiW challenge shares the same data from 2016 to 2018. Table <ref type="table" target="#tab_1">2</ref> presents the evaluation of our FAN on AFEW with comparisons to recent state-of-the-art methods. For a fair comparison, we only list these results obtained by the best single models in previous works. From the last three rows of Table <ref type="table" target="#tab_1">2</ref>, our proposed FAN improves the baseline by 2.36%. Both <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b9">[10]</ref> use VGGFace backbone and a recurrent model with long-short-term memory units. These methods aim to capture temporal dynamic information for videos. Most of the methods focus on improving static face based CNN models and combine scores video-level FER. Both <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b28">[29]</ref> input two LBP maps and a gray image for CNN models. Deeplysupervised networks are used in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b29">[30]</ref>, which add supervision on intermediate layers. For static methods, <ref type="bibr" target="#b30">[31]</ref> gets slightly better performance than ours. However, <ref type="bibr" target="#b30">[31]</ref> uses DenseNet-161 and pretrains it on both large-scale face datasets and their own Situ emotion video dataset. Additionally, <ref type="bibr" target="#b30">[31]</ref> applies complicated post-processing which extracts frame features and compute their mean vector, max-pooled vector, and standard deviation vector. These vectors are then concatenated and finally fed into an SVM classifier. Overall, our FAN improves the baseline significantly and achieves performance comparable to that of the best previous single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visualization and Hyper-parameters</head><p>To better understand the self-attention and relation-attention modules in our FAN, we visualize the attention weights in Figure <ref type="figure" target="#fig_1">2</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows one sequence for each category with blue and orange weight bars, where blue bars represent the self-attention weights (i.e. α in Eq. ( <ref type="formula" target="#formula_1">1</ref>)) of our FAN w/o relation-attention and orange bars the final weights (i.e. αβ in Eq. ( <ref type="formula" target="#formula_4">4</ref>)) of our FAN. In total, both kinds of weights can reflect the importance of frames. Comparing the blue and orange bars, we find that the final weights of our FAN can always assign higher weights to the more obvious face frames, while self-attention module could assign high weights on some obscure face frames, see the 1st, 2th, and 3rd rows of Figure <ref type="figure" target="#fig_1">2</ref> (left). This explicitly explains why adding relation-attention boost performance. Evaluation of Hyper-parameters. We evaluate two hyper-parameters of our FAN on CK+, i.e. backbone CNN networks and the parameter K mentioned in implementation details, to validate the robustness of our method. For the parameter K, besides the default value, we try several other values, i.e. {2, 5, 8}, and find the performance is not sensitive to K. Specifically, our FAN obtains 99.39% with K={2, 5}. and gets 99.69% with K=8. Since the default value, K=3  For the backbone CNN model evaluation, we try the VGGFace model which is widely-used in previous works. Similarly, we also pretrain the VGGFace model on the FER-Plus dataset. Since <ref type="bibr" target="#b4">[5]</ref> shows that it is better to freeze all the feature learning layers after pretrained on FERPlus for VGGFace model, we also conduct the same experiment on CK+ with VGGFace. Figure <ref type="figure" target="#fig_2">3</ref> shows the default comparisons with different backbone CNN models. On CK+, compared with freezing all the feature layers for VGGFace, it gets better results with fine-tuning all layers which may be explained by the domain discrepancy between FERPlus and CK+. Overall, the results are significantly improved by self-attention weights and further improved by the relationattention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We propose Frame Attention Networks for video-based facial expression recognition. The FAN contains a self-attention module and a relation-attention module. The experiments on CK+ and AFEW show that our FAN with only self-attention improves the baseline significantly and adding relationattention further boosts performance. With a visualization on CK+, we demonstrate that our FAN can automatically capture the importance of frames. Our single model achieves performance on par with that of state-of-the-art methods on AFEW and obtains state-of-the-art results on CK+.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our proposed frame attention network architecture.</figDesc><graphic url="image-1.png" coords="2,321.31,72.00,231.60,192.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the self-attention weights (blue bar) and the final weights of FAN (orange bar) on CK+ dataset.</figDesc><graphic url="image-4.png" coords="4,315.21,188.69,243.78,127.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Evaluation of backbone CNN models and training strategies on CK+.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of our FAN with a comparison to stateof-the-art methods on AFEW 8.0 database. It is worth noting that we only compare to the best single models of previous works.</figDesc><table><row><cell>Method</cell><cell cols="2">Model type Accuracy</cell></row><row><cell>CNN-RNN (2016) [27]</cell><cell>Dynamic</cell><cell>45.43</cell></row><row><cell>VGGFace + Undirectional LSTM (2017) [10]</cell><cell>Dynamic</cell><cell>48.60</cell></row><row><cell>HoloNet (2016) [28]</cell><cell>Static</cell><cell>44.57</cell></row><row><cell>DSN-HoloNet (2017) [29]</cell><cell>Static</cell><cell>46.47</cell></row><row><cell>DenseNet-161 (2018) [31]</cell><cell>Static</cell><cell>51.44</cell></row><row><cell>DSN-VGGFace (2018) [30]</cell><cell>Static</cell><cell>48.04</cell></row><row><cell>Score fusion (baseline)</cell><cell>Static</cell><cell>48.82</cell></row><row><cell>FAN w/o Relation-attention</cell><cell>Static</cell><cell>50.92</cell></row><row><cell>FAN</cell><cell>Static</cell><cell>51.18</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China (U1613211, U1713208), Shenzhen Research Program (JCYJ20170818164704758, JSGG20180507182100698), and International Partnership Program of Chinese Academy of Sciences (172644KYSB20150019).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dynamics of facial expression extracted automatically from video</title>
		<author>
			<persName><forename type="first">Gwen</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marian</forename><forename type="middle">Stewart</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Movellan</surname></persName>
		</author>
		<editor>IVC</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
		<editor>IVC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emotion recognition in the wild from videos using images</title>
		<author>
			<persName><forename type="first">Adel</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Kuharenko</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Audio-visual emotion recognition using deep transfer learning and multiple temporal models</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigenori</forename><surname>Kawaai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ester</forename><surname>Gue Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengmei</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiping</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Yan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal multimodal fusion for video emotion classification in the wild</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stphane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frdric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-cue fusion for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuangao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunjeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><forename type="middle">Chandias</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zara</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<editor>CVPRW</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">Mengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A compact deep learning model for robust facial expression recognition</title>
		<author>
			<persName><forename type="first">Chieh-Ming</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hong</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Sarkis</surname></persName>
		</author>
		<editor>CVPRW</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Emotiw 2018: Audio-video, student engagement and group-level affect prediction</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanjot</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
		<idno>arXiv preprint:1808.07773</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for largescale face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training deep networks for facial expression recognition with crowd-sourced label distribution</title>
		<author>
			<persName><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on deep evolutional spatial-temporal networks</title>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Island loss for learning discriminative features in facial expression recognition</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Shehab Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Oreilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Tong</surname></persName>
		</author>
		<editor>FG</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lomo: Latent ordinal model for facial analysis in videos</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marian</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards a practical lipreading system</title>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Video-based emotion recognition using cnn-rnn and c3d hybrid networks</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangju</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Holonet: towards robust emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shandong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning supervised scoring ensemble for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shandong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Videobased emotion recognition using deeply-supervised neural networks</title>
		<author>
			<persName><forename type="first">Yingruo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><forename type="middle">Ck</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<editor>ACM ICMI</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-feature based emotion recognition for video clips</title>
		<author>
			<persName><forename type="first">Chuanhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
