<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast AutoAugment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
							<email>sungbin.lim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kakao</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
							<email>ildoo.kim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
							<email>taesup.kim@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
							<email>chiheon.kim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
							<email>swkim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MILA</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast AutoAugment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A5B398799442F3E569A78FF93C93035C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment <ref type="bibr" target="#b4">[5]</ref> has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet. Our code is open to the public by the official GitHub 2 of Kakao Brain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has become a state-of-the-art technique for computer vision tasks, including object recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, and segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. However, deep learning models with large capacity often suffer from overfitting unless significantly large amounts of labeled data are supported. Data augmentation (DA) has been shown as a useful regularization technique to increase both the quantity and the diversity of training data. Notably, applying a carefully designed set of augmentations rather than naive random transformations in training improves the generalization ability of a network significantly <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>. However, in most cases, designing such augmentations has relied on human experts with prior knowledge on the dataset.</p><p>With the recent advancement of automated machine learning (AutoML), there exist some efforts for designing an automated process of searching for augmentation strategies directly from a dataset. AutoAugment <ref type="bibr" target="#b4">[5]</ref> uses reinforcement learning (RL) to automatically find data augmentation policy when a target dataset and a model are given. It samples an augmentation policy at a time using a controller RNN, trains the model using the policy, and gets the validation accuracy as a reward to update the controller. AutoAugment especially achieves a dramatic improvement in performances on several image recognition benchmarks. However, AutoAugment requires thousands of GPU hours even in a reduced setting, in which the size of the target dataset and the network is small. Recently proposed Population Based Augmentation (PBA) <ref type="bibr" target="#b14">[15]</ref> is a method to deal with this problem, which is based on population-based training method of hyperparameter optimization. In contrast to previous methods, we propose a new search strategy that does not require any repeated training of child models. Instead, the proposed algorithm directly searches for augmentation policies that maximize the match between the distribution of augmented split and the distribution of another, unaugmented split via a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>AutoAug <ref type="bibr" target="#b4">[5]</ref> Fast AutoAug CIFAR-10 5000 3.5 SVHN 1000 1.5 ImageNet 15000 450</p><p>Table <ref type="table">1</ref>: GPU hours comparison of the proposed method with <ref type="bibr" target="#b4">[5]</ref>. We estimate computation cost with an NVIDIA Tesla V100 while AutoAugment measured computation cost in Tesla P100.</p><p>In this paper, we propose an efficient search method of augmentation policies, called Fast AutoAugment, motivated by Bayesian DA <ref type="bibr" target="#b35">[36]</ref>.</p><p>Our strategy is to improve the generalization performance of a given network by learning the augmentation policies which treat augmented data as missing data points of training data. However, different from Bayesian DA, the proposed method recovers those missing data points by the exploitation-and-exploration of a family of inference-time augmentations <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> via Bayesian optimization in the policy search phase. We realize this by using an efficient density matching algorithm that does not require any back-propagation for network training for each policy evaluation. The proposed algorithm can be easily implemented by making good use of distributed learning frameworks such as Ray <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our experiments show that the proposed method can search augmentation policies significantly faster than AutoAugment (see Table <ref type="table">1</ref>), while retaining comparable performances to AutoAugment on diverse image datasets and networks, especially in two use cases: (a) direct augmentation search on the dataset of interest, (b) transferring learned augmentation policies to new datasets. On ImageNet, we achieve an error rate of 19.4% for ResNet-200 trained with our searched policy, which is 0.6% better than 20.0% with AutoAugment.</p><p>This paper is organized as follows. First, we introduce related works on automatic data augmentation in Section 2. Then, we present our problem setting to achieve the desired goal and suggest Fast AutoAugment algorithm to solve the objective efficiently in Section 3. Finally, we demonstrate the efficiency of our method through comparison with baseline augmentation methods and AutoAugment in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are many studies on data augmentation, especially for image recognition. On the benchmark image dataset, such as CIFAR and ImageNet, random crop, flip, rotation, scaling, and color transformation, have been performed as baseline augmentation methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>. Mixup <ref type="bibr" target="#b40">[41]</ref>, Cutout <ref type="bibr" target="#b6">[7]</ref>, and CutMix <ref type="bibr" target="#b38">[39]</ref> have been recently proposed to either replace or mask out the image patches randomly and obtained more improved performances on image recognition tasks. However, these methods are designed manually based on domain knowledge.</p><p>Naturally, automatically finding data augmentation methods from data in principle has emerged to overcome the performance limitation that originated from a cumbersome exploration of methods by a human. Smart Augmentation <ref type="bibr" target="#b21">[22]</ref> introduced a network that learns to generate augmented data by merging two or more samples in the same class. <ref type="bibr" target="#b31">[32]</ref> employed a generative adversarial network (GAN) <ref type="bibr" target="#b8">[9]</ref> to generate images that augment datasets. Bayesian DA <ref type="bibr" target="#b35">[36]</ref> combined Monte Carlo expectation maximization algorithm with GAN to generate data by treating augmented data as missing data points on the distribution of the training set.</p><p>Due to the remarkable successes of NAS algorithms on various computer vision tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref>, several current studies also deal with automated search algorithms to obtain augmentation policies for given datasets and models. The main difference between the previously learned methods and these automated augmentation search methods is that the former methods exploit generative models to create augmented data directly, whereas the latter methods find optimal combinations of predefined transformation functions. AutoAugment <ref type="bibr" target="#b4">[5]</ref> introduced an RL based search strategy that alternately Operation 2 (autocontrast) p1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n z 4 K B J n J f G T y e F n p p A e g m r g W 8 9 g = "</p><formula xml:id="formula_0">&gt; A A A C H 3 i c b V B N T w I x F H x V V M Q v 0 K O X R m L i i e y i i R 6 J X j x i 4 g I J b E i 3 d K G h 2 9 2 0 X R O y 4 T d 4 1 a O / x p v x y r + x C 3 t Q c J I m k 5 n 3 8 q Y T J I J r 4 z g L t L V d 2 t n d K + 9 X D g 6 P j k + q t d O O j l N F m U d j E a t e Q D Q T X D L P c C N Y L 1 G M R I F g 3 W D 6 k P v d F 6 Y 0 j + W z m S X M j 8 h Y 8 p B T Y q z k J c P M n Q + r d a f h L I E 3 i V u Q O h R o D 2 u o N B j F N I 2 Y N F Q Q r f u u k x</formula><p>g / I 8 p w K t i 8 M k g 1 S w i d k j H r W y p J x L S f L d P O 8 a V V R j i M l X 3 S 4 K X 6 e y M j k d a z K L C T E T E T v e 7 l 4 r 9 e r i g d a m v i T X e k 8 3 N r 2 U x 4 5 2 d c J q l h k q 6 i h a n A J s Z 5 W   </p><formula xml:id="formula_1">X j E F a N G z C w h V H H 7 O 0 w n R B F q b K U V 2 5 u 7 3 t I m 6 T Q b 7 n W j + X R T b 9 0 X D Z b h H C 7 g C l y 4 h R Y 8 Q h s 8 o M D h F d 7 g H X 2 g T / S F v l e j W 6 j Y O Y M / Q I s f 8 Y u i Q Q = = &lt; / l a t e x i t &gt; 1 p1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v i Y H h L r L g a b y c Q a y x q Q r t L C v n B Y = " &gt; A A A C I X i c b V B N S w M x F H y p V W v 9 a v X o J V g E L 5 b d K u i x 6 M V j B f s B 7 V K y a b Y N z W a X J C u U p T / C q x 7</formula><formula xml:id="formula_2">k Y R T U I m D R V E 6 7 7 r x M Z L i T K c C j Y v D x L N Y k K n Z M z 6 l k o S M u 2 l i 7 x z f G a V E Q 4 i Z Z 8 0 e K H + 3 k h J q P U s 9 O 1 k S M x E r 3 q Z + K + X K U o H 2 p p 4 3 R 3 p 7 N x K N h P c e C m X c W K Y p M t o Q S K w i X B W F x 5 x x a g R M 0 s I V d z + D t M J U Y Q a W 2</formula><formula xml:id="formula_3">= " &gt; A A A C H 3 i c b V B N T w I x F H x V V M Q v 0 K O X R m L i i e y i i R 6 J X j x i 4 g I J b E i 3 d K G h 2 9 2 0 X R O y 4 T d 4 1 a O / x p v x y r + x C 3 t Q c J I m k 5 n 3 8 q Y T J I J r 4 z g L t L V d 2 t n d K + 9 X D g 6 P j k + q t d O O j l N F m U d j E a t e Q D Q T X D L P c C N Y L 1 G M R I F g 3 W D 6 k P v d F 6 Y 0 j + W z m S X M j 8 h Y 8 p B T Y q z k J c O s O R 9 W 6 0 7 D W Q J v E r c g d S j Q H t Z Q a T C K a R o x a a g g W v d d J z F + R p T h V L B 5 Z Z B q l h A 6 J W P W t 1 S S i G k / W 6 a d 4 0 u r j H A Y K / u k w U v 1 9 0 Z G I q 1 n U W A n I 2 I m e t 3 L x X + 9 X F E 6 1 N b E m + 5 I 5 + f W s p n w z s + 4 T F L D J F 1 F C 1 O B T Y z z s v C I K 0 a N m F l C q O L 2 d 5 h O i C L U 2 E o r t j d 3 v a V N 0 m k 2 3 O t G 8 + m m 3 r o v G i z D O V z A F b h w C y 1 4 h D Z 4 Q I H D K 7 z B O / p A n + g L f a 9 G t 1 C x c w Z / g B Y / 8 0 O i Q g = = &lt; / l a t e x i t &gt; p2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / U C e v E v w t P C R 4 k 9 Y p g k 0 3 I / h M q I = " &gt; A A A C H 3 i c b V B N T w I x F H x V V M Q v 0 K O X R m L i i e y i i R 6 J X j x i 4 g I J b E i 3 d K G h 2 9 2 0 X R O y 4 T d 4 1 a O / x p v x y r + x C 3 t Q c J I m k 5 n 3 8 q Y T J I J r 4 z g L t L V d 2 t n d K + 9 X D g 6 P j k + q t d O O j l N F m U d j E a t e Q D Q T X D L P c C N Y L 1 G M R I F g 3 W D 6 k P v d F 6 Y 0 j + W z m S X M j 8 h Y 8 p B T Y q z k J c O s O R 9 W 6 0 7 D W Q J v E r c g d S j Q H t Z Q a T C K a R o x a a g g W v d d J z F + R p T h V L B 5 Z Z B q l h A 6 J W P W t 1 S S i G k / W 6 a d 4 0 u r j H A Y K / u k w U v 1 9 0 Z G I q 1 n U W A n I 2 I m e t 3 L x X + 9 X F E 6 1 N b E m + 5 I 5 + f W s p n w z s + 4 T F L D J F 1 F C 1 O B T Y z z s v C I K 0 a N m F l C q O L 2 d 5 h O i C L U 2 E o r t j d 3 v a V N 0 m k 2 3 O t G 8 + m m 3 r o v G i z D O V z A F b h w C y 1 4 h D Z 4 Q I H D K 7 z B O / p A n + g L f a 9 G t 1 C x c w Z / g B Y / 8 0 O i Q g = = &lt; / l a t e x i t &gt; 1 p2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w p G n K D j z n D e y Y F G + 6 5 K h Q 9 f 9 A H M = " &gt; A A A C I X i c b V B N S w M x F H y p V W v 9 a v X o J V g E L 5 b d K u i x 6 M V j B f s B 7 V K y a b Y N z W a X J C u U p T / C q x 7 9 N d 7 E m / h n z L Z 7 0 N a B w D D z H m 8 y f i y 4 N o 7 z h Q o b x c 2 t 7 d J O e X d v / + C w U j 3 q 6 C h R l L V p J C L V 8 4 l m g k v W N t w I 1 o s V I 6 E v W N e f 3 m V + 9 4 k p z S P 5 a G Y x 8 0 I y l j z g l B g r d d 2 L e J g 2 5 s N K z a k 7 C + B 1 4 u a k B j l a w y o q D k Y R T U I m D R V E 6 7 7 r x M Z L i T K c C j Y v D x L N Y k K n Z M z 6 l k o S M u 2 l i 7 x z f G a V E Q 4 i Z Z 8 0 e K H + 3 k h J q P U s 9 O 1 k S M x E r 3 q Z + K + X K U o H 2 p p 4 3 R 3 p 7 N x K N h P c e C m X c W K Y p M t o Q S K w i X B W F x 5 x x a g R M 0 s I V d z + D t M J U Y Q a W 2 r Z 9 u a u t r R O O o 2 6 e 1 l v P F z V m r d 5 g y U 4 g V M 4 B x e u o Q n 3 0 I I 2 U J j C M 7 z A K 3 p D 7 + g D f S 5 H C y j f O Y Y / Q N 8 / 4 M u i t A = = &lt; / l a t e x i t &gt; O1(x; 1) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y K A q 4 G Y u T b Y b n 2 Q x r E 6 y E p U H 0 z g = " &gt; A A A C O n i c b V D L S s N A F J 3 U q r W + W l 0 K M l i E u i l J F R T c F N 2 4 s 4 J 9 Q B P C Z D J p h 0 4 m Y W Y i l p C d X + N W l / 6 I W 3 f i 1 g 9 w 0 n a h r R c G z p x z D / f e 4 8 W M S m W a 7 0 Z h p b i 6 t l 7 a K G 9 u b e / s V q p 7 X R k l A p M O j l g k + h 6 S h F F O O o o q R v q x I C j 0 G O l 5 4 + t c 7 z 0 Q I W n E 7 9 U k J k 6 I h p w G F C O l K b d y a I d I j T B i 6 W 3 m p l Z W f 7 y 0 m b b 7 K P + d u J W a 2 T C n B Z e B N Q c 1 M K + 2 W z W K t h / h J C R c Y Y a k H F h m r J w U C U U x I 1 n Z T i S J E R</formula><formula xml:id="formula_4">M W D k p E o p i R r K y n U g S I z x G Q z L Q k K O Q S C e d H p L B Y 8 3 4 M I i E f l z B K f v b k a J Q y k n o 6 c 5 8 b b m o 5 e S / W s 4 I G U g t w m X V l / m 4 h d 1 U c O G k l M e J I h z P V g s S B l U E 8 x y h T w X B i k 0 0 Q F h Q f R 3 E I y Q Q V j r t s s 7 N W k x p G X S b</formula><p>D e u 0 0 b w 7 q 7 W u 5 g m W w A E 4 A n V g g X P Q A j e g D T o A g y f w D F 7 A q / F m f B i f x t e s t W D M P f v g T x n f P 8 A e r M Q = &lt; / l a t e x i t &gt; O2(x; 2)  i has two parameters: the probability p i of calling the operation and the magnitude λ i of the operation. These operations are applied with the corresponding probabilities. As a result, a sub-policy randomly maps an input data to the one of 4 images. Note that the identity map (no augmentation) is also possible with probability (1p 1 )(1p 2 ). trained a child model and RNN controller and showed the state-of-the-art performances on various datasets with different models. Recently, PBA <ref type="bibr" target="#b14">[15]</ref> proposed a new algorithm which generates augmentation policy schedules based on population based training <ref type="bibr" target="#b16">[17]</ref>. Similar to PBA, our method also employs hyperparameter optimization to search for optimal policies but uses Tree-structured Parzen Estimator (TPE) algorithm <ref type="bibr" target="#b1">[2]</ref> for practical implementation.</p><formula xml:id="formula_5">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U T v x w h / X v R 1 0 1 l O 2 0 a W L 1 5 Y t J Z 4 = " &gt; A A A C Q n i c b V C 7 T s M w F H V K g V J e L Y w s F h W o L F V S k E B i q W B h o 0 j 0 I T V R 5 D h O a 9 V x I t t B V F H + g K 9 h h Z G f 4 B f Y E C s D T t s B W o 5 k 6 e i c e 6 / v P V 7 M q F S m + W 4 U V o q r a + u l j f L m 1 v b O b q W 6 1 5 V R I j D p 4 I h F o u 8 h S R j l p K O o Y q Q f C 4 J C j 5 G e N 7 7 O / d 4 D E Z J G / F 5 N Y u K E a M h p Q D F S W n I r x 3 a I 1 A g j l t 5 m b t r M 6 r a i z C f p Y 3 Z p M z 3 G R 7 l 6 4 l Z q Z s O c A i 4 T a 0 5 q Y I 6 2 W z W K t h / h J C R c Y Y a k H F h m r J w U C U U x I 1 n Z T i S J E R 6 j I R l o y l F I p J N O D 8 r g k V Z 8 G E R C P 6 7 g V P 3 d k a J Q y k n o 6 c p 8 f b n o 5 e K / X q 4 I G U h t w m X X l / l 3 C 7 u p 4 M J J K Y 8 T R T i e r R Y k D K o I 5 n l C n w q C F Z t o g r C g + j q I R 0 g g r H T q Z Z 2 b t Z j S M u k 2 G 9 Z p o 3 l 3 V m t d z R M s g Q N w C O r A A u e g B W 5 A G 3 Q A B k / g</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fast AutoAugment</head><p>In this section, we first introduce the search space of the symbolic augmentation operations and formulate a new search strategy, efficient density matching, to find the optimal augmentation policies efficiently. We then describe our implementation based on Bayesian hyperparameter optimization incorporated into a distributed learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search Space</head><p>Let O be a set of augmentation (image transformation) operations O : X → X defined on the input image space X . Each operation O has two parameters: the calling probability p and the magnitude λ which determines the variability of operation. Some operations (e.g. invert, flip) do not use the magnitude. Let S be the set of sub-policies where a sub-policy τ ∈ S consists of</p><formula xml:id="formula_6">N τ consecutive operations { Ō(τ) n (x; p (τ ) n , λ<label>(τ )</label></formula><p>n ) : n = 1, . . . , N τ } where each operation is applied to an input image sequentially with the probability p as follows:</p><formula xml:id="formula_7">Ō(x; p, λ) := O(x; λ) : with probability p x : with probability 1 -p.<label>(1)</label></formula><p>Hence, the output of sub-policy τ (x) can be described by a composition of operations as</p><formula xml:id="formula_8">x(n) = Ō(τ) n (x (n-1)</formula><p>), n = 1, . . . , N τ where x(0) = x and x(Nτ ) = τ (x). Figure <ref type="figure" target="#fig_4">1</ref> shows a specific example of augmented images by τ . Note that each sub-policy τ is a random sequence of image transformations which depend on p and λ, and this enables to cover a wide range of data augmentations. Our final policy T is a collection of N T sub-policies and T (D) indicates a set of augmented images of dataset D transformed by every sub-policies τ ∈ T :</p><formula xml:id="formula_9">T (D) = τ ∈T {(τ (x), y) : (x, y) ∈ D}<label>(2)</label></formula><p>To achieve (3), we propose an efficient strategy for augmentation policy search (see Figure <ref type="figure">2</ref>). First, we conduct the K-fold stratified shuffling <ref type="bibr" target="#b30">[31]</ref> to split the train dataset into D A . As a matter of convenience, we omit k in the notation of datasets in the remaining parts. Next, we train model parameter θ on D M from scratch without data augmentation. Contrary to previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>, our method does not necessarily reduce the given network to child models or proxy tasks.</p><p>After training the model parameter, for each step 1 ≤ t ≤ T , we explore B candidate policies B = {T 1 , . . . , T B } via Bayesian optimization method which repeatedly samples a sequence of sub-policies from search space S to construct a policy T = {τ 1 , . . . , τ N T } and tunes corresponding calling probabilities {p 1 , . . . , p N T } and magnitudes {λ 1 , . . . , λ N T } to minimize the expected loss L(θ|•) on augmented dataset T (D A ) (see line 6 in Algorithm 1). Note that, during the policy exploration-and-exploitation procedure, the proposed algorithm does not train model parameter from scratch again, hence the proposed method find augmentation policies significantly faster than AutoAugment. The concrete Bayesian optimization method is explained in Section 3.2.2.</p><p>As the algorithm completes the exploration step, we select top-N policies over B and denote them T t collectively. Finally, we merge every T t into T * . See Algorithm 1 for the overall procedure. At the end of the process, we augment the whole dataset D train with T * and retrain the model parameter θ. Through the proposed method, we can expect the performance R(θ|•) on augmented dataset T * (D A ) is statistically higher than that on D A : R(θ|T</p><formula xml:id="formula_10">* (D A )) ≥ R(θ|D A )</formula><p>since augmentation policy T * works as optimized inference-time augmentation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> to make the model robustly predict correct answers. Consequently, learned augmentation policies approach (3) and improve generalization performance as we desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Policy Exploration via Bayesian Optimization</head><p>Policy exploration is an essential ingredient in the process of automated augmentation search. Since the evaluation of the model performance for every candidate policies is computationally expensive, we apply Bayesian optimization to the exploration of augmentation strategies. Precisely, at the line 6 in Algorithm 1, we employ the following Expected Improvement (EI) criterion <ref type="bibr" target="#b17">[18]</ref> for acquisition function to explore candidate policies B efficiently:</p><formula xml:id="formula_11">EI(T ) = E min(L(θ|T (D A )) -L † , 0) = min(L -L † , 0)P θ,D A (L|T )dL<label>(4)</label></formula><p>Here the expectation in ( <ref type="formula" target="#formula_11">4</ref>) is taken over the density function P θ,D A on the codomain of value of the loss function L(θ|T (D A )) which measures statistical potential of unexplored augmented data (τ (x), y) ∈ T (D A ) to approximate (3) for given pre-trained model M(•|θ). Recall that T consists of sub-policies τ 1 , . . . , τ N T and corresponding parameters {p 1 , . . . , p N T } and {λ 1 . . . , λ N T } hence the density function P θ,D A (L|T ) is actually determined by these parameters. L † in (4) denotes the constant threshold of loss value determined by the quantile of observations among previously explored policies. We employ variable kernel density estimation <ref type="bibr" target="#b34">[35]</ref> on graph-structured search space S to estimate the density function P θ,D A (L|T ) and eventually approximate the criterion (4). Practically, since the optimization method is already proposed in tree-structured Parzen estimator (TPE) algorithm <ref type="bibr" target="#b1">[2]</ref>, we apply their HyperOpt library for the parallelized implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>Fast AutoAugment searches desired augmentation policies applying aforementioned Bayesian optimization to distributed train splits. In other words, the overall search process consists of two steps, (1) training model parameters on K-fold train data with default augmentation rules and (2) exploration-and-exploitation using HyperOpt to search the optimal augmentation policies. In the below, we describe the practical implementation of the overall steps in Algorithm 1. The following procedures are mostly parallelizable, which makes the proposed method more efficient to be used in actual usage. We utilize Ray <ref type="bibr" target="#b23">[24]</ref> to implement Fast AutoAugment, which enables us to train models and search policies in a distributed manner.</p><p>Shuffle (Line 1): We split training sets while preserving the percentage of samples for each class (stratified shuffling) using StratifiedShuffleSplit method in sklearn <ref type="bibr" target="#b26">[27]</ref>. Merge (Line 7-9): Select the top N best policies for each split and then combine the obtained policies from all splits. This set of final policies is used for re-train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>In this section, we examine the performance of Fast AutoAugment (FAA) on the CIFAR-10, CIFAR-100 <ref type="bibr" target="#b19">[20]</ref>, and ImageNet <ref type="bibr" target="#b5">[6]</ref> datasets and compare the results with baseline preprocessing, Cutout <ref type="bibr" target="#b6">[7]</ref>, AutoAugment (AA) <ref type="bibr" target="#b4">[5]</ref>, and PBA <ref type="bibr" target="#b14">[15]</ref>. For ImageNet, we only compare the baseline, AA, and FAA since PBA does not conduct experiments on ImageNet. We follow the experimental setting of AA for fair comparison, except that an evaluation of the proposed method on AmoebaNet-B model <ref type="bibr" target="#b27">[28]</ref> is omitted. As in AA, each sub-policy consists of two operations (N τ = 2), each policy consists of five sub-policies (N T = 5), and the search space consists of the same 16 operations (ShearX, ShearY, TranslateX, TranslateY, Rotate, AutoContrast, Invert, Equalize, Solarize, Posterize, Contrast, Color, Brightness, Sharpness, Cutout, Sample Pairing). Interestingly, FAA is able to select Cutout in searched policies. We conjecture that Cutout can probably eliminate irrelevant backgrounds and improve the classification accuracy when the inference is performed on a well-trained network. We utilize 5-folds stratified shuffling (K = 5), 2 search width (T = 2), 200 search depth (B = 200), and 10 selected policies (N = 10) for policy evaluation. Due to the efficiency in the proposed search process, FAA can find more numbers of optimized augmentation policies, almost regardless of its number. Therefore, we can consider the number of sub-policies as a hyperparameter to tune.</p><p>When we use a multi-threading functionality for data augmentation, we observe that there is no actual extension of training time by augmentation in comparison to the baseline without augmentation. Moreover, even when we perform both the data augmentation and weight updating by SGD in a single thread as a sequential processing, the increased training time that we observe is only 10-20% over 200 epochs; in total, less than 5 hours on CIFAR-10/100 with WResNet28x10 and a single V100 GPU. Hence the training time overhead by increased number of sub-policies is also limited. Having this in mind, we performed FAA with different numbers of sub-policies and determined the number of sub-policies that produces the best average performances across different datasets and networks. However, as shown in Figure <ref type="figure" target="#fig_7">3</ref>, the performances obtained by 25 numbers of sub-policies are also comparable to those by more numbers of sub-policies. We increase the batch size and adapt the learning rate accordingly to boost the training <ref type="bibr" target="#b37">[38]</ref>. Otherwise, we set other hyperparameters equal to AA if possible. For the unknown hyperparameters, we follow values from the original references or we tune them to match baseline performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Baseline Cutout <ref type="bibr" target="#b6">[7]</ref> AA <ref type="bibr" target="#b4">[5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR-10 and CIFAR-100</head><p>For both CIFAR-10 and CIFAR-100, we conduct two experiments using FAA: (1) direct search on the full dataset given target network (2) transfer policies found by Wide-ResNet-40-2 on the reduced CIFAR-10 which consists of 4,000 randomly chosen examples. As shown in Table <ref type="table" target="#tab_0">2</ref> and<ref type="table" target="#tab_1">3</ref>, overall, FAA significantly improves the performances of the baseline and Cutout for any network while achieving comparable performances to those of AA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 Results</head><p>In Table <ref type="table" target="#tab_0">2</ref>, we present the test set accuracies according to different models. We examine Wide-ResNet-40-2, Wide-ResNet-28-10 [40], Shake-Shake <ref type="bibr" target="#b7">[8]</ref>, Shake-Drop <ref type="bibr" target="#b36">[37]</ref> models to evaluate the test set accuracy of FAA. It is shown that, FAA achieves comparable results to AA and PBA on both experiments. We emphasize that it only takes 3.5 GPU-hours for the policy search on the reduced CIFAR-10. We also estimate the search time via full direct search. By considering the worst case, Pyramid-Net+ShakeDrop requires 780 GPU-hours which is even less than the computation time of AA (5000 GPU-hours).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100 Results</head><p>Results are shown in Table <ref type="table" target="#tab_1">3</ref>. Again, FAA achieves significantly better results than baseline and cutout. However, except Wide-ResNet-40-2, FAA shows slightly worse results than AA and PBA. Nevertheless, the search costs of the proposed method on CIFAR-100 are same as those on CIFAR-10. We conjecture the performance gaps between other methods and FAA are probably caused by the insufficient policy search in the exploration procedure or the over-training of the model parameters in the proposed algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SVHN</head><p>We conducted an experiment with the SVHN dataset <ref type="bibr" target="#b24">[25]</ref> with the same settings in AA. We chose 1,000 examples randomly and applied FAA to find augmentation policies. The obtained policies are applied to an initial model and we obtain the comparable performance to AA. Results are shown in Table <ref type="table" target="#tab_2">4</ref> and Wide-ResNet-28-10 Model with the searched policies performs better than Baseline and Cutout and it is comparable with other methods. We emphasize that we use the same settings as CIFAR while AA tuned several hyperparameters on the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ImageNet</head><p>Following the experiment setting of AA, we use a reduced subset of the ImageNet train data which is composed of 6,000 samples from randomly selected 120 classes. ResNet-50 <ref type="bibr" target="#b11">[12]</ref> on each fold were trained for 90 epochs during policy search phase, and we trained ResNet-50 <ref type="bibr" target="#b11">[12]</ref> and ResNet-200 <ref type="bibr" target="#b12">[13]</ref> with the searched augmentation policy. In Table <ref type="table" target="#tab_3">5</ref>, we compare the validation accuracies of FAA with those of baseline and of AA via ResNet-50 and ResNet-200. In this test, we except the AmoebaNet <ref type="bibr" target="#b27">[28]</ref> since its exact implementation is not open to public. As one can see from the table, the proposed method outperforms benchmarks. Furthermore, our search method is 33 times faster than AA on the same experimental settings (see Table <ref type="table">1</ref>). Since extensive data augmentation protects the network from overfitting <ref type="bibr" target="#b13">[14]</ref>, we believe the performance will be improved by reducing the weight decay which is tuned for the model with default augmentation rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Effect of Number of Augmentation Policies Similar to AA, we hypothesize that as we increase the number of sub-policies searched by FAA, the given neural network should show improved generalization performance. We investigate this hypothesis by testing trained models Wide-ResNet-40-2 and Wide-ResNet-28-10 on CIFAR-10 and CIFAR-100. We select sub-policy sets from a pool of 400 searched sub-policies, and train models again with each of these sub-policy sets. Figure <ref type="figure" target="#fig_7">3</ref> shows the relation between average validation error and the number of sub-policies used in training. This result verifies that the performance improves with more sub-policies up to 100-125 sub-policies.</p><p>As one can observe in Table <ref type="table" target="#tab_0">2</ref>-3, there are small gaps between the performance of policies from direct search and the transferred policies from the reduced CIFAR-10 with Wide-ResNet-40-2. One can see that those gaps increase as the model capacities increase since the searched augmentation policies by the small model have a limitation to improve the generalization performance for the large model (e.g., Shake-Shake). Nevertheless, transferred policies are better than default augmentations; hence, one can apply those policies to different image recognition tasks.</p><p>Comparison between Random Search Strategies We performed additional experiments with two random search strategies (1) Randomly pre-selected augmentations (RPSA), which first selects a certain number (25/50) of augmentation policies randomly from the search space, and then trains Wide-ResNet-28-10 using the selected augmentations over 200 epochs; (2) Random augmentations (RA), that independently samples an augmentation policy for each train input from the whole search space during training with 400 epochs, which is two times more epochs than AA and FAA considering the compensation for the search time of the both algorithms. Recently, the proposed FAA contributed to win the first place in AutoCV competition of NeurIPS 2019 AutoDL challenge <ref type="bibr" target="#b0">[1]</ref>. Especially, since this competition required an AutoML approach under very limited computational resources and time, the (light version of) FAA <ref type="bibr" target="#b2">[3]</ref> was only able to apply for augmentation searching under this situation and eventually leaded to performance improvement. The details of this result will be published in the near future.</p><p>Search of Augmentation Policies per Class Taking advantage of the fact that the algorithm is efficient, we experimented with searching for augmentation policies per class in CIFAR-100 with Wide-ResNet-40-2 Model. We changed search depth B to 100, and kept other parameters the same. With the 70 best-performing policies per class, we obtained a slightly improved error rate. Although it is difficult to see a definite improvement compared to AA and FAA, we believe that further optimization in this direction may improve performances more. Mainly, it is expected that the effect should be greater in the case of a dataset in which the difference between classes such as the object scale is enormous.</p><p>One can try tuning the other meta-parameters of Bayesian optimization such as search depth or kernel type in the TPE algorithm in the augmentation search phase. However, this does not significantly help to improve model performance empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an automatic process of learning augmentation policies for a given task and a convolutional neural network. Our search method is significantly faster than AutoAugment, and its performances overwhelm the human-crafted augmentation methods.</p><p>One can apply Fast AutoAugment to the advanced architectures such as AmoebaNet and consider various augmentation operations in the proposed search algorithm without increasing search costs. Moreover, the joint optimization of NAS and Fast AutoAugment is a a curious area in AutoML. We leave them for future works. We are also going to deal with the application of Fast AutoAugment to various computer vision tasks beyond image classification in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>9 N d 7 E m / h n z L Z 7 0 N a B w D D z H m 8 y f i y 4 N o 7 z h Q o b x c 2 t 7 d J O e X d v / + C w U j 3 q 6 C h R l L V p J C L V 8 4 l m g k v W N t w I 1 o s V I 6 E v W N e f 3 m V + 9 4 k p z S P 5 a G Y x 8 0 I y l j z g l B g r d d 2 L e J i 6 8 2 G l 5 t S d B f A 6 c X N S g x y tY R U V B 6 O I J i G T h g q i d d 9 1 Y u O l R B l O B Z u X B 4 l m M a F T M m Z 9 S y U J m f b S R d 4 5 P r P K C A e R s k 8 a v F B / b 6 Q k 1 H o W + n Y y J G a i V 7 1 M / N f L F K U D b U 2 8 7 o 5 0 d m 4 l m w l u v J T L O D F M 0 m W 0 I B H Y R D i r C 4 + 4 Y t S I m S W E K m 5 / h + m E K E K N L b V s e 3 N X W 1 o n n U b d v a w 3 H q 5 q z d u 8 w R K c w C m c g w v X 0 I R 7 a E E b K E z h G V 7g F b 2 h d / S B P p e j B Z T v H M M f o O 8 f 3 x O i s w = = &lt; / l a t e x i t &gt; 1 p2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w p G n K D j z n D e y Y F G + 6 5 K h Q 9 f 9 A H M = " &gt;A A A C I X i c b V B N S w M x F H y p V W v 9 a v X o J V g E L 5 b d K u i x 6 M V j B f s B 7 V K y a b Y N z Wa X J C u U p T / C q x 7 9 N d 7 E m / h n z L Z 7 0 N a B w D D z H m 8 y f i y 4 N o 7 z h Q o b x c 2 t 7 d J O e X d v / + C w U j 3 q 6 C h R l L V p J C L V 8 4 l m g k v W N t w I 1 o s V I 6 E v W N e f 3 m V + 9 4 k p z S P 5 a G Y x 8 0 I y l j z g l B g r d d 2 L e J g 2 5 s N K z a k 7 C + B 1 4 u a k B j l a w y o q D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r Z 9 u a u t r R O O o 2 6 e 1 l v P F z V m r d 5 g y U 4 g V M 4 B x e u o Q n 3 0 I I 2 U J j C M 7 z A K 3 p D 7 + g D f S 5 H C y j f O Y Y / Q N 8 / 4 M u i t A = = &lt; / l a t e x i t &gt; p2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " / U C e v E v w t P C R 4 k 9 Y p g k 0 3 I / h M q I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>6 j I R l o y F F I p J N O D 8 n g s W Z 8 G E R C P 6 7 g l P 3 t S F E o 5 S T 0 d G e + t l z U c v J f L W e E D K Q W 4 b L q y 3 z c w m 4 q u H B S y u N E E Y 5 n q w U J g y q C e Y 7 Q p 4 J g x S Y a I C y o v g 7 i E R I I K 5 1 2 W e d m L a a 0 D L r N h n X a a N 6 d 1 V p X 8 w R L 4 A A c g T q w w D l o g R v Q B h 2 A w R N 4 B i / g 1 X g z P o x P 4 2 v W W j D m n n 3 w p 4 z v H 7 y e r M I = &lt; / l a t e x i t &gt; O2(x; 2) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j r Q c G 4 Z n s a / 1 t b k 2 P l V y N x h L x G o = " &gt; A A A C O n i c b V D L S s N A F J 3 U q r W + W l 0 K M l i E u i l J F R T c F N 2 4 s 4 J 9 Q B P C Z D J p h 0 4 m Y W Y i l p C d X + N W l / 6 I W 3 f i 1 g 9 w 0 n a h r R c G z p x z D / f e 4 8 W M S m W a 7 0 Z h p b i 6 t l 7 a K G 9 u b e / s V q p 7 X R k l A p M O j l g k + h 6 S h F F O O o o q R v q x I C j 0 G O l 5 4 + t c 7 z 0 Q I W n E 7 9 U k J k 6 I h p w G F C O l K b d y a I d I j T B i 6 W 3 m p s 2 s / n h p M 2 3 3 U f 4 7 c S s 1 s 2 F O C y 4 D a w 5 q Y F 5 t t 2 o U b T / C S U i 4 w g x J O b D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>G b y A V + P N + D A + j a 9 Z a c G Y 9 + y D P z C + f w A s o b B 6 &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of augmented images via a sub-policy in the search space S. Each sub-policy τ consists of 2 operations; for instance, τ =[cutout, autocontrast] is used in this figure. Each operation Ō(τ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 : 3 T 4 Train θ on D M 5 for 6 B← 7 T 8 T</head><label>1345678</label><figDesc>Fast AutoAugmentInput :(θ, D train , K, T, B, N )1 Split D train into K-fold data D shuffling 2 for k ∈ {1, . . . , K} do (k) * ← ∅, (D M , D A ) ← (D t ∈ {0, . . . , T -1} do BayesOptim(T , L(θ|T (D A )), B)// explore-and-exploit t ← Select top-N policies in B augmentation policies9 return T * = k T (k) *Train (Line 4): Train models on each training split. We implement this to run parallelly across multiple machines to reduce total running time if the computational resource is enough.Explore-and-Exploit (Line 6): We use HyperOpt library from Ray with B search numbers and 20 maximum concurrent evaluations. Different from AutoAugment, we do not discretize search spaces since our search algorithm can handle continuous values. We explore one of the possible operations with probability p and magnitude λ. The values of probability and magnitude are uniformly sampled from [0, 1] at the beginning, then HyperOpt modulates the values to optimize the objective L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Validation error (%) of Wide-ResNet-40-2 and Wide-ResNet-28-10 trained on CIFAR-10 and CIFAR-100 as number of sub-policies used in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Test set error rate (%) on CIFAR-10.</figDesc><table><row><cell>PBA [15]</cell><cell>FAA (transfer / direct)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Test set error rate (%) on CIFAR-100.</figDesc><table><row><cell>Model</cell><cell cols="5">Baseline Cutout [7] AA [5] PBA [15] FAA</cell></row><row><cell>Wide-ResNet-28-10</cell><cell>1.5</cell><cell>1.3</cell><cell>1.1</cell><cell>1.2</cell><cell>1.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Test set error rate (%) on SVHN.</figDesc><table><row><cell>Model</cell><cell>Baseline</cell><cell>AA [5]</cell><cell>FAA</cell></row><row><cell cols="4">ResNet-50 23.7 / 6.9 22.4 / 6.2 22.4 / 6.3</cell></row><row><cell cols="4">ResNet-200 21.5 / 5.8 20.00 / 5.0 19.4 / 4.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Validation set Top-1 / Top-5 error rate (%) on ImageNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Comparison of test error (%) of Wide-ResNet-28-10 trained on CIFAR-100 between random search strategies, AA, and FAA.Both the RPSA and RA are performed on CIFAR-100 and repeated 20 times. As shown in the Figure4, the performances of the RPSA is better than baseline but not improved as the number of selected policies increases. And the best performance obtained by RPSA is still worse than FAA. In addition, the RA achieves a little bit worse result than those obtained by RPSA, and the improvement by RA is also less than that by FAA. It is noted that even though we take into account the search time of the proposed method on CIFAR-10/100 (see Table1), the training time for FAA with 200 epochs including the search time is shorter than the training time for the RA with 400 epochs.</figDesc><table><row><cell>17.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>17.60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>17.50</cell><cell></cell><cell></cell></row><row><cell>17.4</cell><cell>17.42</cell><cell></cell><cell></cell><cell></cell></row><row><cell>17.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>17.15</cell><cell></cell></row><row><cell></cell><cell></cell><cell>17.1</cell><cell></cell><cell></cell></row><row><cell>17.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RA</cell><cell>RPSA-25</cell><cell>RPSA-50</cell><cell>AA</cell><cell>FAA</cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We appreciate every reviewer for valuable comments. We are also grateful to Brain Cloud team at Kakao Brain for GPU support.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A A A C I H i c b V D L S g M x F L 2 x P m p 9 t b p 0 E y x C 3 Z Q Z E X R Z d O O y g n 1 A O 5 R M J t O G Z j J D k h F L 6 U e 4 1 Z 1 f 4 0 5 c 6 t e Y a W e h b Q 8 E D u f c y z 0 5 f i K 4 N o 7 z j T Y K m 1 v b O 8 X d 0 t 7</p><p>+ w e F R u X L c 1 n G q K G v R W M S q 6 x P N B J e s Z b g R r J s o R i J f s I 4 / v s v 8 z h N T m s f y 0 U w S 5 k V k K H n I K T F W 6 v Q N S W v P F 4 N y 1 a k 7 c + B V 4 u a k C j m a g w o q 9 I O Y p h G T h g q i d c 9 1 E u N N i T K c C j Y r 9 V P N E k L H Z M h 6 l k o S M e 1 N 5 3 l n + N w q A Q 5 j Z Z 8 0 e K 7 + 3 Z i S S O t J 5 N v J i J i R X v Y y c a 2 A A A C I H i c b V D L S g M x F L 2 x P m p 9 t b p 0 E y</p><p>R S e I N 3 + E C f 6 A s t 0 P d q t I S K n T P 4 B / T z C 7 q f p u U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><p>9 b i 6 b D O y 9 j I k k 1 F W S V L E y 5 p W M r 7 8 0 K m K R E 8 5 k h m E h m P m e R C Z a Y a N N u 1 d T m r J e 0 S X r X L c d u O U 8 3 z f Z 9 U W A F z u E C r s C B W 2 j D I 3 S g C w R S e I N 3 + E C f 6 A s t 0 P d q t I S K n T P 4 B / T z C 7 q f p u U = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p><p>r s a V l 0 m n U r b N 6 4 + 6 8 2 r y a N 1 h E h + g Y 1 Z C F L l A T 3 a A W a i O C n t A z e k G v x p v x Y X w a X 7 P R F W O + c 4 D + w P j 1 v Q K t C 2 l H f 5 o m s B f 1 9 g H u G K j B k s = " &gt; A A A C O n i c b V D L S s N A F J 1 o 1 V p f r S 4 F G S x C 3 Z S k C r o s u n F n B f u A p o a b y a Q d O n k w M x F K y M 6 v c a t L f 8 S t O 3 H r B z h p u 9 D W A w O H c + 5 l z j 1 u z J l U p v l u r K w W 1 t Y 3 i p u l r e 2 d 3 b 1 y Z b 8 j o 0 Q Q 2 i Y R j 0 T P B U k 5 C 2 l b M c V p L x Y U A p f T r j u + z v 3 u I x W S R e G 9 m s R 0 E M A w Z D 4 j o L T k l I 9 s F 0 R q B 6 B G B H h 6 m 2 V O 2 s g e 0 p q t</p><p>r 8 3 U g i k n A S u n s w T y 0 U v F / / 1 c k V I X 2 o T L 7 u e z L 9 b y K b 8 y 0 H K w j h R N C S z a H 7 C s Y p w 3 i P 2 m K B E 8 Y k m Q A T T 1 2 E y A g F E 6 b Z L u j d r s a V l 0 m n U r b N 6 4 + 6 8 2 r y a N 1 h E h + g Y 1 Z C F L l A T 3 a A W a i O C n t A z e k G v x p v x Y X w a X 7 P R F W O + c 4 D + w P j I d 3 I S j f Z X g v e Y j A = " &gt; A A A C O n i c b V D L S s N A F J 1 o 1 V p f r S 4 F G S x C 3 Z S k C r o s u n F n B f u A J p a b 6 a Q d O n k w M x F K y M 6 v c a t L f 8 S t O 3 H r B z h p s 9 D W A w O H c + 5 l z j 1 u x J l U p v l u r K w W 1 t Y 3 i p u l r e 2 d 3 b 1 y Z b 8 j w 1 g Q 2 i Y h D 0 X P B U k 5 C 2 h b M c V p L x I U f J f T r j u 5 z v z u I x W S h c G 9 m k b U 8 W E U M I 8 R U F o a l I 9 s F 0 R i + 6 D G B H h y m 6 a D x E o f k p q t I D 5 N B + W q W T d n w M v E y k k V 5 W g N K k b B H o Y k 9 m m g C A c p + 5 Y Z K S c B o R j h N C 3 Z s a Q R k A m M a F / T A H w q n W R 2 S I p P t D L E X i j 0 C x S e q b 8 3 E v C l n P q u n s w S y 0 U v E / / 1 M k V I T 2 o T L 7 t D m X 2 3 k E 1 5 l 0 7 C g i h W N C D z a F 7 M s Q p x 1 i M e M k G J 4 l N N g A i m r 8 N k D A K I 0 m 2 X d G / W Y k v L p N O o W 2 f 1 x t 1 5 t X m V N 1 h E h + g Y 1 Z C F L l A T 3 a A W a i O C n t A z e k G v x p v x Y X w a X / P R F</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>r e D P e j U / j y / i e j 5 a M Y u c Q / I P x 8 w v P 5 K p 2 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l w</p><p>r e D P e j U / j y / i e j 5 a M Y u c Q / I P x 8 w v P 5 K p 2 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l w</p><p>r e D P e j U / j y / i e j 5 a M Y u c Q / I P x 8 w v P 5 K p 2 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l w </p><p>a l t m 0 7 s / r r a u i w D I 4 A s e g A S x w A V r g F r R B B 2 D w D F 7 A K 3 g z 3 o 1 P 4 8 v 4 n o 2 u G M X O A f g H 4 + c X / J S q k A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i w M L / a g x J L R 5 5</p><p>a l t m 0 7 s / r r a u i w D I 4 A s e g A S x w A V r g F r R B B 2 D w D F 7 A K 3 g z 3 o 1 P 4 8 v 4 n o 2 u G M X O A f g H 4 + c X / J S q k A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i w M L / a g x J L R 5 5</p><p>a l t m 0 7 s / r r a u i w D I 4 A s e g A S x w A V r g F r R B B 2 D w D F 7 A K 3 g z 3 o 1 P 4 8 v 4 n o 2 u G M X O A f g H 4 + c X / J S q k A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i w M L / a g x J L R 5 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M(✓)</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 3       </p><p>V n w z i K R + A s y F + n c j w 6 F S a e j p y T y r W v V y c a O X K 1 I F a q P p q / z a S j Q I b t y M i T g B K s g y W Z B w E y I z 7 8 7 0 m a Q E e K o J J p L p z 5 l k i i U m o B u u 6 t r s 1 Z L W S f + y b V t t + / G q 2 b k t C q y g U 3 S G W s h G 1 6 i D 7 l E X 9 R B B K X p B r + j N e D c + j C / j e z l a M o q d B v o H 4 + c X N O G o G g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 3</p><p>V n w z i K R + A s y F + n c j w 6 F S a e j p y T y r W v V y c a O X K 1 I F a q P p q / z a S j Q I b t y M i T g B K s g y W Z B w E y I z 7 8 7 0 m a Q E e K o J J p L p z 5 l k i i U m o B u u 6 t r s 1 Z L W S f + y b V t t + / G q 2 b k t C q y g U 3 S G W s h G 1 6 i D 7 l E X 9 R B B K X p B r + j N e D c + j C / j e z l a M o q d B v o H 4 + c X N O G o G g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 3</p><p>V n w z i K R + A s y F + n c j w 6 F S a e j p y T y r W v V y c a O X K 1 I F a q P p q / z a S j Q I b t y M i T g B K s g y W Z B w E y I z 7 8 7 0 m a Q E e K o J J p L p z 5 l k i i U m o B u u 6 t r s 1 Z L W S f + y b V t t + / G q 2 b k t C q y g U 3 S G W s h G 1 6 i D 7 l E X 9 R B B K X p B r + j N e D c + j C / j e z l a M o q d B v o H 4 + c X N O G o G g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i 3 Our search space is similar to previous methods except that we use both continuous values of probability p and magnitude λ at [0, 1] which has more possibilities than discretized search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Strategy</head><p>In Fast AutoAugment, we consider searching the augmentation policy as a density matching between a pair of train datasets. Let D be a probability distribution on X × Y and assume dataset D is sampled from this distribution. For a given classification model M(•|θ) : X → Y that is parameterized by θ, the expected accuracy and the expected loss of model M(•|θ) on dataset D are denoted by R(θ|D) and L(θ|D), respectively. For a given augmentation policy T , L(θ|T (D)) denotes the expected loss of model for augmented images of data by <ref type="bibr" target="#b1">(2)</ref>. Note that the value of the loss for fixed policy T can vary according to the randomness in sub-policies due to (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Efficient Density Matching for Augmentation Policy Search</head><p>For any given pair of D train and D valid , our goal is to improve the generalization ability by searching the augmentation policies that match the density of D train with density of augmented D valid . However, it is impractical to compare these two distributions directly for an evaluation of every candidate policy. Therefore, we perform this evaluation by measuring how much one dataset follows the pattern of the other by making use of the model predictions on both datasets. In detail, let us split D train = D M ∪ D A into D M and D A that are used for learning the model parameter θ and exploring the augmentation policy T , respectively. We employ the following objective to find a set of learned augmentation policies T</p><p>where model parameter θ * is trained on D M . It is noted that in this objective, T * approximately minimizes the distance between density of D M and density of T (D A ) from the perspective of maximizing the performance of both model predictions with the same parameter θ. The proposed search objective pursues to find label-preserving transformations that generates unseen but plausible missing data samples. Namely, it does not transform but augment the data space which has to be correctly predicted by a classification network for better generalization. This perspective is also inline with the motivation of Bayesian DA <ref type="bibr" target="#b35">[36]</ref>. In practice, we minimize the categorical cross-entropy loss L(θ|T (D A )) instead of maximizing accuracy in (3).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://autodl.chalearn.org/" />
		<title level="m">NeurIPS 2019 AutoDL challenges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Brain</surname></persName>
		</author>
		<author>
			<persName><surname>Autoclint</surname></persName>
		</author>
		<ptr target="https://github.com/kakaobrain/autoclint" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hernández-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>König</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03852</idno>
		<title level="m">Data augmentation instead of explicit regularization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09846</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A taxonomy of global optimization methods based on response surfaces</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of global optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="383" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scalable neural architecture search for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smart augmentation learning an optimal data augmentation strategy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5858" to="5869" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ray: A distributed framework for emerging {AI} applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data augmentation with manifold exploring geometric transformations for increased performance and robustness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Göbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04420</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<title level="m">Regularized evolution for image classifier architecture search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Apac: Augmented pattern classification with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yokoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03229</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effect of separate sampling on classification accuracy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shahrokh Esfahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="250" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Variable kernel density estimation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Terrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1236" to="1265" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A bayesian data augmentation approach for learning deep models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2797" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02375</idno>
		<title level="m">Shakedrop regularization for deep residual learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04899</idno>
		<title level="m">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference 2016. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
