<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heavy-Hi er Detection Entirely in the Data Plane</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vibhaalakshmi</forename><surname>Sivaraman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Srinivas</forename><surname>Narayana</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mit</forename><surname>Csail</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ori</forename><surname>Rottenstreich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">SOSR &apos;17</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heavy-Hi er Detection Entirely in the Data Plane</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9ABF99842B41A9D85AA0BAD9BE1A80C6</idno>
					<idno type="DOI">10.1145/3050220.3063772</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Software-De ned Networks</term>
					<term>Network Monitoring</term>
					<term>Programmable Networks</term>
					<term>Network Algorithms</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying the "heavy hitter" ows or ows with large tra c volumes in the data plane is important for several applications e.g., ow-size aware routing, DoS detection, and tra c engineering. However, measurement in the data plane is constrained by the need for linerate processing (at 10-100Gb/s) and limited memory in switching hardware. We propose HashPipe, a heavy hitter detection algorithm using emerging programmable data planes. HashPipe implements a pipeline of hash tables which retain counters for heavy ows while evicting lighter ows over time. We prototype HashPipe in P4 and evaluate it with packet traces from an ISP backbone link and a data center. On the ISP trace (which contains over 400,000 ows), we nd that HashPipe identi es 95% of the 300 heaviest ows with less than 80KB of memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>variations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>) can enable dynamic routing of heavy ows <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> and dynamic ow scheduling <ref type="bibr" target="#b40">[41]</ref>.</p><p>It is desirable to run heavy-hitter monitoring at all switches in the network all the time, to respond quickly to short-term tra c variations. Can packets belonging to heavy ows be identi ed as the packets are processed in the switch, so that switches may treat them specially?</p><p>Existing approaches to monitoring heavy items make it hard to achieve reasonable accuracy at acceptable overheads ( §2.2). While packet sampling in the form of NetFlow <ref type="bibr" target="#b11">[12]</ref> is widely deployed, the CPU and bandwidth overheads of processing sampled packets in software make it infeasible to sample at su ciently high rates (sampling just 1 in 1000 packets is common in practice <ref type="bibr" target="#b33">[34]</ref>). An alternative is to use sketches, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref> that hash and count all packets in switch hardware. However, these systems incur a large memory overhead to retrieve the heavy hitters -ideally, we wish to use memory proportional to the number of the heavy ows (say the top hundred). There may be tens of thousands of active ows any minute on an ISP backbone link ( §5) or a data center top-of-rack switch <ref type="bibr" target="#b3">[4]</ref>.</p><p>Emerging programmable switches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> allow us to do more than sample, hash, and count, which suggests opportunities to run novel algorithms on switches. While running at line rates of 10-100 Gbps per port over 10-100 ports, these switches can be programmed to maintain state over multiple packets, e.g., to keep ow identi ers as keys along with counts. Stateful manipulations can also be pipelined over multiple stages, with the results carried by the packet from one stage to the next and compared to stored state in subsequent stages.</p><p>However, switches are also constrained by the need to maintain high packet-processing throughput, having:</p><p>• a deterministic, small time budget (1 ns <ref type="bibr" target="#b6">[7]</ref>) to manipulate state and process packets at each stage; • a limited number of accesses to memory storing state at each stage (typically just one readmodify-write); • a limited amount of memory available per stage (e.g., 1.4MB shared across forwarding and monitoring <ref type="bibr" target="#b6">[7]</ref>);</p><p>• a need to move most packets just once through the pipeline, to avoid stalls and reduced throughput ("feed-forward" <ref type="bibr" target="#b18">[19]</ref>).</p><p>We present HashPipe, an algorithm to track the k heaviest ows with high accuracy ( §3) within the features and constraints of programmable switches. Hash-Pipe maintains both the ow identi ers ("keys") and counts of heavy ows in the switch, in a pipeline of hash tables. When a packet hashes to a location in the rst stage of the pipeline, its counter is updated (or initialized) if there is a hit (or an empty slot). If there is a miss, the new key is inserted at the expense of the existing key. In all downstream stages, the key and count of the item just evicted are carried along with the packet. The carried key is looked up in the current stage's hash table. Between the key looked up in the hash table and the one carried, the key with the larger count is retained in the hash table, while the other is either carried along with the packet to the next stage, or totally removed from the switch if the packet is at the last stage. Hence, HashPipe "smokes out" the heavy keys within the limited available memory, using pipelined operation to sample multiple locations in the hash tables, and evicting lighter keys in favor of heavier keys, with updates to exactly one location per stage.</p><p>We prototype HashPipe in P4 <ref type="bibr" target="#b5">[6]</ref> ( §4) and test it on the public-domain behavioral switch model <ref type="bibr" target="#b31">[32]</ref>. We evaluate HashPipe with packet traces obtained from an ISP backbone link (from CAIDA) and a data center, together containing over 500 million packets. We show that HashPipe can provide high accuracy ( §5). In the backbone link trace, HashPipe incurs less than 5% false negatives and 0.001% false positives when reporting 300 heavy hitters (keyed by transport ve-tuple) with just 4500 counters (less than 80KB) overall, while the trace itself contains 400,000 ows. The errors both in false negatives and ow count estimations are lower among the heavier ows in the top k. At 80KB of memory, HashPipe has 15% lower false negatives with respect to sample and hold <ref type="bibr" target="#b16">[17]</ref>, and 3-4% with respect to an enhanced version of the count-min sketch <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND ON HEAVY-HITTER DETECTION 2.1 Problem Formulation</head><p>Heavy hitters. "Heavy hitters" can refer to all ows that are larger (in number of packets or bytes) than a fraction t of the total packets seen on the link (the "threshold-t" problem). Alternatively, the heavy hitters can be the top k ows by size (the "top-k" problem). Through the rest of this paper, we use the "top-k" denition.</p><p>Flow granularity. Flows can be de ned at various levels of granularity, such as IP address (i.e., host), transport port number (i.e., application), or ve-tuple (i.e., transport connection). With a ner-grained notion of ows, the size and number of keys grows, requiring more bits to represent the key and more entries in the data structure to track the heavy ows accurately. Accuracy. A technique for detecting heavy hitters may have false positives (i.e., reporting a non-heavy ow as heavy), false negatives (i.e., not reporting a heavy ow), or an error in estimating the sizes of heavy ows. The impact of errors depends on the application. For example, if the switch performs load balancing in the data plane, a few false negatives may be acceptable, especially if those heavy ows can be detected in the next time interval. As another example, if the network treats heavy ows as suspected denial-of-service attacks, false positives could lead to unnecessary alarms that overwhelm network operators. When comparing approaches in our evaluations, we consider all three metrics. Overhead. The overhead on the switch includes the total amount of memory for the data structure, the number of matching stages used in the switch pipeline (constrained by a switch-speci c maximum, say 16 <ref type="bibr" target="#b6">[7]</ref>). The algorithms are constrained by the nature and amount of memory access and computation per match stage (e.g., computing hash functions). On high-speed links, the number of active ve-tuple ows per minute can easily be in the tens of thousands, if not more. Our central goal is to maintain data-plane state that is proportional to the target number k of heavy hitters (e.g., 5k or 10k), rather than the number of active ows. In addition, we would like to use as few pipeline stages as possible, since the switch also needs to support other functionality related to packet forwarding and access control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Existing Solutions</head><p>The problem of nding heavy ows in a network is an instance of the "frequent items" problem, which is extremely well studied in the streaming algorithms literature <ref type="bibr" target="#b12">[13]</ref>. While high accuracy and low overhead are essential to any heavy hitter detection approach, we are also speci cally interested in implementing these algorithms within the constraints of emerging programmable switches ( §1). We classify the approaches into two broad categories, sampling and streaming, discussed below. Packet sampling using NetFlow <ref type="bibr" target="#b11">[12]</ref> and sFlow <ref type="bibr" target="#b37">[38]</ref> is commonly implemented in routers today. These technologies record a subset of network packets by sampling, and send the sampled records to collectors for analysis. To keep packet processing overheads and data collection bandwidth low, NetFlow con gurations in practice use aggressively low sampling probabilities, e.g., 1% or even 0.01% <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>. Such undersampling can impact the estimation accuracy.</p><p>Sample and hold <ref type="bibr" target="#b16">[17]</ref> enhances the accuracy of packet sampling by keeping counters for all packets of a ow in a ow table, once a packet from that ow is sampled. Designing large ow tables <ref type="foot" target="#foot_0">1</ref> for fast packet lookup is well-understood: hash table implementations are already common in switches, for example in router forwarding tables and NetFlow <ref type="bibr" target="#b23">[24]</ref>. However, it is challenging to handle hash collisions when adding ows to the ow table, when a packet from a new ow is sampled. Some custom hardware solutions combine the hash table with a "stash" that contains the over ow, i.e., colliding entries, from the hash table <ref type="bibr" target="#b21">[22]</ref>. But this introduces complexity in the switch pipeline, and typically involves the control plane to add entries into the stash memory. Ignoring such complexities, we evaluate sample and hold in §5 by liberally allowing the ow table to lookup packets anywhere in a list of a bounded size. Streaming algorithms implement data structures with bounded memory size and processing time per packet, while processing every packet in a large stream of packets in one pass. The algorithms are designed with provable accuracy-memory tradeo s for speci c statistics of interest over the packets. While these features make the algorithms attractive for network monitoring, the speci c algorithmic operations on each packet determine feasibility on switch hardware.</p><p>Sketching algorithms like count-min sketch <ref type="bibr" target="#b13">[14]</ref> and count sketch <ref type="bibr" target="#b9">[10]</ref> use per-packet operations such as hashing on packet headers, incrementing counters at hashed locations, and determining the minimum or median among a small number of the counters that were hashed to. These operations can all be e ciently implemented on switch hardware <ref type="bibr" target="#b44">[45]</ref>. However, these algorithms do not track the ow identi ers of packets, and hash collisions make it challenging to "invert" the sketch into the constituent ows and counters. Simple workarounds like collecting packets that have high ow count estimates in the sketch could result in signi cant bandwidth overheads, since most packets from heavy ows will have high estimates.</p><p>Techniques like group testing <ref type="bibr" target="#b14">[15]</ref>, reversible sketches <ref type="bibr" target="#b36">[37]</ref>, and FlowRadar <ref type="bibr" target="#b23">[24]</ref> can decode keys from hashbased sketches. However, it is challenging to read o an accurate counter value for a packet in the switch pipeline itself since the decoding happens o the fast packet-processing path.</p><p>Counter-based algorithms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref> focus on measuring the heavy items, maintaining a table of ows and corresponding counts. They employ per-counter increment and subtraction operations, but potentially all counters in the table are updated during some ow insertions. Updating multiple counters in a single stage is challenging within the deterministic time budget for each packet.</p><p>Space saving is a counter-based algorithm that uses only O (k ) counters to track k heavy ows, achieving the lowest memory usage possible for a xed accuracy among deterministic heavy-hitter algorithms-both theoretically <ref type="bibr" target="#b27">[28]</ref> and empirically <ref type="bibr" target="#b12">[13]</ref>. Space saving only updates one counter per packet, but requires nding the item with the minimum counter value in the table. Unfortunately, scanning the entire table on each packet, or nding the minimum in a table e ciently, is not directly supported on emerging programmable hardware ( §1). Further, maintaining data structures like sorted linked lists <ref type="bibr" target="#b27">[28]</ref> or priority queues <ref type="bibr" target="#b40">[41]</ref> requires multiple memory accesses within the per-packet time budget. However, as we show in §3, we are able to adapt the key ideas of the space saving algorithm and combine it with the functionality of emerging switch hardware to develop an e ective heavy hitter algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HASHPIPE ALGORITHM</head><p>As described in §2.2, HashPipe is heavily inspired by the space saving algorithm <ref type="bibr" target="#b27">[28]</ref>, which we describe now ( §3.1). In the later subsections ( §3.2- §3.4), we describe our modi cations to the algorithm to make it amenable to switch implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Space Saving Algorithm</head><p>To track the k heaviest items, space saving uses a table with m (which is O (k )) slots, each of which identi es a distinct ow key and its counter. All slots are initially empty, i.e., keys and counters are set to zero.</p><p>Algorithm 1: Space Saving algorithm <ref type="bibr" target="#b27">[28]</ref> 1 Table T has m slots, either containing (ke j , al j ) at slot j ∈ {1, . . . , m}, or empty. Incoming packet has key iKe . The algorithm is summarized in Algorithm 1. <ref type="foot" target="#foot_1">2</ref> When a packet arrives, if its corresponding ow isn't already in the table, and there is space left in the table, space saving inserts the new ow with a count of 1. If the ow is present in the table, the algorithm updates the corresponding ow counter. However, if the table is full, and the ow isn't found in the table, the algorithm replaces the ow entry that has the minimum counter value in the table with the incoming packet's ow, and increments this minimum-valued counter.</p><p>This algorithm has three useful accuracy properties [28] that we list here. Suppose the true count of ow ke j is c j , and its count in the table is al j . First, no ow counter in the table is ever underestimated, i.e., al j ≥ c j . Second, the minimum value in the table al r is an upper bound on the overestimation error of any counter, i.e., al j ≤ c j + al r . Finally, any ow with true count higher than the average table count, i.e., c j &gt; C/m, will always be present in the table (here C is the total packet count added into the table). This last error guarantee is particularly useful for the threshold-heavy-hitter problem ( §2.1), since by using 1/t counters, we can extract all ows whose true count exceeds a threshold t of the total count. However, this guarantee cannot be extended directly to the top-k problem, since due to the heavy-tailed nature of tra c <ref type="bibr" target="#b16">[17]</ref>, the kth heaviest ow contributes nowhere close to 1/k of the total tra c.</p><p>The operation of nding the minimum counter in the table (line 8)-possibly for each packet-is di cult within switch hardware constraints ( §2.2). In the following subsections, we discuss how we incrementally modify the space saving algorithm to run on switches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling for the Minimum Value</head><p>The rst simpli cation we make is to look at the minimum of a small, constant number d of randomly chosen counters, instead of the entire table (in line 8, Algorithm 1). This restricts the worst-case number of memory accesses per packet to a small xed constant d. The modi ed version, HashParallel, is shown in Algorithm 2. The main change from Algorithm 1 is in the set of table slots examined (while looking up or updating any key), which is now a set of d slots obtained by hashing the incoming key using d independent hash functions.</p><p>In e ect, we sample the table to estimate a minimum using a few locations. However, the minimum of just d slots can be far from the minimum of the entire table of m slots. An in ated value of the minimum could impact the useful error guarantees of space saving ( §3.1). Our evaluations in §5. <ref type="bibr" target="#b3">4</ref> show that the distributions of the minimum of the entire table and of the subsample are comparable.</p><p>However, Algorithm 2 still requires the switch to read d locations at once to determine their minimum, and then write back the updated value. This necessitates a read-modify-write operation, involving d reads and one write anywhere in table memory, within the per-packet time budget. However, multiple reads to the same table (d &gt; 1) are supported neither in switch programming languages today <ref type="bibr" target="#b5">[6]</ref> nor in emerging programmable switching chips <ref type="bibr" target="#b6">[7]</ref>. Further, supporting multiple reads for every packet at line rate requires multiported memories with strict access-latency guarantees, which can be quite expensive in terms of area and power <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Stages of Hash Tables</head><p>The next step is to reduce the number of reads to memory to facilitate operation at line rate. We split the counter table T into d disjoint tables, and we read exactly one slot per table. The algorithm is exactly the same as Algorithm 2; except that now hash function h i returns only slots in the ith stage.</p><p>This design enables pipelining the memory accesses to the d tables, since di erent packets can access di erent tables at the same time. However, packets may need to make two passes through this pipeline: once to determine the counter with the minimum value among d slots, and a second time to update that counter. The second pass is possible through "recirculation" of packets through the switching pipeline <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> with additional metadata on the packet, allowing the switch to increment the minimum-valued counter in the second pass. However, the second pass is potentially needed for every packet, and recirculating every packet can halve the pipeline bandwidth available to process packets.</p><p>Algorithm 2: HashParallel: Sample d slots at once </p><formula xml:id="formula_0">1 Hash functions h i (iKe ) → slots, i ∈ {1, . . . , d } 2 H = {h 1 (iKe ), . . . , h d (iKe )} 3 if ∃ slot j ∈ H with iKe = ke j then 4 al j ← al j + 1 5 else 6 if ∃ empty slot j ∈ H then 7 (ke j , al j ) ← (iKe , 1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feed-Forward Packet Processing</head><p>We now alleviate the need to process a packet more than once through the switch pipeline, using two key ideas. Track a rolling minimum. We track the minimum counter value seen so far (and its key) as the packet traverses the pipeline, by moving the counter and key through the pipeline as packet metadata. Emerging programmable switches allow the use of such metadata to communicate results of packet processing between different stages, and such metadata can be written to at any stage, and used for packet matching at a later stage <ref type="bibr" target="#b41">[42]</ref>.  As the packet moves through the pipeline, the switch hashes into each stage on the carried key, instead of hashing on the key corresponding to the incoming packet. If the keys match in the table, or the slot is empty, the counter is updated in the usual way, and the key needs no longer to be carried forward with the packet. Otherwise, the keys and counts corresponding to the larger of the counters that is carried and the one in the slot is written back into the table, and the smaller of the two is carried on the packet. We leverage arithmetic and logical action operations available in the matchaction tables in emerging switches <ref type="bibr" target="#b6">[7]</ref> to implement the counter comparison. The key may be carried to the next stage, or evicted completely from the tables when the packet reaches the last (i.e., dth) stage. Always insert in the rst stage. If the incoming key isn't found in the rst stage in the pipeline, there is no associated counter value to compare with the key that is in that table. Here, we choose to always insert the new ow in the rst stage, and evict the existing key and counter into packet metadata. After this stage, the packet can track the rolling minimum of the subsequent stages in the usual way described above. The nal algorithm, HashPipe, is shown in Algorithm 3.</p><formula xml:id="formula_1">2 l 1 ← h 1 (iKe ) 3 if ke l 1 = iKe then 4 al l 1 ← al l 1 + 1 5 end processing 6 end 7 else if l 1 is an empty slot then 8 (ke l 1 , al l 1 ) ← (iKe , 1)</formula><p>One consequence of always inserting an incoming key in the rst stage is the possibility of duplicate keys across di erent tables in the pipeline, since the key can exist at a later stage in the pipeline. Note that this is unavoidable when packets only move once through the pipeline. It is possible that such duplicates may occupy space in the table, leaving fewer slots for heavy ows, and causing evictions of heavy ows whose counts may be split across the duplicate entries.</p><p>However, many of these duplicates are easily merged through the algorithm itself, i.e., the minimum tracking merges counters when the carried key has a "hit" in the table. Further, switches can easily estimate the ow count corresponding to any packet in the data plane itself by summing all the matching ow counters; so can a data collector, after reading the tables out of the switch. We also show in our evaluations ( §5.1) that duplicates only occupy a small portion of the table memory.</p><p>Fig. <ref type="figure" target="#fig_4">1</ref> illustrates an example of processing a packet using HashPipe. A packet with a key K enters the switch pipeline (a), and since it isn't found in the rst table, it is inserted there (b). Key B (that was in the slot currently occupied by K) is carried with the packet to the next stage, where it hashes to the slot containing key E. But since the count of B is larger than that of E, B is written to the table and E is carried out on the packet instead (c). Finally, since the count of L (that E hashes to) is larger than that of E, L stays in the table (d). The net e ect is that the new key K is inserted in the table, and the minimum of the three keys B, E, and L-namely E-is evicted in its favor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HASHPIPE PROTOTYPE IN P4</head><p>We built a prototype of HashPipe using P4 version 1.1 <ref type="bibr" target="#b41">[42]</ref>. We veri ed that our prototype produced the same results as our HashPipe simulator by running a small number of arti cially generated packets on the switch behavioral model <ref type="bibr" target="#b31">[32]</ref> as well as our simulator, and ensuring that the hash table is identical in both cases at the end of the measurement interval.</p><p>At a high level, HashPipe uses a match-action stage in the switch pipeline for each hash table. In our algorithm, each match-action stage has a single default action-the algorithm execution-that applies to every packet. Every stage uses its own P4 register arrays-stateful memory that persists across successive packets-for the hash table. The register arrays maintain the ow identi ers and associated counts. The P4 action blocks for the rst two stages are presented in Listings 1 and 2; actions for further stages are identical to that of stage 2. The remainder of this section walks through the P4 language constructs with speci c references to their usage in our HashPipe prototype.</p><p>Hashing to sample locations: The rst step of the action is to hash on the ow identi er (source IP address in the listing) with a custom hash function, as indicated in line 4 of Listing 1. The result is used to pick the location where we check for the key. The P4 behavioral model <ref type="bibr" target="#b31">[32]</ref> allows customized hash function de nitions. We use hash functions of the type h i = (a i • x +b i )%p where the chosen a i , b i are co-prime to ensure independence of the hash functions across stages. Hash functions of this sort are implementable on hardware and have been used in prior work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Registers to read and write ow statistics: The ows are tracked and updated using three registers: one to track the ow identi ers, one for the packet count, and one to test validity of each table index. The result of the hash function is used to index into the registers for reads and writes. Register reads occur in lines 6-9 and register writes occur in lines 15-18 of Listing 1. When a ow identi er is read from the register, it is checked against the ow identi er currently carried. Depending on whether there is a match, either the value 1 is written back or the current value is incremented by 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Packet metadata for tracking current minimum:</head><p>The values read from the registers are placed in packet metadata since we cannot test conditions directly on the register values in P4. This enables us to compute the minimum of the carried key and the key in the table before writing back into the register (lines 11-13 of Listing 1 and lines 3, 6, and 9 of Listing 2). Packet metadata also plays a crucial role in conveying state (the current minimum ow id and count, in this case) from one stage to another. The metadata is later used to compute the sample minimum. The updates that set these metadata across stages are similar to lines 20-22 of Listing 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional state updates to retain heavier ows:</head><p>The rst pipeline stage involves a conditional update to a register to distinguish a hit and a miss for the incoming packet key in the table (line 17, Listing 1). Subsequent stages must also write back di erent values to the table key and count registers, depending on the result of the lookup (hit/miss) and a comparison of the ow counts. Accordingly, we perform a conditional write into the ow id and count registers (lines 4 and 7, Listing 2). Such conditional state updates are feasible at line rate <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We now evaluate HashPipe through trace-driven simulations. We tune the main parameter of HashPipethe number of table stages d-in §5.1. We evaluate the performance of HashPipe in isolation in §5.2, and then compare it to prior sampling and sketching solutions in §5.3. Then, we examine the performance of HashPipe in context of the idealized algorithms it is derived from ( §3) in §5.4. Experiment setup. We compute the k heaviest ows using two sets of traces. The rst trace is from a 10Gb/s ISP backbone link, recorded in 2016 and available from CAIDA <ref type="bibr" target="#b7">[8]</ref>. We measure heavy hitters aggregated by transport 5-tuple. The tra c trace is 17 minutes long, and contains 400 million packets. We split this trace into 50 chunks, each being about 20 seconds long, with 10 million packets. The chunks on average contain about 400,000 5-tuple ows each. Each chunk is one trial, and each data point in the graphs for the ISP trace reports the average across 50 trials. We assume that the switch zeroes out its tables at the end of each trial, which corresponds to a 20 second "table ush" period.</p><p>The second trace, recorded in 2010, is from a data center <ref type="bibr" target="#b3">[4]</ref> and consists of about 100 million packets in total. We measure heavy hitters aggregated by source and destination IPs. We split the trace into 1 second intervals corresponding to the time scale at which data center tra c exhibits stability <ref type="bibr" target="#b4">[5]</ref>.</p><p>The data center trace is two and a half hours long, with roughly 10K packets (300 ows) per second. We additionally replay the trace at two higher packet rates to test whether HashPipe can provide good accuracy over the 1 second time scale. We assume a "typical" average packet size of 850 bytes and a 30% network utilization as reported in prior tra c studies <ref type="bibr" target="#b3">[4]</ref>. For a switch clocked at 1GHz with 48 ports of 10Gb/s each, this corresponds to roughly 20 million packets per second through the entire switch, and 410K packets per second through a single link. At these packet rates, the trace contains 20,000 and 3200 ows per second respectively. For each packet rate, we report results averaged from multiple trials of 1 second each. Metrics. As discussed in §2.1, we evaluate schemes on false negatives (% heavy ows that are not reported), false positives (% non-heavy ows that are reported), and the count estimation error (% error for heavy ows). Note that for the top-k problem, the false positive error is just a scaled version of the false negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tuning HashPipe</head><p>Given a total memory size m, HashPipe's only tunable parameter is the number of table stages d that it uses. Once d is xed, we simply partition the available memory equally into d hash tables. As d increases, the number of table slots over which a minimum is computed increases, leading to increased retention of heavier keys. However, with a xed total memory, an increase in the value of d decreases the per-stage hash table size, increasing the likelihood of hash collisions, and also of duplicates ( §3.4). The switch hardware constrains the number of table stages to a small number, e.g., 6-16 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. Fig. <ref type="figure" target="#fig_6">2</ref> shows the impact of changing d on the false negatives. For the ISP trace, we plot false negatives for di erent sizes of memory m and di erent number of desired heavy hitters k. As expected, the false negatives reduce as d increases starting at d = 2, but the decrease quickly tapers o in the range between 5-8, across the di erent (m, k ) curves. For the data center trace, we show false negatives for k = 350 with a memory of 840  counters (15KB), and we see a similar trend across all three packet rates. The false positives, elided here, also follow the same trend.</p><p>To understand whether duplicates impact the false negative rates, we also show the prevalence of duplicates in HashPipe's hash tables in Fig. <ref type="figure" target="#fig_2">3</ref>. Overall, duplicates only take up between 5-10% of the available table size in the ISP case and between 5-14% in the data center case. As expected, in going from d = 2 and d = 8, the prevalence of duplicates in HashPipe's table increases.</p><p>Fig. <ref type="figure" target="#fig_10">4</ref> shows the count estimation error (as a percentage of actual ow size) for ows in HashPipe's tables at the end of each measurement interval, with a memory size of 640 counters (11.2KB). In general, the error reduces as d increases, but the reduction from d = 4 to d = 8 is less signi cant than the reduction from d = 2 to d = 4. In the ISP trace, the estimation error is stable across ow sizes since most ows recorded by HashPipe have sizes of at least 1000. In the data center trace where there are fewer total ows, there is a more apparent decrease in error with true ow size, with ows of size x &gt; 1000 having near-perfect count estimations. Choosing d = 6 table stages. To summarize, we nd that (i) as the number of table stages increases above d = 4, all the accuracy metrics improve; (ii) however, the improvement dimishes at d = 8 and beyond, due to the increasing prevalence of duplicates and hash collisions. These trends hold across both the ISP and data center scenarios, for a variety of measured heavy hitters k and memory sizes. Hence, we choose d = 6 table stages for all further experiments with HashPipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accuracy of HashPipe</head><p>We plot error vs. memory tradeo curves for HashPipe, run with d = 6 table stages. Henceforth, unless mentioned otherwise, we run the data center trace at the single link packet rate. False negatives. Fig. <ref type="figure" target="#fig_12">5</ref> shows the false negatives as memory increases, with curves corresponding to di erent numbers of reported heavy hitters k. We nd that error decreases with allocated memory across a range of k values, and settles to under 10% in all cases on the ISP    trace at 80KB of memory, which corresponds to 4500 counters. In any one trial with the ISP trace, there are on average 400,000 ows, which is two orders of magnitude higher than the number of counters we use. In the data center trace, the error settles to under 10% at just 9KB of memory (520 counters) for all the k values we tested. <ref type="foot" target="#foot_2">3</ref>These results also enable us to understand the interplay between k and the memory size required for a speci c accuracy. For a 5% false negative rate in the ISP trace, the memory required for k = 60 is 60KB (3375 ≈ 55k counters), whereas the memory required for k = 300 is 110KB (6200 ≈ 20k counters). In general, the factor of k required in the number of counters to achieve a particular accuracy reduces as k increases. Which ows are likely to be missed? It is natural to ask which ows are more likely missed by HashPipe. Fig. <ref type="figure" target="#fig_13">6</ref> shows how the false negative rate changes as the number of desired heavy hitters k is varied, for three di erent total memory sizes. We nd that the heaviest ows are least likely to be missed, e.g., with false negatives in the 1-2% range for the top 20 ows, when using 3000 counters in the ISP trace. This trend is intuitive, since HashPipe prioritizes the retention of larger ows in the table ( §3.1). Most of the higher values of false negative errors are at larger values of k, meaning that the smaller    of the top k ows are more likely to be missed among the reported ows. False positives. Fig. <ref type="figure" target="#fig_14">7</ref> shows the false positives of Hash-Pipe against varying memory sizes, exhibiting the natural trend of decreasing error with increasing memory. In particular, we nd that with the ISP trace, false positive rates are very low, partly owing to the large number of small ows. On all curves, the false positive rate is smaller than 0.1%, dropping to lower than 0.01% at a table size of 80KB. In the data center trace, we nd that the false positive rate hovers under 3% over a range of memory sizes and heavy hitters k.</p><p>In summary, HashPipe performs well both with the ISP backbone and data center traces, recognizing heavyhitter ows in a timely manner, i.e., within 20 seconds and 1 second respectively, directly in the data plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">HashPipe vs. Existing Solutions</head><p>Comparison baselines. We compare HashPipe against two schemes-representative of sampling and sketchingwhich are able to estimate counts in the switch directly ( §2.2). We use the same total memory as earlier, and measure the top k = 150 ows. We use the ISP backbone trace henceforth (unless mentioned otherwise), and compare HashPipe with the following baseline schemes.</p><p>(1) Sample and Hold: We simulate sample and hold <ref type="bibr" target="#b16">[17]</ref> with a ow table that is implemented as a list. As described in §2.2, we liberally allow incoming packets to look up ow entries anywhere in the list, and add new entries up to the total memory size. The sampling probability is set according to the available ow table size, following recommendations from <ref type="bibr" target="#b16">[17]</ref>.</p><p>(2) Count-min sketch augmented with a 'heavy ow' cache: We simulate the count-min sketch <ref type="bibr" target="#b13">[14]</ref>, but use a ow cache to keep ow keys and exact counts starting from the packet where a ow is estimated to be heavy from the sketch. <ref type="foot" target="#foot_3">4</ref> We liberally allow the countmin sketch to use the o ine-estimated exact count of the kth heaviest ow in the trace, as a threshold to identify heavy ows. The ow cache is implemented as a hash table that retains only the pre-existing ow key on a hash collision. We allocate half the available memory each to the sketch and the ow cache.  False negatives. Fig. <ref type="figure" target="#fig_16">8</ref> shows false negatives against varying memory sizes, for the three schemes compared. We see that HashPipe outperforms sample and hold as well as the augmented count-min sketch over the entire memory range. (All schemes have smaller errors as the memory increases.) Notably, at 100KB memory, Hash-Pipe has 15% smaller false negative rate than sample and hold. The count-min sketch tracks the error rate of HashPipe more closely from above, staying within a 3-4% error di erence. Next, we understand where the errors occur in the baseline schemes.</p><p>Where are the errors in the other baselines? Fig. <ref type="figure" target="#fig_17">9</ref> shows the count estimation error (%) averaged across ows whose true counts are higher than the x-value, when running all the schemes with 26KB memory.</p><p>Sample and hold can make two kinds of estimation errors. It can miss the rst set of packets from a heavy ow because of not sampling the ow early enough, or (less likely) miss the ow entirely due to not sampling or the ow table becoming full. Fig. <ref type="figure" target="#fig_17">9</ref> shows that sample and hold makes the former kind of error even for ows with very high true counts. For example, there are relative errors of about 10% even for ows of size more than 80,000 packets. As Fig. <ref type="figure" target="#fig_16">8</ref> shows, the errors becomes less prominent as the memory size increases, since the sampling rate increases too.</p><p>The count-min sketch makes errors because of its inability to discriminate between heavy and light ows during hash collisions in the sketch. This means that a light ow colliding with heavy ows may occupy a ow cache entry, preventing a heavier ow later on from entering the ow cache. For instance in Fig. <ref type="figure" target="#fig_17">9</ref>, even ows as large as 50,000 packets can have estimation errors close to 20%. However, as Fig. <ref type="figure" target="#fig_16">8</ref> shows, the e ect of hash collisions becomes less signi cant as memory size increases.</p><p>On the other hand, HashPipe's average error on ows larger than 20,000 packets-which is 0.2% of the total packets in the interval-is negligible (Fig. <ref type="figure" target="#fig_17">9</ref>). HashPipe has 100% accuracy in estimating the count of ows larger than 30,000 packets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">HashPipe vs. Idealized Schemes</head><p>We now compare HashPipe against the idealized algorithms it is derived from ( §3), namely space saving <ref type="bibr" target="#b27">[28]</ref> and HashParallel.</p><p>There are two reasons why HashPipe may do badly relative to space saving: (i) it may evict a key whose count is much higher than the table minimum, hence missing heavy items from the table, and (ii) it may allow too many duplicate ow keys in the table ( §3.4), reducing the memory available for heavy ows, and evict heavy ows whose counts are underestimated due to the duplicates. We showed that duplicate keys are not very prevalent in §5.1; in what follows, we understand their e ects on false negatives in the reported ows. How far is the subsampled minimum from the true table minimum? Fig. <ref type="figure" target="#fig_4">10</ref> shows the complementary CDF of the minimum count that was chosen by Hash-Pipe, obtained by sampling the algorithm's choice of minimum at every 100th packet in a 10 million packet trace. We don't show the corresponding CCDF for the absolute minimum in the table, which only takes on two values-0 and 1-with the minimum being 1 more than 99% likely. <ref type="foot" target="#foot_5">6</ref> We see that the chance that HashPipe chooses a certain minimum value decreases rapidly as that value grows, judging from the straight-line plot on log scales in Fig. <ref type="figure" target="#fig_4">10</ref>. For example, the minimum counter has a value higher than 5 less than 5% of the time. There are a few larger minimum values (e.g., 100), but they are rarer (less than 0.01% of the time). This suggests that it is unlikely that HashPipe experiences large errors due to the choice of a larger minimum from the table, when a smaller counter exists. Comparison against space saving and HashParallel. We compare the false negatives of the idealized schemes and HashPipe against varying memory sizes in Fig. <ref type="figure" target="#fig_4">11</ref> and Fig. <ref type="figure" target="#fig_19">12</ref>, when reporting two di erent number of heavy hitters, k=150 and k=60 respectively. Two features stand out from the graph for both k values. First, wherever the schemes operate with low false negative error (say less than 20%), the performance of the three schemes is comparable (i.e., within 2-3% of each  other). Second, there are small values of memory where HashPipe outperforms space saving.</p><p>Why is HashPipe outperforming space saving? Space saving only guarantees that the kth heaviest item is in the table when the kth item is larger than the average table count ( §3.1). In our trace, the 60th item and 150th item contribute to roughly 6000 and 4000 packets out of 10 million (resp.), which means that they require at least<ref type="foot" target="#foot_6">7</ref> 1700 counters and 2500 counters in the table (resp.). These correspond to memory sizes of 30KB and 45KB (resp.). At those values of memory, we see that space saving starts outperforming HashPipe on false negatives.</p><p>We also show why space saving fails to capture the heavier ows when it is allocated a number of counters smaller than the minimum number of counters mentioned above. Note that HashPipe attributes every packet to its ow entry correctly (but may miss some packets entirely), since it always starts a new ow at counter value 1. However, space saving increments some counter for every incoming packet (Algorithm 1). In contexts where the number of active ows is much larger than the number of available counters (e.g., 400,000 ows with 1200 counters), this can lead to some ows having enormously large (and grossly overestimated) counters. In contrast, HashPipe keeps the counter values small for small ows by evicting the ows (and counts) entirely from the table.</p><p>At memory sizes smaller than the thresholds mentioned above, incrementing a counter for each packet may result in several small ows catching up to a heavy ow, leading to signi cant false positives, and higher likelihood of evicting truly heavy ows from the table. We show this e ect on space saving in Fig. <ref type="figure" target="#fig_8">13</ref> for k=150 and m=1200 counters, where in fact m = 2500 counters are required as described earlier. The distribution of the number of keys contributing to a false positive ow counter in the table is clearly shifted to the right relative to the corresponding distribution for a true positive. Impact of duplicate keys in the table. Finally, we investigate how duplicate keys and consequent underestimation of ow counts in the table may a ect the errors of HashPipe. In Fig. <ref type="figure" target="#fig_21">14</ref>, we show the bene ts of reporting more than k counters on false negatives, when the top k=300 ows are requested with a memory size of m=2400 counters. While the false negative rates of space saving and HashParallel improve signi cantly with overreporting, the errors for HashPipe remains at throughout the interval, dropping only around 1800 reported ows. We infer that most heavy ows are retained somewhere in the table for space saving and HashParallel, while HashPipe underestimates keys su ciently often that Figure <ref type="figure" target="#fig_8">13</ref>: CDF of the number of distinct ows contributing to a counter in space saving's table, when identifying k=150 heavy ows with m=1200 counters. We show three distributions according to the ow's label after the experiment.</p><p>they are completely evicted-to the point where overreporting does not lower the false negative errors. We nd that overreporting ows only increases the false positive errors slightly for all schemes, with values staying between 0.1-0.5% throughout. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Applications that use heavy hitters. Several applications use information about heavy ows to do better tra c management or monitoring. DevoFlow <ref type="bibr" target="#b15">[16]</ref> and Planck <ref type="bibr" target="#b34">[35]</ref> propose exposing heavy ows with low overhead as ways to provide better visibility and reduce congestion in the network. UnivMon <ref type="bibr" target="#b24">[25]</ref> uses a top-k detection sketch internally as a subroutine in its "universal" sketch, to determine more general statistics about the network tra c. There is even a P4 tutorial application on switch programming, that performs heavy hitter detection using a count-min-sketch-like algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>Measuring per-ow counters. Prior works such as FlowRadar <ref type="bibr" target="#b23">[24]</ref> and CounterBraids <ref type="bibr" target="#b25">[26]</ref> have proposed schemes to measure accurate per-ow tra c counters. Along similar lines, hashing schemes like cuckoo hashing <ref type="bibr" target="#b32">[33]</ref> and d-left hashing <ref type="bibr" target="#b42">[43]</ref> can keep per-ow state memory-e ciently, while providing fast lookups on the state. Our goal is not to measure or keep all ows; just the heavy ones. We show ( §5) that HashPipe uses 2-3 orders of magnitude smaller memory relative to having per-ow state for all active ows, while catching more than 90% of the heavy ows.</p><p>Other heavy-hitter detection approaches. The multiresolution tiling technique in ProgME <ref type="bibr" target="#b45">[46]</ref>, and the hierarchical heavy-hitter algorithm of Jose et al. <ref type="bibr" target="#b20">[21]</ref> solve a similar problem as ours. They estimate heavy hitters in tra c online, by iteratively "zooming in" to the portions of tra c which are more likely to contain heavy ows. However, these algorithms involve the control plane in running their ow-space-partitioning algorithms, while HashPipe works completely within the switch pipeline. Further, both prior approaches require temporal stability of the heavy-hitter ows to detect them over multiple intervals; HashPipe determines heavy hitters using counters maintained in the same interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we proposed an algorithm to detect heavy tra c ows within the constraints of emerging programmable switches, and making this information available within the switch itself, as packets are processed. Our solution, HashPipe, uses a pipeline of hash tables to track heavy ows preferentially, by evicting lighter ows from switch memory over time. We prototype HashPipe with P4, walking through the switch programming features used to implement our algorithm. Through simulations on a real tra c trace, we showed that HashPipe achieves high accuracy in nding heavy ows within the memory constraints of switches today.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 3 al j ← al j + 1 4 else 5 if ∃ empty slot j in T then 6 ( 7 else 8 r 9 (</head><label>3156789</label><figDesc>if ∃ slot j in T with iKe = ke j then ke j , al j ) ← (iKe , 1) ← ar min j ∈ {1, ...,m } ( al j ) ke r , al r ) ← (iKe , al r + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8 else 9 r</head><label>89</label><figDesc>← ar min j ∈H ( al j ) 10 (ke r , al r ) ← (iKe , al r + 1) 11 end 12 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 3 :</head><label>3</label><figDesc>HashPipe: Pipeline of d hash tables 1 Insert in the rst stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>cKe , cV al ) ← (ke l 1 , al l 1 ) (ke l 1 , al l 1 ) ← (iKe , 1) end Track a rolling minimum for i ← 2 to d do l ← h i (cKe ) if ke l = cKe then al l ← al l + cV al end processing end else if l is an empty slot then (ke l , al l ) ← (cKe , CV al ) end processing end else if al l &lt; cV al then swap (cKe , cV al ) with (ke l , al l ) end end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of HashPipe.1 action doStage1 () { 2 mKeyCarried = ipv4.srcAddr; 3 mCountCarried = 0; 4 modify_ eld_with_hash_based_o set (mIndex, 0, stage1Hash, 32) ; 5 6 // read the key and value at that location 7 mKeyTable = owTracker[mIndex]; 8 mCountTable = packetCount[mIndex]; 9 mValid = validBit [mIndex]; // check for empty location or di erent key mKeyTable = (mValid == 0) ? mKeyCarried : mKeyTable; mDif = (mValid == 0) ? 0 : mKeyTable -mKeyCarried; // update hash table owTracker[mIndex] = ipv4 . srcAddr; packetCount[mIndex] = (mDif == 0) ? mCountTable+1: 1; validBit [mIndex] = 1; // update metadata carried to the next table stage mKeyCarried = (mDif == 0) ? 0 : mKeyTable; mCountCarried = (mDif == 0) ? 0 : mCountTable; }</figDesc><graphic coords="6,332.33,172.23,157.94,70.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of table stages (d) on false negatives. Error decreases as d grows, and then attens out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prevalence of duplicate keys in tables. For different d values under the memory range tested, the proportion of duplicates is between 5-10% for the ISP trace and 5-15% for the data center trace (link packet rate).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>ISP backbone (b) Data center (link)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Average estimation error (%) of ows whose true counts are higher than the x-value in the graph. Error decreases as d grows but the bene t diminishes with increasing d. Error decreases with actual ow size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: False negatives of HashPipe with increasing memory. Each trial trace from the ISP backbone contains an average of 400,000 ows, yet HashPipe achieves 5-10% false negatives for the top 60-300 heavy hitters with just 4500 ow counters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: False negatives against di erent numbers of reported heavy ows k. The heavier a ow is, the less likely that it is missed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: HashPipe has false positive rates of 0.01%-0.1% across a range of table sizes and reported heavy hitters in the ISP trace, and under 3% in the data center trace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: False negatives of HashPipe and other baselines. HashPipe outperforms sample and hold and count-min sketch over the entire memory range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of average estimation error (%) ofows whose true counts are higher than the x-value in the graph. Sample and hold underestimates heavy ows due to sampling, and count-min sketch misses heavy ows due to lighter ows colliding in the ow cache. HashPipe has no estimation errors for ows larger than 30,000 packets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Complementary CDF of the minimum chosen by HashPipe on 26KB of memory, sampled every 100th packet from a trace of 10 million packets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Comparison of false negatives of HashPipe to idealized schemes when detecting k=60 heavy hitters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Bene ts of overreporting ows on false negative errors with k=300 heavy ows and m=2400 counters. While space saving and HashParallel improve signicantly by reporting even 2k ows, HashPipe does not, because of evictions due to duplicate entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Listing 2: HashPipe stage with rolling minimum. Fields pre xed with m are metadata elds.</figDesc><table><row><cell cols="2">1 action doStage2{</cell></row><row><cell>2</cell><cell>• • •</cell></row><row><cell>3</cell><cell>mKeyToWrite = (mCountInTable &lt; mCountCarried) ?</cell></row><row><cell></cell><cell>mKeyCarried : mKeyTable));</cell></row><row><cell>4</cell><cell>owTracker[mIndex] = (mDif == 0) ? mKeyTable :</cell></row><row><cell></cell><cell>mKeyToWrite;</cell></row><row><cell>5</cell><cell></cell></row><row><cell>6</cell><cell>mCountToWrite = (mCountTable &lt; mCountCarried) ?</cell></row><row><cell></cell><cell>mCountCarried : mCountTable;</cell></row><row><cell>7</cell><cell>packetCount[mIndex] = (mDif == 0) ? (mCountTable +</cell></row><row><cell></cell><cell>mCountCarried) : mCountToWrite;</cell></row><row><cell>8</cell><cell></cell></row><row><cell>9</cell><cell>mBitToWrite = (mKeyCarried == 0) ? 0 : 1) ;</cell></row><row><cell></cell><cell>validBit [mIndex] = (mValid == 0) ? mBitToWrite : 1) ;</cell></row><row><cell></cell><cell>• • •</cell></row><row><cell>}</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Large exact-match lookups are typically built with SRAM, as large TCAMs are expensive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We show the algorithm for packet counting; it easily generalizes to byte counts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The minimum memory required at k = 300 is 6KB (≈ 300 counters). 8</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>A simpler alternative-mirroring packets with high size estimates from the count-min sketch-requires collecting ≈ 40% of the packets in the data center trace played at the link rate.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We follow guidance from[17, page 288]  to split the memory. 9</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Note that this is di erent from the minimum of space saving, whose distribution contains much larger values.<ref type="bibr" target="#b9">10</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>10 million / 6000 ≈ 1700; 10 million / 4000 ≈ 2500.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous SOSR reviewers, Anirudh Sivaraman, and Bharath Balasubramanian for their feedback. This work was supported partly by NSF grant CCF-1535948, DARPA grant HR0011-15-2-0047, and gifts from Intel and Huawei. We also thank the industrial members of the MIT Center for Wireless Networks and Mobile Computing (Wireless@MIT) for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hedera: Dynamic ow scheduling for data center networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Arista</forename><surname>Networks</surname></persName>
		</author>
		<ptr target="https://www.corporatearmor.com/documents/Arista_7050X_Switch_Architecture_Datasheet.pdf" />
		<title level="m">Arista 7050x switch architecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="https://www.barefootnetworks.com/technology/" />
	</analytic>
	<monogr>
		<title level="j">Barefoot Networks. Barefoot To</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network tra c characteristics of data centers in the wild</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM IMC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MicroTE: Fine grained tra c engineering for data centers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CoNEXT</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Programming protocol-independent packet processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Talayco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>P</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Forwarding metamorphosis: Fast programmable match-action processing in hardware for SDN</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The CAIDA UCSD Anonymized Internet Traces 2016 -March</title>
		<author>
			<persName><surname>Caida</surname></persName>
		</author>
		<ptr target="http://www.caida.org/data/passive/passive_2016_dataset.xml" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cavium and XPliant Introduce a Fully Programmable Switch Silicon Family Scaling to 3.2 Terabits per Second</title>
		<author>
			<persName><surname>Cavium</surname></persName>
		</author>
		<ptr target="http://cavium.com/newsevents-Cavium-and-XPliant-Introduce-a-Fully-Programmable-Switch-Silicon-Family.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer ICALP</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Implementing Heavy-Hitter Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://github.com/p4lang/tutorials/tree/master/SIGCOMM_2016/heavy_hitter" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Net ow</title>
		<author>
			<persName><forename type="first">Cisco</forename><surname>Networks</surname></persName>
		</author>
		<ptr target="http://www.cisco.com/c/en/us/products/ios-nx-os-software/ios-netow/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadjieleftheriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB Endowment</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An improved data stream summary: The count-min sketch and its applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="75" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What&apos;s hot and what&apos;s not: Tracking most frequent items dynamically</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DevoFlow: Scaling ow management for highperformance networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tourrilhes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">New directions in tra c measurement and accounting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Computer Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deriving tra c demands for operational IP networks: Methodology and experience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>True</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Millions of little minions: Using packets for low latency network programming and visibility</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazières</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lazzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsup</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://inst.eecs.berkeley.edu/~cs250/fa10/lectures/lec08.pdf" />
	</analytic>
	<monogr>
		<title level="j">Memory (lecture)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online measurement of large tra c aggregates on commodity switches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Hot-ICE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-learning of MAC addresses and lexicographic lookup of hardware database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Prasad</surname></persName>
		</author>
		<idno>App. 10/747</idno>
		<imprint>
			<biblScope unit="page">332</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Characterization of network-wide anomalies in tra c ows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lakhina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crovella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM IMC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FlowRadar: A better NetFlow for data centers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One sketch to rule them all: Rethinking network ow monitoring with UnivMon</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vorsanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Counter braids: A novel counter architecture for per-ow measurement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dharmapurikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kabbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMETRICS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximate frequency counts over data streams</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB Endowment</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">E cient computation of frequent and top-k elements in data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metwally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Abbadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding repeated elements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science of computer programming</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="152" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="https://mailman.nanog.org/pipermail/nanog/2016-February/thread.html#84418" />
		<title level="m">SFlow vs NetFlow/IPFIX</title>
		<imprint/>
	</monogr>
	<note>NANOG mailing list</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ozdag</surname></persName>
		</author>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ethernet-switch-fm6000-sdn-paper.pdf" />
		<title level="m">Intel Ethernet Switch FM6000 Series -Software De ned Networking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="https://github.com/p4lang/behavioral-model" />
		<title level="m">P4 Language Consortium. P4 Switch Behavioral Model</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Rodler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cuckoo hashing. J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="144" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<ptr target="http://blog.sow.com/2009/06/sampling-rates.html" />
		<title level="m">SFlow sampling rates</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Planck: Millisecond-scale monitoring and control for commodity networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rozner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimal rule caching and lossy compression for longest pre x matching</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rottenstreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tapolcai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reversible sketches for e cient and accurate change detection over network data streams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schweller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM IMC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="http://sow.org/" />
		<title level="m">SFlow</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Packet recirculation</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Horman</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/546476/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Packet transactions: High-level programming for line-rate switches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Licking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Programmable packet scheduling at line rate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Edsall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Language Consortium. The P4 Language Speci cation, Version 1.1.0 -Release Candidate</title>
		<ptr target="http://p4.org/wp-content/uploads/2016/03/p4_v1.1.pdf" />
		<imprint>
			<date type="published" when="2016-01">January 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How asymmetry helps load balancing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vöcking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="568" to="589" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">CMOS VLSI Design: A Circuits and Systems Perspective</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Software de ned tra c measurement with OpenSketch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ProgME: Towards programmable network measurement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mohapatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
