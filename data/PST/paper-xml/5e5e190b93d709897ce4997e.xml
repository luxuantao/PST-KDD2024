<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Reasoning on Large Knowledge Bases and Natural Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
							<email>p.minervini@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">UCL Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matko</forename><surname>Bošnjak</surname></persName>
							<email>m.bosnjak@cs.ucl.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>‡1</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">UCL Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Rocktäschel</surname></persName>
							<email>t.rocktaschel@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">UCL Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<email>s.riedel@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">UCL Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
							<email>e.grefenstette@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">UCL Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University College London</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Reasoning on Large Knowledge Bases and Natural Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning with knowledge expressed in natural language and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. General neural architectures that jointly learn representations and transformations of text are very datainefficient, and it is hard to analyse their reasoning process. These issues are addressed by end-to-end differentiable reasoning systems such as Neural Theorem Provers (NTPs), although they can only be used with small-scale symbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension to NTPs addressing their complexity and scalability limitations, thus making them applicable to real-world datasets. This result is achieved by dynamically constructing the computation graph of NTPs and including only the most promising proof paths during inference, thus obtaining orders of magnitude more efficient models 1 . Then, we propose a novel approach for jointly reasoning over KBs and textual mentions, by embedding logic facts and natural language sentences in a shared embedding space. We show that GNTPs perform on par with NTPs at a fraction of their cost while achieving competitive link prediction results on large datasets, providing explanations for predictions, and inducing interpretable models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The main focus of Artificial Intelligence is building systems that exhibit intelligent behaviour <ref type="bibr" target="#b25">(Levesque 2014)</ref>. Notably, Natural Language Understanding (NLU) and Machine Reading (MR) aim at building models and systems with the ability to read text, extract meaningful knowledge, and reason with it <ref type="bibr">(Etzioni, Banko, and Cafarella 2006;</ref><ref type="bibr" target="#b16">Hermann et al. 2015;</ref><ref type="bibr" target="#b44">Weston et al. 2015;</ref><ref type="bibr" target="#b7">Das et al. 2017)</ref>. This ability facilitates both the synthesis of new knowledge and the possibility to verify and update a given assertion. Traditionally, automated reasoning applied to text requires natural language processing tools that compile it into the structured form of a KB <ref type="bibr" target="#b35">(Niklaus et al. 2018)</ref>. However, the compiled KBs tend to be incomplete, ambiguous, and noisy, impairing the application of standard deductive reasoners <ref type="bibr" target="#b17">(Huang, van Harmelen, and ten Teije 2005)</ref>.</p><p>A rich and broad literature in MR has approached this problem within a variety of frameworks, including Natural Logic (MacCartney and Manning 2007), Semantic Parsing <ref type="bibr" target="#b2">(Bos 2008)</ref>, Natural Language Inference and Recognising Textual Entailment <ref type="bibr" target="#b10">(Fyodorov, Winter, and Francez 2000;</ref><ref type="bibr" target="#b6">Bowman et al. 2015)</ref>, and Question Answering <ref type="bibr">(Hermann et al. 2015)</ref>. Nonetheless, such methods suffer from several limitations. They rely on significant amounts of annotated data to suitably approximate the implicit distribution from which the data is drawn. In practice, this makes them unable to generalise well in the absence of a sufficient quantity of training data or appropriate priors on model parameters (Evans and <ref type="bibr" target="#b9">Grefenstette 2018)</ref>. Orthogonally, even when accurate, such methods cannot explain given predictions <ref type="bibr" target="#b26">(Lipton 2018)</ref>.</p><p>A promising strategy for overcoming these issues consists of combining neural models and symbolic reasoning, given their complementary strengths and weaknesses <ref type="bibr">(d'Avila Garcez et al. 2015;</ref><ref type="bibr" target="#b39">Rocktäschel and Riedel 2017;</ref><ref type="bibr" target="#b47">Yang, Yang, and Cohen 2017;</ref><ref type="bibr">Evans and Grefenstette 2018;</ref><ref type="bibr" target="#b43">Weber et al. 2019)</ref>. While symbolic models can generalise well from a small number of examples, they are brittle and prone to failure when the observations are noisy or ambiguous, or when the properties of the domain are unknown or hard to formalise, all of which being the case for natural language <ref type="bibr" target="#b37">(Raedt et al. 2008;</ref><ref type="bibr" target="#b12">Garnelo and Shanahan 2019)</ref>. Contrarily, neural models are robust to noise and ambiguity but not easily interpretable, making them unable to provide explanations or incorporating background knowledge <ref type="bibr" target="#b15">(Guidotti et al. 2018)</ref>.</p><p>Recent work in neuro-symbolic systems has made progress towards end-to-end differentiable reasoning models that can be trained via backpropagation while maintaining interpretability and generalisation, thereby inheriting the best of both worlds. Among such systems, NTPs <ref type="bibr" target="#b39">(Rocktäschel and Riedel 2017;</ref><ref type="bibr" target="#b32">Minervini et al. 2018</ref>) are end-to-end differentiable deductive reasoners based on Prolog's backward chaining algorithm, where discrete unification between atoms is replaced by a differentiable operator computing the similarities between their embedding representations.</p><p>NTPs are especially interesting since they allow learning interpretable rules from data, by back-propagating the prediction errors to the rule representations. Furthermore, the proving process in NTPs is explainable -the proof path associated with the largest proof score denotes which rules and facts are used in the reasoning process. However, NTPs have <ref type="bibr">arXiv:1912.10824v1 [cs.</ref>LG] 17 Dec 2019</p><p>Rule Group p(X, Y) :-q(Y, X)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rules</head><p>Rule Group p(X, Y) :-q(X, Z), r(Z, Y)</p><p>X Y :- "London is located in the UK"</p><p>"London is standing on the River Thames" only been successfully applied to learning tasks involving very small datasets, since their computational complexity makes them unusable on larger, real-world KBs. Furthermore, most human knowledge is not available in KBs, but in natural language texts which are difficult to reason over automatically.</p><formula xml:id="formula_0">"[X] is located in the [Y]"(X, Y) :- locatedIn(X, Y) locatedIn(X, Y) :-locatedIn(X, Z), locatedIn(Z, Y) KB Rep. Text Representations X Y :-Y X X Y :-Y X X Y :-X Z , Z Y X Y :-X Z , Z Y Recurse k-NN OR</formula><p>In this paper we address these issues by proposing: i) two efficiency improvements for significantly reducing the time and space complexity of NTPs by reducing the number of candidate proof paths and introducing an attention mechanism for rule induction, and ii) an extension of NTPs towards natural language, jointly embedding predicates and textual surface patterns in a shared space by using an end-to-end differentiable reading component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end Differentiable Proving</head><p>NTPs <ref type="bibr" target="#b39">(Rocktäschel and Riedel 2017)</ref> recursively build a neural network enumerating all the possible proof paths for proving a query (or goal) on a given KB, and aggregate all their proof scores via max pooling. They do so by relying on three modules-a unification module, which compares sub-symbolic representations of logic atoms, and mutually recursive or and and modules, which jointly enumerate all possible proof paths, before the final aggregation selects the highest-scoring one.</p><p>In the following, we briefly overview these modules, and the training process used for learning the model parameters from data. We assume the existence of a function-free Datalog KB K containing ground facts in the form [p, A, B]<ref type="foot" target="#foot_0">2</ref> , representing the logical atom p(A, B) where p is a relation type, and A, B are its arguments. <ref type="foot" target="#foot_1">3</ref> It also contains rules in the form H :-B such as</p><formula xml:id="formula_1">[p, X, Y] :-[[q, X, Z], [r, Z, Y]],</formula><p>denoting the rule p(X, Y) :q(X, Z), r(Z, Y), meaning that q(X, Z), r(Z, Y) implies p(X, Y), where X, Y, Z are universally quantified variables.</p><p>Unification Module. In the backward chaining reasoning algorithm, unification is the operator that matches two logic atoms, such as locatedIn(LONDON, UK) and situatedIn(X, Y). Discrete unification checks for equality between the elements composing the two atoms (e.g. locatedIn = situatedIn), and binds variables to symbols via substitutions (e.g. {X/LONDON, Y/UK}). In NTPs, unification matches two atoms by comparing their embedding representations via a differentiable similarity function -a Gaussian kernel -which enables matching different symbols with similar semantics.</p><p>More formally, unify θ (H, G, S) = S creates a neural network module that matches two atoms H and G by comparing their embedding vectors. For instance, given a goal G = [locatedIn, LONDON, UK], a fact H = [situatedIn, X, Y], and a proof state S = (S ψ , S ρ ) consisting of a set of substitutions S ψ and a proof score S ρ , the unify module compares the embedding representations of locatedIn and situatedIn with a Gaussian kernel k, updates the variable binding substitution set S ψ = S ψ ∪ {X/LONDON, Y/UK}, and calculates the new proof score S ρ = min (S ρ , k (θ locatedIn: , θ situatedIn: )) and proof state S = (S ψ , S ρ ).</p><p>OR Module. The or module computes the unification between a goal and all facts and rule heads in a KB, and then recursively invokes the and module on the corresponding rule bodies. Formally, for each rule H :-B 4 in a KB K, or K θ (G, d, S) unifies the goal G with the rule head H, and invokes the and module to prove atoms in the body B, keeping track of the maximum proof depth d: </p><formula xml:id="formula_2">or K θ (G, d, S) = [S | H :-B ∈ K, S ∈</formula><formula xml:id="formula_3">and K θ (B : B, d, S) = [S | d &gt; 0, S ∈ and K θ (B, d, S ), S ∈ or K θ (sub(B, S ψ ), d − 1, S)]<label>(2)</label></formula><p>For example, when invoked on the rule body B of the example mentioned above, the and module will substitute variables with constants for the sub-goal [locatedIn, X, Z] and invoke the or module, whose resulting state will be the basis of the next invocation of and module on [locatedIn, Z, Y].</p><p>Proof Aggregation. After building a neural network that evaluates all the possible proof paths of a goal G on a KB K, NTPs select the proof path with the largest proof score:</p><formula xml:id="formula_4">ntp K θ (G, d) = max S S ρ with S ∈ or K θ (G, d, (∅, 1))<label>(3)</label></formula><p>where d ∈ N is a predefined maximum proof depth. The initial proof state is set to (∅, 1) corresponding to an empty substitution set and to a proof score of 1.</p><p>Training. In NTPs, embedding representations are learned by minimising a cross-entropy loss L K (θ) on the final proof score, by iteratively masking facts in the KB and trying to prove them using other available facts and rules.</p><p>Negative examples are obtained via a corruption process, denoted by corrupt(•), by modifying the subject and object of triples in the KB <ref type="bibr" target="#b33">(Nickel et al. 2016)</ref>:</p><formula xml:id="formula_5">L K (θ) = − F :-[]∈K log ntp K\F θ (F, d) − F∼corrupt(F) log[1 − ntp K θ ( F, d)]<label>(4)</label></formula><p>NTPs can also learn interpretable rules. <ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref> show that it is possible to learn rules from data by specifying rule templates, such as H :</p><formula xml:id="formula_6">-B with H = [θ p: , X, Y] and B = [[θ q: , X, Z], [θ r: , Z, Y]].</formula><p>Parameters θ p: , θ q: , θ r: ∈ R k , denoting rule-predicate embeddings, can be learned from data by minimising the loss in Eq. 4, and decoded by searching the closest representation of known predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Differentiable Reasoning on Large-Scale KBs</head><p>NTPs are capable of deductive reasoning, and the proof paths with the highest score can provide human-readable explanations for a given prediction. However, enumerating and scoring all bounded-depth proof paths for a given goal, as given in Eq. 3, is computationally intractable. For each goal and sub-goal G, this process requires to unify G with the representations of all rule heads and facts in the KB, which quickly becomes computationally prohibitive even for moderately sized KBs. Furthermore, the expansion of a rule like p(X, Y) :q(X, Z), r(Z, Y) via backward chaining causes an increase of the sub-goals to prove, both because all atoms in the body need to be proven, and because Z is a free variable with many possible bindings <ref type="bibr" target="#b39">(Rocktäschel and Riedel 2017)</ref>. We consider two problems -given a sub-goal G such as [p, A, B], we need to efficiently select i) the k f facts that are most likely to prove a sub-goal G, and ii) the k r rules to expand to reach a high-scoring proof state.</p><p>Fact Selection. Unifying a sub-goal G with all facts in the KB K may not be feasible in practice. The number of facts in a real-world KB can be in the order of millions or billions.</p><p>For instance, Freebase contains over 637 × 10 6 facts, while the Google Knowledge Graph contains more than 18 × 10 9 facts <ref type="bibr" target="#b33">(Nickel et al. 2016)</ref>. Identifying the facts F ∈ K that yield the maximum proof score for a sub-goal G reduces to solving the following optimisation problem:</p><formula xml:id="formula_7">ntp K θ (G, 1) = max F :-[]∈K S F ρ = S ρ with S F = unify θ (F, G, (∅, 1))<label>(5)</label></formula><p>Hence, the fact F ∈ K that yields the maximum proof score for a sub-goal G is the fact F that yields the maximum unification score with G. Recall that the unification score between a fact F and a goal G is given by the similarity of their embedding representations θ F: and θ G: , computed via a Gaussian kernel k(θ F , θ G ). Given a goal G, NTPs will compute the unification score between G and every fact F ∈ K in the KB. This is problematic, since computing the similarity between the representations of the goal G and every fact F ∈ K is computationally prohibitive -the number of comparisons is O(|K|n), where n is the number of (sub-)goals in the proving process. However, ntp K θ (G, d) only returns the single largest proof score. This means that, at inference time, we only need the largest proof score for returning the correct output. Similarly, during training, the gradient of the proof score with respect to the parameters θ can also be calculated exactly by using the single largest proof score:</p><formula xml:id="formula_8">∂ntp K θ (G, 1) ρ ∂θ = ∂ max F∈K S F ρ ∂θ = ∂S ρ ∂θ with S ρ = max F∈K S F ρ</formula><p>In this paper, we propose to efficiently compute S , the highest unification score between a given sub-goal G and a fact F ∈ K, by casting it as a Nearest Neighbour Search (NNS) problem. This is feasible since the Gaussian kernel used by NTPs is a monotonic transformation of the negative Euclidean distance.</p><p>Identifying S permits to reduce the number of neural network sub-structures needed for the comparisons between each sub-goal and facts from O(|K|) to O(1). We use the exact and approximate NNS framework proposed by <ref type="bibr" target="#b18">Johnson, Douze, and Jégou (2017)</ref> for efficiently searching K for the best supporting facts for a given sub-goal. Specifically we use the exact L2-nearest neighbour search and, for the sake of efficiency, we update the search index every 10 batches, assuming that the small updates made by stochastic gradient descent do not necessarily invalidate previous search indexes.</p><p>Rule Selection. We use a similar idea for selecting which rules to activate for proving a given goal G. We empirically notice that unifying G with the closest rule heads, such as G = [locatedIn, LONDON, UK] and H = [situatedIn, X, Y], is more likely to generate highscoring proof states. This is a trade-off between symbolic reasoning, where proof paths are expanded only when the heads exactly match with the goals, and differentiable reasoning, where all proof paths are explored.</p><p>This prompted us to implement a heuristic that dynamically selects rules among rules sharing the same template during both inference and learning. In our experiments, this heuristic for selecting proof paths was able to recover valid proofs for a goal when they exist, while drastically reducing the computational complexity of the differentiable proving process.</p><p>More formally, we generate a partitioning P ∈ 2 K of the KB K, where each element in P groups all facts and rules in K sharing the same template, or high-level structure -e.g. an element of P contains all rules with structure θ p: (X, Y) :-θ q: (X, Z), θ r: (Z, Y), with θ p: , θ q: , θ r: ∈ R k . <ref type="foot" target="#foot_3">5</ref>We then redefine the or operator as follows:</p><formula xml:id="formula_9">or K θ (G, d, S) = [S | H :-B ∈ N P (G), P ∈ P, S ∈ and K θ (B, d, unify θ (H, G, S))]</formula><p>where, instead of unifying a sub-goal G with all rule heads, we constrain the unification to only the rules where heads are in the neighbourhood N P (G) of G.</p><p>Learning to Attend Over Predicates. Although NTPs can be used for learning interpretable rules from data, the solution proposed by <ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref> can be quite inefficient, as the number of parameters associated to rules can be quite large. For instance, the rule H :-B, with H = [θ p: , X, Y] and B = [[θ q: , X, Z], [θ r: , Z, Y]], where θ p: , θ q: , θ r: ∈ R k , introduces 3k parameters in the model,</p><p>where k denotes the embedding size, and it may be computationally inefficient to learn each of the embedding vectors if k is large.</p><p>We propose using an attention mechanism (Bahdanau, Cho, and Bengio 2015) for attending over known predicates for defining the rule-predicate embeddings θ p: , θ q: , θ r: . Let R be the set of known predicates, and let R ∈ R |R|×k be a matrix representing the embeddings for the predicates in R. We define θ p: as θ p: = softmax(a p: ) R. where a p: ∈ R |R| is a set of trainable attention weights associated with the predicate p. This sensibly improves the parameter efficiency of the model in cases where the number of known predicates is low, i.e. |R| k, by introducing c|R| parameters for each rule rather than ck, where c is the number of trainable predicate embeddings in the rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jointly Reasoning on Knowledge Bases and Natural Language</head><p>In this section, we show how GNTPs can jointly reason over KBs and natural language corpora. In the following, we assume that our KB K is composed of facts, rules, and textual mentions. A fact is composed of a predicate symbol and a sequence of arguments, e.g. [locationOf, LONDON, UK].</p><p>On the other hand, a mention is a textual pattern between two co-occurring entities in the KB <ref type="bibr" target="#b41">(Toutanova et al. 2015)</ref>, such as "LONDON is located in the UK". We represent mentions jointly with facts and rules in K by considering each textual surface pattern linking two entities as a new predicate, and embedding it in a d-dimensional space by means of an end-to-end differentiable reading component. For instance, the sentence "United Kingdom borders with Ireland" can be translated into the following mention: More formally, given a textual surface pattern t ∈ V * -such as t = [[arg1], borders, with, [arg2]] -the encode θ module first encodes each token w in t by means of a token embedding matrix V ∈ R |V|×k , resulting in a pattern matrix W t ∈ R |t|×k . Then, the module produces a textual surface pattern embedding vector θ t: ∈ R k from W t by means of an end-to-end differentiable encoder. For assessing whether a simple encoder architecture can already provide benefits to the model, we use an encode θ module that aggregates the embeddings of the tokens composing a textual surface pattern via mean pooling:</p><formula xml:id="formula_10">encode θ (t) = 1 |t| w∈t V w• ∈ R k .</formula><p>Albeit the encoder can be implemented by using other differentiable architectures, for this work we opted for a simple but still very effective Bag of Embeddings model <ref type="bibr" target="#b45">(White et al. 2015;</ref><ref type="bibr" target="#b0">Arora, Liang, and Ma 2017)</ref> showing that, even in this case, A related field is differentiable interpreters-program interpreters where declarative or procedural knowledge is compiled into a neural network architecture <ref type="bibr">(Bošnjak et al. 2017;</ref><ref type="bibr" target="#b39">Rocktäschel and Riedel 2017;</ref><ref type="bibr">Evans and Grefenstette 2018)</ref>. This family of models allows imposing strong inductive biases on the models by partially defining the program structure used for constructing the network, e.g., in terms of instruction sets or rules. A major drawback of differentiable interpreters, however, is their computational complexity, so far deeming them unusable except for smaller learning problems. <ref type="bibr" target="#b36">Rae et al. (2016)</ref> use an approximate nearest neighbour data structures for sparsifying read operations in memory networks. <ref type="bibr" target="#b38">Riedel et al. (2013)</ref> pioneered the idea of jointly embedding KB facts and textual mentions in shared embedding space, by considering mentions as additional relations in a KB factorisation setting, and more elaborate mention encoders were investigated by McCallum, Neelakantan, and Verga <ref type="bibr" target="#b5">(2017)</ref>.</p><p>Our work is also related to path encoding models <ref type="bibr" target="#b7">(Das et al. 2017)</ref> and random walk approaches <ref type="bibr" target="#b24">(Lao, Mitchell, and Cohen 2011;</ref><ref type="bibr" target="#b11">Gardner et al. 2014)</ref>, both of which lack a rule induction mechanisms, and to approaches combining observable and latent features of the graph <ref type="bibr" target="#b34">(Nickel, Jiang, and Tresp 2014;</ref><ref type="bibr" target="#b30">Minervini et al. 2016)</ref>. Lastly, our work is related to <ref type="bibr" target="#b47">Yang, Yang, and Cohen (2017)</ref>, a scalable rule induction approach for KB completion, but has not been applied to textual surface patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Datasets and Evaluation Protocols. We report the results of experiments on benchmark datasets -Countries <ref type="bibr" target="#b3">(Bouchard, Singh, and Trouillon 2015)</ref>, Nations, UMLS, and Kinship <ref type="bibr" target="#b21">(Kemp et al. 2006</ref>) -following the same evaluation protocols as <ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref>. Furthermore, since GNTPs allows to experiment on significantly larger datasets, we also report results on the WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref> Baselines. On benchmark datasets, we compare GNTPs with NTPs and two other neuro-symbolic reasoning systems, MINERVA <ref type="bibr" target="#b8">(Das et al. 2018)</ref>, which employs a reinforcement learning algorithm to reach answers by traversing the KB graph, and NeuralLP <ref type="bibr" target="#b47">(Yang, Yang, and Cohen 2017)</ref>, which compiles inference tasks in a sequence of differentiable operations. In addition, we consider DistMult <ref type="bibr" target="#b46">(Yang et al. 2015)</ref> and ComplEx <ref type="bibr" target="#b42">(Trouillon et al. 2016)</ref>, two state-of-the-art black-box neural link predictors suited for large datasets.</p><p>Run-Time Evaluation. To assess the benefits of GNTPs in terms of computational complexity and range of applications, we consider the best hyperparameters we found for the WN18 dataset, and measured the time needed for each training epoch varying the number of unified facts and rules during inference. Results, outlined in Fig. <ref type="figure">2</ref>, show that learning on WN18 quickly becomes infeasible by increasing the number of unified facts and rules. NTPs are a special case of GNTPs where, during the forward pass, there is no pruning of the proof paths.</p><p>From Fig. <ref type="figure">2</ref> we can see that even for KBs a fraction the size of WordNet and Freebase, NTPs rapidly run out of memory, deeming them inapplicable to reasonably sized KBs. Instead, sensible pruning of proof paths in GNTPs drastically increases the efficiency of both the learning and the inference process, allowing to train on large KBs like WordNet. We refer to the Appendix<ref type="foot" target="#foot_4">6</ref> for additional experiments showing run-time improvements by several orders of magnitude.</p><p>Link Prediction Results. We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in <ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref>. Results, presented in Table 1, show that GNTPs achieves better or on-par results in comparison with NTPs and baselines MINERVA <ref type="bibr" target="#b8">(Das et al. 2018)</ref> and NeuralLP <ref type="bibr" target="#b8">(Das et al. 2018)</ref>, consistently through all benchmark datasets. We can also see that models learned by GNTPs are interpretable: in Table <ref type="table" target="#tab_5">1</ref>  Figure <ref type="figure">3</ref>: GNTPs on Countries with generated mentions. We replaced a varying number of relations with textual mentions and integrated them by encoding the mentions using a text encoder (Facts and Mentions) and by simply adding them to the KB (Facts). Two figures contrast the effects of rule learning without attention (left) and with it (right). hand. For instance, we can see that on UMLS, a biomedical KB, the isa and affects relation are transitive.</p><p>Experiments with Generated Mentions. For evaluating different strategies of integrating textual surface patterns, in the form of mentions, in NTPs, we proceeded as follows.</p><p>We replaced a varying number of training set triples from each of the Countries S1-S3 datasets with human-generated textual mentions (for more details, see Appendix). 6 For instance, the fact neighbourOf(UK, IRELAND) may be replaced by the textual mention "UK is neighbouring with IRELAND". The entities UK and IRELAND become the arguments, while the text between them is treated as a new logic predicate, forming a new fact "X is neighbouring with Y"(UK, IRELAND). Then, we evaluate two ways of integrating textual mentions in GNTPs: i) adding them as facts to the KB, and ii) parsing the mention by means of an encoder. The results, presented in Fig. <ref type="figure">3</ref>, show that the proposed encoding module yields consistent improvements of the ranking accuracy in comparison to simply adding the mentions as facts to the KB. This is especially evident in cases where the number of held-out facts is higher, as it is often the case in real-world use cases, where there is an abundance of text but the KBs are sparse and incomplete <ref type="bibr" target="#b33">(Nickel et al. 2016)</ref>. GNTPs are extremely efficient at learning rules involving both logic atoms and textual mentions.</p><p>For instance, by analysing the learned models and their explanations, we can see that GNTPs learn rules such as</p><formula xml:id="formula_11">neighborOf(X, Y) :-"Y is a neighboring state to X"(X, Y) locatedIn(X, Y) :-"X is a neighboring state to Z"(X, Z), "Z is located in Y"(Z, Y)</formula><p>and leverage them during their reasoning process, providing human-readable explanations for a given prediction.</p><p>Table <ref type="table">2</ref>: Link prediction results on the Test-I, Test-II and Test-ALL on FB122. Note that KALE, ASR methods, and KBLR have access to a set of rules provided by <ref type="bibr" target="#b15">Guo et al. (2016)</ref>, while neural link predictors and GNTPs do not. Test-II (6,186 triples) denotes a subset of FB122 that can be inferred via logic rules, while Test-I (5,057 triples) denotes all other test triples. We can see that, even without providing any rule to the model, GNTPs yields better ranking results in comparison with neural link prediction models-since it is able to learn such rules from data-and it is comparable with models that can leverage the provided rules.</p><p>Test-I Test-II Test-ALL Hits@N (%) MRR Hits@N (%) MRR Hits@N (%) MRR 3  Table <ref type="table">2</ref> shows that GNTP, whilst not having access to rules, performs significantly better than neural link predictors, and on-par with methods that have access to all rules. In particular, we can see that on Test-II, a subset of FB122 directly related to logic rules, GNTP yields competitive results. GNTP is able to induce rules relevant for accurate predictions, such as: timeZone(X, Y) :-containedBy(X, Z), timeZone(Z, Y). nearbyAirports(X, Y) :-containedBy(X, Z), contains(Z, Y). children(X, Y) :parents(Y, X). spouse(X, Y) :spouse(Y, X).</p><p>We also evaluate GNTP on WN18 <ref type="bibr" target="#b1">(Bordes et al. 2013) and</ref><ref type="bibr">WN18RR (Dettmers et al. 2018)</ref>. In terms of ranking accuracy, GNTPs is comparable to state-of-the-art models, such as ComplEx and KBLR. In García-Durán and Niepert (2018) authors report a 94.2 MRR for ComplEx and 93.6 MRR for KBLR, while NeuralLP <ref type="bibr" target="#b47">(Yang, Yang, and Cohen 2017)</ref> achieves 94.0, with hits@10 equal to 94.5. GNTP achieves <ref type="bibr">94.2 MRR and 94.31,</ref><ref type="bibr">94.41,</ref><ref type="bibr">94</ref>.51 hits@3, 5, 10, which is on par with state-of-the-art neural link prediction models, while being interpretable via proof paths. Table <ref type="table" target="#tab_7">3</ref> shows an excerpt of validation triples together with their GNTP proof scores and associated proof paths for WN18. On WN18RR, GNTP with MRR of 43.4 performs close to ComplEx (Dettmers et al. 2018) (44.0 MRR) but lags behind <ref type="bibr">NeuralLP (46.3 MRR)</ref>.</p><p>We can see that GNTPs is capable of learning and utilising rules, such as has part(X, Y) :part of(Y, X), and hyponym(X, Y) :hypernym(Y, X). Interestingly, GNTP is able to find non-trivial explanations for a given fact, based on the similarity between entity representations. For instance, it can explain that CONGO is part of AFRICA by leveraging the semantic similarity with AFRICAN COUNTRY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>NTPs combine the strengths of rule-based and neural models but, so far, they were unable to reason over large KBs and natural language. In this paper, we overcome such limitations by considering only the subset of proof paths associated with the largest proof scores during the construction of a dynamic computation graph.</p><p>The proposed model, GNTP, is more computationally efficient by several orders of magnitude, while achieving similar or better predictive performance than NTPs. GNTPs enable end-to-end differentiable reasoning on large KBs and natural language texts, by embedding logic atoms and textual mentions in the same embedding space. Furthermore, GNTPs are interpretable and can provide explanations in terms of logic proofs at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicate Name</head><p>Mentions locatedIn(a, b) a is located in b, a is situated in b, a is placed in b, a is positioned in b, a is sited in b, a is currently in b, a can be found in b, a is still in b, a is localized in b, a is present in b, a is contained in b, a is found in b, a was located in b, a was situated in b, a was placed in b, a was positioned in b, a was sited in b, a was currently in b, a used to be found in b, a was still in b, a was localized in b, a was present in b, a was contained in b, a was found in b neighborOf(a, b) a is adjacent to b, a borders with b, a is butted against b, a neighbours b, a is a neighbor of b, a is a neighboring country of b, a is a neighboring state to b, a was adjacent to b, a borders b, a was butted against b, a neighbours with b, a was a neighbor of b, a was a neighboring country of b, a was a neighboring state to b   Nations and UMLS Furthermore, we consider the Nations, and the Unified Medical Language System (UMLS) datasets <ref type="bibr" target="#b23">(Kok and Domingos 2007)</ref>. UMLS contains 49 predicates, 135 constants and 6529 true facts, while Nations contains 56 binary predicates, 111 unary predicates, 14 constants and 2565 true facts. We follow the protocol used by <ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref> and split every dataset into training, development, and test facts, with a 80%/10%/10% ratio. For evaluation, we take a test fact and corrupt its first and second argument in all possible ways such that the corrupted fact is not in the original KB. Subsequently, we predict a ranking of the test fact and its corruptions to calculate MRR and HITS@m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet and Freebase</head><p>We also evaluate the proposed method on WordNet (WN18) and Freebase (FB122) jointly with the set of rules released by <ref type="bibr" target="#b15">Guo et al. (2016)</ref>. WordNet <ref type="bibr" target="#b29">(Miller 1995</ref>) is a lexical knowledge base for the English language, where entities correspond to word senses, and relationships define lexical relations between them. The WN18 dataset consists of a subset of WordNet, containing 40,943 entities, 18 relation types, and 151,442 triples.</p><p>We also consider WN18RR (Dettmers et al. 2018), a dataset derived from WN18 where predicting missing links is sensibly harder. Freebase (Bollacker, Cook, and Tufts 2007) is a large knowledge graph that stores general facts about the world. The FB122 dataset is a subset of Freebase regarding the topics of people, location and sports, and contains 9,738 entities, 122 relation types, and 112,476 triples.</p><p>For both data sets, we used the fixed training, validation, test sets and rules provided by <ref type="bibr" target="#b15">Guo et al. (2016)</ref>; a subset of the rules is shown in Table <ref type="table" target="#tab_9">5</ref>. Note that a subset of the test triples can be inferred by deductive logic inference.</p><p>For such a reason, following <ref type="bibr" target="#b15">Guo et al. (2016)</ref>, we also partition the test set in two subsets, namely Test-I and Test-II: Test-I contains triples that cannot be inferred by deductive logic inference, while Test-II contains all remaining test triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-Time Performance comparison</head><p>To assess the run-time gains of GNTP, we compare it to NTP with respect to time and memory performance during training.</p><p>In our experiments, we vary the n of the NNS to assess the computational demands by increasing n. First, we compare the average number of examples (queries) per second by running 10 training batches with a maximum batch to fit the memory of NVIDIA GeForce GTX 1080 Ti, for all models. Second, we compare the maximum memory usage of both models on a CPU, over 10 training batches with same batch sizes. The comparison is done on a CPU to ensure that we include the size of the NNS index in GNTP measures and as a fail-safe, in case the model does not fit on the GPU memory.</p><p>The results, presented in Figure <ref type="figure" target="#fig_2">4</ref>, demonstrate that, compared to NTP, GNTP is considerably more time and memory efficiency. In particular, we observe that GNTP yields significant speedups of an order of magnitude for smaller datasets (Countries S1 and S2), and more than two orders of magnitude for larger datasets (Kinship and Nations). Interestingly, with the increased size of the dataset, GNTP consistently achieves higher speedups, when compared to NTP. Similarly, GNTP is more memory efficient, with savings bigger than an order of magnitude, making them readily applicable to larger datasets, even when augmented with textual surface forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters</head><p>For each experiment, the best hyperparameters were selected via cross-validation. We use Adam <ref type="bibr" target="#b22">(Kingma and Ba 2015)</ref> for minimising the loss function in Eq. 4. We searched for the best learning rates in {0.001, 0.005, 0.01, 0.05, 0.1}, for the best L2 regularisation weights in {0.001, 0.0001}. For Freebase and WordNet, we fixed the batch size to 1000, while for Countries, UMLS, Kinship, and Nations we searched the best batch size in {10, 20, 50, 100}. About GNTPs-specific hyperparameters, we searched for the best number of rules k r and facts k f to unify with in {1, 3, 5}.</p><p>Due to time and computational constraints, the embedding size of entities and relation types was set to 100, the number of epochs was also set to 100, while the maximum proof depth d was fixed to 2.</p><p>In all experiments, we observed a quick convergence of the model already in the first 20-30 epochs. On FB122, we found it useful to pre-train rules first (95 epochs), without updating any entity or relation embeddings, and then training the entity embeddings jointly with the rules (5 epochs). This forces GNTPs to learn a good rule-based model of the domain before fine-tuning its representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall architecture of GNTPs. The two main contributions lie in i) the significantly faster inference mechanism, sped up by the k-NN OR component, and ii) the text encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[[[arg1], borders, with, [arg2]], UK, IRELAND], by first identifying sentences or paragraphs containing KB entities, and then considering the textual surface pattern connecting such entities as an extra relation type. While predicates in R are encoded by a look-up operation to a predicate embedding matrix R ∈ R |R|×k , textual surface patterns are encoded by an encode θ : V * → R k module, where V is the vocabulary of words and symbols occurring in textual surface patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Run-time and memory performance of GNTP in comparison with NTP Run-time speedup calculated as the ratio of examples per second of GNTP and NTP. Memory efficiency calculated as a ratio of the memory use of NTP and GNTP. Dashed line denotes equal performance -above it (green) GNTP performs better, below it (red) performs worse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>notable corpus of literature aims at addressing the limitations of neural architectures in terms of generalisation and reasoning abilities. A line of research consists of enriching neural network architectures with a differentiable external memory<ref type="bibr" target="#b40">(Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b13">Graves, Wayne, and Danihelka 2014;</ref><ref type="bibr" target="#b19">Joulin and Mikolov 2015;</ref><ref type="bibr" target="#b14">Grefenstette et al. 2015;</ref><ref type="bibr" target="#b20">Kaiser and Sutskever 2016)</ref>. The underlying idea is that a neural network can learn to represent and manipulate complex data structures, thus disentangling the algorithmic part of the process from the representation of the inputs. By doing so, it becomes possible to train such models from enriched supervision signals, such as from program traces rather than simple input-output pairs.</figDesc><table><row><cell></cell><cell cols="7">Seconds per Epoch Required for Training GNTP on WN18</cell></row><row><cell>f )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 2 3 4 5 Number of Unified Facts (k</cell><cell>184.7 195.5 206.5 216.6 226.6</cell><cell>220.1 240.6 264.8 286.4 311.4</cell><cell>255.3 288.5 322.0 356.5</cell><cell>290.0 332.4 380.2</cell><cell>324.4 381.3</cell><cell>364.5 429.9</cell><cell>396.7 475.7</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell cols="3">3 Number of Unified Rules (k r ) 4 5</cell><cell>6</cell><cell>7</cell></row><row><cell cols="8">Figure 2: Number of seconds per epoch required for training</cell></row><row><cell cols="8">on the WN18 dataset using batches of 1000 examples on a</cell></row><row><cell cols="7">GPU. Missing entries denote out-of-memory errors.</cell><cell></cell></row><row><cell cols="6">the model achieves very accurate results.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Related Work</cell><cell></cell><cell></cell></row></table><note>A</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>we show the decoded rules learned by the model, and learn about the domain at</figDesc><table><row><cell></cell><cell>1.0</cell><cell cols="5">Results on Countries with Textual Mentions (without Attention)</cell><cell></cell><cell>1.0</cell><cell cols="5">Results on Countries with Textual Mentions (with Attention)</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AUC-PR</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7</cell><cell cols="2">Strategy Facts and Mentions Facts Dataset S1 S2 S3</cell><cell></cell><cell></cell><cell></cell><cell>AUC-PR</cell><cell>0.3 0.4 0.5 0.6 0.7</cell><cell cols="2">Strategy Facts and Mentions Facts Dataset S1 S2 S3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400 Held Out Triples 500 600</cell><cell>700</cell><cell>800</cell><cell>900</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400 Held Out Triples 500 600</cell><cell>700</cell><cell>800</cell><cell>900</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Comparison of GNTPs, NTPs, NeuralLP<ref type="bibr" target="#b47">(Yang, Yang, and Cohen 2017)</ref>, and MINERVA<ref type="bibr" target="#b8">(Das et al. 2018</ref>) (from<ref type="bibr" target="#b8">Das et al. (2018)</ref>) on benchmark datasets, with and without attention.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>Metrics</cell><cell>NTP 7</cell><cell cols="2">GNTP</cell><cell>NeuralLP</cell><cell>MINERVA</cell><cell>Rules Learned by GNTP</cell></row><row><cell></cell><cell></cell><cell cols="2">Standard</cell><cell>Attention</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S1</cell><cell></cell><cell cols="6">90.83 ± 15.4 99.98 ± 0.05 100.0 ± 0.0 100.0 ± 0.0 100.0 ± 0.0 locatedIn(X,Y) :-locatedIn(X,Z), locatedIn(Z,Y)</cell></row><row><cell cols="3">Countries 87.40 ± 11.7 90.Kinship AUC-PR S2 MRR 0.35 HITS@1 0.24 HITS@3 0.37</cell><cell>0.719 0.586 0.815</cell><cell>0.759 0.642 0.850</cell><cell>0.619 0.475 0.707</cell><cell>0.720 0.605 0.812</cell><cell>term0(X, Y) :-term0(Y, X) term4(X, Y) :-term4(Y, X) term13(X,Y) :-term13(X, Z), term10(Z, Y)</cell></row><row><cell></cell><cell>HITS@10</cell><cell>0.57</cell><cell>0.958</cell><cell>0.959</cell><cell>0.912</cell><cell>0.924</cell><cell>term2(X,Y) :-term4(X, Z), term7(Z, Y)</cell></row><row><cell></cell><cell>MRR</cell><cell>0.61</cell><cell>0.658</cell><cell>0.645</cell><cell>-</cell><cell>-</cell><cell>commonbloc1(X, Y) :-relngo(Y, X)</cell></row><row><cell>Nations</cell><cell>HITS@1 HITS@3</cell><cell>0.45 0.73</cell><cell>0.493 0.781</cell><cell>0.490 0.736</cell><cell>--</cell><cell>--</cell><cell>timesincewar(X,Y) :-independence(X,Y) unweightedunvote(X,Y) :-relngo(X,Y)</cell></row><row><cell></cell><cell>HITS@10</cell><cell>0.87</cell><cell>0.985</cell><cell>0.975</cell><cell>-</cell><cell>-</cell><cell>ngo(X, Y) :-independence(Y, X)</cell></row><row><cell></cell><cell>MRR</cell><cell>0.80</cell><cell>0.841</cell><cell>0.857</cell><cell>0.778</cell><cell>0.825</cell><cell>isa(X,Y) :-isa(X,Z), isa(Z,Y)</cell></row><row><cell>UMLS</cell><cell>HITS@1 HITS@3</cell><cell>0.70 0.88</cell><cell>0.732 0.941</cell><cell>0.761 0.947</cell><cell>0.643 0.869</cell><cell>0.728 0.900</cell><cell>complicates(X,Y) :-affects(X,Y) affects(X, Y) :-affects(X, Z), affects(Z, Y)</cell></row><row><cell></cell><cell>HITS@10</cell><cell>0.95</cell><cell>0.986</cell><cell>0.983</cell><cell>0.962</cell><cell>0.968</cell><cell>process of(X,Y) :-affects(X,Y)</cell></row></table><note>82 ± 0.88 93.48 ± 3.29 75.1 ± 0.3 92.36 ± 2.41 neighborOf(X,Y) :-neighborOf(X,Z), locatedIn(Z,Y) S3 56.68 ± 17.6 87.70 ± 4.79 91.27 ± 4.02 92.20 ± 0.2 95.10 ± 1.20 neighborOf(X,Y) :-neighborOf(Y,X)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Explanations, in terms of rules and supporting facts, for the queries in the validation set of WN18 provided by GNTPs by looking at the proof paths yielding the largest proof scores.</figDesc><table><row><cell></cell><cell cols="3">Query Score S ρ Proofs / Explanations</cell></row><row><cell></cell><cell></cell><cell>0.995</cell><cell>part of(X, Y) :-has part(Y, X)</cell><cell>has part(AFRICA.N.01, CONGO.N.03)</cell></row><row><cell>WN18</cell><cell>part of(CONGO.N.03, AFRICA.N.01) hyponym(EXTINGUISH.V.04, DECOUPLE.V.03)</cell><cell>0.787 0.987</cell><cell cols="2">part of(X, Y) :-instance hyponym(Y, X) instance hyponym(AFRICAN COUNTRY.N.01, CONGO.N.03) hyponym(X, Y) :-hypernym(Y, X) hypernym(DECOUPLE.V.03, EXTINGUISH.V.04)</cell></row><row><cell></cell><cell>has part(TEXAS.N.01, ODESSA.N.02)</cell><cell>0.961</cell><cell>has part(X, Y) :-part of(Y, X)</cell><cell>part of(ODESSA.N.02, TEXAS.N.01)</cell></row><row><cell cols="2">Results on Freebase and WordNet</cell><cell></cell><cell></cell></row><row><cell cols="4">Link prediction results for FB122 are summarised in Ta-</cell></row><row><cell cols="4">ble 2. The FB122 dataset proposed by Guo et al. (2016) is</cell></row><row><cell cols="4">fairly large scale: it comprises 91,638 triples, 9,738 entities,</cell></row><row><cell cols="4">and 122 relations, as well as 47 rules that can be leveraged</cell></row><row><cell cols="4">by models for link prediction tasks. For such a reason, we</cell></row><row><cell cols="4">consider a series of models that can leverage the presence</cell></row><row><cell cols="4">of such rules, namely KALE (Guo et al. 2016), DistMult</cell></row><row><cell cols="4">and ComplEx using Adversarial Sets (ASR) (Minervini et al.</cell></row><row><cell cols="4">2017)-a method for incorporating rules in neural link pre-</cell></row><row><cell cols="4">dictors via adversarial training-and the recently proposed</cell></row><row><cell cols="4">KBLR (García-Durán and Niepert 2018). Note that, unlike</cell></row><row><cell cols="4">these methods, GNTPs do not have access to such rules and</cell></row><row><cell cols="2">need to learn them from data.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Mentions used for replacing a varying number of training triples in the Countries S1, S2, and S3 datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Examples of the clauses used for Freebase (FB122) and WordNet (WN18)./people/person/languages(X, Z) :-/people/person/nationality(X, Y), /location/country/official language(Y, Z) /location/contains(X, Z) :-/country/administrative divisions(X, Y), /administrative division/capital(Y, Z) /location/location/contains(X, Y) :-/location/country/capital(X, Y)</figDesc><table><row><cell>hyponym(Y, X) :-hypernym(X, Y)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For consistency, we use the same notation as<ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We consider binary predicates, without loss of generality.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Facts are seen as rules with no body and variables, i.e. F :-[].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">Grouping rules with the same structure together makes allows parallel inference to be implemented very efficiently on GPU. This optimisation is also present in<ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">The Appendix can be found at https://github.com/uclnlp/gntp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Results reported in<ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref> were calculated with an incorrect evaluation function, causing artificially better results. We corrected the issues, and recalculated the results.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Datasets</head><p>We run experiments on the following datasets, and report results in terms of Area Under the Precision-Recall Curve <ref type="bibr" target="#b9">(Davis and Goadrich 2006)</ref> (AUC-PR), MRR, and HITS@m <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Countries, UMLS, Nations</head><p>Countries Countries is a dataset introduced by <ref type="bibr" target="#b3">Bouchard, Singh, and Trouillon (2015)</ref> for testing reasoning capabilities of neural link prediction models. It consists of 244 countries, 5 regions (e.g. EUROPE), 23 sub-regions (e.g. WESTERN EU-ROPE, NORTH AMERICA), and 1158 facts about the neighbourhood of countries, and the location of countries and subregions. As in <ref type="bibr" target="#b39">Rocktäschel and Riedel (2017)</ref>, we randomly split countries into a training set of 204 countries (train), a development set of 20 countries (validation), and a test set of 20 countries (test), such that every validation and test country has at least one neighbour in the training set. Subsequently, three different task datasets are created, namely S1, S2, and S3. For all tasks, the goal is to predict locatedIn(c, r) for every test country c and all five regions r, but the access to training atoms in the KB varies. S1: All ground atoms locatedIn(c, r), where c is a test country and r is a region, are removed from the KB. Since information about the sub-region of test countries is still contained in the KB, this task can be solved by using the transitivity rule:</p><p>locatedIn(X, Y) :-locatedIn(X, Z), locatedIn(Z, Y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2:</head><p>In addition to S1, all ground atoms locatedIn(c, s) are removed where c is a test country and s is a sub-region.</p><p>The location of countries in the test set needs to be inferred from the location of its neighbouring countries:</p><p>This task is more difficult than S1, as neighbouring countries might not be in the same region, so the rule above will not always hold. S3: In addition to S2, also all ground atoms locatedIn(c, r) are removed where r is a region and c is a country from the training set training that has a country from the validation or test sets as a neighbour.</p><p>The location of test countries can for instance be inferred using the rule:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Countries with Mentions</head><p>We generated a set of variants of Countries S1, S2, and S3, by randomly replacing a varying number of training set triples with mentions. The employed mentions are outlined in Table <ref type="table">4</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<editor>
			<persName><surname>Iclr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Cook</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Tufts</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2017. 2015. 2007</date>
			<biblScope unit="page" from="1962" to="1963" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wide-coverage semantic analysis with boxer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STEP</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposia</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Programming with a Differentiable Forth Interpreter</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Földiák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kühnberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural-symbolic learning and reasoning: Contributions and challenges</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Iclr. D'avila Garcez</surname></persName>
		</editor>
		<editor>
			<persName><surname>Besold</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2018. 2015</date>
		</imprint>
	</monogr>
	<note>AAAI Spring Symposia</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The relationship between Precision-Recall and ROC curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">;</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<meeting><address><addrLine>Evans, R</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006. 2018. 2006. 2018</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="1" to="64" />
		</imprint>
	</monogr>
	<note>JAIR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">KBlrn: End-to-End Learning of Knowledge Base Representations with Latent, Relational, and Numerical Features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fyodorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Francez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the of the 2nd Workshop on Inference in Computational Semantics. García-Durán, A., and Niepert, M</title>
				<meeting>the of the 2nd Workshop on Inference in Computational Semantics. García-Durán, A., and Niepert, M</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2018</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
	<note>UAI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconciling deep learning with symbolic artificial intelligence: representing objects and relations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Behavioral Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="23" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>CoRR abs/1410.5401</idno>
		<title level="m">Neural Turing Machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Transduce with Unbounded Memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jointly Embedding Knowledge Graphs and Logical Rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<idno>93:1-93:42</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2018. 2016</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
	<note>A Survey of Methods for Explaining Black Box Models</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning with Inconsistent Ontologies</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teije</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billionscale similarity search with gpus</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural GPUs Learn Algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Systems of Concepts with an Infinite Relational Model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistical Predicate Invention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On our best behaviour</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural logic for textual inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-PASCAL@ACL</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Leveraging the schema in latent factor models for knowledge graph completion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fanizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial Sets for Regularising Neural Link Predictors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Towards neural theorem proving at scale</title>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno>CoRR abs/1807.08204</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Review of Relational Machine Learning for Knowledge Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Survey on Open Information Extraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Handschuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scaling memory-augmented neural networks with sparse reads and writes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3621" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic Inductive Logic Programming -Theory and Applications</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">4911</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end Differentiable Proving</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3791" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nlprolog: Reasoning with weak unification for question answering in natural language</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Münchmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6151" to="6161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1502.05698</idno>
		<title level="m">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How well sentence embeddings capture meaning</title>
		<author>
			<persName><forename type="first">L</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADCS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
