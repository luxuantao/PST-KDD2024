<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finite-Time Convergent Recurrent Neural Network with a Hard-Limiting Activation Function for Constrained Optimization with Piecewise-Linear Objective Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
							<email>qsliu@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical and Automation Engineering</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>jwang@mae.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Mechanical and Automation Engi-neering</orgName>
								<orgName type="institution">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finite-Time Convergent Recurrent Neural Network with a Hard-Limiting Activation Function for Constrained Optimization with Piecewise-Linear Objective Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23B8601D9403155B8B4005C662A9DFF5</idno>
					<idno type="DOI">10.1109/TNN.2011.2104979</idno>
					<note type="submission">received September 2, 2010; accepted November 24, 2010. Date of publication March 10, 2011; date of current version April 6, 2011.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Constrained optimization</term>
					<term>convergence in finite time</term>
					<term>global Lyapunov method</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a one-layer recurrent neural network for solving a class of constrained nonsmooth optimization problems with piecewise-linear objective functions. The proposed neural network is guaranteed to be globally convergent in finite time to the optimal solutions under a mild condition on a derived lower bound of a single gain parameter in the model. The number of neurons in the neural network is the same as the number of decision variables of the optimization problem. Compared with existing neural networks for optimization, the proposed neural network has a couple of salient features such as finite-time convergence and a low model complexity. Specific models for two important special cases, namely, linear programming and nonsmooth optimization, are also presented. In addition, applications to the shortest path problem and constrained least absolute deviation problem are discussed with simulation results to demonstrate the effectiveness and characteristics of the proposed neural network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>nonlinear programming based on the penalty method with a finite penalty parameter, which can generate approximate optimal solutions. From then on, many recurrent neural network models have been developed for optimization (see <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> and references therein). Among them, the Lagrange network based on the Lagrangian method was proposed by Zhang and Constantinides <ref type="bibr" target="#b11">[12]</ref> for nonlinear convex programming. The deterministic annealing neural network by Wang <ref type="bibr" target="#b9">[10]</ref> was developed for linear and convex programming. Based on the primal-dual method, Xia <ref type="bibr" target="#b12">[13]</ref> analyzed a primal-dual neural network for linear and quadratic programming. Later on, the projection networks were introduced <ref type="bibr" target="#b13">[14]</ref>, based on the projection method, for linear and nonlinear optimization, which are globally convergent to exact optimal solutions to convex programming problems. In order to reduce the model complexity, the dual, simplified dual, and improved dual neural networks were introduced for solving convex quadratic programming problems with dual variables only <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Forti et al. <ref type="bibr" target="#b0">[1]</ref> investigated the generalized NPC (G-NPC) for nonsmooth optimization, which can be considered as a natural extension of NPC. In <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b16">[17]</ref>, we proposed several onelayer recurrent neural networks for solving linear and quadratic programming problems.</p><p>The finite-time convergence of recurrent neural networks for constrained optimization with their applications has been investigated in the literature. In <ref type="bibr" target="#b17">[18]</ref>, a recurrent neural network described by a sliding-mode dynamics with finite-time global convergence is proposed for solving linear programming problems. In <ref type="bibr" target="#b0">[1]</ref>, to obtain the finite-time convergence of recurrent neural networks for nonsmooth optimization, some sufficient conditions are derived. In particular, for linear programming, if the feasible region is bounded, the convergence of the neural network in <ref type="bibr" target="#b0">[1]</ref> is always in finite time. Moreover, in <ref type="bibr" target="#b18">[19]</ref>, the global convergence in finite time is ensured by suitable conditions of the Łojasiewicz exponent at the equilibrium points. Recently, recurrent neural networks with one neuron and finite-time convergence are proposed for k-winners-takeall operations <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>The existing neural networks for linear and nonlinear programming are mostly constructed based on the penalty parameter or gradient methods with complex dynamics. In this paper, based on the saddle point theorem, a one-layer recurrent neural network with low model complexity is constructed for solving constrained optimization problems. The number of neurons in the neural network is the same as that of decision variables in the concerned optimization problem. Thus the proposed neural network has lower model complexity than most others in the literature, such as the primal-dual neural networks and dual neural networks. The work in this paper can be considered as an extension of that in <ref type="bibr" target="#b0">[1]</ref>. In contrast to the neural networks constructed based on the penalty method and energy function methods in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b21">[22]</ref>, there are two main differences between the neural network of this paper and those in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b21">[22]</ref>. First, finite-time convergence of the present recurrent neural network is guaranteed for an extended class of constrained optimization problems with piecewise-linear objective functions, which includes the linear programming problem as a special case. Second, only a mild condition is required on a single design parameter in the present model over a derived lower bound and no other condition is imposed on the optimization problems (e.g., feasible region could be unbounded).</p><p>The remainder of this paper is organized as follows. In Section II, the problem formulation is presented and the recurrent neural network model is described. The finite-time global convergence of the proposed neural network and its feasibility analysis are presented in Section III. In Section IV, the proposed neural network is tailored for solving linear programming problems and its application to the shortest path problem is presented. Next, in Section V, the proposed neural network is tailored for solving the constrained least absolute deviation problems. Finally, Section VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM FORMULATION AND MODEL DESCRIPTION</head><p>In this section, the optimization problem of interest and the proposed recurrent neural network are described. Consider a constrained optimization problem with a piecewise-linear objective function as follows:</p><formula xml:id="formula_0">minimize f (x) = p i=1 ϕ i n j =1 c i j x j -d i s.t. n j =1 a kj x j ≤ b k , k = 1, 2, . . . , m<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">x = (x 1 , x 2 , . . . , x n ) T ∈ R n , c i j , d i , a kj , b k ∈ R and ϕ i (x) : R n → R is assumed to be piecewise linear as defined below: for l i , h i ∈ R (i = 1, 2, . . . , p) such that ϕ i (v) = h i v, v ≥ 0, l i v, v &lt; 0.<label>(2)</label></formula><p>In <ref type="bibr" target="#b0">(1)</ref>,</p><formula xml:id="formula_2">ϕ i ( n j =1 c i j x j -d i ) (i = 1, 2, . . . , p) can be viewed as the composite function composed of ϕ i (v) and v = n j =1 c i j x j -d i . If l i ≤ h i , and we have ϕ i (v) is convex. Thus the objective function f (x) in (1) is convex if l i ≤ h i .</formula><p>Throughout this paper, we always assume that</p><formula xml:id="formula_3">l i ≤ h i (i = 1, 2, . . . , p). Let C = {c i j } ∈ R p×n , d = (d 1 , d 2 , . . . , d p ) T , A = {a kj } ∈ R m×n , b = (b 1 , b 2 , . . . , b m ) T and ϕ = (ϕ 1 , ϕ 2 , . . . , ϕ p ) T , then<label>(1</label></formula><p>) can be rewritten as the following compact form:</p><formula xml:id="formula_4">minimize f (x) = e T ϕ(Cx -d) s.t. Ax ≤ b (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where e = (1, 1, . . . , 1) T ∈ R p . The optimization problem (1) includes many linear and nonlinear programming problems as its special cases. For example, if p = 1 (i.e., C is a row vector as c ∈ R n ), l = h = 1, and d = 0, (3) reduces to the following linear programming problem:</p><formula xml:id="formula_6">minimize c T x s.t. Ax ≤ b. (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>If l i = -1 and h i = 1 (i = 1, 2, . . . , p), (3) reduces to the constrained least absolute deviation problem (see <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> and references therein) is as follows:</p><formula xml:id="formula_8">minimize Cx -d 1 s.t. Ax ≤ b (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where</p><formula xml:id="formula_10">v 1 = p i=1 |v i | is the L 1 -norm of a vector. If c ii =<label>1</label></formula><p>and c i j = 0 for i = j , then f (x) can be written as the trapezoidal function</p><formula xml:id="formula_11">f (x) = p i=1 c i ϕ i (x i -d i ) (6)</formula><p>by replacing h i and l i with h i /c i and l i /c i , respectively, which has many applications such as wavelet transform. In addition, the piecewise-linear objective function is an interpolated approximation of any nonlinear convex objective function with proper selections of C and d.</p><p>The Lagrangian function of problem ( <ref type="formula" target="#formula_4">3</ref>) is defined as</p><formula xml:id="formula_12">L(x, y, u) = f (x) + y T (Ax -b) (7)</formula><p>where y ∈ R m + = {y ∈ R m : y ≥ 0} are referred to as the Lagrange multipliers. According to the well-known saddle point theorem <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, x * is an optimal solution of problem (3) if and only if there exists y * such that ((x * ) T , (y * ) T ) T is a saddle point of the Lagrangian function, i.e., ((x * ) T , (y * ) T ) T satisfies the following inequalities:</p><formula xml:id="formula_13">L(x * , y) ≤ L(x * , y * ) ≤ L(x, y * ).</formula><p>That is, for any x ∈ R n , and</p><formula xml:id="formula_14">y ∈ R m + f (x * ) + y T (Ax * -b) ≤ f (x * ) + (y * ) T (Ax * -b) ≤ f (x) + (y * ) T (Ax -b). (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>From the left inequality in (8), we have (yy * ) T (Ax *b) ≤ 0 for any y ∈ R m + . By the projection formulation <ref type="bibr" target="#b26">[27]</ref>, it is equivalent to the following equation:</p><formula xml:id="formula_16">y * = (y * + Ax * -b) +<label>(9)</label></formula><p>where u + = (u + 1 , u + 2 , . . . , u + m ) T and u + j = max{u j , 0}( j = 1, 2, . . . , m). Furthermore, (9) is equivalent to</p><formula xml:id="formula_17">y * j ≥ 0, if a j x * -b j = 0 y * j = 0, if a j x * -b j &lt; 0 (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>where a j denotes the j th row of A ( j = 1, 2, . . . , m).</p><p>From the right inequality in (8), we have  Then x * is a minimum point of convex function f (x) + (y * ) T Ax in R n . Thus, x * is a solution of the following inclusion:</p><formula xml:id="formula_19">f (x * ) + (y * ) T Ax * ≤ f (x) + (y * ) T Ax ∀x ∈ R n .</formula><formula xml:id="formula_20">- + + + d p c p1 c pn - - - - 1/ x n ••• ••• ••• ••• ••• . . .</formula><formula xml:id="formula_21">0 ∈ ∂ f (x * ) + A T y *<label>(11)</label></formula><p>where ∂ f is the generalized gradient of f as defined in the Appendix, i.e., ∂ f</p><formula xml:id="formula_22">(x) = C T K [g [l,h] (Cx -d)],</formula><p>where K (•) denotes the closure of the convex hull, the function g</p><formula xml:id="formula_23">[l,h] (v) = (g [l 1 ,h 1 ] (v 1 ), . . . , g [l p ,h p ] (v p )) T with v = (v 1 , v 2 , . . . , v p ) T , l = (l 1 , l 2 , . . . , l p ) T and h = (h 1 , h 2 , . . . , h p ) T ,</formula><p>and its component is defined as</p><formula xml:id="formula_24">g [l i ,h i ] (v i ) = ⎧ ⎪ ⎨ ⎪ ⎩ h i , if v i &gt; 0 [l i , h i ], if v i = 0, (i = 1, 2, . . . , p) l i , if v i &lt; 0. (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>Based on <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref>, the dynamic equation of the proposed recurrent neural network model is described as follows:</p><formula xml:id="formula_26">dx dt = -C T g [l,h] (Cx -d) -σ A T g [0,1] (Ax -b) (13)</formula><p>where is a positive scaling constant, σ is a nonnegative gain parameter, and g [0,1] is the function of an appropriate dimension defined in <ref type="bibr" target="#b11">(12)</ref> with l i = 0 and h i = 1 (i = 1, 2, . . . , m), which is defined as</p><formula xml:id="formula_27">g [0,1] (v) = (g [0,1] (v 1 ), . . . , g [0,1] (v m )) T .</formula><p>The architecture of the neural network ( <ref type="formula">13</ref>) is shown in Fig. <ref type="figure" target="#fig_1">1</ref>, where the neural network has one-layer structure with n neurons. Moreover, although the present neural network model is similar to that in <ref type="bibr" target="#b0">[1]</ref>, here we have extended the neural network with finite-time global convergence for solving a class of constrained optimization problems with piecewiselinear objective functions, and the extra constraints on the feasible region are relaxed, such as the boundedness without empty interior, as will be discussed in the ensuing section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THEORETICAL ANALYSIS</head><p>In this section, we present the theoretical results on the finite-time global convergence of the proposed neural network and guaranteed optimality for problem (3) using the proposed neural network with a sufficiently large value of the gain parameter σ over a derived lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence Analysis</head><p>In this subsection, to study the optimization capability of the neural network <ref type="bibr" target="#b12">(13)</ref>, its convergence property is first investigated by means of the Lyapunov method and nonsmooth analysis (see <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b30">[31]</ref> and references therein). Let us denote M as the optimal solution set of problem (3) and E as the equilibrium point set of the neural network <ref type="bibr" target="#b12">(13)</ref>. Throughout this paper, we always assume that the optimal solution set M is not empty and there exists a finite x * ∈ M.</p><p>Definition 1: x is said to be an equilibrium point of system <ref type="bibr" target="#b12">(13)</ref> </p><formula xml:id="formula_28">if 0 ∈ C T K [g [l,h] (C x -d)] + σ A T K [g [0,1] (A x -b)] i.e., if there exist ξ ∈ K [g [l,h] (C x -d)] and γ ∈ K [g [0,1] (A x - b)] such that C T ξ + σ A T γ = 0. Let ψ(x) = e T ϕ(Cx -d) + σ e T (Ax -b) + , where e = (1, 1, . . . , 1) T ∈ R m . Its generalized gradient is ∂ψ(x) = C T K [g [l,h] (Cx -d)] + σ A T K [g [0,1] (Ax -b)].</formula><p>That is, the neural network in ( <ref type="formula">13</ref>) is actually a negative gradient system of energy function ψ(x). In view of the fact that ψ(x) is convex, the minimum point of ψ(x) corresponds to the equilibrium point of neural network <ref type="bibr" target="#b12">(13)</ref>. Thus, if neural network (13) has a stable equilibrium point, then the minimum point of energy function ψ(x) is guaranteed to be reached. Next, we prove the finite-time global convergence of the proposed neural network as follows.</p><p>Theorem 1: If ψ(x) has a finite minimum, then the state vector of the neural network ( <ref type="formula">13</ref>) is stable in the sense of Lyapunov and globally convergent to an equilibrium point in finite time with any σ ≥ 0.</p><p>Proof: According to the assumption, the equilibrium point set of neural network ( <ref type="formula">13</ref>) is not empty. Denote x as an equilibrium point of neural network <ref type="bibr" target="#b12">(13)</ref>, then it is a minimum point of ψ(x). It follows that 0 ∈ ∂ψ( x).</p><p>Consider the following Lyapunov function:</p><formula xml:id="formula_29">V (x) = (ψ(x) -ψ( x)) + 2 (x -x) T (x -x).<label>(14)</label></formula><p>We have</p><formula xml:id="formula_30">∂ V (x) = (∂ψ(x) + x -x).</formula><p>From the definition of V (x), it follows that V (x) is regular in R n (see Definition 4 in the Appendix). According to the chain rule shown in the Appendix, we have</p><formula xml:id="formula_31">V (x(t)) = ζ(t) T ẋ(t), ∀ζ(t) ∈ ∂ V (x(t)).</formula><p>We choose η ∈ ∂ψ(x), and then</p><formula xml:id="formula_32">ζ(t) = {η(t) + x(t) -x} ∈ ∂ V (x(t)). Since ∂ψ(x) = C T K [g [l,h] (Cx -d)] + σ A T K [g [0,1] (Ax -b)], we have V (x(t)) ≤ sup η∈∂ψ(x) (η + x -x) T {-C T g [l,h] (Cx -d) -σ A T g [0,1] (Ax -b)} ≤ sup η∈∂ψ(x) (η + x -x) T (-η) = -inf η∈∂ψ(x) [ η 2 2 + (x -x) T η]. Since ψ(x) is convex, any η ∈ ∂ψ(x) is monotone. Furthermore, as x is a minimum point of ψ(x), 0 ∈ ∂ψ( x). Thus (x -x) T η ≥ 0 for any η ∈ ∂ψ(x). It follows that V (x(t)) ≤ -inf η∈∂ψ(x) η 2 2 . (<label>15</label></formula><formula xml:id="formula_33">)</formula><p>Then neural network ( <ref type="formula">13</ref>) is stable in the sense of Lyapunov. From the definition of</p><formula xml:id="formula_34">V (x), it follows that V (x) ≥ x - x 2 2 /2. Let L(x 0 ) = {x ∈ R n : V (x) ≤ V (x 0 )}, then for any initial point x 0 ∈ R n , L(x 0 ) is bounded. According to (15),</formula><p>x(t) is also bounded and it follows that the solution x(t) exists on [0, +∞).</p><p>Define (x) = inf η∈∂ψ(x) η 2 2 . If x ∈ E, we have ( x) = 0. Conversely, if there exists x ∈ R n such that ( x) = 0, together with the fact that ∂ψ(x) is a compact convex subset on R n , then there exists η ∈ ∂ψ( x) such that η = 0. Therefore, (x) = 0 if and only if x ∈ E.</p><p>From the boundedness of x(t) and ( <ref type="formula">13</ref>), we get that ẋ(t) 2 is also bounded, denoted by M. Then, there exists an increasing sequence {t k } with lim k→∞ t k = ∞ and a limit point x such that lim k→∞ x(t k ) = x. Similar to the proof in <ref type="bibr" target="#b31">[32]</ref>, we will prove that ( x) = 0. If it does not hold, that is, ( x) &gt; 0, from the definition of (x), it is lower semi-continuous, and then there exist δ &gt; 0 and ω &gt; 0, such that (x) &gt; ω for all</p><formula xml:id="formula_35">x ∈ B( x, δ), where B( x, δ) = {x ∈ R n : x -x 2 ≤ δ} is the δ neighborhood of x. Since lim k→∞ x(t k ) = x, there exists a positive integer N, such that for all k ≥ N, x(t k )-x 2 ≤ δ/2. When t ∈ [t k -δ/(4M), t k + δ/(4M)] and k ≥ N, we have x(t) -x 2 ≤ x(t) -x(t k ) 2 + x(t k ) -x 2 ≤ M|t -t k | + δ 2 ≤ δ. It follows that (x(t)) &gt; ω for all t ∈ [t k -δ/(4M), t k + δ/(4M)]. Since the Lebesgue measure of the set t ∈ k≥N [t k -δ/(4M), t k + δ/(4M)] is infinite, then we have ∞ 0 (x(t))dt = ∞.<label>(16)</label></formula><p>From <ref type="bibr" target="#b14">(15)</ref>, V (x(t)) is monotonically nonincreasing and bounded, and there exists a constant V 0 such that lim t →∞ V (x(t)) = V 0 . We have</p><formula xml:id="formula_36">∞ 0 (x(t))dt = lim s→∞ s 0 (x(t))dt ≤ -lim s→∞ s 0 V (x(t))dt = -lim s→∞ V (x(s)) -V (x(0)) = -V 0 + V (x(0))</formula><p>which contradicts <ref type="bibr" target="#b15">(16)</ref>. Therefore, we have ( x) = 0, and then x ∈ E. That is, the limit point x is an equilibrium point of system <ref type="bibr" target="#b12">(13)</ref>.</p><p>Next, let us define another Lyapunov function</p><formula xml:id="formula_37">Ṽ (x) = (ψ(x) -ψ( x)) + 2 (x -x) T (x -x).</formula><p>Similar to the above proof, we have Ṽ (x(t)) ≥ x -x 2 2 /2 and V (x(t)) ≤ 0. From the continuity of function Ṽ (x(t)), for any ν &gt; 0, there exists τ &gt; 0 such that Ṽ (x(t)) &lt; ν 2 when</p><p>x -x 2 ≤ τ . Since Ṽ (x(t)) is monotonically nonincreasing on interval [0, +∞), there exists a positive integer L, such that, when</p><formula xml:id="formula_38">t ≥ t L x(t) -x 2 2 ≤ 2 Ṽ (x(t)) ≤ 2 Ṽ (x(t L )) &lt; 2ν 2 .</formula><p>That is lim t →+∞ x(t) = x. Since Ṽ (x(t)) ≥ x -x 2 2 /2 is radially unbounded, any state vector of the neural network ( <ref type="formula">13</ref>) is globally convergent to an equilibrium point.</p><p>Finally, inspired by the work in <ref type="bibr" target="#b0">[1]</ref>, we prove that the convergence is in finite time. For a given x ∈ R n , let</p><formula xml:id="formula_39">I -(x) = {i ∈ {1, 2, . . . , p} : c i x -d i &lt; 0}, I 0 (x) = {i ∈ {1, 2, . . . , p} : c i x -d i = 0}, I + (x) = {i ∈ {1, 2, . . . , p} : c i x -d i &gt; 0}, and J -(x) = { j ∈ {1, 2, . . . , m} : a j x -b j &lt; 0}, J 0 (x) = { j ∈ {1, 2, . . . , m} : a j x -b j = 0}, J + (x) = { j ∈ {1, 2, . . . , m} : a j x -b j &gt; 0}</formula><p>, where c i and a j are the i th and j th rows of C and A, respectively, d i and b j are the i th and j th elements of d and b, respectively. Then we have</p><formula xml:id="formula_40">∂ψ(x) = i∈I -(x) l i c T i + i∈I 0 (x) [l i , h i ]c T i + i∈I + (x) h i c T i + σ j ∈J 0 (x) [0, 1]a T j + σ j ∈J + (x) a T j . Assume x(t) / ∈ E, so that 0 / ∈ ∂ψ(x). Since the set ∂ψ(x) is compact and 0 / ∈ ∂ψ(x), we have inf η∈∂ψ(x) η 2 2 = β(I -(x), I 0 (x), I + (x), J 0 (x), J + (x)) &gt; 0.</formula><p>Furthermore, the family of distinct sets of indexes I -(x), I 0 (x), I + (x), J 0 (x), J + (x) obtained by varying x in R n is finite. Then there exists β &gt; 0 such that for any x ∈ R n , we have inf η∈∂ψ(x) η 2 2 ≥ β. From ( <ref type="formula" target="#formula_32">15</ref>), we have</p><formula xml:id="formula_41">V (x(t)) ≤ -β.</formula><p>Integrating both sides of the above inequality, it is easy to</p><formula xml:id="formula_42">verify that V (x(t)) = 0 for t ≥ V (x(0))/β. Since V (x(t)) ≥ x -x 2 2 /2, we have x = x for t ≥ V (x(0))/β. That is, x(t)</formula><p>is globally convergent to an equilibrium point in finite time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feasibility and Optimality Analysis</head><p>In the previous subsection, the proposed neural network is guaranteed to reach an equilibrium point in finite time. To obtain the optimal solution of problem (3), the relationship between the optimal solution of problem (3) and the equilibrium point of neural network <ref type="bibr" target="#b12">(13)</ref> is analyzed in this subsection. In this paper, we define the feasible region where the constraints are satisfied as S = {x ∈ R n : Ax ≤ b}. We denote = {γ :</p><formula xml:id="formula_43">γ = (γ 1 , . . . γ m ) T ∈ R m , 0 ≤ γ ≤ 1 and at least one γ i = 1} and A T = {A T γ : γ ∈ }.</formula><p>Theorem 2: Any equilibrium point of the neural network ( <ref type="formula">13</ref>) is a feasible solution to the optimization problem (3) if</p><formula xml:id="formula_44">σ &gt; max ξ ∈[l, h] C T ξ 2 min A T γ ∈ A T γ 2 (<label>17</label></formula><formula xml:id="formula_45">)</formula><p>where</p><formula xml:id="formula_46">[l, h] = p i=1 [l i , h i ],</formula><p>and is the largest compact set in A T \{0}.</p><p>Proof: Let us define r (x) = e T (Axb) + . According to the chain rule, we have</p><formula xml:id="formula_47">ṙ (x) = ξ T ẋ(t), ∀ξ ∈ ∂r (x(t)).</formula><p>For any x 0 ∈ R n , when x ∈ R n \S, we have</p><formula xml:id="formula_48">ṙ (x) = (A T γ ) T ẋ(t) = γ T A(-C T ξ -σ A T γ ) ≤ A T γ 2 C T ξ 2 -σ A T γ 2 2 = -A T γ 2 (σ A T γ 2 -C T ξ 2 )</formula><p>where</p><formula xml:id="formula_49">ξ ∈ K [g [l,h] (Cx -d)] and γ ∈ K [g [0,1] (Ax -b)]. On one hand, for any x ∈ R n \S and γ ∈ K [g [0,1] (Ax -b)]</formula><p>, from the definition of , A T γ ∈ A T . On the other hand, since A T γ = 0, A T γ belongs to the largest compact set in A T \{0}, i.e., A T γ ∈ . Thus for any x ∈ R n \S and γ ∈ K [g [0,1] (Axb)], we have</p><formula xml:id="formula_50">A T γ 2 ≥ min A T γ ∈ A T γ 2 . Since is compact in R n and 0 / ∈ , it follows that min A T γ ∈ A T γ 2 &gt; 0. Denote α = max ξ ∈[l, h] C T ξ 2 and β = min A T γ ∈ A T γ 2 , where [l, h] = p i=1 [l i , h i ]. Thus, if σ &gt; α/β, we have ṙ (x) ≤ -β(σβ -α) &lt; 0. (<label>18</label></formula><formula xml:id="formula_51">)</formula><p>Integrating ( <ref type="formula" target="#formula_50">18</ref>), we have</p><formula xml:id="formula_52">r (x(t)) ≤ r (x(0)) -β(σβ -α)t.</formula><p>Thus, when t ≥ r (x(0))/(β(σβ -α)), r (x(t)) ≤ 0. That is the state vector of the neural neural <ref type="bibr" target="#b12">(13)</ref> reaches S in finite time.</p><p>Next we prove that, when t ≥ r (x(0))/(β(σβ -α)), the state vector of neural neural (13) remains inside S thereafter.</p><p>If not, we suppose that the trajectory leaves S at time t 1 and stays outside of S for almost all t ∈ (t 1 , t 2 ), where t 1 &lt; t 2 . Then, r (x(t 1 )) = 0, and from the above analysis, r (x(t)) &lt; 0 for almost all t ∈ (t 1 , t 2 ). By the definition of r (x), we have r (x(t)) ≥ 0 for any t ∈ [0, ∞), which contradicts the above result. That is, the state vector of neural neural (13) reaches the feasible region S in finite time and stays there thereafter. Therefore, any equilibrium point of the neural network ( <ref type="formula">13</ref>) is a feasible solution to the optimization problem (3) if <ref type="bibr" target="#b16">(17)</ref> holds.</p><p>Theorem 3: Any equilibrium point of the neural network ( <ref type="formula">13</ref>) is an optimal solution to the optimization problem (3) and vice versa, if <ref type="bibr" target="#b16">(17)</ref> holds.</p><p>Proof: Let x * be an optimal solution of problem <ref type="bibr" target="#b2">(3)</ref>. From <ref type="bibr" target="#b10">(11)</ref>, there exists</p><formula xml:id="formula_53">y * ∈ R m + = {y ∈ R m : y ≥ 0} such that 0 ∈ C T K [g [l,h] (Cx * -d)] + A T y * i.e., there exists ξ * ∈ K [g [l,h] (Cx * -d)], such that C T ξ * + A T y * = 0. (<label>19</label></formula><formula xml:id="formula_54">)</formula><p>If <ref type="bibr" target="#b16">(17)</ref> holds, then</p><formula xml:id="formula_55">A T y * 2 = C T ξ * 2 &lt; σ min A T γ ∈ A T γ 2 = min A T γ ∈ σ A T γ 2 .</formula><p>Then there exists</p><formula xml:id="formula_56">γ * ∈ K [g [0,1] (Ax * -b)] such that y * = σ γ * .</formula><p>Combining with <ref type="bibr" target="#b18">(19)</ref>, we have</p><formula xml:id="formula_57">C T ξ * + σ A T γ * = 0.</formula><p>Thus, x * is an equilibrium point of the neural network <ref type="bibr" target="#b12">(13)</ref>.</p><p>Next, we prove that reverse is true. Let x be an equilibrium point of the neural network <ref type="bibr" target="#b12">(13)</ref>, then there exist</p><formula xml:id="formula_58">ξ ∈ K [g [l,h] (C x -d)] and γ ∈ K [g [0,1] (A x -b)] such that C T ξ + σ A T γ = 0. From Theorem 2, if σ &gt; max ξ ∈[l, h] C T ξ 2 / min A T γ ∈ A T γ 2 ,</formula><p>the equilibrium point of the neural network ( <ref type="formula">13</ref>) is a feasible solution to the optimization problem (3). Thus x ∈ S and A x ≤ b. Then x = ( x1 , x2 , . . . , xn ) T and γ = ( γ1 , γ2 , . . . , γm</p><formula xml:id="formula_59">) T satisfy γi = [0, 1], if a j x -b j = 0 0, if a j x -b j &lt; 0</formula><p>where a j is the j th row of A( j = 1, 2, . . . , m).</p><p>Let ȳ = σ γ , then we have</p><formula xml:id="formula_60">C T ξ + A T ȳ = 0 (<label>20</label></formula><formula xml:id="formula_61">)</formula><formula xml:id="formula_62">and ȳ = ( ȳ + A x -b) + . (<label>21</label></formula><formula xml:id="formula_63">)</formula><p>Thus, from (20), x is a minimum point of function f (x) + ȳT Ax. Then, for any</p><formula xml:id="formula_64">x ∈ R n f ( x) + ȳT A x ≤ f (x) + ȳT Ax. (<label>22</label></formula><formula xml:id="formula_65">)</formula><p>Then, from ( <ref type="formula" target="#formula_62">21</ref>) and the projection formulation <ref type="bibr" target="#b26">[27]</ref>, for any y ∈ R m + , we have</p><formula xml:id="formula_66">(y -ȳ) T (-A x + b) ≥ 0. (<label>23</label></formula><formula xml:id="formula_67">)</formula><p>From ( <ref type="formula" target="#formula_64">22</ref>) and ( <ref type="formula" target="#formula_66">23</ref>), one gets</p><formula xml:id="formula_68">f ( x) + ȳT (A x -b) ≤ f (x) + ȳT (Ax -b) and f ( x) + y T (A x -b) ≤ f ( x) + ( ȳ) T (A x -b).</formula><p>Thus, ( x T , ȳT ) T is a saddle point of the Lagrangian function in <ref type="bibr" target="#b6">(7)</ref>. Therefore, x is an optimal solution of the problem (3).</p><p>According to Theorems 1 and 3, the state vector of the neural network <ref type="bibr" target="#b12">(13)</ref> is globally convergent to an optimal solution of problem (3) in finite time if <ref type="bibr" target="#b16">(17)</ref> holds.</p><p>Remark 1: From the above results, to solve problem (3) using the proposed neural network, it is sufficient to set the gain parameter σ to be sufficiently large. The inequality in <ref type="bibr" target="#b16">(17)</ref> gives a lower bound of σ , which shows that the lower bound usually depends on problem parameters, i.e., C, l, and h in the objective function and A in the constraints. In contrast to the results in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b21">[22]</ref>, some extra conditions are needed to estimate the lower bound of σ , such as bounded feasible region without empty interior, or bounded optimal solution set, which are not always satisfied in optimization. For example, the constrained optimization problems with equality constraints only mostly have an unbounded feasible region and an empty interior. Therefore, the results here are advantageous over those in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b21">[22]</ref> with relaxed conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Estimation of the Lower Bound of σ</head><p>In this subsection, we give the estimation of the lower bound of σ in the following two cases.</p><p>Case I: A is full row-rank, i.e, rank(A) = m.</p><p>In this case, it is easy to calculate the lower bound of σ . In fact, since A is full row-rank, A A T is invertible. On one hand, for any γ ∈ , in which is defined in (17)</p><formula xml:id="formula_69">(A A T ) -1 A A T γ 2 = γ 2 ≥ 1.</formula><p>On the other hand, we have</p><formula xml:id="formula_70">(A A T ) -1 A A T γ 2 ≤ (A A T ) -1 A 2 A T γ 2 .</formula><p>Since A A T is positive definite, we have</p><formula xml:id="formula_71">(A A T ) -1 A 2 = λ max [(A A T ) -1 A((A A T ) -1 A) T ] = λ max [(A A T ) -1 ] = 1 λ min (A A T )</formula><p>where λ max and λ min are the maximum and minimum eigenvalues of the matrix, respectively. Then we have</p><formula xml:id="formula_72">A T γ 2 ≥ λ min (A A T ).</formula><p>Consequently, the lower bound of σ in ( <ref type="formula" target="#formula_44">17</ref>) can be estimated as</p><formula xml:id="formula_73">σ &gt; max ξ ∈[l, h] C T ξ 2 λ min (A A T ) . (<label>24</label></formula><formula xml:id="formula_74">)</formula><p>Case II: A is not full row-rank; i.e, rank(A) = s &lt; m.</p><p>In this case, suppose A = (A T 1 , A T 2 , . . . , A T r ) T , where A 1 is full row-rank, i.e., rank(A 1 ) = s, and A j ( j = 2, 3, . . . , r ) are row vectors. Similar to Case I, for any γ ∈ 1 , where 1 with appropriate dimension has the same definition of defined in above subsection, we have</p><formula xml:id="formula_75">A T 1 γ 2 ≥ λ min (A 1 A T 1</formula><p>). Here we denote ω 1 = 1. For any A j ( j = 2, 3, . . . , r ), there exists a vector q j ∈ R s such that A j = q T j A 1 (in fact, q j = (A 1 A T 1 ) -1 A 1 A T j ). Then for any γ ∈ j , where j with appropriate dimension has the same definition of defined in above subsection, we have</p><formula xml:id="formula_76">(A T 1 , A T j )γ = (A T 1 , A T j )(γ T 1 , γ T j ) T</formula><p>, where γ 1 is the first s elements of γ and γ j is the last one. It follows that</p><formula xml:id="formula_77">(A T 1 , A T j )γ = (A T 1 , (q T j A 1 ) T )(γ T 1 , γ T 2 ) T = A T 1 (I, q j )(γ T 1 , γ T j ) T</formula><p>, where I is an s-dimensional identity matrix. Since (I, q j )(γ T  1 , γ T j ) T = γ 1 + q j γ j and γ is an element of the box set defined as j , it is easy to get the minimum positive value of γ 1 + q j γ j 2 on j , where j is the largest compact set in (I, q j ) j \{0}. We denote the minimum positive value as ω j . Thus for any γ ∈ j , we have (I, q j )γ 2 ≥ ω j . Since A 1 is full row-rank, similar to that in Case I, we have</p><formula xml:id="formula_78">(A T 1 , A T j )γ 2 ≥ λ min (A 1 A T 1 )ω j for j = 2, 3, . . . , r .</formula><p>Then, for any γ ∈ , we have</p><formula xml:id="formula_79">A T γ 2 ≥ λ min (A 1 A T 1 ) min j ω j .</formula><p>Thus, the lower bound of σ in (17) can be estimated as follows:</p><formula xml:id="formula_80">σ &gt; max ξ ∈[l, h] C T ξ 2 λ min (A 1 A T 1 ) min j ω j . (<label>25</label></formula><formula xml:id="formula_81">)</formula><p>The following example illustrates the neural network solution process of a constrained optimization problem with piecewise-linear objective function. Example 1: Let us consider a nonsmooth optimization problem in <ref type="bibr" target="#b2">(3)</ref> </p><formula xml:id="formula_82">with x = (x 1 , x 2 ) T , x 1 + x 2 = 1, h = -l = 1 and C = -1 1 2 -2 4 6 2 -4 4 2 3 -3 T d = -1 -1 -3 3 3 0 T .</formula><p>Fig. <ref type="figure">2</ref> depicts the surface of the objective function in the problem. The above equality constraint can be equivalently converted to two inequality constraints in (3) with</p><formula xml:id="formula_83">A = 1 1 -1 -1 , b = 1 -1 .</formula><p>This problem has a unique optimal solution x * = (1/3, 2/3) T . According to <ref type="bibr" target="#b16">(17)</ref>, the lower bound of σ is estimated as σ &gt; 13.0384. The simulation results are shown in Figs. <ref type="figure">3</ref> and<ref type="figure" target="#fig_3">4</ref> with = 10 -5 . Specifically, Fig. <ref type="figure">3</ref> depicts the phase plot of (x 1 , x 2 ) T from 20 random initial points with σ = 14, which shows that the state variables approach the equality constraint x 1 + x 2 = 1 and then converge to the optimal solution x * . Fig. <ref type="figure" target="#fig_3">4</ref> depicts the convergence of the state variables of the neural network with 10 random initial values. It shows that the state variables of the neural network are globally convergent to the unique optimal solution x * when σ = 10, 14, and 20, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. However, if σ = 6, the neural network converges to an equilibrium point x * = (0.3, 0.6) T , which is not an optimal solution. From Fig. <ref type="figure" target="#fig_3">4</ref>, we can see that the convergence time of the state variables of the neural network is finite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LINEAR PROGRAMMING PROBLEM</head><p>In this section, the proposed neural network in ( <ref type="formula">13</ref>) is tailored for linear programming. Furthermore, a neural network approach to the shortest path problem, as a special case of linear programming problems, is derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Special Case for Linear Programming</head><p>For linear programming (4), the proposed neural network model ( <ref type="formula">13</ref>) can be specifically described as follows:</p><formula xml:id="formula_84">dx dt = -σ A T g [0,1] (Ax -b) -c. (<label>26</label></formula><formula xml:id="formula_85">)</formula><p>Although the neural network in ( <ref type="formula" target="#formula_84">26</ref>) is identical to that proposed in <ref type="bibr" target="#b0">[1]</ref>, the restrictive conditions on the feasible region of the linear programming problems are relaxed here. For many applications, such as the shortest path routing and linear assignment problems, the feasible regions are unbounded. Then the method of this paper is capable of solving more general linear programming problems. For linear programming, the optimal solution can be obtained by the proposed neural network, as stated in the following corollary.</p><p>Corollary 1: For any x 0 ∈ R n , the state vector of the neural network <ref type="bibr" target="#b25">(26)</ref> is stable in the sense of Lyapunov and globally convergent to an optimal solution of problem <ref type="bibr" target="#b3">(4)</ref> </p><formula xml:id="formula_86">in finite time if σ &gt; c 2 min A T γ ∈ A T γ 2</formula><p>where is defined in Theorem 2. Furthermore</p><formula xml:id="formula_87">σ &gt; c 2 λ min (A A T ) if A is of full row-rank.</formula><p>Remark 2: Compared with the neural networks in the literature for linear programming, the proposed neural network has lower model complexity than the most others, such as the Lagrange network <ref type="bibr" target="#b11">[12]</ref>, the primal-dual network <ref type="bibr" target="#b33">[34]</ref>, and the projection network <ref type="bibr" target="#b13">[14]</ref>, which have two-layer structures and n + m neurons. Furthermore, here the finite-time global convergence of the proposed neural network is guaranteed to the optimal solutions of the linear programming problems. Compared to the results in <ref type="bibr" target="#b0">[1]</ref>, the feasible region is restricted to be bounded without empty interior, but here no any extra conditions are imposed on the feasible region. Thus, the results of this paper is capable of solving general linear programming problems. Compared with the neural network model for linear programming proposed in <ref type="bibr" target="#b16">[17]</ref>, if the linear programming problem includes equality constraints, it needs to calculate a matrix inversion in <ref type="bibr" target="#b16">[17]</ref>. In some sense, the calculation of a matrix inversion improves the complexity of the neural network model. Moreover, the feasible region discussed in <ref type="bibr" target="#b16">[17]</ref> is restricted to be bounded, but here the feasible region could be unbounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application for the Shortest Path Problem</head><p>In this subsection, the proposed neural network is utilized for solving the shortest path problem. Consider routing the shortest path from vertex 1 to vertex n in a network with n vertices, and a cost coefficient c i j associated with each edge (i ; j ). Based on the edge path representation <ref type="bibr" target="#b34">[35]</ref>, the primal shortest path problem can be formulated as a linear integer program as follows:</p><formula xml:id="formula_88">minimize n i=1 n j =1 c i j x i j s.t. n k=1 x ik - n l=1 x li = ⎧ ⎪ ⎨ ⎪ ⎩ 1, if i = 1 0, if i = 2, 3, . . . , n -1 -1, if i = n x i j ∈ {0, 1}, i, j = 1, 2, . . . , n</formula><p>where x i j denotes the decision variable associated with the edge from vertices i to j , as defined below</p><formula xml:id="formula_89">x i j =</formula><p>1, if the edge from vertices i to j is in the path; 0, otherwise.</p><p>If an optimal solution exists and it is unique, the above integer program can be reduced to the following linear program <ref type="bibr" target="#b35">[36]</ref>:</p><formula xml:id="formula_90">minimize n i=1 n j =1 c i j x i j s.t. n k=1 x ik - n l=1 x li = δ i1 -δ in x i j ≥ 0, i, j = 1, 2, . . . , n</formula><p>where δ i j is the Kronecker delta function defined as δ i j = 1(i = j ) and δ i j = 0 (i = j ).</p><p>The dual shortest path problem is defined as <ref type="bibr" target="#b36">[37]</ref> maxmize y ny 1 , s.t.</p><formula xml:id="formula_91">y j -y i ≤ c i j , i, j = 1, 2, . . . , n<label>(27)</label></formula><p>where y i denotes the dual decision variable associated with vertex i and y iy 1 is the shortest distance from vertex 1 to vertex i at optimality. Since the objective function and constraints in the dual problem involve variable differences only, an equivalent dual shortest path problem with n -1 variables can be formulated by defining</p><formula xml:id="formula_92">z i = y i -y 1 for i = 1, 2, . . . , n maxmize z n , s.t. z j -z i ≤ c i j , i = j, i, j = 1, 2, . . . , n<label>(28)</label></formula><p>where 1 , and b = (c 12 , . . . , c 1n , c 21 , c 23 , . . . , c 2n , . . . , c n1 , . . . , c n,n-1 ) T ∈ R n(n-1) ; then the linear program in (28) can be written as (4) with x = z and A = (I, M T  1 , M T 2 , . . . , M T n-1 ) T , where I is the (n -1)-dimensional identity matrix and M i (i = 1, 2, . . . , n -1) has the following form:</p><formula xml:id="formula_93">z 1 = 0. Denote z = (z 2 , z 3 , . . . , z n ) T , c = (0, . . . , 0, -1) T ∈ R n-</formula><formula xml:id="formula_94">M i = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 0 • • • 0 -1 0 • • • 0 1 • • • 0 -1 0 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 1 -1 0 • • • 0 0 • • • 0 -1 1 • • • 0 . . . . . . . . . . . . . . . . . . . . . 0 • • • 0 -1 0 • • • 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠</formula><p>in which all elements of the i th column of M i are -1 (i = 1, 2, . . . , n -1).</p><p>The proposed neural network is capable of solving the shortest path problem as specified in the following corollary.</p><p>Corollary 2: The state vector of the neural network ( <ref type="formula" target="#formula_84">26</ref>) is stable in the sense of Lyapunov and globally convergent to the optimal solutions of the dual shortest path problem <ref type="bibr" target="#b27">(28)</ref> in finite time if σ &gt; 1.</p><p>Proof: As c = (0, . . . , 0, -1) T , c 2 = 1. Since rank(A) = n -1, we select the first n -1 rows of A as A 1 , i.e., A 1 = I . Thus λ min (A 1 A T 1 ) = 1. For the other any rows of A, denoted as A j , A j can be written as A j = q T j A 1 and q j has elements of -1, 0 or 1. According to the method introduced in Section III-C, we can get the estimation ω j = 1. Thus this corollary holds from <ref type="bibr" target="#b24">(25)</ref>. The proposed neural network <ref type="bibr" target="#b25">(26)</ref> has a one-layer structure only with n -1 neurons [same as the number of decision variables in the dual shortest path problem <ref type="bibr" target="#b27">(28)</ref>]. Compared to the primal neural networks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> with n 2 neurons and the primal-dual neural network <ref type="bibr" target="#b38">[39]</ref> with n 2 + n neurons, the proposed neural network here has lower model complexity with one order fewer neurons. The proposed neural network has the same model complexity as the dual neural networks <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Nevertheless, the parameter selections for the dual neural networks there are not straightforward, whereas the proposed dual neural network here is guaranteed to converge to exact optimal solutions as long as its gain parameter in the model is larger than one.</p><p>Example 2: Consider a shortest path problem with n being 20. The origin and destination vertices are vertices 1 and 20, respectively. The network topology is shown in Fig. <ref type="figure" target="#fig_4">5</ref>  <ref type="figure"></ref>and<ref type="figure" target="#fig_6">7</ref> show the simulation results of the neural network constructed from the dual linear programming (28) with = 10 -6 and σ = 2. Specifically, Fig. <ref type="figure" target="#fig_5">6</ref> depicts the convergence of the dual variables from a random initial point. Fig. <ref type="figure" target="#fig_6">7</ref> shows the convergence of the dual objective function z n (i.e., the minimum total cost n i=1 n j =1 c i j x i j is 0.9 in the primal linear program) from 10 random initial points. The optimal dual solution needs postprocessing to decode the optimal primal solution in terms of edges <ref type="bibr" target="#b36">[37]</ref>. According to the Complementary Slackness Theorem, given the feasible solutions of x i j and z i to the primal and dual problems, respectively, the solutions are optimal if and only if: 1) x i j = 1 is implied by z jz i = c i j , and 2) z i j = 0 is implied by z jz i &lt; c i j for i, j = 1, 2, . . . , n. In this example, after postprocessing, the optimal primal solution is obtained, which is exactly same as highlighted with solid lines in Fig. <ref type="figure" target="#fig_4">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONSTRAINED LEAST ABSOLUTE DEVIATION PROBLEM</head><p>In this section, the proposed neural network is utilized for solving the constrained least absolute deviation problems.</p><p>A constrained least absolute deviation problem can be written as the following non-smooth optimization problem:</p><formula xml:id="formula_95">minimize Cx -d 1 s.t. Ãx = b<label>(29)</label></formula><p>where C and d are defined in (3), Ã ∈ R m×n and b ∈ R m . In problem (3), let l = -1, h = 1, A = ( ÃT , -ÃT ) T , and b = ( bT , -bT ) T ; then (29) can be equivalently written as <ref type="bibr" target="#b2">(3)</ref>. Thus the proposed neural network is capable of solving the constrained least absolute deviation problem. Corollary 3: For any x 0 ∈ R n , the state vector of the neural network ( <ref type="formula">13</ref>) is stable in the sense of Lyapunov and globally convergent to an optimal solution of problem (29) in finite   </p><formula xml:id="formula_96">if: σ &gt; max ξ ∈[l, h] C T ξ 2 min A T γ ∈ A T γ 2<label>(30)</label></formula><p>where </p><formula xml:id="formula_97">[l, h] = [-1, 1] n and</formula><p>where</p><formula xml:id="formula_99">x = (x 1 , x 2 , x 3 , x 4 ) T and Ã = 2 -1 4 1 2 3 -1 2 , b = 3 -2 .</formula><p>This problem has a unique optimal solution x * = (0, -0.4543, 0.6362, 0) T . Since the feasible region of the problem is unbounded and does not include any interior, the results in <ref type="bibr" target="#b0">[1]</ref> cannot be used for this problem. However, the results of this paper is capable of solving this problem. The lower bound of σ is estimated as σ &gt; 0.4745 according to <ref type="bibr" target="#b29">(30)</ref>. The simulation results are shown in Figs. <ref type="figure" target="#fig_7">8</ref> and<ref type="figure">9</ref> with = 10 -5 , σ = 1, and 10 random initial values. Specifically, Fig. <ref type="figure" target="#fig_7">8</ref> shows the convergence of the state variables of the neural network and Fig. <ref type="figure">9</ref> depicts the convergence of the objective function x 1 . From the simulation results, we can see that the state variables of neural network ( <ref type="formula">13</ref>) is globally convergent to the unique optimal solution in finite time.</p><p>We also give the simulations of ε-insensitive deviation of this problem, i.e., the objective function of ( <ref type="formula" target="#formula_98">31</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presented a one-layer recurrent neural network with a discontinuous hard-limiting activation function for solving a class of constrained optimization problems with piecewise-linear objective functions. The finite-time global convergence of the proposed neural network to optimal solutions can be achieved if its single gain parameter σ is bigger than a derived lower bound. Furthermore, the proposed neural network is tailored for solving several specific constrained optimization problems such as linear programming problems (including the shortest path problem) and nonsmooth optimization problems (including constrained least absolute deviation and ε-insensitive optimization problems). Simulation results were given with numerical examples to illustrate the effectiveness and performance of the proposed neural network. APPENDIX Definition 2: Suppose D ⊂ R n . F : x → F(x) is called a set-valued map from D → R n , if to each point x of a set D, there corresponds to a nonempty closed set F(x) ⊂ R n . Definition 3: A function ϕ : R n → R is said to be Lipschitz near x ∈ R n if there exist ε, δ &gt; 0, such that for any x , x ∈ R n satisfies xx 2 &lt; δ and xx 2 &lt; δ, we have |ϕ(x ) -ϕ(x )| ≤ ε xx 2 . If ϕ is Lipschitz near any point x ∈ R n , then ϕ is also said to be locally Lipschitz in R n .</p><p>Assume that ϕ is Lipschitz near x. The generalized directional derivative of ϕ at x in the direction v ∈ R n is given by ϕ 0 (x; v) = lim sup where K (•) denotes the closure of the convex hull of the corresponding set, N ⊂ R n is an arbitrary set with measure zero, and D ⊂ R n is the set of points where ϕ is not differentiable. Definition 4: A function ϕ : R n → R, which is locally Lipschitz near x ∈ R n , is said to be regular at x if there exists the one-sided directional derivative for any direction v ∈ R n which is given by ϕ (x; v) = lim ξ →0 + ϕ(x + ξv) -ϕ(x) ξ and we have ϕ 0 (x; v) = ϕ (x; v). The function ϕ is said to be regular in R n if it is regular for any x ∈ R n . Regular function is very important in the Lyapunov approach and nonsmooth analysis used in this paper, and has been studied in the literature (see <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b39">[40]</ref> for references). In particular, a nonsmooth convex function on R n is regular at any x ∈ R n . For a finite family of functions ϕ i (i = 1, 2, . . . , n), which are regular at x, we have ∂ where μ(N ) is the Lebesgue measure of set N , B(x, ε) = {y : yx 2 ≤ ε}. A solution of ( <ref type="formula">32</ref>) is an absolutely continuous function x(t) defined in an interval [t 0 , t 1 ](t 0 ≤ t 1 ≤ +∞), which satisfies x(t 0 ) = x 0 and differential inclusion dx dt ∈ φ(x), a.a. t ∈ [t 0 , t 1 ].</p><p>In the regular case, the following chain rule is of key importance in the Lyapunov approach used in this paper.</p><p>Lemma 1 (Chain Rule) <ref type="bibr" target="#b28">[29]</ref>: If V : R n → R is regular at x(t) and x(t) : R → R n is differentiable at t and Lipschitz near t, then d dt V (x(t)) = ξ T ẋ ∀ξ ∈ ∂ V (x(t)).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the proposed neural network in (13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 )Fig. 2 . 2 Fig. 3 .</head><label>2223</label><figDesc>Fig. 2. Objective function in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Transient behaviors of the state variables of the neural network (13) with four different values of σ in Example 1.</figDesc><graphic coords="8,336.71,245.81,179.75,126.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Network topology and the shortest path in Example 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Transient behaviors of the state variables of the neural network (13) in Example 2.</figDesc><graphic coords="9,351.35,58.13,196.06,154.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Global convergence of the dual objective function of the neural network (13) with 10 different initial states in Example 2.</figDesc><graphic coords="10,88.31,58.37,196.06,154.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Transient behaviors of the state variables of the neural network (13) for x 1 in Example 3.</figDesc><graphic coords="10,90.95,294.77,196.06,154.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 Fig. 9 .</head><label>19</label><figDesc>Fig. 9. Convergence of the objective function x 1 in Example 3.</figDesc><graphic coords="10,350.51,58.49,196.06,154.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Transient behaviors of the state variables of the neural network (13) for x ε in Example 3.</figDesc><graphic coords="10,353.75,284.45,196.06,154.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Convergence of the objective function x ε in Example 3.</figDesc><graphic coords="11,89.03,58.37,196.06,154.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) is replaced by x ε , where x ε = 4 i=1 |x i | ε and |x i | ε = max{0, |x i | -ε} with ε &gt; 0. Let ε = 0.01. The simulation results are shown in Figs. 10 and 11 with 10 random initial values. Fig. 10 shows that the state variables of the neural network converge to the unique optimal solution x * = (-0.01, -0.4373, 0.6481, -0.01) T . Fig. 11 depicts the transient value of the objective function x ε .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>+ sv) -ϕ(y) s .The Clarke's generalized gradient of f is defined as∂ϕ(x) = {y ∈ R n : ϕ 0 (x; v) ≥ y T v, ∀v ∈ R n }.When ϕ is locally Lipschitz in R n , ϕ is differentiable for almost all (a.a.) x ∈ R n (in the sense of Lebesgue measure). Then, the Clarke's generalized gradient of ϕ at x ∈ R n is equivalent to∂ϕ(x) = K { lim n→∞ ∇ϕ(x n ) : x n → x, x n / ∈ N , x n / ∈ D}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>n</head><label></label><figDesc>i=1 ϕ i (x) = n i=1 ∂ϕ i (x).Consider the following ordinary differential equation (ODE)dx dt = ψ(x), x(t 0 ) = x 0 . (32)A set-valued map defined asφ(x) = ε&gt;0 μ(N )=0 K [ψ(B(x, ε) -N )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, where the solid lines indicate the shortest path and the dash lines indicate the existing edges. The shortest path of this problem is {n 1 , n 5 , n 7 , n 11 , n 13 , n 17 , n 20 }, i.e., {a 15 , a 57 , a 7,11 , a 11,13 , a 13,17 , a 17,20 }. Figs. 6</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is defined in Theorem 2.</figDesc><table><row><cell cols="3">Example 3: Consider the following constrained least</cell></row><row><cell>absolute deviation problem</cell><cell></cell><cell></cell></row><row><cell>minimize</cell><cell>x 1</cell><cell></cell></row><row><cell>s.t.</cell><cell>Ãx =</cell><cell>b</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region, under Grant CUHK417209E and Grant CUHK417608E.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized neural network for nonsmooth nonlinear programming problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quincampoix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1741" to="1754" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks for nonlinear programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="554" to="562" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network with a discontinuous hard-limiting activation function for quadratic programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="558" to="570" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple &apos;neural&apos; optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="541" />
			<date type="published" when="1986-05">May 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A recurrent neural network for nonlinear convex optimization subject to nonlinear inequality constraints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1385" to="1394" />
			<date type="published" when="2004-07">Jul. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cochocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Unbehauen</surname></persName>
		</author>
		<title level="m">Neural Networks for Optimization and Signal Processing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grasping-force optimization for multifingered robotic hands using a recurrent neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Fok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot. Autom</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="554" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural network for quadratic optimization with bound constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bouzerdoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pattison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="304" />
			<date type="published" when="1993-03">Mar. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linear and quadratic programming neural network analysis</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Maa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shanblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="580" to="594" />
			<date type="published" when="1992-07">Jul. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis and design of a recurrent neural network for linear programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I: Fundam. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="613" to="618" />
			<date type="published" when="1993-09">Sep. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deterministic annealing neural network for convex programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="641" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lagrange programming neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Constantinides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. II: Analog Dig. Signal Process</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="441" to="452" />
			<date type="published" when="1992-07">Jul. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new neural network for solving linear and quadratic programming problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1544" to="1548" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A projection neural network and its application to constrained optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I: Fundam. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="458" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An improved dual neural network for solving a class of quadratic programming problems and its k-winners-take-all application</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2022" to="2031" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simplified dual neural network for quadratic programming with its KWTA application</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1500" to="1510" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network with a discontinuous activation function for linear programming</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1366" to="1383" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An analysis of a class of neural networks for solving linear programming problems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K P</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Zak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1995" to="2006" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convergence of neural networks for programming problems via a nonsmooth Łojasiewicz inequality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quincampoix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1471" to="1486" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A novel recurrent neural network with one neuron and finite-time convergence for k-winners-take-all operation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1140" to="1148" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis and design of a k-winners-take-all model with a single state variable and the heaviside step activation function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1496" to="1506" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Subgradient-based neural networks for nonsmooth convex optimization problems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2391" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for nonsmooth convex optimization subject to linear equality constraints</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf. Neural Inf. Process. Part II</title>
		<title level="s">LNCS</title>
		<meeting>15th Int. Conf. Neural Inf. ess. Part II</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5507</biblScope>
			<biblScope unit="page" from="1003" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constrained least absolute deviation neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="283" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cooperative recurrent neural networks for the constrained L 1 estimator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3192" to="3206" />
			<date type="published" when="2007-07">Jul. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On saddle points and optima for non-smooth and non-convex programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vetrivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="83" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An Introduction to Variational Inequalities and Their Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stampacchia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Academic</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Aubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cellina</surname></persName>
		</author>
		<title level="m">Differential Inclusions: Set-Valued Maps and Viability Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Clarke</surname></persName>
		</author>
		<title level="m">Optimization and Nonsmooth Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized Lyapunov approach for convergence of neural networks with discontinuous or non-Lipschitz activations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grazzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nistri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pancioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="99" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamical behaviors of delayed neural network systems with discontinuous activation functions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="683" to="708" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neural network model for nonsmooth optimization over a compact convex subset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Symp. Neural Netw</title>
		<meeting>3rd Int. Symp. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3971</biblScope>
			<biblScope unit="page" from="344" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robustness of convergence in finite time for linear programming neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Circuit Theory Appl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new neural network for solving linear programming problems and its application</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="525" to="529" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A recurrent neural network for solving the shortest path problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I: Fundam. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="482" to="486" />
			<date type="published" when="1996-06">Jun. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shetty</surname></persName>
		</author>
		<title level="m">Nonlinear Programming: Theory and Algorithms</title>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Primal and dual neural networks for shortest-path routing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., Part A: Syst. Hum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="864" to="869" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Primal and dual assignment networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="784" to="790" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A discrete-time recurrent neural network for shortest-path routing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2129" to="2134" />
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He was a Senior Research Associate in the Department of Manufacturing Engineering and Engineering Management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Filippov</surname></persName>
		</author>
		<author>
			<persName><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Shatin</surname></persName>
		</author>
		<author>
			<persName><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Qingshan Liu (S&apos;07-M&apos;08) received the B.S. degree in mathematics from Anhui Normal University, Wuhu, China, the M.S. degree in applied mathematics from Southeast University, Nanjing, China, and the Ph</title>
		<meeting><address><addrLine>Boston, MA; Kowloon, Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1988">1988. 2001. 2005. 2008. 2008. August 2009 to November 2009. February 2010 to August 2010</date>
		</imprint>
		<respStmt>
			<orgName>Southeast University ; City University of Hong Kong ; Chinese University of Hong Kong</orgName>
		</respStmt>
	</monogr>
	<note>He was a Post-Doctoral Fellow in the Department of Mechanical and Automation Engineering. He is currently an Associate Professor with the School of Automation, Southeast University. His current research interests include optimization theory and applications, artificial neural networks, computational intelligence, and nonlinear systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
