<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive algorithms for sparse system identification $</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-02-24">24 February 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Kalouptsidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics and Telecommunications</orgName>
								<orgName type="institution">University of Athens</orgName>
								<address>
									<addrLine>157 84 Ilissia</addrLine>
									<settlement>Panepistimiopolis, Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gerasimos</forename><surname>Mileounis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics and Telecommunications</orgName>
								<orgName type="institution">University of Athens</orgName>
								<address>
									<addrLine>157 84 Ilissia</addrLine>
									<settlement>Panepistimiopolis, Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Behtash</forename><surname>Babadi</surname></persName>
							<email>behtash@seas.harvard.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vahid</forename><surname>Tarokh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">IEEE Workshop on Statistical Signal Processing</orgName>
								<address>
									<postCode>2009</postCode>
									<settlement>Cardiff</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive algorithms for sparse system identification $</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-02-24">24 February 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">E44163851304F4D804ABF6F5151CA98A</idno>
					<idno type="DOI">10.1016/j.sigpro.2011.02.013</idno>
					<note type="submission">Received 29 November 2009 Received in revised form 7 December 2010 Accepted 18 February 2011 Signal Processing 91 (2011) 1910-1919</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Adaptive estimation Compressive sensing Kalman filtering Expectation-Maximization Volterra series</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, identification of sparse linear and nonlinear systems is considered via compressive sensing methods. Efficient algorithms are developed based on Kalman filtering and Expectation-Maximization. The proposed algorithms are applied to linear and nonlinear channels which are represented by sparse Volterra models and incorporate the effect of power amplifiers. Simulation studies confirm significant performance gains in comparison to conventional non-sparse methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper deals with adaptive identification of sparse systems. Many real-life systems admit sparse representations with few non-zero coefficients. Examples include multipath wireless communication channels where reflections reach the receiver with long delays, imaging, video, etc. <ref type="bibr" target="#b0">[1]</ref>. Many of the above applications require adaptive estimation techniques with minimum computational complexity due to time-varying dynamics and a large number of potential parameters. Wireless communication channels provide a typical representative of the above setup. The wireless channel is described by sparse fading rays and long zero samples and thus admits a sparse representation <ref type="bibr" target="#b1">[2]</ref>. If power amplifiers at the transmitter and receiver ends operate in the linear regime, the channel is represented by a time-varying linear filter whose unit sample response is a sparse vector. Adaptive algorithms such as the least mean squares (LMS), the recursive least squares (RLS) and fast variants thereof have been widely applied for estimation/equalization of such channels.</p><p>Recently, compressive sensing methods have been developed for the estimation of the multipath channels taking into account the sparseness characteristic <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, two different sparsity constraints are incorporated into the quadratic cost function of the LMS algorithm, to take into account the sparse channel coefficient vector. An alternative viewpoint to sparse LMS is the proportionate normalized LMS and its variants <ref type="bibr" target="#b7">[8]</ref>, which assign different step sizes to different coefficients based on their optimal magnitudes. An ' 1 -regularized RLS type algorithm based on a low complexity Expectation-Maximization is derived in <ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b4">[5]</ref> the weighted least absolute shrinkage and selection operator (LASSO) estimates are retrieved recursively using a system of normal equations or via iterative subgradient methods. Previously reported algorithms for sparse system identification using Kalman filtering rely on the Dantzig selector <ref type="bibr" target="#b5">[6]</ref> and on the pseudo-measurement technique <ref type="bibr" target="#b6">[7]</ref>. In <ref type="bibr" target="#b5">[6]</ref> the proposed algorithm is not exclusively based on Kalman filtering as it requires the implementation of an additional optimization algorithm (the Dantzig selector), which leads to increased complexity and execution time. The sparse Kalman filtering approach in <ref type="bibr" target="#b6">[7]</ref>, first performs a Kalman iteration then it generates a fictitious observation from the ' 1 -regularization constraint and carries out a Kalman filter pseudo-measurement update. The computational cost of the Kalman filter pseudo-measurement update is avoided by the proposed technique.</p><p>Very often power amplifiers located at an access point of a downlink channel (e.g., base stations in cellular systems and repeaters for satellite links) operate close to saturation in order to achieve power efficiency. In such cases nonlinearities cannot be ignored without significant loss of performance. A widely used method to describe nonlinear systems is based on Volterra models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Chapter 14]</ref>. Volterra models employ a large number of parameters which increases exponentially with the order of nonlinearity. Therefore, there is a strong need to reduce the number of terms by only considering those terms that actually contribute to the output. All previous algorithms for estimating Volterra kernels treat each kernel equally and identify the complete set of kernels. The major drawback of estimating the complete set of kernels is the large computational/implementation cost. It is shown in this paper that significant performance gains are achieved if insignificant kernels are ignored.</p><p>Adaptive identification of sparse nonlinear systems described by finite Volterra models is considered in this paper. Adaptation is carried out by recursive algorithms that combine Expectation-Maximization and Kalman filtering. The expectation step is carried out by Kalman filtering while the maximization step corresponds to a soft-thresholding function due to the ' 1 -regularization. This algorithmic separation and the particular form of the assumed parameter dynamics, enables us to develop several sparse variants including RLS, LMS and fast RLS schemes. The proposed algorithms are applied to two channel settings. The first setting takes into account the power amplifier (PA) dynamic nonlinearities but ignores multipath fading. The second setting includes fading as well. The overall structure retains its sparse characteristics.</p><p>The rest of this paper is organized as follows. Sparse problem formulation is addressed in Section 2. The proposed algorithms for adaptive tracking of sparse systems are given in Section 3. In Section 4 two important sparse nonlinear channels are discussed. Simulations results are presented in Section 5. Conclusions are discussed in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sparse problem formulation</head><p>Consider the problem of estimating a sparse vector, h, from noisy linear measurements of the form</p><formula xml:id="formula_0">yðiÞ ¼ x T ðiÞh þ vðiÞ, 1 ri rn<label>ð1Þ</label></formula><p>where y(i) and xðiÞ are the system output and input (at time i) and v(i) is zero-mean, independent and identically distributed (i.i.d.) Gaussian noise (of variance s 2 v ). <ref type="foot" target="#foot_0">1</ref> The complex parameter vector h 2 C M is said to be s-sparse if it has s or fewer non-zero elements and s 5 M. Let suppðhÞ denote the support set of h, suppðhÞ ¼ fj : h j a0g, namely the set of non-zero coefficients. The number of elements of the support of h defines the ' 0 quasi-norm of h. Then h is s-sparse if JhJ ' 0 r s5 M.</p><p>Collecting n successive samples from the above equation into a column format results in the following system of linear equations:</p><formula xml:id="formula_1">yðnÞ ¼ XðnÞh þvðnÞ ð<label>2Þ</label></formula><p>where yðnÞ ¼ ½yð1Þ, . . . ,yðnÞ T , XðnÞ ¼ ½x T ð1Þ, . . . ,x T ðnÞ T , vðnÞ ¼ ½vð1Þ, . . . ,vðnÞ T . Eq. ( <ref type="formula" target="#formula_1">2</ref>) provides a noisy representation of a block of successive received samples in terms of the columns of XðnÞ 2 C nÂM also referred to as dictionary.</p><p>The basic problem we need to consider in order to obtain the sparsest estimate of h is the following:</p><formula xml:id="formula_2">min h JhJ ' 0 subject to JyðnÞÀXðnÞhJ ' 2 r d ðP ' 0 Þ</formula><p>where the error tolerance d can be interpreted as a noise removal mechanism when JvðnÞJ ' 2 r d <ref type="bibr" target="#b0">[1]</ref>. The above is a classic problem of combinatorial search, and requires searching through all ð M s Þ possible subsets. The computational complexity of ðP ' 0 Þ is exponential in M and the problem is NP-hard in general <ref type="bibr" target="#b9">[10]</ref>.</p><p>A common approach in the literature to solve problem ðP ' 0 Þ is to replace the ' 0 quasi-norm, by its best convex approximation, the ' 1 -norm, referred to as convex relaxation. Hence, ðP ' 0 Þ is replaced by the following convex optimization problem:</p><formula xml:id="formula_3">min h 1 2 JyðnÞÀXðnÞhJ 2 ' 2 þ gJhJ ' 1 ðP ' 1 Þ</formula><p>The scalar parameter g provides a tradeoff between sparsity and total squared error. A large value of g gives a sparser solution to the problem whereas a small value fits the data well but leads to less sparse solutions. Since ðP ' 1 Þ is convex, many standard ''off the shelf'' optimization tools, like linear programming <ref type="bibr" target="#b10">[11]</ref>, projected gradient methods <ref type="bibr" target="#b11">[12]</ref>, and iterative thresholding <ref type="bibr" target="#b12">[13]</ref> can be employed. Techniques based on convex optimization although quite efficient, suffer from the polynomial runtime and operate in a batch mode (all information are a priori required). Quite often in applications like communications, the measurements arrive sequentially and in many cases the system response is time-varying. In such cases the batch estimators have the disadvantage of requiring greater runtime complexity and larger memory requirements. These challenges may be addressed by using sparse adaptive filters <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, which have the potential of allowing online operation.</p><p>The optimization problem ðP ' 1 Þ is known in the compressed sensing literature as basis pursuit denoising <ref type="bibr" target="#b10">[11]</ref> or LASSO in the context of linear regression <ref type="bibr" target="#b13">[14]</ref>. It has been shown in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> that both techniques achieve performance close to the genie-aided estimator, which knows the locations of the non-zero components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive algorithms</head><p>In this section, we propose a family of sparse adaptive algorithms. Generalizing the stationary environment described by Eq. (2), we now consider time-varying parameters. Let hðnÞ denote the parameter vector at time n. Motivated by the adaptive filtering practice <ref type="bibr">[16, p. 60; 17, p. 272]</ref> we shall assume that the parameter dynamics are governed by the first-order model</p><formula xml:id="formula_4">hðnÞ ¼ hðnÀ1Þþq jL 0 ðnÞ ¼ h 0 þ X n i ¼ 1 q jL 0 ðiÞ, h 0 $ N ðh 0 ,s 2 0 I jL 0 Þ<label>ð3Þ</label></formula><p>The initial parameter h 0 is normally distributed with mean h 0 and uncorrelated coefficients with variance s 2 0 . L 0 denotes the true support set of h 0 (which is unknown), i.e. the set of the non-zero coefficients of h 0 . The noise term qðnÞ is zero outside L 0 ðq i ðnÞa0 iff i 2 L 0 Þ and zeromean Gaussian inside L 0 with diagonal covariance matrix R jL 0 ðnÞ having diagonal elements s 2 q 1 ðnÞ, . . . ,s 2 q d ðnÞ, where d is the ' 0 -norm of h 0 . The variances fs 2 q i ðnÞg d i ¼ 1 are in general allowed to vary with time. The stochastic processes vðnÞ, qðnÞ and the random variable h 0 are mutually independent.</p><p>Instead of the mathematical program ðP ' 1 Þ we focus on maximum likelihood (ML) problem augmented by an ' 1 -penalty, i.e.</p><formula xml:id="formula_5">ĥML ¼ argmax h flogpðyðnÞ; hÞÀgJhJ ' 1 g ð 4Þ</formula><p>where h ¼ h 0 is the vector of unknown parameters. Parameter estimation is carried out by maximizing the penalized log-likelihood, which in many cases is complex and is either difficult or impossible to compute directly or optimize. In such cases, the Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> can be used to maximize logpðyðnÞ; hÞÀgJhJ ' 1 , without explicitly computing it.</p><p>Although the EM algorithm is a framework of iterative algorithms, the derived adaptive algorithms employ only one iteration per time update for computational and storage efficiency <ref type="bibr" target="#b18">[19]</ref>. This is highly desirable in timevarying systems, where the algorithm is expected to track system variations.</p><p>In order to apply the Expectation-Maximization algorithm we have to specify the complete and incomplete data. The vector hðnÞ at time n is taken to represent the complete data vector, whereas yðnÀ1Þ accounts for the incomplete data <ref type="bibr">[19, pp. 31-33]</ref>. The EM algorithm iterates between estimating the conditional expectation of the complete data given the incomplete data and a current estimate of the parameters (the E-step), and maximizing the conditional expectation of the complete log-likelihood minus an ' 1 -penalty (the M-step). The two steps are summarized by the following equation:</p><formula xml:id="formula_6">ĥðnÞ ¼ argmax h fE pðhðnÞjyðnÀ1Þ; ĥðnÀ1ÞÞ ½logpðhðnÞ; hÞÀgJhJ ' 1 g<label>ð5Þ</label></formula><p>The EM algorithm aims to maximize the log-likelihood of the complete data, logpðhðnÞ; hÞ. However, because hðnÞ is a hidden variable, we maximize instead its expectation given the incomplete data yðnÀ1Þ and a current estimate of the parameters ĥðnÀ1Þ</p><p>The analysis starts from the likelihood pðhðnÞ; hÞ, which follows a multivariate Gaussian distribution, given by pðhðnÞ; hÞ ¼ ð2pÞ ÀM=2  ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi detRðnÞ p exp À 1 2</p><formula xml:id="formula_7">ðhðnÞÀhÞ H R À1 ðnÞðhðnÞÀhÞ<label>ð6Þ</label></formula><p>where h is the (unconditional) mean of hðnÞ, h ¼ h 0 and RðnÞ ¼ E½ðhðnÞÀhÞðhðnÞÀhÞ H is the diagonal covariance matrix of the complete data at time n. Now, we take conditional expectation of Eq. ( <ref type="formula" target="#formula_7">6</ref>) with respect to the observed data yðnÀ1Þ given the parameter estimate ĥðnÀ1Þ. This gives us the Expectation step of the EM algorithm E-step : Q ðh, ĥðnÀ1ÞÞ ¼ E pðhðnÞjyðnÀ1Þ; ĥðnÀ1ÞÞ ½logpðhðnÞ; hÞ</p><formula xml:id="formula_8">¼ ÀðM=2Þlog2pþ 1=2h H R À1 ðnÞE½hðnÞjyðnÀ1Þ; ĥðnÀ1Þ ð8Þ þ 1=2E½h H ðnÞjyðnÀ1Þ; ĥðnÀ1ÞR À1 ðnÞhÀ1=2h H R À1 ðnÞh À1=2 tr½R À1 ðnÞE½hðnÞh H ðnÞjyðnÀ1Þ; ĥðnÀ1ÞÀ1=2logðjRðnÞjÞ ¼ constant þ h H R À1 ðnÞE½hðnÞjyðnÀ1Þ; ĥðnÀ1ÞÀ 1 2 h H R À1 ðnÞh<label>ð7Þ</label></formula><p>where the constant incorporates all terms that do not involve h and hence do not affect the maximization. The Maximization step of the EM algorithm, described below, calculates the maximum of the penalized Q-function</p><formula xml:id="formula_9">M-step : ĥðnÞ ¼ argmax h fQ ðh, ĥðnÀ1ÞÞÀgJhJ '1 g ð<label>9Þ</label></formula><p>which in turn leads to the component-wise soft-thresholding function, given by</p><formula xml:id="formula_10">ĥi ðnÞ ¼ sgnðw i ðnÞÞ½jw i ðnÞjÀgR ii ðnÞ þ<label>ð10Þ</label></formula><p>where wðnÞ :¼ E½hðnÞjyðnÀ1Þ; ĥðnÀ1Þ, the signum function sgnðzÞ is zero only when z= 0 in all other cases sgnðzÞ ¼ z=jzj for any z 2 C and ðzÞ þ ¼ maxðRðzÞ,0ÞþjmaxðIðzÞ,0Þ.</p><p>This operation shrinks coefficients above the threshold in magnitude value. The derivation can be found in Appendix.</p><p>In order to carry out the conditional expectation of Eq. ( <ref type="formula" target="#formula_8">7</ref>) (essentially the E-step), one needs to assume a prior on h (n) given the post observations y ( n À1) and ĥ ( n À 1). It is well known that this conditional expectation may be obtained recursively using the Kalman filter, if a Gaussian prior is assumed on h (n) given the past observation. The Kalman filter then determines the posterior probability density function for h (n) recursively over time. Consider the Gaussian prior of the form: Prior ¼ pðhÞðnÞjyðnÀ1Þ; ĥðnÀ1ÞÞ % N ð ĥðnÀ1Þ, PðnÀ1ÞþRðnÞÞ: </p><formula xml:id="formula_11">Given</formula><formula xml:id="formula_12">R ii ðnÞ ¼ s 2 0 þ X n t ¼ 1 s 2 q i ðtÞ % P ii ðnÀ1ÞþR ii ðnÞ ð<label>12Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">EM-Kalman filter</head><p>The method outlined above is named EM-Kalman filter and is summarized in Table <ref type="table">1</ref>(a). For notational clarity, we replace ĥ (n) by h(n) in the resulting algorithm. The Kalman filter computes wðnÞ under the assumption that the variances s 2 v and fs 2 q i ðnÞg d i are known. The noise variances can be estimated in various ways. One method is to use the Maximum Likelihood estimates. These estimates can be obtained by maximizing the Q-function as is done in <ref type="bibr" target="#b20">[21]</ref> </p><formula xml:id="formula_13">s 2 v ¼ 1 n X n t ¼ 1 JyðtÞÀx T ðtÞ ĥðtÞJ 2 ' 2<label>ð13Þ</label></formula><formula xml:id="formula_14">s 2 q i ¼ 1 nÀ1 X n t ¼ 2 J ĥi ðtÞÀ ĥi ðtÀ1ÞJ 2 ' 2<label>ð14Þ</label></formula><p>Alternatively, under the assumption that the state noise covariance satisfies R jL 0 ðnÞ ¼ rðnÞI, both noise disturbances can be estimated adaptively. A smoothed estimate of the state and observation noise can be respectively obtained according to steps 5 and 6 of Table <ref type="table">1</ref></p><formula xml:id="formula_15">(a),</formula><p>where a is a smoothing parameter and RðxÞ is the ramp function (R(x) = x if x Z0 and 0 otherwise). These two methods for online estimation of the noise disturbances are due to Jazwinski <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">EM-RLS filter</head><p>The recursive procedure for the determination of the Kalman filter in the special case of the time-varying random walk model equation ( <ref type="formula" target="#formula_4">3</ref>), resembles the RLS algorithm. In fact, the RLS can be viewed as a special form of Table <ref type="table">1</ref>(a) which provides an alternative for the estimation of the noise variances <ref type="bibr">[16, p. 63]</ref>.</p><p>Indeed the RLS results when the weighted prediction error</p><formula xml:id="formula_16">JðhÞ ¼ X n i ¼ 1</formula><p>bðn,iÞjeðiÞj 2 is minimized. The weighted factor bðn,iÞ (with 0 rbðn,iÞ o 1) is used to discount data and track system changes by allowing recent data to be more heavily weighted. A common choice of bðn,iÞ is the exponentially decaying function bðn,iÞ ¼ l nÀi .</p><p>The RLS filter is given by steps 1-3 of Table </p><formula xml:id="formula_17">s 2 v ¼ l, RðnÞ ¼ ðl À1 À1ÞPðnÀ1Þ ð 15Þ</formula><p>The complete algorithm is presented in Table <ref type="table">1</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fast implementations</head><p>The RLS algorithm of Table <ref type="table">1</ref>(b) admits a fast realization referred to as fast RLS (FRLS) <ref type="bibr" target="#b22">[23]</ref>. FRLS updates the gain with OðMÞ operations utilizing the low displacement rank structure of the covariance matrix. More precisely, the fast RLS reduces the complexity from OðM 2 Þ to OðMÞ. This is achieved by exploiting the shift-invariance structure of the covariance matrix <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">Chapter 38]</ref>. A similar fast realization of the EM-Kalman algorithm can be developed due to the special structure of the state covariance matrix. This algorithm is summarized in Table <ref type="table">2</ref>. Fast implementations tend to suffer from numerical instabilities when implemented in finite precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Algorithms for sparse adaptive tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) EM-Kalman</head><p>Initialization:</p><formula xml:id="formula_18">h 0 ¼ 0, P 0 ¼ d À1 I with d ¼ const.</formula><p>For n :¼ 1,2, . . . do 1:</p><formula xml:id="formula_19">kðnÞ ¼ PðnÀ1Þx Ã ðnÞ s 2 v þ x T ðnÞPðnÀ1Þx Ã ðnÞ 2:</formula><p>wðnÞ ¼ hðnÀ1ÞþkðnÞeðnÞ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>PðnÞ ¼ PðnÀ1ÞþrðnÞIÀkðnÞx T ðnÞPðnÀ1Þ 4:</p><formula xml:id="formula_20">s 2 eðnÞ ¼ s 2 v ðnÞþx T ðnÞPðnÀ1Þx Ã<label>ðnÞ</label></formula><p>5:</p><formula xml:id="formula_21">rðnÞ ¼ arðnÀ1Þþð1ÀaÞR jeðnÞj 2 Às 2 eðnÞ x Ã ðnÞxðnÞ ! 6: s 2 v ¼ as 2 v þð1ÀaÞRðjeðnÞj 2 Àx T ðnÞPðnÀ1Þx Ã ðnÞÞ 7: hðnÞ ¼ sgnðwðnÞÞ½jwðnÞjÀgðP ii ðnÀ1ÞþrðnÞÞ1 þ end For (b) EM-RLS Initialization: h 0 ¼ 0, P 0 ¼ d À1 I with d ¼ const.</formula><p>For n :¼ 1,2, . . . do 1:</p><formula xml:id="formula_22">kðnÞ ¼ PðnÀ1Þx Ã ðnÞ lþx T ðnÞPðnÀ1Þx Ã<label>ðnÞ</label></formula><formula xml:id="formula_23">2:</formula><p>w n ¼ hðnÀ1ÞþkðnÞeðnÞ</p><p>3:</p><formula xml:id="formula_24">PðnÞ ¼ l À1 PðnÀ1ÞÀl À1 kðnÞx T<label>ðnÞPðnÀ1Þ</label></formula><p>4:</p><formula xml:id="formula_25">hðnÞ ¼ sgnðwðnÞÞ½jwðnÞjÀgl À1 P ii ðnÀ1Þ1 þ end For (c) EM-LMS Initialization: h 0 ¼ 0, 0 o mo2l À1 max</formula><p>For n :¼ 1,2, . . . do 1a: hðnÞ ¼ hðnÀ1ÞÀg sgnhðnÀ1Þþmx Ã ðnÞeðnÞ Alternatively step 1a, can be executed as follows 1b:</p><formula xml:id="formula_26">hðnÞ ¼ hðnÀ1ÞÀg sgn<label>hðnÀ1Þ</label></formula><formula xml:id="formula_27">1 þ ejhðnÀ1Þj þ mx Ã ðnÞeðnÞ end For</formula><p>The signum function (sgn(.)) of a vector is the vector of the signum of its component. 2 An alternative formulation would be to consider as h the realized value of the state vector h(n À 1), then the resulting algorithm (would have the form of Table A stable version <ref type="bibr" target="#b23">[24]</ref> introduces redundancy into the computation of the a priori backward prediction error</p><formula xml:id="formula_28">ẽb M ðnÞ ¼ lE b ðnÀ1Þk b ðnÞ êb M ðnÞ ¼ xðnÀMÞþb H M<label>ðnÞxðnÞ</label></formula><p>The rescue mechanism incorporates the evaluation of e b M (n) by a suitable combination of both of these expressions. The difference between ẽb M ðnÞ, êb M ðnÞ is fed back, scaled by a certain gain t</p><formula xml:id="formula_29">e b M ðnÞ ¼ ẽb M ðnÞþt½ êb M ðnÞÀ ẽb M<label>ðnÞ</label></formula><p>The tuning parameter t is experimentally chosen to ensure numerical stability. In addition, the value of l, should be within the range l 2 ð1À1=3M,1À1=10MÞ. The fast implementation cannot be used directly for the Volterra models given by Eqs. ( <ref type="formula" target="#formula_50">22</ref>) and ( <ref type="formula" target="#formula_51">23</ref>) below. These models require a multichannel filtering reformulation <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The LMS family</head><p>For the purposes of simulations presented in the next section, we discuss the LMS variant developed in <ref type="bibr" target="#b3">[4]</ref>. LMS updates a convex function of the prediction error signal eðnÞ plus an ' 1 -penalty</p><formula xml:id="formula_30">JðhðnÞÞ ¼ 1 2 jeðnÞj 2 þ g 0 JhðnÞJ '1 ¼ 1 2 jyðnÞÀx T ðnÞhðnÞj 2 þ g 0 JhðnÞJ '1<label>ð16Þ</label></formula><p>The regularized cost function of Eq. ( <ref type="formula" target="#formula_30">16</ref>), consists of two parts, the instantaneous squared error (of the standard LMS filter) and the ' 1 -penalty which has the potential of finding sparse estimates. At each time step the parameter vector is updated using a gradient-descent algorithm that minimizes JðhðnÞÞ. The update equation which minimizes Eq. ( <ref type="formula" target="#formula_30">16</ref>) is given in step 1a of Table <ref type="table">1</ref>(c). The step-size m controls the rate of convergence of the algorithm to the optimum solution and g ¼ mg 0 <ref type="bibr" target="#b2">[3]</ref>.</p><p>Note that the normalized variant of the above algorithm is a special case of the EM-Kalman filter <ref type="bibr">[16, p. 64]</ref> of Table <ref type="table">1</ref>(a) by setting:</p><formula xml:id="formula_31">P 0 ¼ mI, s 2 v ¼ 1, kðnÞ ¼ mx Ã ðnÞ 1 þ mjxðnÞj 2 , RðnÞ ¼ mxðnÞkðnÞ<label>ð17Þ</label></formula><p>Motivated by <ref type="bibr" target="#b25">[26]</ref>, the authors in <ref type="bibr" target="#b2">[3]</ref> replace the ' 1 -norm penalty by the log-sum penalty function. Hence, the new cost function becomes</p><formula xml:id="formula_32">JðhðnÞÞ ¼ 1 2 jeðnÞj 2 þ g 0 X n t ¼ 1 logð1 þjhðtÞj=e 0 Þ ð<label>18Þ</label></formula><p>Then, the resulting update equation for this cost function becomes step 1b of Table <ref type="table">1</ref>(c), with g ¼ mg 0 =e 0 and e ¼ 1=e 0 . As a rule of thumb e should be set slightly smaller than the expected non-zero magnitudes of hðnÞ <ref type="bibr" target="#b25">[26]</ref>. The logsum penalty function has the potential of being moresparsity encouraging since it better approximates the non-convex ' 0 -norm <ref type="bibr" target="#b25">[26]</ref>. However, computational complexity increases due to the computation of the logarithm at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Convergence of the EM-RLS algorithm in mean</head><p>In this section we examine the asymptotic consistency of the EM-RLS algorithm in a time-invariant environment (this implies l ¼ 1 and hðiÞ ¼ h 0 8i). Analogous results may be obtained for the other EM-Kalman filters as well. We invoke the frequently adopted independence assumption <ref type="bibr">[17, pp. 336, 363]</ref>, which states that the sequence fxðnÞg is independent, identically distributed and according to CN ð0,s 2</p><p>x Þ. It is shown in Appendix that Eq. ( <ref type="formula" target="#formula_10">10</ref>) can be re-written as ĥðnÞ ¼ wðnÞÀgRðnÞsgnð ĥðnÞÞ Recall that wðnÞ can be expressed by the normal equations as follows:</p><formula xml:id="formula_33">wðnÞ ¼ R À1 ðnÞrðnÞ</formula><p>where RðnÞ ¼ P n t ¼ 1 xðtÞx H ðtÞ and rðnÞ ¼ P n t ¼ 1 xðtÞy Ã ðtÞ. The error covariance of the EM-RLS is given by PðnÀ1Þ according to Eqs. ( <ref type="formula" target="#formula_12">12</ref>) and <ref type="bibr" target="#b14">(15)</ref>. Following a similar derivation as in <ref type="bibr">[17, pp. 262-263]</ref>, we find that</p><formula xml:id="formula_34">ĥðnÞ ¼ h 0 þ R À1 ðnÞ X n t ¼ 1 xðtÞv Ã ðtÞÀgPðnÀ1Þsgnð<label>ĥðnÞÞ</label></formula><p>If we take the conditional expectation given fxðiÞg n i ¼ 1 we obtain E½ ĥðnÞ ¼ h 0 þE xðiÞ R À1 ðnÞ</p><formula xml:id="formula_35">X n t ¼ 1 xðtÞE½v Ã ðtÞ zfflfflfflffl}|fflfflfflffl{ ¼ 0 jfxðiÞg n i ¼ 1 2 4 3 5 ÀgE½PðnÀ1Þsgnð ĥðnÞÞ ¼ h 0 Àgs À2 x E½sgnð ĥðnÞÞ<label>ð19Þ</label></formula><p>Table <ref type="table">2</ref> EM-FRLS for sparse adaptive tracking. </p><formula xml:id="formula_36">Initialization: h 0 ¼ 0, k M ðnÞ ¼ 0, a M (0) = 0, b M ð0Þ ¼ 0, að0Þ ¼ 1, E f ð0Þ ¼ l M E b<label>ð0Þ</label></formula><formula xml:id="formula_37">e f M ðnÞ ¼ e f M<label>ðnÞ=aðnÀ1Þ</label></formula><formula xml:id="formula_38">3: a M ðnÞ ¼ a M ðnÀ1ÞÀk M ðnÀ1Þe f Ã M ðnÞ 4: E f ðnÞ ¼ lE f ðnÀ1ÞÀe f M ðnÞe f Ã M ðnÞ 5: a M þ 1 ðnÞ ¼ aðnÀ1Þþ je f M ðnÞj 2 lE f<label>ðnÀ1Þ</label></formula><p>Kalman Gain vector 6: hðnÞ ¼ sgnðwðnÞÞ½jwðnÞjÀgl</p><formula xml:id="formula_39">k M þ 1 ðnÞ ¼ 0 k M<label>ðnÀ1Þ</label></formula><formula xml:id="formula_40">" # þ e f M ðnÞ lE f<label>ðnÀ1Þ</label></formula><formula xml:id="formula_41">1 a M<label>ðnÀ1Þ</label></formula><formula xml:id="formula_42">" # 7: k T M þ 1 ðnÞ ¼ ½k dMe M ðnÞ k b ðnÞ 8: k M ðnÞ ¼ k dMe M þ 1 ðnÞÀk b b M<label>ðnÀ1Þ</label></formula><formula xml:id="formula_43">À1 s À2 x 1 þ end For</formula><p>Next, we compute E½sgnð ĥðnÞÞ. From Eq. ( <ref type="formula" target="#formula_10">10</ref>) ĥðnÞ is given by ĥi ðnÞ ¼</p><formula xml:id="formula_44">w i ðnÞÀgs À2 x if w i ðnÞ 4 gs À2 x w i ðnÞþgs À2 x if w i ðnÞ o Àgs À2 x 0 i f jw i ðnÞj r gs À2 x 8 &gt; &lt; &gt; : Thus, E½sgnð ĥi ðnÞÞ ¼ Z 1 À1 sgnð ĥi ðnÞÞpðw i ðnÞÞ ¼ Z 1 gs À2 x pðw i ðnÞÞÀ Z Àgs 2 x À1 pðw i ðnÞÞ ¼ 1À F w i ðnÞ ðgs À2 x ÞÀ F w i ðnÞ ðÀgs À2 x Þ But, w i ðnÞ $ CN ðh 0,i ,s 2 0 þ P n t ¼ 1 s 2 q i ðtÞÞ so that</formula><p>E½sgnð ĥi ðnÞÞ ¼ 1ÀF w i ðnÞ gs À2</p><p>x Àh 0,i ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi</p><formula xml:id="formula_45">s 2 0 þ P n t ¼ 1 s 2 q i ðtÞ q 0 B @ 1 C A ÀF w i ðnÞ Àgs À2</formula><p>x Àh 0,i ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi</p><formula xml:id="formula_46">s 2 0 þ P n t ¼ 1 s 2 q i ðtÞ q 0 B @ 1 C A If s 2 q i ðtÞ is constant over time then R ii ðnÞ ¼ s 2 0 þ ns 2 q i is</formula><p>constantly increasing. Since h 0,i and gs À2</p><p>x are constant, the two terms ðgs À2</p><p>x Àh 0,i Þ= ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi</p><formula xml:id="formula_47">s 2 0 þ ns 2 q i q and ðÀgs À2 x Àh 0,i Þ= ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi s 2 0 þ ns 2 q i q</formula><p>asymptotically tend to zero. Therefore, as n-1, E½sgnð ĥi ðnÞÞ tends to 1À2F wðnÞ ð0Þ ¼ 0. This shows that the EM-RLS algorithm converges asymptotically in mean to the true parameter vector. Eq. ( <ref type="formula" target="#formula_35">19</ref>) implies that the EM-RLS filter returns a biased estimate of the true parameter vector. This should not surprise us since basis pursuit does not guarantee to detect the correct support set and at the same time estimate all the unknowns consistently. The bias term which occurs in the finite sample case is alleviated by re-weighting, as discussed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">The re-weighted EM-Kalman filter</head><p>The key difference between ðP ' 0 Þ and ðP ' 1 Þ is that the ' 1 -norm penalty depends on the magnitudes of the nonzero components of the parameter vector, whereas the ' 0 -norm penalty does not. As a result, the larger a component of the estimate vector is, the heavier it is penalized by the ' 1 penalty. Motivated by <ref type="bibr" target="#b25">[26]</ref> we employ a reweighted version of the EM-Kalman filter to overcome this unfair penalization. At each iteration the following cost function is maximized:</p><formula xml:id="formula_48">ĥðnÞ ¼ argmax h fE pðhðnÞjyðnÀ1Þ; ĥðnÀ1ÞÞ ½logpðhðnÞ; hÞÀgJWðnÞhJ ' 1 g<label>ð20Þ</label></formula><p>where WðnÞ is a diagonal weighing matrix with positive diagonal elements w 1 (n),y, w M (n). The diagonal elements of WðnÞ must be chosen such that they put lower (higher) weight on the smaller (larger) components of the estimate vector. This mechanism yields a better approximation to the ' 0 quasi-norm penalization, and thus reduces the estimate bias.</p><p>One way to adjust the weighting matrix is to run an adaptation algorithm in parallel to the E-step for supplying the required weights <ref type="bibr" target="#b4">[5]</ref>. However, this considerably increases the computational complexity of the underlying adaptive algorithm. A widely used choice for the diagonal elements of the matrix is w i ðnÞ ¼ 1=ð1 þejw i ðnÞjÞ, for i= 1,2,y,M, where e40 is a stability parameter <ref type="bibr" target="#b25">[26]</ref>. For this particular choice of the weighting matrix, the re-weighted ' 1 -norm is identical to the log-sum penalty <ref type="bibr" target="#b25">[26]</ref> and thus the M-step of Eq. ( <ref type="formula" target="#formula_48">20</ref>) leads to the weighted soft-thresholding function given by ĥi ðnÞ ¼ sgnðw i ðnÞÞ jw i ðnÞjÀ gR ii ðnÞ</p><formula xml:id="formula_49">1 þejw i ðnÞj þ<label>ð21Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Low complexity implementation of the sparse adaptive algorithms</head><p>It must be noted that due to the nature of the softthresholding step, the estimate hðnÞ has many zero entries. This will allow to implement the EM-Kalman, EM-RLS and EM-FRLS algorithms in a low complexity fashion similar to the approach taken in <ref type="bibr" target="#b3">[4]</ref>. The idea is to restrict the matrix operations to the instantaneous support of the estimate hðnÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Application to nonlinear channel estimation</head><p>In what follows, power amplifier nonlinear models are incorporated into the study of two important channels:</p><p>(1) the satellite link and (2) the multipath wireless channel. In both cases, the overall communication channel is represented by baseband time-varying Volterra series.</p><p>A finite order passband Volterra system has the form</p><formula xml:id="formula_50">yðiÞ ¼ X P p ¼ 1 X Mp t 1 ¼ 0 Á Á Á X Mp tp ¼ 0 h p ði,t 1 , . . . ,t p Þ Y p j ¼ 1 xðiÀt j ÞþvðiÞ<label>ð22Þ</label></formula><p>where h p ðÁÞ is the Volterra kernel of order p. The disturbance v(i) is assumed to be white Gaussian noise. P is the highest order of nonlinearity while M p is the pth order system memory. The Volterra model becomes linear when P =1, quadratic when P= 2 and cubic when P=3.</p><p>In bandlimited communications only odd products of the input are considered <ref type="bibr">[9, p. 735</ref>]. In such cases only spectral components inside the frequency band of interest are maintained and the baseband Volterra model [9, p. 734] is used</p><formula xml:id="formula_51">yðiÞ ¼ X bðPÀ1Þ=2c p ¼ 0 X M 2p þ 1 t 1 ¼ 0 Á Á Á X M 2p þ 1 t 2p þ 1 ¼ 0 h 2p þ 1 ði,s 1:2p þ 1 Þ Y p þ 1 j ¼ 1 xðiÀt j Þ Y 2p þ 1 k ¼ p þ 2 x Ã ðiÀt k ÞþvðiÞ ð<label>23Þ</label></formula><p>where</p><formula xml:id="formula_52">h 2p þ 1 ði,s 1:2p þ 1 Þ denotes the baseband kernel with s 1:2p þ 1 ¼ ½t 1 , . . . ,t 2p þ 1 .</formula><p>The above equations can be written in the linear regression form of Eq. ( <ref type="formula" target="#formula_0">1</ref>) using the Kronecker product notation. Indeed consider the baseband case, let</p><formula xml:id="formula_53">x M2p þ 1 ðiÞ ¼ ½xðiÞ,xðiÀ1Þ, . . . ,xðiÀM 2p þ 1 Þ T</formula><p>then xðiÞ is given by the i-fold Kronecker product</p><formula xml:id="formula_54">x ðp þ 1,pÞ M 2p þ 1 ðiÞ ¼ # p þ 1 j ¼ 1 x M 2p þ 1 ðiÞ " # # p k ¼ 1 x Ã M 2p þ 1<label>ðiÞ</label></formula><p>The Kronecker product contains all 2p + 1 order products of the input with p conjugated p +1 unconjugated copies. Likewise hðiÞ ¼ ½h 1 ði,ÁÞ, . . . ,h 2p þ 1 ði,ÁÞ T is obtained by treating the (2p+ 1)-dimensional kernel as an M 2p + 1 2p + 1 column vector. The passband case can be treated in a similar fashion. The regressor vector is now given by</p><formula xml:id="formula_55">x T ðiÞ ¼ ½x T M 1 ðiÞ Á Á Á x ðp þ 1,pÞT M2p þ 1 ðiÞ<label>ð24Þ</label></formula><p>Recovery of the locations, the magnitudes and the Volterra kernels can be accomplished by the convex program ðP ' 1 Þ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Volterra representation for satellite links</head><p>In satellite digital transmission, both the earth station and the satellite repeater employ power amplifiers. The satellite amplifier operates near saturation due to limited power resources and hence behaves in a nonlinear fashion. The satellite link is composed of a power amplifier sandwiched between two linear filters with impulse responses g 1 and g 2 , see Fig. <ref type="figure">1</ref>. The LTI filter with impulse response g 1 describes the cascade of all linear operations preceding the power amplifier. Likewise the LTI filter g 2 represents the cascade of all linear devices following the nonlinearity.</p><p>An analysis of the above system for static power amplifiers is provided by Benedetto and Biglieri <ref type="bibr">[9, p. 735</ref>]. Let us next consider power amplifiers with memory described by Volterra models. To reduce the computational complexity we shall follow standard practice <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and confine our study to diagonal Volterra models 3 given by</p><formula xml:id="formula_56">wðtÞ ¼ X bðPÀ1Þ=2c p ¼ 0 Z k 2p þ 1 ðmÞjzðtÀmÞj 2p zðtÀmÞ dm<label>ð25Þ</label></formula><p>Note that only odd products of the input appear in <ref type="bibr" target="#b24">(25)</ref>, as a consequence of the bandpass property of the nonlinearity <ref type="bibr">[9, p. 735</ref>]. Let us now combine the effects of the linear filters</p><formula xml:id="formula_57">zðtÞ ¼ Z g 1 ðsÞxðtÀsÞ ds, rðtÞ ¼ Z g 2 ðrÞxðtÀrÞ dr<label>ð26Þ</label></formula><p>Straightforward calculations lead to the baseband Volterra model</p><formula xml:id="formula_58">rðtÞ ¼ X bðPÀ1Þ=2c p ¼ 0 Z Á Á Á Z h 2p þ 1 ðs 1:2p þ 1 Þ Â Y p þ 1 i ¼ 1 xðtÀt i Þ Y 2p þ 1 j ¼ p þ 2 x Ã ðtÀt j Þ ds 1:2p þ 1<label>ð27Þ</label></formula><p>The baseband kernel</p><formula xml:id="formula_59">h 2p þ 1 ðs 1:2p þ 1 Þ is given by h 2p þ 1 ðs 1:2p þ 1 Þ ¼ Z g 2 ðrÞ Z k 2p þ 1 ðmÞ Y p þ 1 i ¼ 1 g 1 ðt i ÀmÀrÞ Y 2p þ 1 j ¼ p þ 2 g Ã 1 ðt i ÀmÀrÞ dm dr</formula><p>The Volterra kernels are expressed in the frequency domain as follows:</p><formula xml:id="formula_60">H 2p þ 1 ðx 1:2p þ 1 Þ ¼ G 2 X p l 1 ¼ 0 o 2l 1 þ 1 0 @ 1 A K 2p þ 1 X p l 2 ¼ 0 o 2l 2 þ 1 0 @ 1 A Y p þ 1 i ¼ 1 G 1 ðo i Þ Y 2p þ 1 j ¼ p þ 2 G Ã 1 ðo j Þ</formula><p>In most cases the filter g 1 performs a specific functionality (for instance pulse shaping) and hence is known. Since in this paper we shall deal with channel estimation using known inputs, we may with no loss of generality assume the input signal is the output of g 1 . In this case the Volterra representation from signal x(t) to signal r(t) gets simpler. More precisely we have</p><formula xml:id="formula_61">rðtÞ ¼ X bðPÀ1Þ=2c p ¼ 0 Z h 2p þ 1 ðt 1 ÞjxðtÀt 1 Þj 2p xðtÀt 1 Þ dt 1<label>ð28Þ</label></formula><p>where</p><formula xml:id="formula_62">h 2p þ 1 ðt 1 Þ ¼ h 2p þ 1 ðs 1:2p þ 1 Þdðt 1 Àt 3 Þ Á Á Á dðt 1 Àt 2p þ 1 Þ ¼ Z g 2 ðrÞk 2p þ 1 ðt 1 ÀrÞ dr<label>ð29Þ</label></formula><p>In the frequency domain the Volterra kernels take the form</p><formula xml:id="formula_63">H 2p þ 1 ðo 2p þ 1 Þ ¼ G 2 ðo 2p þ 1 ÞK 2p þ 1 ðo 2p þ 1 Þ ð<label>30Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multipath channels</head><p>In this subsection, fading channels characterized by multipath effects are considered. We shall assume that the modulated signal is amplified by a power amplifier with memory and then transmitted through the wireless medium. The received waveform is the superposition of weighted and delayed versions of the signal resulting from various multipaths plus additive white Gaussian noise.</p><p>We shall assume that the different non-zero fading rays arrive at the receiver at different time instances and they vary slowly with time and frequency. Thus the wireless channel becomes a frequency selective channel <ref type="bibr" target="#b1">[2]</ref>, that is, a linear time invariant system with impulse Fig. <ref type="figure">1</ref>. Digital satellite link. 3 The analysis of general (non-diagonal) Volterra models is similar. response g 2 ðrÞ ¼ X</p><formula xml:id="formula_64">N path i ¼ 1 a i dðrÀt i Þ ð<label>31Þ</label></formula><p>where N path is the number of paths, a i is the attenuation along path i and t i is the clustered delay. If we substitute Eq. ( <ref type="formula" target="#formula_64">31</ref>) into ( <ref type="formula" target="#formula_62">29</ref>) we obtain</p><formula xml:id="formula_65">h 2p þ 1 ðtÞ ¼ X N path i ¼ 1 a i k 2p þ 1 ðtÀt i Þ ð<label>32Þ</label></formula><p>The kernels in the frequency domain become</p><formula xml:id="formula_66">H 2p þ 1 ðo 2p þ 1 Þ ¼ K 2p þ 1 ðo 2p þ 1 Þ X N path i ¼ 1 a i e Àjo 2p þ 1 t i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sparse Volterra channel representation</head><p>The transmission systems described in Sections 4.1 and 4.2 operate in continuous time. Discrete Volterra forms result when the modulation at the transmitter and the sampling device at the receiver are taken into account. We shall consider memoryless modulation schemes whereby</p><formula xml:id="formula_67">xðtÞ ¼ X i s i dðtÀiT s Þ ð<label>33Þ</label></formula><p>The sequence s i consists of i.i.d. (discrete) complex valued random variables and T s denotes the symbol period. Substituting x(t) from Eq. ( <ref type="formula" target="#formula_67">33</ref>) into ( <ref type="formula" target="#formula_58">27</ref>) yields a Volterra description of both channels, given by Eq. ( <ref type="formula" target="#formula_51">23</ref>). The resulting Volterra kernels are sparse. Indeed, it is well documented in the literature that parsimonious models are highly desirable in the representation of memory PA. In fact it has been experimentally observed <ref type="bibr" target="#b26">[27]</ref> that sparse diagonal Volterra models for PA provide enhanced performance in comparison to the full model. The convolutional form of Eq. (32) indicates that the sparsity of the 2p + 1 kernel is at most s k Â s m , where s k is the sparsity of the PA and s m is the sparsity of the multipath coefficients. Similar observations hold for the satellite channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulations</head><p>In this section we evaluate the performance of the proposed algorithms given in Section 3 through computer simulations. Experiments were conducted on both linear and nonlinear channel setups.</p><p>In all experiments the output sequence is disturbed by additive white Gaussian noise, with a signal to noise ratio (SNR) of SNR= 10 dB. SNR is the ratio of the coefficient vector power to the noise power corrupting the output signal ðE½J ĥJ 2 ' 2 =s 2 v Þ. The normalized mean square error, defined as</p><formula xml:id="formula_68">NMSE ¼ 10 log 10 ðE½J ĥÀhJ 2 ' 2 =E½JhJ 2 ' 2 Þ</formula><p>was used to assess performance. The choice of the parameters l, g that were used to compare the performance of the sparse algorithms for each experiment is summarized in Table <ref type="table" target="#tab_5">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Tracking a Rayleigh fading sparse channel</head><p>In this experiment we compare the performance of the three computationally lighter algorithms, i.e. the EM-FRLS, the wEM-FRLS and the sparse LMS filter on a linear channel. We consider a wireless channel with three Rayleigh fading rays; all rays are assumed to fade at the same Doppler frequency of f D =10 Hz <ref type="bibr">[17, pp. 325-326]</ref>. The channel impulse response sequence consists of four non-zero Rayleigh fading rays at positions 4, 10, 16 and 24. In other words, we are assuming a channel length of M= 30 taps with only four active Rayleigh fading rays. The auto-correlation function of the non-zero channel taps is modeled as a zeroth-order Bessel function</p><formula xml:id="formula_69">rðtÞ ¼ J 0 ð2pf D T s Þ</formula><p>where T s is the sampling period and f D is the maximum Doppler frequency of the Rayleigh fading channel.</p><p>All input symbols are drawn from a BPSK constellation. Figs. <ref type="figure">2a</ref> and<ref type="figure">b</ref> show the NMSE learning curve for various adaptive filters, which result from averaging over 20 independent computer runs. As it is readily apparent from the figures, EM-FRLS and the re-weighted EM-FRLS (wEM-FRLS) achieve a performance gain of around 4 and 8 dB, respectively, over the conventional FRLS algorithm. Moreover, the convergence rate and the performance of the FRLS family is better than the LMS family.</p><p>To demonstrate the support tracking performance of the proposed algorithms, after 600 iterations we set the second and third non-zero fading ray to zero while a new fading ray appears at position 11. We note from Figs. <ref type="figure">2(a)</ref> and (b) that the conventional algorithms have a slower support tracking behavior than the sparse variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Estimation of a sparse baseband Volterra channel</head><p>In the second channel setup a third-order baseband Volterra model is considered. It is given by yðnÞ ¼ ð0:4968Àj0:6707ÞxðnÀ3Þþð1:3336þ j0:9303ÞxðnÀ5Þ þð1:2745 þj0:2965Þx 2 ðnÀ1Þx Ã ðnÀ4Þ Àð0:4794þ j1:3298Þx 2 ðnÀ5Þx Ã ðnÀ1Þ</p><p>The above channel is excited by an OFDM signal, which can be approximated by a zero-mean, stationary complex normal random process, CN ð0,1=4Þ. This approximation  relies on the Central Limit Theorem and the fact that an OFDM signal consists of a sum of a large number of independent, identically distributed signals <ref type="bibr">[29, pp. 70-71]</ref>.</p><p>The memory of the linear and cubic part is M 1 =M 3 =7. The algorithms were run for 10 Monte-Carlo runs and the performance of the sparse algorithms is illustrated in Fig. <ref type="figure">3</ref>. The EM-RLS algorithms introduces a performance gain of 8 dB over the conventional RLS. Fig. <ref type="figure">3</ref> illustrates that the LMS family has very slow convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Tracking a Rayleigh fading sparse Hammerstein channel</head><p>In this experiment the multipath channel setup of Eq. ( <ref type="formula" target="#formula_61">28</ref>) is considered. This model is usually encountered in GSM applications. The base station (BSS) communicates with the mobile terminal through the wireless channel. A nonlinear power amplifier at the BSS amplifies the transmitted signal so that it reaches the remote terminal.</p><p>The wireless channel taps for the linear and cubic part were generated by sparse Rayleigh fading rays <ref type="bibr">[17, pp. 325-326]</ref>. All rays are assumed to fade at the same Doppler frequency of f D = 80 Hz with sampling period T s ¼ 0:8 ms. The linear and the cubic part have equal memory size M 1 = M 3 = 50 and the support signal consists of two randomly selected elements for each part. The input signal is drawn from a complex Gaussian distribution CN ð0,1=4Þ. We observe in Fig. <ref type="figure">4a</ref> that the EM-Kalman, re-weighted EM-Kalman (wEM-Kalman) and EM-RLS algorithms provide gains of 7, 12 and 4 dB, respectively, over the corresponding conventional nonsparse algorithms. Moreover, the proposed algorithms perform almost similar NMSE performance with the LMS family but the convergence speed is significantly faster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/sigpro Signal Processing 0165-1684/$ -see front matter &amp; 2011 Elsevier B.V. All rights reserved. doi:10.1016/j.sigpro.2011.02.013</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>À 3 4 Â 10 À 4 , 2 Â 10 À 3 6 Â 10 À 4 a</head><label>31042103104</label><figDesc>l ¼ 0:99, m ¼ 10 À2 , e ¼ 10. b l ¼ 0:995, m ¼ 5 Â 10 À2 , e ¼ 10. c l ¼ 0:997, m ¼ 5 Â 10 À2 , e ¼ 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Fig. 2. NMSE between the EM-FRLS, wEM-FRLS and sparse LMS for linear system identification (support change occurs at n =600). (a) FRLS family. (b) LMS family.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>eðnÞ denotes the prediction error given by eðnÞ ¼ yðnÞÀx T ðnÞ ĥðnÀ1Þ and x Ã ðnÞ denotes the complex conjugate of the vector xðnÞ. Hence wðnÞ depends linearly on h. The Riccati equation that updates PðnÞ indicates that PðnÞ does not depend on h. Moreover, E½eðnÞyðnÀ1Þ ¼ 0 because the prediction error eðnÞ is uncorrelated to measurements. The ith diagonal entry of the prior covariance R ii ðnÞ can be alternatively estimated as follows:2   </figDesc><table><row><cell>PðnÀ1Þx Ã ðnÞ wðnÞ kðnÞ ¼ s 2 v þx T ðnÞPðnÀ1Þx Ã ðnÞ</cell><cell>the</cell><cell>prior,</cell><cell>the</cell><cell>conditional</cell><cell>expectation</cell></row></table><note><p>¼ E½hðnÞjyðnÀ1Þ; ĥðnÀ1Þ is obtained by the following recursive form [16, p. 60; 17, p. 109] (see also Table 1(a)) wðnÞ ¼ ĥðnÀ1ÞþkðnÞeðnÞ, wð0Þ ¼ hð0Þ PðnÞ ¼ PðnÀ1ÞþRðnÞÀkðnÞx T ðnÞPðnÀ1Þ ð 11Þ where kðnÞ is the Kalman gain defined by and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, E b ð0Þ ¼ s 2</figDesc><table><row><cell></cell><cell>x</cell></row><row><cell cols="2">For n :¼ 1,2, . . . do</cell></row><row><cell>Forward Predictor</cell><cell></cell></row><row><cell>1:</cell><cell>e f M ðnÞ ¼ xðnÞþa H M ðnÀ1ÞxðnÀ1Þ</cell></row><row><cell>2:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Choice of parameters for computer experiments.Exp. FRLS ð' 1 ,wÀ' 1 Þ KF ð' 1 ,wÀ' 1 Þ LMS ð' 1 ,logÞ RLS ð' 1 Þ 5.1 a 3 Â 10 À 4 , 2 Â 10 À 3 -3 Â 10 À 4 , 7 Â 10 À 4</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our analysis assumes a system with complex valued variables. N. Kalouptsidis et al. / Signal Processing 91 (2011) 1910-1919</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, sparse approximations have been studied for system identification. Adaptive algorithms combining Expectation-Maximization and Kalman filtering were developed and tested by simulations. Significant performance gains were achieved in comparison to the conventional non-sparse methods.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Derivation of Eq. <ref type="bibr" target="#b9">(10)</ref> The M-step of the EM algorithm consists of setting the sub-gradient of the penalized expected log-likelihood function to zero. Note that the M-step is convex and hence a sub-gradient always exists <ref type="bibr" target="#b29">[30]</ref>. In particular, where ðzÞ þ ¼ maxðRðzÞ,0ÞþjmaxðIðzÞ,0Þ.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compressed channel sensing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CISS</title>
		<meeting>the IEEE CISS</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse LMS for system identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICASSP</title>
		<meeting>the IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of spaRLS and RLS algorithms for adaptive filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalouptsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tarokh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Sarnoff Symposium</title>
		<meeting>the IEEE Sarnoff Symposium</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RLS-weighted LASSO for adaptive estimation of sparse signals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angelosante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICASSP</title>
		<meeting>the IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kalman filtered compressed sensing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICIP</title>
		<meeting>the IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="893" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple method for sparse signal recovery from noisy observations using Kalman filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gurfil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanevsky</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">IBM Research Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proportionate normalized least-mean-squares adaptation in echo cancellers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duttweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="508" to="518" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principles of Digital Transmission: With Wireless Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benedetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Biglieri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Just relax: convex programming methods for identifying sparse signals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1030" to="1051" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient projection for sparse reconstruction: application to compressed sensing and other inverse problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the LASSO</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-oracle performance of basis pursuit under random noise</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ben-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<idno>Arxiv preprint 0903.4579</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">General structure of adaptive algorithms: adaptation and tracking</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Kalouptsidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Adaptive System Identification and Signal Processing Algorithms</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adaptive Filters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sayed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Wiley-IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Statistical signal processing using a class of iterative estimation algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An EM algorithm for wavelet-based image restoration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="906" to="916" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Iterative and sequential algorithms for multisensor signal enhancement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="846" to="859" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jazwinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="475" to="485" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast sequential algorithm for least-squares filtering and prediction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carayannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalouptsidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1394" to="1402" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Numerically stable fast transversal filters for recursive least squares adaptive filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="114" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient algorithms for Volterra system identification</title>
		<author>
			<persName><forename type="first">G.-O</forename><surname>Glentis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koukoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalouptsidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3042" to="3057" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhancing sparsity by reweighted L1 minimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cand Es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="877" to="905" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Behavioral modeling of nonlinear RF power amplifiers considering memory effects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Microw. Theory Tech</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2495" to="2504" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative analysis of behavioral models for RF power amplifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isaksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wisell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ronnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Microw. Theory Tech</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="348" to="359" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-Carrier Digital Communications-Theory and Applications of OFDM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saltzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic, Plenum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex Analysis</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
