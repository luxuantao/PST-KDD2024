<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Neural Networks with Node-wise Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
							<email>zhewei@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<email>yaliang.li@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Weirui</forename><surname>Kuang</surname></persName>
							<email>weirui.kwr@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
							<email>bolin.ding@alibaba-inc.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Neural Networks with Node-wise Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539387</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Neural Architecture Search</term>
					<term>Dynamic Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Neural Architecture Search (NAS) for GNN has received increasing popularity as it can seek an optimal architecture for a given new graph. However, the optimal architecture is applied to all the instances (i.e., nodes, in the context of graph) equally, which might be insufficient to handle the diverse local patterns ingrained in a graph, as shown in this paper and some very recent studies. Thus, we argue the necessity of node-wise architecture search for GNN. Nevertheless, node-wise architecture cannot be realized by trivially applying NAS methods node by node due to the scalability issue and the need for determining test nodes' architectures. To tackle these challenges, we propose a framework wherein the parametric controllers decide the GNN architecture for each node based on its local patterns. We instantiate our framework with depth, aggregator and resolution controllers, and then elaborate on learning the backbone GNN model and the controllers to encourage their cooperation. Empirically, we justify the effects of node-wise architecture through the performance improvements introduced by the three controllers, respectively. Moreover, our proposed framework significantly outperforms state-of-the-art methods on five of the ten real-world datasets, where the diversity of these datasets has hindered any graph convolution-based method to lead on them simultaneously. This result further confirms that node-wise architecture can help GNNs become versatile models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Neural networks; Machine learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b31">29]</ref> have been proposed and applied to solve various tasks on ubiquitous graph data, including social networks <ref type="bibr" target="#b17">[15]</ref>, citation networks <ref type="bibr" target="#b18">[16]</ref>, and biological networks <ref type="bibr" target="#b40">[38]</ref>. When applying GNN to a new graph, Neural Architecture Search (NAS) for GNN <ref type="bibr" target="#b7">[5,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b36">[34]</ref><ref type="bibr" target="#b37">[35]</ref><ref type="bibr" target="#b38">[36]</ref><ref type="bibr" target="#b39">[37]</ref> is often conducted to seek a suitable GNN architecture for handling that graph, e.g., choosing the depth of GNN to be 3 from the candidate depths {2, 3, • • • }, choosing the mean pooling from the candidate pooling operations {min, max, mean}, and so on.</p><p>Existing works in this line follow the convention of NAS to apply the searched optimal architecture to all the instances (i.e., nodes) equally. However, for different nodes in the same graph, their local patterns, including both the topological structures and the node attributes in their neighborhoods, are usually very diverse, making applying the same architecture to all nodes unsuitable. As a piece of evidence, researchers have recently observed the different levels of local assortativity exhibited in each of several real-world graphs <ref type="bibr" target="#b26">[24]</ref>, which lead to unsatisfactory performances of several representative GNNs <ref type="bibr" target="#b23">[21]</ref>. Therefore, we argue that GNN with node-wise architecture is much in demand.</p><p>To be specific, we present three examples in Fig. <ref type="figure" target="#fig_1">1</ref> that justify the necessity of using node-wise architecture from three different aspects of architecture. (1) Different nodes may need different depths for the GNN. Comparing the two rows of Fig. <ref type="figure" target="#fig_1">1a</ref>, the message of a densely connected node propagates much faster than that of a node with rare connections. This phenomenon has been analyzed as that nodes with a larger degree are more quickly to produce over-smoothed node embeddings along with the iterations of graph convolution operations <ref type="bibr" target="#b3">[1,</ref><ref type="bibr" target="#b25">23]</ref>. Node-wise depth has been studied in recent works <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b32">30]</ref> to allow nodes with different local structures to have different depths while avoiding the over-smoothing issue. These works support our idea of node-wise architecture from the aspect of depth. (2) Different nodes may need different aggregators. In Fig. <ref type="figure" target="#fig_1">1b</ref>, with the assumed class labels and in-coming messages, the two target nodes on the left-hand side can be successfully distinguished by a mean/sum pooling, while the two nodes on the right-hand side require a max/min pooling. These two pairs have been used by PNA <ref type="bibr" target="#b5">[3]</ref> to motivate the usage of a mixture of aggregators for a GNN. In our case, we emphasize the necessity of selecting the appropriate aggregator in a node-wise manner. <ref type="bibr" target="#b5">(3)</ref> We  propose a novel notion, the resolution of a GNN layer, as how many neighbors are sampled for aggregating their messages. Sampling is necessary for training GNN models on large graphs, where different resolutions often lead to different computation graphs and thus architectures of the applied GNN. In practice, the widely-adopted neighbor sampler <ref type="bibr" target="#b10">[8]</ref> uses the same pre-specified resolutions of the GNN layers for all nodes. However, when the local patterns of a node are the case of the red node shown in Fig. <ref type="figure" target="#fig_1">1c</ref>, sampling more 1-hop neighbors would be better than sampling more 2-hop neighbors in the sense of reducing the variance of estimation. Thus, this node prefers a high resolution for the second GNN layer while being insensitive to the resolution for the first layer. Motivated by these observations, we study how to search for the optimal GNN architecture in a node-wise manner. To this end, a straightforward extension of existing NAS methods will increase the size of search space linearly w.r.t. the number of nodes, which makes it intractable on large-scale graphs. Moreover, such an extension searches the suitable architectures only for the nodes that are accessible during training, and thus it cannot generate the suitable architectures for the test nodes under the inductive setting.</p><p>To tackle these challenges, we propose a framework wherein there is a parametric controller for each aspect of architecture, e.g., an aggregator controller, to decide which kind of aggregator should be applied. To determine the architecture configuration for a node at a specific layer, the controller first encodes the node's local patterns into a context embedding and then takes choices from the search space based on it. For example, when we assume that features of the node at previous layers are sufficient for determining its desired aggregator at the current layer, we can feed the embeddings of the node at previous layers into the aggregator controller. Intuitively, the backbone GNN model depends on the controllers to predict the suitable architectures, while the controllers make predictions for each node based on the node's local patterns captured by the backbone model. Thus we design a learning method to promote the cooperation between the backbone model and the controllers.</p><p>Since our context-aware controllers have a fixed number of parameters independent of the number of nodes and can generalize to unseen nodes, the proposed framework can achieve node-wise architecture for a GNN model even on a large-scale graph under both transductive and inductive settings. It is worth noting that although we instantiate the proposed framework with depth, aggregator, and resolution controllers, controllers designated for other aspects of GNN architecture can be easily included in our framework.</p><p>We compare our proposed framework with state-of-the-art methods on ten real-world graph datasets. Our method achieves the best performances on half of them, where no or at most one baseline method can reach our 95% confidence interval. Moreover, the datasets we outperform include homophilic and heterophilic graphs, where no existing graph convolution-based method can lead simultaneously. Meantime, the node-wise resolution is shown to improve the performance of a GNN on a large-scale graph. Then we show that the controllers can appropriately correlate the suitable nodewise depth, aggregator, and resolution with each node's local patterns, which explains how node-wise architecture can improve the performance of GNNs. The payment for realizing such node-wise architecture is also empirically evaluated from the perspective of sample efficiency and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We first introduce the notations used in this paper, and give a brief summary of GNN models and NAS for GNN. Let 𝐺 = (V, E) denote a graph with node attributes x 𝑣 for each node 𝑣 ∈ V. Without loss of generality, we consider undirected graphs in this paper, and thus the neighborhood of a node 𝑣 can be denoted by N (𝑣) = {𝑢 |(𝑢, 𝑣) ∈ E}. In this paper, we focus on node-level tasks (e.g., node classification) where each node 𝑣 is associated with a label 𝑦 𝑣 ∈ Y. Our goal is to learn a GNN from the labeled nodes to predict the unlabeled ones. GNN. Existing GNN models, spatial-based <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b33">31]</ref> or spectral-based <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b14">12]</ref>, are often described and implemented in the message passing paradigm, where the representation h (𝑙) 𝑣 of node 𝑣 at the 𝑙-th layer is recursively calculated by aggregating the messages propagated from its neighbors. This calculation can be formulated as</p><formula xml:id="formula_0">h (𝑙) 𝑣 = 𝜎 (Aggr 𝑢 ∈N (𝑣)∪{𝑣 } (𝜙 (𝑙) (h (𝑙−1) 𝑢 ))), h (0) 𝑣 = x 𝑣 ,<label>(1)</label></formula><p>where 𝑙 = 1, . . . , 𝐿, 𝜎 (•) denotes an activation function (e.g., ReLU), 𝜙 (𝑙) (•) denotes any differentiable function such as an MLP, and Aggr(•) denotes a permutation-invariant aggregator such as the mean pooling. In general, we can use h</p><formula xml:id="formula_1">(𝐿)</formula><p>𝑣 as the final node representation z 𝑣 for predicting 𝑦 𝑣 . NAS for GNN. Works in this line has studied some aspects of the architecture for a GNN, e.g., the intra-layer design needs to determine the Aggr(•) and the inter-layer design needs to seek for an optimal depth 𝐿 <ref type="bibr" target="#b36">[34]</ref>. Each aspect is associated with its search space, e.g., the Aggr(•) is allowed to take choice from O = {mean, add, max, min}. Differentiable NAS <ref type="bibr" target="#b21">[19,</ref><ref type="bibr" target="#b38">36]</ref>, one of the most widely adopted NAS approaches, often models each aspect by a random variable, e.g., 𝑂 with possible outcomes O, and parameterizes Pr(𝑂) by a |O|dimensional vector 𝝓 in the form-Pr(𝑂 = 𝑜; 𝝓) = exp(𝝓 𝑜 ) 𝑜 ′ ∈O exp(𝝓 𝑜 ′ ) . Conventionally, 𝝓 is called architecture parameter to be distinguished from the model parameter 𝜽 of the backbone GNN model. Then the search procedure corresponds to optimizing the architecture parameter 𝝓.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GNNS WITH NODE-WISE ARCHITECTURE</head><p>Generally, existing works in the line of NAS for GNN search for an optimal architecture and apply it to all the nodes equally. Suppose the searched architecture corresponds to a GNN that applies mean pooling in its first layer and min pooling in its second layer. In Fig. <ref type="figure" target="#fig_2">2</ref>, when we apply this searched architecture to the graph, the GNN models applied to the three nodes (A, E, and G) will result in the computation graphs shown in the "ordinary GNN" part. However, as discussed in Sec 1, it might be unsatisfactory to apply the same architecture for handling all the nodes, and thus GNN with node-wise architecture is needed. Before introducing how to realize GNN with node-wise architecture, we first present an example in Fig. <ref type="figure" target="#fig_2">2</ref> to show its difference against an ordinary GNN. For depth, the GNN applied to node G has four layers while the GNNs applied to other nodes have a depth of two, where the difference might come from their different node degrees. For aggregator, the GNN applied to node B uses a mean pooling at its first layer while that applied to node F uses a min pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context-aware Controller</head><p>For the differentiable NAS discussed in Sec. 2, suppose there is an aspect of the architecture to be determined in each of the 𝐿 layers, then the distributions Pr(𝑂 (𝑙) ), 𝑙 = 1, . . . , 𝐿 are parameterized by the architecture parameters with a total dimension of 𝐿 × |O|. If we attempt to realize node-wise architecture via a straightforward extension of such NAS method, there would be a dedicated random variable 𝑂 (𝑙) 𝑣 for each node 𝑣 ∈ 𝑉 . Thus, the total dimension of the required architecture parameters will increase along with the number of nodes |𝑉 | linearly, which is unaffordable on large graphs. Moreover, in an inductive setting, the test nodes are inaccessible until the test phase, where the architecture parameters for the test nodes cannot be estimated in advance.</p><p>To achieve GNN with node-wise architecture on large graphs, we propose a framework that utilizes parametric controllers to predict the suitable architectures for the backbone GNN model. The controller makes predictions for each node based on its context that can reflect its local patterns. Thus different nodes are allowed to have different GNN architectures. To determine a specific aspect of the GNN architecture, we characterize the node-wise distribution Pr(𝑂 (𝑙) 𝑣 ) by 𝑔(•), which will encode its input into a context embedding to reflect the local patterns of the node 𝑣 at the stage of the 𝑙-th layer and output a distribution over O. We are allowed to consider different inputs for controllers responsible for different aspects of GNN architecture, with the principle that the inputs should provide sufficient evidence for determining the suitable architecture.</p><p>In our framework, any aspect of GNN architecture can be handled by simply adding a corresponding controller. We exemplify the proposed context-aware controller from the aspects of depth, aggregator, and resolution as follows. Depth controller. We present two different designs for the depth controller 𝑔 (d) (•). In the first design, given the maximal allowed depth 𝐿, we can define the search space as O = {0, . . . , 𝐿}, the nodewise distribution Pr(𝑂 𝑣 ) = 𝑔 (d) ({h (𝑙) 𝑣 , 𝑙 = 0, . . . , 𝐿}), and the final node representation z 𝑣 as follow:</p><formula xml:id="formula_2">z 𝑣 = 𝐿 ∑︁ 𝑜=0 Pr(𝑂 𝑣 = 𝑜)h (𝑜) 𝑣 .<label>(2)</label></formula><p>In the other design, we let the controller to make a choice from the search space O = {0, 1} at each layer, where "1" means to terminate at that layer. Then we define the node-wise and layerwise distribution by Pr(𝑂</p><formula xml:id="formula_3">(𝑙) 𝑣 ) = 𝑔 (𝑑) ({h (𝑙)</formula><p>𝑢 |𝑢 ∈ N (𝑣) ∪ {𝑣 }}) and calculate the final node representation of a node 𝑣 as follow:</p><formula xml:id="formula_4">z 𝑣 = 𝐿 ∑︁ 𝑙=0 (Pr(𝑂 (𝑙) 𝑣 = 1) 𝑙−1 𝑘=0 (1 − Pr(𝑂 (𝑘) 𝑣 = 1)))h (𝑙) 𝑣 ,<label>(3)</label></formula><p>where the products express the probability of being terminated at the 𝑙-th layer but not any of the previous layers.</p><p>Aggregator controller. In addition to the dimension-wise pooling operations, we also include a special "self_msg" operation which receives the message of the target node itself while ignoring any incoming message. Then the search space of our aggregator controller can be expressed as O = {max, min, add, mean, self_msg}. With the aggregator controller 𝑔 (a) (•), the first step in each message passing iteration is to predict the aggregator to be applied, according to Pr(𝑂</p><formula xml:id="formula_5">(𝑙) 𝑣 ) = 𝑔 (a) ({h (𝑙−1) 𝑢 |𝑢 ∈ N (𝑣) ∪ {𝑣 }}). Given the predicted dis- tribution Pr(𝑂 (𝑙)</formula><p>𝑣 ) over O, the message passing procedure defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>) becomes:</p><formula xml:id="formula_6">h (𝑙) 𝑣 = 𝜎 ( ∑︁ 𝑜 ∈ O Pr(𝑂 (𝑙) 𝑣 = 𝑜)𝑜 𝑢 ∈N (𝑣)∪{𝑣 } (𝜙 (h (𝑙−1) 𝑢 ))).<label>(4)</label></formula><p>In most practical cases, 𝜙 (𝑙) (•) defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is implemented by an MLP. Since the different choices of the aggregator often lead to drastically different statistics of their outputs <ref type="bibr" target="#b39">[37]</ref>, it would be unstable in optimizing a 𝜙 (𝑙) (•) shared among the candidate aggregators. Thus, we allow each aggregator 𝑜 ∈ O to have a dedicated transformation 𝜙</p><formula xml:id="formula_7">(𝑙) 𝑜 (•).</formula><p>Resolution controller. Sampling is indispensable when we train a GNN model on large graphs because an entire three-hop neighborhood can often fail to fit into the GPU memory, not to mention a larger neighborhood. In this paper, we consider one of the most widely adopted samplers-neighbor sampler <ref type="bibr" target="#b10">[8]</ref>, where a fixed number of nodes are sampled in each hop. We define a GNN layer's resolution as the number of nodes sampled in the corresponding hop and regard resolution as one aspect of GNN architecture. Then the search space of the resolution controller consists of several concrete resolution configurations, e.g., O = {15-10-5, 14-10-7, 16-8-5}, where "15-10-5", means sampling 15, 10, and 5 neighbors in the 3, 2, and 1-hop, respectively. It is worth noting that, in most cases, sampling is conducted only for training but evaluation, where the estimated node embedding ĥ(𝑙) 𝑣 and the exact node embedding h</p><formula xml:id="formula_8">(𝑙) 𝑣</formula><p>are calculated based on a sampled or an entire neighborhood of 𝑣, respectively. In each time of evaluation, we infer the exact node embeddings for all the nodes and maintain their final node representations, i.e., z 𝑣 , 𝑣 ∈ V. We assume z 𝑣 is informative for determining the suitable resolution for node 𝑣 and choose the resolution for it according to 𝑜 ∼ Pr(𝑂 𝑣 ) = 𝑔 (r) (h</p><formula xml:id="formula_9">(0)</formula><p>𝑣 , z 𝑣 , 𝑦 𝑣 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modelling and Optimization</head><p>In the proposed framework, controllers are not restricted to any particular functional form. We discuss some choices to show what patterns in a node's context should be recognized by the controllers for making their decisions. For 𝑔 (d) ({h</p><formula xml:id="formula_10">(𝑙) 𝑣 , 𝑙 = 1, . . . , 𝐿}), we can feed (h (1) 𝑣 , . . . , h (𝐿)</formula><p>𝑣 ) into a LSTM <ref type="bibr" target="#b12">[10]</ref> and produce the distribution based on its last hidden state. In this way, the order matters, which reflects the intuition that the depth controller observes how the node embedding changes along the iterations of message passing and then attends to the suitable layer. For 𝑔 (d) ({h (𝑙) 𝑢 |𝑢 ∈ N (𝑣) ∪ {𝑣 }}), the intuition is to compare the embedding of target node h</p><formula xml:id="formula_11">(𝑙)</formula><p>𝑣 with the embeddings of its neighbors, so that the controller can determine to terminate at the current layer when it finds that the embeddings have been similar to some extent. Thus, a simple choice is to define Pr(𝑂</p><formula xml:id="formula_12">(𝑙) 𝑣 = 1) = 1 1+exp{𝑎 } with 𝑎 = 𝑏 + 1 |N (𝑣) | 𝑢 ∈N (𝑣) (h (𝑙) 𝑣 ) T Wh (𝑙)</formula><p>𝑢 , where 𝑏 and W are trainable parameters of 𝑔 (d) (•). For 𝑔 (a) ({h (𝑙−1) 𝑢 |𝑢 ∈ N (𝑣) ∪ {𝑣 }}), we aim to let the controller choose suitable aggregator based on some basic statistics of the neighbors' embeddings. Thus, we can parameterize 𝑔 (𝑎) (•) as an MLP fed with the concatenation of h  <ref type="formula" target="#formula_4">3</ref>), and Eq. ( <ref type="formula" target="#formula_6">4</ref>)), we notice that the final node representation z 𝑣 depends on both 𝝓 and 𝜽 . Intuitively, the controllers make predictions for each node based on the node embeddings calculated by the backbone GNN model, while the backbone GNN model depends on the controllers to predict the suitable architectures. Thus, to encourage their cooperation, we learn 𝝓 and 𝜽 jointly. On the one hand, only 𝝓 d and 𝝓 a directly participate in the forward propagation to produce z 𝑣 and thus can be optimized in a differentiable manner, e.g., making gradient descent like DARTS <ref type="bibr" target="#b21">[19]</ref> or making exponentiated gradient descent like GAEA <ref type="bibr" target="#b19">[17]</ref>. On the other hand, we regard the resolution controller 𝑔 𝝓 r (•) as a parametric policy with action space (i.e., candidate resolutions) O r , from which we sample the resolutions. Since we cannot directly calculate the gradients of z 𝑣 w.r.t. the sampled resolutions, we adopt policy gradient method <ref type="bibr" target="#b27">[25]</ref> to optimize it. The goal of 𝑔 𝝓 r (•) is to select a suitable resolution configuration for each node 𝑣, such that the estimated node embedding ĥ(𝑙) 𝑣 can better approximate the exact one h (𝑙) 𝑣 . To this end, we design the reward function to be:</p><formula xml:id="formula_13">𝑅(𝑣, 𝑜) = −∥z 𝑣 − ẑ𝑣 ∥ 2 2 , 𝑣 ∈ V, 𝑜 ∈ O r .</formula><p>The pseudo-code for learning both 𝝓 and 𝜽 can be found in Algorithm 1.</p><p>Algorithm 1 Learning a GNN with node-wise architecture.</p><p>Input: Graph 𝐺 = (V, E) with splits V train and V valid , Graphs sampler 𝑆, learning rate 𝛼, and #epochs 𝑇 . Output: Learned parameters 𝝓 = (𝝓 d , 𝝓 a , 𝝓 r ) and 𝜽 .</p><p>1: Randomly initialize 𝝓 (0) and 𝜽 (0) ; 2: for 𝑡 = 0, 1, . . . ,𝑇 − 1 do</p><formula xml:id="formula_14">3: Infer z 𝑣 for 𝑣 ∈ V by 𝜽 (𝑡 ) , 𝝓 (𝑡 ) d , 𝝓 (𝑡 ) a 4: 𝑜 𝑣 ∼ Pr 𝝓 r (𝑂 𝑣 ) = 𝑔 𝝓 (𝑡 ) r (h (0) 𝑣 , z 𝑣 , 𝑦 𝑣 ) for 𝑣 ∈ V valid ; 5: 𝐺 valid ∼ 𝑆 (𝐺, 𝑣, 𝑜 𝑣 ), 𝑣 ∈ V valid ;</formula><p>// graph sampling 6:</p><p>Infer ẑ𝑣 for 𝑣 ∈ V valid from 𝐺 valid by 𝜽 (𝑡 ) , 𝝓</p><formula xml:id="formula_15">(𝑡 ) d , 𝝓<label>(𝑡 ) a ; 7:</label></formula><p>𝝓 </p><formula xml:id="formula_16">(𝑡 +1) r ← 𝝓 (𝑡 ) r +𝛼∇ 𝝓 r 1 | V valid | 𝑣 ∈V valid 𝑅(𝑣,</formula><formula xml:id="formula_17">𝜽 (𝑡 +1) ← 𝜽 (𝑡 ) − 𝛼∇ 𝜽 L train (𝝓 (𝑡 +1) d , 𝝓 (𝑡 +1) a</formula><p>, 𝜽 (𝑡 ) ); // gradient descent 12: end for 13: return 𝝓 (𝑇 ) and 𝜽 (𝑇 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Our proposed framework provides a unified view for achieving GNN with node-wise architecture, which enables: (1) controllers responsible for different aspects to be fed with respective information as if it is suitable for determining the corresponding aspect; (2) new aspects of the architecture to be added; (3) controllers to be optimized jointly with the backbone GNN model, no matter they are differentiable or not. Connections to related works. There are several recent works that our framework can express. Ala-GCN <ref type="bibr" target="#b32">[30]</ref> terminates the iteration of message passing when an indicator for over-smoothing is active, which is roughly the similarity between h 𝑣 and {h 𝑢 |𝑢 ∈ N (𝑣)} and can be regarded as a non-parametric version of Eq. ( <ref type="formula" target="#formula_6">4</ref>). IterGNN <ref type="bibr" target="#b28">[26]</ref> determines the depth of GNN on-the-fly similarly as Eq. ( <ref type="formula" target="#formula_4">3</ref>), but not in a node-wise manner. Policy-GNN <ref type="bibr" target="#b16">[14]</ref> also uses a policy to determine each node's depth, where policy gradient method is utilized; Considering RL's notorious sample complexity and the massive variance of policy gradients, we prefer to update the controllers by gradient descent unless they are not differentiable. PNA <ref type="bibr" target="#b5">[3]</ref> improves the expressiveness by considering a mixture of different aggregators, where the mixing coefficients are not flexibly determined based on the local patterns of each node similar to our aggregator controller. More methods that determine a specific aspect of each node's architecture include JKNet <ref type="bibr" target="#b34">[32]</ref>, GAT <ref type="bibr" target="#b29">[27]</ref>, and GeniePath <ref type="bibr" target="#b22">[20]</ref>. When our framework is restricted to only one controller of their corresponding aspect, our framework degenerates to these methods. Significance. Generalizing a single aspect of architecture to more than one is nontrivial, as the enlarged search space poses difficulties in learning the controllers. To the best of our knowledge, we are the first attempt to consider more than one aspect and provide rigorous sample complexity studies about this generalization (see Sec. 4.3). Besides, we introduce the concept of resolution for GNN architecture and design a corresponding controller, which has not been considered before but is helpful for handling large graphs (see Sec. 4.1.2). More importantly, our framework is motivated by handling the diverse local patterns ingrained in a graph. In contrast, prior works, e.g., Ala-GCN and Policy-GNN, focus on utilizing nodewise depth to alleviate the over-smoothing issue. To confirm our motivation, we will empirically show in Sec. 4.1.1 the advantages of our framework in performing well on the graphs where diverse levels of local assortativity exist. GNN architectures with fixed spectral property (e.g., low-pass) cannot perform well on such graphs, while GPR-GNN achieves satisfactory performances by explicitly adjusting the graph spectrum. In this sense, our empirical results provide an exciting motivation for node-wise architecture-the potential of promoting an ordinary GNN architecture to express a more broad scope of graph filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first compare the performance of our proposed framework with state-of-the-art methods on several real-world datasets. Then we empirically justify the effects of depth, aggregator, and resolution controllers, respectively. To better understand both the benefits and burdens of node-wise architecture, we conduct experiments to study the sample efficiency and running time. To keep our notations terse, we use NW-GNN to denote the instantiation of our proposed framework. Datasets. We follow <ref type="bibr" target="#b4">[2]</ref> to adopt ten node classification datasets that are diverse enough for making a fair and comprehensive comparison. We defer their details to Appendix (refer to Table <ref type="table">6</ref> for their statistics). Their sizes (i.e., number of nodes) vary in a broad range. Moreover, according to their levels of homophily, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results on Real-world Datasets</head><formula xml:id="formula_18">H (𝐺) = 1 |𝑉 | 𝑣 ∈𝑉 | {𝑢 |𝑢 ∈N (𝑣)∧𝑦 𝑢 =𝑦 𝑣 } | | N (𝑣) |</formula><p>[22], the first five datasets are homophilic, while the last five ones are heterophilic. Settings. We follow the "dense split" setting of <ref type="bibr" target="#b4">[2]</ref>, where the node set of each graph is randomly partitioned into train/valid/test sets with a ratio of 60%/20%/20%. We produce ten random splits on each dataset and conduct hyper-parameter optimization (HPO) for each method, where the optimal hyper-parameter configuration is determined w.r.t. the mean accuracy over the valid sets. We report the mean accuracy on the test sets for comparison. Methods. We instantiate our proposed framework by incorporating a vanilla backbone GNN model with depth and aggregator controllers. We defer the study about resolution controller to Sec. 4.1.2 because the scales of the datasets considered here allow full-batch training. Then we categorize our baselines into three classes:</p><p>(1) Manually designed architectures: Conventionally, we adopt MLP and the widely adopted GNN architectures including ChebyNet <ref type="bibr" target="#b6">[4]</ref>, GCN <ref type="bibr" target="#b14">[12]</ref>, GraphSAGE <ref type="bibr" target="#b10">[8]</ref>, GIN <ref type="bibr" target="#b33">[31]</ref>, APPNP <ref type="bibr" target="#b15">[13]</ref>, and GPR-GNN <ref type="bibr" target="#b4">[2]</ref> as the baselines to be compared.</p><p>(2) NAS without node-wise architecture: Then a NAS-related baseline naturally comes up, which uses the same backbone and search spaces as that of the proposed method NW-GNN. Specifically, this baseline exhaustively enumerates the possible depths and aggregators and applies the searched optimal architecture to all the nodes equally. We call this baseline NAS * for short, which serves as NW-GNN's optimal counterpart under the standard NAS setting.</p><p>(3) Single-aspect Node-wise architectures: We consider GAT <ref type="bibr" target="#b29">[27]</ref>, JKNet <ref type="bibr" target="#b34">[32]</ref>, PNA <ref type="bibr" target="#b5">[3]</ref>, and Policy-GNN <ref type="bibr" target="#b16">[14]</ref> which can be regarded as adaptively determining the incoming neighbors, depth, aggregator, and depth, respectively. We implement NW-GNN and NAS * with the open-source GNN package-GraphGym <ref type="bibr" target="#b36">[34]</ref>, and the other baselines with their available open-source implementations. More details about implementation can be found in Appendix B. Results and Analysis. We present the results in Table <ref type="table" target="#tab_1">1</ref>, where the bold letters imply the best result on each dataset. Overall, NW-GNN achieves best performances on half of the ten datasets, where none or at most one baseline can reach its 95% confidence interval.</p><p>(1) NW-GNN is versatile: The datasets on which NW-GNN outperforms the baselines include both homophilic and heterophilic graphs. No graph convolution-based methods (i.e., all except for MLP, APPNP, and GPR-GNN) can lead on them simultaneously. For example, GCN consistently performs well on homophilic graphs, but its performances on heterophilic graphs fall behind the leading ones by significant margins.</p><p>(2) A single searched architecture is insufficient: Compared to GCN, NAS * can use different architectures on different graphs and leads to relatively balanced performances on these two genres of graphs. However, it entirely fails to handle the heterophilic graph Actor. In contrast, NW-GNN exhibits competitive performances on all these graphs. As NAS * takes a single choice of the optimal depth and aggregator, it essentially serves as ablation of node-wise architecture. Meanwhile, some recent studies have shown that achieving satisfactory performances on these considered heterophilic graphs depends more on the capacity of addressing the diverse levels of assortativity ingrained in the graph. Therefore, we attribute NW-GNN's versatility to its capability of adjusting depth and aggregator in a node-wise manner.</p><p>(3) Node-wise architecture really helps for handling diverse local patterns: Let us take Chameleon and Squirrel as examples, where the local assortativity of node span in a broad range (see Fig. <ref type="figure">7</ref> in Appendix). The larger local assortativity is, the more homophilic a node is. As illustrated in Fig. <ref type="figure" target="#fig_5">3</ref>, NW-GNN consistently outperforms those traditional graph convolution-based models on all the levels  of local assortativity. Considering that GPR-GNN adjusts the graph spectrums directly to handle both genres of graphs well, our architecture controllers have the potential to extend the expressiveness of an ordinary GNN, acting a more broad scope of graph filters. (4) Our controllers can choose the "ground-truth" architecture: All GNN-based methods are outperformed by MLP on Cornell, and NW-GNN outperforms most graph convolution-based methods on Cornell, where the topological information might not be helpful for node classification. Our advantage is rooted in the correct decisions made by our aggregator controller, where the "self_msg" operator is preferred. In this way, NW-GNN can degenerate into an MLP. As for the other graph convolution-based methods, although they either add self-loop or consider a residual path, the weights for in-coming and self messages cannot be adjusted as effectively as our aggregator controller does. The assertion that topological information is not helpful is supported by the learned spectral coefficients of GPR-GNN. Among the polynomial coefficients learned by GPR-GNN, only the coefficient of the zero-order term is a large positive number; those corresponding to higher-order terms are close to zero.</p><p>To better understand the behavior of the controllers, it would be helpful to show the relationships between the local patterns and the searched node-wise architectures on these datasets. However, in our controllers, node features and topological structures together determine the predicted architectures. Thus, identifying such relationships is similar to explaining the behavior of GNNs, where typical algorithms such as GNNExplainer <ref type="bibr" target="#b35">[33]</ref> cannot provide satisfactory explanations on these adopted datasets. Therefore, we have to study such relationships on synthetic datasets in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Study on Large-scale Graph.</head><p>Protocol. To evaluate the proposed resolution controller, we conduct experiment on a real-world large-scale dataset ogbn-products <ref type="bibr" target="#b13">[11]</ref>. This dataset provides a node classification task on a graph with 2,449,029 nodes and 61,859,140 edges. Its scale makes full-batch training of GNN impracticible, and thus graph sampling becomes necessary. Hence, we adopt GraphSAGE as the backbone and train the model with neighbor sampler <ref type="bibr" target="#b10">[8]</ref>. Specifically, we aim to train a three-layer GraphSAGE model, which, at each step, requires the 3-hop neighborhood of each target node in the current mini-batch to be sampled. Without resolution controller, the sampler uses the default resolution "15-10-5", that is, to sample 15, 10, and 5 neighbors at the 1th, 2nd, and 3rd hop, respectively. When coupled with our resolution controller, its search space consists of "15-10-5", "14-12-5", "16-10-2", and "15-9-7". The controller is responsible for determining a resolution from this space in each step, based on the context (i.e., local patterns). We update the controller according to what we have elaborated in Sec. 3.2. In this experiment, we focus on comparing GraphSAGE without the resolution controller (denoted by "w/o") to that with the resolution controller (denoted by "w/"). We run each setting for ten times and report its mean test accuracy. Results and Analysis. We show the experimental results in Table 2, where the performance of the baseline (i.e., "w/o") is directly copied from the leaderboards of OGB. GraphSAGE with our resolution controller (i.e., "w/") outperforms that without a controller, where one std. below the mean of the former is still higher than the mean of the latter. This comparison confirms the advantages of adaptive node-wise resolutions in training GNNs with a graph sampler, which also suggests the effectiveness of our proposed resolution controller. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Justification of the Controllers' Effects</head><p>We have shown the benefits of node-wise architecture via the advantages of NW-GNN on real-world datasets, where the impacts of the introduced controllers entangle together. In the following experiments, we study the effects of depth, aggregator, and resolution controllers on three synthetic datasets, each of which calls for node-wise depth, aggregator, and resolution, respectively. More details about the construction of these synthetic datasets and the implementations can be found in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Effect of Node-wise Depth.</head><p>For each node in our randomly generated graph, we label it by counting the number of nodes in its 𝑘-hop neighborhood, where 𝑘 ∼ Uniform({1, 2, 3, 4}). We consider three different settings about node attributes: (1) "w/o feat.": no node attribute; (2) "hard feat.": the ground-truth depth (i.e., 𝑘) for each node can be inferred from its local information; and (3) "easy feat.": the ground-truth depth for each node is given in its attributes. We compare NW-GNN to both GCN and GPR-GNN on this node regression task, where mean absolute error (MAE) on the test set is reported. The results are presented in Table <ref type="table" target="#tab_3">3</ref> where NW-GNN is comparable w.r.t. GCN when there is no node attribute while surpasses all the baselines by remarkable margins under the other two settings. We attribute these advantages to our capability of using node-wise depths, so that our method can make predictions by directly approximating the generation process of ground-truth labels. To validate this capability, we conduct a case study about the relationships between our controller's predicted depth and the ground-truth depth and present the results in Fig. <ref type="figure" target="#fig_6">4</ref>. Although the predictions made by our controller are imperfect, the average predicted depth positively correlates with the ground-truth depth. Meanwhile, under the "easy feat." setting, the MAE upon test nodes that have a correctly predicted depth is 0.15 while that upon test nodes with wrong depths is 0.53. These observations confirm the importance of using node-wise depth and the advantage of our depth controller over any fixed depth, which justify our conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of Node-wise Aggregator.</head><p>For each node in our graph, we randomly generate a numeric attribute by sampling from Uniform([−1.0, 1.1]) and two 5-dimensional one-hot attributes. Then we label each node by propagating their numeric attribute for two iterations, where the aggregators used are indicated by its two one-hot attributes, with the choices from {max, min, add, mean, self_msg}. We consider three different settings about node attributes: (1) "w/o feat.": Only the numeric attribute; (2) "hard feat. ": The one-hot attribute corresponding to the second iteration of message passing is given; and (3) "easy feat.": The two one-hot attributes are given. We compare NW-GNN to Message Passing Neural Networks (MPNN) <ref type="bibr" target="#b8">[6]</ref>, GCN, and GPR-GNN on this node regression task, where the mean absolute error (MAE) on the test set is adopted as the measure of performance. We present the results in Table <ref type="table" target="#tab_4">4</ref>, where NW-GNN surpasses all the baselines under all the settings. To validate that the advantages of NW-GNN come from its capacity of choosing aggregators in a node-wise manner, we conduct a case study for NW-GNN under the "easy feat." setting. Specifically, we regard predicting the groundtruth aggregator as a 5-class classification task, and the accuracy of our controller is 54%. Meanwhile, the MAE on test nodes whose aggregators are correctly predicted is 0.78 while that on incorrectly predicted nodes is 2.02. These results confirm that NW-GNN can achieve node-wise aggregator for the backbone GNN model. We construct a synthetic dataset and define a node classification task on it. At first, we generate 150 graphs, each of which is a tree, consisting of a root node, five child nodes, and twenty-five leaf nodes (five for each child). Then we randomly label each root node as either positive or negative and generate the attributes for the nodes according to its label. In detail, if the root node is a positive example, we allow fifteen leaf nodes to have attributes drawn from N (•|1, 0.5) while the other ten have attributes drawn from N (•|0, 0.5). Otherwise, we allow only twelves leaf nodes to have attributes drawn from N (•|1, 0.5). If we allocate the attributes sampled from N (•|1, 0.5) for the five child nodes as equally as possible, the graph belongs to Group1. If we allocate the attributes sampled from N (•|1, 0.5) to the five child nodes with splits as skewed as possible, the graph belongs to Group2. Finally, we randomly split the generated graphs into equal-sized train/valid/test sets.  To study the impact of the resolution controller, we adopt a twolayer GraphSAGE and neighbor sampler <ref type="bibr" target="#b10">[8]</ref> as our testbed, where the fixed resolutions {1-5, 2-3, 3-2, 5-1} are considered as baselines and the search space of our resolution controller. Meanwhile, we consider both the default mean pooling and the max pooling for the GraphSAGE models for the generality reason. We repeat the training course of each method ten times, each of which consists of 20 epochs.</p><p>The results are shown in Fig. <ref type="figure" target="#fig_8">5a</ref>, where NW-GNN outperforms the fixed resolutions, whichever aggregator the backbone model uses. We conduct a 𝑡-test for the results, where the improvement of NW-GNN is significant (𝑝-value &lt; 0.05) under the "mean pooling" setting but not significant under the "max pooling" setting.</p><p>Furthermore, we conduct a case study to understand the behavior of the resolution controller. Specifically, we collect the predictions made by our resolution controller for each graph in the valid set and then average the predictions for Group1 and Group2, respectively. We present the averaged predictions for each group at the first five epochs in Fig. <ref type="figure" target="#fig_8">5b</ref>, where the height of each bar reflects the probability of the corresponding resolution. The bar with or without shadow corresponds to Group1 or Group2, respectively. Under both the "mean pooling" and the "max pooling" settings, the controller learns to use resolution "1-5" for Group1 while use resolution "5-1" for Group2. For the graphs in Group2, since the attributes are allocated for the child nodes unequally, the mean of the attributes in each child's leaf nodes varies a lot among the child nodes. Thus, applying resolution "5-1" for Group2 reduces the variance, which explains the improvements brought in by the resolution controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Analysis</head><p>We have validated the benefits brought in by the designed controllers. Meanwhile, as they also introduce additional trainable parameters, in this section, we study their influences from both sample efficiency and running time. We use "w/o depth" and "w/o aggr" to indicate NW-GNN with only aggregator controller or depth controller, respectively. Sample Efficiency. The methods considered in this comparison include GCN, NW-GNN, and NW-GNN with ablation of either the depth or the aggregator controller. In addition to the "dense split" (i.e., 60%/20%/20%) used in Sec. 4.1, we also consider splits "𝑥/𝑥/(1 − 2𝑥)" with 𝑥 ∈ {2.5%, 5%, 10%, 20%}. The results are shown in Fig. <ref type="figure" target="#fig_10">6</ref>. Overall, the performances of each method grow with the increase of training set ratio. On Cora, a typical homophilic graph, GCN benefits from its low-pass filter nature and outperforms NW-GNN when the training set is very limited. However, NW-GNN  exhibits a much steeper growth curve and overtakes GCN with the "dense split". On Chameleon, NW-GNN surpasses GCN under different settings consistently, and the slopes of their performance curves are comparable. These observations suggest that the sample efficiency of NW-GNN is at least as good as GCN, where the additional parameters introduced by our controllers will not be a burden. For the ablation study, NW-GNN leads on both the datasets under the "dense split" setting, but, with a smaller training set, the removal of a controller (e.g., the depth controller or the aggregator controller) may improve the performances. This phenomenon is caused by the decrease in the number of trainable parameters. Running Time. We present the running time for each method in Table <ref type="table" target="#tab_5">5</ref>, where all training processes are executed on a NVIDIA GeForce RTX 2080 Ti GPU (12GB memory). NW-GNN is slower than other methods due to the existence of the controllers, which make inferences about the architectures before the execution of graph convolution operation. Compared to the SOTA method GPR-GNN, such increase is around 2∼5 times, which is tolerable in practice, considering the performance improvement shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE DIRECTIONS</head><p>Motivated by observations with GNN applications, we propose a framework to enable the node-wise architecture for GNN models, in which the designed context-aware controllers can automatically utilize the local information of each node. A series of experiments show that the proposed framework outperforms state-of-the-art methods on six real-world datasets. Since no existing graph convolutionbased model can simultaneously win on such a collection of various graphs, we can confirm that node-wise architecture can make the inflexible GNN models versatile. In this paper, We have focused on demonstrating the effects of node-wise depth, aggregator, and resolution and how the corresponding controllers work. In the future, more additional controllers can be added as other instantiations of the proposed framework, which deserves further studies. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples that motivate node-wise architecture for GNN. (a) Each row denotes a message propagation process, where red color indicates the message has been propagated to the node. (b) Nodes with red color are target nodes to predict. The numbers shown in yellow color nodes denote their messages to be propagated towards the target nodes. (c) Blue color nodes and yellow color nodes are 1-hop and 2-hop neighbors of the red color node, where the numbers denotes the node attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example to illustrate the difference between node-wise architecture and using the same architecture.</figDesc><graphic url="image-9.png" coords="3,54.25,274.66,239.21,125.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>𝑙−1) 𝑣 and the max, min, add, and mean values of {h (𝑙−1) 𝑢 , 𝑢 ∈ N (𝑣)}. As for 𝑔 (r) 𝜙 (h (0) 𝑣 , z 𝑣 , 𝑦 𝑣 ), we simply feed an MLP with the concatenation of h (0) 𝑣 , z 𝑣 , and 𝑦 𝑣 . Let us denote the parameters of the backbone GNN model by 𝜽 , the parameters of the controllers by 𝝓 = (𝝓 d , 𝝓 a , 𝝓 r ) where the subscripts imply the corresponding controllers. According to Eq. (2), Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 . 1</head><label>11</label><figDesc>Study on Both Homophilic and Heterophilic Graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performances v.s. local assortativity on two heterophilic graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Correlatioin between predicted and ground-truth depths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The predictions of resolution controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of the node-wise resolution study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of the sample efficiency study.</figDesc><graphic url="image-12.png" coords="8,440.17,83.91,115.31,87.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝑜 𝑣 ) log Pr 𝝓 r (𝑂 𝑣 =</figDesc><table><row><cell></cell><cell cols="2">𝑜 𝑣 );</cell><cell cols="4">// policy gradients</cell></row><row><cell>8:</cell><cell>𝝓</cell><cell>(𝑡 +1) d</cell><cell>← 𝝓</cell><cell>(𝑡 )</cell><cell></cell><cell></cell><cell>(𝑡 +1) a</cell><cell>←</cell></row><row><cell></cell><cell>𝝓</cell><cell>(𝑡 )</cell><cell></cell><cell></cell><cell>(𝑡 ) d , 𝝓</cell><cell>(𝑡 ) a , 𝜽 (𝑡 ) );</cell><cell>// gradient descent</cell></row><row><cell>9:</cell><cell cols="3">r 𝑜 𝑣 ∼ 𝑔 𝝓 (𝑡 +1)</cell><cell>(h</cell><cell>(0)</cell><cell></cell></row></table><note>d − 𝛼∇ 𝝓 d L valid (𝝓 (𝑡 ) d , 𝝓 (𝑡 ) a , 𝜽 (𝑡 ) ) and 𝝓 a − 𝛼∇ 𝝓 a L valid (𝝓 𝑣 , z 𝑣 , 𝑦 𝑣 ) for 𝑣 ∈ V train ; 10: 𝐺 train ∼ 𝑆 (𝐺, 𝑣, 𝑜 𝑣 ) for 𝑣 ∈ V train ; // graph sampling 11:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on real-world datasets: Mean accuracy (%) with its 95% confidence interval.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell></cell><cell cols="2">PubMed</cell><cell cols="2">Computers</cell><cell>Photo</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell><cell>Cornell</cell></row><row><cell></cell><cell></cell><cell>MLP</cell><cell></cell><cell>76.99±1.40</cell><cell cols="2">75.11±1.24</cell><cell cols="2">86.80±0.40</cell><cell cols="2">84.58±0.60</cell><cell>88.90±0.52</cell><cell>46.93±1.54</cell><cell>39.22±0.66</cell><cell>30.62±0.80</cell><cell>90.49±4.05 90.65±4.09</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell>87.22±0.74</cell><cell cols="2">79.86±0.78</cell><cell cols="2">88.80±0.53</cell><cell cols="2">88.57±0.54</cell><cell>93.13±0.27</cell><cell>60.53±1.35</cell><cell>33.98±0.76</cell><cell>46.78±0.89</cell><cell>77.38±4.18</cell><cell>76.07±4.80</cell></row><row><cell></cell><cell cols="3">ChebyNet</cell><cell>86.96±0.72</cell><cell cols="2">79.29±1.14</cell><cell cols="2">88.92±0.36</cell><cell cols="2">89.21±0.37</cell><cell>94.87±0.22</cell><cell>61.31±1.36</cell><cell>39.46±0.75</cell><cell>42.32±0.93</cell><cell>86.06±2.14</cell><cell>82.45±2.61</cell></row><row><cell></cell><cell cols="4">GraphSAGE 87.63±1.56</cell><cell cols="2">79.78±0.82</cell><cell cols="2">90.29±0.61</cell><cell cols="2">90.53±0.31</cell><cell>94.60±0.25</cell><cell>65.51±1.36</cell><cell>40.63±0.85</cell><cell>48.99±0.65</cell><cell>79.03±1.20</cell><cell>71.41±1.23</cell></row><row><cell></cell><cell></cell><cell>GIN</cell><cell></cell><cell>83.74±0.90</cell><cell cols="2">75.95±1.20</cell><cell cols="2">88.38±0.40</cell><cell cols="3">57.18±0.21 69.03±22.38 40.18±13.74 32.81±1.05</cell><cell>28.70±2.77</cell><cell>79.67±3.78</cell><cell>78.36±1.56</cell></row><row><cell></cell><cell cols="3">APPNP</cell><cell>88.10±0.73</cell><cell cols="2">79.58±0.70</cell><cell cols="2">88.35±0.23</cell><cell cols="2">86.38±0.39</cell><cell>93.43±0.32</cell><cell>53.76±1.44</cell><cell>39.55±1.01</cell><cell>36.40±1.50</cell><cell>88.36±2.59</cell><cell>90.00±2.71</cell></row><row><cell></cell><cell cols="6">GPR-GNN 88.48±0.51 79.49±1.15</cell><cell cols="2">90.90±0.65</cell><cell cols="2">88.70±0.45</cell><cell>93.95±0.44</cell><cell>67.26±1.49</cell><cell>40.74±0.53</cell><cell>52.31±1.09 91.48±2.02 89.67±2.65</cell></row><row><cell></cell><cell></cell><cell>NAS  *</cell><cell></cell><cell>87.09±1.08</cell><cell cols="2">80.27±0.89</cell><cell cols="2">88.32±0.56</cell><cell cols="2">89.78±0.18</cell><cell>93.30±0.26</cell><cell>67.66±1.25</cell><cell>34.58±0.54</cell><cell>55.79±1.52</cell><cell>89.74±3.65</cell><cell>90.59±3.95</cell></row><row><cell></cell><cell></cell><cell>GAT</cell><cell></cell><cell>88.03±0.62</cell><cell cols="2">80.70±0.60</cell><cell cols="2">88.13±0.59</cell><cell cols="2">90.28±0.29</cell><cell>93.60±0.36</cell><cell>66.48±1.02</cell><cell>35.98±0.23</cell><cell>53.31±1.16</cell><cell>78.87±0.86</cell><cell>76.00±1.01</cell></row><row><cell></cell><cell></cell><cell>JKNet</cell><cell></cell><cell>87.08±0.89</cell><cell cols="2">77.86±0.75</cell><cell cols="2">87.68±0.42</cell><cell cols="2">86.91±1.14</cell><cell>92.55±0.57</cell><cell>64.20±1.92</cell><cell>33.64±0.56</cell><cell>44.72±0.48</cell><cell>75.53±1.16</cell><cell>66.73±1.73</cell></row><row><cell></cell><cell></cell><cell>PNA</cell><cell></cell><cell>83.71±1.14</cell><cell cols="2">74.76±1.19</cell><cell cols="2">83.38±0.75</cell><cell cols="2">89.88±0.60</cell><cell>93.13±0.27 72.32±1.28 30.29±0.76</cell><cell>55.20±2.60</cell><cell>78.52±4.58</cell><cell>67.05±5.56</cell></row><row><cell></cell><cell cols="8">Policy-GNN 87.88±1.98 82.92±8.32 87.76±0.70</cell><cell>N/A</cell><cell></cell><cell>89.88±2.32</cell><cell>68.07±1.51</cell><cell>35.36±0.39</cell><cell>55.76±2.18 77.55±12.12 77.61±1.76</cell></row><row><cell></cell><cell cols="3">NW-GNN</cell><cell>88.03±0.78</cell><cell cols="7">79.36±0.89 91.22±0.38 91.27±0.14 95.12±0.23 69.06±0.93 44.48±0.69 56.64±0.48 81.80±3.59</cell><cell>84.26±3.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>on Chameleon</cell><cell></cell><cell></cell><cell></cell><cell cols="2">on Squirrel</cell><cell></cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean Accuracy</cell><cell>0.6 0.8</cell><cell>GIN GAT NW-GNN</cell><cell></cell><cell></cell><cell>Mean Accuracy</cell><cell>0.8 0.4 0.6</cell><cell>GIN GAT NW-GNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">−0.2 0.4</cell><cell cols="2">0.0 Local Assortativity 0.2 0.4</cell><cell>0.6</cell><cell cols="2">−0.2</cell><cell cols="2">0.0 Local Assortativity 0.2 0.4</cell><cell>0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>GraphSAGE w/o or w/ the resolution controller on ogbn-products: Mean test accuracy (%) with Std. deviation.</figDesc><table><row><cell>Metric</cell><cell>w/o</cell><cell>w/</cell></row><row><cell cols="3">Mean test accuracy 78.29±0.16 78.92±0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of the node-wise depth study: Mean absolute error (MAE) with its 95% confidence interval.</figDesc><table><row><cell cols="2">Method</cell><cell>w/o feat.</cell><cell>hard feat.</cell><cell>easy feat.</cell></row><row><cell></cell><cell>𝐿 = 2</cell><cell cols="2">1.95±0.00 1.88±0.01</cell><cell>1.46±0.06</cell></row><row><cell>GCN</cell><cell>𝐿 = 3</cell><cell>1.96±0.00</cell><cell>1.95±0.01</cell><cell>1.61±0.03</cell></row><row><cell></cell><cell>𝐿 = 4</cell><cell cols="2">1.95±0.00 1.98±0.01</cell><cell>1.67±0.03</cell></row><row><cell></cell><cell cols="2">𝛼 = 0.1 2.22±0.00</cell><cell>2.21±0.00</cell><cell>1.83±0.02</cell></row><row><cell>GPR-GNN</cell><cell cols="2">𝛼 = 0.5 2.08±0.01</cell><cell>2.09±0.01</cell><cell>1.79±0.02</cell></row><row><cell></cell><cell cols="2">𝛼 = 0.7 2.11±0.03</cell><cell>2.08±0.04</cell><cell>1.81±0.05</cell></row><row><cell cols="2">NW-GNN</cell><cell cols="3">1.97±0.01 1.67±0.09 0.38±0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of the node-wise aggregator study: Mean absolute error (MAE) with its 95% confidence interval.</figDesc><table><row><cell>Method</cell><cell>w/o feat.</cell><cell>hard feat.</cell><cell>easy feat.</cell></row><row><cell>add</cell><cell>1.97±0.00</cell><cell>1.83±0.01</cell><cell>1.84±0.01</cell></row><row><cell cols="2">mean 1.97±0.00</cell><cell>1.88±0.01</cell><cell>1.79±0.00</cell></row><row><cell>MPNN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>max</cell><cell>1.96±0.00</cell><cell>1.89±0.00</cell><cell>1.85±0.00</cell></row><row><cell>min</cell><cell>1.97±0.00</cell><cell>1.90±0.00</cell><cell>1.91±0.01</cell></row><row><cell>GCN</cell><cell>1.81±0.00</cell><cell>1.81±0.01</cell><cell>1.81±0.01</cell></row><row><cell>GPR-GNN</cell><cell>1.81±0.00</cell><cell>1.81±0.01</cell><cell>1.81±0.01</cell></row><row><cell>NW-GNN</cell><cell cols="3">1.71±0.01 1.55±0.06 1.35±0.08</cell></row><row><cell cols="3">4.2.3 Effect of Node-wise Resolution.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average running time per epoch (ms) and (/) average total running time (s).</figDesc><table><row><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed Computers</cell><cell>Photo</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell><cell>Cornell</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>We thank the reviewers for their valuable comments. Zhewei Wei's work was partially done at Gaoling School of Artificial Intelligence, Peng Cheng Laboratory, Beijing Key Laboratory of Big Data Management and Analysis Methods and MOE Key Lab of Data Engineering and Knowledge Engineering. Zhewei Wei was supported in part by National Natural Science Foundation of China (No. 61932001, No. 61972401), by the major key project of PCL (PCL2021A12), Beijing Natural Science Foundation (No. 4222028) and by Alibaba Group through Alibaba Innovative Research Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MORE ON OPTIMIZATION</head><p>As we have discussed in Sec. 3.2, the forward propagation of our model involves the backbone GNN model, the depth controller, and the aggregator controller. denoting the loss function by L, we can calculate its gradients ∇ 𝜽 L, ∇ 𝝓 𝒅 L, and ∇ 𝝓 𝒂 L, respectively. Meanwhile, for each node 𝑣 ∈ V, the forward propagation results in z 𝑣 with entire graph and results in ẑ𝑣 with a sampled graph which involves the resolution controller. Specifically, our resolution controller rolls out a resolution configuration for each node 𝑣 ∈ V, where the resolution determines how to sample a neighborhood for calculating ẑ𝑣 . Thus, we can calculate reward signals to update 𝝓 by policy gradients.</p><p>Since the parameters of our controllers play a similar role as the architecture parameter defined in differentiable NAS, we follow DARTS <ref type="bibr" target="#b21">[19]</ref> to alternatively update 𝜽 on the training set D train and 𝝓 on the valid set D val . We summarize our learning procedure in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS B.1 On Real-world Datasets</head><p>We consistently preprocess the ten datasets. First, we follow GPR-GNN <ref type="bibr" target="#b4">[2]</ref> to normalize the attributes for each node. The splits of nodes into train/valid/test sets are conducted with different random seeds for the ten trials, where we ensure the ten seeds used for different methods are the same to keep the comparison fair.</p><p>Since the baselines are implemented with the code of GPR-GNN, we inherit their configurations mostly. For all the baselines except for NAS * , we conduct HPO for them with learning rate ∈ {0.002, 0.01, 0.05}, weight decay ∈ {0, 5×10 −4 }, and depth 𝐿 ∈ {2, 3}. For NAS * , in addition to these search spaces, we allow it to select candidate aggregators for its graph convolutional layers from {min, max, add, mean, self_msg}. For NW-GNN, we use the second design of depth controller (see Sec. 3.1). Both the depth and aggregator controllers are parameterized as a MLP and fed with the node embedding calculated at the corresponding layer. In addition to learning rate and weight decay, we search for some dedicated hyper-parameters for NW-GNN, including selecting the temperature of controllers' softmax operation from {1.0, 10.0}, whether to include "self_msg" in the search space of aggregator controller, and whether to optimize the backbone GNN model and the controllers equally. The optimal hyper-parameter configurations are provided in our source code.</p><p>Each training course has 1,000 epochs, where the model is evaluated on the valid set after each epoch. The checkpoint at the epoch with the best performance on the valid set is reserved. For HPO, each configuration is evaluated by the mean (valid) accuracy without leaking the test set. Node-wise depth. First, we generate a random graph whose degree distribution follows a power law. Its basic statistics are as follow: Number of nodes is 2,467, number of edges is 19,688, average degree is 7.98, and clustering coefficient equals 0.126. Since the values of the labels span a wide range with the maximal value of 2,467 and the minimal value of 5, we take a logarithm for them for the numeric stability reason. Under the "w/o feat. " setting, we use the constant "one" as the node attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 On Synthetic Datasets</head><p>Under the "hard feat." setting, for each node 𝑣, we first sample a 16-dimensional attribute from N (•|0, 0.25I) and execute message passing for 𝑘 𝑣 iterations, where summation is adopted as the aggregator. In this way, a 16-dimensional one-hot vector can be constructed to indicate the index of maximal value in the 16dimensional aggregated message. We concatenate the two vectors and regard it as the attribute for this node. As for "easy feat. " setting, we simply assign a 4-dimensional one-hot vector for each node as its attribute, implying the ground-truth depth of this node.</p><p>Finally, we randomly split the node set of this graph into train, valid, and test set with a ratio of 40%/30%/30%. Node-wise aggregator. The graph is randomly generated in the same way as before. Its basic statistics are as follow: Number of nodes is 2,117, number of edges is 25,298, average degree is 11.94, and clustering coefficient equals 0.094. The nodes on this graph are also splitted in the same way as before. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>13.59/3.02 10.56/2.13 11.80/2.94 10.81/3.11 10.42/3.14 12.23/3.10 12.68/2.58 14.16/2.87 10.51/2.19 11.95/2.42</idno>
	</analytic>
	<monogr>
		<title level="j">JKNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nw-Gnn</surname></persName>
		</author>
		<idno>12.26/2.68 13.25/2.68 16.44/3.70 38.62/21.43 23.06/5.55 12.10/3.08 12.02/2.43 32.46/11.60 12.25/2.48 16.41/3.42</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nw-Gnn</surname></persName>
		</author>
		<idno>12.04/2.47 16.74/3.72 20.50/4.58 38.04/12.78 22.84/5.17 12.65/3.67 12.38/2.51 32.88/7.69 17.95/4.02 15.93/3.55 NW-GNN w/ both 14.07/3.29 13.75/3.22 21.34/4.76 41.44/8.96 23.37/5.39 13.89/3.47 18.48/3.76 33.87/14.27 20.48/4.81 23.11/4.90</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AutoAttend: Automated Attention Representation Search</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation</title>
		<author>
			<persName><forename type="first">Mingguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10994</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Open Graph Benchmark: Datasets for Machine Learning on Graphs. Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Policy-GNN: Aggregation Optimization for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting positive and negative links in online social networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graphs over time: densification laws, shrinking diameters and possible explanations</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
				<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometry-Aware Gradient Algorithms for Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot Graph Neural Architecture Search with Dynamic Search Space</title>
		<author>
			<persName><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zean</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geniepath: Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is Homophily a Necessity for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns</title>
		<author>
			<persName><forename type="first">Susheel</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinith</forename><surname>Budde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous GNNs</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoliang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 34th Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AutoGEL: An Automated Graph Neural Network with Explicit Link Information</title>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">When Do GNNs Work: Understanding and Improving Neighborhood Aggregation</title>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Wing</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">GNNExplainer: Generating Explanations for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00742</idno>
		<title level="m">Automated Machine Learning on Graphs: A Survey</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICDE</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
