<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
							<email>gdesjardins@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<email>simonyan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
							<email>korayk@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName><surname>London</surname></persName>
						</author>
						<title level="a" type="main">Natural Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF9CACA1E6F37D62192A3B908AD6A80D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep networks have proven extremely successful across a broad range of applications. While their deep and complex structure affords them a rich modeling capacity, it also creates complex dependencies between the parameters which can make learning difficult via first order stochastic gradient descent (SGD). As long as SGD remains the workhorse of deep learning, our ability to extract highlevel representations from data may be hindered by difficult optimization, as evidenced by the boost in performance offered by batch normalization (BN) <ref type="bibr" target="#b6">[7]</ref> on the Inception architecture <ref type="bibr" target="#b24">[25]</ref>.</p><p>Though its adoption remains limited, the natural gradient <ref type="bibr" target="#b0">[1]</ref> appears ideally suited to these difficult optimization issues. By following the direction of steepest descent on the probabilistic manifold, the natural gradient can make constant progress over the course of optimization, as measured by the Kullback-Leibler (KL) divergence between consecutive iterates. Utilizing the proper distance measure ensures that the natural gradient is invariant to the parametrization of the model. Unfortunately, its application has been limited due to its high computational cost. Natural gradient descent (NGD) typically requires an estimate of the Fisher Information Matrix (FIM) which is square in the number of parameters, and worse, it requires computing its inverse. Truncated Newton methods can avoid explicitly forming the FIM in memory <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, but they require an expensive iterative procedure to compute the inverse. Such computations can be wasteful as they do not take into account the highly structured nature of deep models.</p><p>Inspired by recent work on model reparametrizations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13]</ref>, our approach starts with a simple question: can we devise a neural network architecture whose Fisher is constrained to be identity? This is an important question, as SGD and NGD would be equivalent in the resulting model. The main contribution of this paper is in providing a simple, theoretically justified network reparametrization which approximates via first-order gradient descent, a block-diagonal natural gradient update over layers. Our method is computationally efficient due to the local nature of the reparametrization, based on whitening, and the amortized nature of the algorithm. Our second contribution is in unifying many heuristics commonly used for training neural networks, under the roof of the natural gradient, while highlighting an important connection between model reparametrizations and Mirror Descent <ref type="bibr" target="#b2">[3]</ref>. Finally, we showcase the efficiency and the scalability of our method across a broad-range of experiments, scaling our method from standard deep auto-encoders to large convolutional models on ImageNet <ref type="bibr" target="#b19">[20]</ref>, trained across multiple GPUs. This is to our knowledge the first-time a (non-diagonal) natural gradient algorithm is scaled to problems of this magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Natural Gradient</head><p>This section provides the necessary background and derives a particular form of the FIM whose structure will be key to our efficient approximation. While we tailor the development of our method to the classification setting, our approach generalizes to regression and density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>We consider the problem of fitting the parameters ✓ 2 R N of a model p(y | x; ✓) to an empirical distribution ⇡(x, y) under the log-loss. We denote by x 2 X the observation vector and y 2 Y its associated label. Concretely, this stochastic optimization problem aims to solve:</p><formula xml:id="formula_0">✓ ⇤ 2 argmin ✓ E (x,y)⇠⇡ [ log p(y | x, ✓)] .<label>(1)</label></formula><p>Defining the per-example loss as `(x, y), Stochastic Gradient Descent (SGD) performs the above minimization by iteratively following the direction of steepest descent, given by the column vector r = E ⇡ [d`/d✓]. Parameters are updated using the rule ✓ (t+1)</p><formula xml:id="formula_1">✓ (t) ↵ (t) r (t)</formula><p>, where ↵ is a learning rate. An equivalent proximal form of gradient descent <ref type="bibr" target="#b3">[4]</ref> reveals the precise nature of ↵:</p><formula xml:id="formula_2">✓ (t+1) = argmin ✓ ⇢ h✓, ri + 1 2↵ (t) ✓ ✓ (t) 2 2 (2)</formula><p>Namely, each iterate ✓ (t+1) is the solution to an auxiliary optimization problem, where ↵ controls the distance between consecutive iterates, using an L 2 distance. In contrast, the natural gradient relies on the KL-divergence between iterates, a more appropriate distance measure for probability distributions. Its metric is determined by the Fisher Information matrix,</p><formula xml:id="formula_3">F ✓ = E x⇠⇡ ( E y⇠p(y|x,✓) " ✓ @ log p @✓ ◆ ✓ @ log p @✓ ◆ T #) ,<label>(3)</label></formula><p>i.e. the covariance of the gradients of the model log-probabilities wrt. its parameters. The natural gradient direction is then obtained as r N = F 1 ✓ r. See <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> for a recent overview of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fisher Information Matrix for MLPs</head><p>We start by deriving the precise form of the Fisher for a canonical multi-layer perceptron (MLP) composed of L layers. We consider the following deep network for binary classification, though our approach generalizes to an arbitrary number of output classes.</p><formula xml:id="formula_4">p(y = 1 | x) ⌘ h L = f L (W L h L 1 + b L ) (4) • • • h 1 = f 1 (W 1 x + b 1 )</formula><p>The parameters of the MLP, denoted</p><formula xml:id="formula_5">✓ = {W 1 , b 1 , • • • , W L , b</formula><p>L }, are the weights W i 2 R Ni⇥Ni 1 connecting layers i and i 1, and the biases b i 2 R Ni . f i is an element-wise non-linear function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us define</head><p>i to be the backpropagated gradient through the i-th non-linearity. We ignore the off block-diagonal components of the Fisher matrix and focus on the block F Wi , corresponding to interactions between parameters of layer i. This block takes the form:</p><formula xml:id="formula_6">F Wi = Ex⇠⇡ y⇠p h vec i h T i 1 vec i h T i t T i</formula><p>, where vec(X) is the vectorization function yielding a column vector from the rows of matrix X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assuming that</head><p>i and activations h i 1 are independent random variables, we can write: where X(i, j) is the element at row i and column j of matrix X and x(i) is the i-th element of vector</p><formula xml:id="formula_7">F Wi (km, ln) ⇡ Ex⇠⇡ y⇠p [ i (k) i (l)] E ⇡ [h i 1 (m)h i 1 (n)] ,<label>(5)</label></formula><formula xml:id="formula_8">✓ t ✓ t+T ⌦ t ⌦ t+1 ⌦ t+T F (✓ t ) 1 2 F (✓ t ) 1 2</formula><p>x. F Wi (km, ln) is the entry in the Fisher capturing interactions between parameters W i (k, m) and W j (l, n). Our hypothesis, verified experimentally in Sec. 4.1, is that we can greatly improve conditioning of the Fisher by enforcing that</p><formula xml:id="formula_9">E ⇡ ⇥ h i h T i ⇤ = I</formula><p>, for all layers of the network, despite ignoring possible correlations in the 's and off block diagonal terms of the Fisher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Projected Natural Gradient Descent</head><p>This section introduces Whitened Neural Networks (WNN), which perform approximate whitening of their internal representations. We begin by presenting a novel whitened neural layer, with the assumption that the network statistics µ</p><formula xml:id="formula_10">i (✓) = E[h i ] and ⌃ i (✓) = E[h i h T i ] are fixed.</formula><p>We then show how these layers can be adapted to efficiently track population statistics over the course of training. The resulting learning algorithm is referred to as Projected Natural Gradient Descent (PRONG). We highlight an interesting connection between PRONG and Mirror Descent in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Whitened Neural Layer</head><p>The building block of WNN is the following neural layer,</p><formula xml:id="formula_11">h i = f i (V i U i 1 (h i 1 c i 1 ) + d i ) .<label>(6)</label></formula><p>Compared to Eq. 4, we have introduced an explicit centering parameter c i 1 2 R Ni 1 , equal to µ i 1 , which ensures that the input to the dot product has zero mean in expectation. This is analogous to the centering reparametrization for Deep Boltzmann Machines <ref type="bibr" target="#b12">[13]</ref>. The weight matrix</p><formula xml:id="formula_12">U i 1 2 R Ni 1 ⇥Ni</formula><p>1 is a per-layer PCA-whitening matrix whose rows are obtained from an eigendecomposition of ⌃ i 1 :</p><formula xml:id="formula_13">⌃ i = Ũi • diag ( i ) • Ũ T i =) U i = diag ( i + ✏) 1 2 • Ũ T i .<label>(7)</label></formula><p>The hyper-parameter ✏ is a regularization term controlling the maximal multiplier on the learning rate, or equivalently the size of the trust region. The parameters V i 2 R Ni⇥Ni 1 and d i 2 R Ni are analogous to the canonical parameters of a neural network as introduced in Eq. 4, though operate in the space of whitened unit activations U i (h i c i ). This layer can be stacked to form a deep neural network having L layers, with model parameters</p><formula xml:id="formula_14">⌦ = {V 1 , d 1 , • • • V L , d L } and whitening coefficients = {U 0 , c 0 , • • • , U L 1 , c L 1 },</formula><p>as depicted in Fig. <ref type="figure" target="#fig_0">1a</ref>. Though the above layer might appear over-parametrized at first glance, we crucially do not learn the whitening coefficients via loss minimization, but instead estimate them directly from the model statistics. These coefficients are thus constants from the point of view of the optimizer and simply serve to improve conditioning of the Fisher with respect to the parameters ⌦, denoted F ⌦ . Indeed, using the same derivation that led to Eq. 5, we can see that the block-diagonal terms of</p><formula xml:id="formula_15">F ⌦ now involve terms E ⇥ (U i h i )(U i h i ) T ⇤</formula><p>, which equals identity by construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Updating the Whitening Coefficients</head><p>As the whitened model parameters ⌦ evolve during training, so do the statistics µ i and ⌃ i . For our model to remain well conditioned, the whitening coefficients must be updated at regular intervals, Algorithm 1 Projected Natural Gradient Descent if mod(t, T ) = 0 then . amortize cost of lines <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> 6:</p><p>for all layers i do 7:</p><p>Compute canonical parameters</p><formula xml:id="formula_16">W i = V i U i 1 ; b i = d i W i c i 1 . . proj. P 1 (⌦) 8:</formula><p>Estimate µ i and ⌃ i , using N s samples from D. Update parameters</p><formula xml:id="formula_17">V i W i U 1 i 1 ; d i b i + V i U i 1 c i 1 .</formula><p>. proj. P (✓) Perform SGD update wrt. ⌦ using samples from D.</p><p>14:</p><p>t t + 1 15: until convergence while taking care not to interfere with the convergence properties of gradient descent. This can be achieved by coupling updates to with corresponding updates to ⌦ such that the overall function implemented by the MLP remains unchanged, e.g. by preserving the product V i U i 1 before and after each update to the whitening coefficients (with an analoguous constraint on the biases).</p><p>Unfortunately, while estimating the mean µ i and diag(⌃ i ) could be performed online over a minibatch of samples as in the recent Batch Normalization scheme <ref type="bibr" target="#b6">[7]</ref>, estimating the full covariance matrix will undoubtedly require a larger number of samples. While statistics could be accumulated online via an exponential moving average as in RMSprop <ref type="bibr" target="#b26">[27]</ref> or K-FAC <ref type="bibr" target="#b7">[8]</ref>, the cost of the eigendecomposition required for computing the whitening matrix U i remains cubic in the layer size. In the simplest instantiation of our method, we exploit the smoothness of gradient descent by simply amortizing the cost of these operations over T consecutive updates. SGD updates in the whitened model will be closely aligned to NGD immediately following the reparametrization. The quality of this approximation will degrade over time, until the subsequent reparametrization. The resulting algorithm is shown in the pseudo-code of Algorithm 1. We can improve upon this basic amortization scheme by updating the whitened parameters ⌦ using a per-batch diagonal natural gradient update, whose statistics are computed online. In our framework, this can be implemented via the reparametrization</p><formula xml:id="formula_18">W i = V i D i 1 U i 1 , where D i 1 is a diagonal matrix updated such that V [D i 1 U i 1 h i 1 ] = 1,</formula><p>for each minibatch. Updates to D i 1 can be compensated for exactly and cheaply by scaling the rows of U i 1 and columns of V i accordingly. A simpler implementation of this idea is to combine PRONG with batch-normalization, which we denote as PRONG + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Duality and Mirror Descent</head><p>There is an inherent duality between the parameters ⌦ of our whitened neural layer and the parameters ✓ of a canonical model. Indeed, there exist linear projections P (✓) and P 1 (⌦), which map from canonical parameters ✓ to whitened parameters ⌦, and vice-versa. P (✓) corresponds to line 10 of Algorithm 1, while P 1 (⌦) corresponds to line 7. This duality between ✓ and ⌦ reveals a close connection between PRONG and Mirror Descent <ref type="bibr" target="#b2">[3]</ref>.</p><p>Mirror Descent (MD) is an online learning algorithm which generalizes the proximal form of gradient descent to the class of Bregman divergences B (q, p), where q, p 2 and : ! R is a strictly convex and differentiable function. Replacing the L 2 distance by B , mirror descent solves the proximal problem of Eq. 2 by applying first-order updates in a dual space and then projecting back onto the primal space. Defining ⌦ = r ✓ (✓) and ✓ = r ⇤ ⌦ (⌦), with ⇤ the complex conjugate of , the mirror descent updates are given by: It is well known <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref> that the natural gradient is a special case of MD, where the distance generating function<ref type="foot" target="#foot_0">1</ref> is chosen to be (✓) = 1 2 ✓ T F ✓. The mirror updates are somewhat unintuitive however. Why is the gradient r ✓ applied to the dual space if it has been computed in the space of parameters ✓ ? This is where PRONG relates to MD. It is trivial to show that using the function ˜ (✓) = 1 2 ✓ T p F ✓, instead of the previously defined (✓), enables us to directly update the dual parameters using r ⌦ , the gradient computed directly in the dual space. Indeed, the resulting updates can be shown to implement the natural gradient and are thus equivalent to the updates of Eq. 9 with the appropriate choice of (✓):</p><formula xml:id="formula_19">⌦ (t+1) = r ✓ ⇣ ✓ (t) ⌘ ↵ (t) r ✓<label>(8)</label></formula><formula xml:id="formula_20">✓ (t+1) = r ⌦ ⇤ ⇣ ⌦ (t+1) ⌘<label>(9)</label></formula><formula xml:id="formula_21">⌦(t+1) = r ✓ ˜ ⇣ ✓ (t) ⌘ ↵ (t) r ⌦ = F 1 2 ✓ (t) ↵ (t) E ⇡  dd ✓ F 1 2 ✓(t+1) = r ⌦ ˜ ⇤ ⇣ ⌦(t+1) ⌘ = ✓ (t) ↵ (t) F 1 E ⇡  dd ✓<label>(10)</label></formula><p>The operators r and r ⇤ correspond to the projections P (✓) and P 1 (⌦) used by PRONG to map from the canonical neural parameters ✓ to those of the whitened layers ⌦. As illustrated in Fig. <ref type="figure" target="#fig_0">1b</ref>, the advantage of this whitened form of MD is that one may amortize the cost of the projections over several updates, as gradients can be computed directly in the dual parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Related Work</head><p>This work extends the recent contributions of <ref type="bibr" target="#b16">[17]</ref> in formalizing many commonly used heuristics for training MLPs: the importance of zero-mean activations and gradients <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, as well as the importance of normalized variances in the forward and backward passes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref>. More recently, Vatanen et al. <ref type="bibr" target="#b27">[28]</ref> extended their previous work <ref type="bibr" target="#b16">[17]</ref> by introducing a multiplicative constant i to the centered non-linearity. In contrast, we introduce a full whitening matrix U i and focus on whitening the feedforward network activations, instead of normalizing a geometric mean over units and gradient variances.</p><p>The recently introduced batch normalization (BN) scheme <ref type="bibr" target="#b6">[7]</ref> quite closely resembles a diagonal version of PRONG, the main difference being that BN normalizes the variance of activations before the non-linearity, as opposed to normalizing the latent activations by looking at the full covariance. Furthermore, BN implements normalization by modifying the feed-forward computations thus requiring the method to backpropagate through the normalization operator. A diagonal version of PRONG also bares an interesting resemblance to RMSprop <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5]</ref>, in that both normalization terms involve the square root of the FIM. An important distinction however is that PRONG applies this update in the whitened parameter space, thus preserving the natural gradient interpretation. K-FAC <ref type="bibr" target="#b7">[8]</ref> is closely related to PRONG and was developed concurrently to our method. It targets the same layer-wise block-diagonal of the Fisher, approximating each block as in Eq. 5. Unlike our method however, KFAC does not approximate the covariance of backpropagated gradients as the identity, and further estimates the required statistics using exponential moving averages (unlike our approach based on amortization). Similar techniques can be found in the preconditioning of the Kaldi speech recognition toolkit <ref type="bibr" target="#b15">[16]</ref>. By modeling the Fisher matrix as the covariance of a sparsely connected Gaussian graphical model, FANG <ref type="bibr" target="#b18">[19]</ref> represents a general formalism for exploiting model structure to efficiently compute the natural gradient. One application to neural networks <ref type="bibr" target="#b7">[8]</ref> is in decorrelating gradients across neighbouring layers.</p><p>A similar algorithm to PRONG was later found in <ref type="bibr" target="#b22">[23]</ref>, where it appeared simply as a thought experiment, but with no amortization or recourse for efficiently computing F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We begin with a set of diagnostic experiments which highlight the effectiveness of our method at improving conditioning. We also illustrate the impact of the hyper-parameters T and ✏, controlling the frequency of the reparametrization and the size of the trust region. Section 4.2 evaluates PRONG on unsupervised learning problems, where models are both deep and fully connected. Section 4.3 then moves onto large convolutional models for image classification. Experimental details such as model architecture or hyper-parameter configurations can be found in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introspective Experiments</head><p>Conditioning. To provide a better understanding of the approximation made by PRONG, we train a small 3-layer MLP with tanh non-linearities, on a downsampled version of MNIST (10x10) <ref type="bibr" target="#b10">[11]</ref>.</p><p>The model size was chosen in order for the full Fisher to be tractable. Fig. <ref type="figure" target="#fig_1">2(a-b</ref>) shows the FIM of the middle hidden layers before and after whitening the model activations (we took the absolute value of the entries to improve visibility). Fig. <ref type="figure" target="#fig_1">2c</ref> depicts the evolution of the condition number of the FIM during training, measured as a percentage of its initial value (before the first whitening reparametrization in the case of PRONG). We present such curves for SGD, RMSprop, batch normalization and PRONG. The results clearly show that the reparametrization performed by PRONG improves conditioning (reduction of more than 95%). These observations confirm our initial assumption, namely that we can improve conditioning of the block diagonal Fisher by whitening activations alone. performing auto-encoder of Section 4.2 on the MNIST dataset. Figures <ref type="figure" target="#fig_2">3a-3b</ref> plot the reconstruction error on the training set for various values of ✏ and T . As ✏ determines a maximum multiplier on the learning rate, learning becomes extremely sensitive when this learning rate is high<ref type="foot" target="#foot_1">2</ref> . For smaller step sizes however, lowering ✏ can yield significant speedups often converging faster than simply using a larger learning rate. This confirms the importance of the manifold curvature for optimization (lower ✏ allows for different directions to be scaled drastically different according to their corresponding curvature). Fig 3b compares the impact of T for models having a proper whitened initialization (solid lines), to models being initialized with a standard "fan-in" initialization (dashed lines) <ref type="bibr" target="#b9">[10]</ref>. These results are quite surprising in showing the effectiveness of the whitening reparametrization as a simple initialization scheme. That being said, performance can degrade due to ill conditioning when T becomes excessively large (T = 10 5 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Learning</head><p>Following Martens <ref type="bibr" target="#b11">[12]</ref>, we compare PRONG on the task of minimizing reconstruction error of a dense 8-layer auto-encoder on the MNIST dataset. Reconstruction error with respect to updates and wallclock time are shown in Fig. <ref type="figure" target="#fig_2">3 (c,</ref><ref type="figure">d</ref>). We can see that PRONG significantly outperforms the baseline methods, by up to an order of magnitude in number of updates. With respect to wallclock, our method significantly outperforms the baselines in terms of time taken to reach a certain error threshold, despite the fact that the runtime per epoch for PRONG was 3.2x that of SGD, compared to batch normalization (2.3x SGD) and RMSprop (9x SGD). Note that these timing numbers reflect performance under the optimal choice of hyper-parameters, which in the case of batch normalization yielded a batch size of 256, compared to 128 for all other methods. Further breaking down the performance, 34% of the runtime of PRONG was spent performing the whitening reparametrization, compared to 4% for estimating the per layer means and covariances. This confirms that amortization is paramount to the success of our method. <ref type="foot" target="#foot_2">3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervised Learning</head><p>We now evaluate our method for training deep supervised convolutional networks for object recognition. Following <ref type="bibr" target="#b6">[7]</ref>, we perform whitening across feature maps only: that is we treat pixels in a given feature map as independent samples. This allows us to implement the whitened neural layer as a sequence of two convolutions, where the first is by a 1x1 whitening filter. PRONG is compared to SGD, RMSprop and batch normalization, with each algorithm being accelerated via momentum.</p><p>Results are presented on CIFAR-10 [9] and the ImageNet Challenge (ILSVRC12) datasets <ref type="bibr" target="#b19">[20]</ref>. In both cases, learning rates were decreased using a "waterfall" annealing schedule, which divided the learning rate by 10 when the validation error failed to improve after a set number of evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>We now evaluate PRONG on CIFAR-10, using a deep convolutional model inspired by the VGG architecture <ref type="bibr" target="#b21">[22]</ref>. The model was trained on 24 ⇥ 24 random crops with random horizontal reflections. Model selection was performed on a held-out validation set of 5k examples. Results are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. With respect to training error, PRONG and BN seem to offer similar speedups compared to SGD with momentum. Our hypothesis is that the benefits of PRONG are more pronounced for densely connected networks, where the number of units per layer is typically larger than the number of maps used in convolutional networks. Interestingly, PRONG generalized better, achieving 7.32% test error vs. 8.22% for batch normalization. This reflects the findings of <ref type="bibr" target="#b14">[15]</ref>, which showed how NGD can leverage unlabeled data for better generalization: the "unlabeled" data here comes from the extra crops and reflections observed when estimating the whitening matrices.</p><p>ImageNet Challenge Dataset Our final set of experiments aims to show the scalability of our method. We applied our natural gradient algorithm to the large-scale ILSVRC12 dataset (1.3M images labelled into 1000 categories) using the Inception architecture <ref type="bibr" target="#b6">[7]</ref>. In order to scale to problems of this size, we parallelized our training loop so as to split the processing of a single minibatch (of size 256) across multiple GPUs. Note that PRONG can scale well in this setting, as the estimation of the mean and covariance parameters of each layer is also embarassingly parallel. Eight GPUs were used for computing gradients and estimating model statistics, though the eigen decomposition required for whitening was itself not parallelized in the current implementation. Given the difficulty of the task, we employed the enhanced version of the algorithm (PRONG+), as simple periodic whitening of the model proved to be unstable. Figure <ref type="figure" target="#fig_3">4 (c-d</ref>) shows that batch normalisation and PRONG + converge to approximately the same top-1 validation error (28.6% vs 28.9% respectively) for similar cpu-time. In comparison, SGD achieved a validation error of 32.1%. PRONG + however exhibits much faster convergence initially: after 10 5 updates it obtains around 36% error compared to 46% for BN alone. We stress that the ImageNet results are somewhat preliminary. While our top-1 error is higher than reported in <ref type="bibr" target="#b6">[7]</ref> (25.2%), we used a much less extensive data augmentation pipeline. We are only beginning to explore what natural gradient methods may achieve on these large scale optimization problems and are encouraged by these initial findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We began this paper by asking whether convergence speed could be improved by simple model reparametrizations, driven by the structure of the Fisher matrix. From a theoretical and experimental perspective, we have shown that Whitened Neural Networks can achieve this via a simple, scalable and efficient whitening reparametrization. They are however one of several possible instantiations of the concept of Natural Neural Networks. In a previous incarnation of the idea, we exploited a similar reparametrization to include whitening of backpropagated gradients <ref type="foot" target="#foot_3">4</ref> . We favor the simpler approach presented in this paper, as we generally found the alternative less stable for deep networks. This may be due to the difficulty in estimating gradient covariances in lower layers, a problem which seems to mirror the famous vanishing gradient problem. <ref type="bibr" target="#b16">[17]</ref>.</p><p>Maintaining whitened activations may also offer additional benefits from the point of view of model compression and generalization. By virtue of whitening, the projection U i h i forms an ordered representation, having least and most significant bits. The sharp roll-off in the eigenspectrum of ⌃ i may explain why deep networks are ammenable to compression <ref type="bibr" target="#b1">[2]</ref>. Similarly, one could envision spectral versions of Dropout <ref type="bibr" target="#b23">[24]</ref> where the dropout probability is a function of the eigenvalues. Alternative ways of orthogonalizing the representation at each layer should also be explored, via alternate decompositions of ⌃ i , or perhaps by exploiting the connection between linear auto-encoders and PCA. We also plan on pursuing the connection with Mirror Descent and further bridging the gap between deep learning and methods from online convex optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) A 2-layer natural neural network. (b) Illustration of the projections involved in PRONG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fisher matrix for a small MLP (a) before and (b) after the first reparametrization. Best viewed in colour. (c) Condition number of the FIM during training, relative to the initial conditioning. All models where initialized such that the initial conditioning was the same, and learning rate where adjusted such that they reach roughly the same training error in the given time.</figDesc><graphic coords="5,118.99,95.19,107.80,107.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Optimizing a deep auto-encoder on MNIST. (a) Impact of eigenvalue regularization term ✏. (b) Impact of amortization period T showing that initialization with the whitening reparametrization is important for achieving faster learning and better error rate. (c) Training error vs number of updates. (d) Training error vs cpu-time. Plots (c-d) show that PRONG achieves better error rate both in number of updates and wall clock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification error on CIFAR-10 (a-b) andImageNet (c-d). On CIFAR-10, PRONG achieves better test error and converges faster. On ImageNet, PRONG + achieves comparable validation error while maintaining a faster covergence rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 :</head><label>1</label><figDesc>Input: training set D, initial parameters ✓. 2: Hyper-parameters: reparam. frequency T , number of samples N</figDesc><table><row><cell>i</cell><cell>0; t</cell><cell>0</cell></row><row><cell>4: repeat</cell><cell></cell><cell></cell></row><row><cell>5:</cell><cell></cell><cell></cell></row></table><note><p>s , regularization term ✏. 3: U i I; c</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As the Fisher and thus ✓ depend on the parameters ✓ (t) , these should be indexed with a time superscript, which we drop for clarity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Unstable combinations of learning rates and ✏ are omitted for clarity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We note that our whitening implementation is not optimized, as it does not take advantage of GPU acceleration. Runtime is therefore expected to improve as we move the eigen-decompositions to GPU.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The weight matrix can be parametrized as Wi = R T i ViUi 1, with Ri the whitening matrix for i.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are extremely grateful to Shakir Mohamed for invaluable discussions and feedback in the preparation of this manuscript. We also thank Philip Thomas, Volodymyr Mnih, Raia Hadsell, Sergey Ioffe and Shane Legg for feedback on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mirror descent and nonlinear projected subgradient methods for convex optimization</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Lett</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
		<title level="m">Proximal Splitting Methods in Signal Processing</title>
		<imprint>
			<date type="published" when="2009-12">December 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>JMLR. 2011</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, Tricks of the Trade</title>
		<title level="s">Lecture Notes in Computer Science LNCS</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1524</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning via Hessian-free optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines and the centering trick</title>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Riemannian metrics for neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
		<idno>arXiv, abs/1303.0818</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel training of deep neural networks with natural gradient and parameter averaging</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning made easier by linear transformations in perceptrons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<title level="m">The Information Geometry of Mirror Descent. arXiv</title>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling up natural gradient by sparsely factorizing the inverse fisher matrix</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerated gradient descent by factor-centering decomposition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName><surname>Schraudolph</surname></persName>
		</author>
		<idno>IDSIA-33-98</idno>
	</analytic>
	<monogr>
		<title level="j">Istituto Dalle Molle di Studi sull&apos;Intelligenza Artificiale</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The natural gradient by analogy to signal whitening, and recipes and tricks for its use</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Projected natural actorcritic</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">C</forename><surname>Philip S Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridhar</forename><surname>Giguere</surname></persName>
		</author>
		<author>
			<persName><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pushing stochastic gradient towards second-order methods -backpropagation learning with transformations in nonlinearities</title>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Vatanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICONIP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
