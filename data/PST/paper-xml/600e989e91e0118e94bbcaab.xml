<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A multi-perspective combined recall and rank framework for Chinese procedure terminology normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-25">January 25, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kui</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tong</forename><surname>Ruan</surname></persName>
							<email>ruanton@mail.ecust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A multi-perspective combined recall and rank framework for Chinese procedure terminology normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-25">January 25, 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.09101v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical terminology normalization aims to map the clinical mention to terminologies come from a knowledge base, which plays an important role in analyzing Electronic Health Record(EHR) and many downstream tasks. In this paper, we focus on Chinese procedure terminology normalization. The expression of terminologies are various and one medical mention may be linked to multiple terminologies. Previous study explores some methods such as multi-class classification or learning to rank(LTR) to sort the terminologies by literature and semantic information. However, these information is inadequate to find the right terminologies, particularly in multi-implication cases. In this work, we propose a combined recall and rank framework to solve the above problems. This framework is composed of a multi-task candidate generator(MTCG), a keywords attentive ranker(KAR) and a fusion block(FB). MTCG is utilized to predict the mention implication number and recall candidates with semantic similarity. KAR is based on Bert with a keywords attentive mechanism which focuses on keywords such as procedure sites and procedure types. FB merges the similarity come from MTCG and KAR to sort the terminologies from different perspectives. Detailed experimental analysis shows our proposed framework has a remarkable improvement on both performance and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mining and exploring structured data from Electronic Health Record(EHR) plays a key role in many applications such as clinical research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and decision support systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. However, the non-standard and various expression of medical terminologies hinders the further utilization of EHR. For example, "? ???????" could be described as "??????????", "???? ????" and "????????????". If the analysis is directly based on such data, a lot of valid information will be ignored and wasted. Therefore description of medical terminologies should be normalized before analyzing EHR. Medical terminology normalization aims to link the description of medical terminology in EHR to standard entities in a knowledge base. Among all the terminologies, the descriptions of procedure are most diverse and requires professional skills. Clinicians often add some extra description such as procedure instruments and concrete procedure steps, most of which are irrelevant for normalization but useful in describing the process of procedure. In this paper, we focus on Chinese procedure terminology normalization. The procedure terminologies come from International Classification of Disease version 9(ICD-9).</p><p>We notate the statement of procedure written by doctors in EHR as procedure mention and its corresponding entities in knowledge base as terminologies. The characteristics and challenges of Chinese procedure terminology normalization are: 1) Multi implication. As proposed in <ref type="bibr" target="#b4">[5]</ref>, multi implication means a procedure mention may be normalized to multiple terminologies. An example is shown in the first part of Figure <ref type="figure" target="#fig_1">1</ref>. M1 maps to two terminologies because two procedure sites "??(skin)" and "????(Subcutaneous tissue)" are presented in one procedure mention. 2)Short text. Unlike entity linking or entity normalization, no context information is available in this task. Both procedure mentions and terminologies are short text, with an average length of 12 and 9 characters in Chinese respectively. 3)Keywords sensitive. Keywords such as procedure site and procedure type matter in terminology normalization. As shown in the second part of Figure <ref type="figure" target="#fig_1">1</ref>, M2 matches with T3 because "?????(pheochromocytoma)" is a sub-concept of "??(lesion)" and "? ?(Transabdominal)" is just an extra description which has no affect on the result. M3 and M4 are not normalized to T3 because they share different procedure site and procedure type respectively. 4)High efficiency. Knowledge base usually has a large amount of terminologies, the designed algorithm should generate normalized terminologies quickly and efficiently.</p><p>Previous studies on medical terminology normalization could be divided into two categories: "direct rank" and "rank follows recall". Direct rank based methods such as string match, dictionary look up <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, multi-class classification <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> and point-wise learning to rank <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> directly select the mapping terminologies from the whole knowledge base. Among them, methods such as string match and dictionary look up cannot process synonyms that are literally different but semantically the same; The output space of multi-class classification is the same as the number of terminologies in the knowledge base. Multiclass classification approaches may not perform well on the terminologies which have not been appeared in the training data. Point-wise learning to rank regards terminology normalization as a binary classification problem. The input of the model is a medical mention with a terminology, the output is the similarity of the two texts. With the size of knowledge base increases, point-wise learning to rank methods faces serious efficiency problem. Rank follows recall approaches <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> applies a two-step framework which first generate candidate terminologies by heuristic rules or statistic methods then rank the candidates in a point-wise way. The recall step only decreases the scale of the candidates without providing other information for sorting the candidates. The similarity calculated in recall step is often wasted. Thus the output terminologies could only be sorted during the rank step. We find that the recall and rank step should sort the input terminologies from different perspectives, then combine their results to complement each other. Besides, all of the methods mentioned above could not solve multi-implication problem except <ref type="bibr" target="#b4">[5]</ref>, which utilize a sequence generation model to generate all possible terminologies with the cost of losing efficiency.</p><p>In this paper, we propose a novel "combined recall and rank" framework with a multi-task candidate generator(MTCG), a keywords attentive ranker(KAR) and a fusion block(FB). We first recall literature and semantic similar terminologies by a pairwise recall model. Then rank these candidates according to context and keywords information. Finally the similarities come from recall and rank step are merged to generate the normalized results. In contrast with previous pairwise model, we propose an effective online negative sampling strategy which greatly improves recall rate. And compared with other rank model, keywords such as procedure site and procedure type are fully utilized for better performance. To handle multi-implication problem, we define it as a classification task and train it with the recall model at the same time.</p><p>Overall, the contributions of this paper are as follows:</p><p>? We propose a novel "combined recall and rank" framework for Chinese procedure terminology normalization. A fusion block is designed to merge the results from different perspective of recall and rank model.</p><p>? We design a multi-task candidate generator aims to predict both the implication number and recall the candidate terminologies. An effective online negative sampling strategy is proposed to find informative negative samples for training. To the best of our knowledge, negative sample strategy is first considered in medical terminology normalization.</p><p>? We introduce a keywords attentive ranker, which focuses on the procedure site and procedure type of mentions and terminologies. The attention on keywords provides another perspective for sorting the terminologies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There are two areas related to our work: medical terminology normalization and text matching. Text matching aims to infer the relation of two given texts. In this task, text matching methods could be used to prediction the similarity of procedure mentions and terminologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Medical terminology normalization</head><p>Medical terminology normalization aims to find standard terminologies for a medical mention, usually the terminologies come from a knowledge base or a terminology dictionary. Most of the proposed methods could divided into two categories: "direct rank" and "rank follows recall". Direct rank directly finds the normalized terminologies from the whole knowledge base. Early methods <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref> match medical mention to terminologies by heuristic rules.With the appearance of machine learning, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> regard medical terminology normalization as a multi-class classification problem where the number of categories is the same as the number of terminologies. <ref type="bibr" target="#b10">[11]</ref> designs a multi-task framework which normalize both mentions and abbreviations at the same time. <ref type="bibr" target="#b19">[20]</ref> utilize recurrent neural network(RNN) to capture the semantic meaning of medical mentions and terminologies. Point-wise learning to rank is also used to normalize terminologies. <ref type="bibr" target="#b12">[13]</ref> propose a multi-view convolutional neural network(CNN) and a multi-task framework to normalize both procedure and disease mentions. When the terminology knowledge base is large, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b20">21]</ref> propose a recall model to generate possible terminologies then followed by a rank model to sort. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> first generate candidates by bm25, then rank the terminologies by CNN and Bert respectively. <ref type="bibr" target="#b14">[15]</ref> applies active learning on lab indicator normalization when training the binary classification rank model, which aims to reduce the cost of annotation. <ref type="bibr" target="#b20">[21]</ref> design a list-wise model with semantic type regulation to sort the terminologies. <ref type="bibr" target="#b17">[18]</ref> recalls candidates by clustering then sort them based on ngram features. <ref type="bibr" target="#b4">[5]</ref> design a sequence generate and rank framework which could solve multi-implication problem, but they ignore the efficiency problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text matching</head><p>Text matching aims to predict the relevance of two given texts. In this task, text matching is applied to judge the similarity of medical mentions and terminologies. The proposed methods in text matching could be divided into two categories <ref type="bibr" target="#b21">[22]</ref>: representation based and interaction based. Representation based methods encode text into vectors and calculate vector similarity as their similarity. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and <ref type="bibr" target="#b24">[25]</ref> utilize different encoder structure such as LSTM, CNN and Bert respectively. Representation based methods are efficient but there is no interaction between the two input sentences. With the appearance of attention mechanism, a lot of works have been focused on interaction based methods <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b28">[29]</ref> uses bidirectional LSTM to encode the sentences and applies cross attention between two sentences. <ref type="bibr" target="#b29">[30]</ref> utilizes an advanced bilateral multi-perspective matching for natural language sentences. Bert <ref type="bibr" target="#b30">[31]</ref> is also used in this area and performs well on many tasks. <ref type="bibr" target="#b31">[32]</ref> studies the behaviors of Bert on ranking task. <ref type="bibr" target="#b21">[22]</ref> explore a late interaction formula based on Bert which contains both representation and interaction. <ref type="bibr" target="#b32">[33]</ref> summaries the function of the components in interactive based model and proposes three key features for alignment. <ref type="bibr" target="#b33">[34]</ref> proposes an Enhanced-RCNN for learning sentence similarity which is far less complex compared with Bert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials and methods</head><p>The whole architecture of our framework is shown in Figure Here we regard procedure mention and its corresponding terminologies as similar text pairs. Mention implication number prediction is a regression problem. However, the statistics of mention implication number shows the data follows a long tail distribution in Table <ref type="table" target="#tab_3">1</ref>. Therefore we redefine mention implication number prediction as a classification problem and set the total category number as 3. The label of some cases where its implication number is bigger than 3 is all set to 3, which will be adjusted in fusion block during inference time.</p><p>The base model used in MTCG is Bert, a multi-layer transformer architecture. The input of each transformer layer is a matrix E ? (l, d), which is the input embedding matrix or the output of last transformer block. And l is the sequence length, d is the input dimension. Then multi-head self-attention is applied for encoding hidden context information, which is composed of multiple self-attention block. The output of multi-head attention is computed by:</p><formula xml:id="formula_0">M ultiHead(E) = [head 1 , head 2 , ..., head n ]W o (2) head i = sof tmax( Q i K iT ? d head )V i (3) [Q i , K i , V i ] = E[W i q , W i k , W i v ]<label>(4)</label></formula><p>The three parameter matrix W i q , W i k , W i v of ith head is used to map the input matrix in to hidden space, with the shape of (d, d head ). And usually d = n * d head where n is the number of heads. That means the [head 1 , head 2 , ..., head n ] ? (l, d) and W o ? (d, d).</p><p>A feed-forward neural network block is then used to further process the output of multi-head attention. As the shown in formula 5, in which</p><formula xml:id="formula_1">W 1 ? (d, d f f ), W 2 ? (d f f , d), b 1 ? (d f f , 1), b 2 ? (d, 1). d f f is a hyper parameter.</formula><p>Other components such as residual network and layer normalization are the same as <ref type="bibr" target="#b4">[5]</ref>. In MTCG, the output of Bert H ? (l, d) is used to represent procedure mentions and terminologies. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, During the training process, the input of the model is a procedure mention M , a positive terminology P and a negative terminology N . For each procedure mention, its standard terminologies are regarded as positive samples and the remained terminologies are all possible negative samples. After encoding by Bert, the sequence hidden output is notated as H M , H P , H N . Then an average pooling layer is applied in sequence length dimension to get the vector representation of the input sequence, which is marked as V M , V P , V N respectively. For the recall task, we use triple loss to make the representation of mention closer to its positive samples and farther from its negative samples. And the distance of two vector</p><formula xml:id="formula_2">F N N (x) = max(0, xW 1 + b 1 )W 2 + b 2 (5) E ["#$] E [&amp;$] E [&amp;'] E ? E ? E ? E ? E ? E ? E ? E [$(&amp; ] E ? E ? E ? E ? E ? E [$(&amp; ]</formula><formula xml:id="formula_3">D(d 1 , d 2 ) is calculated by ||d 1 -d 2 || 2 .</formula><p>For the mention implication number prediction task, we take the CLS token of procedure mention CLS m out, then followed by a FNN and softmax layer. Crossentropy is applied for this classification task. The output of softmax layer is ? and the y is the implication number label of current mention. The total loss is the sum of triple loss and crossentropy, which is shown in formula 6-8. loss = loss t + loss c (6) <ref type="formula">7</ref>)</p><formula xml:id="formula_4">loss t = max(0, D(V M , V P ) + margin -D(V M , V N ))(</formula><formula xml:id="formula_5">loss c = y log ?(8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Negative sampling</head><p>As described above, positive samples and negative samples is needed during training candidate generator. The positive samples are fixed and easy to confirm, but there is a large amount of negative samples of a medical mention. For each mention, we choose k n negative samples for training, where k n is a hyper parameters. The quality of negative samples directly impacts model performance. The most simple strategy is random sampling. However, random sampling ignores informative samples and does not provide enough information for the model, which normally leads to a poor performance. Thus we design following negative sample strategies: training M on (X,Y,N)</p><p>N ? ? 5:</p><p>V m = M (X)</p><p>6:</p><p>V kb = M (KB)</p><p>7:</p><formula xml:id="formula_7">S = Euclidean distance(V m * V kb ) 8:</formula><p>S index = Argsort(S) // Ascending order 9:</p><p>for each i ? |X| do 10:</p><p>n ? ? end for 22: end for ? Tf-idf: We calculate the tf-idf value of procedure mentions and terminologies from knowledge base. And choose the top-k n negative samples.</p><p>? Tree coding: The terminologies in ICD9 are roughly categorized by pro-cedures site, which could be conducted by its code. For example, the code of "?????(Meningotomy)" is 01.3100 and the code of "?? ???(Thalamus incision)" is 01.4101. The two terminologies belong to the same category because their code shares the same prefix "01". Therefore,for each mention, we first find the categories C m of is mapping terminologies, then randomly choose k n terminologies which belong to C m .</p><p>? Keywords replacing We manually collect a keywords dictionary contains procedure site and procedure type. For each mention, we generate negative samples by replacing the keywords in its medical terminologies with other keywords in the dictionary. For instance, a medical mention "????????(Meningotomy)" is mapped to terminology "??? ?????(Meningotomy)". The negative samples of keywords replacing could be "????????(Meningotomy)" and "??????? ?(Meningotomy)".</p><p>? The first three strategies choose the "hard negative samples" by heuristic rules. However, online negative sampling selects negative samples dynamically from the candidate generator. This end-to-end approach selects more suitable samples for the model to learn and improves its performance. The changing negative samples also provide diversity which enhanced the robustness for the model, which could be regarded an approach of data argumentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Keywords Attentive Ranker</head><p>Keywords attentive ranker(KAR) aims to rank the candidates based on procedure sites and procedure types. The training data of KAR is generated by MTCG. We choose top-10 nearest terminologies for each mention as its candidates. KAR is trained in a point-wise way and the similarity is measured by the last output logits. The label is set to 1 if the mention is mapped to the terminology else 0.</p><p>To capture the keyword information, we propose a keyword attentive mechanism on Bert. Figure <ref type="figure">5</ref> shows the input of an example, we add two special token [PS] and [PT] at the beginning of the input sequence. In which PS stands for procedure site and PT stands for procedure type. Then the input of KAR is s = {[CLS], [P S], [P T ], mention, [SEP ], candidate, [SEP ]}. Token [PS] only attends to procedure site words in mention and its candidate, token [PT] only pays attention to procedure type words in the sequence. This could be easily achieved by multiplying a self-attention mask in the transformer layer. More concretely, formula 3 could be written as formula 9, where ? represent elementwise product. M ask is the mask matrix and M ask[i][j] = 1 if s[i] attends to s[j] else 0. The keywords are matched by a vocabulary collated by ourselves.</p><p>[CLS] attends to the whole word in the input sequence and capture the semnatic information of mention and candidate in general, [PS] and [PT] only capture the procedure site and procedure type information respectively. We use an average pooling layer to merge the hidden output of [CLS], [PS] ,[PT] focus on different perspective. And followed by a FNN layer with a sigmoid function. The loss is calculated in formula 10-11, where y is the label of mention and its candidates.</p><formula xml:id="formula_8">head i = (sof tmax( Q i K iT ? d head ) ? M ask)V i (9) loss = y log ? + (1 -y) log (1 -?)(10) ? = KAR(mention, candidate)(11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusion Block</head><p>The fusion block is used in inference time, which merges the results of MTCG and KAR. First, given a medical mention m, MTCG recalls a candidates set C contains k c terminologies. For each candidate c i in C, d(m, c i ) stands for the euclidean distance of m and ith candidate in C, where d(m, c i ) ? R. Then KAR calculates the similarity with keywords information of m and c i , the output is notated as sr m,ci . The final similarity score s m,ci of m and c i takes both the similarity of the two models into consideration. s m,ci is calculated as follows:</p><formula xml:id="formula_9">s m,ci = sc m,ci + sr m,ci<label>2</label></formula><formula xml:id="formula_10">sc m,Ci = 1 - d(m, c i ) kc j=0 d(m, c j )</formula><p>Then the candidates C is sorted by s m,Ci in descending order. Suppose the mention implication number predicted in MTCG is x where x ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. To handle the situation that the medical implication number is bigger than 3, we use a threshold ? to choose the possible terminologies. The final output terminologies is shown in formula 14, where ? is a hyper parameter.</p><formula xml:id="formula_11">output = {c i |i ? [1, x]} if x &lt; 3 {c i |s m,ci &gt; ?, i ? [4, |C|]ori ? [1, 3]} if x = 3<label>(14)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The dataset is available at OpenKG, which is provided from the CHIP 2019 clinical entity normalization task. The statistic of training set and test set is shown in Table <ref type="table" target="#tab_3">1</ref>. The mentions are collated from Chinese electronic health record and annotated by medical staff. And the knowledge base is ICD9-2017-PUMCH procedure codes(ICD9), which contains 9867 terminologies. The knowledge base is a tree structure and each terminology has its unique code. These terminologies are basically classified by procedure site which could be conducted by its code prefix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment settings</head><p>We implement out model by pytorch and runs on 4 NVIDIA GeForce GTX 1080Ti GPUs. Owing to the high cost of pre-training Bert, we directly adopted BERT-base parameters pre-trained by Google in the Chinese general corpus. The optimizer of MTCG and KAR is both AdamW and they share the same learning rate 2e-5. For MTCG, the negative sample number for each mention k n is set to 4, the margin of triple loss is set to 1 and the candidate number k c generated during inference time is set to 10. In FB, the threshold ? is set to 0.8. Following the setting in <ref type="bibr" target="#b4">[5]</ref>, we use 5-fold cross validation to measure the performance of out proposed model. We use accuracy to evaluate the performance of different methods. Considering the multi implication problem in this task, a sample is correct only when both the implication number and terminologies are exactly the same with the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art methods</head><p>We first compare our methods with two statistic methods: Tf-idf and editdistance. A baseline Bert-rank model is also concluded. Besides, we also compare out methods with some state-of-the-art methods published in recent years. <ref type="bibr" target="#b17">[18]</ref> using combines different similarity algorithms based on n-gram as features to train a binary classifier to find similar symptoms. <ref type="bibr" target="#b14">[15]</ref> is a "recall and rank" method which uses tf-idf to recall candidates and apply ESIM for similarity calculation in lab-indicator. However, both of their methods is not able to handle multi-implication thus we only choose the most similar one as the final output. <ref type="bibr" target="#b4">[5]</ref> is a generative rerank method which propose a generative method for recalling and a Bert-base ranker for sort, and the implication number is generated during the first step. Note that the problem definition is not exactly same in different papers, so we slightly adjust these methods to satisfy our task for fair comparison.</p><p>The result is shown in Table <ref type="table" target="#tab_4">2</ref>. It can be observed that our proposed method achieve the highest score in all metrics, with the uni-implication accuracy of 93.48%, the multi-implication accuracy of 53.02% and the total acc of 91.47%, which has a clearly improvements compared with other referenced methods. Tf-idf, edit-distance, <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b14">[15]</ref> performs poorly because they only consider the literature and semantic features, which is not enough for Chinese medical  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation analysis</head><p>To further investigate the importance of each block in our framework, we conduct an ablation study. As the shown in Table <ref type="table" target="#tab_5">3</ref>, we have following observations:</p><p>(1) MTCG plays an important role in our framework, without the high quality candidates generated by the recall model, the performance declines nearly 25%, from 91.47% to 68.62%. (2) KAR leads to a high performance on multiimplication. Because more than one procedure site or procedure type appears in multi-implication case. The candidate generator focuses more on literal and semantic similarity which ignores the keywords. Table <ref type="table">4</ref> shows an example, where candidate generator ignores " ?" and the two generated terminologies are all  <ref type="formula">3</ref>) Only use the output of MTCG or KAR could achieve relative high performance. However, their results focus on different perspective: literature similarity and keywords similarity, which is complementary for each other. As shown in Table <ref type="table" target="#tab_6">5</ref>, the two cases is correct only when the result of MTCG and KAR all be taken into consideration. When the keywords in the medical mention is miss or unmatched with our vocabulary, the similarity comes from MTCG is a powerful complementary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effectiveness and efficiency of recalling</head><p>We explore the effectiveness and efficiency of recalling in this section. Here we use Recall@k as a metric to evaluate the recall performance. The result is shown in Table <ref type="table">4</ref>.6. Compared with <ref type="bibr" target="#b4">[5]</ref>, which utilize a generative model to recall. Our candidate generator based on deep metric learning performs better both in efficiency and effectiveness, especially an increase of nearly 42 times in speed. Because generative-based method is decoded word-by-word thus has a lower speed, our embedding-based model supports parallel computing and we use <ref type="bibr" target="#b34">[35]</ref> to quickly find the neighbors of a given embedding. Besides, we also experimented with different number of layers in Bert. As shown in the result, with the decrease of the number of layers, there is no obvious decrease in performance and a certain increase in speed, which prove that the deep metric learning is suitable for this task. The last two lines in Table <ref type="table">4</ref>.6 also shows online negative sampling is superior than tf-idf when the parameters decrease. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this work, we first analysis four challenges for Chinese procedure normalization: multi-implication, short text, keywords sensitive and high efficiency. A novel framework is proposed which is composed of three parts. To handle multi implication problem and generate candidate terminologies quickly, we design a multi-task candidate generator(MTCG) based on deep metric learning. Online negative sampling strategy is used to generate hard samples for training which greatly improves recall rate. The keyword attentive ranker(KAR) is proposed to focus on procedure sites and procedure type of medical mentions and terminologies. A fusion block(FB) is also concluded to merge the results of MTCG and KAR. The experiments result shows our proposed framework outperforms existing methods and achieve the state of the art result on this task. In the future, we have great interests on the following three areas: 1) The candidate generator could be further speed up with tiny model architecture. 2) Automatically explore keywords without vocabulary. 3) Improve the performance with limited annotation data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of multi implication and keywords sensitive</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the input of KAR. Two special token [PS] and [PT] is added. the lines shows the attention of the two tokens. [PS] could only attends to procedure site tokens and [PT] could only attends to procedure type tokens, where the red tokens represent procedure site and blue tokens represent procedure type</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Online negative sampling We choose the top-k n negative samples from the candidates of training set by the candidate generator after each training epoch. Random selection is used during the first epoch. The training process with online negative sampling is shown in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy of different negative sampling strategies on test set during training process</figDesc><graphic url="image-1.png" coords="12,183.62,333.02,244.00,152.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The statistics of train and test data</figDesc><table><row><cell cols="4">Dataset Uni-implication Multi-implication Total</cell></row><row><cell>Train</cell><cell>3801</cell><cell>199</cell><cell>4000</cell></row><row><cell>Test</cell><cell>2851</cell><cell>149</cell><cell>3000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance of different methods in terms of accuracy analyzed in the first section. With the use of Bert, Bert-base rannking and<ref type="bibr" target="#b4">[5]</ref> get high performance but still has a gap compared with our method. Besides, their methods are time consuming because bert-base ranking is a point-wise method and<ref type="bibr" target="#b4">[5]</ref> generate candidates based on a sequence generative model.</figDesc><table><row><cell>Method</cell><cell cols="3">Uni-implication Multi-implication Total</cell></row><row><cell>Tf-idf</cell><cell>49.3</cell><cell>-</cell><cell>46.8</cell></row><row><cell>Edit-distance</cell><cell>50.8</cell><cell>-</cell><cell>48.3</cell></row><row><cell cols="2">BERT-based ranking 88.6</cell><cell>-</cell><cell>84.2</cell></row><row><cell>Zhang</cell><cell>37.76</cell><cell>-</cell><cell>37.76</cell></row><row><cell>Liang</cell><cell>69.63</cell><cell>-</cell><cell>69.63</cell></row><row><cell>Transformer</cell><cell>91.1</cell><cell>52.4</cell><cell>89.3</cell></row><row><cell>Our methods</cell><cell>93.48</cell><cell>53.02</cell><cell>91.47</cell></row><row><cell>terminology normalization as</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>: Ablation study</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Uni-implication Multi-implication Total</cell></row><row><cell cols="2">Our methods 93.48</cell><cell>53.02</cell><cell>91.47</cell></row><row><cell>w/o MTCG</cell><cell>72.21</cell><cell>-</cell><cell>68.62</cell></row><row><cell>w/o KAR</cell><cell>92.63</cell><cell>41.61</cell><cell>90.1</cell></row><row><cell>w/o FB</cell><cell>92.9</cell><cell>51.68</cell><cell>90.85</cell></row><row><cell cols="4">Table 4: An normalization example. Our proposed KAR is able to focues on</cell></row><row><cell>different procedure site</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mention</cell><cell></cell><cell>Ca prediction</cell><cell>Kra prediction</cell></row><row><cell cols="3">?????+????? ?????</cell><cell>??????</cell></row><row><cell></cell><cell></cell><cell cols="2">??????? ?????</cell></row><row><cell>relevant to "??". (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>An normalization example. Our proposed method sort the terminologies from multi perspective thus lead to a high performance In this section, we compare different negative sample strategies used in MTCG. As shown in Table6, online negative sampling has a huge improvement on accuracy compared with other strategies. Keywords replacing performs poor than random selection because it only replace the keywords, various expression examples are not presented during the training process. Tree coding and tf-idf find harder samples than random selection by statistic information, but their negative samples unchanged during each epoch. online negative sampling selects the most difficult samples in each epoch by the model as negative samples and achieves the best result. Figure6shows accuracy on test set at each epoch.</figDesc><table><row><cell>Mention</cell><cell>Recall results</cell><cell cols="3">sc m,ci sr m,ci s m,ci</cell></row><row><cell cols="3">????????+?????? ????????? 0.36</cell><cell>0.89</cell><cell>1.25</cell></row><row><cell></cell><cell cols="2">????????? 0.09</cell><cell>0.37</cell><cell>0.46</cell></row><row><cell></cell><cell>????</cell><cell>0.08</cell><cell>0.04</cell><cell>0.12</cell></row><row><cell></cell><cell cols="2">????????? 0.06</cell><cell>0.61</cell><cell>0.67</cell></row><row><cell>?????????</cell><cell cols="2">????????? 0.18</cell><cell cols="2">0.0001 0.1801</cell></row><row><cell></cell><cell>??????</cell><cell>0.17</cell><cell cols="2">0.0063 0.1763</cell></row><row><cell cols="4">4.5 Impacts of different negative sampling strategies</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different negative sampling strategiesNote that the accuracy of online negative sample is low because we use random sample in the first epoch. It is clear to see that with the use of online negative sampling, the accuracy has a rapid growth and leads other strategies with a large gap during the whole training process.</figDesc><table><row><cell>Negative Sample Strategy</cell><cell>acc</cell></row><row><cell>Keywords replace</cell><cell>43.83</cell></row><row><cell>Random</cell><cell>65.10</cell></row><row><cell>Tree-coding</cell><cell>79.03</cell></row><row><cell>Tf-idf</cell><cell>86.17</cell></row><row><cell>Online negative sampling</cell><cell>90.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The efficiency of different recall modelIn this section, we compare the accuracy of implication number prediction task. As shown in table 8, the candidate generator performs best with the total accu-racy of 98.23. What is interesting is that the model training separately on Bert performs poor than ours both in uni-implication and multi-implication, which indicates that the implication prediction task is affected by candidate generate: the representation contains enrich context used for recall is also benefit for implication number prediction.</figDesc><table><row><cell>Methods</cell><cell cols="5">Recall@2 Recall@5 Recall@7 Recall@10 Speed</cell></row><row><cell>Transformer</cell><cell>88%</cell><cell>88.70%</cell><cell>89.10%</cell><cell>89.20%</cell><cell>12 mentions/sed</cell></row><row><cell>Bert on 12 layers</cell><cell>94.20%</cell><cell>97.23%</cell><cell>97.83%</cell><cell>98.30%</cell><cell>507 mentions/sed</cell></row><row><cell>Bert on 10 layers</cell><cell>93.63%</cell><cell>96.80%</cell><cell>97.40%</cell><cell>97.83%</cell><cell>588 mentions/sed</cell></row><row><cell>Bert on 8 layers</cell><cell>93.93%</cell><cell>96.93%</cell><cell>97.47%</cell><cell>97.97%</cell><cell>691 mentions/sed</cell></row><row><cell>Bert on 6 layers</cell><cell>93.23%</cell><cell>96.9%</cell><cell>97.43%</cell><cell>97.7%</cell><cell>833 mentions/sed</cell></row><row><cell>Bert on 2 layers</cell><cell>91.43%</cell><cell>95.07%</cell><cell>96.23%</cell><cell>96.57%</cell><cell>1500 mentions/sed</cell></row><row><cell cols="2">Bert tf idf 2 layers 83.93%</cell><cell>90.17%</cell><cell>91.33%</cell><cell>92.43%</cell><cell>1500 mentions/sed</cell></row><row><cell cols="4">4.7 Implication number prediction</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The result of multi implication number prediction</figDesc><table><row><cell>Method</cell><cell cols="3">Uni-implication Multi-implication Total</cell></row><row><cell cols="2">BERT-based ranking 100</cell><cell>-</cell><cell>96.3</cell></row><row><cell>delimiter +</cell><cell>96.6</cell><cell>70.3</cell><cell>95.6</cell></row><row><cell>Transformer</cell><cell>98.6</cell><cell>76.4</cell><cell>97</cell></row><row><cell>Bert-only</cell><cell>99.26</cell><cell>65.77</cell><cell>97.8</cell></row><row><cell>Bert-multi-task</cell><cell>99.58</cell><cell>70.71</cell><cell>98.23</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances and emerging applications in text and data mining for biomedical discovery</title>
		<author>
			<persName><forename type="first">Graciela</forename><forename type="middle">H</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tasnia</forename><surname>Tahsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">C</forename><surname>Britton C Goodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><forename type="middle">S</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Application of text mining in the biomedical domain</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wilco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynand</forename><surname>Fleuren</surname></persName>
		</author>
		<author>
			<persName><surname>Alkema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting early psychiatric read-mission with natural language processing of narrative discharge summaries</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><surname>Perlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Translational psychiatry</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="921" to="e921" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards drug safety surveillance and pharmacovigilance: current progress in detecting medication and adverse drug events from electronic health records</title>
		<author>
			<persName><forename type="first">Feifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhyuday</forename><surname>Jagannatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A knowledge-driven generative model for multi-implication chinese medical procedure entity normalization</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1490" to="1499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ulisboa: Recognition and normalization of medical concepts</title>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">M</forename><surname>Couto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015">SemEval 2015. 2015</date>
			<biblScope unit="page" from="406" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sieve-based entity linking for the biomedical domain</title>
		<author>
			<persName><forename type="first">D'</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audis: an automatic crfenhanced disease normalization in biomedical text</title>
		<author>
			<persName><forename type="first">Hsin-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Yu</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep neural models for medical concept normalization in user-generated texts</title>
		<author>
			<persName><forename type="first">Zulfat</forename><surname>Miftahutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hybrid normalization method for medical concepts in clinical narrative using semantic matching</title>
		<author>
			<persName><forename type="first">Yen-Fu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<biblScope unit="page">732</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task character-level attentional networks for medical concept normalization</title>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengya</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1239" to="1256" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An ensemble cnn method for biomedical entity normalization</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 5th Workshop on BioNLP Open Shared Tasks</title>
		<meeting>The 5th Workshop on BioNLP Open Shared Tasks</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="143" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task medical concept normalization using multi-view convolutional neural network</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ishani</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukannya</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitesh</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitava</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahanandeeshwar</forename><surname>Gattu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11164</idno>
		<title level="m">Medical entity linking using triplet network</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lab indicators standardization method for the regional healthcare platform: a case study on heart failure</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Qi Ye, and Ping He</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert-based ranking for biomedical entity normalization</title>
		<author>
			<persName><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<biblScope unit="page">269</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cnn-based ranking for biomedical entity normalization</title>
		<author>
			<persName><forename type="first">Haodi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An effective standardization method for the lab indicators in regional medical health platform using n-grams and stacking</title>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1602" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using an ensemble of generalised linear and deep learning models in the smm4h 2017 medical concept normalisation task</title>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Belousov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Nenadic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Social Media Mining for Health Applications</title>
		<meeting>the Second Workshop on Social Media Mining for Health Applications</meeting>
		<imprint>
			<publisher>SMM4H). Health Language Processing Laboratory</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Normalising medical concepts in social media texts by learning semantic representation</title>
		<author>
			<persName><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generate-and-rank framework with semantic type regularization for biomedical concept normalization</title>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8452" to="8464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12832</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view response selection for humancomputer conversation</title>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sentence-bert: Sentence embeddings using siamese bert. Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extending long short-term memory for multi-view structured learning</title>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Sundar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="338" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04378</idno>
		<title level="m">Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<title level="m">Enhanced lstm for natural language inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<title level="m">Understanding the behaviors of bert in ranking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Runqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00300</idno>
		<title level="m">Simple and effective text matching with richer alignment features</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhanced-rcnn: An efficient method for learning sentence similarity</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengbin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niantao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2500" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
