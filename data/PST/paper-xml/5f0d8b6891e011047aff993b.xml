<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProtTrans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ghalia</forename><surname>Rehawi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
						</author>
						<title level="a" type="main">ProtTrans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bioinformatics</term>
					<term>High Performance Computing</term>
					<term>Natural Language Processing</term>
					<term>Language Modeling</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive language models (Transformer-XL, XLNet) and two auto-encoder models (Bert, Albert) on data from UniRef and BFD containing up to 393 billion amino acids (words) from 2.1 billion protein sequences (22-and 112-times the entire English Wikipedia). The LMs were trained on the Summit supercomputer at Oak Ridge National Laboratory (ORNL), using 936 nodes (total 5616 GPUs) and one TPU Pod (V3-512 or V3-1024). We validated the advantage of up-scaling LMs to larger models supported by bigger data by predicting secondary structure (3-states: Q3=76-84, 8-states: Q8=65-73), sub-cellular localization for 10 cellular compartments (Q10=74) and whether a protein is membrane-bound or water-soluble (Q2=89). Dimensionality reduction revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein shape. This implied learning some of the grammar of the language of life realized in protein sequences. The successful up-scaling of protein LMs through HPC to larger data sets slightly reduced the gap between models trained on evolutionary information and LMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>H igh-Performance Computing (HPC) has recently been advancing hand-in-hand with Deep Learning (DL) to achieve new scientific breakthroughs in both fields. More powerful supercomputers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and advanced libraries <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> enable the training of ever more complex models on bigger data sets using advanced processing units such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) at increasing speeds and efficiency. HPC hardware is advancing both through infrastructure of supercomputers, such as Fugaku <ref type="bibr" target="#b7">[8]</ref>, Summit <ref type="bibr" target="#b0">[1]</ref> or the SuperMUC-NG <ref type="bibr" target="#b8">[9]</ref>, and through its components, such as TPU pods <ref type="bibr" target="#b1">[2]</ref>, specifically designed to ease large scale neural network training for users. Concurrent software improvements in form of more efficient libraries such as Horovod <ref type="bibr" target="#b5">[6]</ref> allow executing general purpose code on large distributed clusters with minor code changes.</p><p>Through contextualized Language Models (LMs) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, Natural Language Processing (NLP) has been benefiting more from advances in HPC than other fields. In particular Transformers <ref type="bibr" target="#b11">[12]</ref> have reached state-of-the-art performance in several tasks including translation, summarization and question answering <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. LMs are trained on unlabelled data; this independence of expensive validated data opened vast sets of raw big data allowing to up-scale LMs in NLP by orders of magnitude. The self-supervised training exclusively relies upon the sequential order of the input. Two approaches make use of this information, namely auto-regressive (predict next token in a sequence, given all previous tokens) and auto-encoding (reconstruction of corrupted input) training. Once trained, LMs can extract features, referred to as embeddings, to use as input in subsequently trained supervised models (transfer-learning). This two-step training outsources the computationally expensive LM pre-training to the HPC infrastructure while the computationally simple inference can be done on commodity hardware.</p><p>Protein research provides an excellent use-case for transfer-learning as large amounts of exponentially growing but unlabelled data contrast much more limited sets with experimental annotations. One example for this is the "sequencestructure" gap <ref type="bibr" target="#b14">[15]</ref>, i.e. the gap between the number of proteins for which one-dimensional (1D) sequences are known and the orders of magnitude smaller subset of proteins for which their three-dimensional (3D) structures are known. Knowing these structures is crucial for understanding their function. Such understanding is needed, e.g. to possibly disrupt the binding of the spiky S1 protein of the SARS-Cov-2 virus that by binding to the human receptor ACE2 caused the COVID-19 pandemic. The sequence-structure and sequencefunction gaps, or more generally the sequence-annotation gaps keep growing exponentially. Closing those gaps through prediction methods based on artificial intelligence (AI) is one of the crucial challenges for computational biology and bioinformatics.</p><p>Recently, the leap of NLP through advanced LMs have successfully been generalized toward understanding the language of life through advanced LMs trained on proteins <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. The main concept behind these approaches is to interpret protein sequences as sentences and their constituent -amino acids -as single words. Protein sequences are constrained to adopt particular 3D shapes (referred to as protein 3D structure) optimized for accomplishing particular functions. These constraints mirror the rules of grammar and meaning in natural language thereby allowing to map algorithms from NLP directly onto protein sequences. During training, the LM learns to extract those constraints from millions of examples and store the derived knowledge in its weights. While existing solutions in Protein Bioinformatics <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> usually have to search for evolutionary related proteins in exponentially growing databases, LMs offer a potential alternative to this increasingly time-consuming database search as they extract features directly from single protein sequences. On top, the performance of existing solutions deteriorates if not a sufficient number of related sequences can be found, e.g. the quality of predicted protein structures correlates strongly with the number of effective sequences found in today's databases <ref type="bibr" target="#b30">[31]</ref>. Additionally, some proteins are intrinsically hard to align (e.g. intrinsically disordered proteins <ref type="bibr" target="#b31">[32]</ref> or proteins which do not have any related sequences (dark proteome, <ref type="bibr" target="#b32">[33]</ref>).</p><p>In this project (named by ProtTrans), we pursued two objectives. Firstly, we explored the limits of up-scaling language models trained on proteins as well as protein sequence databases used for training. Secondly, we compared the effects of auto-regressive and auto-encoding pre-training upon the success of the subsequent supervised training, and compared all LMs to existing state-of-the-art solutions using evolutionary information <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data for Language Models (LMs)</head><p>In this work, we assessed the impact of database size on performance through two data sets: UniRef100 <ref type="bibr" target="#b34">[35]</ref> (with 216M protein sequences) and BFD <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> (with 2,122M sequences). The latter merged all protein sequences available in UniProt <ref type="bibr" target="#b37">[38]</ref> and proteins translated from multiple metagenomic sequencing projects, making it the largest collection of protein sequences available at the time of writing. The original BFD set contained several copies of identical sequences; only one of those was kept, resulting in a subset with 2.1 billion (2.1B) protein sequences (with &gt;393B amino acids requiring 527GB of disk space as text); we dubbed this set as BFD. This compared to UniRef100 with 216M proteins (80B amino acids, 150GB disk space; Fig. <ref type="figure" target="#fig_0">1</ref>. Overall, BFD was about eight times larger than the largest data sets used previously <ref type="bibr" target="#b18">[19]</ref>. Despite the 8-fold increase in data, the number of tokens increased only five-fold (Fig. <ref type="figure" target="#fig_0">1b</ref>, because UniRef100 sequences were longer than those in BFD (1.6-fold). A similar trend held for disk storage (Fig. <ref type="figure" target="#fig_0">1c</ref>. Translating LMs from NLP to proteins interprets amino acids as words. Thereby, protein databases contain several orders of magnitude more tokens than corpora used in NLP, e.g., Google's Billion Word data set <ref type="bibr" target="#b38">[39]</ref> is one of the biggest for NLP with about 829 million tokens (words), i.e. about 500-times fewer than BFD with 393 billion tokens. Both UniRef100 and BFD were tokenized with a single space (indicating word-boundaries) between each token. Each protein sequence was stored on a separate line, with lines/proteins representing the equivalent of "sentences". Additionally, an empty line was inserted between each protein sequence in order to indicate the "end of a document" as some LMs such as Bert use consecutive sequences for an auxiliary task, i.e. next-sentence prediction, which was not used in this work. As a minor filtering step, all non-generic or unresolved amino acids (B, O, U, Z) were mapped to 'unknown' (X). After this pre-processing, Uniref100 required 150GB GB of storage, BFD 734 GB. For training ProtTXL, the data was transformed to pytorch tensors on the fly. For ProtBert and ProtAlbert, the data had to be pre-processed and stored as tensorflow records, raising the storage to 2.3TB and 22TB for UniRef100 and BFD, respectively. Given tensorflow records with terabytes, data sets had to be chunked into 6000 files for thousands of parallel workers. We also compared the amino acid frequencies between databases as shown in Fig. <ref type="figure" target="#fig_0">1d</ref> in order to detect potential biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data for supervised training</head><p>The information learnt by the LMs was condensed in form of embeddings which were compared quantitatively through their value for subsequent 2nd-step supervised training. Toward this end we used previously published data sets for ease of comparison to state-of-the-art methods based on evolutionary information and to methods extracting features through pretrained LMs.</p><p>Per-residue prediction: When predicting properties on the level of single residues, the data set published alongside NetSurfP-2.0 <ref type="bibr" target="#b24">[25]</ref> was used for 3-and 8-state secondary structure prediction. The NetSurfP-2.0 dataset was created through PISCES <ref type="bibr" target="#b39">[40]</ref> selecting highest resolution protein structures (resolution &lt;=2.5A) from the PDB <ref type="bibr" target="#b40">[41]</ref>. The set was redundancy-reduced such that no pair of proteins had &gt;25% pairwise sequence identity (PIDE), leaving 10791 proteins to train. About 500 proteins were randomly removed from this set and used as validation set to determine hyperparameters such as early stopping. The final performance was evaluated on three different data sets, each with &lt;25% PIDE to the training set: CB513 (513 proteins; <ref type="bibr" target="#b41">[42]</ref>), TS115 (115 proteins; <ref type="bibr" target="#b42">[43]</ref>) and CASP12 (21 proteins; <ref type="bibr" target="#b43">[44]</ref>).</p><p>Per-protein prediction: For the prediction of features of entire proteins, the DeepLoc <ref type="bibr" target="#b25">[26]</ref> data set was used to classify proteins into membrane-bound and water-soluble and for classifying proteins into ten classes of subcellular localization (also referred to as cellular compartments). This DeepLoc data set was created by pulling all proteins with experimentally annotated localization from UniProt (release: 2016_04). Proteins in this set were redundancy reduced at a level of PIDE&lt;30% and split into 6621 proteins for training and 1841 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data: unsupervised embeddings</head><p>The embeddings extracted by the LMs were also evaluated visually by projecting the high-dimensional representations down to two dimensions using t-SNE <ref type="bibr" target="#b44">[45]</ref>. A non-redundant (PIDE&lt;40%) version of the SCOPe database <ref type="bibr" target="#b45">[46]</ref> (release 2.07 with 14323 proteins) served as one way to interpret the t-SNE plots. For a subset of those proteins, we used experimentally annotated EC (Enzyme Commission <ref type="bibr" target="#b46">[47]</ref>) numbers for functional classifications. Taxonomic identifiers from UniProt mapped proteins into one of the three major domains of life (archaea, bacteria, or eukarya) or to viruses (removing all proteins with missing classifications). The number of iterations for the t-SNE projections was set to 3000 and the perplexity to 30 for all plots with the exception of the amino acid plot for which we used a perplexity of 5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Models stage 1: LMs to extract embeddings</head><p>In this work, four LMs which achieved significant improvements in NLP (Bert <ref type="bibr" target="#b47">[48]</ref>, Albert <ref type="bibr" target="#b48">[49]</ref>, Transformer-XL <ref type="bibr" target="#b49">[50]</ref> and XLNet <ref type="bibr" target="#b12">[13]</ref>) were trained on protein sequences. Bert was the first bidirectional model in NLP which tried to reconstruct corrupted tokens, and is considered the de-facto standard for transfer learning in NLP. Albert reduced Bert's complexity by hard parameter sharing between its attention layers which allows to increase the number of attention heads (64 chosen here). Transformer-XL was chosen because it overcomes the problem of having a maximum sequence length, which was inherent to all previous Transformer based models (including Bert and Albert). With the average length of an English sentence around 15-30 words <ref type="bibr" target="#b50">[51]</ref>, an upper sentence length limit is no problem for sentence-level NLP tasks but many proteins are more than 10-times longer resulting in an average length of about 350 residues (residues is the term used to describe amino acids joined in a protein sequence, i.e. the sentence length measured in number of words). For example, around 20% of the sequences in UniRef100 (216M sequences) are longer than 510. Transformer-XL still cuts sequences into fragments but allows for flow of information between fragments for longer proteins by re-using hidden states of fragments which have already been processed. This memory is uni-directional as fragments are processed sequentially. XLNet uses the memory mechanism introduced by Transformer-XL to also allow for processing of sequences of arbitrary length. While the memory remains uni-directional for both, Transformer-XL and XLNet, only XLNet allows to gather bidirectional context within one memory fragment while Transformer-XL has only access to uni-directional context. All these models were trained on UniRef100 and Transformer-XL was additionally trained on BFD (Table <ref type="table" target="#tab_3">1</ref> for model parameters). Largely, we used configurations successfully transferred from NLP to protein sequences <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b51">[52]</ref>, with the exception of the number of layers that was increased to optimize memory utilization. Bert, TransformerXL and XLNet were trained with a hidden layer size (dimensionality of the features which can be extracted) of 1024 while Albert was trained with a hidden layer size of 4096. Models which use positional encoding like Bert and Albert, can process only sequences shorter or equal to the length of the positional encoding which has to be set before training. Setting the length of the positional encoding to 40k allowed the models to process protein sequences up to a length of 40k. Albert, Bert and Transformer-XL were optimized using the Lamb optimizer <ref type="bibr" target="#b52">[53]</ref> designed for large batch sizes, while XLNet was optimized using Adam. No auxiliary tasks like Bert's nextsentence prediction were used for any model described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ProtTXL:</head><p>The Transformer-XL versions trained here on protein sequences are referred to as to ProtTXL (only ProtTXL when trained on UniRef100 and ProtTXL-BFD when trained on BFD). Both LMs were trained with the configuration shown in Table <ref type="table" target="#tab_3">1</ref>, sharing a dropout rate of 15%, a memory length of 512 tokens and using mixed precision. The number of layers, number of heads, batch size, learning rate, weight decay, training steps and warm-up steps were adjusted according to training set size as well as GPU utilization. We focused especially on the complex interplay between learning rate and the number of warm-up steps which was shown to be crucial to prevent deeper layers of creating instability during training <ref type="bibr" target="#b53">[54]</ref> and speed-up model convergence <ref type="bibr" target="#b54">[55]</ref>. Here, the number of warm-up steps was set to cover at least one epoch for each data set. We tested initial learning rates between 0.001 and 0.005 which were increased linearly at every training step over the warm-up period. To avoid model divergence during training, the learning rate had to be (i) reduced along with the warm-up steps (for BFD), or (ii) increased for both (for Uniref100). Even after increasing the warm-up steps to two epochs, the maximum learning rate remained at 0.0025  for both data sets. Beyond this point, the training diverged.</p><p>Using weight decay to regularize the network increased the GPU memory usage as it required to compute the norm of all weight vectors on our models, thus reducing the batch size. ProtTXL-BFD was trained for 40k steps in total, with 13.6k warm-up steps using a learning rate of 0.0005, while ProtTXL was trained for 31k steps with 5k warm-up steps using a learning rate of 0.002. The Lamb optimizer was able to handle the resulting batch sizes of 44k and 22k for ProtTXL-BFD and ProtTXL, respectively, without divergence. ProtBert: Bert was trained using both UniRef100 and BFD-100 datasets, these two versions are referred as ProtBert and ProtBert-BFD, respectively. Both LMs were trained with the configuration shown in Table <ref type="table" target="#tab_3">1</ref>. Compared to the original Bert publication, the number of layers was increased in order to potentially reach better performance in supervised downstream tasks, while keeping inference time as well as GPU memory consumption at a reasonable level. Unlike Transformer-XL which was trained on Nvidia GPUs, mixedprecision was not used to train other models because those were trained on TPUs. Similar to the Bert version trained in the Lamb paper <ref type="bibr" target="#b52">[53]</ref>, ProtBert was first trained for 300k steps on sequences with a maximum length of 512 and then for another 100k steps on sequences with a length of a maximum length of 2k. While ProtBert-BFD was trained for 800k steps, then for another 200k steps for sequences with maximum length of 512 and 2k, respectively .This allows the model to first extract useful features from shorter sequences while using a bigger batch size, which makes training on longer sequences and thus overall training more efficient.</p><p>ProtAlbert: We referred to Albert trained on UniRef100 as to ProtAlbert. We used the configuration from the official GitHub repository for Albert (version: xxlarge v2) with 12 attention layers. For Albert the number of layers is increased through the number of times that Albert stacks its single layer. Compared to the original publication, we were able to increase the global batch size from 4096 to 10752 despite using the same hardware. The reason for this counter-intuitive effect is the reduced vocabulary size in protein sequences because the entire diversity of the protein universe is mapped to 20 different amino acids, compared to tens of thousands of different words. As ProtAlbert was also trained on TPUs, no mixed-precision was used for training. Similar to ProtBert, ProtAlbert was first trained for 150k steps on sequences with a maximum length of 512 and then for another 150k steps on sequences with a maximum length of 2k.</p><p>ProtXLNet: XLNet was trained on UniRef100 (Pro-tXLNet) using the original NLP configuration <ref type="bibr" target="#b12">[13]</ref> (Table <ref type="table" target="#tab_3">1</ref>) except for the number of layers that was increased to 30 layers which reduced the global batch size to 1024. Due to the relatively small batch-size, we used the original optimizer: Adam with a learning rate of 0.00001. The model was trained through more steps, i.e. 20k warm-up and 847k steps to compensate for the smaller batch-size of this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Models stage 2: supervised models</head><p>The second-stage supervised models using the embeddings from the LMs as input were deliberately kept relatively minimal to focus the differential analysis on the power of the LM embeddings. All our experiments used the pre-trained LMs as feature extractors without fine-tuning, i.e. without gradient back-propagating to the LMs. Thereby, we could proxy the information contained in the embeddings through the performance of the supervised tasks. The supervised models have been described before <ref type="bibr" target="#b16">[17]</ref>. To briefly summarize: we applied tasks on two different levels, namely per-residue and per-protein predictions. For the per-residue prediction a simple two-layer convolutional neural network (CNN) was trained on the embeddings. The first layer of our CNN compressed the output of the language models down to 32 dimensions using a window size of 7 (1024 for ProtBert, Prot-TXL and ProtXLNet, 4096 for ProtAlbert). The compressed representation was fed to two different CNNs each having again a window size of 7. One of these CNNs was trained on predicting secondary structure in 3-states, the other was trained on predicting 8-states. The network was trained on both outputs simultaneously by adding their losses (multitask learning). For the per-protein prediction features were also extracted from the last layer of the LMs. However, for this task the representations were averaged (mean-pooled) over the length-dimension of the protein resulting in a fixed-size representation for all proteins. The resulting vector (1024dimensional for ProtBert and ProtTXL, 4096-dimensional for ProtAlbert) was used as an input to a single feed forward layer with 32 neurons which compressed information before making the final predictions for both per-protein tasks simultaneously (multi-task learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Hardware ORNL Summit &amp; Rhea:</head><p>The Oak Ridge National Laboratory (ORNL) provides several clusters for researchers who need computational resources not provided by research facilities such as universities. Here, we used Summit and Rhea. Summit was used to train the deep learning models, while Rhea was used for the pre-processing of data sets including the distributed generation of tensorflow records.</p><p>Summit is the world's second fastest computer, consisting of approximately 4618 nodes. Each node has two IBM POWER9 processors and six NVIDIA Volta V100 with 16GB of memory each (Figure 2 <ref type="bibr" target="#b0">[1]</ref>). Every POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in both directions. A single node has 0.5 TB of DDR4 main memory and 1.6TB of non-volatile memory that can be used as a burst buffer. Summit is divided into racks with each rack having 18 nodes. In all of our experiments we reserved 936 nodes for training. As having nodes on the same rack decreases the communication overhead, we reserved entire racks.</p><p>The smaller cluster (Rhea) contains two partitions: Rhea and GPU. The Rhea partition has 512 node, each with 128 GB of memory and two Intel R Xeon R E5-2650. The GPU partition has only 9 nodes, each with 1 TB of memory and two Intel R Xeon R E5-2695. Reha reduced the time needed for creating tensorflow records for the BFD dataset from 7.5 months (!) to fewer than two days, by converting the original sequential script to distributed processing using MPI. The generation script used two nodes of the GPU partition, with a total of 112 parallel threads.</p><p>Google TPU Pod: In 2016, Google introduced tensor processing unit (TPU) as its application-specific integrated circuit optimized for training neural networks. TPUs can be accessed through Google Cloud. Training the protein LMs used the latest TPU generation (V3) with 512 cores. These cores are divided into hosts with each host having access to 8 cores. Consequently, we had access to 64 hosts, and each core had 16 GiB of high-bandwidth memory. Training on the TPUs required access to a virtual machine on Google Cloud and storage on Google Bucket <ref type="bibr" target="#b55">[56]</ref>. The workflow as well as the different scales of TPUs are depicted in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Software</head><p>Summit integrates several pre-configured modules which include the most popular libraries and tools required for simu-lation, deep learning, distributed training and other purposes. We used the IBM Watson Machine Learning module versions 1.6.0 and 1.6.2 for our deep learning training. In contrast to this, the Google Cloud server, which we used for the TPU Pod training, had to be configured manually because only the operating system was installed.</p><p>Pytorch was used to train ProtTXL, tensorflow to train ProtBert, ProtAlbert and ProtXLNet. Both libraries used the Horovod framework <ref type="bibr" target="#b5">[6]</ref> to train the models on distributed clusters such as Summit. Horovod supports distributed GPU training with minimal change in the code. It supports different backends including MPI, NCCL and IBM PowerAI distributed deep learning (DDL). We tested all three backends and found DDL to be the fastest for our training purpose on Summit. The time needed to finish a single batch with ProtTXL-BFD increased from one to two nodes due to the communication overhead (Fig. <ref type="figure" target="#fig_7">4</ref>). After two nodes the communication overhead plateaued, even when scaling up to 936 nodes with 5616 GPUs. Summit has integrated DDL in their Watson Machine Learning module which comes with most DDL libraries including pytorch, tensorflow, apex, DDL and horovod. However, Summit has only a license for using DDL up to 954 nodes. Contrary to Summit, training on TPU Pods did not require any changes in the Tensorflow code to use either a single TPU host or to distribute workload among multiple TPU hosts.</p><p>Mixed precision allows to fit bigger models and batch sizes into GPU memory by using 16-bit precision only or a mix of 16-bit and 32-bit precision. Nvidia's APEX library <ref type="bibr" target="#b56">[57]</ref> was used for mixed precision training of ProtTXL, due to its pytorch support. As ProtTXL training became instable when training with 16 Bit precision, we switched to almost half precision training (storing all model weights at 16 Bit precision; exception: batch-normalization layers), while keeping a master copy of the model's weights in 32 Bit. We did not use mixed-precision for models trained on TPUs.</p><p>Another optimization technique/library crucial for our training on Summit was IBM's large model support (LMS) <ref type="bibr" target="#b57">[58]</ref>. Similar to gradient checkpointing <ref type="bibr" target="#b58">[59]</ref>, LMS virtually extends the GPU memory by outsourcing parts of the model from GPU to main memory. This allows training models larger than the GPU memory. The obvious drawback of LMS is the increase in training time due to shuttling data between CPU and GPU and back. However, the reduced memory consumption of the model allows to increase the batch size, potentially compensating for the communication overhead. Compared to gradient checkpointing, LMS provides easier integration into existing code by operating directly on a computational graph defined by users and automatically adds swap-in and swapout nodes for transferring tensors from GPU memory to main memory and vice versa. We have tested LMS on ProtTXL as well as ProtBert (Figure <ref type="figure" target="#fig_7">4</ref>). As Pytorch and tensorflow have different strategies to integrate LMS, we also compared the effect of LMS on batch-size, model size and training time using the two different libraries. ProtTXL was used to evaluate the effect of Pytorch's implementation of LMS while ProtBert was trained for a few steps BFD using Summit to evaluate tensorflow's implementation of LMS. Training ProtBert for a few steps was sufficient to assess the effect of LMS on batchsize, model size as well as an estimate of training time. In the end, we used LMS only for ProtTXL to strike a balance between model size and training time. The number of LM parameters could be increased by about 15.6% for ProtTXL-BFD and to 6.6% for ProtBert (5a). Additionally, we could increase the batch size by 700% for ProtTXL-BFD (Figures <ref type="figure" target="#fig_2">5b and 5c</ref>). The NV-Link between CPU and GPU on Summitnodes, reduced the training time for ProtTXL by 60%while it increased by 72% for ProtBert (Figure <ref type="figure" target="#fig_2">5d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised embeddings from LMs informative</head><p>The embeddings extract some of the information learned by the LMs in the first stage of unsupervised learning. To establish that our protein LMs have extracted an understanding akin to the grammar in NLP, we projected the highdimensional embedding space down to two dimensions using t-SNE <ref type="bibr" target="#b44">[45]</ref> and visualized proteins according to annotated structural, functional or evolutionary information.</p><p>Capturing biophysical features of amino acids. Applying t-SNE to the first embedding layer visualized information extracted by the LMs representing individual amino acids irrespective of their surrounding context (residues next to it). As previously established for another protein LM <ref type="bibr" target="#b23">[24]</ref>, the t-SNE projections (e.g. ProtBert Fig. <ref type="figure">6A</ref>) suggested that all LMs captured essential biophysical aspects of amino acids. These included charge, polarity, amino acid size (small amino acids A, C, G, P, S, T separated from large F, H, R, W, Y), hydrophobicity, even to the level of aliphatic (A, I, L, M, V) vs. aromatic (W, F, Y).</p><p>Capturing protein structure classes. To assess which aspects of protein structure were captured by the unsupervised LMs, we averaged over the length-dimension of the representations derived from the last layer of each model. This created fixed-size representations for each protein. These we applied to the SCOPe database <ref type="bibr" target="#b45">[46]</ref> classifying proteins by their 3D structures (Methods). On the most coarse-grained level, SCOPe distinguishes between all-alpha, all-beta, al-pha|beta, alpha&amp;beta, multi-domain, membrane/cell surface and small proteins. ProtTXL (SOM Fig. <ref type="figure" target="#fig_7">14D</ref>) and ProtBert (SOM Fig. <ref type="figure" target="#fig_2">15D</ref>) produced higher entropy embeddings, while ProtAlbert (SOM Fig. <ref type="figure" target="#fig_0">12D</ref>), ProtXLNet (SOM Fig. <ref type="figure" target="#fig_0">13D</ref>) and ProtBert-BFD (6D) packed proteins into denser clusters. Consequently, ProtAlbert, ProtXLNet and especially ProtBert-BFD embeddings visually separated the proteins better than ProtTXL embeddings. Although sequence length was not explicitly encoded and our pooling squeezed sequences to a fixed vector size, all models separated small from long proteins (light blue, e.g. ProtBert-BFD Fig. <ref type="figure">6D</ref>). All models also to distinguished between water-soluble and transmembrane proteins (brown, e.g. ProtBert-BFD Fig. <ref type="figure">6D</ref>) and to some extent secondary structure composition, i.e. all-alpha versus all-beta (dark blue vs. dark green, e.g. ProtBert-BFD Fig. <ref type="figure">6D</ref>).</p><p>Capturing aspects of protein function. Using the same proteins as for SCOPe but different annotations (ECnumbers <ref type="bibr" target="#b59">[60]</ref>), we assessed whether the LM embeddings captured aspects of protein function, namely EC numbers (proteins from SCOPe without known ECs were removed, making Figs. 6F and 6D not directly comparable). Although most proteins were scattered for all LMs, ProtTXL clustered some proteins into transferases, hydrolases and oxidoreductases (particular types of enzymes, SOM 14F).</p><p>Capturing domains of life and viruses. The following three domains of life are distinguished: archaea, bacteria, and eukarya, while viruses are not considered as life.</p><p>We used the same SCOPe proteins and fixed-size representations for this analysis. Despite being trained differently (ProtTXL/ProtXLNet predicting next token vs. Prot-Bert/ProtAlbert reconstructing noise), all models captured domain-specific aspects (e.g. ProtBert-BFD Fig. <ref type="figure">6E</ref>). In general, Eukarya and bacteria were separated best by all LMs, while viruses and archaea formed less homogeneous clusters. When comparing the different LMs, the same trend as for protein structure classes could be observed: ProtTXL (SOM 14E) and ProtBert (SOM 15E) produced higher entropy clusters while ProtAlbert (SOM 15E) and ProtXLNet (SOM 13E) produce visually easier separable clusters.</p><p>Using a different protein set <ref type="bibr" target="#b25">[26]</ref>, we analyzed whether or not the embeddings captured aspects of protein function as proxied by the cellular compartment (also referred to as subcellular localization) and membrane-association. All LMs distinguished some aspects of localization with nuclear and extracellular proteins forming the most coherent clusters (e.g. ProtBert-BFD Fig. <ref type="figure">6C</ref>). The LMs also picked up the membrane-association, clustering most proteins homogeneously (Fig. <ref type="figure">6B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embeddings good input for supervised predictions</head><p>Successful protein predictions exclusively using embeddings as input constitutes an even more important acid test than any statistical clustering analysis could. Toward this end, we compared secondary structure (per-residue level) and localization (per-protein level) predictions, along with the classification into membrane/non-membrane proteins (per-protein level). Protein LMs were only used as feature extractors, i.e. LMs were not fine-tuned on a specific task.</p><p>Per-residue prediction of secondary structure. Secondary structure was predicted by CNNs using only embeddings extracted from the last layer of our pre-trained LMs. All models were evaluated using standard measures for performance (Q3/Q8: three/eight-state per-residue accuracy, i.e. percentage of residues predicted correctly in either of the 3/8 states). Performance differed slightly between different data sets: from Q3(CASP12)=71-76% (interval marks one standard error), over Q3(CB513)=74-83%, to Q3(TS115)=75-84% (Fig. <ref type="figure">7</ref>; results for 8-state predictions confined to Fig. <ref type="figure" target="#fig_0">10</ref> Supplementary Material). The computed standard error intervals fail to completely reflect the real spread of the data, because the three data sets were not consistent, i.e. the average over their performance differed by some level of statistical significance. Ultimately, this reflects problems with each of those data sets: CASP12 was too small, but completely new to all methods compared; CB513 was the largest set (513 proteins), but allowed for substantial redundancy, and TS115 (115 proteins) allowed for even more redundancy. Despite these shortcomings, these data sets enabled direct comparison to state-of-the-art methods using evolutionary information.</p><p>For simplicity, we use the worst and the best performance among the three data sets in the following to highlight the performance variation depending on the test set. For the four LMs trained on UniRef100 this resulted in Q3(ProtTXL)=71-75, Q3(ProtBert)=75-83, Q3(ProtAlbert)=74-82, and Q3(ProtXLNet)=73-81 (for 8-states: Q8(ProtTXL)=59-63, Q8(ProtBert)=63-72, Q8(ProtAlbert)=62-70 and Q8(ProtXLNet)=62-69). For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases Ligases Translocases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eukaryota Bacteria</head><p>Fig. <ref type="figure">6</ref>: Unsupervised training captures various features of proteins: We used t-SNE projections to assess which features the LMs trained here learnt to extract from proteins. Exemplarily for ProtBert-BFD, the best-performing model on supervised tasks, we showed that the protein LMs trained here captured biophysical-and biochemical properties of single amino acids during pre-training (Panel A). A redundancy reduced version (30%) of the DeepLoc <ref type="bibr" target="#b25">[26]</ref> dataset was used to assess whether the LM learnt to classify proteins into membrane-bound and water-soluble (Panel B) or according to their cellular compartment (Panel C). Not all proteins in the set had annotations for both features, making Panels B and C not directly comparable. Further, a redundancy reduced version (40%) of the Structural Classification of Proteins -extended (SCOPe) database was used to assess whether ProtBert-BFD captured structural (Panel D), functional (Panel F) or lineage-specific (Panel E) features of proteins without any labels. Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtBert-BFD (average over the length of the protein). The high-dimensional embeddings were projected to 2D using t-SNE. ProtBert-BFD captured protein information on different levels: ranging from structural features as annotated in the main classes in SCOPe, over functional aspects as defined by in the Enzyme Commission (E.C.) numbers or the cellular compartment to the branch of the protein within the tree of life, without ever having been explicitly trained on any of these features. Comparing different features for the same datasets revealed that potentially heterogeneous clusters are only formed due to the multi-modal nature of proteins, e.g. the eukaryotic proteins are well separated from bacterial proteins (Panel E) but form internally multiple sub-clusters in structure space (Panel D). Fig. <ref type="figure">7</ref>: Performance comparison of Language models on supervised tasks: the predictive power of the embeddings derived from the Language Models (LMs) trained here (ProtBert, ProtAlbert, ProtTXL, ProtXLNet) was assessed via threestate secondary structure prediction (y-axis: Q3). To simplify comparability to other approaches, we used the same training and test data sets (red:CASP12, yellow:TS115, blue:CB513) as an existing approach, i.e. NetSurfP-2.0 <ref type="bibr" target="#b24">[25]</ref>. All LMs developed here were evaluated by training a simple network on top of the representations extracted from the last layer of the pre-trained LMs. As comparison, a method using evolutionary information was also added (NetSurfP-2.0, left side of the bar chart). Approaches using only the proposed embeddings (ProtBert, ProtAlbert, ProtTXL, ProtXLNet) are located one the right side of the bar chart. While outperforming uncontextualized (ProtVec <ref type="bibr" target="#b60">[61]</ref>) as well as existing, LSTM-based LMs (SeqVec <ref type="bibr" target="#b16">[17]</ref>), all LMs trained here still fall short compared to methods using evolutionary information.</p><p>ProtTXL and ProtBert we could also analyze the influence of the size of the database used to train the LMs: the 10-times larger BFD improved slightly over UniRef100, i.e. Q3(ProtBert-BFD) -Q3(ProtBert)= +1.3 and Q8(ProtBert-BFD) -Q8(ProtBert)= +2.3. For ProtTXL only a minimal improvement could be observed for TS115, i.e. Q3 and Q8 improved by one percentage point. However, none of these differences was statistically significant, especially, in the light of the relatively high variation between test sets. All databases and all models (Prot-TXL/ProtBert/ProtAlbert/ProtXLNet, BFD/UniRef) improved significantly over the approach using only contextfree feature extractors such as word2vec-based approaches (dubbed DeepProtVec in Figs. <ref type="figure" target="#fig_0">7 and 10</ref>). However, none of the solutions improved in any way over the state-of-the-art methods using evolutionary information (methods left of the dashed vertical line in Figs. <ref type="figure" target="#fig_0">7 and 10</ref>), with ProtBert-BFD reducing the gap between those different approaches.</p><p>Per-protein prediction of 10-state localization and 2-state membrane/non-membrane proteins. The feed forward model was trained to predict protein localization in ten different classes and to binary classify membrane/non-membrane proteins. For simplicity, performance was evaluated using standard accuracy (Q10 for localization, Q2 for membrane/non-membrane). ProtBert and ProtAlbert numerically performed best: Q10(ProtBert)=74, Q10(ProtAlbert)=74, while ProtTXL as well as ProtXL-Net performed substantially worse: Q10(ProtTXL)=66, Q10(ProtXLNet)=68. The 10-fold increase from UniRef100 to BFD when training ProtTXL or ProtBert appeared to have little effect: Q10(ProtTXL-BFD)=65, Q10(ProtBert-BFD)=74 (Fig. <ref type="figure">8</ref>). However, again those differences were not statistically significant either way.</p><p>For the binary classification into membrane/nonmembrane proteins (Q2), the trend observed for localization (Q10) largely remained: ProtBert and ProtAlbert performed best (Q2(ProtBert)=89, Q2(ProtAlbert)=88, Fig. <ref type="figure">8</ref>). However, for Q2 ProtXLNet largely closed the performance gap from Q2 (Q2(ProtXLNet)=87) while ProtTXL again performed worst (Q2(ProtTXL)=85). As for localization, there was little difference between the small (UniRef100) and large (BFD) data set used for generating the LMs: Q2(ProtTXL) -Q2(ProtTXL-BFD) = +1 and Q2(ProtBert) -Q2(ProtBert-BFD) = 0, although the trend form localization (worse for larger data set) was reversed.</p><p>On one hand, the per-protein predictions using only embeddings as input, like those for secondary structure, remained behind the best state-of-the-art methods using evolutionary information (methods left of the dashed vertical line in Fig. <ref type="figure">8</ref>). On the other hand, performance was substantially and statistically significantly higher for ProtAlbert/ProtBert/ProtTXL/ProtXLNet than for the word2veclike solutions (DeepProtVec in Fig. <ref type="figure">8</ref>). However, in contrast to the per-residue solutions, the per-protein predictions outperformed some popular methods that did use evolutionary information (Fig. <ref type="figure">8</ref>), specifically ProtBert-BFD reached a value only a few percentage points below the current state-ofthe-art using evolutionary information (Q2(ProtBert-BFD)-  </p><note type="other">Evolutionary Information Language Modelling 78</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subcell Mem</head><p>Fig. <ref type="figure">8</ref>: Performance comparison on protein-level supervised tasks: the protein LMs trained here (ProtTXL, ProtBert, ProtAlbert, ProtXLNet) were compared on the prediction of subcellular localization in 10-states (red bars) as well as on classifying proteins into membrane-bound and soluble (blue bars) using the dataset of an existing approach, i.e. DeepLoc <ref type="bibr" target="#b25">[26]</ref>). A simple two-layer neural network is trained on top of fixed-size representations for each protein which were derived by averaging over the length dimension of embeddings extracted from the last layer of the language models. The performance of all our LMs falls short when being compared to an existing approach which uses evolutionary information (DeepLoc). However, transformer-based protein LMs introduced here outperform previously published LSTM-based protein LM approaches (DeepSeqVec) as well as uncontextualized approaches using word2vec (DeepProtVec). Q2(Deeploc)=-3, Q10(ProtBert-BFD)-Q10(DeepLoc)=-4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fast predictions from embeddings</head><p>Although embedding-based predictions were less accurate than those using evolutionary information, one crucial advantage of representations derived from protein LMs is their speed-up compared to database searches required to generate evolutionary information. This speed-up was quantified by comparing the time required to generate representations for each protein in the human proteome (20.353 proteins with a median sequence length of 415 residues) using our protein LMs or mmseqs2 <ref type="bibr" target="#b61">[62]</ref>, the fastest tool to gather evolutionary information from protein sequence databases at the moment. The same parameters as in NetSurfP-2.0 <ref type="bibr" target="#b24">[25]</ref> were used to search with mmseqs2 the human proteome against two large protein sequence database (UniRef90=113M and UniRef100=216M proteins), i.e. the number of iterations was set to two (profile search) and the maximum number of sequences passing the pre-filtering was set to 2.000. For the database search we used an IntelR c XeonR c Scalable Processor "Skylake" Gold 6248 with 40 threads, SSD and 377GB main memory, while protein LMs were run on a single Nvidia P100 with 16GB memory using dynamic batch size based on the variable sequences length. Using the experimental setup described above, mmseqs2 is around 8-or 4-times slower than the fastest LMs (SeqVec and ProtBert, Fig. <ref type="figure">9 (a)</ref>) when searching UniRef100 or UniRef90, respectively.</p><p>When checking the effect of protein sequence length on the inference speed of protein LMs (Fig. <ref type="figure" target="#fig_9">11 (b</ref>)), we noticed that SeqVec is the slowest model (9.92s) for long proteins (up to 4096 residues), while ProtBert is the fastest (0.91s). We used only single sequence processing on a Nvidia Titan V with 12GB vRAM.</p><p>We also investigated the cross-effect of sequence length and batch-size (see Table <ref type="table" target="#tab_6">2</ref>) on the inference speed of different protein LMs. When using a single Nvidia Titan V on varying batch-sizes <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref> as well as sequence lengths (128, 256, 512), SeqVec provided the fastest inference with an average of 0.02 seconds per protein when using a batch size of 32, followed by ProtBert (0.03s). However, the batch-size of Prot-Bert could have been further increased on the same hardware but was limited to allow a direct comparison between all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Supercomputers such as the green energy-driven Summit <ref type="bibr" target="#b0">[1]</ref> and Google's cloud TPU Pod <ref type="bibr" target="#b1">[2]</ref>, combined with optimized libraries such as IBM DDL <ref type="bibr" target="#b6">[7]</ref> and Horovod <ref type="bibr" target="#b5">[6]</ref> set the stage for training LMs with billions of free parameters on large corpora with terabytes of data in hours or days. Increasing model size improves performance for some NLP applications <ref type="bibr" target="#b13">[14]</ref>, although the massive data challenges the communication between thousands of nodes and divergence between large batches during training. Here, we presented some solutions to overcome these challenges by fully utilizing 20% of Summit for the training of TransformerXL <ref type="bibr" target="#b49">[50]</ref>, as well as, by using one TPU Pod V3-512 for the training of Bert <ref type="bibr" target="#b47">[48]</ref>, Albert <ref type="bibr" target="#b48">[49]</ref> and XLNet <ref type="bibr" target="#b12">[13]</ref> on protein sequences. This translated into the parallel use of 5616 GPUs on Summit or 512 TPU cores on a EV LM Fig. <ref type="figure">9</ref>: Inference Speed Comparison: The time required to generate protein representations for the human proteome (20.353 proteins) is compared using either our protein LMs or mmseqs2 (protein sequence search tool <ref type="bibr" target="#b61">[62]</ref> used to generate evolutionary information; NetSurfP-2.0 <ref type="bibr" target="#b24">[25]</ref> parameters are used). Here, we used mmseqs2 (red bar) to search each protein in the human proteome against two large protein sequence database (UniRef90 and UniRef100 with 113M and 216M proteins, respectively). Only embedding or search time is reported, i.e. no pre-processing or pre-training was measured. mmseqs2 was run on a Intel R Xeon R Scalable Processor "Skylake" Gold 6248 with 40 threads, SSD and 377GB main memory, while protein LMs were run on a single Nvidia P100 with 16GB memory using dynamic batch size depending on sequence length (blue bar). TPU Pod, while avoiding training divergence with specialized optimizers such as LAMB <ref type="bibr" target="#b52">[53]</ref> up to a global batch size of 44K samples (here: proteins).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">HPC challenges for larger protein LMs on Summit</head><p>Up-scaling LMs to the enormous sizes of protein databases (our largest data set of BFD contained 112-times the number of words in the English Wikipedia) on Summit threw up six main challenges that we addressed as follows.</p><p>(1) Architecture: Summit is based on IBM Power processors; most libraries and software tools are written for Intel and AMD. This makes finding compatible tools directly from the developers often challenging. However, the IBM Watson Machine Learning Module, included almost all necessary deep learning libraries, for others common package management tools such as Anaconda <ref type="bibr" target="#b62">[63]</ref> were available.</p><p>(2) Communication overhead: large-scale training increased the communication overhead. IBM DDL used the least computation time on Summit.</p><p>(3) Distributed training: using thousands of GPUs with Tensorflow <ref type="bibr" target="#b2">[3]</ref> and Pytorch <ref type="bibr" target="#b3">[4]</ref> required extremely efficient distributed communication between nodes and assignment of work loads (tokenized text files) to workers (GPUs). Horovod <ref type="bibr" target="#b5">[6]</ref> provided the best training for both of these frameworks on Summit.</p><p>(4) File sharing: parallel file access can increase run-time.</p><p>During training, multiple nodes access the same files holding model parameters and logs. To address this, separate log copies for each node were used, while storing a single copy of the model on the master node. Data set files remained shared, not impairing file reading.</p><p>(5) Pre-processing:, especially tokenization, of batches on the fly increased GPU waiting and CPU processing while reducing storage. For small data sets (few GBs), pre-processing and disk-storing batches before training appeared optimal. For large data sets (TBs), there was a trade-off between disk space and training time. In our hands, the best solution was using ORNL's Rhea cluster, cutting pre-processing from &gt;215 to &lt;2 days through MPI. (6) Deep learning library: The integration of LMS into Pytorch (ProtTXL) required adjusting only a few parameters; in contrast, Tensorflow (ProtBert) required more code changes. Tensorflow might compensate for this problem by auto-tuning certain parameters such as the memory usage; however, for our use-case, this failed. The different parameters for Pytorch and Tensorflow resulted in different behaviors with respect to swapping in and out nodes between GPU and CPU. This in turn varied speed and model/batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised LMs learned simple protein features</head><p>Some rudimentary information about how proteins are formed, shaped, and function has been learned by the LMs because all models (ProtBert, ProtAlbert, ProtTXL, ProtXL-Net) extracted valuable information as revealed by the embeddings. The basic understanding extended from biophysical features of the amino acid building blocks (e.g. hydrophobicity, charge, and size, Fig. <ref type="figure">6A</ref>), over classifications of protein structure (Fig. <ref type="figure">6D</ref>), and protein function (Fig. <ref type="figure">6F</ref>), to the macroscopic level of the domains of life (Fig. <ref type="figure">6E</ref>). Global structural properties (e.g. overall secondary structure content, Fig. <ref type="figure">6D</ref>) and global biochemical properties (e.g. membraneboundness, Fig. <ref type="figure">6B</ref>) appeared most distinctive. In contrast, local features relying on short motifs were less separated (ECnumbers: Fig. <ref type="figure">6F</ref>, localization: Fig. <ref type="figure">6C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bi-directional beats uni-directional for proteins</head><p>The t-SNE and UMAP analyses suggested that the LMs had extracted some level of understanding of the language of life. However, any such statistical differences have ultimately limited validity unless they are predictive. In this sense prediction is the acid test of understanding. To pass this test, we extracted the embeddings learned by the LMs directly as input to predict aspects of protein structure and function (per-residue prediction of secondary structure and per-protein prediction of localization and membrane/non-membrane). Overall, the supervised results confirmed <ref type="bibr" target="#b16">[17]</ref> that evolutionary information scientifically and statistically significantly outperformed LMs not using such information (on all perresidue 7,10 and per-protein tasks 8). However, ProtBert-BFD reduced the gap from embeddings-only input to those approaches. Newer contextual models improved both over previous LM-based approaches <ref type="bibr" target="#b16">[17]</ref> (3-5 percentage points in Q3) and over non-contextualized word2vec-type approaches <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref> (12-17 percentage points in Q3). A merger of models using evolutionary information and embeddings might bring the best.</p><p>In NLP uni-directional models (auto-regressive) perform on par with bi-directional models (auto-encoding) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b66">[67]</ref>. In contrast, it bi-directional context appeared crucial to model aspects of the language of life. While autoencoding models such as Albert <ref type="bibr" target="#b48">[49]</ref> utilize context to both sides during loss calculation, auto-regressive models such as TransformerXL <ref type="bibr" target="#b49">[50]</ref> consider only context to one side. This difference resulted in a substantial performance difference between ProtTXL and ProtXLNet (XLNet extends Transformer-XL to capture bi-directional context), both trained on UniRef100: Q3(ProtXLNet)-Q3(ProtTXL)=3.6, Q8(ProtXLNet)-Q8(ProtTXL)=4.0, Q10(ProtXLNet)-Q10(ProtTXL)=2, Q2(ProtXLNet)-Q2(ProtTXL)=2. This might be compensated for by first pre-train on sequences and their reverse and then concatenating the output of unidirectional LMs applied on both directions. While this does not allow the LM to use bi-directional context during training, it allows supervised networks to combine context derived independently from both sides. One example for an autoregressive model that makes use of this is ELMo <ref type="bibr" target="#b9">[10]</ref> which concatenates the embeddings derived from a forward and a backward LSTM. Interestingly, ELMo trained on protein sequences (SeqVec) performs better than the uni-directional ProtTXL but worse (Q3,Q8) or equal (Q2,Q10) than the bi-directional ProtXLNet: Q3(ProtXLNet)-Q3(SeqVec)=1.0, Q8(ProtXLNet)-Q8(SeqVec)=0.7, Q10(ProtXLNet)-Q10(SeqVec)=0, Q2(ProtXLNet)-Q2(SeqVec)=0. While part of this difference might be explained by the difference in model size (SeqVec=93M vs. ProtXLNet=409M) and training data (SeqVec=30M vs. ProtAlbert=224M), pure uni-directionality as used in TransformerXL seems to be detrimental for modeling protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Bigger data not always better?</head><p>LMs were trained on the largest protein database ever used for this purpose, namely BFD <ref type="bibr" target="#b35">[36]</ref>, more than an order of magnitude larger than UniProt <ref type="bibr" target="#b37">[38]</ref>, the standard in the field. Although bigger did not equate better for all 2 nd stage predictions, some models clearly improved through more data. Nevertheless, given the immense increase, the highest performance increase remained rather limited with respect to existing LMs <ref type="bibr" target="#b16">[17]</ref> (Q3=Q3(ProtBert-BFD)-Q3(SeqVec)=4.7%) despite a significant increase in model size (SeqVec=93M vs. ProtBert=420M) and data size (SeqVec=30M vs. Prot-Bert=216M). Although a Q3 of 4-5 percentage points might imply an improvement that is crucial for the methods using such predictions <ref type="bibr" target="#b67">[68]</ref>, the value has also to be put into relation to the GPU/TPU hours needed to train those models: while SeqVec needed around 1680 GPU hours, ProtTXL needed 202176 GPU hours and ProtBert-BFD needed 116736 TPU core hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Protein LMs reached a ceiling?</head><p>Applying techniques from NLP to proteins opens new opportunities to extract information from proteins in a selfsupervised, data-driven way. New protein representations may complement existing solutions, most successful when combining evolutionary information and machine learning <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>. The gain in inference speed for protein LMs compared to traditional models using evolutionary information is so significant that some analyses might prefer much faster and slightly less accurate to better but much slower, for instance, when time or resources for much slower are amiss. Nevertheless, given the experiments described here and in previous work <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we might expect an upper limit for what protein LMs can learn when using auto-regressive or auto-encoding exclusively. Although this work explicitly addressed the possibility of reaching that limit, we could only conclude: bi-directional models appeared superior over uni-directional models. Answers to the following questions might advance from the status-quo. (1) Why do LSTM-based approaches require fewer parameters and resources while performing similarly at downstream prediction tasks (Q3(ProtBert-BFD)-Q3(SeqVec)=4.7%) compared to Transformer-based approaches? (2) Would the addition of auxiliary tasks such as next-sentence or sentence-order prediction offered by BERT or Albert suit protein sequences? A suggestion might be the usage of structure information <ref type="bibr" target="#b71">[72]</ref> or evolutionary relationship <ref type="bibr" target="#b19">[20]</ref>. (3) Addressing model vs. data parallelism: Were the large models introduced here still too small to capture all data? Unfortunately, this brings up training efficiency as recently investigated by sparse Transformers <ref type="bibr" target="#b72">[73]</ref> or attention optimized with locality-sensitive hashing (LSH) <ref type="bibr" target="#b73">[74]</ref> as introduced recently by the Reformer model <ref type="bibr" target="#b74">[75]</ref>. and solve all the related Google TPU and servers issues. Last, not least, thanks to all those who deposit their experimental data in public databases, and to those who maintain these databases. Fig. <ref type="figure" target="#fig_0">10</ref>: Performance comparison of Language models on supervised tasks: similar to the analysis performed for three-state secondary structure (Fig. <ref type="figure">7</ref>), the features learnt by the proposed Language models (LMs) trained here (ProtBert, ProtAlbert, ProtTXL, ProtXLNet) were also evaluated on eight-state secondary structure prediction (y-axis: Q8). The same datasets (NetSurfP-2.0 <ref type="bibr" target="#b24">[25]</ref>), pre-processing steps as well as the same supervised models were used for this analysis, confirming the trend suggested by the three-state secondary structure prediction.   The effect of protein sequence length on the inference time of the protein LMs trained here and a previously published LM (SeqVec) were compared using a Nvidia Titan V with 12GB memory (batch-size=1). Longer proteins take disproportionate long to embed for all language models. In particular, SeqVec was affected due to the sequential nature of the LSTMs used this LM. Transformer-based models require longer for proteins because the maps that need to be computed square with sequence length. In contrast LSTMs, the computation of attention can be parallelized, resulting in lower inference time for long proteins when using transformer-based LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Large Scale Dataset Training: here we compare the two datasets that were used in this study for language modelling (UniRef100, BFD) with a frequently used, redundancy reduced dataset (UniRef50). a) shows the number of sequences in each dataset in millions. (b) shows the number of residues/tokens in each dataset in billions. (c) shows size of each dataset raw text files as well as after converting to tensors in terabytes. (d) shows the frequency of each aminoacid/token in the each dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>AFig. 2 :Fig. 3 :Fig. 4 :</head><label>234</label><figDesc>Fig. 2: Summit Architecture: Panel A shows a single node of the Summit super computer consisting of two power9 CPUs and 6 V100 GPUs while Panel B shows how they they are connected including their connection speed.</figDesc><graphic url="image-4.png" coords="6,316.08,241.00,181.44,80.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Large Scale Deep Learning Training: The figures show the effect of enabling (red bars) or disabling (blue bars) large model support (LMS) on both, model size as well as batch size, when we tested ProtTXL or ProtBert on Nvidia V-100 16GB GPUs. It highlights the difference between applying LMS using PyTorch (ProtTXL) or tensorflow (ProtBert). Panel (a) shows the effect of using LMS on the maximum model size that can fit in the memory of a single V-100. Panels (b,c) compare the effect of LMS on the maximum local (b) and global batch size (c) that can fit in the GPU. The number of hours required to finish a single epoch using 936 nodes, each with 6 GPUs when LMS being enabled is shown in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>N</head><label></label><figDesc>e t S u r f P -2 . 0 P r o t T X L -B F D P r o t T X L P r o t X L N e t P r o t B e r t P r o t B e r t -B F D P r o t A l b e r t D e e p S e q V e c D e e p P r o t V e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>D</head><label></label><figDesc>e e p L o c i L o c -E u k P r o t T X L -B F D P r o t T X L P r o t B e r t P r o t B e r t -B F D P r o t A l b e r t P r o t X L N e t D e e p S e q V e c D e e p P r o t V e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 4 )</head><label>4</label><figDesc>Might full precision training stabilize training and speed up convergence by leveraging 32-bit floats? Mixed precision training, employed in this evaluation, uses 16 Bit as well as 32 Bit vectors; this made it more difficult for the model to converge during training. Training the models presented here in full precision might stabilize training and thus provide more informative representations. Overall, our results established that the combination of HPC solutions for building protein LMs and subsequent training of supervised prediction methods scaled up to the largest data sets ever used in the field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>N e t S u r f P -2 . 0 P r o t T X L -B F D P r o t T X L P r o t X L N e t P r o t B e r t P r o t B e r t -B F D P r o t A l b e r t D e e p S e q V e c D e e p P r o t V e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig.11: Inference speed depends on sequence length: The effect of protein sequence length on the inference time of the protein LMs trained here and a previously published LM (SeqVec) were compared using a Nvidia Titan V with 12GB memory (batch-size=1). Longer proteins take disproportionate long to embed for all language models. In particular, SeqVec was affected due to the sequential nature of the LSTMs used this LM. Transformer-based models require longer for proteins because the maps that need to be computed square with sequence length. In contrast LSTMs, the computation of attention can be parallelized, resulting in lower inference time for long proteins when using transformer-based LMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Large</figDesc><table /><note>Scale Deep Learning Training: the table shows the configurations used for training the protein language models introduced here (ProtTXL, ProtBert, ProtXLNet, ProtAlbert) using either Summit or a TPU Pod v3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison of inference speed:The analysis distinguished proteins of different length, as well as different batch sizes (numbers of proteins processed: 1, 16 and 32; cap at 32 due to limitation of GPU memory to 12GB vRAM). For simplicity, no proteins longer than 512 is shown . Each test was repeated 100 times and the average time per protein was reported. The experiment was conducted using a single Nvidia Titan V GPU.</figDesc><table><row><cell></cell><cell>10</cell><cell>ProtBert</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ProtTXL</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time in ms per protein</cell><cell>4 6 8</cell><cell>ProtXLNet ProtAlbert SeqVec</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1 2 8</cell><cell>2 5 6</cell><cell>5 1 2</cell><cell>1 0 2 4</cell><cell>2 0 4 8</cell><cell>4 0 9 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sequence Length</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank primarily Tim Karl (TUM) and Jian Kong (TUM) for invaluable help with hardware and software; Inga Weise and Aline Schmidt (both TUM) for support with many other aspects of this work; Florian Matthes (TUM) for his invaluable support and encourage for us. Thanks for invaluable support and feedback from NVIDIA, in particular to to Ulrich Michaelis, Ada Sedova, Geetika Gupta, Axel Koehler, Frederic Pariente, Jonathan Lefman, and Thomas Bradley. No aspect of this work could have been realized without strong support from many at ORNL: thanks; these include John Gounley,Hong-Jun Yoon, Georgia Tourassi, Bill, Brian, Junqi, Graham and Vernica for helping us on fixing issues that occurred while training on Summit. Furthermore, special thanks to Jack Wells for giving us the opportunity to access and work with Summit. From IBM, we would like to thank Nicolas Castet and Bryant Nelson for their help to fix issues and enhance the performance of IBM PowerAI. From Google, we would like to deeply thank Jamie Kinney, Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mishra, Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte and all TFRC Team for their invaluable support to setup our project on Google Cloud This work was supported by a grant from Software Campus through the German Ministry for Research and Education (BMBF: Bundesministerium fuer Bildung und Forschung), a grant from the Alexander von Humboldt foundation through the German Ministry for Research and Education (BMBF: Bundesministerium fuer Bildung und Forschung),and by a grant from the Deutsche Forschungsgemeinschaft (DFG-GZ: RO1320/4-1). We gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan GPU used for this research development phase. We also want to thank LRZ (Leibniz Rechenzentrum) for providing us access to DGX-1(V100) for the testing phase.</p><p>Finally and most importantly, this research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725, and resources of TPU pods under TensorFlow Research Cloud grant. Furthermore, Rostlab acknowledge support from Google Cloud and Google Cloud Research Credits program to fund this project under Covid19 HPC Consortium grant.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Online Material (SOM) -1.1 Supervised Learning</head><p>On the level of single residues we also compared the prediction performance of the LMs introduced here on 8-state secondary structure prediction performance (Fig. <ref type="figure">10</ref>). On the level of single residues, we also compare our protein LMs on results for secondary structure prediction in 8-states as shown in Fig. <ref type="figure">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-1.2 Protein LM inference speed</head><p>The effect of varying sequence lengths (128, 256, 512) and different batch sizes <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref> on the inference time of the protein LMs introduced here is reported in table 2. The effect of sequence length on different LM architectures (LSTMbased SeqVec and transformer-based ProtTrans) was also visualized in figure <ref type="figure">11</ref>. The x-axis represents different sequence length from 128 up to 4096, while the y-axis represents the time of inference in ms for a single protein with a batch size of 1 on a Nvidia Titan V with 12GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-1.3 Unsupervised Learning</head><p>Using t-SNE projections, the information content stored within the novel embeddings was qualitatively assessed on various levels, ranging from different aspects of protein function (E.C. numbers, subcellular localization and membraneboundness) to the level of kingdoms of life, i.e. Eukaryota, Bacteria and Archaea (for completeness here also including Viruses).         Towards this end, contextualized, fixed-size representations were generated for all proteins in both datasets by mean-pooling over the representations extracted from the last layer of ProtBert (average over the length of the protein). The high-dimensional embeddings were projected to 2D using t-SNE. ProtBert formed less dense clusters compared to the same model trained on a larger dataset (ProtBert-BFD Fig. <ref type="figure">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All alpha All beta Alpha &amp; beta (a|b) Alpha &amp; beta (a+b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Membrane, cell surface Small proteins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oxidoreductases Transferases Hydrolases Lyases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Isomerases</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Announcing Supercomputer Summit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<pubPlace>Oak Ridge, TN (United States</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>Oak Ridge National Lab</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In-Datacenter Performance Analysis of a Tensor Processing Unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ser. ISCA &apos;17</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture, ser. ISCA &apos;17<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017-06">Jun. 2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
				<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><surname>Larochelle</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NVIDIA cuda software and gpu parallel computing architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Symposium on Memory Management, ser. ISMM &apos;07</title>
				<meeting>the 6th International Symposium on Memory Management, ser. ISMM &apos;07<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="103" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Horovod: Fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Del Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Finkler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02188</idno>
	</analytic>
	<monogr>
		<title level="j">PowerAI DDL</title>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Press release announcing Supercomputer Fugaku</title>
		<author>
			<persName><forename type="first">F</forename><surname>Limited</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RIKEN, Tech. Rep</title>
		<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extreme Scale-out Super-MUC Phase 2 -lessons learned</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jamitzky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01507</idno>
		<imprint>
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
	<note>astro-ph, physics:physics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Universal Language Model Finetuning for Text Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bridging the protein sequence-structure gap by structure predictions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biophysics and Biomolecular Structure</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="113" to="136" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative Models for Graph-Based Protein Design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><surname>Larochelle</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">831</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling aspects of the language of life through transfer-learning protein sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">723</biblScope>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ProGen: Language Modeling for Protein Generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<idno>03.07.982272</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pre-Training of Deep Bidirectional Protein Sequence Representations with Structural Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05625</idno>
		<imprint>
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
	<note>cs, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating Protein Transfer Learning with TAPE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><surname>Larochelle</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9689" to="9701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language modelling for biological sequences -curated datasets and baselines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J A</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Johansen</surname></persName>
		</author>
		<idno>03.09.983585</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-End Differentiable Learning of Protein Structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="292" to="301" />
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">622803</biblScope>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jespersen</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25674</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25674" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepLoc: Prediction of protein subcellular localization using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Almagro Armenteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Snderby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3387" to="3395" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted interresidue orientations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1496" to="1503" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pred-MutHTP: Prediction of disease-causing and neutral mutations in human transmembrane proteins</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kulandaisamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zaucha</surname></persName>
		</author>
		<idno type="DOI">10.1002/humu.23961</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/humu.23961" />
	</analytic>
	<monogr>
		<title level="j">Human Mutation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="590" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evolutionary couplings and sequence variation effect predict protein binding sites</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25585</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25585" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">_eprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TMSEG: Novel prediction of transmembrane helices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bernhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kloppmann</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25155</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.25155" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1706" to="1716" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Protein 3D Structure Computed from Evolutionary Sequence Variation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e28766</biblScope>
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Protein flexibility and intrinsic disorder</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
		<idno type="DOI">10.1110/ps.03128904</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1110/ps.03128904" />
	</analytic>
	<monogr>
		<title level="j">Protein Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unexpected features of the dark proteome</title>
		<author>
			<persName><forename type="first">N</forename><surname>Perdigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="15" to="898" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining evolutionary information and neural networks to predict protein secondary structure</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Genetics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="55" to="72" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UniRef clusters: A comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Clustering huge protein sequence sets in linear time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="603" to="606" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">UniProt: A worldwide hub of protein knowledge</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D506" to="D515" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PISCES: A protein sequence culling server</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dunbrack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1589" to="1591" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Protein Data Bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sixty-five years of the long march in protein secondary structure prediction: The final stretch?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="482" to="494" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="519" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Assessment of hard target modeling in CASP12 reveals an emerging role of alignmentbased contact prediction methods</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Abriata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SCOPe: Classification of large macromolecular structures in the structural classification of proteins-extended database</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Chandonia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D475" to="D481" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Enzyme Nomenclature 1992. Recommendations of the Nomenclature committee of the International Union of Biochemistry and Molecular Biology</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992nd. 1992</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Characteristics of Sentence Length in Running Text</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schils</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="26" />
			<date type="published" when="1993-01">Jan. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transforming the language of life: Transformer neural networks for protein prediction tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13243</idno>
		<title level="m">A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation</title>
				<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tpu</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/docs/system-architecture" />
		<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<ptr target="https://github.com/NVIDIA/apex" />
	</analytic>
	<monogr>
		<title level="j">Nvidia Apex</title>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">TFLMS: Large Model Support in TensorFlow by Graph Rewriting</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Imai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02037</idno>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00079</idno>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The ENZYME database in 2000</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="305" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Anaconda software distribution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Analytics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
	<note>Computer software. Vers</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<imprint>
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">FastText.zip: Compressing text classification models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">ZeRO: Memory Optimization Towards Training A Trillion Parameter Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02054</idno>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improving fold recognition without folds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Przybylski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">341</biblScope>
			<biblScope unit="page" from="255" to="269" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Prediction of protein secondary structure at better than 70% accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page" from="584" to="599" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Improved prediction of protein secondary structure by use of sequence profiles and neural networks</title>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="7558" to="7562" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">PHD: predicting one-dimensional protein structure by profile based neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Enzymology</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page" from="525" to="539" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
	<note>cs, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Generating Long Sequences with Sparse Transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: Towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing</title>
				<meeting>the Thirtieth Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
