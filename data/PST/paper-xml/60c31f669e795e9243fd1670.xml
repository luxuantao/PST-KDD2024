<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
							<email>liuqiang@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siming</forename><surname>He</surname></persName>
							<email>siminghe@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
							<email>jiangjianguo@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ♣ Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Alibaba Group ♦ University of Pennsylvania ¶ Microsoft {lqs19</orgName>
								<address>
									<addrLine>dm18,chenyuxi18</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467350</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Heterogeneous Graphs</term>
					<term>Graph Representation Learning</term>
					<term>Graph Benchmark</term>
					<term>Heterogeneous Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB) 1 , consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN-which significantly outperforms all previous models on HGB-to accelerate the advancement of HGNNs in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Neural networks; • Mathematics of computing → Graph algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As graph neural networks (GNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> have already occupied the centre stage of graph mining research within recent years, the researchers begin to pay attention to their potential on heterogeneous graphs (a.k.a., Heterogeneous Information Networks) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. Heterogeneous graphs consist of multiple types of nodes and edges with different side information, connecting the novel and effective graph-learning algorithms to the noisy and complex industrial scenarios, e.g., recommendation.</p><p>To tackle the challenge of heterogeneity, various heterogeneous GNNs (HGNNs) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> have been proposed to address the relevant tasks, including node classification, link prediction, and knowledge-aware recommendation. Take node classification for example, numerous HGNNs, such as HAN <ref type="bibr" target="#b35">[36]</ref>, GTN <ref type="bibr" target="#b42">[43]</ref>, RSHN <ref type="bibr" target="#b44">[45]</ref>, HetGNN <ref type="bibr" target="#b43">[44]</ref>, MAGNN <ref type="bibr" target="#b11">[12]</ref>, HGT <ref type="bibr" target="#b19">[20]</ref>, and HetSANN <ref type="bibr" target="#b16">[17]</ref> were developed within the last two years.</p><p>Despite various new models developed, our understanding of how they actually make progress has been thus far limited by the unique data processing and settings adopted by each of them. To fully picture the advancements in this field, we comprehensively reproduce the experiments of 12 most popular HGNN models by using the codes, datasets, experimental settings, hyperparameters released by their original papers. Surprisingly, we find that the results generated by these state-of-the-art HGNNs are not as exciting as promised (Cf. Table <ref type="table">1</ref>), that is:</p><p>(1) The performance of simple homogeneous GNNs, i.e., GCN <ref type="bibr" target="#b20">[21]</ref> and GAT <ref type="bibr" target="#b31">[32]</ref>, is largely underestimated. Even vanilla GAT can outperform existing HGNNs in most cases with proper inputs. (2) Performances of some previous works are mistakenly reported due to inappropriate settings or data leakage.</p><p>Our further investigation also suggests:</p><p>(3) Meta-paths are not necessary in most heterogeneous datasets.</p><p>(4) There is still considerable room for improvements in HGNNs.</p><p>In our opinion, the above situation occurs largely because the individual data and experimental setup by each work obstructs a fair and consistent validation of different techniques, thus greatly hindering the advancements of HGNNs.</p><p>To facilitate robust and open HGNN developments, we build the Heterogeneous Graph Benchmark (HGB). HGB currently contains 11 heterogeneous graph datasets that vary in heterogeneity (the number of node and edge types), tasks (node classification, link prediction, and knowledge-aware recommendation), and domain (e.g., academic graphs, user-item graphs, and knowledge graphs). HGB provides a unified interface for data loading, feature processing, and evaluation, offering a convenient and consistent way to compare HGNN models. Similar to OGB <ref type="bibr" target="#b17">[18]</ref>, HGB also hosts a leaderboard (https://www.biendata.xyz/hgb) for publicizing reproducible state-of-the-art HGNNs.</p><p>Finally, inspired by GAT's significance in Table <ref type="table">1</ref>, we take GAT as backbone to design an extremely simple HGNN model-Simple-HGN. Simple-HGN can be viewed as GAT enhanced by three existing techniques: (1) learnable type embedding to leverage type information, (2) residual connections to enhance modeling power, and (3) 𝐿 2 normalization on the output embedddings. In ablation studies, these techniques steadily improve the performance. Experimental results on HGB suggest that Simple-HGN can consistently outperform previous HGNNs on three tasks across 11 datasets, making it to date the first HGNN model that is significantly better than the vanilla GAT.</p><p>To sum up, this work makes the following contributions: • We revisit HGNNs and identify issues blocking progress; • We benchmark HGNNs by HGB for robust developments;</p><p>• We refine HGNNs by designing the Simple-HGN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Heterogeneous Graph</head><p>A heterogeneous graph <ref type="bibr" target="#b28">[29]</ref> can be defined as 𝐺 = {𝑉 , 𝐸, 𝜙,𝜓 }, where 𝑉 is the set of nodes and 𝐸 is the set of edges. Each node 𝑣 has a type 𝜙 (𝑣), and each edge 𝑒 has a type 𝜓 (𝑒). The sets of possible node types and edge types are denoted by 𝑇 𝑣 = {𝜙 (𝑣) : ∀𝑣 ∈ 𝑉 } and 𝑇 𝑒 = {𝜓 (𝑒) : ∀𝑒 ∈ 𝐸}, respectively. When |𝑇 𝑣 | = |𝑇 𝑒 | = 1, the graph degenerates into an ordinary homogeneous graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>GNNs aim to learn a representation vector 𝒉 (𝐿) 𝑣 ∈ R 𝑑 𝐿 for each node 𝑣 after L-layer transformations, based on the graph structure and the initial node feature 𝒉 (0) 𝑣 ∈ R 𝑑 0 . The final representation can serve various downstream tasks, e.g., node classification, graph classification (after pooling), and link prediction.</p><p>Graph Convolution Network (GCN) <ref type="bibr" target="#b20">[21]</ref> is the pioneer of GNN models, where the 𝑙 𝑡ℎ layer is defined as</p><formula xml:id="formula_0">𝑯 (𝑙) = 𝜎 ( Â𝑯 (𝑙−1) 𝑾 (𝑙) ),<label>(1)</label></formula><p>where 𝑯 (𝑙) is the representation of all nodes after the 𝑙 𝑡ℎ layer. 𝑾 (𝑙) is a trainable weight matrix. 𝜎 is the activation function, and Â is the normalized adjacency matrix with self-connections.</p><p>Graph Attention Network (GAT) <ref type="bibr" target="#b31">[32]</ref> later replaces the average aggregation from neighbors, i.e., Â𝑯 (𝑙−1) , as a weighted one, where the weight 𝛼 𝑖 𝑗 for each edge ⟨𝑖, 𝑗⟩ is from an attention mechanism as (layer mark (𝑙) is omitted for simplicity)</p><formula xml:id="formula_1">𝛼 𝑖 𝑗 = exp LeakyReLU 𝒂 𝑇 [𝑾𝒉 𝑖 ∥𝑾𝒉 𝑗 ] 𝑘 ∈N 𝑖 exp LeakyReLU 𝒂 𝑇 [𝑾𝒉 𝑖 ∥𝑾𝒉 𝑘 ] ,<label>(2)</label></formula><p>where 𝒂 and 𝑾 are learnable weights and N 𝑖 represents the neighbors of node 𝑖. Multi-head attention technique <ref type="bibr" target="#b30">[31]</ref> is also used to improve the performance.</p><p>Many following works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37]</ref> improve GCN and GAT furthermore, with focuses on homogeneous graphs. Actually, these homogeneous GNNs can also handle heterogeneous graphs by simply ignoring the node and edge types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Meta-Paths in Heterogeneous Graphs</head><p>Meta-paths <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> have been widely used for mining and learning with heterogeneous graphs. A meta-path is a path with a predefined (node or edge) types pattern, i.e., P ≜ 𝑛 1</p><formula xml:id="formula_2">𝑟 1 − − → 𝑛 2 𝑟 2 − − → • • • 𝑟 𝑙 − → 𝑛 𝑙+1</formula><p>, where 𝑟 𝑖 ∈ 𝑇 𝑒 and 𝑛 𝑖 ∈ 𝑇 𝑣 . Researchers believe that these composite patterns imply different and useful semantics. For instance, "author↔paper↔author" meta-path defines the "co-author" relationship, and "user Given a meta-path P, we can re-connect the nodes in 𝐺 to get a meta-path neighbor graph 𝐺 P . Edge 𝑢 → 𝑣 exists in 𝐺 P if and only if there is at least one path between 𝑢 and 𝑣 following the meta-path P in the original graph 𝐺.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ISSUES WITH EXISTING HETEROGENEOUS GNNS</head><p>We analyze popular heterogeneous GNNs (HGNNs) organized by the tasks that they aim to address. For each HGNN, the analysis will be emphasized on its defects found in the process of reproducing its result by using its official code, the same datasets, settings, and hyperparameters as its original paper, which is summarized in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node Classification</head><p>3.1.1 HAN <ref type="bibr" target="#b35">[36]</ref>. Heterogeneous graph attention network (HAN) is among the early attempts to tackle with heterogeneous graphs. Firstly, HAN needs multiple meta-paths selected by human experts.</p><p>Then HAN uses a hierarchical attention mechanism to capture both node-level and semantic-level importance. For each meta-path, the node-level attention is achieved by a GAT on its corresponding meta-path neighbor graph. And the semantic-level attention, which gives the final representation, refers to a weighted average of the node-level results from all meta-path neighbor graphs.</p><p>A defect of HAN is its unfair comparison between HAN and GAT. Since HAN can be seen as a weighted ensemble of GATs on many meta-path neighbor graphs, a comparison with the vanilla GAT is essential to prove its effectiveness. However, the GCN and GAT baselines in this paper take only one meta-path neighbor graph as input, losing a large part of information in the original graph, even though they report the result of the best meta-path neighbor graph.</p><p>To make a fair comparison, we feed the original graph into GAT by ignoring the types and only keeping the features of the targettype nodes. We find that this simple homogeneous approach consistently outperforms HAN, suggesting that the homogeneous GNNs are largely underestimated (See Table <ref type="table">1</ref> for details).</p><p>Most of the following works also follow HAN's setting to compare with homogeneous GNNs, suffering from the "information missing in homogeneous baselines" problem, which leads to a positive cognitive deviation on the performance progress of HGNNs.</p><p>Table <ref type="table">1</ref>: Reproduction of Heterogeneous GNNs with simple GCN and GAT as baselines-all reproduction experiments use official codes and the same dataset, settings, hyperparameters as the original paper. The line with star (*) are results reported in the paper, and the lines without star are our reproduction. "-" means the results are not reported in the original paper. We mark the reproduction terms with &gt;1 point gap compared to the reported results by ↑ and ↓. We also keep the standard variance terms above 1.</p><p>HAN <ref type="bibr" target="#b35">[36]</ref> GTN <ref type="bibr" target="#b42">[43]</ref> RSHN <ref type="bibr" target="#b44">[45]</ref> HetGNN <ref type="bibr" target="#b43">[44]</ref> MAGNN <ref type="bibr" target="#b11">[12]</ref> Dataset The main drawback of GTN is that it consumes gigantic amount of time and memory. For example, it needs 120 GB memory and 12 hours to train a GTN on DBLP with only 18,000 nodes. In contrast, GCN and GAT only take 1 GB memory and 10 seconds of time.</p><p>Moreover, when we test the GTN and GAT five times using the official codes of GTN, we find from Table <ref type="table">1</ref> that their average scores are not significantly different, though GTN consumes &gt; 400× time and 120× memory of GAT. <ref type="bibr" target="#b44">[45]</ref>. Relation structure-aware heterogeneous graph neural network (RSHN) builds coarsened line graph to obtain edge features first, then uses a novel Message Passing Neural Network (MPNN) <ref type="bibr" target="#b12">[13]</ref> to propagate node and edge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">RSHN</head><p>The experiments in RSHN have serious problems according to the official code. First, it does not use validation set, and just tune hyperparameters on test set. Second, it reports the accuracy at the epoch with best accuracy on test set in the paper. As shown in Table 1, our well-tuned GAT can even reach 100% accuracy under this improper setting on the AIFB and BGS datasets, which is far better than the 91.67% and 66.32% reported in their paper. <ref type="bibr" target="#b43">[44]</ref>. Heterogeneous graph neural network (Het-GNN) first uses random walks with restart to generate neighbors for nodes, and then leverages Bi-LSTM to aggregate node features for each type and among types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">HetGNN</head><p>HetGNN has the same "information missing in homogeneous baselines" problem as HAN: when comparing it with GAT, a sampled graph instead of the original full graph is fed to GAT. As demonstrated in Table <ref type="table">1</ref>, GAT with correct inputs gets clearly better performance. <ref type="bibr" target="#b11">[12]</ref>. Meta-path aggregated graph neural network (MAGNN) is an enhanced HAN. The motivation is that when HAN deals with meta-path neighbor graphs, it only considers two endpoints of the meta-paths but ignores the intermediate nodes. MAGNN proposes several meta-path encoders to encode all the information along the path, instead of only the endpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">MAGNN</head><p>However, there are two problems in the experiments of MAGNN. First, MAGNN inherits the "information missing in homogeneous baselines" problem from HAN, and also underperforms GAT with correct inputs.</p><p>More seriously, MAGNN has a data leakage problem in link prediction, because it uses batch normalization, and loads positive and negative links sequentially during both training and testing periods. In this way, samples in a minibatch are either all positive or all negative, and the mean and variance in batch normalization will provide extra information. If we shuffle the test set to make each minibatch contains both positive and negative samples randomly, the AUC of MAGNN drops dramatically from 98.91 to 71.49 on the Last.fm dataset. <ref type="bibr" target="#b19">[20]</ref>. Heterogeneous graph transformer (HGT) proposes a transformer-based model for handling large academic heterogeneous graphs with heterogeneous subgraph sampling. As HGT mainly focuses on handling web-scale graphs via graph sampling strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>, the datasets used in its paper (&gt; 10,000,000 nodes) are unaffordable for most HGNNs, unless adapting them by subgraph sampling. To eliminate the impact of subgraph sampling techniques on the performance, we apply HGT with its official code on the relatively small datasets that are not used in its paper, producing mixed results when compared to GAT (See Table <ref type="table" target="#tab_3">3</ref>). <ref type="bibr" target="#b16">[17]</ref>. Attention-based graph neural network for heterogeneous structural learning (HetSANN) uses a type-specific graph attention layer for the aggregation of local information, avoiding manually selecting meta-paths. HetSANN is reported to have promising performance in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">HGT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">HetSANN</head><p>However, the datasets and preprocessing details are not released with the official codes, and responses from its authors are not received as of the submission of this work. Therefore, we directly apply HetSANN with standard hyperparameter-tuning, giving unpromising results on other datasets (See Table <ref type="table" target="#tab_3">3</ref>). <ref type="bibr" target="#b27">[28]</ref>. Relational graph convolutional network (RGCN) extends GCN to relational (multiple edge types) graphs. The convolution in RGCN can be interpreted as a weighted sum of ordinary graph convolution with different edge types. For each node 𝑖, the 𝑙 𝑡ℎ layer of convolution are defined as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Link Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">RGCN</head><formula xml:id="formula_3">𝒉 (𝑙) 𝑖 = 𝜎 𝑟 ∈𝑇 𝑒 𝑗 ∈N 𝑟 𝑖 1 𝑐 𝑖,𝑟 𝑾 (𝑙) 𝑟 𝒉 (𝑙) 𝑗 + 𝑾 (𝑙) 0 𝒉 (𝑙−1) 𝑖 ,<label>(3)</label></formula><p>where 𝑐 𝑖,𝑟 is a normalization constant and 𝑊 0 ,𝑊 𝑟 s are learnable parameters. <ref type="bibr" target="#b4">[5]</ref>. General attributed multiplex heterogeneous network embedding (GATNE) leverages the graph convolution operation to aggregate the embeddings from neighbors. It relies on Skip-gram to learn a general embedding, a specific embedding and an attribute embedding respectively, and finally fuses all of them. In fact, GATNE is more a network embedding algorithm than a GNN-style model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">GATNE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge-Aware Recommendation</head><p>Recommendation is a main application for Heterogeneous GNNs, but most related works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> only focus on their specific industrial data, resulting in non-open datasets and limited transferability of the models. Knowledge-aware recommendation is an emerging sub-field, aiming to improve recommendation by linking items with entities in an open knowledge graph. In this paper, we mainly survey and benchmark models on this topic. <ref type="bibr" target="#b33">[34]</ref> and KGNN-LS <ref type="bibr" target="#b32">[33]</ref>. KGCN enhances the item representation by performing aggregations among its corresponding entity neighborhood in a knowledge graph. KGNN-LS further poses a label smoothness assumption, which posits that similar items in the knowledge graph are likely to have similar user preference. It adds a regularization term to help learn such a personalized weighted knowledge graph. <ref type="bibr" target="#b34">[35]</ref>. KGAT shares a generally similar idea with KGCN. The main difference lies in an auxiliary loss for knowledge graph reconstruction and the pretrained BPR-MF <ref type="bibr" target="#b26">[27]</ref> features as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">KGCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">KGAT</head><p>Although not detailed in its paper, an important contribution of KGAT is to introduce the pretrained features into this tasks, which greatly improves the performance. Based on this finding, we successfully simplify KGAT and obtain similar or even better performance (See Table <ref type="table" target="#tab_5">5</ref>, denoted as KGAT−).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary</head><p>In summary, the prime common issue of existing HGNNs is the lack of fair comparison with homogeneous GNNs and other works-to some extent-encourage the new models to equip themselves with novel yet redundant modules, instead of focusing more on progress in performance. Additionally, a non-negligible proportion of works have individual issues, e.g., data leakage <ref type="bibr" target="#b11">[12]</ref>, tuning on test set <ref type="bibr" target="#b44">[45]</ref>, and two-order-of-magnitude more memory and time consumption without effectiveness improvements <ref type="bibr" target="#b42">[43]</ref>.</p><p>In light of the significant discrepancy, we take the initiative to setup a heterogeneous graph benchmark (HGB) with these three tasks on diverse datasets for open, reproducible heterogeneous graph research (See §4). Inspired by the promising advantages of the simple GAT over dedicated and relatively-complex heterogeneous GNN models, we present a simple heterogeneous GNN model with GAT as backbone, offering promising results on HGB (See §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HETEROGENEOUS GRAPH BENCHMARK 4.1 Motivation and Overview</head><p>Issues with current datasets. Several types of datasets-academic networks (e.g., ACM, DBLP), information networks (e.g., IMDB, Reddit), and recommendation graphs (e.g., Amazon, MovieLens)-are the most frequently-used datasets, but the detailed task settings could be quite different in different papers. For instance, HAN <ref type="bibr" target="#b35">[36]</ref> and GTN <ref type="bibr" target="#b42">[43]</ref> discard the citation links in ACM, while others use the original version. Besides, different splits of the dataset also contribute to uncomparable results. Finally, the recent graph benchmark OGB <ref type="bibr" target="#b17">[18]</ref> mostly focuses on benchmarking graph machine learning methods on homogeneous graphs and is not dedicated to heterogeneous graphs.</p><p>Issues with current pipelines. To fulfill a task, components outsides HGNNs can also play critical roles. For example, MAGNN <ref type="bibr" target="#b11">[12]</ref> finds that not all types of node features are useful, and a preselection based on validation set could be helpful (See § 4.3). RGCN <ref type="bibr" target="#b27">[28]</ref> uses DistMult <ref type="bibr" target="#b38">[39]</ref> instead of dot product for training in link prediction. We need to control the other components in the pipeline for fair comparison.</p><p>HGB. In view of these practical issues, we present the heterogeneous graph benchmark (HGB) for open, reproducible heterogeneous GNN research. We standardize the process of data splits, feature processing, and performance evaluation, by establishing the HGB pipeline "feature preprocessing → HGNN encoder → downstream decoder". For each model, HGB selects the best fit feature preprocessing and downstream decoder based on its performance on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Construction</head><p>HGB collects 11 widely-recognized medium-scale datasets with predefined meta-paths from previous works, making it available to all kinds of HGNNs. The statistics are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Node Classification.</head><p>Node Classification follows a transductive setting, where all edges are available during training and node labels are split according to 24% for training, 6% for validation and 70% for test in each dataset.</p><p>• DBLP<ref type="foot" target="#foot_1">2</ref> is a bibliography website of computer science. We use a commonly used subset in 4 areas with nodes representing authors, papers, terms and venues. • IMDB<ref type="foot" target="#foot_2">3</ref> is a website about movies and related information. A subset from Action, Comedy, Drama, Romance and Thriller classes is used. • ACM is also a citation network. We use the subset hosted in HAN <ref type="bibr" target="#b35">[36]</ref>, but preserve all edges including paper citations and references.</p><p>• Freebase <ref type="bibr" target="#b2">[3]</ref> is a huge knowledge graph. We sample a subgraph of 8 genres of entities with about 1,000,000 edges following the procedure of a previous survey <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Link</head><p>Prediction. Link prediction is formulated as a binary classification problem in HGB. The edges are split according to 81% for training, 9% for validation and 10% for test. Then the graph is reconstructed only by edges in the training set. For negative node pairs in testing, we firstly tried uniform sampling and found that most models could easily make a nearly perfect prediction (See Appendix B). Finally, we sample 2-hop neighbors for negative node pairs, of which are 1:1 ratio to the positive pairs in the test set.</p><p>• Amazon is an online purchasing platform. We use the subset preprocessed by GATNE <ref type="bibr" target="#b4">[5]</ref>, containing electronics category products with co-viewing and co-purchasing links between them. • LastFM is an online music website. We use the subset released by HetRec 2011 <ref type="bibr" target="#b3">[4]</ref>, and preprocess the dataset by filtering out the users and tags with only one link. • PubMed<ref type="foot" target="#foot_3">4</ref> is a biomedical literature library. We use the subset constructed by HNE <ref type="bibr" target="#b40">[41]</ref>.</p><p>4.2.3 Knowledge-aware recommendation. We randomly split 20% of user-item interactions as test set for each user, and for the left 80% interactions as training set.</p><p>• Amazon-book is a subset of Amazon-review<ref type="foot" target="#foot_4">5</ref> related to books.</p><p>• LastFM is a subset extracted from last.fm with timestamp from January, 2015 to June, 2015. • Yelp-2018<ref type="foot" target="#foot_5">6</ref> is a dataset adapted from 2018 edition of the Yelp challenge. Local businesses like restaurants and bars are seen as items.</p><p>• Movielens is a subset of Movielens-20M <ref type="foot" target="#foot_6">7</ref> , which is a widely used dataset for recommendation.</p><p>To assure the quality of dataset, we use 10-core setting to filter low-frequency nodes. To align items to knowledge graph entities, we adopt the same procedure as <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Preprocessing</head><p>As pointed out in § 4.1, the preprocessing for input features has a great impact on the performance. Our preprocessing methods are as follows.</p><p>Linear Transformation. As the input feature of different types of nodes may vary in dimension, we use a linear layer with bias for each node type to map all node features to a shared feature space. The parameters in these linear layers will be optimized along with the following HGNN.</p><p>Useful Types Selection. In many datasets, only features of a part of types are useful to the task. We can select a subset of node types to keep their features, and replace the features of nodes of other types as one-hot vectors. Combined with linear transformation, the replacement is equivalent to learn an individual embedding for each node of the unselected types. Ideally, we should enumerate all subsets of types and report the best one based on the performance </p><p>where 𝑅 𝑟 is a learnable square matrix (sometimes regularized with diagonal matrix) for type 𝑟 ∈ 𝑇 𝑒 . We find that DistMult outperforms dot product sometimes even when there is only single type of edge to predict. We try both dot product and DistMult decoders, and report the best results. The loss function is binary cross-entropy. </p><p>Following KGAT <ref type="bibr" target="#b34">[35]</ref>, we opt for BPR <ref type="bibr" target="#b26">[27]</ref> loss for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss(𝑢, 𝑣</head><formula xml:id="formula_6">+ , 𝑣 − ) = − log sigmoid 𝑓 (𝑢, 𝑣 + ) − 𝑓 (𝑢, 𝑣 − ) ,<label>(6)</label></formula><p>where (𝑢, 𝑣 + ) is a positive pair, and (𝑢, 𝑣 − ) is a negative pair sampled at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Settings</head><p>We evaluate all methods for all datasets by running 5 times with different random seeds, and reporting the average score and standard deviation.</p><p>4.5.1 Node Classification. We evaluate node classification with Macro-F1 and Micro-F1 metrics for both multi-class (DBLP, ACM, Freebase) and multi-label (IMDB) datasets. The implementation is based on sklearn<ref type="foot" target="#foot_7">8</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Link Prediction.</head><p>We evaluate link prediction with ROC-AUC (area under the ROC curve) and MRR (mean reciprocal rank) metrics.</p><p>Since we usually need to determine a threshold when to classify a pair as positive for the score given by decoder, ROC-AUC can evaluate the performance under difference threshold at a whole scope. MRR can evaluate the ranking performance for different methods. Following <ref type="bibr" target="#b40">[41]</ref>, we calculate MRR scores clustered by the head of pairs in test set, and return the average of them as MRR performance.</p><p>4.5.3 Knowledge-aware Recommendation. Recommendation task focuses more on ranking instead of classification. Therefore, we adopt recall@20 and ndcg@20 as our evaluation metrics. The average metrics for all users in test set are reported as benchmark performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A SIMPLE HETEROGENEOUS GNN</head><p>Inspired by the advantage of the simple GAT over advanced and dedicated heterogeneous GNNs, we present Simple-HGN, a simple and effective method for modeling heterogeneous graph. Simple-HGN adopts GAT as backbone with enhancements from the redesign of three well-known techniques: Learnable edge-type embedding, residual connections, and 𝐿 2 normalization on the output embeddings. Figure <ref type="figure" target="#fig_1">1</ref> illustrates the full pipeline with Simple-HGN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learnable Edge-type Embedding</head><p>Though GAT has powerful capacity in modeling homogeneous graphs, it may be not optimal for heterogeneous graphs due to the neglect of node or edge types. To tackle this problem, we extend the original graph attention mechanism by including edge type information into attention calculation. Specifically, at each layer, we allocate a 𝑑 𝑙 −dimensional embedding 𝒓 (𝑙) 𝜓 (𝑒) for each edge type 𝜓 (𝑒) ∈ 𝑇 𝑒 , and use both edge type embeddings and node embeddings to calculate the attention score as follows <ref type="foot" target="#foot_8">9</ref> :</p><formula xml:id="formula_7">α𝑖 𝑗 = exp LeakyReLU 𝒂 𝑇 [𝑾𝒉 𝑖 ∥𝑾𝒉 𝑗 ∥𝑾 𝑟 𝒓 𝜓 ( ⟨𝑖,𝑗 ⟩) ] 𝑘 ∈N 𝑖 exp LeakyReLU 𝒂 𝑇 [𝑾𝒉 𝑖 ∥𝑾𝒉 𝑘 ∥𝑾 𝑟 𝒓 𝜓 ( ⟨𝑖,𝑘 ⟩) ] ,<label>(7)</label></formula><p>where 𝜓 (⟨𝑖, 𝑗⟩) represents the type of edge between node 𝑖 and node</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑗, and 𝑾 (𝑙)</head><p>𝑟 is a learnable matrix to transform type embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Residual Connection</head><p>GNNs are hard to be deep due to the over-smoothing and gradient vanishing problems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>. A famous solution to mitigate this problem in computer vision is residual connection <ref type="bibr" target="#b14">[15]</ref>. However, the original GCN paper <ref type="bibr" target="#b20">[21]</ref> showed a negative result for residual connection on graph convolution. Recent study <ref type="bibr" target="#b21">[22]</ref> finds that well-designed pre-activation implementation could make residual connection great again in GNNs.</p><p>Node Residual. We add pre-activation residual connection for node representation across layers. The aggregation at the 𝑙 𝑡ℎ layer can be expressed as</p><formula xml:id="formula_8">𝒉 (𝑙) 𝑖 = 𝜎 𝑗 ∈N 𝑖 𝛼 (𝑙) 𝑖 𝑗 𝑾 (𝑙) 𝒉 (𝑙−1) 𝑗 + 𝒉 (𝑙−1) 𝑖 , (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where</p><formula xml:id="formula_10">𝛼 (𝑙)</formula><p>𝑖 𝑗 is the attention weight about edge ⟨𝑖, 𝑗⟩ and 𝜎 is an activation function (ELU <ref type="bibr" target="#b5">[6]</ref> by default). When the dimension changes in the 𝑙−th layer, an additional learnable linear transformation 𝑾</p><formula xml:id="formula_11">(𝑙) 𝑟𝑒𝑠 ∈ R 𝑑 𝑙 +1 ×𝑑 𝑙 is needed, i.e., 𝒉 (𝑙) 𝑖 = 𝜎 𝑗 ∈N 𝑖 𝛼 (𝑙) 𝑖 𝑗 𝑾 (𝑙) 𝒉 (𝑙−1) 𝑗 + 𝑾 (𝑙) res 𝒉 (𝑙−1) 𝑖 . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>Edge Residual. Recently, Realformer <ref type="bibr" target="#b15">[16]</ref> reveals that residual connection on attention scores is also helpful. After getting the raw attention scores α via Eq. ( <ref type="formula" target="#formula_7">7</ref>), we add residual connections to them,</p><formula xml:id="formula_13">𝛼 (𝑙) 𝑖 𝑗 = (1 − 𝛽) α (𝑙) 𝑖 𝑗 + 𝛽𝛼 (𝑙−1) 𝑖 𝑗 ,<label>(10)</label></formula><p>where hyperparameter 𝛽 ∈ [0, 1] is a scaling factor.</p><p>Multi-head Attention. Similar to GAT, we adopt multi-head attention to enhance model's expressive capacity. Specifically, we perform 𝐾 independent attention mechanisms according to Equation <ref type="bibr" target="#b7">(8)</ref>, and concatenate their results as the final representation.</p><p>The corresponding updating rule is:</p><formula xml:id="formula_14">𝛼 (𝑙) 𝑖 𝑗𝑘 = (1 − 𝛽) α (𝑙) 𝑖 𝑗𝑘 + 𝛽𝛼 (𝑙−1) 𝑖 𝑗𝑘 ,<label>(11) ĥ</label></formula><formula xml:id="formula_15">(𝑙) 𝑖𝑘 = 𝑗 ∈N 𝑖 𝛼 (𝑙) 𝑖 𝑗𝑘 𝑾 (𝑙) 𝑘 𝒉 (𝑙−1) 𝑗 ,<label>(12)</label></formula><formula xml:id="formula_16">𝒉 (𝑙) 𝑖 = 𝜎 𝐾 ∥ 𝑘=1 ĥ(𝑙) 𝑖𝑘 + 𝑾 (𝑙) res(𝑘) 𝒉 (𝑙−1) 𝑖 , (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>where ∥ denotes concatenation operation, and α (𝑙)</p><p>𝑖 𝑗𝑘 is attention score computed by the 𝑘 𝑡ℎ linear transformation 𝑾 𝑙 𝑘 according to Equation <ref type="bibr" target="#b8">(9)</ref>.</p><p>Usually the output dimension cannot be divided exactly by the number of heads. Following GAT, we no longer use concatenation but adopt averaging for the representation in the final (𝐿 𝑡ℎ ) layer, i.e., 𝒉</p><formula xml:id="formula_18">(𝐿) 𝑖 = 1 𝐾 𝐾 𝑘=1 ĥ(𝐿) 𝑖𝑘 . (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>Adaptation for Link Prediction. We slightly modify the model architecture for better performance on link prediction. Edge residual is removed and the final embedding is the concatenation of  embeddings from all the layers. This adapted version is similar to JKNet <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">𝐿 2 Normalization</head><p>We find that an 𝐿 2 normalization on the output embedding is extremely useful, i.e., is the final representation from Equation <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_20">𝒐 𝑖 = 𝒉 (𝐿) 𝑖 ∥𝒉 (𝐿) 𝑖 ∥ ,<label>(15)</label></formula><p>The normalization on the output embedding is very common for retrieval-based tasks, because the dot product will be equivalent to the cosine similarity after normalization. But we also find its improvements for classification tasks, which was also observed in computer vision <ref type="bibr" target="#b25">[26]</ref>. Additionally, it suggests to multiply a scaling parameter 𝛼 to the output embedding <ref type="bibr" target="#b25">[26]</ref>. We find that tuning an appropriate 𝛼 indeed improves the performance, but it varies a lot in different datasets. We thus keep the form of Eq. ( <ref type="formula" target="#formula_20">15</ref>) for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We benchmark results for 1) all HGNNs discussed in Section 3, 2) GCN and GAT, and 3) Simple-HGN on HGB. All experiments are reported with the average and the standard variance of five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmark</head><p>Tables <ref type="table" target="#tab_4">3, 4</ref>, and 5 report results for node classification, link prediction, and knowledge-aware recommendation, respectively. The results show that under fair comparison, 1) the simple homogeneous GAT can matches the best HGNNs in most cases, and 2) inherited from GAT, Simple-HGN consistently outperforms all advanced HGNNs methods for node classification on four datasets, link prediction on three datasets, and knowledge-aware recommendation on three datasets.</p><p>Implementations of all previous HGNNs are based on their official codes to avoid errors introduced by re-implementation. The only modification occurs on their data loading interfaces and downstream decoders, if necessary, to make their codes adapt to the HGB pipeline.</p><p>We use Adam optimizer with weight decay for all methods, and tune hyperparameters based on the validation set performance. The details of hyperparameters are recorded in Appendix C. For the methods requiring meta-paths, the meta-paths used in benchmark datasets are shown in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Time and Memory Consumption</head><p>We test the time and memory consumption of all available HGNNs for node classification on the DBLP dataset. The results are showed in Figure <ref type="figure" target="#fig_2">2</ref>. It is worth noting that we only measure the time consumption of one epoch for each model, but the needed number of epochs until convergence could be various and hard to exactly define. HetSANN is omitted due to our failure to get a reasonable Micro-F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>The ablation studies on all the three tasks are summarized in Table <ref type="table" target="#tab_6">6</ref>. Residual connection and 𝐿 2 normalization consistently improve performance, but type embedding only slightly boosts the performance on node classification, although it is the best way in our experiments to encode type information explicitly under the GAT framework. We will discuss the possible reasons in § 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSION</head><p>In this work, we identify the neglected issues in heterogeneous GNNs, setup the heterogeneous graph benchmark (HGB), and introduce a simple and strong baseline Simple-HGN. The goal of this work is to understand and advance the developments of heterogeneous GNNs by facilitating reproducible and robust research.</p><p>Notwithstanding the extensive and promising results, there are still open questions remaining for heterogeneous GNNs and broadly heterogeneous graph representation learning.</p><p>Is explicit type information useful? Ablation studies in Table <ref type="table" target="#tab_6">6</ref> suggest the type embeddings only bring minor improvements. We hypothesize that the main reason is that the heterogeneity of node features already implies the different node and edge types. Another possibility is that the current graph attention mechanism <ref type="bibr" target="#b31">[32]</ref> is too weak to fuse the type information with feature information. We leave this question for future study.</p><p>Are meta-paths or variants still useful in GNNs? Meta-paths <ref type="bibr" target="#b28">[29]</ref> are proposed to separate different semantics with human prior. However, the premise of (graph) neural networks is to avoid the feature engineering process by extracting implicit and useful features underlying the data. Results in previous sections also suggest that meta-path based GNNs do not generate outperformance over the homogeneous GAT. Are there better ways to leverage meta-paths in heterogeneous GNNs than existing attempts? Will meta-paths still be necessary for heterogeneous GNNs in the future and what are the substitutions?</p><p>A TIME AND MEMORY CONSUMPTION </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RANDOM NEGATIVE</head><p>The distribution of negative samples of test set in link prediction task has a great impact on the performance score. The results with random negative test in our benchmark are shown in Table <ref type="table" target="#tab_7">7</ref>. As we can see, the scores are greater than those in Table <ref type="table" target="#tab_4">4</ref> by a large margin. Most works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44]</ref> evaluate link prediction with randomly sampled negative pairs, which are easy to distinguish from the positive pairs for most methods. However, in real world scenarios, we usually need to discriminate positive and negative node pairs with similar characters, instead of random ones, due to the widely used "retrieve then re-rank" industrial pipeline. Therefore, we choose to use sampled 2-hop neighbors as our negative test set in benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HYPER-PARAMETERS</head><p>We search learning rate within {1, 5}×{10 −6 , 10 −5 , 10 −4 , 10 −3 , 10 −2 } in all cases, and {0, 1, 2, 5} × {10 −6 , 10 −5 , 10 −4 , 10 −3 } for weight decay rate. We set dropout rate as 0.1 in recommendation, and 0.5 in node classification and link prediction by default. For batch size, we will try {1024, 2048, 4096, 8192}, unless the code of the author has special requirements. For training epoch, we will use early stop mechanism based on the evaluation on validation set to promise fully training.</p><p>For brevity, we will denote some variables. Suppose dimension of embeddings for graph layers as 𝑑, dimension of edge embeddings as 𝑑 𝑒 , dimension of attention vector (if exists) as 𝑑 𝑎 , number of graph layers as 𝐿, number of attention heads as 𝑛 ℎ , negative slope of LeakyReLU as 𝑠.</p><p>For input feature type, we use feat = 0 to denote using all given features, feat = 1 to denote using only target node features, and feat = 2 to denote all nodes with one-hot features. Moreover, as suggested in GTN paper, we aggregate the keyword node information as attribute to neighbors and use the left subgraph to do node classification. We also tried to use the whole graph for GTN. Unfortunately, it collapse in that case, which indicates GTN is sensitive to the graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Simple-HGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 RSHN</head><p>For IMDB and DBLP, we set 𝐿 = 2 and 𝑑 = 16. For ACM, we 𝐿 = 3 and 𝑑 = 32. We use feat = 0, 1, 2 for ACM, IMDB and DBLP respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 HetGNN</head><p>C.5.1 Node classification. We set 𝑑 = 128, feat = 0, and batch size as 200 for all datasets. For random walk, we set walk length as 30 and the window size as 5.</p><p>C.5.2 Link prediction. We set 𝑑 = 128, feat = 2, and batch size as 200 for all datasets. For random walk, we set walk length as 30 and the window size as 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 MAGNN</head><p>We set 𝑑 = 64, 𝑑 𝑎 = 128 and 𝑛 ℎ = 8 in all cases.</p><p>C.6.1 Node classification. We use feat = 1 in all cases. For DBLP and ACM datasets, we set batch size as 8, and number of neighbor samples as 100. For IMDB dataset, we use full batch training. C.6.2 Link prediction. We set batch size as 8, and number of neighbor samples as 100 for LastFM. For other datasets, we failed to adapt the MAGNN code to them because there is too much hard-coding.  C.9 GCN C.9.1 Node classification. We set 𝑑 = 64 for all datasets. We set 𝐿 = 3 for DBLP, ACM and Freebase, and 𝐿 = 4 for IMDB. We use feat = 2 for DBLP and Freebase, and feat = 0 for ACM and IMDB. C.12 GATNE C.12.1 Link prediction. We set 𝑑 = 200, 𝑑 𝑒 = 10, 𝑑 𝑎 = 20, feat = 2 for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 HetSANN</head><p>For random walk, we set walk length as 30 and the window size as 5. For neighbor sampling, we set negative samples for optimization as 5, neighbor samples for aggregation as 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.13 KGCN and KGNN-LS</head><p>C.13.1 Recommendation. For all datasets, we set 𝑑 (0) = 64 and 𝑑 (1) = 48. We also tried to stack more graph layers, but performance deteriorates when we do that, which is also found in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>  experiments. We use sum aggregator because it has best overall performance as reported in <ref type="bibr" target="#b33">[34]</ref>.</p><p>C.14 KGAT C.14.1 Recommendation. We set 𝑑 (0) = 64, 𝑑 (1) = 32, 𝑑 (2) = 16 for all datasets. For attention mechanism, we keep the Bi-Interaction aggregator according to their official code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D META-PATHS</head><p>The meta-paths used in benchmark experiments are shown in Table 8. We try to select meta-paths following prior works. For example, meta-paths in DBLP, IMDB and LastFM are from <ref type="bibr" target="#b11">[12]</ref>. Metapaths in ACM dataset are based on <ref type="bibr" target="#b35">[36]</ref>, and we also add some meta-paths related to citation and reference relation, which are not used in <ref type="bibr" target="#b35">[36]</ref>. For Freebase dataset, we first count the most frequent meta-paths with length from 2 to 4, and then manually select 7 of them according to the performance on validation set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>→item" indicates the first user may be a potential costumer of the last item.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: HGB pipeline and Simple-HGN. In this illustration, we assume only the features of Type 2 nodes are kept in the Feature Preprocessing period. The purple parts are the improvements over GAT in Simple-HGN.</figDesc><graphic url="image-2.png" coords="7,53.80,106.43,154.31,135.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Time and memory comparison for HGNNs on DBLP. The area of the circles represent the (relative) memory consumption of the corresponding models.</figDesc><graphic url="image-4.png" coords="10,95.84,108.54,156.14,100.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>C. 1 . 1</head><label>11</label><figDesc>Node classification. We set 𝑑 = 𝑑 𝑒 = 64, 𝑛 ℎ = 8, 𝛽 = 0.05 for all datasets. For DBLP, ACM and Freebase datasets, we set 𝐿 = 3, 𝑠 = 0.05. For IMDB dataset, we set 𝐿 = 6, 𝑠 = 0.1. We set feat = 0 for IMDB, feat = 1 for ACM, and feat = 2 for DBLP and Freebase.C.1.2 Link prediction. We set 𝑑 = 64, 𝑑 𝑒 = 32, 𝑛 ℎ = 2, 𝛽 = 0, 𝑠 = 0.01 for all datasets. For Amazon and PubMed, we use DistMult as decoder, and set 𝐿 = 3. For LastFM, we use dot product as decoder, and set 𝐿 = 4. We use feat = 2 for all datasets.C.1.3 Recommendation. For all datasets, we set 𝑑 (0) = 64, 𝑑(1) = 32, 𝑑(2) = 16, 𝑛 ℎ = 1, 𝑠 = 0.01 as suggested in<ref type="bibr" target="#b34">[35]</ref>.C.2 HANC.2.1 Node classification. We set 𝑑 = 8, 𝑑 𝑎 = 128, 𝑛 ℎ = 8 and 𝐿 = 2 for all datasets. For input feature type, we use feat = 2 in Freebase, and feat = 1 in other datasets. We have also tried larger 𝑑, but the vairation of performance becoms very large. Therefore, we keep 𝑑 = 8 as suggested in HAN's code. C.3 GTN C.3.1 Node classification. We use adaptive learning rate suggested in their paper for all datasets. We set 𝑑 = 64, number of GTN channels as 2. For DBLP and ACM, we set 𝐿 = 2. For IMDB dataset, we set 𝐿 = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>C. 7 . 1</head><label>71</label><figDesc>Node classification. For ACM, we set 𝑑 = 64, 𝐿 = 3, 𝑛 ℎ = 8 and feat = 0. For IMDB, we set 𝑑 = 32, 𝐿 = 2, 𝑛 ℎ = 4 and feat = 1. For DBLP, we set 𝑑 = 64, 𝐿 = 2, 𝑛 ℎ = 4 and feat = 2. C.8 HGT C.8.1 Node Classification. We use layer normalization in each layer, and set 𝑑 = 64, 𝑛 ℎ = 8 for all datasets. 𝐿 is set to 2, 3, 3, 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C. 9 . 2</head><label>92</label><figDesc>Link prediction. We set 𝑑 = 64, 𝐿 = 2, and feat = 2 for all datasets. C.10 GAT C.10.1 Node classification. We set 𝑑 = 64, 𝑛 ℎ = 8 for all datasets. For DBLP, ACM and Freebase, we set 𝑠 = 0.05 and 𝐿 = 3. For IMDB, we set 𝑠 = 0.1 and 𝐿 = 5. We use feat = 2 for DBLP and Freebase, feat = 1 for ACM, and feat = 0 for IMDB. C.10.2 Link prediction. We set 𝑑 = 64, 𝑛 ℎ = 4, 𝐿 = 3 and feat = 2 for all datasets. C.11 RGCN C.11.1 Node classification. We set 𝐿 = 5 for all datasets. For ACM, we set 𝑑 = 16, feats = 2. For DBLP and Freebase, we set 𝑑 = 16, feats = 3. For IMDB, we set 𝑑 = 32, feats = 1. C.11.2 Link prediction. We set 𝐿 = 4, 𝑑 = 64 and feat = 2 for all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of HGB datasets.</figDesc><table><row><cell>Node Classification</cell><cell>#Nodes</cell><cell>#Node Types</cell><cell>#Edges</cell><cell>#Edge Types</cell><cell cols="2">Target #Classes</cell></row><row><cell>DBLP</cell><cell>26,128</cell><cell>4</cell><cell>239,566</cell><cell>6</cell><cell>author</cell><cell>4</cell></row><row><cell>IMDB</cell><cell>21,420</cell><cell>4</cell><cell>86,642</cell><cell>6</cell><cell>movie</cell><cell>5</cell></row><row><cell>ACM</cell><cell>10,942</cell><cell>4</cell><cell>547,872</cell><cell>8</cell><cell>paper</cell><cell>3</cell></row><row><cell>Freebase</cell><cell>180,098</cell><cell>8</cell><cell cols="2">1,057,688 36</cell><cell>book</cell><cell>7</cell></row><row><cell>Link Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Target</cell></row><row><cell>Amazon</cell><cell>10,099</cell><cell>1</cell><cell>148,659</cell><cell>2</cell><cell cols="2">product-product</cell></row><row><cell>LastFM</cell><cell>20,612</cell><cell>3</cell><cell>141,521</cell><cell>3</cell><cell cols="2">user-artist</cell></row><row><cell>PubMed</cell><cell>63,109</cell><cell>4</cell><cell cols="2">244,986 10</cell><cell cols="2">disease-disease</cell></row><row><cell cols="7">Recommendation Amazon-book LastFM Movielens Yelp-2018</cell></row><row><cell>#Users</cell><cell></cell><cell>70,679</cell><cell>23,566</cell><cell cols="2">37,385</cell><cell>45,919</cell></row><row><cell>#Items</cell><cell></cell><cell>24,915</cell><cell>48,123</cell><cell></cell><cell>6,182</cell><cell>45,538</cell></row><row><cell>#Interactions</cell><cell></cell><cell cols="2">846,434 3,034,763</cell><cell cols="3">539,300 1,183,610</cell></row><row><cell>#Entities</cell><cell></cell><cell cols="2">113,487 106,389</cell><cell cols="2">24,536</cell><cell>136,499</cell></row><row><cell>#Relations</cell><cell></cell><cell>39</cell><cell>9</cell><cell></cell><cell>20</cell><cell>42</cell></row><row><cell>#Triplets</cell><cell cols="3">2,557,746 464,567</cell><cell cols="3">237,155 1,853,704</cell></row></table><note>on the validation set, but due to the high consumption to train the model 2 |𝑇 𝑣 | times, we decide to only enumerate three choices, i.e. using all given node features, using only features of target node type, or replacing all node features as one-hot vectors.4.4 Downstream Decoders and Loss function4.4.1 Node Classification. After setting the final dimension of HGNNs the same as the number of classes, we then adopt the most usual loss functions. For single-label classification, we use softmax and cross-entropy loss. For multi-label datasets, i.e. IMDB in HGB, we use a sigmoid activation and binary cross-entropy loss.4.4.2 Link Prediction.As RGCN<ref type="bibr" target="#b1">[2]</ref> suggests, DistMult<ref type="bibr" target="#b38">[39]</ref> performs better than direct dot product, due to multiple types of edges, i.e. for node pair 𝑢, 𝑣 and a target edge type 𝑟 , Prob 𝑟 (𝑢, 𝑣 is positive) = sigmoid HGNN(𝑢) 𝑇 𝑅 𝑟 HGNN(𝑣) ,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">DBLP</cell><cell cols="2">IMDB</cell><cell cols="2">ACM</cell><cell cols="2">Freebase</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Macro-F1</cell><cell>Micro-F1</cell></row><row><cell>RGCN</cell><cell>91.52±0.50</cell><cell>92.07±0.50</cell><cell>58.85±0.26</cell><cell>62.05±0.15</cell><cell>91.55±0.74</cell><cell>91.41±0.75</cell><cell>46.78±0.77</cell><cell>58.33±1.57</cell></row><row><cell>HAN</cell><cell>91.67±0.49</cell><cell>92.05±0.62</cell><cell>57.74±0.96</cell><cell>64.63±0.58</cell><cell>90.89±0.43</cell><cell>90.79±0.43</cell><cell>21.31±1.68</cell><cell>54.77±1.40</cell></row><row><cell>GTN</cell><cell>93.52±0.55</cell><cell>93.97±0.54</cell><cell>60.47±0.98</cell><cell>65.14±0.45</cell><cell>91.31±0.70</cell><cell>91.20±0.71</cell><cell>-</cell><cell>-</cell></row><row><cell>RSHN</cell><cell>93.34±0.58</cell><cell>93.81±0.55</cell><cell>59.85±3.21</cell><cell>64.22±1.03</cell><cell>90.50±1.51</cell><cell>90.32±1.54</cell><cell>-</cell><cell>-</cell></row><row><cell>HetGNN</cell><cell>91.76±0.43</cell><cell>92.33±0.41</cell><cell>48.25±0.67</cell><cell>51.16±0.65</cell><cell>85.91±0.25</cell><cell>86.05±0.25</cell><cell>-</cell><cell>-</cell></row><row><cell>MAGNN</cell><cell>93.28±0.51</cell><cell>93.76±0.45</cell><cell>56.49±3.20</cell><cell>64.67±1.67</cell><cell>90.88±0.64</cell><cell>90.77±0.65</cell><cell>-</cell><cell>-</cell></row><row><cell>HetSANN</cell><cell>78.55±2.42</cell><cell>80.56±1.50</cell><cell>49.47±1.21</cell><cell>57.68±0.44</cell><cell>90.02±0.35</cell><cell>89.91±0.37</cell><cell>-</cell><cell>-</cell></row><row><cell>HGT</cell><cell>93.01±0.23</cell><cell>93.49±0.25</cell><cell>63.00±1.19</cell><cell>67.20±0.57</cell><cell>91.12±0.76</cell><cell>91.00±0.76</cell><cell>29.28±2.52</cell><cell>60.51±1.16</cell></row><row><cell>GCN</cell><cell>90.84±0.32</cell><cell>91.47±0.34</cell><cell>57.88±1.18</cell><cell>64.82±0.64</cell><cell>92.17±0.24</cell><cell>92.12±0.23</cell><cell>27.84±3.13</cell><cell>60.23±0.92</cell></row><row><cell>GAT</cell><cell>93.83±0.27</cell><cell>93.39±0.30</cell><cell>58.94±1.35</cell><cell>64.86±0.43</cell><cell>92.26±0.94</cell><cell>92.19±0.93</cell><cell>40.74±2.58</cell><cell>65.26±0.80</cell></row><row><cell cols="9">Simple-HGN 94.01±0.24 94.46±0.22 63.53±1.36 67.36±0.57 93.42±0.44 93.35±0.45 47.72±1.48 66.29±0.45</cell></row></table><note>Node classification benchmark. Vacant positions ("-") mean that the models run out of memory on large graphs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Link prediction benchmark. Vacant positions ("-") are due to lack of meta-paths on those datasets.</figDesc><table><row><cell></cell><cell cols="2">Amazon</cell><cell cols="2">LastFM</cell><cell cols="2">PubMed</cell></row><row><cell></cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>ROC-AUC</cell><cell>MRR</cell></row><row><cell>RGCN</cell><cell>86.34±0.28</cell><cell>93.92±0.16</cell><cell>57.21±0.09</cell><cell>77.68±0.17</cell><cell>78.29±0.18</cell><cell>90.26±0.24</cell></row><row><cell>GATNE</cell><cell>77.39±0.50</cell><cell>92.04±0.36</cell><cell>66.87±0.16</cell><cell>85.93±0.63</cell><cell>63.39±0.65</cell><cell>80.05±0.22</cell></row><row><cell>HetGNN</cell><cell>77.74±0.24</cell><cell>91.79±0.03</cell><cell>62.09±0.01</cell><cell>83.56±0.14</cell><cell>73.63±0.01</cell><cell>84.00±0.04</cell></row><row><cell>MAGNN</cell><cell>-</cell><cell>-</cell><cell>56.81±0.05</cell><cell>72.93±0.59</cell><cell>-</cell><cell>-</cell></row><row><cell>HGT</cell><cell>88.26±2.06</cell><cell>93.87±0.65</cell><cell>54.99±0.28</cell><cell>74.96±1.46</cell><cell>80.12±0.93</cell><cell>90.85±0.33</cell></row><row><cell>GCN</cell><cell cols="3">92.84±0.34 97.05±0.12 59.17±0.31</cell><cell>79.38±0.65</cell><cell>80.48±0.81</cell><cell>90.99±0.56</cell></row><row><cell>GAT</cell><cell>91.65±0.80</cell><cell>96.58±0.26</cell><cell>58.56±0.66</cell><cell>77.04±2.11</cell><cell>78.05±1.77</cell><cell>90.02±0.53</cell></row><row><cell cols="7">Simple-HGN 93.40±0.62 96.94±0.29 67.59±0.23 90.81±0.32 83.39±0.39 92.07±0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Knowledge-aware recommendation benchmark. KGAT-refers to KGAT without redundant designs (See § 3.3.2). GCN and GAT are not included, because they are already very similar to KGCN and KGAT-. The KGCN and KGAT works focus more on incorporating knowledge into user-item graphs than new architectures.</figDesc><table><row><cell></cell><cell cols="2">Amazon-Book</cell><cell cols="2">LastFM</cell><cell cols="2">Yelp-2018</cell><cell cols="2">MovieLens</cell></row><row><cell></cell><cell>recall@20</cell><cell>ndcg@20</cell><cell>recall@20</cell><cell>ndcg@20</cell><cell>recall@20</cell><cell>ndcg@20</cell><cell>recall@20</cell><cell>ndcg@20</cell></row><row><cell>KGCN</cell><cell>0.1464±0.0002</cell><cell>0.0769±0.0002</cell><cell>0.0819±0.0002</cell><cell>0.0705±0.0002</cell><cell>0.0683±0.0003</cell><cell>0.0431±0.0003</cell><cell>0.4237±0.0008</cell><cell>0.2753±0.0005</cell></row><row><cell>KGNN-LS</cell><cell>0.1448±0.0003</cell><cell>0.0759±0.0001</cell><cell>0.0806±0.0003</cell><cell>0.0695±0.0002</cell><cell>0.0671±0.0003</cell><cell>0.0422±0.0002</cell><cell>0.4218±0.0008</cell><cell>0.2741±0.0005</cell></row><row><cell>KGAT</cell><cell>0.1507±0.0003</cell><cell>0.0802±0.0004</cell><cell>0.0877±0.0003</cell><cell>0.0749±0.0003</cell><cell>0.0697±0.0002</cell><cell>0.0450±0.0001</cell><cell>0.4532±0.0004</cell><cell>0.3007±0.0008</cell></row><row><cell>KGAT−</cell><cell>0.1486±0.0003</cell><cell>0.0790±0.0002</cell><cell>0.0890±0.0002</cell><cell>0.0762±0.0002</cell><cell>0.0715±0.0001</cell><cell>0.0460±0.0001</cell><cell>0.4553±0.0003</cell><cell>0.3031±0.0006</cell></row><row><cell cols="9">Simple-HGN 0.1587±0.0011 0.0854±0.0005 0.0917±0.0006 0.0797±0.0003 0.0732±0.0003 0.0466±0.0003 0.4618±0.0007 0.3090±0.0007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies for Simple-HGN.</figDesc><table><row><cell>Task</cell><cell cols="2">Node Classification</cell><cell cols="2">Link Prediction</cell><cell cols="2">Recommendation</cell></row><row><cell>Dataset</cell><cell cols="2">IMDB</cell><cell cols="2">Last.fm</cell><cell cols="2">Movielens-20M</cell></row><row><cell>Metric</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>recall@20</cell><cell>ndcg@20</cell></row><row><cell>Simple-HGN</cell><cell cols="2">63.53±1.36 67.36±0.57</cell><cell>67.59±0.23</cell><cell>90.81±0.32</cell><cell>0.4626±0.0006</cell><cell>0.3532±0.0005</cell></row><row><cell>w.o. type embedding</cell><cell>63.04±1.00</cell><cell>67.06±0.40</cell><cell>67.61±0.13</cell><cell>90.52±0.13</cell><cell cols="2">0.4632±0.0005 0.3537±0.0007</cell></row><row><cell>w.o. L2 normalization</cell><cell>58.06±1.62</cell><cell>65.33±0.69</cell><cell>61.07±0.96</cell><cell>82.51±1.56</cell><cell>0.3837±0.0187</cell><cell>0.2816±0.0173</cell></row><row><cell>w.o. residual connections</cell><cell>61.61±2.34</cell><cell>66.28±1.11</cell><cell>63.33±0.78</cell><cell>84.13±0.62</cell><cell>0.4261±0.0004</cell><cell>0.3192±0.0006</cell></row></table><note>where 𝒐 𝑖 is the output embedding of node 𝑖 and 𝒉 (𝐿) 𝑖</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Link prediction benchmark with random negative test. 04±0.22 99.21±0.15 91.40±0.30 96.04±0.25 for ACM, DBLP, Freebase and IMDB respectively. For input feature type, we use feat = 2 in Freebase, feat = 1 in IMDB and DBLP, and feat = 0 in ACM. C.8.2 Link Prediction. For all datasets, we use layer normalization in each layer, and set 𝑑 = 64, 𝑛 ℎ = 8, feat = 2 and DistMult as decoder.</figDesc><table><row><cell></cell><cell cols="2">Amazon</cell><cell cols="2">Last.fm</cell><cell cols="2">PubMed</cell></row><row><cell></cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>ROC-AUC</cell><cell>MRR</cell><cell>ROC-AUC</cell><cell>MRR</cell></row><row><cell>RGCN</cell><cell>89.76±0.33</cell><cell>95.76±0.22</cell><cell>81.90±0.29</cell><cell>96.68±0.14</cell><cell>88.32±0.08</cell><cell>96.89±0.20</cell></row><row><cell>GATNE</cell><cell>96.67±0.08</cell><cell>98.68±0.06</cell><cell>87.42±0.22</cell><cell>96.35±0.24</cell><cell>78.36±0.92</cell><cell>90.64±0.49</cell></row><row><cell>HetGNN</cell><cell>95.51±0.39</cell><cell>97.91±0.08</cell><cell>87.35±0.02</cell><cell>96.15±0.18</cell><cell>84.14±0.01</cell><cell>91.00±0.03</cell></row><row><cell>MAGNN</cell><cell>-</cell><cell>-</cell><cell>76.50±0.21</cell><cell>85.68±0.04</cell><cell>-</cell><cell>-</cell></row><row><cell>HGT</cell><cell>91.70±2.31</cell><cell>96.07±0.68</cell><cell>80.49±0.78</cell><cell>95.48±0.38</cell><cell>90.29±0.68</cell><cell>97.31±0.09</cell></row><row><cell>GCN</cell><cell>98.57±0.21</cell><cell>99.77±0.02</cell><cell>84.71±0.1</cell><cell>96.60±0.12</cell><cell>86.06±1.23</cell><cell>98.80±0.56</cell></row><row><cell>GAT</cell><cell>98.45±0.11</cell><cell>99.61±0.22</cell><cell>83.55±2.11</cell><cell>91.45±5.66</cell><cell>87.57±1.23</cell><cell>98.38±0.11</cell></row><row><cell cols="2">Simple-HGN 98.74±0.25</cell><cell>99.52±0.08</cell><cell>91.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Meta-paths used in benchmark.</figDesc><table><row><cell>Dataset</cell><cell>Meta-path</cell><cell>Meaning</cell></row><row><cell>DBLP</cell><cell>APA APTPA APVPA</cell><cell>A: author P: paper T: term V: venue</cell></row><row><cell></cell><cell>MDM, MAM</cell><cell>M: movie</cell></row><row><cell>IMDB</cell><cell>DMD, DMAMD</cell><cell>D: director</cell></row><row><cell></cell><cell>AMA, AMDMA</cell><cell>A: actor</cell></row><row><cell></cell><cell></cell><cell>P: paper</cell></row><row><cell></cell><cell>PAP, PSP</cell><cell>A: author</cell></row><row><cell>ACM</cell><cell>PcPAP, PcPSP</cell><cell>S: subject</cell></row><row><cell></cell><cell>PrPAP, PrPSP</cell><cell>c: citation relation</cell></row><row><cell></cell><cell></cell><cell>r: reference relation</cell></row><row><cell>Freebase</cell><cell>BB BFB BLMB BPB BPSB BOFB BUB</cell><cell>B: book F: film L: location M: music P: person S: sport O: organization U: business</cell></row><row><cell>LastFM</cell><cell>UU, UAU UATAU AUA, ATA AUUA</cell><cell>U: user A: artist T: tag</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">All codes and data are available at https://github.com/THUDM/HGB, and the HGB leaderboard is at https://www.biendata.xyz/hgb.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://web.cs.ucla.edu/~yzsun/data/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://www.kaggle.com/karrrimba/movie-metadatacsv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://pubmed.ncbi.nlm.nih.gov</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://www.yelp.com/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://grouplens.org/datasets/movielens/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">We omit the superscript (𝑙) in this equation for the sake of brevity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is supported by the NSFC for Distinguished Young Scholar (61825602) and NSFC (61836013). The authors would like to thank Haonan Wang from UIUC and Hongxia Yang from Alibaba for their kind feedbacks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>ICML&apos;19. PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno>SIGMOD&apos;08. 1247-1250</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Second workshop on information heterogeneity and fusion in recommender systems (HetRec2011)</title>
		<author>
			<persName><forename type="first">Iván</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsvi</forename><surname>Kuflik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys&apos;11</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="387" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning for attributed multiplex heterogeneous network</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metapath-guided heterogeneous graph neural network for intent recommendation</title>
		<author>
			<persName><forename type="first">Junxiong</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2478" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MAGNN: metapath aggregated graph neural network for heterogeneous graph embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WWW&apos;20</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;17. PMLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<title level="m">Inductive representation learning on large graphs</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11747</idno>
		<title level="m">Real-Former: Transformer Likes Residual Attention</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An attention-based graph neural network for heterogeneous structural learning</title>
		<author>
			<persName><forename type="first">Huiting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hantao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4132" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>ICLR&apos;17</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">KRED: Knowledge-Aware Document Representation for News Recommendations</title>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiun-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In RecSys&apos;20. 200-209</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Dual Heterogeneous Graph Attention Network to Improve Long-Tail Performance for Shop Search in E-Commerce</title>
		<author>
			<persName><forename type="first">Xichuan</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3405" to="3415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">L2-constrained softmax loss for discriminative face verification</title>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI&apos;09</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC&apos;18</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mining Heterogeneous Information Networks: Principles and Methodologies</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan and Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge graph convolutional networks for recommender systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3307" to="3313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kgat: Knowledge graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In WWW&apos;19</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR&apos;18</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="5449" to="5458" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<title level="m">Embedding entities and relations for learning and inference in knowledge bases</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MultiSage: Empowering GCN with Contextualized Multi-Embeddings on Web-Scale Multipartite Networks</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Pancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2434" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Heterogeneous Network Representation Learning: A Unified Framework with Survey and Benchmark</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding negative sampling in graph representation learning</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1666" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS&apos;19</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Relation structure-aware heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICDM&apos;19</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
