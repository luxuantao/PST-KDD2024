<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Designing the Topology of Graph Neural Networks: A Novel Feature Fusion Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-29">29 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
							<email>weilanning18z@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">4Paradigm. Inc</orgName>
								<address>
									<addrLine>4 Lenovo</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
							<email>zhaohuan@4paradigm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">4Paradigm. Inc</orgName>
								<address>
									<addrLine>4 Lenovo</addrLine>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computing Technology</orgName>
								<orgName type="department" key="dep2">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Designing the Topology of Graph Neural Networks: A Novel Feature Fusion Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-29">29 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.14531v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Topology Design</term>
					<term>Neural Architecture Search</term>
					<term>Over-smoothing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, Graph Neural Networks (GNNs) have shown superior performance on diverse real-world applications. To improve the model capacity, besides designing aggregation operations, GNN topology design is also very important. In general, there are two mainstream GNN topology design manners. The first one is to stack aggregation operations to obtain the higher-level features but easily got performance drop as the network goes deeper. Secondly, the multiple aggregation operations are utilized in each layer which provides adequate and independent feature extraction stage on local neighbors while are costly to obtain the higher-level information. To enjoy the benefits while alleviating the corresponding deficiencies of these two manners, we learn to design the topology of GNNs in a novel feature fusion perspective which is dubbed F 2 GNN. To be specific, we provide a feature fusion perspective in designing GNN topology and propose a novel framework to unify the existing topology designs with feature selection and fusion strategies. Then we develop a neural architecture search method on top of the unified framework which contains a set of selection and fusion operations in the search space and an improved differentiable search algorithm. The performance gains on eight real-world datasets demonstrate the effectiveness of F 2 GNN. We further conduct experiments to show that F 2 GNN can improve the model capacity while alleviating the deficiencies of existing GNN topology design manners, especially alleviating the over-smoothing problem, by utilizing different levels of features adaptively. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Data mining; ? Computing methodologies ? Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) have been widely used due to their promising performance in various graph-based applications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. In the literature, different GNN models can be built by designing the aggregation operations and the topology. To improve the model capacity, diverse aggregation operations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref> are designed to aggregate the information from the neighborhood. On the other hand, topology design is also important for the model capacity <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>. One typical topology design manner in GNNs is to stack the aggregation operations. The higher-order neighborhoods can be accessed based on the stacking manner <ref type="bibr" target="#b21">[22]</ref>, thus, higher-level features can be extracted recursively to increase the model capacity <ref type="bibr" target="#b25">[26]</ref>. However, as the network goes deeper, the node representations of connected nodes become indistinguishable, which is called the over-smoothing problem <ref type="bibr" target="#b25">[26]</ref>. To address this problem, the identity skip-connection is applied in topology design thus the features of different levels can be utilized to improve the model capacity, e.g., JK-Net <ref type="bibr" target="#b49">[50]</ref>, Res-GCN <ref type="bibr" target="#b23">[24]</ref> and DenseGCN <ref type="bibr" target="#b23">[24]</ref>. Apart from the stacking manner in GNN topology design, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> use multiple aggregation operations in each layer to extract features independently and fuse these features to enhance the information extraction of local neighbors. However, multiple aggregations require more resources, hence it is costly to obtain higher-level information when stacking more layers. To summarize, there are two mainstream GNN topology design manners as shown in Figure <ref type="figure" target="#fig_0">1</ref> <ref type="foot" target="#foot_1">2</ref> : (i) stacking aggregation operations to obtain the higher-level features (methods on the yellow background); (ii) using multiple aggregation operations to obtain the adequate and independent feature extraction stage on local neighbors. Both of these two manners can improve model capacity. However, the former is easy to get the performance drop due to the over-smoothing problem, and the latter is costly to obtain the higher-level information. Then when designing a GNN model for a specific task, a natural question arises: Can we enjoy the benefits while alleviate the corresponding deficiencies on top of these two topology design manners? In that way, we can further improve the model capacity on top of existing GNN models.</p><p>However, it is non-trivial to achieve this since there lacks a systematic approach for the GNN topology design in existing works, and it is unknown how to combine the aforementioned two GNN topology design manners. In the literature, the topology of a neural network can be represented by its "computational graph" where the nodes represent the operations and the directed edges link operations in different layers <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. In this way, the topology of a neural network can be obtained by designing the links among the nodes in the "computational graph". In existing GNNs, designing the links among operations is equivalent to selecting the features of different levels which proved useful in improving the performance <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref>. Nevertheless, the fusion strategy designed to make better utilization of the selected features is also indispensable in improving the GNN model capacity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. By reviewing extensive human-designed GNNs, we observe that diverse fusion strategies are employed to integrate the features generated by linked operations. As the representative GNNs shown in Figure <ref type="figure" target="#fig_0">1</ref>, Res-GCN <ref type="bibr" target="#b23">[24]</ref> adds up the residual features in each layer, JK-Net <ref type="bibr" target="#b49">[50]</ref> provides three fusion operations (maximum for example) to integrate the representations of the intermediate layers, and PNA <ref type="bibr" target="#b8">[9]</ref> concatenates the results of multiple aggregation operations. Therefore, in designing the topology of GNNs, it is important to take the links as well as the fusion strategies into consideration.</p><p>On top of the existing GNNs, we design a novel framework to unify the GNN topology designs with feature selection and fusion strategies. Therefore, the topology design target is transformed into the design of these 2 strategies. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, without loss of generality, the framework is represented as a directed acyclic graph (DAG), which is constructed with an ordered sequence of blocks. Based on this framework, diverse topology designs including existing works can be modeled by different selection and fusion strategies.</p><p>Then, another challenge is to design adaptive feature selection and fusion strategies to achieve the SOTA performance given a specific graph-based task, since the preferable GNN topology can significantly differ across different datasets <ref type="bibr" target="#b52">[53]</ref>. Therefore, to address this challenge, we borrow the power of neural architecture search (NAS), which has been successful in designing data-specific CNNs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b61">62]</ref> and GNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b58">59]</ref>, to achieve the adaptive topology design. To be specific, we firstly propose a novel search space that contains a set of selection and fusion operations, and then develop an improved differentiable search algorithm based on a popular one, i.e., DARTS <ref type="bibr" target="#b28">[29]</ref>, by addressing the obvious optimization gap induced by two opposite operations in the search space. Finally, we extract the optimal strategies based on the unified framework when the searching process terminates, then an adaptive topology design is obtained.</p><p>In this paper, we learn to design the topology of GNNs in a novel feature fusion perspective and it can be dubbed F 2 GNN (Feature Fusion GNN). Extensive experiments are conducted by integrating the proposed method with predefined and learnable aggregation operations on eight real-world datasets, then the performance gains demonstrate the effectiveness of the proposed method. Furthermore, we conduct experiments to evaluate the advantages of F 2 GNN in designing the topology of GNNs, from which we can observe that F 2 GNN can enjoy the benefits and alleviate the deficiencies of existing topology design manners, especially alleviating the over-smoothing problem, by utilizing different levels of features adaptively.</p><p>To summarize, the contributions of this work are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Topology Designs in Graph Neural Network</head><p>GNNs are built by designing the aggregation operations and topologies. One mainstream topology design manner is to stack aggregation operations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>. The high-level information can be captured to improve the model capacity while easily resulting in the over-smoothing problem in deeper networks <ref type="bibr" target="#b25">[26]</ref>. To address this problem and improve the model capacity, the identity skipconnections are provided additionally to integrate different levels of features. JK-Net <ref type="bibr" target="#b49">[50]</ref> and DAGNN <ref type="bibr" target="#b29">[30]</ref> integrate the features of all the intermediate layers at the end of GNNs; ResGCN <ref type="bibr" target="#b23">[24]</ref> and DenseGCN <ref type="bibr" target="#b23">[24]</ref> have the same connection schemes as ResNet <ref type="bibr" target="#b17">[18]</ref> and DenseNet <ref type="bibr" target="#b19">[20]</ref>; GCNII <ref type="bibr" target="#b5">[6]</ref> adds up the initial features in each layer. Apart from the stacking manner, multiple aggregation operations provide adequate and independent feature extraction on the local neighbors to improve the model capacity. PNA <ref type="bibr" target="#b8">[9]</ref> and HAG <ref type="bibr" target="#b22">[23]</ref> provide multiple aggregation operations in each layer to learn features from local neighbors independently; MixHop <ref type="bibr" target="#b0">[1]</ref>, IncepGCN <ref type="bibr" target="#b37">[38]</ref> and InceptionGCN <ref type="bibr" target="#b20">[21]</ref>, which are the inceptionlike methods, provide multiple aggregations in each branch to extract different levels of features independently. However, multiple aggregations require more resources, hence it is costly to obtain higher-level information.</p><p>Considering the benefits and deficiencies of the existing topology designs, we propose a novel method F 2 GNN in the feature fusion perspective to design the topology of GNNs adaptively. It can unify the existing topology designs and provide a platform to explore more topology designs, which are more expressive than these humandesigned ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Architecture Search</head><p>NAS methods use a search algorithm to find the SOTA neural networks automatically in a pre-defined search space and representative methods are <ref type="bibr">[25, 29, 35-37, 48, 62]</ref>. Very recently, researchers tried to automatically design GNNs by NAS. These methods mainly focus on designing the aggregation operations on top of the vanilla stacking manner, e.g., GraphNAS <ref type="bibr" target="#b13">[14]</ref> provides the attention function, attention head number, embedding size, etc. Similar search spaces are also used in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60]</ref>. Several methods further provide the skipconnection design based on this stacking manner, e.g., SNAE <ref type="bibr" target="#b57">[58]</ref> and SNAG <ref type="bibr" target="#b58">[59]</ref> are built on JK-Net <ref type="bibr" target="#b49">[50]</ref> and learn to select and fuse the features of intermediate layers in the output node, Auto-Graph <ref type="bibr" target="#b26">[27]</ref> is built on DenseNet <ref type="bibr" target="#b19">[20]</ref> and learns to select features in each layer, GraphGym <ref type="bibr" target="#b52">[53]</ref> provides the residual feature selection and fusion strategies in designing GNNs on top of the stacking manner. Besides these methods, RAN <ref type="bibr" target="#b41">[42]</ref> learns to design the GNN topologies. It uses the computational graph to represent the GNN topology and then designs the links with a randomly-wired graph generator <ref type="bibr" target="#b46">[47]</ref>. However, it lacks the explorations of fusion strategies that can improve feature utilization in GNNs.</p><p>Apart from the search space designs, diverse search algorithms are provided to search the SOTA neural networks from the proposed search space, e.g., incorporating the Reinforcement Learning (RL) into the searching strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62]</ref> or using the Evolutionary Algorithm (EA) directly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41]</ref>. Considering the search efficiency, the differentiable algorithm is proposed to search architectures with gradient descent. It relaxes the discrete search space into continuous and then treats the architecture search problem as a bi-level optimization problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>More graph neural architecture search methods can be found in <ref type="bibr" target="#b56">[57]</ref>. Compared with existing methods which mainly focus on designing aggregation operations, in this work we explore the additional topology design, which can thus be regarded as an orthogonal and complementary approach to improve the GNN model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURE FUSION FRAMEWORK</head><p>In this section, we elaborate on the proposed feature fusion framework and show how to translate this framework into diverse GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Proposed Unified Framework</head><p>In the literature, GNNs can be built by designing the aggregation operation and its topology. The topology is designed in two major manners: stack aggregation operations to obtain higher-level features or use multiple operations to provide rich and independent local feature extractions. By reviewing existing human-designed GNNs, we observe that features of different levels are selected due to diverse selection strategies, and it leads to the utilization of diverse fusion strategies to integrate these features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50]</ref>. In other words, the feature selection and fusion strategies lead to the key difference of topology designs in GNNs. Therefore, the topology designs utilized in existing methods can be unified with these two strategies.</p><p>Based on this motivation, we propose a feature fusion framework to unify these two topology design manners. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, without loss of generality, the framework is represented as a DAG which is constructed with an ordered sequence of one input block, ? SFA blocks (? = 4 for example; SFA: selection, fusion and aggregation), and one output block. The input block only contains a simple pre-process operation, i.e., Multilayer Perceptron (MLP) in this paper, that supports the subsequent blocks. The SFA block contains the selection, fusion and aggregation operations. For the ?-th SFA block as shown in Figure <ref type="figure" target="#fig_1">2</ref>(b), there exists ? predecessors thus ? feature selection operations ? ? are utilized to select the features generated by the previous blocks. One fusion operation ? ? is used to fuse these selected features, and one aggregation operation ? ? is followed to aggregate messages from the neighborhood. Therefore, the high-level features can be generated by</p><formula xml:id="formula_0">H ? = ? ? (? ? ({? 0 ? (H 0 ), ? ? ? , ? ?-1 ? (H ?-1 )})).</formula><p>In the output block, a 2-layer MLP that serves as the post-process operation is provided after the ? + 1 selection operations and one fusion operation. On top of the unified framework, the topology design is transformed into the design of selection and fusion strategies. Compared with existing methods that focus on designing aggregation operations, our framework provides a platform to explore the GNN topology designs which is more expressive than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Translating the framework into diverse GNNs</head><p>The feature fusion framework can unify the existing topology designs thus we can translate the framework into diverse GNNs. The formulation and illustrations of the most closely related methods are shown in Figure <ref type="figure" target="#fig_2">3</ref>. For simplicity, the feature transformation is ignored. Vanilla GNNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref> are constructed by stacking aggregation operations and these methods can be approximated as the single path in our framework. ResGCN <ref type="bibr" target="#b23">[24]</ref> and JK-Net <ref type="bibr" target="#b49">[50]</ref> provide extra identity skip-connections to utilize the different levels of features, thus diverse selection and fusion operations are utilized as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>(b) and (c), respectively. GCNII <ref type="bibr" target="#b5">[6]</ref> uses the initial residual connections to address the over-smoothing problem, and it can be approximated as in Figure <ref type="figure" target="#fig_2">3(d)</ref>. PNA <ref type="bibr" target="#b8">[9]</ref> uses ? aggregation operations in each layer, and it can be approximated by ? branches in this framework. MixHop <ref type="bibr" target="#b0">[1]</ref> concatenates the features based on the power of adjacency matrix A ? . ? is the set of the adjacency powers, and this method can be approximated as in Figure <ref type="figure" target="#fig_2">3(f)</ref>. For the graph neural architecture search methods which focus on design aggregation operations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b59">60]</ref> or provide additional skip-connection learning <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> on top of the stacking manner, they can be treated as the variants of vanilla and JK-Net, respectively. Thus, we can see the advantage of the proposed framework, which can provide a systematic and unified view of existing GNN topology designs. Next, we show how to obtain the data-specific GNN topology design on top of this unified framework.</p><formula xml:id="formula_1">Method Equation Vanilla GNN H ? +1 = ? ? (H ? ) ResGCN [24] H ? +1 = ? ? (H ? + H ? -1 ) JK-Net [50] H ?????? = ? ? (H 1 , ? ? ? , H ? ) GCNII [6] H ? +1 = ?H 0 + (1 -?) ? ? (H ? ) PNA [9] H ? +1 = ? ??? ? ? (H ? ) MixHop [1] H ? +1 = ? ??? A ? H ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN GNNS WITH THE FRAMEWORK</head><p>On top of the unified framework, generating one adaptive topology design is a non-trivial task since it is still unknown how to design the optimal feature selection and fusion strategies given a specific task.</p><p>In this paper, we adopt the NAS method to address this challenge.</p><p>As the commonly-recognized paradigm of NAS, the search space which contains the selection and fusion operation sets is provided.</p><p>Then the topology design can be decomposed into the decision of operations in the search space. To demonstrate the versatility of our method, we further provide the aggregation operation set thus we can design GNNs with topologies and aggregation operations simultaneously. The differentiable search algorithm <ref type="bibr" target="#b28">[29]</ref> is widely used considering the efficiency and effectiveness. Nevertheless, we observe the performance gap in applying this algorithm due to the two opposite selection operations, i.e., ZERO and IDENTITY. Thus, we develop a new algorithm to alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Space</head><p>One GNN can be built by designing the aggregation operations and the topology. The aggregation operations are widely explored in existing methods. In general, GNNs use the same aggregation operation in each layer. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref> learn to design the layer-wise aggregations with the help of the NAS methods as introduced in Section 2.2. Therefore, in this paper, we design the topology designs on these two aggregation variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Predefined aggregation operation.</head><p>The predefined aggregation is utilized in each SFA block and then we need to design the selection and fusion operations in the framework with the help of NAS. After the search terminates, we can obtain the designed GNN with the given aggregation operation. We provide 2 aggregation operations GraphSAGE and GAT thus this method can be dubbed F 2 SAGE and F 2 GAT, respectively. Other aggregation operations like GCN and GIN can be trivially integrated with the proposed framework. Without loss of generality, we provide a set of candidate selection operations O ? and fusion operations O ? in the search space as shown in Table <ref type="table" target="#tab_1">1</ref>. Selection Operations. For the selection operation, there are only "selected" and "not selected" stages for each feature in existing methods. Thus, we provide two operations IDENTITY and ZERO GCN, GAT, SAGE, GIN to select features, which can be represented as ? (h) = h and ? (h) = 0 ? h, respectively. Fusion Operations. In SFA block and the output block, one fusion operation is needed to fuse the selected features. Based on the literature, we provide six fusion operations to fuse these features with the summation, average, maximum, concatenation, LSTM cell and attention mechanism, which are denoted as SUM, MEAN, MAX, CONCAT, LSTM and ATT, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Learnable aggregation operation.</head><p>Compared with the existing graph neural architecture search methods which focus on designing the aggregation operations, we provide the extra aggregation operation set O ? thus we can design GNNs with topologies and aggregation operations simultaneously. This method is dubbed F 2 GNN. Aggregation Operations. Four widely used aggregation operations are used in this paper : GCN <ref type="bibr" target="#b21">[22]</ref>, GAT <ref type="bibr" target="#b42">[43]</ref>, GraphSAGE <ref type="bibr" target="#b16">[17]</ref> and GIN <ref type="bibr" target="#b48">[49]</ref>, which denoted as GCN, GAT, SAGE and GIN, respectively. In this paper, we focus on designing the topology of GNNs thus only four aggregations are provided. More operations in existing methods can be trivially added if needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Improved Search Algorithm</head><p>Based on the proposed framework and the search space, the search algorithm is used to search operations from the corresponding operation set. Considering the efficiency and effectiveness which have been demonstrated in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref>, without loss of generality, the differentiable search algorithm is employed. Preliminary: Differentiable Architecture Search. A supernet is defined to subsume all models on top of the unified framework and the search space, and it is achieved by mixing the candidate operations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. The results of the mixed operation can be calculated by a weighted summation of all candidate operations which denoted as ? (?) = and ? ? is the corresponding learnable supernet parameter for ? ? <ref type="bibr" target="#b28">[29]</ref>. Based on the relaxation function, the discrete selection of operations is relaxed into continuous and we can generate the final results in the output block step by step as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Thus the supernet can be optimized with gradient descent which can accelerate the search process in orders of magnitude. After finishing the search process, we preserve the operations with the largest weights in each mixed operation, from which we obtain the searched GNN. The optimization gap in feature fusion. We optimize the supernet in the search process and then derive the GNN after the search is finished. However, it is difficult to generate the best childnet from the supernet since we optimize the supernet in the search process and only select the childnet in reality, which is called optimization gap <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45]</ref> and the evaluation results can be found in Section 5.4.2. The performance drop caused by the optimization gap is extremely obvious in the our method since we provide two opposite operations in the selection operation set. In the following, we briefly explain this problem.</p><p>For the features generated by block ?, the results of mixed selection operation in block ? (? &gt; ?) can be represented as the weighted summation of ZERO and IDENTITY operations as shown in</p><formula xml:id="formula_2">?? ? (x ? ) = ?? | O ? | ?=1 ? ? ? ? ? ? ? ? (x ? ) = ? ? ? 1 0 + ? ? ? 2 x ? = ? ? ? 2 x ? .<label>(1)</label></formula><p>Then the results of mixed fusion operation in block ? can be generated by</p><formula xml:id="formula_3">? ? (x) = ?? | O ? | ?=1 ? ? ? ? ? ? (x) = ?? | O ? | ?=1 ? ? ? ? ? ? ({ ?? ? (x ? )|? = 0, ? ? ? , ? -1}),<label>(2)</label></formula><p>where ? ? ? is the ?-th candicate fusion operation in block ?. When the weight of ZERO operation ? 1 is larger than the weight of IDENTITY operation ? 2 , in the childnet, the ZERO operation should be chosen and one zero tensor results in Eq. ( <ref type="formula" target="#formula_2">1</ref>) are expected. Furthermore, one zero tensor results will be obtained in Eq. ( <ref type="formula" target="#formula_3">2</ref>) if no feature is selected in this block. However, in the supernet, the mixed operation results ? ? ? 2 x ? in Eq. (1) will be generated. That is, in this case, one zero tensor result is expected while we got ? ? ? 2 x ? in reality in each mixed selection operation. The IDENTITY operation has a large influence in Eq. (1) when ZERO is selected, and the influence will accumulate along with the feature selection operation in the framework. Therefore, the gap between the supernet results and the childnet results in our framework is extremely obvious due to these two opposite selection operations, and we cannot derive the best childnet from the supernet due to this gap as the evaluation in Section 5.4.2. Improved search with the usage of temperature. Considering the influence of IDENTITY operation, we add a temperature in Softmax function as</p><formula xml:id="formula_4">? ? = exp(? ? /?) |O| ?=1 exp(? ? /?)</formula><p>. Thus, with a small temperature ?, the operation weight vector c close to a one-hot vector, and the results of mixed selection operation in Eq. ( <ref type="formula" target="#formula_2">1</ref>) close to a zero tensor when the ZERO operation is selected. That is, the IDENTITY operation will have a smaller influence on the selection results when the ZERO operation is chosen, and the optimization gap in our method can be alleviated. Similar solutions can be found in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48]</ref>. In this paper, we set ? = 0.001 and the influence of different temperatures will be shown in Section 5.4.2. Deriving process. The architecture searching task is treated as the bi-level optimization problem. Our method is optimized with gradient descent introduced in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref>. More details can be found in Appendix A. After finishing the search process, we preserve the operation with the largest weight in each mixed operation, from which we obtain the searched architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Settings</head><p>Datasets. As shown in Table <ref type="table" target="#tab_2">2</ref>, eight widely used real-world datasets are selected to evaluate the performance of our method considering the graph types and graph sizes. Cora <ref type="bibr" target="#b38">[39]</ref>, DBLP <ref type="bibr" target="#b1">[2]</ref> and PubMed <ref type="bibr" target="#b38">[39]</ref> are citation networks where each node represents a paper, and each edge represents the citation relation between two papers; Computers <ref type="bibr" target="#b30">[31]</ref> is the Amazon co-purchase graph where nodes represent goods that are linked by an edge if these goods are frequently bought together; Physics <ref type="bibr" target="#b39">[40]</ref> is a co-authorship graph where nodes are authors who are connected by an edge if they coauthor a paper; Actor <ref type="bibr" target="#b33">[34]</ref> is a co-occurrence graph where each node correspond to an actor and each edge denotes co-occurrence of these two actors on the same Wikipedia page; Wisconsin <ref type="bibr" target="#b33">[34]</ref> is the hyperlinked web page graph where nodes represent web pages and edges are hyperlinks between them; In Flickr <ref type="bibr" target="#b53">[54]</ref>, nodes represent images and edges represent two images that share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.).</p><p>Baselines. On top of the predefined aggregations, we provide nine GNNs constructed with different topologies. (a) GNNs are constructed by stacking two and four aggregation operations; (b) based on the stacking manner, we construct 4-layers GNNs on top of three commonly used topology designs ResGCN <ref type="bibr" target="#b23">[24]</ref>, DenseGCN <ref type="bibr" target="#b25">[26]</ref> and JK-Net <ref type="bibr" target="#b49">[50]</ref>. They are denoted as RES, DENSE and JK, respectively; (c) 4-layer GNNII is constructed based on the topology shown in Figure <ref type="figure" target="#fig_2">3</ref> Compared with F 2 GNN which designs the topology and aggregation operations simultaneously, we provide three graph neural architecture search baselines: (a) an RL based method SNAG <ref type="bibr" target="#b57">[58]</ref>, (b) a differentiable method SANE <ref type="bibr" target="#b58">[59]</ref>, and (c) a random search algorithm that uses the same search space as F 2 GNN.</p><p>More details of these baselines are provided in Appendix B.1. Implementation details. For Cora, DBLP, Computers, PubMed and Physics, we split the dataset with 60% for training, 20% for validation and test each considering supernet training and evaluation. For Wisconsin and Actor, we adopt the 10 random splits used in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b60">61]</ref>(48%/32%/20% of nodes per class for train/validation/test). For Flickr dataset, we adopt the split in <ref type="bibr" target="#b53">[54]</ref>(50%/25%/25% of nodes Table <ref type="table">3</ref>: Performance comparisons of our method and all baselines. We report the average test accuracy and the standard deviation with 10 splits. "L2" and "L4" mean the number of layers of the base GNN architecture, respectively. The best result in each group is highlighted in gray, and the second best one is underlined. The group accuracy rank and the overall accuracy rank of each method are calculated on each dataset. The average rank on all datasets is provided. The Top-2 methods in each group and the Top-3 methods in this table are highlighted in gray. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparisons</head><p>The results are given in Table <ref type="table">3</ref>. Firstly, based on two predefined aggregation operations, there is no absolute winner among eight human-designed GNNs constructed with existing topology designs. Among them, with the utilization of different feature selection and fusion strategies, six baselines have a better performance than two stacking baselines in two groups in general. The performance gain demonstrates the importance of feature utilization in improving the model capacity. Secondly, by designing GNN topologies with adaptive feature selection and fusion strategies, the proposed method can achieve the top rank on two predefined aggregations. In particular, the SOTA performance is achieved on seven out of eight datasets. Thirdly, with the same search space, the Random baselines also achieve considerable performance gains on all these datasets, which demonstrates the effectiveness of the unified framework in Section 3. Nevertheless, the Random baseline is outperformed by the proposed method, which indicates the usefulness of the improved search algorithm in designing the topology of GNNs.</p><p>On the other hand, looking at the results of the learnable aggregation operations, SANE and SNAG only focus on selecting features in the output block based on a stacking manner (A sketch of the topology is given in Figure <ref type="figure" target="#fig_10">8</ref> in Appendix B.1.). These methods have limited topology design space and can be treated as special instance of F 2 GNN. Compared with these two methods, F 2 GNN and Random methods design GNNs on the proposed unified framework and achieve higher performance. Thus it demonstrates the effectiveness of the adaptive topology design, i.e., feature selection and fusion strategies based on the unified framework. Besides, <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref> have shown the efficiency advantage of the differentiable methods (our method and SANE) over those RL and randomly based methods. We also obtained the two orders of magnitude less search cost in our experiments (The details of the search cost comparison are shown in Section 5.4.2.). These 2 differentiable methods stay at the top of the rank list (F 2 GNN ranks 2.75 and SANE ranks 6.38), which indicates the power of differentiable search algorithms in designing GNNs. Searched topologies. We visualize the searched topologies on Cora, Physics and Actor datasets with different aggregation operations in Figure <ref type="figure" target="#fig_5">4</ref>. We emphasize on several interesting observations in the following:</p><p>? The topologies are different for different aggregation operations and datasets. The performance gain and the top ranking demonstrate the necessity of designing data-specific GNN topologies.</p><p>? The initial features generated by the input block are utilized in the output block in almost all GNNs. These features contain more information about the node itself which are important for node representation learning as mentioned in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>? We can benefit from the multiple aggregation design manner which provides adequate and independent local feature extractions, e.g., two aggregation operations are selected in the second layer on Cora and the first layer on Physics.</p><p>? On Actor, we obtained an MLP network based on F 2 SAGE. This topology design is consistent with <ref type="bibr" target="#b60">[61]</ref> which shows that the graph structure is not always useful for the final performance, and it further </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Advantages of the Adaptive Topology Design</head><p>In designing the topology of GNNs, stacking aggregation operations devoted to obtaining higher-level features but easily got the oversmoothing problem as the network goes deeper, and the multiple aggregation operations provide adequate and independent feature extraction stage on local neighbors while are costly to obtain the higher-level information. The performance gains in Table <ref type="table">3</ref> and the searched topology designs in Figure <ref type="figure" target="#fig_5">4</ref> indicate the effectiveness of designing the topology of GNNs with two design manners. With the proposed F 2 GNN which can design topology with the NAS method adaptively, we show the advantages of our method in alleviating the deficiencies of the existing two topology design manners. Alleviating the over-smoothing problem. As the network goes deeper, the node representations become indistinguishable and easily got performance drop, which is called over-smoothing <ref type="bibr" target="#b25">[26]</ref>. MAD (Metric for Smoothness) <ref type="bibr" target="#b3">[4]</ref> is used to measure the smoothness of the features. In Figure <ref type="figure" target="#fig_6">5</ref>, we show the comparisons of test accuracy and MAD value on the Cora dataset on the conditions of different layers and SFA blocks. For comparisons, on top of the GraphSAGE, we provide three topologies that have been proved helpful in alleviating the over-smoothing problem, and more results can be found in Appendix B.4. Other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38]</ref> which can alleviate this problem will be left into future work. In Figure <ref type="figure" target="#fig_6">5</ref>, RES, JK and MixHop can achieve stable performance and higher MAD values compared with the stacking baseline. It demonstrates the effectiveness of different levels of features in alleviating the over-smoothing problem. Compared with these baselines, F 2 SAGE can achieve the best performance and higher MAD values by utilizing features in each block adaptively. It can further indicate the effectiveness of our method. Flexibility in obtaining the higher-level features. The multiple aggregation operations provide an adequate and independent feature extraction stage on local neighbors while are costly to obtain the higher-level information. To make a comparison with this topology design manner, we visualize the utilization of different levels of features on the Cora dataset in Figure <ref type="figure" target="#fig_7">6</ref>. If the features of level ? are selected in Block ?, then the cell (??, ? ?) is denoted as 1, otherwise, as 0. We obtained the features of level 2 with four aggregation operations as shown in Figure <ref type="figure" target="#fig_7">6</ref>(a) and obtained the features of level 5 with eight aggregation operations as shown in Figure <ref type="figure" target="#fig_7">6(c</ref>). However, with the same number of aggregations, i.e., 8 and 4, PNA can only obtain the features of level 2 and level 1, respectively. Besides, on the Cora dataset, our method achieves higher performance than PNA with 35% (0.39MB on F 2 SAGE and 0.60MB on PNA) and 15% (0.22MB on F 2 GAT and 0.26MB on PNA) fewer parameters on the GraphSAGE and GAT, respectively. Thus, our method is more efficient and flexible in obtaining the higher-level features.</p><p>From Figure <ref type="figure" target="#fig_7">6</ref>, we can observe that the lower-level features, which are distinguishable and helpful for prediction <ref type="bibr" target="#b25">[26]</ref>, are more likely to be utilized in each block compared with higher-level ones. Besides, the over-smoothing problem is generated by stacking aggregation operations thus the features in higher-level become indistinguishable. This problem can be alleviated by selecting different levels of features in each block adaptively based on the proposed method. As a summary, we evaluate the over-smoothing problem and visualize how different levels of features are utilized in our framework. The results demonstrate that our method can alleviate corresponding deficiencies of these two manners by utilizing features adaptively in each block. Combined with the performance gain and the top ranking in Table <ref type="table">3</ref>, the advantages of our method over the existing two topology design manners are significant and the aforementioned topology design target can be achieved with the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation study</head><p>In this section, we conduct ablation studies on the proposed framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">The Importance of Fusion Strategy.</head><p>In designing topologies, existing methods mainly focus on the links among operations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. By reviewing the extensive human-designed GNNs, we observe that the feature selection and fusion strategies result in the main difference of topology designs in GNNs. It indicates that the selection and fusion strategies are key components in designing the topology of GNNs, and the fusion strategies should be considered since they can improve the utilization of features in different levels <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59</ref>]. To evaluate the importance of fusion strategy, we learn to design topologies with fixed fusion operations instead. As shown in Table <ref type="table" target="#tab_4">4</ref>, with the three predefined fusion operations, the performance drop is evident on three variants, which demonstrates the importance of the fusion strategy. Therefore, designing the topology of GNNs with the selection and fusion operations is significant compared with the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.2</head><p>The Evaluation of the Improved Search Algorithm. The efficiency of the differentiable search algorithm over others is significant and has been proved in existing methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b58">59]</ref>. Therefore, we provide an improved differentiable search algorithm to design GNNs. In Table <ref type="table">3</ref>, SNAG and Random methods need to sample architectures and then train from scratch. In our experiments, they require 26.44 and 0.903 GPU hours to search GNNs on the Cora dataset. SANE and F 2 GNN employ the differentiable search algorithm and the search cost are 0.007 and 0.028 GPU hours on the Cora dataset, respectively. However, we observe that the optimization gap has a large influence on the feature selection operation due to the two opposite selection operations. As shown in Table <ref type="table" target="#tab_5">5</ref>, we use the validation  accuracy to quantify the optimization gap in the feature fusion framework. These accuracies are obtained at the end of the search and after architecture derivation without fine-tuning. We can observe that when the temperature is too large, i.e., 1 and 0.1, the validation accuracy gap of the supernet and the childnet is large when we search the feature selection and fusion operations. In these cases, the ZERO operation is selected for most features. However, in the F 2 AGG method, which only searches aggregation operations based on the selection operation IDENTITY and fusion operation SUM, the accuracy gap is much smaller. Therefore, considering the two opposite operations in the selection operation set, we use the small temperature of 0.001 instead. The performance gains in Table <ref type="table">3</ref> indicate the effectiveness of the improved search algorithm in addressing the optimization gap in feature fusion and obtaining the expressive GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.3</head><p>The Influence of the Number of SFA Blocks. In this paper, we only use four SFA blocks for example. Here we conduct experiments to show the influences of the number of SFA blocks. In Figure <ref type="figure" target="#fig_8">7</ref>, F 2 SAGE achieves a stable performance on the condition of different SFA blocks, yet the stacked baselines obtained the performance drop due to the over-smoothing problem as shown in Figure <ref type="figure" target="#fig_6">5</ref>. The increasing number of SFA blocks do not bring about the performance drop due to the adaptive utilization of different levels of features, which then demonstrates the strong ability of our method in alleviating the over-smoothing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we provide a novel feature fusion perspective in designing the GNN topology and propose a novel framework to unify the existing topology designs with feature selection and fusion strategies. In this way, designing the topology of GNNs is transformed into designing the feature selection and fusion strategies in the unified framework. To obtain an adaptive topology design, we develop a NAS method. To be specific, we provide a set of candidate selection and fusion operations in the search space and develop an improved differentiable search algorithm to address the obvious optimization gap in our method. Extensive experiments are conducted on the predefined and learnable aggregation operations on eight real-world datasets. The results demonstrate the effectiveness and versatility of the proposed method, and we can enjoy the benefits and alleviate the deficiencies of existing topology design manners, especially alleviating the over-smoothing problem, by utilizing different levels of features adaptively.</p><p>For future work, we will investigate the influence of different candidate operations and algorithms, and learn in-depth the connections between the searched topology designs and the graph properties. Besides, we will explore F 2 GNN in large-scale graphs, e.g., those provided in the open graph benchmark <ref type="bibr" target="#b18">[19]</ref>. for ? = 1 to ? + 1 do 6:</p><p>H ? = { } 7:</p><p>for ? = 0 to ? - </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A OPTIMIZATION</head><p>In this paper, we optimize the supernet parameters ? and operation parameters W with gradient descent as shown in Alg. 1. In the unified framework, the output H ?????? can be calculated as shown in Line 4-14. The mixed selection operation ? ? ? ? and fusion operation ? ? ? are given in Eq. ( <ref type="formula" target="#formula_2">1</ref>) and Eq. ( <ref type="formula" target="#formula_3">2</ref>), respectively. For the learnable aggregation operation, the mixed aggregation operationin in block ? can be calculated as</p><formula xml:id="formula_5">? ? ? (H, A) = | O ? | ?=1 ? ? ? ? ? ? (H, A).</formula><p>For the predefined aggregation, ? ? ? (H, A) equals to the results of the predefined operation. After obtaining the output H ?????? , the cross-entropy loss L ????? and L ????? can be generated and we can update the parameters with gradient descent as shown in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DETAILS OF EXPERIMENTS B.1 Baselines</head><p>To make a fair comparison with F 2 SAGE and F 2 GAT, we provide nine GNNs constructed with the predefined aggregation operation and different topology designs. For two stacking baselines, RES and JK methods, the topology designs of these methods are shown in Figure <ref type="figure" target="#fig_2">3</ref>. The topology design of the DENSE method is constructed based on DenseGCN <ref type="bibr" target="#b23">[24]</ref>. To be more specific, the aggregation operations are densely connected and the fusion operation CONCAT is utilized in each layer.</p><p>For GCNII, the aggregation operation can be represented by H ?+1 = ? (((1 -? ? ) PH ? + ? ? H 0 )((1 -? ? )I ? + ? ? W ? ). ? ? and ? ? are the hyperparameters, P = D-1/2 ? D-1/2 is the graph convolution matrix with the renormalization trick. For simplicity, we ignore the feature transformation thus the feature fusion strategy in GCNII can be represented as H ?+1 = ?H 0 + (1 -?)? ? (H ? ) where ? ? is the aggregation operation. The illustration is shown in Figure <ref type="figure" target="#fig_2">3(d)</ref>. The GNNII baseline in our experiment is constructed on top of Figure <ref type="figure" target="#fig_2">3(d)</ref> where we add the features of H 0 and H ?-1 in Block ?, then an aggregation operation or MLP followed behind.  In Figure <ref type="figure" target="#fig_2">3</ref>(e), one layer PNA with four aggregation operations is provided. The PNA baseline in our experiment is constructed by stacking two layers with eight aggregation operations. In Figure <ref type="figure" target="#fig_2">3</ref>(f), one layer MixHop with three aggregation operations is provided and the MixHop baseline in our experiment is constructed by stacking two layers.</p><p>We provide the performance comparisons of the GCNII, PNA and MixHop baselines used in our experiment and used in PyG <ref type="foot" target="#foot_2">3</ref> . As shown in Table <ref type="table">7</ref>, our baselines can achieve considerable performance on top of the unified framework with the same evaluation stage which will be introduced in the following.</p><p>Compared with F 2 GNN, we provide the SNAG and SANE baselines. As shown in Figure <ref type="figure" target="#fig_10">8</ref>, they focus on designing the aggregation operations in each layer, connections and layer aggregations in the output block. The search spaces are shown in Table <ref type="table" target="#tab_7">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation details</head><p>All models are implemented with Pytorch <ref type="bibr" target="#b32">[33]</ref> on a GPU 2080Ti (Memory: 12GB, Cuda version: 10.2). Thus, for consistent comparisons of baseline models, we use the implementation of all GNN baselines by the popular GNN library: Pytorch Geometric (PyG) (version 1.6.1) <ref type="bibr" target="#b12">[13]</ref>  <ref type="foot" target="#foot_3">4</ref> .</p><p>For SNAG <ref type="bibr" target="#b57">[58]</ref> 5 , an RL based method to design the aggregation operations in each layer, connections and layer aggregations in the output block, we use the 4-layer backbone to make a fair comparison with the proposed methods. In the search process, we set the search Table <ref type="table">7</ref>: Performance comparisons of the baselines used in our experiment and used in PyG. We report the average test accuracy and the standard deviation with 10 splits. "L2" and "L4" mean the number of layers of the base GNN architecture, respectively. "OOM" means out of memory.      For SANE <ref type="bibr" target="#b58">[59]</ref> <ref type="foot" target="#foot_5">6</ref> , a differentiable method to design aggregation operations in each layer, connections and layer aggregations in the output block, the 4-layer backbone is utilized. In the search process, we set the search epoch to 30. One candidate GNN can be obtained after the search process. Repeat 5 times with different seeds, we can get 5 candidates.</p><p>For each random baseline in Table <ref type="table">3</ref>, we randomly sample 100 GNNs from the designed search space and train these methods from scratch. 1 candidate GNN is derived based on the validation accuracy.</p><p>For our method, we set the search epoch to 400 and ? to 0.001. One candidate GNN can be obtained after the search process. Repeat 5 times with different seeds, we can get 5 candidates.</p><p>The searched GNNs and all human-designed baselines are finetuned individually with the hyperparameters shown in Table <ref type="table" target="#tab_9">8</ref>. Each method owns 30 hyper steps. In each hyper step, a set of hyperparameters will be sampled from Table <ref type="table" target="#tab_9">8</ref> based on Hyperopt<ref type="foot" target="#foot_6">7</ref> , and then generate final performance on the split data. We choose the hyperparameters for each candidate with the validation accuracy, and then select the candidate for SNAG, SANE and the proposed method with the validation accuracy.</p><p>After that, we report the final test accuracy and the standard deviations based on 10 repeat runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Searched Topologies</head><p>For sake of space, we only show the searched topologies of F 2 SAGE and F 2 GAT. The results of F 2 GNN are shown in Figure <ref type="figure" target="#fig_12">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Alleviating the over-smoothing problem</head><p>For sake of space, we only provide 4 baselines in Figure <ref type="figure" target="#fig_6">5</ref>, and the complete results are shown in Figure <ref type="figure" target="#fig_13">10</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The illustration of two topology design manners. For simplicity, the aggregation operation is denoted by the rectangle. The fusion operation ?, ? and represent the summation, maximum and concatenation, respectively.</figDesc><graphic url="image-1.png" coords="1,334.85,198.62,204.22,86.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The proposed framework consists of an ordered sequence of one input block, ? SFA blocks (four as an example here) and one output block. (b) For the ?-th SFA block, we have ? selection operations ? ? and one fusion operation ? ? to utilize the features generated by ? predecessors. Then one aggregation operation ? ? is followed to aggregated messages from the neighborhood.</figDesc><graphic url="image-2.png" coords="3,328.85,83.69,216.22,147.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The formulation and illustrations of the closely related methods. ? denotes the summation operation and denotes the concatenation operation.</figDesc><graphic url="image-3.png" coords="4,213.95,83.69,335.43,83.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>?=1 ? ? ? ? (?), where ? ? ? (0, 1) is the weight of ?-th candidate operation ? ? (?). In general, the operation weight ? ? is generated by one relaxation function ? ? = exp(? ? ) |O| ?=1 exp(? ? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(d); (d) the topology of 1-layer PNA and MixHop are shown in Figure 2(e) and Figure 2(f), respectively. The PNA and MixHop baselines in our experiment are constructed by stacking two layers. (e) the topology designs are constructed by selecting operations from the search space randomly, which is denoted as Random in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The searched topologies on Cora, Physics and Actor datasets with two human-designed aggregation operations: GraphSAGE and GAT. The index of each block in the framework is annotated and the dark blocks indicate they are not used in the searched GNNs. demonstrates the effectiveness and the versatility of our method. More searched topologies are shown in Appendix B.3.</figDesc><graphic url="image-4.png" coords="7,70.70,83.68,204.21,290.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparisons of test accuracy and the MAD value on the Cora dataset. The predefined aggregation operation is the GraphSAGE. "L4" represents the 4 layer baseline (? = 4 in our method), and so on. Darker colors mean larger values.</figDesc><graphic url="image-5.png" coords="7,334.85,83.69,204.21,84.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: We visualize the usage of different levels of features in F 2 SAGE on the Cora dataset. ? is the number of SFA blocks. The cell (??, ? ?) = 1 represents the features of level ? are selected in Block ?.Table 4: Performance comparisons of different fusion operations. The best results are highlighted and the second are underlined.</figDesc><graphic url="image-6.png" coords="8,64.69,83.69,216.22,180.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance comparisons of the different SFA block numbers on F 2 SAGE. Different colors represent different block numbers.</figDesc><graphic url="image-7.png" coords="8,328.85,170.63,216.22,89.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 F</head><label>1</label><figDesc>2 GNN -Feature Fusion GNN Require: Input graph G = ( V, E), the adjacency matrix A and the feature matrix H ????? . Training set D ????? and validation set D ????? , the epoch ? for search. Ensure: The searched architecture. 1: Random initialize the parameters ? and W. 2: for ? = 1 to ? do 3: Calculate the operation weights C on top of ? . 4: H 0 = MLP(H ????? ) 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The 4-layer GNN backbone used in SANE and SNAG. The "AGG" rectangle represents the aggregation operation in each layer. The dashed lines represent the learnable connections between the intermediate layers and the layer aggregation. The "Layer aggregation" rectangle represents the fusion operation used to integrate these selected features.</figDesc><graphic url="image-8.png" coords="12,402.04,83.69,72.08,78.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>) 35.19(0.62) 81.57(2.51) 51.75(0.59) GAT GNNII (L4) 85.40(1.06) 83.83(0.33) 88.44(0.25) 91.91(0.11) 96.14(0.15) 30.29(0.78) 55.29(6.25) 53.03(0.29) PNA (L2) 85.06(0.72) 83.46(0.47) 86.81(0.47) 90.84(0.24) 95.85(0.18) 28.14(1.99) 47.65(5.12) 54.02(0.33) MixHop (L2) 85.38(1.04) 82.50(0.34) 88.91(0.19) 91.27(0.37) 96.46(0.21) 35.70(0.90) 81.57(4.40) 53.67(0.30) PyG GCNII (L4) 83.27(1.14) 82.78(0.56) 89.03(0.22) 42.08(3.07) 96.08(0.07) 32.16(2.83) 67.25(7.89) 42.91(0.82) PNA (L2) 86.99(0.57) 84.05(0.18) 88.99(0.15) 91.61(0.11) 96.60(0.11) 33.43(1.35) 68.24(6.00) 52.18(0.17) MixHop (L2) 86.51(0.55) 82.24(0.50) 88.58(0.38) 90.57(0.33) 96.53(0.14) 36.89(0.77) 78.04(4.19) OOM (a) F 2 GNN on Cora (b) F 2 GNN on Phisics (c) F 2 GNN on Actor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The searched topologies on Cora, Physics and Actor datasets with F 2 GNN.</figDesc><graphic url="image-12.png" coords="13,100.73,502.47,144.14,135.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparisons of test accuracy and the MAD value on the Cora dataset. Predefined aggregation operations are the GraphSAGE and GAT. "L4" represents the 4 layer baseline (? = 4 in our method), and so on. Darker colors mean larger values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>epoch to 500. In each epoch, we sample 10 architectures and use the validation accuracy to update the controller parameters. After training finished, we sample 5 candidates with the controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>In this paper, we provide a novel feature fusion perspective in designing the GNN topology and propose a novel framework to unify the existing topology designs with feature selection and fusion strategies. It transforms the GNN topology design into the design of this two strategies.? To obtain the adaptive topology design, we develop a NAS method on top of the unified framework containing a novel search space and an improved differentiable search algorithm. ? Extensive experiments on eight real-world datasets demonstrate that the proposed F 2 GNN can improve model capacity (performance) while alleviating the deficiencies, especially alleviating the over-smoothing problem. Notations. We represent a graph as G = (V, E),where V and E represent the node and edge sets. A ? R |V |?|V | is the adjacency matrix of this graph where |V | is the node number. The class of node ? is represented as ? ? . In the proposed framework, the output features of block ? are denoted as H ? , and in other methods, H ? represents the output features of layer ?. H ?????? represents the output of a GNN.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The operations used in our search space.</figDesc><table><row><cell></cell><cell>Operations</cell></row><row><cell>Selection O ?</cell><cell>ZERO, IDENTITY</cell></row><row><cell>Fusion O ?</cell><cell>SUM, MEAN, MAX, CONCAT, LSTM, ATT</cell></row><row><cell>Aggregation O ?</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the eight datasets in our experiments.</figDesc><table><row><cell>Datasets</cell><cell cols="4">#Nodes #Edges #Features #Classes</cell></row><row><cell>Cora [39]</cell><cell>2,708</cell><cell>5,278</cell><cell>1,433</cell><cell>7</cell></row><row><cell cols="3">Computers [31] 13,381 245,778</cell><cell>767</cell><cell>10</cell></row><row><cell>DBLP [2]</cell><cell cols="2">17,716 105,734</cell><cell>1,639</cell><cell>4</cell></row><row><cell>PubMed [39]</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell></row><row><cell>Physics [40]</cell><cell cols="2">34,493 495,924</cell><cell>8,415</cell><cell>5</cell></row><row><cell>Wisconsin [34]</cell><cell>251</cell><cell>466</cell><cell>1,703</cell><cell>5</cell></row><row><cell>Actor [34]</cell><cell>7,600</cell><cell>30,019</cell><cell>932</cell><cell>5</cell></row><row><cell>Flickr [54]</cell><cell cols="2">89,250 899,756</cell><cell>500</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparisons of different fusion operations. The best results are highlighted and the second are underlined.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>PubMed</cell><cell>Physics</cell></row><row><cell>F</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>2 SAGE-SUM 84.73(0.63) 89.39(0.21) 96.44(0.01) F 2 SAGE-MEAN 84.30(0.61) 89.58(0.22) 96.42(0.03) F 2 SAGE-CONCAT 86.07(0.45) 89.31(0.19) 96.69(0.01) F 2 SAGE 87.72(0.26) 89.73(0.26) 96.72(0.01)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of the validation accuracy in the supernet and the childnet on the Cora dataset.</figDesc><table><row><cell>Temperature ?</cell><cell cols="4">F 2 SAGE Supernet Childnet Supernet Childnet F 2 AGG</cell></row><row><cell>1</cell><cell>80.33</cell><cell>6.68</cell><cell>86.83</cell><cell>85.71</cell></row><row><cell>0.1</cell><cell>73.65</cell><cell>10.96</cell><cell>84.23</cell><cell>83.86</cell></row><row><cell>0.01</cell><cell>70.13</cell><cell>70.13</cell><cell>84.60</cell><cell>84.60</cell></row><row><cell>0.001</cell><cell>80.15</cell><cell>80.15</cell><cell>86.83</cell><cell>86.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>? = ? ? ? (H ? , A) //Aggregation 15:Calculate the training loss L ????? and update W.16:Calculate the operation weights C on top of ? . 17:Calculate the validation loss L ????? and update ? . 18: Preserve the operation with the largest weight in each mixed operation. 19: return The searched architecture.</figDesc><table><row><cell></cell><cell cols="2">1 do</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell>? ? (H ? ) = ? ?</cell><cell>|O? | ?=1 ?</cell><cell>? ? ? ?</cell><cell>? ? ? (H ? ) = ?</cell><cell>? ? 2 H ? //Selection</cell></row><row><cell>9:</cell><cell cols="4">H ? = H ? ? {? ? (H ? ) } ? ?</cell><cell></cell></row><row><cell>10:</cell><cell>H ? = ? ? ? (H ? ) =</cell><cell cols="4">|O ? | ?=1 ? ? ? ? ? ? (H ? ) //Fusion</cell></row><row><cell>11:</cell><cell>if ? = ? + 1 then</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>12:</cell><cell cols="4">H ?????? = MLP(H ? )</cell><cell></cell></row><row><cell>13:</cell><cell>else</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>14:</cell><cell>H</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The search spaces in SANE and SNAG.</figDesc><table><row><cell></cell><cell>SANE</cell><cell>SNAG</cell></row><row><cell>Aggregation</cell><cell>SAGE-SUM, SAGE-MEAN, SAGE-MAX, GCN, GAT, GAT-SYM, GAT-COS, GAT-LINEAR, GAT-GEN-LINEAR, GIN, GeniePath</cell><cell>GCN, SAGE-SUM, SAGE-MEAN, SAGE-MAX, MLP, GAT, GAT-SYM, GAT-COS, GAT-LINEAR, GAT-GEN-LINEAR</cell></row><row><cell>Skip-connection</cell><cell>ZERO, IDENTITY</cell><cell>ZERO, IDENTITY</cell></row><row><cell>Layer aggregation</cell><cell>CONCAT, MAX, LSTM</cell><cell>CONCAT, MAX, LSTM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>(0.42) 84.46(0.45) 89.21(0.24) 91.38(0.27) 96.45(0.15) 35.70(1.11) 81.57(4.13) 52.24(0.29) PNA (L2) 84.29(0.67) 82.76(0.42) 89.25(0.26) 90.67(0.42) 96.32(0.10) 33.89(2.68) 75.29(6.46) 52.09(0.73) MixHop (L2) 84.81(0.95) 82.65(0.65) 89.25(0.28) 88.56(1.61) 96.11(0.17</figDesc><table><row><cell>Topology</cell><cell>Cora</cell><cell>DBLP</cell><cell>PubMed</cell><cell>Computer</cell><cell>Physics</cell><cell>Actor</cell><cell>Wisconsin</cell><cell>Flickr</cell></row><row><cell>GNNII (L4)</cell><cell>85.83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters we used in this paper.</figDesc><table><row><cell>hyperparameter</cell><cell>Operation</cell></row><row><cell>Embedding size</cell><cell>16, 32, 64, 128, 256, 512</cell></row><row><cell>Learning rate</cell><cell>[0.001, 0.01]</cell></row><row><cell>Dropout rate</cell><cell>0, 0.1, 0.2,? ? ? ,0.9</cell></row><row><cell>Weight decay</cell><cell>[0.0001, 0.001]</cell></row><row><cell>Optimizer</cell><cell>Adam, AdaGrad</cell></row><row><cell cols="2">Activation function Relu, ELU</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Lanning is a research intern in 4Paradigm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Since aggregation function is the key component regarding different GNN models<ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b58">59]</ref>, we omit other parameters in each GNN layer for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/pyg-team/pytorch_geometric/tree/master/examples</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/pyg-team/pytorch_geometric</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/AutoML-Research/SNAG</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/AutoML-Research/SNAE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/hyperopt/hyperopt</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking Graph Neural Network Search from Messagepassing</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GraphPAS: Parallel Architecture Search for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moctard</forename><surname>Babatounde Oloulade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2182" to="2186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive darts: Bridging the optimization gap for nas in the wild</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="638" to="655" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adanet: Adaptive structural learning of artificial neural networks</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Just jump: Dynamic neighborhood aggregation in graph neural networks</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04849</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">InceptionGCN: receptive field aware graph convolutional network for disease prediction</title>
		<author>
			<persName><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayan</forename><surname>Shekarforoush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Burwinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerome</forename><surname>Vivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kort?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<editor>IPMI. Springer</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="73" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lurong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04064</idno>
		<title level="m">Enhance Information Propagation for Graph Neural Network by Heterogeneous Aggregations</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><surname>Ghanem</surname></persName>
		</author>
		<idno>CVPR. 1620-1630</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">AutoGraph: Automated Graph Neural Network</title>
		<author>
			<persName><forename type="first">Yaoman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">One-shot Graph Ne&apos;ural Architecture Search with Dynamic Search Space</title>
		<author>
			<persName><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zean</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Asap: Architecture search, anneal and prune</title>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihi</forename><surname>Zelnik</surname></persName>
		</author>
		<editor>AISTATS. PMLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="493" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 8026-8037</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Leon Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Kurakin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evolutionary architecture search for graph neural networks</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10199</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Don&apos;t stack layers in graph neural networks</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Magli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>wire them randomly</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Graph attention networks. ICLR</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quanming Yao, and Zhiqiang He. 2021. Pooling architecture search for graph classification</title>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Weight-sharing neural architecture search: A battle to shrink the optimization gap</title>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengsu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding the wiring evolution in differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>AISTATS. 874-882</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1284" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICML. 5453-5462</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph structure of neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10881" to="10891" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Saining Xie</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fakedetector: Effective fake news detection with deep diffusive neural network</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1826" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">AutoSF: Searching Scoring Functions for Knowledge Graph Embedding</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00742</idno>
		<title level="m">Automated Machine Learning on Graphs: A Survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Simplifying Architecture Search for Graph Neural Network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICDE</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
