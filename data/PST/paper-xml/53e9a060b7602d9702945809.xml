<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Carving Differential Unit Test Cases from System Test Cases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Elbaum</surname></persName>
							<email>elbaum@cse.unl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nebraska Lincoln</orgName>
								<address>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><forename type="middle">Nee</forename><surname>Chin</surname></persName>
							<email>hchin@cse.unl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nebraska Lincoln</orgName>
								<address>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Dwyer</surname></persName>
							<email>dwyer@cse.unl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nebraska Lincoln</orgName>
								<address>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Dokulil</surname></persName>
							<email>jdokulil@cse.unl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Nebraska Lincoln</orgName>
								<address>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Carving Differential Unit Test Cases from System Test Cases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1C3BF96CF60116148266CA4ED9A99345</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.5 [Software Engineering]: Testing and Debugging Reliability</term>
					<term>Experimentation</term>
					<term>Verification Automated test generation</term>
					<term>carving and replay</term>
					<term>regression testing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unit test cases are focused and efficient. System tests are effective at exercising complex usage patterns. Differential unit tests (DUT) are a hybrid of unit and system tests. They are generated by carving the system components, while executing a system test case, that influence the behavior of the target unit, and then re-assembling those components so that the unit can be exercised as it was by the system test. We conjecture that DUTs retain some of the advantages of unit tests, can be automatically and inexpensively generated, and have the potential for revealing faults related to intricate system executions. In this paper we present a framework for automatically carving and replaying DUTs that accounts for a wide-variety of strategies, we implement an instance of the framework with several techniques to mitigate test cost and enhance flexibility, and we empirically assess the efficacy of carving and replaying DUTs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Software engineers develop unit test cases to validate individual program units (e.g., methods, classes, packages) before they are integrated into the whole system. By focusing on an isolated unit, unit tests are not constrained by other parts of the system in exercising the target unit. This smaller scope for testing usually results in significantly more efficient test execution and fault isolation relative to whole system testing and debugging <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b18">18]</ref>. Unit test cases are also used as a component of several popular development methods, such as extreme programming (XP) <ref type="bibr" target="#b2">[2]</ref>, test driven development (TDD) practices <ref type="bibr" target="#b3">[3]</ref>, continuous testing <ref type="bibr" target="#b35">[35]</ref>, and efficient test prioritization and selection techniques <ref type="bibr" target="#b31">[31]</ref>.</p><p>Developing effective suites of unit test cases presents a number of challenges. Specifications of unit behavior are usually informal and are often incomplete or ambiguous, leading to the development of overly general or incorrect unit tests. Furthermore, such specifications may evolve independently of implementations requiring additional maintenance of unit tests even if implementations remain unchanged. Testers may find it difficult to imagine sets of unit input values that exercise the full-range of unit behavior and thereby fail to exercise the different ways in which the unit will be used as a part of a system. An alternative approach to unit test development, that does not rely on specifications, is based on the analysis of a unit's implementation. Testers developing unit tests in this way may focus, for example, on achieving a coverage-adequacy criteria of the target unit's code. Such tests, however, are inherently susceptible to errors of omission with respect to specified unit behavior and may thereby miss certain faults. Finally, unit testing requires the development of test harnesses or the setup of a testing framework (e.g., junit <ref type="bibr" target="#b17">[17]</ref>) to make the units executable in isolation.</p><p>System tests are usually developed based on documents that are commonly available for most software systems that describe the system's functionality from the user's perspective, for example, requirement documents and user's manuals. This makes system tests appropriate for determining the readiness of a system for release, or to grant or refuse acceptance by customers. Additional benefits accrue from testing system-level behaviors directly. First, system tests can be developed without an intimate knowledge of the system internals, which reduces the level of expertise required by test developers and which makes tests less-sensitive to implementationlevel changes that are behavior preserving. Second, system tests may expose faults that unit tests do not, for example, faults that emerge only when multiple units are integrated and jointly utilized. Finally, since they involve executing the entire system no test harnesses need be constructed.</p><p>While system tests are an essential component of all practical software validation methods, they do have several disadvantages. They can be expensive to execute; for large systems, days or weeks, and considerable human effort may be needed for running a thorough suite of system tests <ref type="bibr" target="#b23">[23]</ref>. In addition, even very thorough system testing may fail to exercise the full-range of behavior implemented by system's units; thus, system testing cannot be viewed as an effective replacement for unit testing. Finally, fault isolation and repair during system testing can be significantly more expensive than during unit testing.</p><p>The preceding characterization of unit and system tests, although not comprehensive, illustrates that system and unit tests have complementary strengths and that they offer a rich set of tradeoffs. In this paper, we present a general framework for carving and replaying of what we call differential unit tests (DUT) which aim at exploiting those tradeoffs. We termed them differential because their primary function is detecting differences between multiple versions of a unit's implementation. DUTs are meant to be focused and efficient, like traditional unit tests, yet they are automatically generated along with a custom test-harness, making them inexpensive to develop and easy to evolve. In addition, since they indirectly capture the notion of correctness encoded in the system tests from which they are carved, they have the potential for revealing faults related to complex patterns of unit usage.</p><p>In our approach, DUTs are created from system tests by capturing components of the exercised system that influence the behavior of the targeted unit, and that reflect the results of executing the unit; we term this carving. Those components are automatically assembled into a test harness that establishes the pre-state of the unit that was encountered during system test execution. From that state, the unit is replayed and the resulting state is queried to determine if there are differences with the recorded unit post-state.</p><p>Ideally DUTs will (a) retain the fault detection effectiveness of system tests on the target unit, (b) only report small numbers of differences that are not indicative of differing system test results, (c) be executed faster than system tests, and (d) be applicable across multiple system versions. We empirically investigate DUT carving and replay techniques with respect to these criteria through a controlled study within the context of regression testing where we compare the performance of system tests and carved unit tests. The results indicate that carved test cases can be as effective as system test cases in terms of fault detection, but much more efficient.</p><p>When compared against emerging work on providing automated extraction of powerful unit tests from system executions, <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref>, the contributions of this paper are: (i) a framework for automatically carving and replaying DUTs that accounts for a widevariety of implementation strategies with different tradeoffs; (ii) a new state-based strategy for carving and replay at a method level that offers a range of costs, flexibility, and scalability; and (iii) an evaluation criteria and an empirical assessment of the efficiency and effectiveness of carving and replay of DUTs on multiple versions of a Java application. We believe these contributions lay a solid and general foundation for further study of carving and replay of DUTs and we outline several directions for future work in Section 6. In the next Section, we present our framework for carving and replay testing. Section 3 details the implementation of one of those instantiations. Section 4 describes our study and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A FRAMEWORK FOR TEST CARVING AND REPLAY</head><p>Java programs can have millions of allocated heap instances <ref type="bibr" target="#b14">[14]</ref> and hundreds of thousands of live instances at any time. Consequently, carving the raw state of real programs is impractical. We believe that cost-effective carving and replay (CR) based testing will require the application of multiple strategies that select information in raw program states and use that information to trade a measure of effectiveness to achieve practical cost. Strategies might include, for example, carving a single representative of each equivalence class of program states or pruning information from a carved state that a method under test is guaranteed to not be dependent on. The space of possible strategies is vast and we believe that a general framework for CR testing will aid in exploring cost-effectiveness trade-offs possible in the space of CR testing techniques.</p><p>Regardless of how one develops, or generates, a unit test, there are four essential steps: (1) identify a program state from which to initiate testing, (2) establish that program state, (3) execute the unit from that state, and (4) judge the resulting state as to its correctness. In the rest of this Section, we define a general framework that allows different strategies to be applied in each of these steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Program States and Program Executions</head><p>For the purposes of explaining our framework, we consider a Java program to be a kind of state machine. At any point during the execution of a program the program state, S, can be defined, conceptually, as all of the values in memory. A program execution can be formalized either as a sequence of program states or as a sequence of program actions that cause state changes. A sequence of program states is written as σ = s0, s1, . . . where si ∈ S and s0 is the initial program state as defined by Java. A state si+1 is reached from si by executing a single action (e.g., bytecode). A sequence of program actions is written as σ. We denote the final state of an action sequence s(σ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Basic Carving and Replaying</head><p>Figure <ref type="figure">1</ref> illustrates the CR process. Given a system test case stx, carving a unit test case ctx m for target unit m during the execution of stx consists of capturing spre, the program state immediately before the first instruction of an activation of method m, and spost, the program state immediately after the final instruction of the activation of m has executed. The captured pair of states (spre, spost), defines a differential unit test case for a method, ctx m . States in this pair can be defined by capturing the appropriate states in σ, or through the cumulative effects of a sequence of program actions, by capturing s(σ) at the appropriate points in σ. A CR testing approach is said to be state-based if it records pairs (spre, spost) and action-based if it records pairs (σpre, spost) where spre = s(σpre).</p><p>In practice, it is common for a method, m, to undergo some modification, e.g., to m ′ , over the program lifetime. To efficiently validate the effects of a modification, we replay ctx m on m ′ . Replaying a differential unit test for a method m ′ requires the ability to either load state spre into memory or execute σpre depending on how the state was carved. From this state, execution of m ′ is initiated and it continues until it reaches the point corresponding to the carved spost. At that point, the current execution state, s ′ post , is compared to spost. If the resulting states are the same, we can attest that the change did not affect the behavior of the target unit. However, if the change altered the behavior of m, then further processing will be required to determine whether the alteration matches the developer's expectations.</p><p>There are multiple techniques for diagnosing the root cause of detected differences. For example, a difference could trigger the execution of system test stx to determine whether a difference manifests at higher levels of abstractions, the results of ctx m could be compared with the results of manually developed unit tests for m, or intermediate states within the execution of m and m ′ (e.g., after every statement) could be compared to identify the earliest point at which states differ. We discuss support for some of these diagnostics in Section 2.4 and leave the others for future work.</p><p>Several fundamental challenges must be addressed in order to make CR cost-effective. First, the proposed basic carving procedure is at best inefficient and likely impractical. Inefficient because a method may only depend on a small portion of the program state, thus storing the complete state is wasted effort. Furthermore, two distinct complete program states may be identical from the point of view of a given method, thus carving complete states would yield redundant unit tests. Impractical because storing the complete state of a program may be prohibitively expensive in terms of time and space. Second, changes to m may render ctx m unexecutable in m ′ . Reducing the cost of CR testing is important, but we must produce DUTs that are robust to changes so that they can be executed across a series of system versions, recovering the overhead of carving. Finally, the use of complete post-states to detect behavioral differences is not only inefficient but may also be too sensitive to behavior differences caused by reasons other than faults (e.g., fault fixes, improvements, internal refactoring) leading to the generation of brittle tests. The following sections address these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improving CR with Projections</head><p>We focus CR testing on a single method by defining projections on carved pre-states that preserve information related to the unit under test and provide significant reduction in pre-state size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">State-based Projections</head><p>A state projection function π : S → S preserves selected program state components. For example, a state projection function may preserve only the values of reference fields, thereby eliminating all scalar fields, which would maintain the heap shape of a program state. Many useful state projections are based on the notion of heap reachability. A reference r ′ is reachable in one dereference from r if the value of some field f of r holds r ′ ; let reach(r) = {r ′ | ∃ f ∈F v f ield (r, f ) = r ′ } where v f ield is the dereference function. References reachable through any chain of dereferences up to length k from r are defined by using the iterated composition of this binary relation, S 1≤i≤k reach i (r); as a notational convenience we will refer to this as reach k (r). The positive-transitive closure of the relation, reach + (r), defines the set of all reachable references from r in one or more dereferences.</p><p>State-based CR testing approaches should use projections that retain at most the interface reachable projection which is defined to preserve the set of heap objects reachable from a calling context, {r | ∃p∈P aramsreach + (p)}. This includes the local frame of the method, all reachable objects from parameters P arams to the method (including this), and all fields of those objects. Robustness to change under this projection is identical to that of the complete program pre-state since all data that the method could possibly reference is captured. It is possible to trade robustness for reduction in carving cost by defining projections that eliminate more state information. Section 3 presents two projections that exercise this trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Action-based Projections and Transformations</head><p>Projections on sequences of program actions, π : σ → σ. can be used to distill the portion of a program run that affects the pre-state of a unit method. Unfortunately, a purely action-based approach to state-capture will not work for all Java programs. For example, a program that calls native methods does not, in general, have access to the native methods instructions. To accommodate this, we can allow for transformation of actions during carving, i.e., re-  place one sequence of instructions with another. Transformation could be used, for example, to replace a call to a native method with an instruction sequence that implements the side-effects of the native method. More generally, one could design an instance of π that would replace any trace portion with a summarizing action sequence. Reduction aims at thinning a single carved test case by retaining only the projected pre-state (in Figure <ref type="figure" target="#fig_0">2</ref> for example the projection of spre carved from ctxm leads to a smaller spre). Reducing a DUT's pre-state results in smaller space requirements and, more importantly, in quicker replay since loading time is a function of the pre-state size. As we shall see, depending on the type of projection, these gains may be achieved at the expense of reduced fault detection power (e.g., a projection may discard an object that was necessary to expose the fault). Furthermore, test executability may be sacrificed as well. State-based projections may become unexecutable if the data structures used by the target unit changes, for example, shifting from an array to a heap-based structure, even if behavior is preserved. Action-based projections may become unexecutable if the behavior of a unit method changes so that a different number or sequence of methods is needed in the modified program to produce the desired pre-state. Still, reduction can be a valuable mechanism to improve the efficiency of CR by keeping just the portions of the pre-state that are most likely to be relevant to the targeted method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Applying Projections</head><p>Filtering aims at removing redundant DUTs from the suite. Consider a method that is invoked during the program initialization and is independent of the program parameters. Such method would be exercised by all system tests in the same way and result in multiple identical DUTs for that particular method. A simple filter would remove such duplicate tests, keeping just the unique DUTs. Now consider a simple accessor method with no parameters that just returns the value of a scalar field. If this method is invoked by the tests from different pre-states, then multiple DUTs will be carved, and a simple lossless filter will not discard any DUT even though they exercise similar behavior. In this case, applying a projection that preserves the pre-state components directly reachable from this would result in many DUTs that are redundant (in Figure <ref type="figure" target="#fig_0">2</ref>, π(spre) for ctxm and for ctzm are identical so one of them can be removed). Clearly, in some cases, this kind of lossy filtering may result in a lower fault detection capability since we may discard a DUT that is indeed different and hence potentially valuable. Note that, contrary to test case reduction, filtering only uses projections to judge test equivalence, consequently, test executability is preserved since the DUTs that are kept are complete. In practice, however, reduction and filtering are likely to be applied in tandem such that reduced tests are then filtered, or filtered tests are then reduced (without necessarily using the same projection for reduction and filtering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Adjusting Sensitivity through Differencing Functions</head><p>The basic CR testing approach described earlier compares a carved complete post-state to a post-state produced during replay to detect behavioral differences in a unit. The use of complete post-states is both inefficient and unnecessary for the same reasons as outlined above for pre-states. While we could use comparison of post-state projections to address these issues, we believe that there is a more flexible solution.</p><p>Method unit test are typically structured so that, after a sequence of method calls that establish a desired pre-state, the method under test is executed. When it returns, additional method calls and comparisons are executed to implement a pseudo-oracle. For example, unit tests for a red-black tree might execute a series of insert and delete calls and then query the tree-height and compare it to an expected result to judge partial correctness. We allow a similar kind of pseudo-oracle in CR testing by defining differencing functions on post-states that preserve selected information about the results of executing the unit under test. These differencing functions can take the form of post-state projections or can be more aggressive, capturing simple properties of post-states, such as tree height or size, and consequently may greatly reduce the size of post-states while preserving information that is important for detecting behavioral differences.</p><p>We define differencing functions that map states to a selected differencing domain, dif : S → D. Differencing in CR testing is achieved by evaluating dif(spost) = dif(s post ′ ). State projection functions are simply differencing functions where D = S. In addition to the reachability projections defined in the previous sub-section, projections on unit method return values, called return differencing, and on fields of the unit instance, this, called instance differencing, are useful since they correspond to techniques used widely in hand-built unit tests.</p><p>A central issue in differential testing is the degree to which differencing functions are able to detect changes that correspond to faults while masking implementation changes. We refer to this as the sensitivity of a differencing function. Clearly, comparing complete post-states will be highly-sensitive, detecting both faults and implementation changes. A projection function that only records the return value of the method under test will be insensitive to implementation changes while preserving some fault-sensitivity. Note also that these differencing functions provide different incomplete views on program state. Their incompleteness reduces cost and it may add some level of insensitivity to changes in the implementation, but it could also reduce their fault detection effectiveness.</p><p>We address this by allowing for multiple differencing functions to be applied in CR testing which has the potential to increase faultsensitivity, without necessarily increasing implementation changesensitivity. For example, using a pair of return and instance differencing functions allows one to detect faults in both instance field updates and method results, but will not expose differences related to deeper structural changes in the heap. Fault isolation efficiency could also be enhanced by the availability of multiple differencing functions, since each could focus on a specific property or set of program state components that which will help developers restrict their attention on a potentially small portion of program state that may reflect the fault. There is another differencing dimension that can improve fault isolation. It consists of generalizing the definition of DUTs to capture a sequence of post-states, (spre, σpost), that capture intermediate points during the execution of the method under test. Figure <ref type="figure" target="#fig_2">3</ref> illustrates a scenario in which a generalized DUT begins execution of m at spre. Conceptually, during replay a sequence of post-states is differenced with corresponding states at intermediate states of the method under test. For example, at point 1, the test compares the current state to the captured spost1, similarly at points 2 and 3 the pre and post-states of the call out of the unit are compared. Using a sequence of post-states requires that a correspondence be defined between locations in m and m ′ . Correspondences could be defined using a variety of approaches, for example, one could use the calls out of m and m ′ to define points for post-state comparison (as is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>) or common points in the text of m and m ′ could be detected via textual differencing. Fault isolation is enhanced using multiple post-states, since if the first detected difference is at location i then that difference was introduced in the region of execution between location i -1 and i. Of course, storing multiple post-states may be expensive so we advocate the use of σpost to narrow the scope of code that must be considered for fault isolation once a behavioral difference is attributed to a fault.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">INSTANTIATING THE FRAMEWORK</head><p>In this section we describe the architecture and some implementation details of a state-based instantiation of the framework. (Section 5 discusses existing carving and replay implementations which are exclusively action-based).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Architecture</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the architecture of the CR tools, with the shaded rectangles being the primary components. The carving activity starts with the Carver class which takes four inputs: the program name, the target method m within the program, the system test case stx inputs, and options to bound the carving process.</p><p>Carver utilizes a custom class loader CustomLoader (that employs BCEL <ref type="bibr" target="#b13">[13]</ref>) to incorporate into the program: a singleton Con-textFactory class configured to store pre and post states, and invocations of the ContextFactory at the entry and exit(s) of m. Then, every execution of m will cause two invocations of ContextFactory: one to store spre and one to store spost. ContextFactory utilizes the ContextBounding class to assist with the determination of what part of the state should be stored when test case reduction is utilized. By default, ContextBounding performs the most conservative projection: an interface reachability projection (as described in Section 2.3). More restrictive projections can be performed through the BoundingAnalysis class; we have implemented two such projections and describe them in the next section. Finally, the open source package XStream, described in more detail in the next section, performs state serialization and temporary storage. Finally, the data can be compressed with the off-the shelf compression utility bzip. The Replay component shares many of the classes with Carver. As in Carver, Replay instruments the class of the target unit, in this case m ′ , and utilizes the ContextFactory, but only to store spost. The ContextLoader class obtains and loads spre, using XStream to unmarshall the stored program state, and then invokes the target unit for execution.</p><p>Two set of scripts, represented with double-side rectangles in Figure <ref type="figure" target="#fig_3">4</ref>, are utilized to provide the filtering and differencing mechanisms. While a test suite is being generated, only the DUTs that capture a unique spre (not captured by others DUTs), and that can be replayed successfully in the same version where they were carved, are retained. Once a test suite of DUTs is generated, test case filtering can be performed to remove redundant test cases based on the same set of projections available through BoundingAnalysis. Dif scripts compare two spost according to a specified differencing function to determine whether the changes from m to m ′ generate a behavioral difference. Currently, differencing functions on return values, on instance fields, on full program state (the default) are fully automated. To facilitate experimentation with different Dif functions our tools currently store the full spost, but we plan to implement options to store only dif(spost) which has the potential to significantly reduce the cost of carving, replay and differencing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interesting Implementation Aspects</head><p>In this section we briefly describe the most interesting aspects of the implementation.</p><p>Limitations of the java.io.Serializable interface. Our approach requires the ability to save and restore object data representing the program state. However, the Java java.io.Serializable interface limits the type of objects that can be serialized. For example, Java designates file handler objects as transient (non-serializable) because it reasonably assumes that a handler's value is unlikely to be persistent, and restoring it could enable illegal accesses. The same limitations apply to other objects, such as database connections and network streams. In addition, the Java serialization interface may impose additional constraints on serialization. For example, it may not serialize classes, methods, or fields declared as private or final in order to avoid potential security threats.</p><p>Fortunately, we are not the first to face these challenges. We found multiple serialization libraries that offer more advanced and flexible serialization capabilities with various degrees of customization. We ended up choosing the XStream library <ref type="bibr" target="#b39">[39]</ref> because it comes bundled with many converters for non-serializable types and a default converter that uses reflection to automatically capture all object fields, it serializes to XML which is more compact and easier to read than native Java serialization, and it has built-in mechanisms to traverse and manage the storage of the heap which was essential in implementing the following projections. Note that, even with the assistance of powerful auxiliary libraries such as XStream, the object representations we capture are often simplifications of the real object states that may be observed during an execution. Still, as we shall see, these simplifications make DUTs affordable while retaining much of the power of system tests.</p><p>Interface k-bounded reachable projection. The interface k-bounded reachable projection defines the set of preserved references to include only those reachable via reference chains of length k, i.e., {r | ∃p∈P aramsreach k (p)}. Using small values of k can greatly reduce the size of the recorded pre-state and for many methods it will have no impact on unit-test robustness. For example, a value of 1 would suffice for a method whose only dereferences are accesses to fields of this. In the implementation, when traversing the program using Xstream to store the program state, we keep track of the length of dereference chains to halt traversal when k is reached.</p><p>If the unit accesses data along a reference chain of length greater than k, then a k-bounded projection will retain insufficient data about the pre-state to allow replay. Our implementation dynamically detects this situation and issues a SentinelAccessException to distinguish replay failure from an application exception. This is achieved by extending Xstream with a custom converter that automatically transforms objects that lie at a depth of k + 1 to contain an additional boolean field that marks it as a sentinel instance. The unit under test is then instrumented to insert a test of this boolean field and raise the exception if true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>May-reference reachable projection.</head><p>The may-reference reachable projection uses a static analysis that calculates a characterization of the heap instances that may be referenced by a method activation either directly or through method calls. This characterization is expressed as a set of regular expression of the form: pf1 . . . fn(F + )? This captures an access path that is rooted at a parameter p and consists of n dereferences by the named fields fi. If the analysis calculates that the method may reference an object through a dereference chain of length greater than n, the optional final term is included to capture objects that are reachable from the end of the chain through dereference of fields in the set F . Let reachF (r) = {r ′ | ∃ f ∈F v f ield (r, f ) = r ′ } capture reachability restricted to a set of fields F ; reach f denotes reachability for the singleton set f . For a regular expression of the form pf1 . . . fm, where m ≤ n, we construct the set: reach f 1 (p) ∪ . . . ∪ reach fm (. . . (reach f 1 (p))), since we want to capture all references touched along the path. If the regular expression ends with the term F + then we union an additional term of the form reach + F (reach fm (. . . (reach f 1 (p)))). We conjecture that this projection can significantly reduce the size of carved pre-states while retaining arbitrarily large heap structures that are relevant to the method under test.</p><p>We implemented a k-bounded access path based may-reference analysis that used the flow-insensitive context-sensitive equivalenceclass based read-write analysis implemented in Indus <ref type="bibr" target="#b27">[27]</ref>. This analysis partitions parameter and variable names into equivalence classes. The two distinct features of the analysis are: 1) for each equivalence class, an abstract heap structure based on the names involved in read/write access is maintained, and 2) distinct equivalence classes are maintained for each method scope except in the case of static fields and variable names occurring in methods involved in recursive call chains. We generate regular expressions that capture the set of all possible referenced access paths up to a given fixed length, k, with a default of k = 2. When traversing the program using Xstream, we simultaneously keep track of all regular expressions and mark only those objects that lie on a defined access path for storage in XML. This analysis is also capable of detecting when a method is side-effect free and in such cases the storage of post-states is skipped since method return values completely define the effect of such method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EMPIRICAL STUDY</head><p>The goal of the study is to assess execution efficiency, fault detection effectiveness, and robustness of DUTs. We will perform such assessment through the comparison of system tests and their corresponding carved unit test cases in the context of regression testing. Within this context, we are interested in the following questions: RQ1: Can carving techniques save regression testing costs? We would like to compare the cost of reusing carved unit test cases versus the costs of utilizing regression test selection techniques that work on system test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2:</head><p>What is the fault detection effectiveness of the carved test cases? This is important because saving testing costs while reducing fault detection is rarely an enticing trade-off.</p><p>RQ3: How robust are the carved tests in the presence of software evolution? We would like to assess the reusability of the carved unit test cases under a real evolving system, and examine how different types of change can affect the carved tests robustness and sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Testing Techniques</head><p>Let P be a program, let P ′ be a modified version of P , and let T be a test suite developed initially for P . Regression testing seeks to test P ′ . To facilitate regression testing, test engineers may re-use T to the extent possible. In this study we considered four types of test regression techniques, two that work with system tests (S) and two that worked with carved tests (C):</p><p>S-retest-All. When P is modified, creating P ′ , we simply reuse all non-obsolete test cases in T to test P ′ ; this is known as the retest-all technique <ref type="bibr" target="#b21">[21]</ref>. It is widely used in industry <ref type="bibr" target="#b23">[23]</ref> and it is often used as a control technique in regression testing experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-selection.</head><p>The retest all technique can be expensive: rerunning all test cases may require an unacceptable amount of time or human effort. Regression test selection techniques <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b32">32]</ref> use information about P , P ′ , and T to select a subset of T , T ′ , with which to test P ′ . We utilize the modified entity technique <ref type="bibr" target="#b10">[10]</ref>, which selects test cases that exercise methods, in P , that (1) have been deleted or changed in producing P ′ , or (2) use variables or structures that have been deleted or changed in producing P ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-selection-k. Similar in concept to S-selection, this technique</head><p>executes all DUTs, carved with a k-bounded reachable projection, that exercise methods that were changed in P ′ . This technique follows the conjecture that deeper references are less likely to be required for replay, so bounding the carving depth may improve the CR efficiency while maintaining a DUT's strengths. Within this technique we explore depth bounding levels of 1, 5, and ∞ (unlimited depth which corresponds to the interface reachable projection.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-selection-mayref. Similar to C-selection-k except that it carves</head><p>DUTs utilizing a may-reference reachable projection. This technique is based on the notion that program changes are more likely to affect the parts of the heap reachable by the method under test or by the methods invoked by the method under test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Measures</head><p>Regression test selection techniques achieve savings by reducing the number of test cases that need to be executed on P ′ , thereby reducing the effort required to retest P ′ . We conjecture that CR techniques achieve additional savings by focusing on units of P ′ . To evaluate these effects, we measure the time to execute and the time to check the outputs of the test cases in the original test suite, the selected test suite, and the carved selected test suites. For a carved test suite we also measure the time and space to carve the original DUT test suite.</p><p>One potential cost of regression test selection is the cost of missing faults that would have been exposed by the system tests prior to test selection. Similarly, DUTs may miss faults due to the use of projections aimed at improving carving efficiency. We will measure fault detection effectiveness by computing the percentage of faults found by each test suite. We will also qualify our findings by analyzing instances where the outcomes of a carved test case is different from its corresponding system test case.</p><p>To evaluate the robustness of the carved test cases in the presence of program changes, we are interested in considering three potential outcomes of replaying a ctx m on unit m ′ : 1) fault is detected, ctx m causes m ′ to reveal a behavioral differences due to a fault; 2) false difference is detected, ctx m causes m ′ to reveal a behavioral change from m to m ′ that is not a fault (not captured by stx); and test is unexecutable, ctx m is ill-formed with respect to m ′ . Tests may be ill-formed for a variety of reasons, e.g., object protocol changes, internal structure of object changes, invariants change, and we refer to the degree to which a test set becomes ill-formed under a change its sensitivity to change. We assess robustness by computing the percentage of carved tests and program units falling into each one of the outcomes. Since the robustness of a test case depends on the change, we qualify robustness by analyzing the relationship between the type of change and sensitivity of the DUTs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Artifact</head><p>The artifact we will use to perform this experiment study is Siena <ref type="bibr" target="#b9">[9]</ref>. Siena is an event notification middleware implemented in Java. This artifact is available for download in the Subject Infrastructure Repository (SIR) <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b30">30]</ref>. SIR provides Siena's source code, a system level test suite with 567 test cases, multiple versions corresponding to product releases, and a set of seeded faults in each version (the authors were not involved in this latest activity).</p><p>For this study we consider Siena's core components (not on the application included in the package that is built with those components). We utilize the five versions of Siena that have seeded faults that did not generate compilation errors (faults that generated compilation errors cannot be tested) and that were exposed by at least one system test case (faults that were not found by system tests would not affect our assessment). For brevity, we summarize the most relevant information to our study in Table <ref type="table" target="#tab_2">1</ref> and point the reader to SIR <ref type="bibr" target="#b30">[30]</ref> to obtain more details about the process employed to prepare the Siena artifact for the empirical study. Table <ref type="table" target="#tab_2">1</ref> provides the number of methods, methods changed between versions and covered by the system test suite, system tests covering the changed methods, and faults included in each version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Study Setup and Design</head><p>The overall process consisted of the following steps. First, we prepared the test suites generated by S-retest-all, S-selection, Cselection-k*, and C-selection-mayref for their automatic execution. The preparation of the system level test suites was trivial because they were already available in the repository. The preparation of the carved selection suites (C-selection-k* and C-selection-mayref ), required for us to run the CR tool to carve all the DUTs for all the methods in v0 executed by the system tests.</p><p>Second, we run each of the generated test suites on the fault-free versions of Siena to obtain an oracle for each version. In the case of the system test suite, the oracle consisted in the set of outputs generated by the program. For the carved tests, the oracle consisted of the method return value and the relevant spost (we later explore several alternative projections to define the relevant state).</p><p>Third, we run each test suite on each faulty instance of each version (some versions contained multiple faults) and recorded their execution time. We dealt with each fault instance individually to control for potential masking effects among faults that might obscure the measurement of fault detection performance of the tests.</p><p>Fourth, for each test suite, we compared the outcome of each test case between the fault-free version (oracle) and the faulty instances of each version. To compare the system test outcomes between correct and fault versions, we used pre-defined differencing functions that are part of our implementation which ignore "nondeterministic" output data (e.g, dates, times, random numbers). For the DUTs, we performed a similar differencing, but applied to the target method return values and spost. When the outcome of a test case differed between the fault-free and the faulty version, a fault is found.</p><p>Last, we compared the measures across the test suites generated by S-retest-all, S-selection, C-selection-k*, and C-selection- mayref. We then repeated the same steps to collect data for the same techniques when utilizing test case filtering and compression.</p><formula xml:id="formula_0">Carving Metric Reduction C-select-k C-select 1 5 ∞ mayref</formula><p>The results emerging from this comparison are presented in the next section. All these activities were performed on an Opteron 250 processor, with 4GB of RAM, running Linux-Fedora, and Java 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>In this section we provide the results addressing each research question regarding carving and replaying efficiency, fault detection effectiveness, and robustness and sensitivity of the DUTs suites. RQ1: Efficiency. We first focus on the efficiency of the carving process. Although our infrastructure completely automates carving, this process does consume time and storage so it is important to assess its efficiency as it might impact its adoption and scalability. Table <ref type="table" target="#tab_3">2</ref> summarizes the time (in minutes) and the size (in MB) that took to carve and store the complete initial suite of 21232 DUTs utilizing the different techniques without and with the use of compression on the spre and spost. Each column in the table contains a test case reduction technique.</p><p>For Siena, constraining the carving depth affects the carving time. This is more noticeable when carving at k = 1 which takes 70% of the time required to carve with either k = 5 or the whole spre. We observe a similar pattern in terms of storage requirements. Observe that for depths greater than one the differences in storage space are minimal due to the rather "shallow" nature of the artifact (dereference chains with length greater than 2 are rare in Siena). The may-reference projection requires almost three times of additional analysis time, but we expect that it will be able to provide some gains in replay time. In the second row of Table <ref type="table" target="#tab_3">2</ref> we see that simply compressing the state data increased the carving time in proportion to the carved state size (and it will add uncompression time as well for the DUTs selected for replaying), but it consistently provided a two-orders of magnitude reduction in the space required by the DUTs, offering a clear space for time tradeoff.</p><p>It is important to note that the carving numbers reported in Table 2 correspond to the initial carving of the complete DUT suite -DUTs carved for each of the over 100 methods in Siena from each of the over 560 system tests that may execute each method -and can be performed automatically without the tester's participation. During the evolution of the system, DUTs will be replayed repeatedly amortizing the initial carving costs, and only a subset of the DUTs will need to be recarved (e.g., recarving the DUTs affected by the changes in v6 would only require 2% of the original carving time). Recarving will be necessary when is determined that changes in the program may affect a DUT's relevant pre-state. We believe that existing impact analysis techniques <ref type="bibr" target="#b24">[24]</ref> could be used, for example, to determine what DUTs must be recarved when a unit is changed, and we plan to integrate those into our infrastructure in the future.</p><p>We now proceed to analyze the replay efficiency. Replay efficiency is particularly important since it is likely that a carved DUT will be repeatedly replayed as the target unit evolves but still is meant to preserve its original intended behavior. Figure <ref type="figure" target="#fig_5">5</ref> sum-   marizes the replay execution times for some of the techniques we consider. Each observation corresponds to the replay time of each generated test suite under each version, while the lines joining observations are just meant to assist in the interpretation. Note that the plots for C-select-k5 and C-select-k∞ overlapped almost completely so we display only one of them.</p><p>The test suite resulting from the S-retest-all technique consistently averages 43 minutes per version. The test suites resulting from S-select for each version averages 28 minutes per version, with savings over S-retest-all ranging from barely a minute in v7 to a maximum of 41 minutes in v6. (Factors that affect the efficiency of this technique are not within the scope of this paper but can be found at <ref type="bibr" target="#b16">[16]</ref>). On average, S-select takes 65% of the time required by S-retest-all.</p><p>On average, all the C-selection-k* techniques replay execution time was less than 6 minutes. They took less than half a minute to replay v6 and up to 8 minutes for C-selection-k∞ to replay v5. On average, these suites take 12% of the time required by S-retestall, and 19% of the time required by S-select. Contrary to what we expected, the application of C-selection-mayref does not result in efficiency improvements. We conjecture that the outcome may be different for artifacts with more complex heap structures and we will assess that conjecture in future work. Last, we observe that handling compressed files (only shown for k = 5) increased the replaying time by a factor of four, making unlikely its application in spite of the storage savings.</p><p>We also measured the diffing time required by all techniques. For the system test suites the diffing times were consistently less than a minute, and for the C-selection-k* suites it averaged 6 minutes. Overall, although the diffing activity is important to the performance of the carved suites, implementing simple incremental differencing functions could dramatically improve their diffing performance. For example, we could first compare the return values to see if they reveal any differences, and if they do not not, then compare the rest of the post-state. This simple technique would suffice to reduce v5 diffing time by 96%.  This indicates that a DU T test suite can be as effective as a system test suite at detecting faults, even when using aggressive projections.</p><p>It is worth noting, however, that when computing fault detection effectiveness over a whole DUT suite we do not account for the fact that, for some system tests, their corresponding carved DUTs may have lost or gained fault detection effectiveness. We conjecture that this is a likely situation with our artifact because many of the faults are detected by multiple system tests, so there were many carved DUTs that could have detected each fault. To address this situation we perform an effectiveness analysis at the test case level.</p><p>For each carving technique we compute: 1) PP, the percentage of passing selected system tests (selected utilizing S-Selection) that have all corresponding carved unit test cases passing, and 2) FF: the percentage of failing system tests that have at least one corresponding failing carved unit test case. Table <ref type="table" target="#tab_6">3</ref> presents the PP and FF values for all the techniques under all version instances. In general we observe that most PP and FF values are over 90% indicating that DUTs carved from a system test case tend to conserve much of their effectiveness. We now discuss some interesting exceptions to this general tendency on PP and FF values.</p><p>We find that, independent of the DUT suite, for v7 : f 1, only 24% of the passing system tests had all their associated DUTs passing. The rest of the system tests had at least one DUT that detected a behavioral difference that was not detected by the system test case oracle because it did not propagate to the output to be detected. This is one example where a DUT can be more powerful than its corresponding system test.</p><p>When using the test suite resulting from C-selection-k1, we note that the FF values are on average 84%. In the cases where FF is not 100% such as in v5, we observed that replaying the carved test suite utilizing C-selection-k1 did not detect any of the behavioral differences exhibited by the selected system test cases. This reduction in FF was due to the depth-1 projection which did not capture enough pre-state to detect a behavioral difference. The other carved suites, however, did detect this fault.</p><p>Still, for the other suites, 3 out of the 300 failing system tests did not have any corresponding DUT on the changed methods failing (99%). We observed a similar situation in v7 : f 2 where 18 out of 203 DUTs (9%) did not expose behavioral differences even though the corresponding system tests failed. When we analyzed the reasons for this reduction in FF we discovered that in both cases the tool did not carve in v0 the pre-state for one of the changed methods. The tool did not carve any pre-state for those methods because the system test case did not reach them. Changes in the code structure (e.g., addition of a method call, handling of an exception), however, made the system test cases reach those changed methods (and expose a fault) in later versions. In both circumstances, improved DUTs that would have resulted in 100% FF could have been  RQ3: Robustness and sensitivity. We have previously examined how DUTs obtained through C-selection-k1 are quite fragile in terms of their executability, and how certain code changes may make a method reach a new part of the heap that was not originally carved. A complementary way to evaluate the robustness and sensitivity of DUTs is to compare their performance in the presence of methods that changed, and in the presence of methods that changed and are indeed faulty. We performed such detailed comparison on the suites generated with C-selection-∞. Table <ref type="table" target="#tab_8">4</ref> summarizes the findings and we now briefly discuss distinct instances of the scenarios we found.</p><p>In both faulty instances of v7, the version with the most methods changed <ref type="bibr" target="#b10">(10)</ref>, none of the behavioral differences were found by methods other than the faulty ones. This is clearly an ideal situation, which is also present in v6. V 1 : f 3 represents perhaps a more common case were 50% of the DUTs going through nonfaulty changed methods failed, but 100% of the DUTs traversing faulty methods actually failed. V 1 : f 2 presents a scenario in which carving generates more behavioral differences for the nonfaulty method than for the change and faulty one, showing that even for correct changes the number of affected DUTs may be large (26 out of 130).</p><p>It is worth noting that the differencing functions offer an opportunity to control this problem. For example, a more relaxed differencing mechanism focused on just return values could have detected the fault while reducing the number of false differences if the fault manifests itself in the return value. Mechanisms to select and appropriately combine these differencing functions will be important for the robustness and sensitivity of DUTs. In addition, we anticipate that as the carving and replay components of the framework become parts of an IDE, the additional change information available in the developer's environment could help to reduce the number of false positives. For example, code modifications due to refactoring that do not affect the target unit's interface would be expected to retain the same behavior. However, changes that can be mapped to the bug repository would be expected to affect the unit's behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Our work was inspired by Weide's notion of modular testing as a means to evaluate the modular reasoning property of a piece of software <ref type="bibr" target="#b36">[36]</ref>. Although Weide's focus was not on testing but on the evaluation of the fragility of modular reasoning, he raised some important questions regarding the potential applicability of what he called a "modular regression technique" that led to our work.</p><p>Within the context of regression testing, our approach is also similar to Binkley's semantic guided regression testing in that it aims to reduce testing costs by running a subset of the program <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>. Binkley's technique proposes the utilization of static slicing to identify potential semantic differences between two versions of a program. He also presents an algorithm to identify the system tests that must be run on the slices resulting from the differences between the program versions. The fundamental distinction between this and our approach is that we do not run system level tests, but rather smaller and more focused unit tests. Another important distinction is that the testing target are not the semantic differences between versions, but rather methods in the program.</p><p>The preliminary results from our original test carving prototype <ref type="bibr" target="#b28">[28]</ref> evidenced the potential of carved tests to improve the efficiency and the focus of a large system test suite, identified challenges to scale-up the approach, and defined some scenarios under which the carved test cases would and would not perform well. We have built on that work by presenting a generic framework for differential carving, extending the type of analysis we performed to make the approach more scalable, and by developing a full set of tools that can enable us to explore different techniques on various programs.</p><p>We are aware of two other research efforts related to the notion of test carving. First, Orso et al. prototyped the notion of selective record and replay mechanisms of program executions by capturing the interactions between the observed subsystem and its context, and then replaying just the result of those interactions <ref type="bibr" target="#b25">[25]</ref>. Second, the test factoring approach introduced by Saff et al. takes a similar approach to Orso's with the creation of what they called mock objects that serve to create the scaffolding to support the execution of the test unit <ref type="bibr" target="#b34">[34]</ref>. The same group introduced a tool set for fully-featured Java execution environments that can handle many of the subtle interactions present in this programming language (e.g., callbacks, arrays, native methods) <ref type="bibr" target="#b33">[33]</ref>. In terms of our framework, both of these approaches would be considered action-based CR approaches. We have presented, what is to the best of our knowledge, the first state-based approach to CR testing.</p><p>Saff et al. describe their approach in detail allowing us to provide a more in depth comparison with our approach. While carving a method test case, their infra-structure records the sequence of calls that can influence the method and then they record the sequence of calls made by the method and the return values and unit state side-effects of those calls. In our framework, this would amount to calculating σ such that s(σ) = spre for the method of interest and then calculating summarizing traces σcall i that reflect the return value and side effects for each call out of the method and carving spre i , the relevant pre-state for each call. During replay the same sequence of calls with the same parameters is expectedany deviation results in a report of a difference during replay. In our framework, we would identify the points at which the, n, calls out of the method occur as post-state locations to define a DUT of the form (σ, (spre 1 , . . . , spre n )).</p><p>Both of these action-based approaches, capture the interactions between the target unit and its context and then build the scaffolding to replay just those interactions. Hence, they do not incur in costs associated with capturing and storing the system state for each targeted unit. On the other hand, this approach may generate tests that are too sensitive to simple changes that do not effect meaning of the unit, e.g., changing the order of independent method calls. Saff et al. have identified this issue and propose to analyze the lifespan of the factored test cases across sequences of method modifications <ref type="bibr" target="#b33">[33]</ref>. This is a critical factor in judging the cost-effectiveness of CR testing and we have started to study this issue in Section 4.5.</p><p>These two related efforts have shown their feasibility in terms of being able to replay tests and the latter approach has provided initial evidence that it can save time and resources under several scenarios. Neither approach, however, has been evaluated in terms of its fault detection effectiveness which ultimately determines the value of the carved tests, or in the context of regression testing.</p><p>Our work also relates to efforts aimed at developing unit test cases. Several frameworks grouped under the umbrella of Xunit have been developed to support software engineers in the development of unit tests. Junit, for example, is a popular framework for the Java programming language that lets programmers attach testing code to their classes to validate their behavior <ref type="bibr" target="#b17">[17]</ref>.</p><p>There are also multiple approaches that automate, to different degrees, the generation of unit tests. For example, commercial tools such as Jtest develops unit test cases by analyzing method signatures and selecting test cases that increase some coverage criteria <ref type="bibr" target="#b20">[20]</ref>. Some of these tools aim to assess software robustness (e.g., whether an exception is thrown <ref type="bibr" target="#b12">[12]</ref>). Others utilize some type of specification such as pre and post conditions or operational abstractions, to guide the test case generation and actually check whether the test outcome meets the expectation results <ref type="bibr">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b38">38]</ref>. Interestingly enough, Parasoft new version of JTest enhances the unit test case generated with "Sniffer", a tool that monitors running applications to pick interesting values to exercise the target unit <ref type="bibr" target="#b20">[20]</ref>, which can be perceived as a primitive type of carving projection.</p><p>Although carving also aims to generate unit test cases, the approach we propose is different from previous unit test case generation mechanisms since it consists of the projection of a system test case onto the targeted software unit. As such, we expect for carved unit tests to retain some of the interesting interactions exposed by systems tests that are harder to design into regular unit test cases that often fail to consider the system's context.</p><p>As stated, the post-state differencing functions that regulate the detection of differences between encodings of unit behavior belongs to a larger body of testing work on differential-based oracles. For example, the work of Weyuker <ref type="bibr" target="#b37">[37]</ref> on the development of pseudo-oracles, Jaramillo et al. <ref type="bibr" target="#b19">[19]</ref> on using comparisons to check for optimization induced errors in compilers, or the comparison of program spectra <ref type="bibr" target="#b29">[29]</ref> are instances of utilizing differencingtype oracles at the system or subsystem level. When focusing at the unit level of object oriented programs, as we are doing, Binder suggests the term "concrete state" oracles, which aim to compare the value of all the unit's attributes against what is expected <ref type="bibr" target="#b4">[4]</ref>. <ref type="bibr">Briand et al. refer</ref> to this type of oracle as a "precise" oracle because it was the most accurate one employed in their studies <ref type="bibr" target="#b8">[8]</ref>. Overall, the notion of testing being fundamentally differential has long been understood <ref type="bibr" target="#b37">[37]</ref>, since the pseudo-oracles against which systems are judged correct are themselves subject to error. Thus, the question we aimed to answer is not whether our CR method judges a system correct or incorrect, but rather whether it is capable of cost-effectively detecting differences between encodings of system behavior that developers can easily mine to judge whether the difference reflects an error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We have presented a general framework for automatically carving and replaying DUTs. The framework incorporates sophisticated projection and differencing strategies that can be instantiated in various ways to accommodate distinct trade-offs. We have implemented a state-based instance of the framework that mitigates testing costs through two types of reachability-based projections, and that can adjust the DUTs sensitivity through differencing functions. Our evaluation of this implementation has revealed that DUTs can be automatically generated from system tests, reduce average test suite execution time to a fifth of our best system selection technique, and still retain most of the fault detection power of system tests.</p><p>The experiences gained while instantiating and assessing the framework suggest several directions for future work. First, we will perform further studies not only to confirm our findings on other artifacts under similar settings, but also to compare DUTs with traditional unit tests developed by software engineers. We conjecture that software engineers develop rather shallow unit tests and that we can effectively complement those with DUTs that expose the target units to more complex execution settings.</p><p>Second, we will extend our implementation with additional features to reduce the cost of CR testing while preserving test effectiveness. We will store the results of applying differencing functions to post-states rather than storing post-states themselves. We will provide mechanisms for testers to define differencing functions besides the ones provided by the framework. We expect that experience applying these techniques to a broad collection of examples will expose additional opportunities for cost-reduction. For example, when collecting the data for Siena we realized that applying some "lossy" projections to filter DUTs may yield more interesting tradeoffs between scalability and fault detection effectiveness.</p><p>Third, we are exploring techniques to combine multiple DUTs to create a compound DUT for a larger program unit such as a class. This can be achieved by clustering multiple DUTs based on the identity of the receiver object. For a sequence of method calls, ci, . . . , cj , on an object in a system test, the set of DUTs for those calls is replaced by a single DUT that captures (s pr ei , (s po st i , . . . , s po st j )).</p><p>This test would start at sprei and the sequence of calls are replayed for each class method as (s postk-1 , s postk ) where k = i + 1, .., j. This effectively transfers the effects of methods on the receiver object throughout the sequence achieving a kind of interaction testing between calls. We plan to implement this approach and assess it relative to other class-oriented testing techniques. Last, we will develop a supporting infrastructure to increase the use of DUTs in practice. We will incorporate the CR framework capabilities within software development IDEs. We will leverage some of the static analysis techniques already at our disposal to determine, for example, when changes in a method may suggest a re-carving operation targeted at that specific method. We would also like to extend the analysis performed after a DUTs detects a behavioral difference on a unit that is later deemed correct. In this situation, we would like to know what other DUTs might be obsolete and require re-carving.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample applications of projections functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 illustrates two potential applications of projections on DUTs: test case reduction and test cases filtering.Reduction aims at thinning a single carved test case by retaining only the projected pre-state (in Figure2for example the projection of spre carved from ctxm leads to a smaller spre). Reducing a DUT's pre-state results in smaller space requirements and, more importantly, in quicker replay since loading time is a function of the pre-state size. As we shall see, depending on the type of projection, these gains may be achieved at the expense of reduced fault detection power (e.g., a projection may discard an object that was necessary to expose the fault). Furthermore, test executability may be sacrificed as well. State-based projections may become unexecutable if the data structures used by the target unit changes, for example, shifting from an array to a heap-based structure, even if behavior is preserved. Action-based projections may become unexecutable if the behavior of a unit method changes so that a different number or sequence of methods is needed in the modified program to produce the desired pre-state. Still, reduction can be a valuable mechanism to improve the efficiency of CR by keeping just the portions of the pre-state that are most likely to be relevant to the targeted method.Filtering aims at removing redundant DUTs from the suite. Consider a method that is invoked during the program initialization and is independent of the program parameters. Such method would be exercised by all system tests in the same way and result in multiple identical DUTs for that particular method. A simple filter would remove such duplicate tests, keeping just the unique DUTs. Now consider a simple accessor method with no parameters that just returns the value of a scalar field. If this method is invoked by the tests from different pre-states, then multiple DUTs will be carved, and a simple lossless filter will not discard any DUT even though they exercise similar behavior. In this case, applying a projection that preserves the pre-state components directly reachable from this would result in many DUTs that are redundant (in Figure2, π(spre) for ctxm and for ctzm are identical so one of them can be removed). Clearly, in some cases, this kind of lossy filtering may result in a lower fault detection capability since we may discard a DUT that is indeed different and hence potentially valuable. Note that, contrary to test case reduction, filtering only uses projections to judge test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Differencing sequences of post-states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CR Tool Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Execution times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Siena's components attributes.</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Version Methods Changed-covered</cell><cell>Tests executing</cell><cell>Faults</cell></row><row><cell></cell><cell></cell><cell>methods</cell><cell>changed methods</cell><cell></cell></row><row><cell>v0</cell><cell>109</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>v1</cell><cell>100</cell><cell>2</cell><cell>494</cell><cell>3</cell></row><row><cell>v5</cell><cell>111</cell><cell>2</cell><cell>494</cell><cell>1</cell></row><row><cell>v6</cell><cell>111</cell><cell>2</cell><cell>8</cell><cell>1</cell></row><row><cell>v7</cell><cell>107</cell><cell>10</cell><cell>550</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : Carving times and sizes to generate initial DUT suite.</head><label>2</label><figDesc></figDesc><table><row><cell>Plain</cell><cell>Minutes</cell><cell>113</cell><cell>157</cell><cell>158</cell><cell>467</cell></row><row><cell></cell><cell>MB</cell><cell cols="3">1.1K 1.9K 1.9K</cell><cell>2K</cell></row><row><cell cols="2">Compressed Minutes</cell><cell>129</cell><cell>186</cell><cell>188</cell><cell>496</cell></row><row><cell></cell><cell>MB</cell><cell>6</cell><cell>7</cell><cell>7</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 : Fault Detection Effectiveness. RQ2: Fault detection effectiveness.</head><label>3</label><figDesc>The test suites directly generated by S-selection, C-selection-k*, and C-selection-mayref detected as many faults as the S-retest-all technique.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Robustness and sensitivity.generated by re-carving the test cases in later versions (carve from vi instead of v0to replay in vi+1). More generally, these observations point out again fo the need for mechanisms to detect changes in the code that should trigger re-carving.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the National Science Foundation through CAREER award 0347518, and awards 0429149, 0444167, and 0411043, by the Army Research Office through DURIP award W911NF-04-1-0104, and by an IBM Eclipse Award. We would like to thank B. Weide for inspiring this effort, S. Reddy for the feasibility exploration she provided through her thesis, and Matt Jorde for optimizing the implementation. We would also like to thank V.Ranganath for supporting our use of Indus and O. Tkachuk for implementing preliminary versions of the static analysis.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Useful features of a test automation system (part iii)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Testing Techniques Newsletter</title>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Extreme Programming Explained : Embrace Change</title>
		<author>
			<persName><forename type="first">K</forename><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley Professional</publisher>
		</imprint>
	</monogr>
	<note>first edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Beck</surname></persName>
		</author>
		<title level="m">Test Driven Development: By Example</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Binder</surname></persName>
		</author>
		<title level="m">Testing Object-Oriented Systems: Models, Patterns, and Tools</title>
		<meeting><address><addrLine>Reading, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="943" to="951" />
		</imprint>
	</monogr>
	<note>Object Technologies. first edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantics guided regression test cost reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="498" to="516" />
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An implementation of and experiment with semantic differencing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Capellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Raszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Maintenance</title>
		<imprint>
			<date type="published" when="2001-11">Nov. 2001</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Korat: automated testing based on java predicates</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boyapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khurshid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Software Testing and Analysis</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Assessing and improving state-based class testing: A series of experiments</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Labiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Softw. Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="770" to="793" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Achieving scalability and expressiveness in an internet-scale event notification service</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carzaniga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium Principles of Distributed Computing</title>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="219" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TestTube: A system for selective regression testing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple and practical approach to unit testing: The jml and junit</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Leavens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Object-Oriented Programming</title>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<biblScope unit="page" from="231" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jcrasher: an automatic robustness tester for java</title>
		<author>
			<persName><forename type="first">C</forename><surname>Csallner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Smaragdakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software Practice and Expererience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1025" to="1050" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Dahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Zyl</surname></persName>
		</author>
		<ptr target="http://jakarta.apache.org/bcel/" />
		<title level="m">Byte code engineering library</title>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study of the allocation behavior of the specjvm98 java benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dieckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Holzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Object-Oriented Programming</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="92" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Supporting controlled experimentation with testing techniques: An infrastructure and its potential impact</title>
		<author>
			<persName><forename type="first">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering: An International Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="435" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the effects of changes on the cost-effectiveness of regression testing techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kallakuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Malishevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanduri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software Testing, Verification, and Reliability</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="83" />
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Gamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><surname>Junit</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/junit" />
		<imprint>
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simplifying failure-inducing input</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Software Testing and Analysis</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comparison checking: An approach to avoid debugging of optimized code</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jaramillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Software Engineering Conference/ Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jtest product overview</title>
		<author>
			<persName><surname>Jtest</surname></persName>
		</author>
		<ptr target="http://www.parasoft.com/jsp/products/home.jsp?product=Jtest" />
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Insights into regression testing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Maintenance</title>
		<imprint>
			<date type="published" when="1989-10">Oct. 1989</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of integration testing and software regression at the integration level</title>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Maintenance</title>
		<imprint>
			<date type="published" when="1990-11">Nov. 1990</date>
			<biblScope unit="page" from="290" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regression testing in an industrial environment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Onoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poonawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suganuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="81" to="86" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Empirical Comparison of Dynamic Impact Analysis Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Apiwattanapong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Software Engineering</title>
		<meeting>the International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selective capture and replay of program executions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Dynamic Analysis</title>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eclat: Automatic generation and classification of test inputs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Object-Oriented Programming</title>
		<imprint>
			<date type="published" when="2005-07">July 2005</date>
			<biblScope unit="page" from="504" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pruning interference and ready dependence for slicing concurrent java programs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hatcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Compiler Construction</title>
		<imprint>
			<date type="published" when="2004-04">April 2004</date>
			<biblScope unit="page" from="39" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Carving module test cases from system test cases: an application to regression testing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Nebraska -Lincoln, Computer Science and Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The use of program profiling for software maintenance with applications to the year 2000 problem</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Software Engineering Conference/ Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="432" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Do</surname></persName>
		</author>
		<ptr target="http://cse.unl.edu/galileo/php/sir/index.php" />
		<title level="m">Software infrastructure repository</title>
		<imprint>
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On test suite composition and cost-effective regression testing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Malishevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kallakuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions of Software Engineering and Methodologies</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="331" />
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analyzing regression test selection techniques</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harrold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="529" to="551" />
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated test factoring for java</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of Automated Software Engineering</title>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
			<biblScope unit="page" from="114" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic mock object creation for test factoring</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Program Analysis for Software Tools and Engineering</title>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<biblScope unit="page" from="49" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An experimental evaluation of continuous testing during development</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGSOFT International Symposium on Software Testing and Analysis</title>
		<meeting>the ACM SIGSOFT International Symposium on Software Testing and Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modular regression testing&quot;: Connections to component-based software</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Component-based Software Engineering</title>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On testing non-testable programs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Weyuker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="465" to="470" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tool-assisted unit-test generation and selection based on operational abstractions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Notkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Engineering Journal</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Xstream -1.1.2</title>
		<author>
			<persName><surname>Xstream</surname></persName>
		</author>
		<ptr target="http://xstream.codehaus.org" />
		<imprint>
			<date type="published" when="2005-08">Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
