<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sanity Checks for Saliency Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
							<email>juliusad@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
							<email>gilmer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Muelly</surname></persName>
							<email>muelly@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
							<email>goodfellow@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
							<email>beenkim@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sanity Checks for Saliency Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E43D5BDDC8FDF6DBA5699F49BFF16DCA</idno>
					<note type="submission">* Work done during the Google AI Residency Program. Original Image Gradient SmoothGrad Guided BackProp</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As machine learning grows in complexity and impact, much hope rests on explanation methods as tools to elucidate important aspects of learned models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Explanations could potentially help satisfy regulatory requirements <ref type="bibr" target="#b2">[3]</ref>, help practitioners debug their model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and perhaps, reveal bias or other unintended effects learned by a model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Saliency methods<ref type="foot" target="#foot_1">3</ref> are an increasingly popular class of tools designed to highlight relevant features in an input, typically, an image. Despite much excitement, and significant recent contribution <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, the valuable effort of explaining machine learning models faces a methodological challenge: the difficulty of assessing the scope and quality of model explanations. A paucity of principled guidelines confound the practitioner when deciding between an abundance of competing methods.</p><p>We propose an actionable methodology based on randomization tests to evaluate the adequacy of explanation approaches. We instantiate our analysis on several saliency methods for image classification with neural networks; however, our methodology applies in generality to any explanation approach. Critically, our proposed randomization tests are easy to implement, and can help assess the suitability of an explanation method for a given task at hand. In a broad experimental sweep, we apply our methodology to numerous existing saliency methods, model architectures, and data sets. To our surprise, some widely deployed saliency methods are independent of both the data the model was trained on, and the model parameters. Consequently, Figure <ref type="figure">1</ref>: Saliency maps for some common methods compared to an edge detector. Saliency masks for 3 inputs for an Inception v3 model trained on ImageNet. We see that an edge detector produces outputs that are strikingly similar to the outputs of some saliency methods. In fact, edge detectors can also produce masks that highlight features which coincide with what appears to be relevant to a model's class prediction. We find that the methods most similar (see Appendix for SSIM metric) to an edge detector, i.e., Guided Backprop and its variants, show minimal sensitivity to our randomization tests.</p><p>these methods are incapable of assisting with tasks that depend on the model, such as debugging the model, or tasks that depend on the relationships between inputs and outputs present in the data.</p><p>To illustrate the point, Figure <ref type="figure">1</ref> compares the output of standard saliency methods with those of an edge detector. The edge detector does not depend on model or training data, and yet produces results that bear visual similarity with saliency maps. This goes to show that visual inspection is a poor guide in judging whether an explanation is sensitive to the underlying model and data.</p><p>Our methodology derives from the idea of a statistical randomization test, comparing the natural experiment with an artificially randomized experiment. We focus on two instantiations of our general framework: a model parameter randomization test, and a data randomization test.</p><p>The model parameter randomization test compares the output of a saliency method on a trained model with the output of the saliency method on a randomly initialized untrained network of the same architecture. If the saliency method depends on the learned parameters of the model, we should expect its output to differ substantially between the two cases. Should the outputs be similar, however, we can infer that the saliency map is insensitive to properties of the model, in this case, the model parameters. In particular, the output of the saliency map would not be helpful for tasks such as model debugging that inevitably depend on the model parameters.</p><p>The data randomization test compares a given saliency method applied to a model trained on a labeled data set with the method applied to the same model architecture but trained on a copy of the data set in which we randomly permuted all labels. If a saliency method depends on the labeling of the data, we should again expect its outputs to differ significantly in the two cases. An insensitivity to the permuted labels, however, reveals that the method does not depend on the relationship between instances (e.g. images) and labels that exists in the original data.</p><p>Speaking more broadly, any explanation method admits a set of invariances, i.e., transformations of data and model that do not change the output of the method. If we discover an invariance that is incompatible with the requirements of the task at hand, we can safely reject the method. As such, our tests can be thought of as sanity checks to perform before deploying a method in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions</head><p>1. We propose two concrete, easy to implement tests for assessing the scope and quality of explanation methods: the model parameter randomization test, and the data randomization test. These tests apply broadly to explanation methods.</p><p>2. We conduct extensive experiments with several explanation methods across data sets and model architectures, and find, consistently, that some of the methods tested are independent of both the model parameters and the labeling of the data that the model was trained on.</p><p>3. Of the methods we tested, Gradients &amp; GradCAM pass the sanity checks, while Guided BackProp &amp; Guided GradCAM fail. In the other cases, we observe a visual perception versus ranking dichotomy, which we describe in our results. <ref type="bibr" target="#b3">4</ref>. Consequently, our findings imply that the saliency methods that fail our tests are incapable of supporting tasks that require explanations that are faithful to the model or the data generating process.</p><p>5. We interpret our findings through a series of analyses of linear models and a simple 1-layer convolutional sum-pooling architecture, as well as a comparison with edge detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and Related Work</head><p>In our formal setup, an input is a vector x ∈ R d . A model describes a function S : R d → R C , where C is the number of classes in the classification problem. An explanation method provides an explanation map E : R d → R d that maps inputs to objects of the same shape.</p><p>We now briefly describe some of the explanation methods we examine. The supplementary materials contain an in-depth overview of these methods. Our goal is not to exhaustively evaluate all prior explanation methods, but rather to highlight how our methods apply to several cases of interest. The gradient explanation for an input x is E grad (x) = ∂S ∂x <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. The gradient quantifies how much a change in each input dimension would a change the predictions S(x) in a small neighborhood around the input.</p><p>Gradient Input. Another form of explanation is the element-wise product of the input and the gradient, denoted x ∂S ∂x , which can address "gradient saturation", and reduce visual diffusion <ref type="bibr" target="#b12">[13]</ref>. Integrated Gradients (IG) also addresses gradient saturation by summing over scaled versions of the input <ref type="bibr" target="#b13">[14]</ref>. IG for an input x is defined as</p><formula xml:id="formula_0">E IG (x) = (x -x) × 1 0 ∂S(x+α(x-x) ∂x</formula><p>dα, where x is a "baseline input" that represents the absence of a feature in the original input x.</p><p>Guided Backpropagation (GBP) <ref type="bibr" target="#b8">[9]</ref> builds on the "DeConvNet" explanation method <ref type="bibr" target="#b9">[10]</ref> and corresponds to the gradient explanation where negative gradient entries are set to zero while backpropagating through a ReLU unit.</p><p>Guided GradCAM. Introduced by Selvaraju et al. <ref type="bibr" target="#b18">[19]</ref>, GradCAM explanations correspond to the gradient of the class score (logit) with respect to the feature map of the last convolutional unit of a DNN. For pixel level granularity GradCAM can be combined with Guided Backpropagation through an element-wise product.</p><p>SmoothGrad (SG) <ref type="bibr" target="#b15">[16]</ref> seeks to alleviate noise and visual diffusion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> for saliency maps by averaging over explanations of noisy copies of an input. For a given explanation map E, SmoothGrad is defined as</p><formula xml:id="formula_1">E sg (x) = 1 N N i=1 E(x + g i ),</formula><p>where noise vectors g i ∼ N (0, σ 2 )) are drawn i.i.d. from a normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Other Methods &amp; Similarities. Aside gradient-based approaches, other methods 'learn' an explanation per sample for a model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>. More recently, M. Ancona <ref type="bibr" target="#b23">[24]</ref> showed that for ReLU networks (with zero baseline and no biases) the -LRP and DeepLift (Rescale) explanation methods are equivalent to the input gradient. Similarly, Lundberg and Lee <ref type="bibr" target="#b17">[18]</ref> proposed SHAP explanations which approximate the shapley value and unify several existing methods.</p><p>Fragility. Ghorbani et al. <ref type="bibr" target="#b24">[25]</ref> and Kindermans et al. <ref type="bibr" target="#b25">[26]</ref> both present attacks against saliency methods; showing that it is possible to manipulate derived explanations in unintended ways. Nie et al. <ref type="bibr" target="#b26">[27]</ref> theoretically assessed backpropagation based methods and found that Guided BackProp and DeconvNet, under certain conditions, are invariant to network reparamaterizations, particularly random Gaussian initialization. Specifically, they show that Guided BackProp and DeconvNet both seem to be performing partial input recovery. Our findings are similar for Guided BackProp and its variants. Further, our work differs in that we propose actionable sanity checks for assessing explanation approaches. Along similar lines, Mahendran and Vedaldi <ref type="bibr" target="#b27">[28]</ref> also showed that some backpropagation-based saliency methods lack neuron discriminativity.</p><p>Current assessment methods. Both Samek et al. <ref type="bibr" target="#b28">[29]</ref> and Montavon et al. <ref type="bibr" target="#b29">[30]</ref> proposed an input perturbation procedure for assessing the quality of saliency methods. Dabkowski and Gal <ref type="bibr" target="#b16">[17]</ref> proposed an entropy-based metric to quantify the amount of relevant information an explanation mask captures. Performance of a saliency map on an object localization task has also been used for assessing saliency methods. Montavon et al. <ref type="bibr" target="#b29">[30]</ref> discuss explanation continuity and selectivity as measures of assessment.</p><p>Randomization. Our label randomization test was inspired by the work of Zhang et al. <ref type="bibr" target="#b30">[31]</ref>, although we use the test for an entirely different purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization &amp; Similarity Metrics</head><p>We discuss our visualization approach and overview the set of metrics used in assessing similarity between two explanations.</p><p>Visualization. We visualize saliency maps in two ways. In the first case, absolute-value (ABS), we take absolute values of a normalized<ref type="foot" target="#foot_2">4</ref> map. For the second case, diverging visualization, we leave the map as is, and use different colors to show positive and negative importance.</p><p>Similarity Metrics. For quantitative comparison, we rely on the following metrics: Spearman rank correlation with absolute value (absolute value), Spearman rank correlation without absolute value (diverging), the structural similarity index (SSIM), and the Pearson correlation of the histogram of gradients (HOGs) derived from two maps. We compute the SSIM and HOGs similarity metric on ImageNet examples without absolute values. <ref type="foot" target="#foot_3">5</ref> These metrics capture a broad notion of similarity; however, quantifying human visual perception is still an active area of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Parameter Randomization Test</head><p>The parameter settings of a model encode what the model has learned from the data during training, and determine test set performance. Consequently, for a saliency method to be useful for debugging a model, it ought to be sensitive to model parameters.</p><p>As an illustrative example, consider a linear function of the form f (x) = w 1 x 1 + w 2 x 2 with input x ∈ R 2 . A gradient-based explanation for the model's behavior for input x is given by the parameter values (w 1 , w 2 ), which correspond to the sensitivity of the function to each of the coordinates. Changes in the model parameters therefore change the explanation.</p><p>Our proposed model parameter randomization test assesses an explanation method's sensitivity to model parameters. We conduct two kinds of randomization. First we randomly re-initialize all weights of the model both completely and in a cascading fashion. Second, we independently randomize a single layer at a time while keeping all others fixed. In both cases, we compare the resulting explanation from a network with random weights to the one obtained with the model's original weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cascading Randomization</head><p>Overview. In the cascading randomization, we randomize the weights of a model starting from the top layer, successively, all the way to the bottom layer. This procedure destroys the learned weights from the top layers to the bottom ones. Figure <ref type="figure" target="#fig_0">2</ref> visualizes the cascading randomization for several saliency methods. In Figures <ref type="figure">3</ref> and<ref type="figure" target="#fig_2">4</ref>, we show the Spearman metrics as well as the SSIM and HOGs similarity metrics.</p><p>The gradient shows sensitivity while Guided BackProp is invariant. We find that the gradient map is sensitive to model parameters. We also observe sensitivity for the GradCAM masks. On the other hand, across all architectures and datasets, Guided BackProp and Guided GradCAM show no change regardless of model degradation.  The danger of the visual assessment. On visual inspection, we find that integrated gradients and gradient input show a remarkable visual similarity to the original mask. In fact, from Figure <ref type="figure" target="#fig_0">2</ref>, it is still possible to make out the structure of the bird even after multiple blocks of randomization. This visual similarity is reflected in the rank correlation with absolute value (Figure <ref type="figure">3</ref>-Top), SSIM, and the HOGs metric (Figure <ref type="figure" target="#fig_2">4</ref>). However, re-initialization disrupts the sign of the map, so that the Spearman rank correlation without absolute values goes to zero (Figure <ref type="figure">3</ref>-Bottom) almost as soon as the top layers are randomized. This observed visual perception versus numerical ranking dichotomy indicates that naive visual inspection of the masks does not distinguish networks of similar structure but widely differing parameters. We explain the source of this phenomenon in our discussion section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Independent Randomization</head><p>Overview. As a different form of the model parameter randomization test, we conduct an independent layer-by-layer randomization with the goal of isolating the dependence of the explanations by layer. Consequently, we can assess the dependence of saliency masks on lower versus higher layer weights.</p><p>Results. We observe a correspondence between the results from the cascading and independent layer randomization experiments (see Figures ??, ??, ??, and ?? in the Appendix). As previously observed, Guided Backprop and Guided GradCAM masks remain almost unchanged regardless of the layer that is independently randomized across all networks. Similarly, we observe that the structure of the input is maintained, visually, for the gradient input and Integrated Gradient methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Randomization Test</head><p>The feasibility of accurate prediction hinges on the relationship between instances (e.g., images) and labels encoded by the data. If we artificially break this relationship by randomizing the labels, no predictive model can do better than random guessing. Our data randomization test evaluates the sensitivity of an explanation method to the relationship between instances and labels. An explanation method insensitive to randomizing labels cannot possibly explain mechanisms that depend on the relationship between instances and labels present in the data generating process. For example, if an explanation did not change after we randomly assigned diagnoses to CT scans, then evidently it did not explain anything about the relationship between a CT scan and the correct diagnosis in the first place (see <ref type="bibr" target="#b31">[32]</ref> for an application of Guided BackProp as part of a pipepline for shadow detection in 2D Ultrasound).</p><p>In our data randomization test, we permute the training labels and train a model on the randomized training data. A model achieving high training accuracy on the randomized training data is forced to memorize the randomized labels without being able to exploit the original structure in the data. As it turns out, state-of-the art deep neural networks can easily fit random labels as was shown in Zhang et al. <ref type="bibr" target="#b30">[31]</ref>.</p><p>In our experiments, we permute the training labels for each model and data set pair, and train the model to greater than 95% training set accuracy. Note that the test accuracy is never better than randomly guessing a label (up to sampling error). For each resulting model, we then compute explanations on the same test bed of inputs for a model trained with true labels and the corresponding model trained on randomly permuted labels.  Gradient is sensitive. We find, again, that gradients, and its smoothgrad variant, undergo substantial changes. In addition, the GradCAM masks also change becoming more disconnected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN -MNIST</head><p>Sole reliance on visual inspection can be misleading. For Guided BackProp, we observe a visual change; however, we find that the masks still highlight portions of the input that would seem plausible, given correspondence with the input, on naive visual inspection. For example, from the diverging masks (Figure <ref type="figure" target="#fig_3">5</ref>-Right), we see that the Guided BackProp mask still assigns positive relevance across most of the digit for the network trained on random labels.</p><p>For gradient input and integrated gradients, we also observe visual changes in the masks obtained, particularly, in the sign of the attributions. Despite this, the input structure is still clearly prevalent in the masks. The effect observed is particularly prominent for sparse inputs like MNIST where the background is zero; however, we observe similar effects for Fashion MNIST (see Appendix). With visual inspection alone, it is not inconceivable that an analyst could confuse the integrated gradient and gradient input masks derived from a network trained on random labels as legitimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We now take a step back to interpret our findings. First, we discuss the influence of the model architecture on explanations derived from NNs. Second, we consider methods that approximate an element-wise product of input and gradient, as several local explanations do <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18]</ref>. We show, empirically, that the input "structure" dominates the gradient, especially for sparse inputs. Third, we explain the observed behavior of the gradient explanation with an appeal to linear models. We then consider a single 1-layer convolution with sum-pooling architecture, and show that saliency explanations for this model mostly capture edges. Finally, we return to the edge detector and make comparisons between the methods that fail our sanity checks and an edge detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The role of model architecture as a prior</head><p>The architecture of a deep neural network has an important effect on the representation derived from the network. A number of results speak to the strength of randomly initialized models as classification priors <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. Moreover, randomly initialized networks trained on a single input can perform tasks like denoising, super-resolution, and in-painting <ref type="bibr" target="#b35">[36]</ref> without additional training data. These prior works speak to the fact that randomly initialized networks correspond to non-trivial representations.</p><p>Explanations that do not depend on model parameters or training data might still depend on the model architecture and thus provide some useful information about the prior incorporated in the model architecture. However, in this case, the explanation method should only be used for tasks where we believe that knowledge of the model architecture on its own is sufficient for giving useful explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Element-wise input-gradient products</head><p>A number of methods, e.g., -LRP, DeepLift, and integrated gradients, approximate the element-wise product of the input and the gradient (on a piecewise linear function like ReLU). To gain further insight into our findings, we can look at what happens to the input-gradient product E(x) = x ∂S ∂x , if the input is kept fixed, but the gradient is randomized. To do so, we conduct the following experiment. For an input x, sample two random vectors u, v (we consider both the truncated normal and uniform distributions) and consider the element-wise product of x with u and v, respectively, i.e., x u, and x v. We then look at the similarity, for all the metrics considered, between x u and x v as noise increases. We conduct this experiment on ImageNet samples. We observe that the input does indeed dominate the product (see Figure ?? in Appendix). We also observe that the input dominance persists even as the noisy gradient vectors change drastically. This experiment indicates that methods that approximate the "input-times-gradient" could conceivably mostly return the input, in cases where the gradients look visually noisy as they tend to do.  To better understand our findings, we analyze the output of the saliency methods tested on two simple models: a linear model and a 1-layer sum pooling convolutional network. We find that the output of the saliency methods, on a linear model, returns a coefficient that intuitively measures the sensitivity of the model with respect to that variable. However, these methods applied to a random convolution seem to result in visual artifacts that are akin to an edge detector. Similarly for SmoothGrad we have E sg (x) = w (the gradient is independent of the input, so averaging gradients over noisy inputs yields the same model weight). Integrated Gradients reduces to "gradient input" for this case:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis for simple models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear</head><formula xml:id="formula_2">E IG (x) = (x -x) 1 0 ∂f (x + α(x -x)) ∂x dα = (x -x) 1 0 wαdα = (x -x) w/2 .</formula><p>Consequently, we see that the application of the basic gradient method to a linear model will pass our sanity check. Gradients on a random model will return an image of white noise, while integrated gradients will return a noisy version of the input image. We did not consider Guided Backprop and GradCAM here because both methods are not defined for the linear model considered above.</p><p>1 Layer Sum-Pool Conv Model. We now show that the application of these same methods to a 1-layer convolutional network may result in visual artifacts that can be misleading unless further analysis is done. Consider a single-layer convolutional network applied to a grey-scale image x ∈ R n×n . Let w ∈ R 3×3 denote the 3 × 3 convolutional filter, indexed as w ij for i, j ∈ {-1, 0, 1}. We denote by w * x ∈ R n×n the output of the convolution operation on the image x. Then the output of this network can be written as l(x) = n i=1 n j=1 σ(w * x) ij , where σ is the ReLU non-linearity applied point-wise. In particular, this network applies a single 3x3 convolutional filter to the input image, then applies a ReLU non-linearity and finally sum-pools over the entire convolutional layer for the output. This is a similar architecture to the one considered in <ref type="bibr" target="#b33">[34]</ref>. As shown in Figure <ref type="figure" target="#fig_5">6</ref>, we see that different saliency methods do act like edge detectors. This suggests that the convolutional structure of the network is responsible for the edge detecting behavior of some of these saliency methods.</p><p>To understand why saliency methods applied to this simple architecture visually appear to be edge detectors, we consider the closed form of the gradient ∂ ∂xij l(x). Let a ij = 1 {(w * x) ij ≥ 0} indicate the activation pattern of the ReLU units in the convolutional layer. Then for i, j ∈ [2, n -1] we have</p><formula xml:id="formula_3">∂ ∂x ij l(x) = 1 k=-1 1 l=-1 σ ((w * x) i+k,j+l )w kl = 1 k=-1 1 l=-1</formula><p>a i+k,j+l w kl (Recall that σ (x) = 0 if x &lt; 0 and 1 otherwise). This implies that the 3 × 3 activation pattern local to pixel x ij uniquely determines ∂ ∂xij . It is now clear why edges will be visible in the produced saliency mask -regions in the image corresponding to an "edge" will have a distinct activation pattern from surrounding pixels. In contrast, pixel regions of the image which are more uniform will all have the same activation pattern, and thus the same value of ∂ ∂xij l(x). Perhaps a similar principle applies for stacked convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The case of edge detectors.</head><p>An edge detector, roughly speaking, is a classical tool to highlight sharp transitions in an image. Notably, edge detectors are typically untrained and do not depend on any predictive model. They are solely a function of the given input image. As some of the saliency methods we saw, edge detection is invariant under model and data transformations.</p><p>In Figure <ref type="figure">1</ref> we saw that edge detectors produce images that are strikingly similar to the outputs of some saliency methods. In fact, edge detectors can also produce pictures that highlight features which coincide with what appears to be relevant to a model's class prediction. However, here the human observer is at risk of confirmation bias when interpreting the highlighted edges as an explanation of the class prediction. In Figure ?? (In Appendix), we show a qualitative comparison of saliency maps of an input image with the same input image multiplied element-wise by the output of an edge detector. The result indeed looks strikingly similar, illustrating that saliency methods mostly use the edges of the image.</p><p>While edge detection is a fundamental and useful image processing technique, it is typically not thought of as an explanation method, simply because it involves no model or training data. In light of our findings, it is not unreasonable to interpret some saliency methods as implicitly implementing unsupervised image processing techniques, akin to edge detection, segmentation, or denoising. To differentiate such methods from model-sensitive explanations, visual inspection is insufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>The goal of our experimental method is to give researchers guidance in assessing the scope of model explanation methods. We envision these methods to serve as sanity checks in the design of new model explanations. Our results show that visual inspection of explanations alone can favor methods that may provide compelling pictures, but lack sensitivity to the model and the data generating process. Invariances in explanation methods give a concrete way to rule out the adequacy of the method for certain tasks. We primarily focused on invariance under model randomization, and label randomization. Many other transformations are worth investigating and can shed light on various methods we did and did not evaluate. Along these lines, we hope that our paper is a stepping stone towards a more rigorous evaluation of new explanation methods, rather than a verdict on existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cascading randomization on Inception v3 (ImageNet).Figure shows the original explanations (first column) for the Junco bird. Progression from left to right indicates complete randomization of network weights (and other trainable variables) up to that 'block' inclusive. We show images for 17 blocks of randomization. Coordinate (Gradient, mixed_7b) shows the gradient explanation for the network in which the top layers starting from Logits up to mixed_7b have been reinitialized. The last column corresponds to a network with completely reinitialized weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Cascading randomization on Inception v3 (ImageNet).Figure shows the original explanations (first column) for the Junco bird. Progression from left to right indicates complete randomization of network weights (and other trainable variables) up to that 'block' inclusive. We show images for 17 blocks of randomization. Coordinate (Gradient, mixed_7b) shows the gradient explanation for the network in which the top layers starting from Logits up to mixed_7b have been reinitialized. The last column corresponds to a network with completely reinitialized weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Similarity Metrics for Cascading Randomization. Figure showing HOGs similarity and SSIM between original input masks and the masks generated as the Inception v3 is randomized in a cascading manner. Caption Note: For SSIM: Inception v3 -ImageNet, IG and gradient input coincide, while GradCAM, Guided GradCAM, and Guided BackProp are clustered together at the top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Explanation for a true model vs. model trained on random labels. Top Left: Absolutevalue visualization of masks for digit 0 from the MNIST test set for a CNN. Top Right: Saliency masks for digit 0 from the MNIST test set for a CNN shown in diverging color. Bottom Left: Spearman rank correlation (with absolute values) bar graph for saliency methods. We compare the similarity of explanations derived from a model trained on random labels, and one trained on real labels. Bottom Right: Spearman rank correlation (without absolute values) bar graph for saliency methods for MLP. See appendix for corresponding figures for CNN, and MLP on Fashion MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Explanations derived for the 1-layer Sum-Pooling Convolution architecture. We show gradient, SmoothGrad, Integrated Gradients, and Guided Back-Prop explanations. (See Appendix for Similarity Metrics).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Model. Consider a linear model f : R d → R defined as f (x) = w • x where w ∈ R d are the model weights. For gradients we have E grad (x) = ∂(w•x) ∂x = w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Similarity Metrics for Cascading Randomization. We show results for Inception v3 on ImageNet, CNN on Fashion MNIST, and MLP on MNIST. See appendix for MLP on Fashion MNIST and CNN on MNIST. In all plots, y axis is the rank correlation between original explanation and the randomized explanation derived for randomization up to that layer/block, while the x axis corresponds to the layers/blocks of the DNN starting from the output layer. The vertical black dashed line indicates where successive randomization of the network begins, which is at the top layer.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Inception v3 -ImageNet</cell><cell cols="3">CNN -Fashion MNIST</cell><cell>MLP-MNIST</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>See Note</cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Correlation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ABS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">original</cell><cell cols="2">logits</cell><cell cols="2">7c 7b 7a 6e 6d 6c 6b 6a 5d 5c 5b 4a 3b 2b 2a 1a</cell><cell>original</cell><cell>fc2</cell><cell>output-fc</cell><cell>conv-hidden2 conv-hidden1</cell><cell>original</cell><cell>output hidden3</cell><cell>hidden2 hidden1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixed</cell><cell>Conv2d</cell><cell></cell><cell></cell></row><row><cell>Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>See Caption Note</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Correlation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No ABS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">original</cell><cell cols="2">logits</cell><cell cols="3">7c 7b 7a 6e 6d 6c 6b 6a 5d 5c 5b 4a 3b 2b 2a 1a</cell><cell>original</cell><cell>fc2</cell><cell>output-fc</cell><cell>conv-hidden2 conv-hidden1</cell><cell>original</cell><cell>output hidden3</cell><cell>hidden2 hidden1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixed</cell><cell>Conv2d</cell><cell></cell><cell></cell></row><row><cell cols="8">Figure 3: HOGs Similarity: Inception v3 -ImageNet</cell><cell></cell><cell></cell><cell>SSIM: Inception v3 -ImageNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>See Caption Note</cell></row><row><cell>original</cell><cell>logits</cell><cell cols="6">7c 7b 7a 6e 6d 6c 6b 6a 5d 5c 5b 4a 3b 2b 2a 1a</cell><cell></cell><cell cols="2">original</cell><cell>logits</cell><cell>7c 7b 7a 6e 6d 6c 6b 6a 5d 5c 5b 4a 3b 2b 2a 1a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixed</cell><cell>Conv2d</cell><cell></cell><cell></cell><cell>Mixed</cell><cell>Conv2d</cell></row></table><note><p>Top: Spearman Rank correlation with absolute values, Bottom: Spearman Rank correlation without absolute values. Caption Note: For Inception v3 on ImageNet no ABS, the IG, gradient-input, and gradients all coincide. For MLP-MNIST IG and gradient-input coincide.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>All code to replicate our findings will be available here: https://goo.gl/hBmhDt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We refer here to the broad category of visualization and attribution methods aimed at interpreting trained models. These methods are often used for interpreting deep neural networks particularly on image data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We normalize the maps to the range [-1.0, 1.0]. Normalizing in this manner potentially ignores peculiar characteristics of some saliency methods. For example, Integrated gradients has the property that the attributions sum up to the output value. This property cannot usually be visualized. We contend that such properties will not affect the manner in which the output visualizations are perceived.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>See appendix for a discussion on calibration of these metrics.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the Google PAIR team for open source implementation of the methods used in this work. We thank Martin Wattenberg and other members of the Google Brain team for critical feedback that helped improved the work. Lastly, we thank anonymous reviewers for feedback that helped improve the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making machine learning models interpretable</title>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Vellido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>David Martín-Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><forename type="middle">Jg</forename><surname>Lisboa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accountability of ai under the law: The role of explanation</title>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><surname>Kortz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Budish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bavitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Schieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Waldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01134</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">European union regulations on algorithmic decision-making and a&quot; right to explanation</title>
		<author>
			<persName><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08813</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Debugging machine learning models</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Cadamuro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Reliable Machine Learning in the Wild</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interpretability issues in fuzzy modeling</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Magdalena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Interpretable &amp; explorable approximations of black box models</title>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01154</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Fulton</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.05189</idno>
		<title level="m">Causal falling rule lists</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning how to explain neural networks: Patternnet and patternattribution</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName><surname>Schütt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkn7CBaTW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visualizing deep neural network decisions: Prediction difference analysis</title>
		<author>
			<persName><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tameem</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04595</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Not just a black box: Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01713</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/sundararajan17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">Smoothgrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6970" to="6979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07450</idno>
		<title level="m">Grad-cam: Why did you say that? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03296</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to explain: An informationtheoretic perspective on model interpretation</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/chen18j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">University of Montreal</title>
		<imprint>
			<biblScope unit="volume">1341</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Mãžller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10547</idno>
		<title level="m">Interpretation of neural networks is fragile</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><surname>Dähne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00867</idno>
		<title level="m">Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A theoretical explanation for perplexing behaviors of backpropagation-based visualizations</title>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Salient deconvolutional networks</title>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="120" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating the visualization of what a deep neural network has learned</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2660" to="2673" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Methods for interpreting and understanding deep neural networks</title>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ICLR</title>
		<meeting>5th ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic shadow detection in 2d ultrasound</title>
		<author>
			<persName><forename type="first">Qingjie</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Housden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Matthew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards better understanding of gradient-based attribution methods for deep neural networks</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th ICLR</title>
		<meeting>6th ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bipin</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1089" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Understanding intermediate layers using linear classifier probes</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10925</idno>
		<title level="m">Deep image prior</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
