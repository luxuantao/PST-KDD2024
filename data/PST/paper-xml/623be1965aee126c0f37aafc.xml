<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Prompt Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Menglin</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">MMSegmentation&apos;s reproduction on SETR</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Prompt Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. * Equal contribution. 1 As pointed out in [4], all state-of-the-art models in contemporary NLP are now powered by a few Transformer-based models (e.g., BERT [14], T5 [62], BART [42], GPT-3 [5]) This also applies to vision-language field recently, i.e., CLIP [61].</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For a variety of recognition applications, the most accurate results are now obtained by adapting large foundation models pre-trained on massive curated or raw data, a finding that mirrors developments in natural language processing (NLP) <ref type="bibr" target="#b9">[4]</ref>. 1 At first glance,this is a success story: one can make rapid progress on multiple recognition problems simply by leveraging the latest and greatest foundation model. In practice, however, adapting these large models to downstream tasks presents its own challenges. The most obvious (and often the most effective) adaptation strategy is full fine-tuning of the pre-trained model on the task at hand, end-to-end. However, this strategy requires one to store and deploy a separate copy of the backbone parameters for every single task. This is an expensive and often infeasible proposition, especially for modern Transformerbased architectures, which are significantly larger than their convolutional neural networks (ConvNet) counterparts, e.g., ViT-Huge <ref type="bibr" target="#b21">[16]</ref> (632M parameters) vs. ResNet-50 <ref type="bibr" target="#b32">[27]</ref> (25M parameters). We therefore ask, what is the best way to adapt large pre-trained Transformers to downstream tasks in terms of effectiveness and efficiency?  One straightforward approach is to turn to other strategies that we have perfected for adapting ConvNets to new tasks, as in Fig. <ref type="figure" target="#fig_1">1(a)</ref>. A popular approach is to fine-tune only a subset of the parameters, such as the classifier head [52, <ref type="bibr" target="#b37">32,</ref><ref type="bibr" target="#b14">9]</ref> or the bias terms <ref type="bibr" target="#b11">[6]</ref>. Prior research has also looked at adding additional residual blocks (or adapters) to the backbone <ref type="bibr" target="#b51">[64,</ref><ref type="bibr" target="#b68">81]</ref>. One could implement similar strategies for Transformers. However, in general these strategies under-perform full fine-tuning in accuracy.</p><p>We explore a different route in this paper. Instead of altering or fine-tuning the pre-trained Transformer itself, we modify the input to the Transformer. Drawing inspiration from the recent advances on Prompting in NLP <ref type="bibr">[46,</ref><ref type="bibr">44,</ref><ref type="bibr">41,</ref><ref type="bibr">47]</ref>, we propose a new simple and efficient method to adapt transformer models for downstream vision tasks (Fig. <ref type="figure" target="#fig_1">1(b</ref>)), namely Visual-Prompt Tuning (VPT). Our method only introduces a small amount of task-specific learnable parameters into the input space while freezing the entire pre-trained Transformer backbone during downstream training. In practice, these additional parameters are simply prepended into the input sequence of each Transformer layer and learned together with a linear head during fine-tuning.</p><p>On 24 downstream recognition tasks spanning different domains using a pretrained ViT backbone, VPT beats all other transfer learning baselines, even surpassing full fine-tuning in 20 cases, while maintaining the advantage of storing remarkably fewer parameters (less than 1% of backbone parameters) for each individual task (Fig. <ref type="figure" target="#fig_1">1(c)</ref>). This result demonstrates the distinctive strength of visual prompting: whereas in NLP, prompt tuning is only able to match full fine-tuning performance under certain circumstances <ref type="bibr">[41]</ref>. VPT is especially effective in the low-data regime, and maintains its advantage across data scales. Finally, VPT is competitive for a range of Transformer scales and designs (ViT-Base/Large/Huge, Swin). Put together, our results suggest that VPT is one of the most effective ways of adapting ever-growing vision backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer models <ref type="bibr" target="#b55">[68]</ref> have gained huge success in NLP <ref type="bibr" target="#b19">[14,</ref><ref type="bibr" target="#b49">62,</ref><ref type="bibr" target="#b10">5]</ref>. The triumph of the Transformer architecture also extends to various computer vision tasks, including image classification <ref type="bibr" target="#b21">[16,</ref><ref type="bibr">48]</ref>, object detection <ref type="bibr" target="#b12">[7,</ref><ref type="bibr">45]</ref>, semantic segmentation <ref type="bibr" target="#b53">[66,</ref><ref type="bibr" target="#b70">83]</ref>, panoptic segmentation <ref type="bibr" target="#b60">[73]</ref>, video understanding <ref type="bibr" target="#b26">[21]</ref> and fewshot learning <ref type="bibr" target="#b20">[15]</ref>, surpassing previous state-of-the-art approaches. Transformers are also being widely used in recent self-supervised pre-training methods <ref type="bibr" target="#b14">[9,</ref><ref type="bibr" target="#b31">26,</ref><ref type="bibr" target="#b7">2]</ref>. Given their superior performance and much larger scale compared to ConvNets, how to efficiently adapt Transformers to different vision tasks remains an important open problem. Our proposed VPT provides a promising path forward.</p><p>Transfer learning has been extensively studied for vision tasks in the context of ConvNets <ref type="bibr" target="#b73">[86]</ref> and many techniques have been introduced including side tuning <ref type="bibr" target="#b68">[81]</ref>, residual adapter <ref type="bibr" target="#b50">[63]</ref>, bias tuning <ref type="bibr" target="#b11">[6]</ref>, etc. Relatively little attention has been paid to vision Transformers adaptation and how well these aforementioned methods perform on this brand new type of architecture remains unknown. On the other hand, given the dominance of large-scale pre-trained Transformerbased Language Models (LM) <ref type="bibr" target="#b19">[14,</ref><ref type="bibr" target="#b49">62,</ref><ref type="bibr" target="#b10">5]</ref>, many approaches <ref type="bibr" target="#b30">[25,</ref><ref type="bibr" target="#b29">24,</ref><ref type="bibr" target="#b36">31]</ref> have been proposed to efficiently fine-tune LM for different downstream NLP tasks <ref type="bibr" target="#b59">[72,</ref><ref type="bibr" target="#b58">71]</ref>. Among them, we focus on the following two representative methods in our experiments for benchmarking purposes: Adapters <ref type="bibr" target="#b47">[60]</ref> and BitFit <ref type="bibr" target="#b66">[79]</ref>.</p><p>Adapters <ref type="bibr" target="#b35">[30]</ref> insert extra lightweight modules inside each Transformer layer. One adapter module generally consists of a linear down-projection, followed by a nonlinear activation function, and a linear up-projection, together with a residual connection <ref type="bibr" target="#b46">[59,</ref><ref type="bibr" target="#b47">60]</ref>. Instead of inserting new modules, <ref type="bibr" target="#b11">[6]</ref> proposed to update the bias term and freeze the rest of backbone parameters when fine-tuning ConvNets. BitFit <ref type="bibr" target="#b7">[2]</ref> applied this technique to Transformers and verified its effectiveness on LM tuning. Our study demonstrates that VPT, in general, provides improved performance in adapting Transformer models for vision tasks, relative to the aforementioned two well-established methods in NLP.</p><p>Prompting <ref type="bibr">[46]</ref> originally refers to prepending language instruction to the input text so that a pre-trained LM can "understand" the task. With manually chosen prompts, GPT-3 shows strong generalization to downstream transfer learning tasks even in the few-shot or zero-shot settings <ref type="bibr" target="#b10">[5]</ref>. In addition to the follow-up works on how to construct better prompting texts <ref type="bibr" target="#b52">[65,</ref><ref type="bibr" target="#b38">33]</ref>, recent works propose to treat the prompts as task-specific continuous vectors and directly optimize them via gradients during fine-tuning, namely Prompt Tuning <ref type="bibr">[44,</ref><ref type="bibr">41,</ref><ref type="bibr">47]</ref>. Compared to full fine-tuning, it achieves comparable performance but with 1000? less parameter storage. Although prompting has also been applied to visionlanguage models recently <ref type="bibr" target="#b48">[61,</ref><ref type="bibr" target="#b72">85,</ref><ref type="bibr" target="#b40">35,</ref><ref type="bibr" target="#b64">77,</ref><ref type="bibr" target="#b23">18]</ref>, prompting is still limited to the input of text encoders. Due to the disparity between vision and language modalities, in this paper we ask: can the same method can be applied successfully to vision backbones? To the best of our knowledge, we are the first work to tackle this question and investigate the generality and feasibility of visual prompting.   </p><formula xml:id="formula_0">? ! ! ! " ! # CLS Embed " ! # ! ! ! " ! # ! " $ % # % % ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We propose Visual-Prompt Tuning (VPT) for adapting large pre-trained vision Transformer models. VPT injects a small number of learnable parameters into Transformer's input space and keeps the backbone frozen during the downstream training stage. The overall framework is presented in Fig. <ref type="figure" target="#fig_3">2</ref>. We first define the notations in Sec. 3.1, then describe VPT formally in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>For a plain Vision Transformer (ViT) <ref type="bibr" target="#b21">[16]</ref> with N layers, an input image is divided into m fixed-sized patches {I j ? R 3?h?w | j ? N, 1 ? j ? m}. h, w are the height and width of the image patches. Each patch is then first embedded into d-dimensional latent space with positional encoding:</p><formula xml:id="formula_1">e j 0 = Embed(I j ) e j 0 ? R d , j = 1, 2, . . . m .<label>(1)</label></formula><p>We denote the collection of image patch embeddings,</p><formula xml:id="formula_2">E i = {e j i ? R d | j ? N, 1 ? j ? m}, as inputs to the (i+1)-th Transformer layer (L i+1</formula><p>). Together with an extra learnable classification token ([CLS]), the whole ViT is formulated as:</p><formula xml:id="formula_3">[x i , E i ] = L i ([x i-1 , E i-1 ]) i = 1, 2, . . . , N<label>(2)</label></formula><formula xml:id="formula_4">y = Head(x N ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">x i ? R d denote [CLS]'s embedding at L i+1 's input space. [?,</formula><p>?] indicates stacking and concatenation on the sequence length dimension, i.e., [x i , E i ] ? R (1+m)?d . Each layer L i consists of Multiheaded Self-Attention (MSA) and Feed-Forward Networks (FFN) together with LayerNorm <ref type="bibr" target="#b6">[1]</ref> and residual con-nections <ref type="bibr" target="#b32">[27]</ref>. A neural classification head is used to map the final layer's [CLS] embedding, x N , into a predicted class probability distribution y. <ref type="foot" target="#foot_0">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual-Prompt Tuning (VPT)</head><p>Given a pre-trained Transformer model, we introduce a set of p continuous embeddings of dimension d, i.e., prompts, in the input space after the Embed layer.</p><p>Only the task-specific prompts are being updated during fine-tuning, while the Transformer backbone is kept frozen. Depending on the number of Transformer layers involved, our approach has two variants, VPT-shallow and VPT-deep, as shown in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>VPT-Shallow. Prompts are inserted into the first Transformer layer L 1 only. Each prompt token is a learnable d-dimensional vector. A collection of p prompts is denoted as</p><formula xml:id="formula_6">P = {p k ? R d | k ? N, 1 ? k ? p}, the shallow-prompted ViT is: [x 1 , Z 1 , E 1 ] = L 1 ([x 0 , P, E 0 ])<label>(4)</label></formula><formula xml:id="formula_7">[x i , Z i , E i ] = L i ([x i-1 , Z i-1 , E i-1 ]) i = 2, 3, . . . , N<label>(5)</label></formula><formula xml:id="formula_8">y = Head(x N ) ,<label>(6)</label></formula><p>where Z i ? R p?d represents the features computed by the i-th Transformer layer, and</p><formula xml:id="formula_9">[x i , Z i , E i ] ? R (1+p+m)?d .</formula><p>The colors ? and ? indicate learnable and frozen parameters, respectively. Notably for ViT, x N is invariant to the location of prompts since they are inserted after positional encoding, e.g., [x 0 , P, E 0 ] and [x 0 , E 0 , P] are mathematically equivalent. This also applies to VPT-Deep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VPT-Deep.</head><p>Prompts are introduced at every Transformer layer's input space. For (i+1)-th Layer L i+1 , we denote the collection of input learnable prompts as</p><formula xml:id="formula_10">P i = {p k i ? R d | k ? N, 1 ? k ? m}.</formula><p>The deep-prompted ViT is formulated as:</p><formula xml:id="formula_11">[x i , , E i ] = L i ([x i-1 , P i-1 , E i-1 ]) i = 1, 2, . . . , N<label>(7)</label></formula><formula xml:id="formula_12">y = Head(x N ) .<label>(8)</label></formula><p>Storing Visual Prompts. VPT is beneficial in presence of multiple downstream tasks. We only need to store the learned prompts and classification head for each task and re-use the original copy of the pre-trained Transformer model, significantly reducing the storage cost. For instance, given a ViT-Base with 86 million (M) parameters and d = 768, 50 shallow prompts and deep prompts yield additional p ? d = 50 ? 768 = 0.038M, and N ? p ? d = 0.46M parameters, amounting to only 0.04% and 0.53% of all ViT-Base parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate VPT for a wide range of downstream recognition tasks with pretrained Transformer backbones across scales. We first describe our experimental setup in Sec. 4.1, including the pre-trained backbone and downstream tasks, and a brief introduction of alternative transfer learning methods. Then we demonstrate the effectiveness and practical utility of our method in Sec. 4.2. We also systematically study how different design choices would affect performance (Sec. 4.3), which leads to an improved understanding of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Pre-trained Backbones. We experiment with two Transformer architectures in vision, Vision Transformers (ViT) <ref type="bibr" target="#b21">[16]</ref> and Swin Transformers (Swin [48]). All backbones in this section are pre-trained on ImageNet-21k <ref type="bibr" target="#b18">[13]</ref>. We follow the original configurations, e.g., number of image patches divided, existence of [CLS], etc. More details are included in Appendix A.</p><p>Baselines. We compare both variants of VPT with other commonly used finetuning protocols:</p><p>(a) Full: fully update all backbone and classification head parameters.</p><p>(b) Methods that focus on the classification head. They treat the pre-trained backbone as a feature extractor, whose weights are fixed during tuning: -Linear: only use a linear layer as the classification head.</p><p>-Partial-k: fine-tune the last k layers of backbone while freezing the others, as adopted in <ref type="bibr" target="#b65">[78,</ref><ref type="bibr" target="#b69">82,</ref><ref type="bibr" target="#b43">56,</ref><ref type="bibr" target="#b31">26]</ref>. It redefines the boundary of backbone and classification head. -Mlp-k: utilize a multilayer perceptron (MLP) with k layers, instead of a linear layer, as classification head. (c) Methods that update a subset backbone parameters or add new trainable parameters to backbone during fine-tuning: -Sidetune [81]: train a "side" network and linear interpolate between pretrained features and side-tuned features before being fed into the head. -Bias <ref type="bibr" target="#b11">[6,</ref><ref type="bibr" target="#b66">79]</ref>: fine-tune only the bias terms of a pre-trained backbone.</p><p>-Adapter <ref type="bibr" target="#b35">[30,</ref><ref type="bibr" target="#b46">59,</ref><ref type="bibr" target="#b47">60]</ref>: insert new MLP modules with residual connection inside Transformer layers.</p><p>Downstream Tasks. We experiment on the following two collections of datasets: FGVC consists of 5 benchmarked Fine-Grained Visual Classification tasks including CUB-200-2011 <ref type="bibr" target="#b57">[70]</ref>, NABirds <ref type="bibr" target="#b54">[67]</ref>, Oxford Flowers <ref type="bibr" target="#b42">[55]</ref>, Stanford Dogs [37] and Stanford Cars <ref type="bibr" target="#b24">[19]</ref>. If a certain dataset only has train and test sets publicly available, we randomly split the training set into train (90%) and val (10%), and rely on val to select hyperparameters.</p><p>VTAB-1k <ref type="bibr" target="#b67">[80]</ref> is a collection of 19 diverse visual classification tasks, which are organized into three groups: Natural -tasks that contain natural images captured using standard cameras; Specialized -tasks that contain images captured via specialized equipment, such as medical and satellite imagery; and Structured -tasks that require geometric comprehension like object counting. Each task of VTAB contains 1000 training examples. Following <ref type="bibr" target="#b67">[80]</ref>, we use the provided 800-200 split of the train set to determine hyperparameters and run the final evaluation using the full training data. We report the average accuracy score on test set within three runs. We report the average accuracy on the FGVC datasets, and the average accuracy on each of the three groups in VTAB. The individual results on each task are in Appendix C, as are image examples of these aforementioned tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Tab. 1 presents the results of fine-tuning a pre-trained ViT-B/16 on averaged across 4 diverse downstream task groups, comparing VPT to the other 7 tuning protocols. We can see that: 1. VPT-Deep outperforms Full (Tab. 1(a)) on 3 out of the 4 problem classes (20 out of 24 tasks), while using significantly fewer total model parameters (1.18? vs. 24.02?). Thus, even if storage is not a concern, VPT is a promising approach for adapting larger Transformers in vision. Note that this result is in contrast to comparable studies in NLP, where prompt tuning matches, but does not exceed full fine-tuning [41]. 2. VPT-Deep outperforms all the other parameter-efficient tuning protocols (Tab. 1(b,c)) across all task groups, indicating that VPTdeep is the best fine-tuning strategy in storage-constrained environments. 3. Although sub-optimal than VPT-deep, VPT-shallow still offers non-trivial performance gain than head-oriented tuning methods in Tab. 1(b), indicating that VPT-shallow is a worthwhile choice in deploying multi-task fine-tuned models if the storage constraint is severe. We vary the training data between 10% and 80% and compare all methods. The same pre-trained ViT-B is used for downstream training. Task-averaged results for each method on different training data scales are presented in Fig. <ref type="figure">3</ref>. Fig. <ref type="figure">3</ref> shows that VPT-deep outperforms all the other baselines across data scales. Digging deeper, methods that use less trainable parameters, i.e., VPT, Linear, Adapter, Bias, dominate over Full in the low-data regimes. This trend, however, is reversed when more training data is available for Linear and Adapter. In contrast, VPT-deep still consistently outperforms Fullacross training data sizes. Although Bias offers similar advantages, it still marginally under-performs VPT-deep across the board (Fig. <ref type="figure">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>right).</head><p>VPT on different backbone scales. Fig. <ref type="figure">4</ref> shows VTAB-1k performance under 3 different backbone scales: ViT-Base/Large/Huge. VPT-deep is significantly better than Linear and VPT-shallow across all 3 backbone choices and 3 subgroups of VTAB-1k. More importantly, the advantages of VPT-deep VPT on hierarchical Transformers. We extend VPT to Swin [48], which employs MSA within local shifted windows and merges patch embeddings at deeper layers. For simplicity and without loss of generality, we implement VPT in the most straightforward manner: the prompts are attended within the local windows, but are ignored during patch merging stages. The experiments are conducted on the ImageNet-21k supervised pre-trained Swin-Base. VPT continues to outperform other parameter-efficient fine-tuning methods (b, c) for all three subgroups of VTAB Tab. 2, though in this case Full yields the highest accuracy scores overall (at a heavy cost in total parameters).</p><p>It is surprising that the advantage of VPT-deep over VPT-shallow diminishes for Natural : VPT-shallow yields slightly better accuracy scores than full fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation on Model Design Variants</head><p>We ablate different model design choices on the supervised ImageNet-21k pretrained ViT-Base and evaluate them on VTAB, with same setup in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Location. An important distinction between VPT and other methods</head><p>is the extra learnable parameters introduced as inputs for the Transformer layers. Fig. <ref type="figure">5</ref> ablates different choices on how and where to insert prompts in the input space, and how they would affect the final performance.</p><p>Prepend or Add? Instead of prepending prompts to the sequence of the image patches embeddings E i as described in Sec. 3.2, another option is to directly add prompts element-wise to those embeddings, keeping the Transformer's input sequence length the same as before. Though this variant is competitive to Full in some cases (e.g., VTAB-Natural ), its performance generally falls behind with Latent or pixel space? Instead of inserting the prompts as latent vectors for the first Transformer layer, one could introduce prompts in the pixel level before the Embed layer in Eq. (1), i.e., Prepend-pixel and Concat-channel. Fig. <ref type="figure">5</ref> shows that the adaption performance decreases for these two variants. For example, the accuracy score of prepending shallow prompts before the projection layer (Prepend-pixel) drops 6.9%, compared to the default prepending in the embedding space (Prepend) on VTAB-Natural. The performance further deteriorates (even as large as 30 accuracy scores drop on VTAB-Natural ) if we instead concatenate a new channel to the input image (Concat-channel). These observations suggest that it's easier for prompts to learn condensed task-dependent signals in the latent input space of Transformers. Prompt Length. This is the only additional hyper-parameter needed to tune for VPT compared to full fine-tuning. For easy reference, we also ablate two other baselines on their individual additional hyper-parameters, i.e., number of layers for Mlp and reduction rate for Adapter. As shown in Fig. <ref type="figure">6</ref>, the optimal prompt length varies across tasks. Notably, even with as few as only one prompt, VPT-deep still significantly outperforms the other 2 baselines, and remains competitive or even better compared to full fine-tuning on VTAB-Structured and Natural.</p><p>Prompt Depth. Fig. <ref type="figure">7</ref> ablates which and how many layers to insert prompts. Each variant reports the best prompt length selected with val set. VPT's performance is positively correlated with the prompt depth in general. Yet the accuracy drops if we insert prompts from top to bottom, suggesting that prompts at earlier Transformer layers matter more than those at later layers.</p><p>Final Output. Following the original configuration of ViT, we use the final embedding of [CLS], i.e., x N , as the classification head input, which is also the default setting in our ViT experiments. As shown in Fig. <ref type="figure">8</ref>, if we use the average pooling on image patch output embeddings E N as final output (Image-pool), the  results essentially remain the same (e.g., 82.4 vs. 82.3 for VTAB-Specialized ). However, if the pooling involves final prompt outputs Z N (Prompt-pool and Global-pool), the accuracy could drop as large as 8 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head><p>Visualization. Fig. <ref type="figure">9</ref> shows t-SNE [51] visualizations of x N , i.e., embeddings of [CLS] after the last Transformer layer and before the classification head, for 3 tasks in VTAB (SVNH <ref type="bibr" target="#b41">[54]</ref>, EuroSAT <ref type="bibr" target="#b33">[28]</ref>, Clevr/count <ref type="bibr" target="#b39">[34]</ref>), one for each subgroup. All plots show that VPT-deep enables linearly separable representations while using less parameters than Full. We also observe that extra tunable parameters for every Transformer layer (VPT-deep) improve the performance, compared to VPT-shallow, which only inserts prompts for the first layer's input. Interestingly on Clevr/count (Fig. <ref type="figure">9</ref>(c)), VPT-deep and Full recover the underlying manifold structure of the task (counting objects in images vs. street number or landscape recognition), unlike VPT-shallow and Linear.</p><p>Combine VPT with Bias Tuning. Previous sections reveal that Bias is a competitive parameter-efficient tuning baseline (e.g., Tab. 1(c)). Based on this observation, we explore another protocol where we update both prompts and the bias terms of the pre-trained backbone, keeping everything else in the backbone frozen (VPT+Bias). As shown in Tab. 3, to our surprise, incorporating? Bias with VPT does not yield superior results in general, even undermines VPTdeep for all 3 task subgroups. This suggests that these two methods are not necessarily complementary to each other. Apply VPT to more vision tasks. We explore the feasibility of VPT beyond visual classification, by evaluating ADE20K <ref type="bibr" target="#b71">[84]</ref> semantic segmentation task with a Transformer model, SETR-PUP <ref type="bibr" target="#b70">[83]</ref>. It adds a standard ConvNet head to the ViT backbone to perform segmentation. The de-facto approach is still fully fine-tuning the pre-trained backbone together with the ConvNet head (Full). We include two more protocols for comparison: only update the head layers (Head Only), update head layers and bias vectors in the backbone (Bias). In Tab. 4, we report val mIoU results with and without multi-scale inference. Though parameter-efficient protocols could not compete with Full, VPT is still comparable with Bias. Notably, VPT offers competitive results to a fully fine-tuned state-of-the-art ConvNet model (DeepLab v3+ <ref type="bibr" target="#b13">[8]</ref>), while tuning significantly less parameters (15M vs. 64M, respectively).</p><p>Apply VPT to more pre-training methods. In addition to the backbones pre-trained with labeled data, we experiment with two self-supervised objectives: MAE <ref type="bibr" target="#b31">[26]</ref> and MoCo v3 <ref type="bibr" target="#b14">[9]</ref>. Tab. 5 reports the results on VTAB-1k with ViT-B. We observe that both variants of VPT surpass Linear, yet the comparisons among other techniques are less conclusive. For MAE, other parameter-efficient methods, e.g., Partial-1, outperform both VPT and Linear. In the case of MoCo v3, VPT no longer holds the best performance, though it is still competitive with the others. This suggests that these two self-supervised ViTs are fundamentally different from the supervised ones in previous sections. Exactly why and how these differences arise remain open questions.</p><p>Apply VPT to ConvNets. We examine the idea of adding trainable parameters in the input space of ConvNets: padding both height and width by p learnable prompt pixels for the input image. Though this operation seems unconventional, we implement VPT this way given there is no obvious solution to add location-invariant prompts similar to the Transformer counterparts. In fact  this approach has been explored before in the adversarial attack literature <ref type="bibr" target="#b22">[17]</ref>. The value of p in our experiment is 2 orders of magnitude smaller than previous work: e.g., 5 vs. 263. Most importantly, we cast this idea in the lens of transfer learning.</p><p>Tab. 6 presents the results for ConvNeXt-B [49] (pre-trained on ImageNet-21k) and ResNet-50 <ref type="bibr" target="#b32">[27]</ref> (pre-trained on ImageNet-1k), respectively. VPT works well in a larger ConvNet backbone, ConvNeXt-B, offering accuracy gains over other sparse tuning protocols (b, c), and outperforming Full on 8 out of 19 cases. The advantages of VPT, however, diminish with smaller ConvNet (ResNet-50), as there is no clear winner for all 19 VTAB-1k tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present Visual Prompt Tuning, a new parameter-efficient approach to leverage large vision Transformer models for a wide range of downstream tasks. VPT introduces task-specific learnable prompts in the input space, keeping the pretrained backbone fixed. We show that VPT can surpass other fine-tuning protocols (often including full fine-tuning) while dramatically reducing the storage cost. Our experiments also raise intriguing questions on fine-tuning dynamics of vision Transformers with different pre-training objectives, and how to transfer to broader vision recognition tasks in an efficient manner. We therefore hope our work will inspire future research on how best to tap the potential of large foundation models in vision.</p><p>Acknowledgement Menglin is supported by a Meta AI research grant awarded to Cornell University. We would like to thank Alexander Rush, Yin Cui for valuable suggestions and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>We use PyTorch <ref type="bibr" target="#b45">[58]</ref> to implement all experiments on NVIDIA A100-40GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Classification Experiments</head><p>VPT. We use val set of each dataset to find best prompt length p, see Sec. 3.2. The prompt length is the only VPT-specific hyper-parameter that we tune. For Transformer backbones, the range of p is {1, 5, 10, 50, 100, 200} and {1, 5, 10, 50} for ViT and Swin, respectively. The maximum choice of p is approximately close to the number of image patch tokens within each MSA for both architectures (ViT: 196, Swin: 49). We also apply a dropout of 0.1 for VPT-deep. For ConvNets, the range of p is {1, 3, 5, 7, 9, 11}. Each prompt is randomly initialized with xavier uniform initialization scheme <ref type="bibr" target="#b27">[22]</ref>. We follow the original backbone' design choices, such as the existence of the classification tokens [CLS], or whether or not to use the final [CLS] embeddings for the classification head input.</p><p>Adapter. Adapters <ref type="bibr" target="#b35">[30]</ref> insert extra lightweight modules inside each Transformer layer. One adapter module generally consists of a linear down-projection (with a reduction rate r), followed by a nonlinear activation function, and a linear up-projection, together with a residual connection. <ref type="bibr" target="#b46">[59,</ref><ref type="bibr" target="#b47">60]</ref> exhaustively searched all possible configurations and found that only inserting adapters after the FFN "Add &amp; LayerNorm" sub-layer works the best. Therefore we also use this setup in our own implementation. We sweep the reduction rate r in {8, 64, 256}.</p><p>Augmentation and other hyper-parameters. We adopt standard image augmentation strategy during training: normalize with ImageNet means and standard deviation, randomly resize crop to 224?224 and random horizontal flip for five FGVC datasets, and resize to 224?224 for the VTAB-1k suite. <ref type="foot" target="#foot_1">3</ref> Tab. 7 summarizes the optimization configurations we used. Following [52], we conduct grid search to find the tuning-specific hyper-parameters, learning rate, and weight decay values using val set of each task. Following the linear scaling rule [38, <ref type="bibr" target="#b28">23,</ref><ref type="bibr" target="#b14">9,</ref><ref type="bibr" target="#b31">26]</ref>, the learning rate is set as Table <ref type="table">7</ref>. Implementation details for each fine-tuning method evaluated. ?: we observe that for VPT-shallow sometimes benefit from a larger base LR for 6 out of 24 tasks evaluated, where we search from {1000.0, 500.0, 250.0, 100.0}   Datasets and pre-trained backbones specifications. Tabs. 8 and 9 summarize the statistics and details of the evaluated classification datasets and all the pre-trained backbones used in the paper. Fig. <ref type="figure" target="#fig_1">10</ref> includes image examples of all 24 classification tasks evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Semantic Segmentation Experiments</head><p>ADE20K <ref type="bibr" target="#b71">[84]</ref> is a challenging scene parsing benchmark with 150 fine-grained labels.</p><p>The training and validation sets contain 20,210 and 2,000 images respectively. We utilize the public codebase MMSegmentation <ref type="bibr" target="#b17">[12]</ref> in our implementation. 4 The ViT-L backbone is supervisely pre-trained on ImageNet-21k. 5  SETR <ref type="bibr" target="#b70">[83]</ref> is a competitive segmentation framework using ViT as the encoder. PUP is a progressive upsampling strategy consisting of consecutive convolution layers and bilinear upsampling operations. Among multiple decoder choices, PUP works the best according to MMSegmentation's reproduction therefore we also use it as in our implementation. 6  When applying VPT to SETR-PUP, we only insert prompts into SETR's ViT encoder backbone. For the decoder, only image patch embeddings are used as inputs and prompt embeddings are discarded. Same as recognition tasks, only the PUP decoder head and prompts are learned during training and the ViT backbone is frozen.</p><p>For full fine-tuning, we use the same hyper-parameters as in MMSegmentation. For HeadOnly, Bias, and VPT, we use the hyper-parameter sweep on learning rate {0.05, 0.005, 0.0005, 0.001}. The optimal learning rate is 0.005 for all methods. We sweep prompt length p ? {1, 5, 10, 50, 100, 200}. For VPT, we also change the learning rate multiplier to 1.0 instead of the default 10.0, so the decoder head and prompts share the same learning rate. Other hyper-parameters remain the same as full fine-tuning. Prompt initialization. In NLP, prompt tuning could benefit from more sophisticated prompt initialization, as shown in <ref type="bibr">[41]</ref>. We investigate if this is the case for visual prompting as well. We utilize prototype representations for downstream target classes so that the prompts are initialized with embeddings that enumerate the output space. Since we want the model to produce an output embedding that is close to one of these prototype representations given a test example, initializing prompts in this manner might give the model some hints about the target categories thus help improve the optimization process.</p><p>Concretely, we use the averaged final [CLS] embeddings whithin each target class of the down-stream dataset train split. Given the pre-trained ViT with N layers, and the down-stream train set with c target classes, for each training example, we compute the final [CLS] embeddings, xN ? R d . Then we average these embeddings within each target class to get 7 Setting prompt length p = c, 8 we initialize P with {x k N } k=c k=1 for VPT-shallow , and initialize each Pi with {x k N } k=c k=1 , where i = 0, 1, . . . , N -1, for VPT-deep.</p><formula xml:id="formula_13">{x k N ? R d | k ? N, 1 ? k ? c}.</formula><p>We compare the fine-tuning performance using the above initialization strategy (CLS) against the default random initialization (Random) in Fig. <ref type="figure" target="#fig_10">13</ref>. We also report results when we fix the prompts during the fine-tuning stage (?-fixed). As shown in Fig. <ref type="figure" target="#fig_10">13</ref>, it's quite surprising that our default random initialization (Random) works the best in general, consistently across different subgroups of VTAB without extra   Sensitivity to prompt length for the prompt depth experiments. We select the best prompt length for each variant with val sets. We also include the same prompt length for all depth choices. i ? j indicates the Transformer layer indices that prompts are inserted into. The 1-st layer refers to the one closest to input. ViT-B has a total of 12 layers pre-processing steps described above (CLS). CLS works comparably in Natural and Specialized subgroups.<ref type="foot" target="#foot_2">9</ref> </p><p>Prompt depth vs. prompt length. In Fig. <ref type="figure">7</ref> of the main paper, we ablate the number of layers we insert prompts in. For each prompt depth variant, Fig. <ref type="figure">7</ref> reports the results using the best prompt length for each task ("? ? ? (best)" in Fig. <ref type="figure" target="#fig_11">14</ref>). Here we adopt another setting where the best prompt length from 1 ? 12 are used for all other prompt depth variants. Comparing both "? ? ? (best)" and "? ? ?", we observe that there are varied sensitivities to prompt length for different depths, especially if we insert prompts in nine layers only (3 ?12, 12 ? 3).  Ensemble -Average min max Fig. <ref type="figure" target="#fig_1">15</ref>. Performance of a five-run ensemble. We report the averaged, the best among five runs as well. Best performance is bolded in each column Prompt ensembling.</p><p>[41] demonstrated prompt's efficiency in the context of model ensembling. For an ensemble of k models, we only need to store the learnt prompt vectors instead of k copies of the whole fine-tuned model parameters (e.g., k?2.5GB for ViT-H). Furthermore, given one test example during inference time, only one forward pass is executed with a specially-designed batch with replicated original data but varied prompts.</p><p>Given such advantages, we also investigate VPT's effectiveness on prompt ensembling. We train 5 different prompts for each VTAB task with different random seeds, using the same pre-trained ViT-B backbone and hyper-parameters as in Tab. 1. Fig. <ref type="figure" target="#fig_1">15</ref> shows that the ensembled VPT-deep outperforms the average or even the best singleprompt counterparts, as well as other ensembled fine-tuning methods including Full.</p><p>Test of statistical significance. We conduct non-parametric paired one-tailed t-test (the Wilcoxon signed-rank test <ref type="bibr" target="#b62">[75]</ref>) on whether VPT-deep's performance is greater than other fine-tuning methods across 19 VTAB tasks (the null hypothesis H0 states that the mean VTAB performance difference between VPT-deep and alternate baseline method is zero. The alternative hypothesis H1 states that VPT-deep outperforms the baseline method on VTAB). Tab. 10 presents the p-values of each test, with the number of observations equal to 19 for each method compared (we use the averaged accuracy scores among 5 runs for 19 VTAB tasks and all fine-tuning methods). For all of the fine-tuning protocols compared, VPT-deep's improvements are statistically significant (p &lt; 0.05). We also conduct un-paired one-tailed t-test with unequal variances (Welch's ttest <ref type="bibr" target="#b61">[74]</ref>), comparing the individual runs (the number of observations = 5) for each VTAB task (H0 states that VPT-deep and the other baseline perform the same for a specific VTAB task, while H1 states that VPT-deep outperforms the other baseline for a specific VTAB task). Fig. <ref type="figure" target="#fig_1">16</ref> presents the p-values for each &lt;VPT-deep, baseline method&gt; pair on each task. We reject H0 on 127 out of 19 ? 8 = 152 cases (p &lt; 0.05). Compared to Full, VPT-deep achieves statistically significant better performance on 11 out of 19 tasks.</p><p>Effect of different fine-tuning hyper-parameters. In Fig. <ref type="figure" target="#fig_13">17</ref>, we present different tuning protocol's performance on different fine-tuning hyper-parameters including learning rate and weight decay. For our proposed VPT-deep, we also ablate different choices of prompt length p, which is the only hyper-parameter that needs to be manually tuned. All experiments are evaluated on the val set of KITTI/Distance task (VTAB-Specialized ). We observe different behaviors between Linear and VPT. Both methods freeze backbone parameters during fine-tuning stage. Linear probing is more sensitive to weight decay values in general, whereas VPT is influenced by both learning rate and weight decay values. VPT with larger prompt length is also less sensitive to the choice of learning rate.</p><p>Effect of image resolution. The original ViT paper <ref type="bibr" target="#b21">[16]</ref> found that fine-tuning with higher image resolutions (384?384) is beneficial to downstream recognition tasks. All recognition experiments presented in the main paper are fine-tuned on 224?224 resolution. As shown in Tab. 11, we re-run the VTAB experiments with the same setup as in Tab. 1 but in the 384 resolution instead of the default 224. We can see that, VPT-deep still achieves the best performance among all parameter-efficient tuning  <ref type="table" target="#tab_1">11</ref>. ViT-B/16 pre-trained on supervised ImageNet-21k, fine-tuned with resolution 384?384. We also include VPT with image resolution 224?224, p = 380, so the effective image resolution is 384?384. For each method and each downstream task group, we report the average test accuracy score and number of wins in (?) compared to Full. "Total params" denotes total parameters needed for all <ref type="bibr" target="#b29">24</ref>    Another interesting observation from Tab. 11 is that with 224 fine-tune resolution and a larger value of p = 380, VPT could achieve similar or better performance compared to Full with 384 resolution, while using the same input sequence length yet significantly less trainable parameters. Empirical computational cost. One possible limitation of VPT is the extra input sequence length for Transformers. In theory the complexity of MSA is quadratic w.r.t. the input sequence length, but this might not be the case for real-world speed due to hardware details like lane widths and cache sizes <ref type="bibr" target="#b21">[16]</ref>. In Tab. 12 and Fig. <ref type="figure" target="#fig_14">18</ref>, we study the empirical computational cost, i.e., latency, and peak GPU memory usage at both training and inference times, for all the fine-tuning protocols studied. All experiments use the same A100 GPU with a batch size 64 for both training and inference.</p><p>We can see that the theoretical quadratic scaling w.r.t. sequence length barely happens to VPT. For instance, doubling the length (p = 200 vs. m = 198) basically only lead to 2? (instead of 4?) inference latency and peak GPU memory w.r.t. full fine-tuning. For training, the latency would be largely reduced with less number of prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Supplementary Results</head><p>Numerical results of Table <ref type="table" target="#tab_1">1</ref>. Tabs. 13 and 14 present per-task results for 24 classification tasks evaluated in Tab. 1.</p><p>Per-task results on training data ablations. Fig. <ref type="figure" target="#fig_15">19</ref> presents the per-task results for five FGVC datasets. We observe a similar trend in Fig. <ref type="figure">3</ref>: while all parameterefficient methods outperform full fine-tuning in small-to-medium data regime, VPTdeep consistently surpasses Full across data scales for five FGVC tasks.</p><p>More t-SNE visualizations. In Fig. <ref type="figure" target="#fig_16">20</ref>, We presents more t-SNE visualizations, similar to Fig. <ref type="figure">9</ref>, for all VTAB datasets with less than or equal to 20 target classes.    In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 4582-4597. Association for Computational Linguistics, Online (Aug 2021) 2, 3 45. Li, Y., Xie, S., Chen, X., Dollar, P., He, K., Girshick, R.: Benchmarking detection transfer learning with vision transformers. arXiv preprint arXiv:2111.11429 (2021) 3 46. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586 (2021) 2, 3 47. Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., Tang, J.: P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021) 2, 3 48. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., <ref type="bibr">Guo</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Backbone</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visual-Prompt Tuning (VPT) vs. other transfer learning methods. (a) Current transfer learning protocols are grouped based on the tuning scope: Full fine-tuning, Head-oriented, and Backbone-oriented approaches. (b) VPT instead adds extra parameters in the input space. (c) Performance of different methods on a wide range of downstream classification tasks adapting a pre-trained ViT-B backbone, with mean and standard deviation annotated. VPT outperforms Full fine-tuning 20 out of 24 cases while using less than 1% of all model parameters</figDesc><graphic url="image-27.png" coords="2,230.65,35.15,146.35,111.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed Visual-Prompt Tuning. We explore two variants: (a) prepend a set of learnable parameters to each Transformer encoder layer's input (VPT-deep); (b) only insert the prompt parameters to the first layer's input (VPTshallow). During training on downstream tasks, only the parameters of prompts and linear head are updated while the whole Transformer encoder is frozen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Performance comparison on different downstream data scales, averaged across 5 FGVC tasks. VPT-deep is compared with Linear (left), Adapter (middle) and Bias (right). Highlighted region shows the accuracy difference between VPT-deep and the compared method. Results of VPT-shallow are Full presented in all plots for easy reference. The size of markers are proportional to the percentage of tunable parameters in log scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Ablation on prompt location. We illustrate different location choices at top, and present the results at bottom. For easy comparison, two blue dashed lines represent the performance of the default VPT-deep and VPT-shallow respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Ablation on prompt depth. We select the best prompt length for each variant with val sets. i ? j indicates the Transformer layer indices that prompts are inserted into. The 1-st layer refers to the one closest to input. ViT-B has 12 layers in total</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig. 9. t-SNE visualizations of the final [CLS] embedding xN of 3 VTAB tasks from the test set, from Tab. 1. VPT could produce linearly separable features without updating backbone parameters</figDesc><graphic url="image-100.png" coords="12,35.27,36.49,113.31,120.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .Fig. 12 .</head><label>1012</label><figDesc>Fig. 10. Dataset examples for all classification tasks evaluated</figDesc><graphic url="image-125.png" coords="18,81.68,502.64,90.06,51.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>7</head><label></label><figDesc>if c &gt; 200, we further apply k-means (k = 200) to class-averaged embeddings and use the corresponding 200 centroid embeddings as {x k N ? R d } k=200 k=1 . 8 if c &gt; 200, we set p = 200 so that prompt length won't be too large. In fact, for VTAB, only the Sun397 task in the Natural subgroup has over 200 classes. See Tab. 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Effect of prompt initialization. For easy comparison, the two blue dashed line represents the performance of default VPT-deep and VPT-shallow, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.<ref type="bibr" target="#b19">14</ref>. Sensitivity to prompt length for the prompt depth experiments. We select the best prompt length for each variant with val sets. We also include the same prompt length for all depth choices. i ? j indicates the Transformer layer indices that prompts are inserted into. The 1-st layer refers to the one closest to input. ViT-B has a total of 12 layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Effect of different fine-tuning hyperparameters. Evaluated on the VTAB-Specialized : KITTI/Distance task. Other tuning methods are shaded in gray</figDesc><graphic url="image-188.png" coords="24,296.17,207.50,83.31,71.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 .</head><label>18</label><figDesc>Fig.<ref type="bibr" target="#b23">18</ref>. Peak GPU memory and latency (ms/img) during both training (left) and inference time (right). For easy comparison, the gray dashed lines represent latency and memory of full fine-tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 .</head><label>19</label><figDesc>Fig.<ref type="bibr" target="#b24">19</ref>. Effect of downstream data size, for each of FGVC tasks. The size of markers are proportional to the percentage of tunable parameters in log scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. More t-SNE visualization of the final [CLS] embedding xN of more VTAB tasks. We include tasks that have less or equal to 20 target classes for visualization</figDesc><graphic url="image-198.png" coords="29,35.23,357.20,113.30,120.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 3, 5, 6, 9, 17 49. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. CVPR (2022) 14, 17 50. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017) 16 51. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(11) (2008) 12 52. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., Van Der Maaten, L.: Exploring the limits of weakly supervised pretraining. In: ECCV (2018) 2, 15 53. Matthey, L., Higgins, I., Hassabis, D., Lerchner, A.: dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/ (2017) 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>ViT-B/16 pre-trained on supervised ImageNet-21k. For each method and each downstream task group, we report the average test accuracy score and number of wins in (?) compared to Full. "Total params" denotes total parameters needed for all 24 downstream tasks. "Scope" denotes the tuning scope of each method. "Extra params" denotes the presence of additional parameters besides the pre-trained backbone and linear head. Best results among all methods except Full are bolded. VPT outshines the full fine-tuning 20 out of 24 cases with significantly less trainable parameters</figDesc><table><row><cell></cell><cell>ViT-B/16 (85.8M)</cell><cell cols="3">Total params Input Backbone params Scope Extra</cell><cell>FGVC</cell><cell cols="3">VTAB-1k Natural Specialized Structured</cell></row><row><cell></cell><cell>Total # of tasks</cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>7</cell><cell>4</cell><cell>8</cell></row><row><cell>(a)</cell><cell>Full</cell><cell>24.02?</cell><cell>?</cell><cell></cell><cell>88.54</cell><cell>75.88</cell><cell>83.36</cell><cell>47.64</cell></row><row><cell></cell><cell>Linear</cell><cell>1.02?</cell><cell></cell><cell></cell><cell cols="2">79.32 (0) 68.93 (1)</cell><cell>77.16 (1)</cell><cell>26.84 (0)</cell></row><row><cell>(b)</cell><cell>Partial-1</cell><cell>3.00?</cell><cell></cell><cell></cell><cell cols="2">82.63 (0) 69.44 (2)</cell><cell>78.53 (0)</cell><cell>34.17 (0)</cell></row><row><cell></cell><cell>Mlp-3</cell><cell>1.35?</cell><cell></cell><cell>?</cell><cell cols="2">79.80 (0) 67.80 (2)</cell><cell>72.83 (0)</cell><cell>30.62 (0)</cell></row><row><cell></cell><cell>Sidetune</cell><cell>3.69?</cell><cell>?</cell><cell>?</cell><cell cols="2">78.35 (0) 58.21 (0)</cell><cell>68.12 (0)</cell><cell>23.41 (0)</cell></row><row><cell>(c)</cell><cell>Bias</cell><cell>1.05?</cell><cell>?</cell><cell></cell><cell cols="2">88.41 (3) 73.30 (3)</cell><cell>78.25 (0)</cell><cell>44.09 (2)</cell></row><row><cell></cell><cell>Adapter</cell><cell>1.23?</cell><cell>?</cell><cell>?</cell><cell cols="2">85.66 (2) 70.39 (4)</cell><cell>77.11 (0)</cell><cell>33.43 (0)</cell></row><row><cell>(ours)</cell><cell>VPT-shallow VPT-deep</cell><cell>1.04? 1.18?</cell><cell>?</cell><cell>?</cell><cell cols="4">84.62 (1) 76.81 (4) 89.11 (4) 78.48 (6) 82.43 (2) 54.98 (8) 79.66 (0) 46.98 (4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Different Transformer architecture: Swin-B pre-trained on supervised ImageNet-21k as backbone. For each method and each downstream task group, we report the average test accuracy score and number of wins in (?) compared to Full. The column "Total params" denotes total parameters needed for all 19 downstream tasks. Best results among all methods except Full are bolded</figDesc><table><row><cell></cell><cell>Swin-B</cell><cell>Total</cell><cell></cell><cell>VTAB-1k</cell><cell></cell></row><row><cell></cell><cell>(86.7M)</cell><cell cols="4">params Natural Specialized Structured</cell></row><row><cell></cell><cell>Total # of tasks</cell><cell></cell><cell>7</cell><cell>4</cell><cell>8</cell></row><row><cell>(a)</cell><cell>Full</cell><cell>19.01?</cell><cell>79.10</cell><cell>86.21</cell><cell>59.65</cell></row><row><cell></cell><cell>Linear</cell><cell cols="2">1.01? 73.52 (5)</cell><cell>80.77 (0)</cell><cell>33.52 (0)</cell></row><row><cell>(b)</cell><cell>Mlp-3</cell><cell cols="2">1.47? 73.56 (5)</cell><cell>75.21 (0)</cell><cell>35.69 (0)</cell></row><row><cell></cell><cell>Partial</cell><cell cols="2">3.77? 73.11 (4)</cell><cell>81.70 (0)</cell><cell>34.96 (0)</cell></row><row><cell>(c)</cell><cell>Bias</cell><cell cols="2">1.06? 74.19 (2)</cell><cell>80.14 (0)</cell><cell>42.42 (0)</cell></row><row><cell>(ours)</cell><cell>VPT-shallow VPT-deep</cell><cell cols="2">1.01? 79.85 (6) 1.05? 76.78 (6)</cell><cell>82.45 (0) 84.53 (0)</cell><cell>37.75 (0) 53.35 (0)</cell></row></table><note><p>over Full indeed still hold as the model scale increases, i.e., VPT-deep significantly outperforms Full on Natural and Structured groups, while offering nearly equivalent performance on Specialized.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Combining</figDesc><table><row><cell>VTAB-Natural</cell><cell>73.30 (3)</cell><cell>76.81 (4)</cell><cell>79.78 (5) ?2.97</cell><cell>78.48 (6)</cell><cell>77.64 (6) ?0.84</cell></row><row><cell cols="2">VTAB-Specialized 78.25 (0)</cell><cell>79.66 (0)</cell><cell>81.38 (0) ?1.72</cell><cell>82.43 (2)</cell><cell>82.22 (2) ?0.21</cell></row><row><cell cols="2">VTAB-Structured 44.09 (2)</cell><cell>46.98 (4)</cell><cell>45.89 (3) ?1.09</cell><cell>54.98 (8)</cell><cell>53.87 (6) ?1.11</cell></row></table><note><p>VPT with Bias with a pre-trained ViT-B in Sec. 4.2. For each method and each downstream task group, we report the average test accuracy score and number of wins in (?) compared to Full. The difference between the hybrid methods and their VPT counterpart are color coded Bias VPT-shallow VPT-shallow + Bias VPT-deep VPT-deep + Bias</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Semantic Segmentation: ADE20k<ref type="bibr" target="#b71">[84]</ref> validation results with SETR<ref type="bibr" target="#b70">[83]</ref> on ViT-L. The best mIoU scores among all methods but Full are bolded. Results of fully fine-tuning a ResNet-101<ref type="bibr" target="#b13">[8]</ref> are included. SS/MS: single/multi-scale inference</figDesc><table><row><cell>Backbone</cell><cell></cell><cell></cell><cell>ViT-L/16</cell><cell></cell><cell></cell><cell>ResNet-101</cell></row><row><cell>Method</cell><cell cols="2">Full [83] Head Only</cell><cell cols="3">Bias VPT-deep VPT+Bias</cell><cell>Full [8]</cell></row><row><cell>mIoU-SS</cell><cell>48.31</cell><cell cols="2">35.12 43.40</cell><cell>42.11</cell><cell>44.04</cell><cell>45.47</cell></row><row><cell>mIoU-MS</cell><cell>50.07</cell><cell cols="2">37.46 45.33</cell><cell>44.06</cell><cell>45.63</cell><cell>46.27</cell></row><row><cell>Tunable params (M)</cell><cell>318.31</cell><cell cols="2">13.18 13.46</cell><cell>13.43</cell><cell>15.79</cell><cell>63.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Different pre-trained objectives: MAE<ref type="bibr" target="#b31">[26]</ref> and MoCo v3<ref type="bibr" target="#b14">[9]</ref> with a ViT-B backbone. For each method and each downstream task group, we report the average test accuracy score and number of wins in (?) compared to Full. "Total params" denotes total parameters needed for all 24 downstream tasks. Best results among all methods except Full are bolded</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MAE</cell><cell></cell><cell></cell><cell cols="2">MoCo v3</cell><cell></cell></row><row><cell></cell><cell>ViT-B/16</cell><cell>Total</cell><cell></cell><cell>VTAB-1k</cell><cell></cell><cell>Total</cell><cell></cell><cell>VTAB-1k</cell><cell></cell></row><row><cell></cell><cell>(85.8M)</cell><cell>params</cell><cell cols="4">Natural Specialized Structured params</cell><cell cols="3">Natural Specialized Structured</cell></row><row><cell></cell><cell>Total # of tasks</cell><cell></cell><cell>7</cell><cell>4</cell><cell>8</cell><cell></cell><cell>7</cell><cell>4</cell><cell>8</cell></row><row><cell>(a)</cell><cell>Full</cell><cell>19.01?</cell><cell>59.29</cell><cell>79.68</cell><cell>53.82</cell><cell>19.01?</cell><cell>71.95</cell><cell>84.72</cell><cell>51.98</cell></row><row><cell>(b)</cell><cell>Linear Partial-1</cell><cell cols="3">1.01? 18.87 (0) 2.58? 58.44 (5) 78.28 (1) 53.72 (0)</cell><cell>23.70 (0) 47.64 (1)</cell><cell cols="3">1.01? 67.46 (4) 2.58? 72.31 (5) 84.58 (2) 81.08 (0)</cell><cell>30.33 (0) 47.89 (1)</cell></row><row><cell>(c)</cell><cell>Bias Adapter</cell><cell cols="2">1.03? 54.55 (1) 1.17? 54.90 (3)</cell><cell cols="2">75.68 (1) 47.70 (0) 75.19 (1) 38.98 (0)</cell><cell cols="2">1.03? 72.89 (3) 1.22? 74.19 (4)</cell><cell cols="2">81.14 (0) 53.43 (4) 82.66 (1) 47.69 (2)</cell></row><row><cell>(ours)</cell><cell>VPT-shallow VPT-deep</cell><cell cols="2">1.01? 39.96 (1) 1.04? 36.02 (0)</cell><cell>69.65 (0) 60.61 (1)</cell><cell>27.50 (0) 26.57 (0)</cell><cell cols="2">1.01? 67.34 (3) 1.01? 70.27 (4)</cell><cell>82.26 (0) 83.04 (0)</cell><cell>37.55 (0) 42.38 (0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Apply VPT to ConvNets: ResNet-50 and ConvNeXt-Base. For each method and each downstream task group, we report the average test accuracy score and number of wins in (?) compared to Full. "Total params" denotes total parameters needed for all 19 downstream tasks. Best results among all methods except Full are bolded</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">ConvNeXt-Base (87.6M)</cell><cell></cell><cell cols="2">ResNet-50 (23.5M)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Total</cell><cell></cell><cell>VTAB-1k</cell><cell></cell><cell>Total</cell><cell></cell><cell>VTAB-1k</cell><cell></cell></row><row><cell></cell><cell></cell><cell>params</cell><cell cols="4">Natural Specialized Structured params</cell><cell cols="3">Natural Specialized Structured</cell></row><row><cell></cell><cell>Total # of tasks</cell><cell></cell><cell>7</cell><cell>4</cell><cell>8</cell><cell></cell><cell>7</cell><cell>4</cell><cell>8</cell></row><row><cell>(a)</cell><cell>Full</cell><cell>19.01?</cell><cell>77.97</cell><cell>83.71</cell><cell>60.41</cell><cell>19.08?</cell><cell>59.72</cell><cell>76.66</cell><cell>54.08</cell></row><row><cell></cell><cell>Linear</cell><cell cols="2">1.01? 74.48 (5)</cell><cell>81.50 (0)</cell><cell>34.76 (1)</cell><cell cols="2">1.08? 63.75 (6)</cell><cell>77.60 (3)</cell><cell>30.96 (0)</cell></row><row><cell>(b)</cell><cell>Partial-1</cell><cell cols="2">2.84? 73.76 (4)</cell><cell>81.64 (0)</cell><cell>39.55 (0)</cell><cell cols="4">4.69? 64.34 (6) 78.64 (2) 45.78 (1)</cell></row><row><cell></cell><cell>Mlp-3</cell><cell cols="2">1.47? 73.78 (5)</cell><cell>81.36 (1)</cell><cell>35.68 (1)</cell><cell cols="2">7.87? 61.79 (6)</cell><cell>70.77 (1)</cell><cell>33.97 (0)</cell></row><row><cell>(c)</cell><cell>Bias</cell><cell cols="2">1.04? 69.07 (2)</cell><cell>72.81 (0)</cell><cell>25.29 (0)</cell><cell cols="2">1.10? 63.51 (6)</cell><cell>77.22 (2)</cell><cell>33.39 (0)</cell></row><row><cell cols="2">(ours) Visual-Prompt Tuning</cell><cell cols="4">1.02? 78.48 (6) 83.00 (1) 44.64 (1)</cell><cell cols="2">1.09? 66.25 (6)</cell><cell>77.32 (2)</cell><cell>37.52 (0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Specifications of the various datasets evaluated.</figDesc><table><row><cell>Dataset</cell><cell>Description</cell><cell cols="2"># Classes Train</cell><cell>Val</cell><cell>Test</cell></row><row><cell cols="2">Fine-grained visual recognition tasks (FGVC)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CUB-200-2011 [70]</cell><cell>Fine-grained bird species recognition</cell><cell>200</cell><cell>5,394 ?</cell><cell>600 ?</cell><cell>5,794</cell></row><row><cell>NABirds [67]</cell><cell>Fine-grained bird species recognition</cell><cell>55</cell><cell>21,536 ?</cell><cell cols="2">2,393 ? 24,633</cell></row><row><cell>Oxford Flowers [55]</cell><cell cols="2">Fine-grained flower species recognition 102</cell><cell>1,020</cell><cell cols="2">1,020 6,149</cell></row><row><cell>Stanford Dogs [37]</cell><cell>Fine-grained dog species recognition</cell><cell>120</cell><cell>10,800 ?</cell><cell cols="2">1,200 ? 8,580</cell></row><row><cell>Stanford Cars [19]</cell><cell>Fine-grained car classification</cell><cell>196</cell><cell>7,329 ?</cell><cell>815 ?</cell><cell>8,041</cell></row><row><cell cols="2">Visual Task Adaptation Benchmark (VTAB-1k) [80]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-100 [39]</cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell>10,000</cell></row><row><cell>Caltech101 [43]</cell><cell></cell><cell>102</cell><cell></cell><cell></cell><cell>6,084</cell></row><row><cell>DTD [11]</cell><cell></cell><cell>47</cell><cell></cell><cell></cell><cell>1,880</cell></row><row><cell>Flowers102 [55]</cell><cell>Natural</cell><cell>102</cell><cell cols="2">800/1000 200</cell><cell>6,149</cell></row><row><cell>Pets [57]</cell><cell></cell><cell>37</cell><cell></cell><cell></cell><cell>3,669</cell></row><row><cell>SVHN [54]</cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell>26,032</cell></row><row><cell>Sun397 [76]</cell><cell></cell><cell>397</cell><cell></cell><cell></cell><cell>21,750</cell></row><row><cell>Patch Camelyon [69]</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>32,768</cell></row><row><cell>EuroSAT [29] Resisc45 [10]</cell><cell>Specialized</cell><cell>10 45</cell><cell cols="2">800/1000 200</cell><cell>5,400 6,300</cell></row><row><cell>Retinopathy [36]</cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell>42,670</cell></row><row><cell>Clevr/count [34]</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>15,000</cell></row><row><cell>Clevr/distance [34]</cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell>15,000</cell></row><row><cell>DMLab [3]</cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell>22,735</cell></row><row><cell>KITTI/distance [20] dSprites/location [53]</cell><cell>Structured</cell><cell>4 16</cell><cell cols="2">800/1000 200</cell><cell>711 73,728</cell></row><row><cell>dSprites/orientation [53]</cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>73,728</cell></row><row><cell>SmallNORB/azimuth [40]</cell><cell></cell><cell>18</cell><cell></cell><cell></cell><cell>12,150</cell></row><row><cell>SmallNORB/elevation [40]</cell><cell></cell><cell>9</cell><cell></cell><cell></cell><cell>12,150</cell></row></table><note><p>? : we randomly sampled the train and val sets since there are no public splits available</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Specifications of different pre-trained backbones used in the paper. Parameters (M) are of the feature extractor. "Batch size" column reports the batch size for Linear / Partial / {Full, Bias, Adapter} / VPT (p &lt; 100) / VPT (p ? 100). All backbones are pre-trained on ImageNet<ref type="bibr" target="#b18">[13]</ref> with resolution 224?224 where b is the batch size used for the particular model, and base lr is chosen from the range specified in Tab. 7. The optimal hyper-parameter values for each experiment can be found in Appendix C.</figDesc><table><row><cell></cell><cell>Pre-trained</cell><cell>Pre-trained</cell><cell># params</cell><cell>Feature dim</cell><cell></cell><cell>Pre-trained</cell></row><row><cell>Backbone</cell><cell>Objective</cell><cell>Dataset</cell><cell>(M)</cell><cell>d</cell><cell>Batch Size</cell><cell>Model</cell></row><row><cell>ViT-B/16 [16]</cell><cell></cell><cell></cell><cell>85</cell><cell>768</cell><cell>2048 / 1280 / 128 / 128 / 64</cell><cell>checkpoint</cell></row><row><cell>ViT-L/16 [16]</cell><cell>Supervised</cell><cell>ImageNet-21k</cell><cell>307</cell><cell>1024</cell><cell>2048 / 640 / 64 / 64 / 32</cell><cell>checkpoint</cell></row><row><cell>ViT-H/14 [16]</cell><cell></cell><cell></cell><cell>630</cell><cell>1280</cell><cell>1024 / 240 / 28 / 28 / 14</cell><cell>checkpoint</cell></row><row><cell>ViT-B/16 [16] ViT-B/16 [16]</cell><cell>MoCo v3 [9] MAE [26]</cell><cell>ImageNet-1k</cell><cell>85</cell><cell>768</cell><cell>2048 / 1280 / 128 / 128 / 64</cell><cell>checkpoint checkpoint</cell></row><row><cell>Swin-B [48]</cell><cell>Supervised</cell><cell cols="2">ImageNet-21k 88</cell><cell>1024</cell><cell>1024 / 1024 / 128 / 80 / -</cell><cell>checkpoint</cell></row><row><cell cols="2">ConvNeXt-Base [49] Supervised</cell><cell cols="2">ImageNet-21k 88</cell><cell>1024</cell><cell>1024 / 1024 / 128 / 128 / -</cell><cell>checkpoint</cell></row><row><cell>ResNet-50 [27]</cell><cell>Supervised</cell><cell>ImageNet-1k</cell><cell>23</cell><cell>2048</cell><cell>2048 / 2048 / 384 / 256 / -</cell><cell>checkpoint</cell></row><row><cell>base lr ?b/256,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>81.1 85.4 59.6</head><label></label><figDesc></figDesc><table><row><cell>68.8 69.5 67.9 45.0 73.1 70.8 76.7 78.6</cell><cell>83.6 77.1 78.8 74.9 69.4 78.4 77.1 79.3 82.9 Average</cell><cell>46.7 26.6 34.1 30.4 23.0 44.2 33.1 47.5 54.9</cell><cell>N a tu ra l 76.5 69.2 70.0 68.4 53.4 74.2 71.2 77.1 79.9</cell><cell>S p e c ia li z e d 84.3 77.9 79.5 76.7 76.5 79.6 78.8 80.0 83.8 Best</cell><cell>S tr u c tu re d 49.2 27.3 36.0 31.8 26.1 46.3 34.9 49.8 57.8</cell><cell>N a tu ra l 78.8 69.2 71.7 68.9 52.3 78.0 73.2 78.1</cell><cell>S p e c ia li z e d 85.2 77.7 80.6 77.0 77.5 83.3 81.0 80.5 Ensemble</cell><cell>S tr u c tu re d 50.2 27.4 37.4 31.8 23.7 50.4 36.6 52.1</cell><cell>N a tu ra l 3.1 0.4 2.1 1.0 7.3 4.9 2.4 1.3 2.5</cell><cell>S p e c ia li z e d S tr u c tu re d 1.7 3.6 0.6 0.8 1.8 3.2 2.1 1.3 8.1 0.6 4.9 6.2 3.9 3.5 1.1 4.6 2.5 4.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Non-parametric paired one-tailed t-test (the Wilcoxon signed-rank test) on whether VPT-deep's performance is greater than other methods on 19 VTAB tasks. Results show that, VPT-deep is indeed statistically significantly better than other fine-tuning protocols (p &lt; 0.05) Un-paired one-tailed t-test with unequal variances (Welch's t-test) on whether VPT-deep's performance is greater than other methods for each VTAB task. Results show that, VPT-deep is statistically significantly better than other fine-tuning protocols (p &lt; 0.05) in most instances</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell>(ours)</cell></row><row><cell></cell><cell cols="8">Full Linear Mlp-3 Partial-1 Sidetune Bias Adapter VPT-shallow</cell></row><row><cell>Is VPT-deep statistically</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>significantly better?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>p-value</cell><cell cols="3">1.2e-03 2.7e-05 1.9e-06</cell><cell>1.9e-05</cell><cell>1.9e-06</cell><cell cols="2">1.9e-06 3.8e-06</cell><cell>2.7e-05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>downstream tasks. Best results among all methods except Full are bolded</figDesc><table><row><cell></cell><cell>ViT-B/16</cell><cell>Fine-tune</cell><cell>Total</cell><cell></cell><cell>VTAB-1k</cell></row><row><cell></cell><cell>(85.8M)</cell><cell cols="5">Resolution params Natural Specialized Structured</cell></row><row><cell></cell><cell>Total # of tasks</cell><cell></cell><cell></cell><cell>7</cell><cell>4</cell><cell>8</cell></row><row><cell>(a)</cell><cell>Full Full</cell><cell cols="2">384 19.07? 224 19.07?</cell><cell>72.57 75.88</cell><cell>83.05 83.36</cell><cell>50.86 47.64</cell></row><row><cell></cell><cell>Linear</cell><cell></cell><cell cols="2">1.01? 66.30 (2)</cell><cell>76.77 (0)</cell><cell>27.86 (0)</cell></row><row><cell>(b)</cell><cell>Mlp-3</cell><cell>384</cell><cell cols="2">1.27? 66.45 (3)</cell><cell>77.77 (0)</cell><cell>38.03 (0)</cell></row><row><cell></cell><cell>Partial-1</cell><cell></cell><cell cols="2">2.58? 67.91 (4)</cell><cell>76.94 (0)</cell><cell>37.16 (0)</cell></row><row><cell></cell><cell>Sidetune</cell><cell></cell><cell cols="2">3.12? 47.08 (1)</cell><cell>40.34 (0)</cell><cell>24.18 (0)</cell></row><row><cell>(c)</cell><cell>Bias</cell><cell>384</cell><cell cols="2">1.03? 70.30 (4)</cell><cell>76.06 (0)</cell><cell>45.35 (1)</cell></row><row><cell></cell><cell>Adapter</cell><cell></cell><cell cols="2">1.11? 69.42 (6)</cell><cell>77.11 (0)</cell><cell>30.62 (0)</cell></row><row><cell></cell><cell>VPT-shallow (p ? {1, 5, 10, 50, 100, 200})</cell><cell>384</cell><cell cols="2">1.02? 75.30 (4)</cell><cell>78.50 (0)</cell><cell>46.56 (2)</cell></row><row><cell></cell><cell>VPT-deep (p ? {1, 5, 10, 50, 100, 200})</cell><cell>384</cell><cell cols="2">1.19? 79.37 (6)</cell><cell>82.86 (2)</cell><cell>56.36 (7)</cell></row><row><cell>(ours)</cell><cell>VPT-shallow (p = 380)</cell><cell>224</cell><cell cols="2">1.07? 75.07 (3)</cell><cell>79.03 (0)</cell><cell>46.21 (2)</cell></row><row><cell></cell><cell>VPT-deep (p = 380)</cell><cell>224</cell><cell cols="2">1.78? 74.20 (4)</cell><cell>82.30 (2)</cell><cell>54.50 (6)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>Cost analysis using a ViT-B/16 pre-trained on supervised ImageNet-21k. For each method and each downstream task group, we report the latency (ms/img) and peak GPU memory usage (GB) at both training and inference time. "Tuned params" denotes the fraction of learnable parameters needed. "Scope" denotes the tuning scope of each method. "Extra params" denotes the presence of additional parameters besides the pre-trained backbone and linear head. All experiments use the same A100 GPU</figDesc><table><row><cell></cell><cell>ViT-B/16</cell><cell>Tuned</cell><cell>Scope</cell><cell>Extra</cell><cell>Train</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>(85.8M)</cell><cell cols="7">params Input Backbone Head params Latency Memory Latency Memory</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ms/img)</cell><cell cols="2">(GB) (ms/img)</cell><cell>(GB)</cell></row><row><cell>(a)</cell><cell>Full</cell><cell>100%</cell><cell>?</cell><cell>?</cell><cell>358.7</cell><cell>11.7</cell><cell>69.7</cell><cell>0.87</cell></row><row><cell></cell><cell>Linear</cell><cell>0.09%</cell><cell></cell><cell></cell><cell>148.9</cell><cell>0.9</cell><cell>64.4</cell><cell>0.87</cell></row><row><cell>(b)</cell><cell>Partial-1</cell><cell>8.35%</cell><cell></cell><cell>?</cell><cell>193.2</cell><cell>1.4</cell><cell>66.1</cell><cell>0.87</cell></row><row><cell></cell><cell>Mlp-3</cell><cell>1.45%</cell><cell></cell><cell>?</cell><cell>164.3</cell><cell>0.9</cell><cell>64.4</cell><cell>0.87</cell></row><row><cell></cell><cell>Sidetune</cell><cell>10.09%</cell><cell></cell><cell>?</cell><cell>164.6</cell><cell>1.2</cell><cell>66.9</cell><cell>0.91</cell></row><row><cell></cell><cell>Bias</cell><cell>0.21%</cell><cell></cell><cell></cell><cell>296.9</cell><cell>10.1</cell><cell>65.6</cell><cell>0.87</cell></row><row><cell>(c)</cell><cell>Adapter (r = 8)</cell><cell>2.12%</cell><cell>?</cell><cell>?</cell><cell>293.4</cell><cell>9.9</cell><cell>68.2</cell><cell>0.87</cell></row><row><cell></cell><cell>Adapter (r = 64)</cell><cell>0.36%</cell><cell></cell><cell>?</cell><cell>294.4</cell><cell>9.8</cell><cell>68.3</cell><cell>0.87</cell></row><row><cell></cell><cell>Adapter (r = 256)</cell><cell>0.17%</cell><cell></cell><cell>?</cell><cell>271.4</cell><cell>9.8</cell><cell>68.0</cell><cell>0.87</cell></row><row><cell></cell><cell>VPT-shallow (p = 1)</cell><cell>0.09%</cell><cell></cell><cell></cell><cell>205.9</cell><cell>10.3</cell><cell>68.1</cell><cell>0.88</cell></row><row><cell>(ours)</cell><cell>VPT-deep (p = 1) VPT-shallow (p = 200)</cell><cell>0.10% 0.27%</cell><cell>?</cell><cell>?</cell><cell>213.6 350.6</cell><cell>10.3 25.8</cell><cell>69.4 138.8</cell><cell>0.88 1.84</cell></row><row><cell></cell><cell>VPT-deep (p = 200)</cell><cell>2.19%</cell><cell></cell><cell></cell><cell>360.1</cell><cell>25.8</cell><cell>140.8</cell><cell>1.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 .</head><label>13</label><figDesc>Per-task fine-tuning results from Tab. 1 for VTAB-1k with a pre-trained ViT-B/16 Bias 72.8 87.0 59.2 97.5 85.3 59.9 51.4 73.30 (3) 78.7 91.6 72.9 69.8 78.25 (0) 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 44.09 (2) Adapter-256 74.1 86.1 63.2 97.7 87.0 34.6 50.8 70.50 (4) 76.3 88.0 73.1 70.5 76.98 (0) 45.7 37.4 31.2 53.2 30.3 25.4 13.8 22.1 32.39 (0) Adapter-64 74.2 85.8 62.7 97.6 87.2 36.3 50.9 70.65 (4) 76.3 87.5 73.7 70.9 77.10 (0) 42.9 39.9 30.4 54.5 31.9 25.6 13.5 21.4 32.51 (0) Adapter-8 74.2 85.7 62.7 97.8 87.2 36.4 50.7 70.67 (4) 76.9 89.2 73.5 71.6 77.80 (0) 45.2 41.8 31.1 56.4 30.4 24.6 13.2 22.0 33.09 (0) VPT-shallow 77.7 86.9 62.6 97.5 87.3 74.5 51.2 76.81 (4) 78.2 92.0 75.6 72.9 79.66 (0) 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 46.98 (4) Tuned / Total (%) 0.18 0.10 0.04 0.27 0.08 0.19 0.36 0.17 0.01 0.05 0.09 0.01 0.04 0.10 0.18 0.09 0.09 0.10 0.10 0.19 0.19 0.13 VPT-deep 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.48 (6) 81.8 96.1 83.4 68.4 82.43 (2) 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 54.98 (8) Tuned / Total (%) 0.20 0.20 0.15 0.10 0.04 0.54 0.41 0.23 1.06 1.07 0.15 0.02 0.57 0.54 2.11 1.07 0.54 0.12 0.55 2.12 2.11 1.14</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-100</cell><cell>Caltech101</cell><cell>DTD</cell><cell>Flowers102</cell><cell>Pets</cell><cell>SVHN</cell><cell>Sun397</cell><cell>Mean</cell><cell>Patch Camelyon</cell><cell>EuroSAT</cell><cell>Resisc45</cell><cell>Retinopathy</cell><cell>Mean</cell><cell>Clevr/count</cell><cell>Clevr/distance</cell><cell>DMLab</cell><cell>KITTI/distance</cell><cell>dSprites/location</cell><cell>dSprites/orientation</cell><cell>SmallNORB/azimuth</cell><cell>SmallNORB/elevation</cell><cell>Mean</cell></row><row><cell>(a)</cell><cell>Full</cell><cell cols="7">68.9 87.7 64.3 97.2 86.9 87.4 38.8</cell><cell cols="5">75.88 79.7 95.7 84.2 73.9</cell><cell cols="9">83.36 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1</cell><cell>47.64</cell></row><row><cell cols="2">Head-oriented</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Linear</cell><cell cols="19">63.4 85.0 63.2 97.0 86.3 36.6 51.0 68.93 (1) 78.5 87.5 68.6 74.0 77.16 (1) 34.3 30.6 33.2 55.4 12.5 20.0</cell><cell cols="2">9.6 19.2 26.84 (0)</cell></row><row><cell></cell><cell>Partial-1</cell><cell cols="21">66.8 85.9 62.5 97.3 85.5 37.6 50.6 69.44 (2) 78.6 89.8 72.5 73.3 78.53 (0) 41.5 34.3 33.9 61.0 31.3 32.8 16.3 22.4 34.17 (0)</cell></row><row><cell>(a)</cell><cell>Mlp-2 Mlp-3</cell><cell cols="21">63.2 84.8 60.5 97.6 85.9 34.1 47.8 67.70 (2) 74.3 88.8 67.1 73.2 75.86 (0) 45.2 31.6 31.8 55.7 30.9 24.6 16.6 23.3 32.47 (0) 63.8 84.7 62.3 97.4 84.7 32.5 49.2 67.80 (2) 77.0 88.0 70.2 56.1 72.83 (0) 47.8 32.8 32.3 58.1 12.9 21.2 15.2 24.8 30.62 (0)</cell></row><row><cell></cell><cell>Mlp-5</cell><cell cols="17">59.3 84.4 59.9 96.1 84.4 30.9 46.8 65.98 (1) 73.7 87.2 64.8 71.5 74.31 (0) 50.8 32.3 31.5 56.4</cell><cell cols="4">7.5 20.8 14.4 20.4 29.23 (0)</cell></row><row><cell></cell><cell>Mlp-9</cell><cell cols="17">53.1 80.5 53.9 95.1 82.6 24.4 43.7 61.90 (1) 78.5 83.0 60.2 72.3 73.49 (0) 47.5 27.9 28.9 54.0</cell><cell cols="4">6.2 17.7 10.8 16.2 26.15 (0)</cell></row><row><cell cols="2">Backbone-oriented</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sidetune</cell><cell cols="17">60.7 60.8 53.6 95.5 66.7 34.9 35.3 58.21 (0) 58.5 87.7 65.2 61.0 68.12 (0) 27.6 22.6 31.3 51.7</cell><cell cols="2">8.2 14.4</cell><cell cols="2">9.8 21.8 23.41 (0)</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Visual-Prompt Tuning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Prompt length (p)</cell><cell>100</cell><cell>5</cell><cell cols="2">1 200</cell><cell cols="2">50 200</cell><cell>1</cell><cell>79.4</cell><cell>5</cell><cell>50</cell><cell>50</cell><cell>10</cell><cell>28.7</cell><cell cols="8">100 200 100 100 100 100 200 200</cell><cell>137.5</cell></row><row><cell>(ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Prompt length (p)</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>1</cell><cell>1</cell><cell>50</cell><cell>5</cell><cell>12.4</cell><cell cols="2">100 100</cell><cell>10</cell><cell>1</cell><cell>52.8</cell><cell cols="3">50 200 100</cell><cell>50</cell><cell>10</cell><cell cols="3">50 200 200</cell><cell>107.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 .</head><label>14</label><figDesc>Per-task fine-tuning results from Tab. 1 for five FGVC tasks, with a pretrained ViT-B/16</figDesc><table><row><cell></cell><cell></cell><cell cols="5">CUB-200-2011 NABirds Oxford Flowers Stanford Dogs Stanford Cars</cell><cell>Mean</cell></row><row><cell>(a)</cell><cell>Full</cell><cell>87.3</cell><cell>82.7</cell><cell>98.8</cell><cell>89.4</cell><cell>84.5</cell><cell>88.54</cell></row><row><cell cols="2">Head-oriented</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Linear</cell><cell>85.3</cell><cell>75.9</cell><cell>97.9</cell><cell>86.2</cell><cell cols="2">51.3 79.32 (0)</cell></row><row><cell></cell><cell>Partial-1</cell><cell>85.6</cell><cell>77.8</cell><cell>98.2</cell><cell>85.5</cell><cell cols="2">66.2 82.63 (0)</cell></row><row><cell>(a)</cell><cell>Mlp-2 Mlp-3</cell><cell>85.7 85.1</cell><cell>77.2 77.3</cell><cell>98.2 97.9</cell><cell>85.4 84.9</cell><cell cols="2">54.9 80.28 (0) 53.8 79.80 (0)</cell></row><row><cell></cell><cell>Mlp-5</cell><cell>84.2</cell><cell>76.7</cell><cell>97.6</cell><cell>84.8</cell><cell cols="2">50.2 78.71 (0)</cell></row><row><cell></cell><cell>Mlp-9</cell><cell>83.2</cell><cell>76.0</cell><cell>96.2</cell><cell>83.7</cell><cell cols="2">47.6 77.31 (0)</cell></row><row><cell cols="2">Backbone-oriented</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sidetune</cell><cell>84.7</cell><cell>75.8</cell><cell>96.9</cell><cell>85.8</cell><cell cols="2">48.6 78.35 (0)</cell></row><row><cell></cell><cell>Bias</cell><cell>88.4</cell><cell>84.2</cell><cell>98.8</cell><cell>91.2</cell><cell cols="2">79.4 88.41 (3)</cell></row><row><cell>(b)</cell><cell>Adapter-256 Adapter-64</cell><cell>87.2 87.1</cell><cell>84.3 84.3</cell><cell>98.5 98.5</cell><cell>89.9 89.8</cell><cell cols="2">68.6 85.70 (2) 68.6 85.67 (2)</cell></row><row><cell></cell><cell>Adapter-8</cell><cell>87.3</cell><cell>84.3</cell><cell>98.4</cell><cell>88.8</cell><cell cols="2">68.4 85.46 (1)</cell></row><row><cell cols="2">Visual-Prompt Tuning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>VPT-shallow</cell><cell>86.7</cell><cell>78.8</cell><cell>98.4</cell><cell>90.7</cell><cell cols="2">68.7 84.62 (1)</cell></row><row><cell></cell><cell>Prompt length (p)</cell><cell>100</cell><cell>50</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>90</cell></row><row><cell>(ours)</cell><cell>Tuned / Total (%)</cell><cell>0.31</cell><cell>0.54</cell><cell>0.23</cell><cell>0.20</cell><cell>0.26</cell><cell>0.31</cell></row><row><cell></cell><cell>VPT-deep</cell><cell>88.5</cell><cell>84.2</cell><cell>99.0</cell><cell>90.2</cell><cell cols="2">83.6 89.11 (4)</cell></row><row><cell></cell><cell>Prompt length (p)</cell><cell>10</cell><cell>50</cell><cell>5</cell><cell>100</cell><cell>200</cell><cell>73</cell></row><row><cell></cell><cell>Tuned / Total (%)</cell><cell>0.29</cell><cell>1.02</cell><cell>0.14</cell><cell>1.17</cell><cell>2.27</cell><cell>0.98</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Some Transformer architectures in Vision such as Swin [48] do not use [CLS] and treat global pooled EN as input for Head. We follow their designs when adapting VPT to these Transformer variants. See Appendix A for more details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Following the default settings in VTAB, we don't adopt other augmentations</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2"><p>Utilizing the per-class averaged [CLS] features, we also tried several other different implementation variants, including using per-layer [CLS] embeddings for VPT-deep instead of only the final output [CLS] vector. They perform either the same as or even much worse than the CLS strategy above, and none of them is able to out-perform the default Random.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>VTAB tasks 6e-07 2e-06 3e-06 4e-06 2e-08 3e-05 8e-04 4e-01 2e-05 4e-07 2e-06 3e-06 2e-11 6e-06 2e-06 7e-05 5e-02 3e-04 2e-05 2e-04 3e-07 6e-05 1e-03 5e-05 1e-02 5e-05 1e-03 2e-04 3e-02 4e-03 6e-03 2e-03 1e-03 2e-07 1e-09 3e-05 1e-07 2e-06 5e-07 8e-07 1e+00 8e-05 5e-05 8e-05 6e-05 2e-03 8e-05 1e-01 5e-06 2e-01 2e-02 1e-01 6e-02 2e-01 1e-01 3e-01 3e-01 1e-03 1e-04 1e-03 4e-02 5e-02 8e-04 3e-05 1e-01 1e-04 9e-09 2e-08 7e-09 6e-06 2e-06 5e-06 5e-01 2e-09 3e-09 2e-08 6e-10 4e-09 1e-08 2e-07 1e+00 1e+00 3e-02 1e+00 1e-01 2e-01 6e-01 1e+00 3e-04 1e-05 3e-05 5e-06 3e-05 7e-03 4e-06 4e-05 2e-01 3e-11 1e-06 3e-05 7e-11 1e-03 1e-05 1e-02 6e-06 1e-08 3e-09 2e-10 1e-09 4e-10 1e-06 1e-04 9e-03 1e-05 5e-06 1e-05 1e-06 7e-06 1e-06 5e-02 4e-04 2e-06 3e-07 2e-07 2e-08 6e-03 6e-06 8e-02 1e-01 2e-07 6e-08 2e-06 1e-05 2e-04 1e-08 7e-07 2e-03 6e-05 2e-04 3e-04 2e-06 1e-04 1e-04 8e-04 6e-05 6e-06 2e-05 3e-05 2e-05</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extended Analysis</head><p>Effect of expanding input sequence length. As shown in Tab. 1, by expanding the input sequence with learnable prompts, VPT achieves better performance than Full on the 20 out of 24 tasks evaluated. To investigate whether the advantage of VPT is due to its enlarged input sequence length, we experiment on two more variants: (1) the prompts are kept frozen during fine-tuning stage (Prompt-Fixed). ( <ref type="formula">2</ref>) only tuning the [CLS] token ([CLS]-Learned). From Fig. <ref type="figure">11</ref> we can see that, updating prompt embeddings (Prompt-Learned) offers significant gains, while Prompt-Fixed yields comparable results w.r.t. Linear. This suggests that the final performance of VPT is mainly contributed by the learned prompt embeddings instead of the enlarged sequence length. Updating the [CLS] token performs similarly as updating 1 prompt ([CLS] vs. Learnedp=1), but still lags behind the default setting where we manually select the best number of prompt tokens based on the val set.</p><p>Sharing prompts. We examine the effect of sharing parameters of prompts in Fig. <ref type="figure">12</ref> by setting the same prompt embedding within Transformer layers (Shared-intra), among all layers (Shared-inter), and for all prompts inserted in the Transformer (Shared-all). We can observe that: (1) Sharing prompts within layer (Shared-intra) performs competitively or slightly outperforms the performance of using one prompt (Defaultp=1), further demonstrating the value of expanding input sequence. (2) Although Shared-intra under-performs Default in general, surprisingly, Shared-inter slightly outperforms our default VPT-deep while using similar number of trainable parameters (total number of parameters for all VTAB tasks: 1.14? vs. 1.13? for Shared-inter vs. Default, respectively). Closer examination reveals that the optimal prompt length p for Shared-inter is in general larger than Default, i.e., average prompt length on all VTAB tasks: 64.58 vs. 60.94, for Shared-inter vs. Default, respectively. (3) Sharing the same prompt embedding both among and within lay-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Patch Camelyon (VTAB-Specialized)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Retinopathy</surname></persName>
		</author>
		<imprint>
			<publisher>VTAB-Specialized</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Clevr/distance</title>
		<imprint/>
		<respStmt>
			<orgName>VTAB-Structured</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">dSprites/location (VTAB-Specialized) dSprites/orientation</title>
		<imprint>
			<publisher>VTAB-Specialized</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">SmallNORB/elevation (VTAB-Specialized) DMLab (VTAB-Specialized)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03801</idno>
		<title level="m">Deepmind lab</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H.</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tinytl: Reduce memory, not parameters for efficient on-device learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">*</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2021">2021) 2, 3, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2020">2020) 1, 3, 4, 6, 17</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial reprogramming of neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06687</idno>
		<title level="m">Domain adaptation via prompt learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained car detection for visual census estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Video action transformer network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">Aug 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016) 1, 5, 14</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring visual engagement signals for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Prompting visual-language models for efficient video understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04478</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<editor>NeurIPS Autodiff Workshop</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adapterfusion: Nondestructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adapterhub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Systems Demonstrations. Online (2020) 3, 6</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rotation equivariant cnns for digital pathology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Linmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Glue: A multitask benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The generalization of &apos;student&apos;s&apos;problem when several different population varlances are involved</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cpt: Colorful prompt tuning for pre-trained vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11797</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Side-tuning: a baseline for network adaptation via additive side networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
