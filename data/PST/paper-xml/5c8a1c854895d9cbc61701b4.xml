<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yading</forename><surname>Yuan</surname></persName>
							<email>yading.yuan@mssm.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Chao</surname></persName>
							<email>ming.chao@mountsinai.org</email>
						</author>
						<author>
							<persName><forename type="first">Yeh-Chi</forename><surname>Lo</surname></persName>
							<email>yeh-chi.lo@mountsinai.org</email>
						</author>
						<author>
							<persName><forename type="first">Y-C</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks with Jaccard Distance</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Radia-tion Oncology</orgName>
								<orgName type="department" key="dep2">Icahn School of Medicine at Mount Sinai</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81E0CB02BB8EC3CD73787EF98A4A055E</idno>
					<idno type="DOI">10.1109/TMI.2017.2695227</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2017.2695227, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>fully convolutional neural networks</term>
					<term>image segmentation</term>
					<term>jaccard distance</term>
					<term>melanoma</term>
					<term>dermoscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this article, we present a fully automatic method for skin lesion segmentation by leveraging a 19-layer deep convolutional neural networks (CNNs) that is trained endto-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre-and post-processing, which allows its adoption in a variety of medical image segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ABCD criteria <ref type="bibr" target="#b3">[4]</ref>, proper interpretation of dermoscopic images is normally time-consuming, complex, and prone to suffer from inter-and intra-observer variabilities. Therefore, computerized analysis methods have been developed to assist dermatologists in improving their efficiency and objectivity of visual interpretation of dermoscopic images. A review of the state-of-the-art methods up to 2012 is presented in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Automatically segmenting melanoma from the surrounding skin is an essential step in computerized analysis of dermoscopic images <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b6">[7]</ref>. However, this task is not trivial because melanoma usually has a large variety of appearance in size, shape, and color along with different types of skin and texture. Meanwhile, some lesions have irregular and fuzzy borders, and in some cases the contrast between lesion and the surrounding skin is pretty low. In addition, artifacts and intrinsic cutaneous features, such as hairs, frames, blood vessels and air bubbles can make the automatic segmentation more challenging, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>In the past decades, researchers have developed various computer algorithms to conquer these challenges. These algorithms can be broadly classified as clustering, thresholding, region merging and splitting, active contour models and supervised learning. The advantages and drawbacks of each method have been discussed and compared in many articles such as <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>In some recent efforts to tackle this problem, Celebi et al. <ref type="bibr" target="#b9">[10]</ref> developed an automated method for skin lesion border detection by ensembling four different thresholding methods. Peruch et al. <ref type="bibr" target="#b10">[11]</ref> proposed a Mimicking Expert Dermatologists' Segmentation (MEDS) method, in which Principal Component Analysis (PCA) is firstly applied to project the RGB images onto the first principal component of the color histogram, then a thresholding technique that mimicks the cognitive process of human dermatologists is used to cluster the colors (and thus pixels) into lesional and non-lesional skin. Thresholding-based methods rely on the histogram distribution of image color (or its variants), which may be undesirably altered in cases when significant amount of hair and bubbles are present. In order to improve the effectiveness of the classic GVF snake in the presence of distractions or noise in the vicinity of a real boundary, Zhou et al. <ref type="bibr" target="#b11">[12]</ref> integrated a mass density function into the optimization objective functional, which can be solved with the support of mean shift estimation. The optimization procedure, however, involves a large amount of computation to achieve convergence. In <ref type="bibr" target="#b12">[13]</ref>, Sadri et al. introduced a fixed-grid wavelet networks in which orthogonal least squares was used to calculate the network weights and to optimize the network structure in a supervised way. Another supervised method was proposed by Xie et al. in <ref type="bibr" target="#b13">[14]</ref>, where a self-generating neural network is combined with the genetic algorithms for skin lesion segmentation. These supervised methods, along with the aforementioned unsupervised ones, employ hand-crafted features that require specialized domain knowledge. Meanwhile, pre-processing steps, such as color space transform, contrast enhancement, lesion localization and artifact removal are usually introduced to facilitate the lesion segmentation procedure <ref type="bibr" target="#b14">[15]</ref>.</p><p>Recently, convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16]</ref> have become one of the most powerful tools in machine learning and computer vision. These models have the capability of learning hierarchical features from raw image data, and thus the hand-crafted features are not needed. Besides the success in natural image recognition tasks such as image classification <ref type="bibr" target="#b16">[17]</ref>, CNNs have also showed promising performance in various medical image computing problems such as mitosis detection on histology images <ref type="bibr" target="#b17">[18]</ref>, body parts recognition on CT images <ref type="bibr" target="#b18">[19]</ref>, cerebral microbleeds detection on MR images <ref type="bibr" target="#b19">[20]</ref>, 2D/3D image registration <ref type="bibr" target="#b20">[21]</ref> and skin cancer classification <ref type="bibr" target="#b21">[22]</ref>. Specifically for medical image segmentation, CNNs have been applied to segment brain tumor on MR images <ref type="bibr" target="#b22">[23]</ref>, urinary bladder on CT urolography <ref type="bibr" target="#b23">[24]</ref> left ventricle on cardiac MRI <ref type="bibr" target="#b24">[25]</ref>, and skin lesion from nondermoscopic images <ref type="bibr" target="#b25">[26]</ref> among others. These segmentation algorithms perform patch-based image classification, in which the entire input image is divided into small patches and the CNN model is applied onto each patch to predict if it is within the target (foreground) or outside of the target (background). Since each patch only represents a local region of the image, this approach only incorporates limited contextual information contained in the patch. Enlarging patch size can increase the contextual information, but excessively large patch size may lose fine details in the final segmentation. Sliding-window <ref type="bibr" target="#b17">[18]</ref> can also be implemented to incorporate both local and global context into segmentation, however, this procedure is computationally demanding and inefficient due to the highly overlapping patches.</p><p>Inspired by the latest advances in deep learning research, we propose a novel framework based on deep CNNs to automatically segment skin lesions in dermoscopic images. Instead of developing sophisticated pre-and post-processing algorithms and hand-crafted features, we focus on designing appropriate network architecture and effective training strategies such that our deep learning model can handle images under various acquisition conditions, which makes our method highly scalable. In this study, we exploit a model known as fully convolutional network (FCN) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> that extends the convolution process across the entire image and predicts the segmentation mask as a whole. As the result, the redundant computation encountered in the patch-based methods can be avoided. In the current applications of FCN on segmentation, binary cross entropy between predicted values and targets is usually used as the loss function. However, since the skin lesion usually occupies a small portion of the dermoscopic image, the minimization of cross entropy tends to be biased toward background. In order to address this issue, we propose a novel loss function based Jaccard distance that is tailed to medical image segmentation problem. We evaluate our model on two publicly available databases. One is ISBI 2016 challenge dataset for skin lesion analysis towards melanoma detection <ref type="bibr" target="#b28">[29]</ref>, and the other is the PH2 database <ref type="bibr" target="#b29">[30]</ref>. Our model shows superior segmentation performance than those state-of-the-art methods on both databases.</p><p>Our contributions in this paper are three fold. Firstly, we introduce a fully automated method for skin lesion segmentation by leveraging the discriminative power of a 19-layer deep FCN. To the best of our knowledge, our work is among the first few attempts to use deep neural networks to tackle this challenging problem <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. This model does not rely on the prior knowledge of data and is trained in an endto-end fashion. We investigate a set of training strategies to ensure effective and efficient learning with limited training data. Secondly, we design an appropriate loss function that naturally handles the lesion-background imbalance of pixelwise classification for medical image segmentation. Our results show that this loss function can further improve the segmentation performance. At last, We extensively evaluate the effectiveness, efficiency and the generalization capability of the proposed model using two large databases. Our model can be easily generalized to other challenging medical image segmentation problems. This paper is organized as follows. we introduce the details of the proposed FCN-based segmentation method in Section III, then report the experimental design and results in Section IV. Finally, we discuss the results in Section V and conclude our study in Section VI.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FULLY CONVOLUTIONAL NETWORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network architecture</head><p>We train a FCN to map from an RGB input to a posterior probability map using the architecture shown in Figure <ref type="figure" target="#fig_1">2</ref>. The network contains19 layers with 290, 129 trainable parameters, and Table I describes the architectural details. We fix the stride as 1 and use Rectified Linear Units (ReLUs) as the activation function <ref type="bibr" target="#b16">[17]</ref> for each convolutional/deconvolutional layer. For output layer, we use sigmoid function as the activation function. Pixel-wise classification is performed and FCN is essentially served as a filter that projects the entire input image to a map where each element represents the probability that the corresponding input pixel belongs to the tumor.</p><p>With the successive convolution and pooling layers, CNNs can integrate contextual information from regional to global scales, resulting in reduced resolution in the output layer. In contrast, image segmentation calls for classifying each pixel into either foreground or background in combination with fullresolution output. In order to address this conflict between multi-scale information aggregation and full-resolution pixelwise classification, we implement a strategy of using upsampling and deconvolutional layers to recover lost resolution while carrying over the global perspective from pooling layers <ref type="bibr" target="#b32">[33]</ref>.</p><p>The up-sampling layer performs the reverse operation of pooling and reconstructs the original size of activation, and the deconvolutional layer densifies the coarse activation map obtained from up-sampling through swapping the forward and backward passes of a convolution, thus a single input activation is projected into multiple outputs after deconvolution, yielding an enlarged and dense feature map. Echo to the convolution path where image information is aggregated from fine details to global concept, a hierarchical structure of deconvolutional layers is used to recover image details at different levels, with the lower layers encoding overall image information and higher layers capturing fine details regarding skin tumors. In this way, the network can take both global information and fine details into account for tumor segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss function</head><p>For any pixel in the input image x ij , the corresponding FCN output p(w|x ij ) represents an estimated posterior probability that this pixel belongs to the skin tumor. Since FCN essentially performs a pixel-wise classification, cross entropy is usually used as the loss function <ref type="bibr" target="#b26">[27]</ref>[28]:</p><formula xml:id="formula_0">L(w) = - 1 N i × N j i,j [t ij ln p(w|x ij ) + (1 -t ij ) ln(1 -p(w|x ij ))]<label>(1)</label></formula><p>where t ij ∈ {0, 1} is the actual class of x ij with t ij = 1 for tumor and t ij = 0 for background. In dermoscopy, a skin tumor usually occupies a small region in the whole dermoscopic image. As the result, the pixel-wise classification tends to be biased towards the background and therefore there is a higher chance for tumor to be partially segmented or even missed. A typical solution is to assign a weight to each pixel during training that compensates the different frequencies of pixels from each class, so the contributions of tumor and background are re-balanced <ref type="bibr" target="#b27">[28]</ref>. However, this pixel-wise re-weighting procedure brings additional computation cost, especially when image augmentations are employed in the training procedure.</p><p>In this work, we propose a novel loss function based on Jaccard distance. Jaccard index <ref type="bibr" target="#b33">[34]</ref>, also known as the Jaccard similarity coefficient, is one of the most frequently used evaluation measures in medical image segmentation. The Jaccard distance, which measures dissimilarity between two sets, is complementary to the Jaccard index. Let M represent the ground truth of segmentation, which is normally a manuallyidentified tumor region, and C represent a computer-generated mask, the Jaccard distance is defined as:</p><formula xml:id="formula_1">d J (M, C) = 1-J(M, C) = 1- |M ∩ C| |M | + |C| -|M ∩ C| .<label>(2)</label></formula><p>d J (M C) itself is not differentiable, which makes it difficult to be directly applied into backpropagation. Even some minimization methods for non-differentiable functions can be introduced, it would be computationally expensive to generate a binary mask from continuous FCN output for each iteration during optimization. Thus, we designed the following loss function:</p><formula xml:id="formula_2">L dJ = 1 - i,j (t ij p ij ) i,j t 2 ij + i,j p 2 ij - i,j (t ij p ij ) ,<label>(3)</label></formula><p>With this loss function, a weight map is not needed to rebalance pixels from tumor region and background. Meanwhile, the proposed loss function is differentiable:</p><formula xml:id="formula_3">∂L dJ ∂p ij = - t ij [ i,j t 2 ij + i,j p 2 ij - i,j (t ij p ij )] [ i,j t 2 ij + i,j p 2 ij - i,j (t ij p ij )] 2 + (2p ij -t ij )[ i,j (t ij p ij )] [ i,j t 2 ij + i,j p 2 ij - i,j (t ij p ij )] 2 ,<label>(4)</label></formula><p>which can be efficiently integrated into backpropagation during network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FCN Training</head><p>Training a deep network model with a limited number of samples is a challenging task. As described in the previous section, our model has 19 layers and 290, 129 parameters to be learned. Even though the number of training samples available in this study is considered as a large-scale study in the field of digital dermoscopic image analysis, it is still relatively small as compared to the size of the network. In order to address this issue, we firstly apply FCN model to convert image segmentation into a pixel-wise classification problem. As the result, each pixel can be considered as an independent sample during network training, which greatly boosts the number of equivalent training samples. Then, We initialize the network weights using Xavier's technique <ref type="bibr" target="#b34">[35]</ref> and employ the following strategies to improve the efficiency of network training while reducing overfitting.</p><p>1) Adam stochastic optimization: Stochastic gradient descent (SGD) with mini-batch is usually employed as the optimization algorithm for neural network training <ref type="bibr" target="#b35">[36]</ref>. It is well known that learning rate is one of the critical hyperparameters that have a significant impact on classification performance. However, selecting proper learning rate and strategy can be fairly challenging. One commonly used strategy is to anneal the learning rate at each iteration t as α t0+t where α and t 0 dictate the initial learning rate and the time when the annealing starts respectively, but it tends to have slow convergence when the loss function is highly sensitive to some directions in parameter space while insensitive to others. The momentum algorithm <ref type="bibr" target="#b36">[37]</ref> can mitigate this issue, but with the expense of introducing another hyperparameter.</p><p>We adopt adam optimization algorithm <ref type="bibr" target="#b37">[38]</ref>, or adaptive moments, to adjust the learning rate based on the first and the second-order moments of the gradient at each iteration. Here the momentum is incorporated as an estimate of the first moment and the effective stepsize at each iteration t depends on the ratio between the bias-corrected first and second-order moments as ∆ t = α • mt / √ vt , with a smaller value indicating that there is greater uncertainty about whether the direction of mt corresponds to the direction of the true gradient. We found adam is fairly robust to the choice of hyperparameters, and set the learning rate α as 0.003 to speed up the training procedure in this study.</p><p>2) Batch normalization: We employ the batch normalization <ref type="bibr" target="#b38">[39]</ref> to reduce the internal covariate shift by normalizing the input distribution of every layer to the standard Gaussian distribution for each training mini-batch. For this purpose, we add a batch normalization layer to the output of every convolutional and de-convolutional layer. The batch normalization is performed over all locations in the same feature map such that different elements in the same map are normalized in the same way. We set batch size as 18 in this study.</p><p>3) Dropout: Dropout <ref type="bibr" target="#b39">[40]</ref> provides a powerful but computationally inexpensive way to reduce overfitting when training a very deep FCN model with limited data. This technique sets the output of each neuron in a given layer to zero with probability p, thus removes the contribution of those "dropped out" neurons from both forward pass and back-propagation. The subset of disabled neurons is drawn independently for each mini-batch and forms a different network architecture, then dropout trains the ensemble of all sub-networks that have different architectures but share weights in one epoch. In this way, a neuron cannot rely on the presence of particular other neurons and it is, therefore, forced to learn more robust features that are useful among different random subsets. This makes the trained FCN model more robust and improves the generalization ability.</p><p>We use dropout with p = 0.5 before conv5 and deconv4 layers in Figure <ref type="figure" target="#fig_1">2</ref>. Although it roughly doubles the number of iterations for convergence, dropout does reduce the overfitting substantially in our experiments.</p><p>4) Image augmentation: In order to improve the robustness of the proposed FCN model under a wide variety of image acquisition conditions, we employ image augmentation to further reduce model overfitting by artificially enlarging the training dataset with various image transformations. For the tumor segmentation on dermoscopic images, we primarily look for invariance to mild geometric transformation as well as robustness to pixel value variations, so we implement the following two types of image augmentations. Note that these augmentations only require little extra computation, so the transformed images are generated from the original images for every mini-batch within each iteration.</p><p>One type of image augmentation consists of a series of geometric transformations. Each original image in a minibatch is firstly flipped horizontally with probability of 0.5, then flipped vertically with another probability of 0.5. The rotation operation is applied to rotate the image by angle θ, which is randomly sampled from a Gaussian distribution in the range of [-40 The other type of image augmentation focuses on randomly normalizing the contrast of each channel in the training images. Specifically, given S c as the c-th channel of the input image, we introduce l L and l H -th percentile of S c where l L and l H are randomly selected from [0, 20] and [90, 100] respectively. Then the new value at each pixel in this channel can be determined as:</p><formula xml:id="formula_4">S ′ c =      0 S c &lt; S c (l L ) Sc-Sc(lL) Sc(lH )-Sc(lL) S c (l L ) ≤ S c ≤ S c (l H ) 1 S c &gt; S c (l H ) . (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>This scheme simulates the scenario that the input images are acquired with various intensities, contrasts and color illuminations. At test time, l L and l H are fixed as 10 and 95, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation</head><p>Our FCN-based lesion segmentation algorithm was implemented with Python based on Theano <ref type="bibr" target="#b40">[41]</ref> and Lasagne packages. The experiments were conducted on a Dell XPS 8900 desktop with Intel(R) i7-6700 3.4 GHz CPU and a GPU of Nvidia GeForce GTX 1060 with 6GB GDDR5 memory.</p><p>Our aim is to have the FCN model automatically learn those features that are useful for lesion segmentation, so a simple pre-processing is employed to facilitate the following learning procedure while preserving the original image information. By observing most of images in the training set have a height to width ratio of 3 : 4, we resize the images to 192 × 256 using bi-linear interpolation. RGB channels are kept as the input to the FCN model and each channel is rescaled to [0, 1].</p><p>The output of FCN model is a posterior probability map where each pixel value represents the probability that the pixel belongs to the lesion. We employ a dual-threshold method to obtain a binary tumor mask from this probability map. Specifically, a relatively high threshold (th H = 0.8 in our experiments) is firstly applied to the FCN output to generate a series of tumor candidates. The mass of each candidate region, which is the area weighted by the pixel values within this region, is calculated. The tumor center is chosen as the centroid of the region that has the largest mass among these candidates. Then a lower threshold (th L = 0.5) is applied to the probability map. After filling small holes with morphological dilation, the final tumor mask is determined as the region that embraces the tumor center. Since this post-processing only involves thresholding and morphological operations, the whole lesion segmentation can be done within two seconds.</p><p>Finally, a bagging-type ensemble strategy is implemented to combine outputs of different FCNs to further improve the image segmentation performance on the testing images. For each fold in the 5-fold cross validation, the total number of epochs is set as 500, and the optimal epoch that yields the best performance on the validation dataset is saved. Also saved is the corresponding trained FCN model. After cross validation is completed, another FCN model is trained using the entire training data and the total number of epochs is set as the maximum value among the optimal epochs obtained from cross validation. The outputs of these six FCN models are then averaged to predict the final segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL DESIGN AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Databases</head><p>We used two dermoscopy databases in our experiments. One is provided by the International Skin Imaging Collaboration (ISIC) for the 2016 International Symposium on Biomedical Imaging (ISBI 2016) challenge titled "Skin lesion analysis toward melanoma detection" <ref type="bibr" target="#b28">[29]</ref>. This database includes a training dataset with 900 annotated dermoscopic images (173 melanomas), and a testing dataset with 379 images (75 melanomas). They are 8-bit RGB images and the image size ranges from 542 × 718 to 2848 × 4288. Our model was trained and validated on the training set, and evaluated on the testing set. In order to further evaluate the generalization ability of our model, we also tested with an independent database called PH2 <ref type="bibr" target="#b29">[30]</ref> collected from Dermatology Service of Hospital Pedro Hispano in Portugal, which contains 200 dermoscopic images including 80 common nevi, 80 atypical nevi, and 40 melanomas. The image size in PH2 is fixed as 560 × 768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance evaluation</head><p>The output of the FCN model is binarized to a lesion mask. The performance of the proposed segmentation algorithm was assessed by comparing the computer-generated lesion mask with the ground truths created by expert clinicians. We used the evaluation metrics suggested in the ISBI 2016 challenge for benchmarking <ref type="bibr" target="#b28">[29]</ref>, including pixel-wise accuracy (AC), sensitivity (SE), specificity (SP), dice coefficient (DI), and Jaccard index (JA). Let T P , T N , F P , F N refer to the number of true positives, true negatives, false positives, and false negatives respectively, the evaluation metrics are defined as:</p><formula xml:id="formula_6">• AC = (T P + T N )/(T P + F P + T N + F N ) • SE = T P/(T P + F N ) • SP = T N/(T N + F P ) • DI = 2 • T P/(2 • T P + F N + F P ) • JA = T P/(T P + F N + F P ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Key component validation</head><p>As the first step, we analyzed the effect of some key components of the proposed model to the lesion segmentation performance. These components included input image size, optimization method, image augmentation, as well as cost function. In each experiment, we replaced one component with an alternative setting while keeping other components unchanged, then trained and evaluated the modified model using 5-fold cross validation on the ISBI training set, where 720 images were used for training and the rest 180 images were used for validation in each fold.  2) Adam optimization: We compared the adam optimization method to SGD with Nesterov momentum, in which the learning rate was set as 0.003 and momentum as 0.9. Figure <ref type="figure" target="#fig_2">3</ref> shows how the loss function is minimized on both training and validation datasets as network training processes -one with Adam optimization (red lines) and the other with Nesterov momentum (black lines). It is clear to see that Adam makes rapid progress lowering the loss function and consistently converges faster than Nesterov momentum on both training and validation datasets. Meanwhile, the validation curve of Nesterov momentum is quite noisy, which makes it less reliable than Adam when picking a model for testing phase. As shown in Table <ref type="table" target="#tab_0">III</ref>, the Jaccard index was reduced to 0.838 when using SGD with Nesterov momentum.</p><p>3) Image augmentation: We implemented two types of image augmentations in this work, with one consisting of   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss function:</head><p>We compared the proposed cost function based on Jaccard distance with the conventionally used cross entropy. The specificity using cross entropy as cost function was 0.972, which was slightly higher than the one using Jaccard distance (0.971). This is because the cross entropy models the contribution from both foreground and background pixels while Jaccard distance focuses on the foreground pixels, which is more related to the task of image segmentation. As the results shown in Table <ref type="table">V</ref>, the cost function based on Jaccard distance yielded a higher sensitivity (0.926 vs. 0.918 ) and a higher Jaccard index (0.861 vs. 0.854).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation on ISBI 2016 testing database</head><p>After validating those key components in our model, we applied the trained network to the 379 dermoscopic images in ISBI 2016 testing dataset. Figure <ref type="figure" target="#fig_4">4</ref> shows several challenging examples of automatic segmentation on both melanoma and non-melanoma lesions, including cases with irregular shapes (Fig. <ref type="figure" target="#fig_4">4(a) -(d)</ref>), cases with dark corners (Fig. <ref type="figure" target="#fig_4">4</ref> (b) and (c)), cases with inhomogeneous appearance (Fig. <ref type="figure" target="#fig_4">4</ref> (a), (b) and </p><formula xml:id="formula_7">(d))</formula><p>, cases with heavy hair artifacts (Fig. <ref type="figure" target="#fig_4">4</ref> (e) and (f)), cases with color charts (Fig. <ref type="figure" target="#fig_4">4</ref> (f) and (g)), as well as cases with extremely low contrast (Fig. <ref type="figure" target="#fig_4">4</ref> (g) and (h)). Overall, our model delineated the skin lesion accurately and is robust to various image acquisition conditions. Figure <ref type="figure" target="#fig_5">5</ref> plots the distribution of the segmentation results on 304 non-melanoma and 75 melanoma lesions. The Jaccard index for non-melanoma lesions ranged from 0.181 to 0.971 with the mean of 0.843, while it ranged from 0.526 to 0.967 with the mean of 0.861 for melanoma lesions.</p><p>Table <ref type="table" target="#tab_0">VI</ref> compares the results of the proposed FCN model with the top three performers on the ISBI 2016 Challenge testing dataset. Here FCN 1 -5 are models trained during 5fold cross validation (720 images for training and 180 images for validation), and FCN-6 was trained using the entire 900 images. Five out of these six models achieved performance that place them as the second rank in the 2016 ISBI challenge on melanoma lesion segmentation, with FCN-6 performing best to yield a Jaccard index of 0.836. Ensembling these FCN models further boosted the segmentation performance in terms of Jaccard index to 0.847, which outperformed the top performer of ISBI 2016 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation on PH2 database</head><p>In order to further evaluate the performance of the proposed FCN model, we conducted the following two experiments on PH2, an independent dermoscopic image database that is widely used for algorithm validation and benchmark <ref type="bibr" target="#b29">[30]</ref>. In the first experiment, we directly applied the pre-trained model to perform image segmentation on PH2. Note that only FCN-2, the best cross-validated model, was considered in this section to simplify the evaluation procedure. Only the lower threshold (th L ) was experimentally changed to 0.6 by visually evaluating the segmentation performance on randomly selected 10 images. In the second experiment, we fine-tuned the weights of FCN-2 by further training the network with PH2 data. Specifically, the PH2 database was randomly divided into  <ref type="bibr" target="#b41">[42]</ref>, an image-wise supervised learning (IWSL) by Bi et al. <ref type="bibr" target="#b42">[43]</ref>, and a saliency-based method (SSLS) <ref type="bibr" target="#b43">[44]</ref> and its robust variation (RSSLS) by Ahn et al. <ref type="bibr" target="#b44">[45]</ref>. Besides DI, we also used divergence value (DV) to evaluate the segmentation performance, which measures the percentage of segmentation errors and can be obtained by:</p><formula xml:id="formula_8">DV = M ⊕ C M × 100% = F P + F N T P + F N × 100%,<label>(6)</label></formula><p>where M ⊕ C is the XOR operation between the ground truth and the computer-generated lesion mask.</p><p>The direct application of FCN-2 on PH2 achieved a reasonably good segmentation result with DI of 0.915 and DV of 18.48%, even though they are slightly inferior to the results of those state-of-the-art methods. By further training the FCN model with PH2 data, our retrained FCN model achieved the best performance with DI of 0.938 and DV of 12.81%. Because we only used 100 images for training and validation, a model retraining can be completed within 5 minutes.</p><p>It should be noted that not all the images in PH2 were used in the first three of the fore-mentioned studies due to various reasons. Ma employed 160 images with 8 images diagnosed as melanoma and 152 as either typical or atypical nevi, and Bi excluded 40 images if they either had multiple lesions, or were heavily affected by illumination or hair artifacts. Ahn excluded 40 images where the lesions connected to the image edges in their initial study of SSLS. On the contrary, our study employed all the 200 images to evaluate the proposed FCN model under different conditions. Figure <ref type="figure" target="#fig_6">6</ref> shows a few segmentation results on some challenging cases, including cases affected by hair artifacts (Fig. <ref type="figure" target="#fig_6">6</ref> (a) and (c)) and by illumination variations (Fig. <ref type="figure" target="#fig_6">6 (b</ref>)), cases whose boundaries reached the image edges (Fig. <ref type="figure" target="#fig_6">6</ref> (b) and (d)), and two melanoma cases (Fig. <ref type="figure" target="#fig_6">6 (c</ref>) and (d)). Our method achieved promising results on these challenging cases, demonstrating that the proposed FCN model is fairly robust to various patient populations and can be quickly adapted to new patient population via transfer learning.</p><p>V. DISCUSSIONS Delineating lesion from surrounding skin regions often serves as one of the prerequisite steps for computerized    distinct boundaries to adjacent skin, as well as the presence of various artifacts. In this work, we presented a fully deep convolutional network architecture for automatic skin lesion segmentation that was found to outperform the state-of-the-art on two publicly available databases. Efficiently training a deep network is a challenging task for skin lesion segmentation. When a network goes deeper, problems such as vanish gradient may become more prominent, which makes it more difficult to tune the parameters of the early layers <ref type="bibr" target="#b34">[35]</ref>. Meanwhile, a deeper network usually requires more training samples to reduce overfitting, however, the dataset available in skin lesion segmentation is fairly limited as compared to natural images. This dilemma further exacerbates the difficulties in training the deep network. We exploited the following strategies to address these issues: 1) Instead of patch-wise supervised learning, we employed a fully convolutional model to predict the lesion mask in a pixel-wise fashion, which greatly boosted the number of equivalent training samples; 2) Batch normalization reduced the internal covariate shift and Adam optimization allowed us to use a fairly large learning rate to speed up the training procedure; 3) Dropout and image augmentation efficiently alleviated overfitting with limited training data, and 4) We used Jaccard distance as the loss function during network training, which is more related to the task of image segmentation as compared to cross entropy, yielding an improved segmentation performance. It should be noted that these training strategies are general enough to be extended to other medical image segmentation problems.</p><p>Instead of developing sophisticated pre-and post-processing techniques to account for various image acquisition conditions and to remove artifacts, we designed a fully trainable system with minimal pre-and post-processing to perform lesion segmentation in an end-to-end fashion. Figure <ref type="figure" target="#fig_7">7</ref> shows an example of how the trained network deals with these artifacts. Here (a) is the original image, (b) and (c) show the outputs of the first two convolutional layers where each activation map visualizes how the network responds to different image features. We can see that the lesion, skin hairs and the color chart are activated in different maps, which greatly facilitates further processing in the following layers to predict a clear probability map as shown in (d). The strategy of relying on the trained network while simplifying pre-and post-processing also dramatically increased the speed of inference while maintaining the the generalization of the proposed method. For a typical 768 × 1024 image, the entire segmentation procedure only took about 0.14 second.</p><p>The generalization capability of our model was further validated by testing on the independent PH2 database. Without changing any parameters in the trained network, our method already achieved consistently promising performance on this new database. This is because our method only uses raw image data as input and allows the neural network to directly learn features from them. A well designed network tends to learn a more comprehensive and robust features as compared to handcraft features that reflect the prior knowledge of the feature designers and their understanding of data. On the other hand, since the PH2 images are acquired at the Dermatology Service of Hospital Pedro Hispano in Portugal, the segmentation performance is inevitably affected by variabilities such as patient population, image acquisition conditions, as well as the dermatologists who contoured the lesions. By fine tuning the FCN model on the PH2 data, we were able to address these variations and further boost the segmentation performance. Transfer learning has been shown to be beneficial in medical image analysis tasks such as computer-aided detection (CADe) <ref type="bibr" target="#b45">[46]</ref>, we demonstrated in this work it is also an effective and efficient way to build a deep network for lesion segmentation with small dataset.</p><p>In order to achieve a good segmentation performance, a few parameters were experimentally determined using grid search during the 5-fold cross-validation with ISBI training dataset. For the proposed FCN model, the mainly tuned hyper-parameters included the number of layers, kernel size and the number of feature channels in each convolutional/deconvolutional layer. A set of hyper-parameters was selected if its model performed best on the validation dataset.</p><p>There is a trade-off between model complexity and its generalization ability, as well as the computational demand for model training. We found that increasing the capability of the FCN model by adding additional layers and doubling feature channels did not necessarily yield performance improvement while significantly increasing the time for model training. Another set of parameters included two thresholds that convert a probability map to a binary tumor mask. The selection criterion for the higher threshold was to make sure all the tumors could be correctly identified. Considering that a lot of datasets may not have labeled images, the operator normally needs to have dermatologists draw contours on several images then adjust the lower threshold to match manually-drawn contours. We mimicked this scenario when applying the trained FCN model to the PH2 database without retraining the model. It should be noted that we did not spend much effort to fine tune these two thresholds since it is not the focus of our study, but the segmentation performance might be improved by reoptimizing them for a new dataset.</p><p>Although our model has achieved promising segmentation accuracy on most of cases in two independent databases, there are cases where our model needs further improvement. Figure <ref type="figure" target="#fig_8">8</ref> (a) shows the most challenging case (ISIC_0011338) among all the ISBI testing images where both our model and all the top three teams in the ISBI challenge performed poorly, while (b) shows a failed case that has very low contrast. Fig. <ref type="figure" target="#fig_8">8 (c</ref>) and (d) are examples for lesion under-and over-segmentation. While the level of under/over segmentation can be controlled by adjusting the lower threshold on the probability map, it is not straightforward to automatically tune this parameter based on a specific image. One way to further improve the segmentation performance is to combine the proposed method with other post-processing techniques, such as levelsets <ref type="bibr" target="#b23">[24]</ref>. Another option is to integrate Bayesian learning, such as conditional random field (CRF) <ref type="bibr" target="#b46">[47]</ref>, into our model to improve its discriminative capability. These interesting topics are worthy being investigated in the future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we proposed a fully automatic framework based on deep convolutional neural network for skin lesion segmentation on dermoscopic images. Several effective training strategies were implemented to tackle the challenges that training a deep network may face when only limited training data is available. We designed a novel loss function based on the Jaccard distance to further boost the segmentation performance. Compared to the conventionally used cross entropy, the Jaccard distance based loss function directly maximizes the overlap between the foreground of the ground truth and that of the predicted segmentation mask, and thus eliminates the needs of data re-balancing when the numbers of foreground and background pixels are highly unbalanced, such as binary medical image segmentation. Our approach outperformed the state-of-the-art methods when evaluating on an open challenge database of Skin Lesion Analysis Towards Melanoma Detection in ISBI 2016, as well as on another publicly available database of PH2. Our results also clearly demonstrated that the proposed method is robust to various image artifacts and imaging acquisition conditions while using minimum pre-and post-processing. We believe this method can generalize well to other medical image segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Challenges of automated lesion segmentation on dermoscopic images. (a) large variety of sizes; (b) irregular and fuzzy borders; (c) low contrast between lesion and the surrounding skin; and (d) artifacts. Red dash line represents the skin lesions contoured by dermatologists.</figDesc><graphic coords="1,316.56,175.80,241.63,189.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the proposed fully convolutional network (FCN). This model consists two pathways, in which contextual information is aggregated via convolution (c) and pooling (p) in the convolutional path and full image resolution is recovered via deconvolution (d) and up-sampling (u) in the deconvolutional path. The architectural details are described in Table I</figDesc><graphic coords="3,70.80,55.85,470.02,110.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison between Adam and SGD with Nesterov momentum for training and validating of the proposed model.</figDesc><graphic coords="6,70.92,138.60,207.07,161.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>geometric transformations (GT) and the other focusing on image contrast random normalization (CRN). In order to investigate the effect of each image augmentation strategy to the final segmentation performance, we conducted the following three experiments. In the first experiment, the FCN model was trained with the original images and no image augmentation was applied, which yielded an average Jaccard index of 0.820 with training time of 11 seconds per epoch. We then artificially increased training set by altering the contrast of each image channel in the second experiment and applying geometric transformations in the third experiment, respectively. As can be observed from TableIV, both image augmentation strategies improved the segmentation performance, with 1.7% gain by altering contrast and 2.8% gain by geometric transformations, respectively. The training time per epoch was 12 seconds when including contrast alteration and 26 seconds when including geometric transformations. Since geometric transformations include image flipping (vertically and/or horizontally), rotation and scaling, this augmentation strategy can potentially generate more diverse training data as compared to simple contrast alteration, thus yield a better segmentation performance. Finally, the combination of these two strategies improved the Jaccard index by about 5%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Segmentation results for eight examples in the ISBI 2016 testing dataset, where (a) -(d) are melanoma and (e) -(h) are non-melanoma lesions. In each figure, the red dash line indicates the outline contoured by dermatologist and the yellow solid line is the result of automatic segmentation. The second and fourth rows are the corresponding probability maps generated by the proposed FCN model.</figDesc><graphic coords="8,48.96,56.23,519.08,364.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of segmentation performance in terms of Jaccard index on ISBI 2016 testing dataset.</figDesc><graphic coords="8,72.12,480.48,204.77,161.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Segmentation results of some challenging cases in the PH2 database, including cases affected by hair artifacts (a) and by illumination variation (b), a case with low contrast (c) and one whose boundary reaches the image edges (d). The top row includes two non-melanoma and the bottom row are two melanoma lesions. The red dash and yellow solid contours indicate the ground truth and the segmentation results of fine-tuned model, respectively.</figDesc><graphic coords="8,320.40,487.39,234.11,187.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visualization of activations in our network. The image in (a) is the input to our model after contrast re-normalization, and (d) is the predicted probability map. (b) and (c) are the outputs of the first two convolutional layers where each activation map shows how the network responds to different image features.</figDesc><graphic coords="9,72.96,55.63,465.97,178.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Cases where our results were suboptimal. The red dash and yellow solid contours indicate the ground truth and the segmentation result, respectively. (a) shows the most challenging case in the ISBI 2016 testing dataset where both our method and all the top three teams in the ISBI 2016 challenge performed poorly. (b) is a failed case that has very low contrast. (c) and (d) are examples for lesion under-and over-segmentation, respectively.</figDesc><graphic coords="10,316.56,56.08,241.87,190.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ARCHITECTURAL</head><label>I</label><figDesc>DETAILS OF THE PROPOSED FCN MODEL.</figDesc><table><row><cell>Conv</cell><cell>Filter</cell><cell>Output</cell><cell>Deconv</cell><cell>Filter</cell><cell>Output</cell></row><row><cell>c-1</cell><cell>5 × 5</cell><cell>188 × 252 × 8</cell><cell>d-1</cell><cell>5 × 5</cell><cell>21 × 29 × 64</cell></row><row><cell>c-2</cell><cell>3 × 3</cell><cell>186 × 250 × 16</cell><cell>u-1</cell><cell>2 × 2</cell><cell>42 × 58 × 64</cell></row><row><cell>p-1</cell><cell>2 × 2</cell><cell>93 × 125 × 16</cell><cell>d-2</cell><cell>4 × 4</cell><cell>45 × 61 × 32</cell></row><row><cell>c-3</cell><cell>4 × 4</cell><cell>90 × 122 × 32</cell><cell>u-2</cell><cell>2 × 2</cell><cell>90 × 122 × 32</cell></row><row><cell>p-2</cell><cell>2 × 2</cell><cell>45 × 61 × 32</cell><cell>d-3</cell><cell>4 × 4</cell><cell>93 × 125 × 16</cell></row><row><cell>c-4</cell><cell>4 × 4</cell><cell>42 × 58 × 64</cell><cell>u-3</cell><cell>2 × 2</cell><cell>186 × 250 × 16</cell></row><row><cell>p-3</cell><cell>2 × 2</cell><cell>21 × 29 × 64</cell><cell>d-4</cell><cell>3 × 3</cell><cell>188 × 252 × 8</cell></row><row><cell>c-5</cell><cell>5 × 5</cell><cell>17 × 25 × 64</cell><cell>output</cell><cell>5 × 5</cell><cell>192 × 256 × 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This image is then rescaled by a Gaussian random factor in the range of [0.7, 1.3], and shifted by a uniformly-distributed random 2d displacement vector within the range of 10 pixels. Although the resulting training samples are highly dependent, this scheme significantly increases the size of training set and allows us to use a deeper FCN without substantial overfitting.</figDesc><table /><note><p>• , 40 • ].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We then trained FCN-2 with 50 images and validated with the rest 50 images in group I. The total number of epochs was set to 100. The model performing best on the validation images was selected and applied to perform image segmentation in group II. The same procedure was repeated by using images in group II as training and validation, and the tuned model was tested on group I.TableVIIcompares our model with several recently published methods, including a deformable model (DM) by Ma et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">PERFORMANCE EVALUATION ON THE PH2 DATABASE</cell></row><row><cell></cell><cell>DM</cell><cell>IWSL</cell><cell>SSLS</cell><cell>RSSLS</cell><cell>FCN</cell><cell>FCN-retrained</cell></row><row><cell>DI</cell><cell>-</cell><cell>0.925</cell><cell>0.913</cell><cell>0.911</cell><cell>0.915</cell><cell>0.938</cell></row><row><cell>DV(%)</cell><cell>13.92</cell><cell>14.99</cell><cell>17.48</cell><cell>16.45</cell><cell>18.48</cell><cell>12.81</cell></row><row><cell cols="7">two groups (I and II), each of which included 100 images.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGMENT</head><p>The authors are grateful to International Skin Imaging Collaboration (ISIC), the organizers of 2016 International Symposium on Biomedical Imaging (ISBI 2016) challenge of "Skin lesion analysis towards melanoma detection", and the Dermatology Service of Hospital Pedro Hispano in Portugal, who make the dermoscopic image data publicly available. They also thank the anonymous reviewers whose comments and suggestions helped improve this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer statistics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="7" to="30" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>CA Cancer J Clin.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combinations of radiation therapy and immunotherapy for melanoma: a review of clinical outcomes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Postow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Radiation Oncol Biol Phys</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="986" to="997" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Final version of 2009 AJCC melanoma staging and classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gershenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Clin Oncology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="6199" to="6206" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dermoscopy compared with naked eye examination for the diagnosis of primary melanoma: a meta-analysis of studies performed in a clinical setting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vestergaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Macaskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Menzies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Dermatol</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="669" to="676" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computerized analysis of pigmented skin lesions: a review</title>
		<author>
			<persName><forename type="first">K</forename><surname>Korotkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. intell. Med</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="90" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated melanoma recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ganster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wildling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A methodological approach to the classification of dermoscopy images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Kingravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Aslandogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Stoecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imag. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparison of segmentation methods for melanoma diagnosis in dermoscopy images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Marçal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lesion border detection in dermoscopy images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Stoecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imag. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="153" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lesion border detection in dermoscopy images using ensembles of thresholding methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Emre</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Skin Res. Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="252" to="e258" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simpler, faster, more accurate melanocytic lesion segmentation through meds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-M</forename><surname>Cappelleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peserico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="557" to="565" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mean shift based gradient vector flow for image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation of dermoscopy images using wavelet networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Sadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zekri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kolahdouzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1134" to="1141" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic segmentation of dermoscopy images using self-generating neural networks seeded by genetic algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1012" to="1019" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A state-of-the-art survey on lesion border detection in dermoscopy images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Adv. Neural Inf. Process. Sys</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI 2013</title>
		<meeting>MICCAI 2013</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-instance deep learning: Discover discriminative local anatomies for bodypart recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1332" to="1343" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A CNN Regression Approach for Real-Time 2D/3D Registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1352" to="1363" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans.Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Urinary bladder segmentation in CT urography using deep-learning convolutional neural network and level sets</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Samala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Caoili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1882" to="1896" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Avendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheradvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jafarkhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Extraction of skin lesions from non-dermoscopic images using deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nasr-Esfahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soroushmehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Najarian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02374</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI 2015</title>
		<meeting>MICCAI 2015</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halpern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01397</idno>
		<title level="m">Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PH 2-A dermoscopic image database for research and benchmarking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Marcal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMBC 2013</title>
		<meeting>EMBC 2013</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="5437" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning ensembles for melanoma recognition in dermoscopy images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04662</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated melanoma recognition in dermoscopy images via very deep residual networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="994" to="1004" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV 2015</title>
		<meeting>ICCV 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The distribution of the flora in the alpine zone</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New phytologist</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="1912">1912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent tricks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="421" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML 2013</title>
		<meeting>ICML 2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Theano: A python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T D</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A novel approach to segment skin lesions in dermoscopic images based on a deformable model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M R</forename><surname>Tavares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="615" to="623" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated skin lesion segmentation via image-wise supervised learning and multi-scale superpixel based cellular automata</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISBI 2016</title>
		<meeting>ISBI 2016</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1059" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automated saliency-based lesion segmentation in dermoscopic images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMBC</title>
		<meeting>EMBC</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="3009" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency-based lesion segmentation via background detection in dermoscopic images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
