<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prodigy: Improving the Memory Latency of Data-Indirect Irregular Workloads Using Hardware-Software Co-Design</title>
				<funder>
					<orgName type="full">Air Force Research Laboratory</orgName>
				</funder>
				<funder ref="#_xWzMcfh">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
				<funder ref="#_RH92Scj #_4EYpuRZ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nishil</forename><surname>Talati</surname></persName>
							<email>talatin@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><surname>May</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin</orgName>
								<address>
									<settlement>Madison</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armand</forename><surname>Behroozi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yichen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuba</forename><surname>Kaszyk</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christos</forename><surname>Vasiladiotis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tarunesh</forename><surname>Verma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brandon</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawen</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">Magnus</forename><surname>Morton</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Agreen</forename><surname>Ahmadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><surname>Austin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>O'boyle</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><surname>Drelinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prodigy: Improving the Memory Latency of Data-Indirect Irregular Workloads Using Hardware-Software Co-Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Irregular workloads are typically bottlenecked by the memory system. These workloads often use sparse data representations, e.g., compressed sparse row/column (CSR/CSC), to conserve space at the cost of complicated, irregular traversals. Such traversals access large volumes of data and offer little locality for caches and conventional prefetchers to exploit.</p><p>This paper presents Prodigy, a low-cost hardware-software codesign solution for intelligent prefetching to improve the memory latency of several important irregular workloads. Prodigy targets irregular workloads including graph analytics, sparse linear algebra, and fluid mechanics that exhibit two specific types of datadependent memory access patterns. Prodigy adopts a "best of both worlds" approach by using static program information from software, and dynamic run-time information from hardware. The core of the system is the Data Indirection Graph (DIG)-a proposed compact representation used to express program semantics such as the layout and memory access patterns of key data structures. The DIG representation is agnostic to a particular data structure format and is demonstrated to work with several sparse formats including CSR and CSC. Program semantics are automatically captured with a compiler pass, encoded as a DIG, and inserted into the application binary. The DIG is then used to program a low-cost hardware prefetcher to fetch data according to an irregular algorithm's data structure traversal pattern. We equip the prefetcher with a flexible prefetching algorithm that maintains timeliness by dynamically adapting its prefetch distance to an application's execution pace.</p><p>We evaluate the performance, energy consumption, and transistor cost of Prodigy using a variety of algorithms from the GAP, HPCG, and NAS benchmark suites. We compare the performance of Prodigy against a non-prefetching baseline as well as stateof-the-art prefetchers. We show that by using just 0.8KB of storage, Prodigy outperforms a non-prefetching baseline by 2.6? and saves energy by 1.6?, on average. Prodigy also outperforms modern data prefetchers by 1.5-2.3?.</p><p>Index Terms-DRAM stalls, irregular workloads, graph processing, hardware-software co-design, programming model, programmer annotations, compiler, and hardware prefetching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sparse irregular algorithms are widely deployed in several application domains including social networks <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b75">[76]</ref>, online navigation systems <ref type="bibr" target="#b38">[39]</ref>, machine learning <ref type="bibr" target="#b41">[42]</ref>, and genomics <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Despite their prevalence, current hardwaresoftware implementations on the CPUs offer sub-optimal performance that can be further improved. This is due to the irregular  nature of their memory access patterns over large data sets, which are too big to fit in the on-chip caches, leading to several costly DRAM accesses. Therefore, traditional techniques to improve memory latency-out-of-order processing, on-chip caching, and spatial/address-correlating data prefetching <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b94">[95]</ref>, are inadequate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program the prefetcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program the prefetcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate prefetch requests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate prefetch requests</head><p>There is a class of prefetchers <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b97">[98]</ref> which focuses on linked data structure traversals using pointers. In graph algorithms, for example, these prefetchers fall short for two reasons. First, graph algorithms often use compressed data structures with indices instead of pointers. Second, graph traversals access a series of elements in a data structure within a range determined by another data structure. These prefetchers are not designed to accommodate such complex indirection patterns.</p><p>Recently, several prefetching solutions have been proposed targeting irregular workloads. Hardware prefetchers rely on capturing memory access patterns using explicit programmer support <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, learning techniques <ref type="bibr" target="#b76">[77]</ref>, and intelligent hardware structures <ref type="bibr" target="#b98">[99]</ref>. Limitations of these approaches include their limited applicability to a subset of data structures and indirect memory access patterns <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b98">[99]</ref> or high complexity and hardware cost to support generalization <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b76">[77]</ref>. While software prefetching <ref type="bibr" target="#b6">[7]</ref> can exploit static semantic view of algorithms, it lacks dynamic run-time information and struggles to maintain prefetch timeliness.</p><p>In this paper, we propose a hardware-software co-design for improving the memory latency of several important irregular workloads exhibiting arbitrary combinations of two specific memory access patterns. The goals of this design are threefold: (a) automatically prefetch all the key data structures expressing irregular memory accesses, (b) exploit dynamic run-time information for prefetch timeliness, and (c) realize a low-cost hardware prefetching mechanism. To this end, we propose a compact representation called the Data Indirection Graph (DIG) to communicate workload attributes from software to the hardware. The DIG representation efficiently encodes the program semantics, i.e., the layout and access patterns of key data structures, in a weighted directed graph structure. Fig. <ref type="figure" target="#fig_0">1</ref> presents the overview of our proposal. The relevant program semantics are extracted through a compile-time analysis, and this information is then encoded in terms of the DIG representation and inserted in the application binary. During run-time, the DIG is used to program the hardware prefetcher making it cognizant of the indirect memory access patterns of the workload so it can cater its prefetches accordingly.</p><p>Prodigy is a pattern-specific solution that targets two types of data-dependent indirect memory accesses, which we call singlevalued indirection and ranged indirection. Single-valued indirection uses data from one data structure to index into another data structure; it is commonly used to find vertex properties in graph algorithms. Ranged indirection uses two values from one data structure as base and bounds to index into a series of elements in another data structure; this technique is commonly used to find neighbors of a vertex in graph algorithms. Based on this observation, we propose a compact DIG representation that abstracts this information in terms of a weighted directed graph (unrelated to the input graph data set). The nodes of the DIG represent the memory layout information of the data structures, i.e., address bounds and data sizes of arrays. Weighted edges represent the type of indirection between data structures. We present a compiler pass to automatically extract this information and instrument the binary with API calls to generate the DIG at a negligible cost. Our results show that the DIG is agnostic to any particular data representation; it works well for various sparse data formats including compressed sparse row/column (CSR/CSC).</p><p>We design a low-cost hardware prefetcher that can be programmed using the DIG representation communicated from software. We store the DIG in prefetcher-local memory to make informed prefetching choices. The prefetcher reacts to demand accesses and prefetch fills <ref type="foot" target="#foot_0">1</ref> to the L1D cache and issues nonbinding prefetches (i.e., prefetched data placed in the L1D cache) based on an irregular algorithm's memory traversal pattern. To track the progress of the prefetch sequences and enable non-blocking prefetching, we introduce the PreFetch status Handling Register (PFHR) file. Additionally, we present an adaptive prefetching algorithm that selectively drops prefetch sequences when the core catches up to the prefetcher. We name our system ProDIGy as it uses software analysis coupled with hardware prefetcher using the program's DIG representation.</p><p>We evaluate the benefits of Prodigy in terms of performance, energy consumption, and hardware overhead. For evaluation, we use five graph algorithms from the GAP benchmark suite <ref type="bibr" target="#b15">[16]</ref> with five real-world large-scale data sets from <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b58">[59]</ref>, two sparse linear algebra algorithms from the HPCG benchmark suite <ref type="bibr" target="#b28">[29]</ref>, and two computational fluid dynamics algorithms from the NAS parallel benchmark suite <ref type="bibr" target="#b11">[12]</ref>. We compare our design with a non-prefetching baseline, GHBbased global/delta correlation (G/DC) data prefetcher, and stateof-the-art prefetchers, i.e., IMP <ref type="bibr" target="#b98">[99]</ref>, Ainsworth and Jones' <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, DROPLET <ref type="bibr" target="#b14">[15]</ref>, and software prefetching <ref type="bibr" target="#b7">[8]</ref>.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> presents a highlight of performance benefits of Prodigy on the PageRank algorithm running on the livejournal data set <ref type="bibr" target="#b58">[59]</ref>. Compared to a non-prefetching baseline, Prodigy reduces the DRAM stalls by 8.2? resulting in a significant endto-end speedup of 2.9? compared to the marginal speedups observed using a traditional G/DC prefetcher that cannot predict irregular memory access patterns and DROPLET <ref type="bibr" target="#b14">[15]</ref> which only prefetches a subset of data structures. Section VI presents further comparisons with <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b98">[99]</ref>. Across a complete set of 29 workloads, we show a significant average speedup of 2.6? and energy savings of 1.6? compared to a non-prefetching baseline. Using our evaluation framework, we further show that Prodigy outperforms IMP <ref type="bibr" target="#b98">[99]</ref>, Ainsworth and Jones' prefetcher <ref type="bibr" target="#b5">[6]</ref>, and DROPLET <ref type="bibr" target="#b14">[15]</ref> by 2.3?, 1.5?, and 1.6?, respectively. The compact DIG representation allows Prodigy to achieve high speedups at a mere 0.8KB of hardware storage overhead. In comparison, by simply scaling the non-prefetching baseline to use more cores to maximize the memory bandwidth and achieve similar throughout would require 5? more cores.</p><p>Prodigy is a specialized approach for critical memory latency-bound applications. When a processor is not running these applications, Prodigy will be turned off. In the age of dark silicon <ref type="bibr" target="#b34">[35]</ref>, state-of-the-art hardware frequently employs specialized accelerators for key applications. With Prodigy's low-cost design (0.8KB storage requirement), it is a modest price to pay for the efficiency it provides.</p><p>In summary, we make the following contributions:</p><p>? A compact representation of data traversal patterns, called a DIG (Data Indirection Graph), for irregular workloads   with any combination of two specific data-dependent memory access patterns.  A low-cost hardware prefetching design that uses this representation to prefetch data based on an irregular algorithm's memory traversal pattern in a timely manner. ? A resulting hardware-software co-designed system with an average speedup of 1.7? compared to the state-of-the-art prefetchers; average speedup and energy savings of 2.6? and 1.6? compared to a non-prefetching baseline at a negligible storage requirement of 0.8KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>In this section, we use breadth-first search (BFS) graph algorithm as a representative irregular algorithm and discuss its data structures and algorithmic traversal pattern that leads to sub-optimal performance on CPUs.</p><p>Compressed sparse row (CSR) is a space-efficient technique for representing a sparse matrix, and it is commonly used to represent in-memory graph data sets. It uses two arrays to store a graph: an edge list that stores the non-zero elements of the graph's adjacency matrix in a one-dimensional array, and an offset list that contains the base index/pointer of the edge list elements for each vertex. For example, consider a graph and its CSR structure as shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>.</p><p>Typically, BFS graph traversal uses CSR format to conserve space by storing non-zero values. BFS traverses all vertices at the current depth (i.e., distance from the source vertex) before moving onto the next depth. BFS is a fundamental algorithm, and is the basis of other graph algorithms (e.g., BC and SSSP). In addition to the offset and edge lists, BFS also uses two software arrays called the work queue and the visited list. The work queue<ref type="foot" target="#foot_1">2</ref> stores a set of vertices to be processed in the future. The visited list keeps track of already processed vertices to avoid processing them again. Fig. <ref type="figure" target="#fig_3">3</ref>(a) describes the traversal pattern of the BFS algorithm. We assume that offset list and edge list data structures are populated in memory. In addition, memory is allocated for work queue and visited list. As a first step, the source vertex (source) is pushed onto the work queue. Then, the algorithm chooses a vertex from the work queue and scans its neighbors (by indexing into offset list and edge list). If any of the scanned neighbors has not already been visited, then it is marked visited and is added to the work queue. A graphical representation of this traversal is shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>.</p><p>We observe two major bottlenecks in this algorithm: (a) data-dependent loads to the offset, edge, and visited lists and (b) a load-dependent branch instruction. Data-dependent reads for large-scale graphs are costly latency-wise because of their massive data footprint and random memory access patterns. Due to lack of locality, data for most of these loads are not found in caches. Moreover, control-flow instructions incur high penalty for two reasons. First, their data-dependent nature makes it challenging for branch predictors to predict the correct branch outcomes. Second, as reported by Srinivasan and Lebeck <ref type="bibr" target="#b88">[89]</ref>, in the case of an incorrectly predicted branch, much unnecessary work is performed while waiting for the load operation to return its data and correct the mispredicted branch. To better understand this bottleneck, Fig. <ref type="figure">4</ref> shows the breakdown of execution times for various irregular workloads running on an eight-core machine with three levels of cache hierarchy using the methodology shown in Section V. The figure clearly shows that these applications are stalled on DRAM for more than 50% of the time and have non-negligible branch misprediction stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED PROGRAMMING MODEL</head><p>Prodigy's novel programming model captures an algorithm's semantic behavior, including its data structure layout and memory access patterns, in a compact graph representation which is communicated to the hardware. We present two techniques to construct this representation within the program-(a) manual code insertion by the programmer, and (b) automatic code generation using compiler analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Indirection Graph (DIG): Compact Representation of Program Semantics</head><p>We make the key observation that two specific data-dependent indirect memory access patterns are used in a wide range of irregular workloads. Taking this as a foundation, we can construct combinations of these patterns that span sets of irregular memory accesses for different algorithms.</p><p>With this insight, we propose a graph representation, which we call a Data Indirection Graph (DIG), to capture the relationship between data structures for irregular algorithms. In a DIG, each node represents a data structure (e.g., the visited list in BFS), and each directed weighted edge represents a datadependent access. Fig. <ref type="figure" target="#fig_4">5</ref> shows an example DIG representation for the BFS algorithm. Nodes of the DIG, which store data structure information, have the following fields: node_ida unique identifier to reference the data structure, and an address identifier-a method for identifying which part of the address space belongs to the data structure represented by the node. For example, the address identifier for an array are: base_addr-base address of the array, capacitynumber of data elements in the array, and data_size-data size of each element of the array in bytes.</p><p>Edges of the DIG, which store the algorithmic traversal pattern between data structures have the following fields: src_base_addr-base address of the source data structure from which data are read to index into the destination data structure, dest_base_addr-base address of the data structure that is indexed into, and edge_type-data-dependent indirect access pattern from source node to destination node. As stated before, Prodigy supports two types of indirection patterns that are abstracted using edge weights of w0 and w1. Fig. <ref type="figure" target="#fig_4">5(c,</ref><ref type="figure">d</ref>) show these two types of data-dependent indirection functions supported by our representation, i.e., single-valued indirection (e.g., indirection between edge list and visited list for BFS) and ranged indirection (e.g., indirection between offset list and edge list in BFS). Additionally, we define a special edge called a trigger edge (w2 in Fig. <ref type="figure" target="#fig_4">5(a)</ref>), which is a selfedge to the data structure triggering prefetches. Trigger edge contains node_base_addr-data structure base address, and edge_type-details of prefetch sequence initialization (more details in Section IV). A trigger edge represents the control flow specifying the prefetch sequence to initialize.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Construction and Communication of the DIG</head><p>This section discusses how to generate the DIG representation from software and communicate it to hardware. We first describe how a programmer can achieve this by manually inserting simple annotations to the application source code using our API calls. To reduce the burden on the programmer, we further propose a compiler analysis and code generation technique to automatically analyze the application source code, construct the DIG representation, and instrument the application binary using the proposed API calls.</p><p>1) Using Programmer Annotations: Assuming that the programmer is cognizant of the key data structures and traversal algorithms used in the application, they can add simple API calls in the application source code to construct the DIG representation. Fig. <ref type="figure" target="#fig_5">6</ref> presents these modifications for BFS, where three unique API calls are used to annotate the DIG. registerNode()-register a node of the DIG. This call writes a node's information into the prefetcher memory; the arguments to this call are the base address of this data structure, total number of elements, size of data elements, and the node ID. registerTravEdge()-register an edge of the DIG. This call writes edge information into the prefetcher memory; the arguments to this call are the addresses of the source and destination nodes, and the type of indirection (i.e., w0/w1 as shown in Fig. <ref type="figure" target="#fig_4">5</ref>). registerTrigEdge()-register a trigger edge of the DIG. This call writes the base address of the trigger data structure into the prefetcher registers. The second argument (w2) holds information about the type of prefetch to be initiated (more details in Section IV-C).  2) Using Compiler Analysis: Identifying indirections in non-trivial programs (e.g., <ref type="bibr" target="#b15">[16]</ref>) can be complicated for the programmer, often requiring in-depth application knowledge. Our compiler alleviates this manual work by automatically identifying these indirections and transforms the program by annotating it with prefetcher API calls. Our compiler analyzes the application source code once for annotation with a negligible cost compared to the graph reordering approaches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b91">[92]</ref> that incur significant cost of profiling and re-organizing the input data set. Node and edge identification avoids complex interprocedural analysis by performing the resolution of their relationships during execution. Prefetching is only triggered for indirections whose edges consist of these resolved and registered nodes, as seen in Fig. <ref type="figure" target="#fig_7">8(d)</ref>. This section describes the operation of our LLVM-based compiler analyses and transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compiler analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code generation</head><formula xml:id="formula_0">define void @kernel(i64* %0, i64* %1, i64* %2) { %10 = call i32 @registerTrigEdge(i64* %0, i32 5) %11 = call i32 @registerTravEdge(i64* %0, i64* %1, i32 1) [...] ; loop %16 = getelementptr inbounds i64, i64* %0, i64 %.01 %17 = load i64, i64* %16, align 4 %19 = getelementptr inbounds i64, i64* %1, i64 %17 %20 = load i64, i64* %19, align 4 [...] } define void @main() { %3 = call i8* @malloc(i64 4000) %4 = call i32 @registerNode(i64* %3, i32 1000, i32 4, i32 0) %7 = call i8* @malloc(i64 4000) %8 = call i32 @registerNode(i64* %7, i32 1000, i32 4, i32<label>1</label></formula><formula xml:id="formula_1">define void @kernel(i64* %0, i64* %1, i64* %2) { %10 = call i32 @registerTrigEdge(i64* %0, i32 5) %11 = call i32 @registerTravEdge(i64* %0, i64* %1, i32 1) [...] ; loop %16 = getelementptr inbounds i64, i64* %0, i64 %.01 %17 = load i64, i64* %16, align 4 %19 = getelementptr inbounds i64, i64* %1, i64 %17 %20 = load i64, i64* %19, align 4 [...] } define void @main() { %3 = call i8* @malloc(i64 4000) %4 = call i32 @registerNode(i64* %3, i32 1000, i32 4, i32 0) %7 = call i8* @malloc(i64 4000) %8 = call i32 @registerNode(i64* %7, i32 1000, i32 4, i32<label>1</label></formula><p>First, our compiler analysis extracts information required for node registration from allocations. Apart from conventional defaults (i.e., malloc), the user can specify custom allocators. The pseudocode for this procedure is presented in Fig. <ref type="figure" target="#fig_7">8(a)</ref>. Fig. <ref type="figure" target="#fig_6">7(c</ref>) shows two node registrations, each using information from the immediately preceding malloc calls. Next, by tracking the use of these nodes, it extracts edge information and detects their associated indirection patterns. Fig. <ref type="figure" target="#fig_6">7(b</ref>) contains a single-valued indirection in the form of a load to b[a[i]] (line 4), which corresponds to the LLVM IR in lines 6-9 of Fig. <ref type="figure" target="#fig_6">7(c</ref>). As the base addresses of these two arrays form the edge between the nodes, our pass extracts them and uses them in the registerEdge() function along with the final argument that specifies the type of edge being registered-in this case, a single-valued indirection. Our code generation pass places the edge registration calls as soon as all the required arguments have been defined. In Fig. <ref type="figure" target="#fig_6">7</ref>, the pointers to the arrays are passed into the kernel as arguments, allowing edges to be registered at the start of the function (lines 2-3). Ranged  At the final stage, our analysis picks trigger edges using the set of traversal edges identified previously. If a node from that set does not have an incoming edge, then it has a trigger edge (i.e., a self-edge to the trigger node). For example, the address calculations in lines 6 and 8 in Fig. <ref type="figure" target="#fig_6">7</ref>(c) form a traversal edge. However, because the node with address generation in line 6 does not have any incoming edges, it is designated as a trigger edge, with its registration inserted in line 2.</p><p>The code generated by our compiler pass and the programmer annotations use the same API, presented in Fig. <ref type="figure" target="#fig_7">8(d)</ref>, and can complement each other, thus improving the overall accuracy of our compiler. For example, the programmer can choose to manually annotate the relevant nodes, and rely on the compiler to identify edges.</p><p>3) Application Hardware Interface: A small SRAM-based memory unit is used on the hardware prefetcher that is memory mapped to hold the DIG. Once software generates the DIG using API calls presented above, these calls are translated into a set of store operations by a run-time library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED HARDWARE DESIGN</head><p>A. Memory Requirements for a DIG Fig. <ref type="figure">9(a-c</ref>) show three prefetcher-local memory structures to store a DIG representation. As described in Section III, the node table and the edge table store properties of DIG nodes and edges, respectively. The base address, number of elements, and data size of each node specified by software are converted into base and bound addresses by the runtime library, and then stored into the node table. Because the DIG captures program semantics from the source code, these tables store virtual addresses. Additionally, we use an edge index table to find outgoing edges from a DIG node, which mimics the software offset list in hardware. To perform prefetching, Prodigy state machine uses these structures to extract program's data structures and traversal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Prefetch Status Handling Registers</head><p>A typical prefetch sequence for graph workloads can span four or more data structures. While the prefetcher is waiting to receive multiple outstanding data requests, it is important to track which responses belong to which issued requests. In addition, prefetch opportunities may be lost if the prefetcher is blocking, i.e., waiting for a whole prefetch sequence to complete before accepting a new one. To address these challenges, we introduce a hardware structure called PreFetch status Handling Register (PFHR) file for Prodigy, which addresses both of these issues at once. While PFHRs are analogous to the Miss Status Handling Registers (MSHRs) in non-blocking caches, PFHRs have a unique design because they also have to track the status of long prefetch sequences in addition to making their host hardware structure non-blocking. Fig. <ref type="figure">9</ref>(d) shows the hardware structure for PFHR file, where each row has the following entries. Free indicates if a PFHR is free or occupied. Node ID denotes the DIG node ID of an outstanding prefetch request. Prefetch trigger address stores the virtual address from which the prefetch sequence is initiated. This is used to drop the prefetch sequence if the demand sequence advances close to the prefetch sequence. Outstanding prefetch addresses stores the cache line-aligned physical addresses of outstanding prefetch requests. Upon a prefetch fill, Prodigy performs a CAM look-up in this column to find the PFHR that is keeping track of that request. Offset bitmap stores a bitmap of outstanding prefetch byte-addresses in a cache line whose address is indicated in the previous entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prefetching Algorithm</head><p>The prefetching algorithm has two phases: (a) prefetch sequence initialization and (b) prefetch sequence advance.</p><p>1) Prefetch Sequence Initialization Algorithm: This algorithm dictates actions to perform upon a prefetch trigger event. A prefetch trigger event occurs when Prodigy observes a demand load request to a data structure with a trigger edge. To dynamically adapt to changing machine states (e.g., cache contents), Prodigy initializes multiple prefetch sequences at once and selectively drops some prefetch sequences.</p><p>The role of a trigger edge is to indicate the parameters to initialize prefetch sequence(s), which include the prefetch bounds and prefetch direction as shown in Fig. <ref type="figure" target="#fig_8">10</ref>. The prefetch bounds represent a look-ahead distance for prefetching (i.e., j) and the number of prefetch sequences to initialize (i.e., k -j + 1). Additionally, the data structure traversal direction can also be defined, i.e., ascending or descending order of their memory addresses. Intuitively, when the prefetch depth, i.e., number of nodes on the DIG's critical path, is high, the time to traverse an entire path is long. Hence, a small lookahead distance is effective to balance data processing and data fetch times. Similarly, for a short critical path, a large lookahead distance is effective. This simple intuition is incorporated in a heuristic to determine the prefetch look-ahead distance, where the distance decreases with an increase in the prefetch depth of up to three. For algorithms traversing through four or more data structures, a look-ahead distance of one is used. In practice, we found there was little performance variation when the look-ahead distance is up to 4? smaller/greater than the ideal value.</p><p>Moreover, to adapt to dynamic data processing speed of the core, Prodigy uses a feedback from load requests to selectively drop prefetch sequences. As shown in Fig. <ref type="figure">9(d)</ref>, we store a trigger address in each PFHR entry to record the starting address of the prefetch sequence. When the core demands the trigger address of a live prefetch sequence, we drop the sequence because the prefetcher can only partially hide the memory latency. Instead, we choose to hide the full latency of future load operations by prefetching ahead. This way, dropping of prefetch sequence(s) helps Prodigy to always run ahead of the core, and multiple prefetch sequence initialization ensures the liveliness of some prefetch sequence(s) even if few others are terminated.</p><p>2) Prefetch Sequence Advance Algorithm: Upon servicing a prefetch, Prodigy reads its data to issue further prefetch requests using two types of indirection functions, i.e., singlevalued indirection and ranged indirection (see Section III-A).</p><p>Single-valued indirection is an indirection type that connects two arrays, where the source array stores indices/pointers to index into the destination array as shown in Fig. <ref type="figure" target="#fig_4">5(c</ref>). This traversal function is common in irregular algorithms (e.g., graph algorithms use vertex identifier to index into data storage (e.g., visited list for BFS and vertex scores for PageRank)). Notably, pointers are a special class of this indirection type, where the address of the destination can be found by using the pointer itself. With node information stored in the DIG, the prefetcher can interpret the address as an index (or a pointer) and indexes into the next array as done in software using the base address and data size of the next DIG node.</p><p>Ranged indirection is an indirection type in which an array stores pairs of base and bound indices (or pointers) pointing to a section of another array which is accessed together as shown in Fig. <ref type="figure" target="#fig_4">5(d)</ref>. Fundamentally, this access pattern summarizes a streaming access through a portion of memory specified by this pair. For example, in CSR/CSC representations, ranged indirection is used in graph algorithms to find neighbors of a vertex using offset list and edge list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardware Flow of Prodigy</head><p>Fig. <ref type="figure" target="#fig_9">11</ref> shows the operation of Prodigy and its interaction with the rest of the system. The figure shows that the graph data structures are populated in memory for the BFS algorithm on an example graph same as Fig. <ref type="figure" target="#fig_3">3</ref>. For simplicity, we assume that a cache line size is a single data block and caches are not yet populated. Once the prefetcher is programmed, it snoops on load requests from the core to the L1D and waits for a demand request within the address ranges of the data structure with the trigger edge. Similar to the prefetching algorithm, Prodigy state machine has two phases for issuing prefetches: prefetch sequence initialization and advance.  Upon observing a load request 1 that falls in the trigger data structure (i.e., workQueue), a prefetch sequence is initialized. Based on the prefetch-lookahead distance of (let us assume) 2 communicated via a trigger edge as described in Section IV-C, Prodigy computes memory address 0x108 (i.e., 0x100+2?4) to prefetch. Lastly, this address is translated to a physical address using the TLB and issued for prefetching 2 . A new PFHR is allocated for tracking this prefetch request. Fig. <ref type="figure" target="#fig_9">11</ref>(b) shows the second prefetching phase, where demand and prefetch requests are serviced with their data resident in the cache. Upon receiving the demand request, the core traverses through other data structures 3 ld 0x124 (0x11c+2?4; using index of 2 and data size of 4). Note that further load requests do not trigger prefetch sequences until another access to workQueue. Upon prefetch fills, Prodigy finds the PFHR entry keeping track of this request using a CAM look-up. Once identified, a source DIG node corresponding to this prefetch fill, its outgoing edges, and data indirection type are found by indexing into the edge and edge index tables. Using the single-valued indirection w0 and prefetched data, next prefetch address of 0x12C is computed. Lastly, a prefetch request is sent 4 by translating its address using the TLB and a new PFHR is allocated; this process repeats until a leaf DIG node is encountered. A new PFHR is only allocated for prefetch addresses belonging to non-leaf DIG nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Prodigy in a Parallel Execution Setting</head><p>In a multi-core execution, a private instance of Prodigy is present on each core. Prodigy snoops on the L1D cache to trigger prefetch sequences. Prodigy supports trigger data structures that are contiguously partitioned across multiple threads in the virtual address space. Thus, Prodigy supports both staticallyscheduled (OpenMP-static) and dynamically-scheduled or work stealing-based compilers (OpenMP-dynamic, CILK <ref type="bibr" target="#b35">[36]</ref>). With this contiguous partitioning, Prodigy mostly prefetches the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. OS Integration</head><p>Prodigy works best when the number of user threads does not exceed the core count. This allows the use of thread affinity to ensure only one user context is needed in the prefetcher. In the event that a thread which uses Prodigy is preempted by the kernel, the prefetching is paused upon thread descheduling. The data in Prodigy's prefetcher-local memory structures remains untouched. This cached data can be used to resume prefetching when the thread is rescheduled. In the rare event that another user thread is scheduled that requires the prefetcher, the context needs to be saved/restored from the prefetcher data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Prefetch Throttling Mechanism</head><p>While Prodigy focuses on designing a novel prefetching mechanism, we do not implement a prefetch throttling mechanism because it is out of the scope of this paper. We envision Prodigy to be used alongside a prefetch throttling mechanism similar to <ref type="bibr" target="#b87">[88]</ref> that can identify and prevent prefetch-induced cache pollution to further improve performance. We leave studying the best throttling techniques as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. METHODOLOGY</head><p>This section describes the simulation infrastructure, algorithms and data sets, and state-of-the-art prefetching systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Infrastructure</head><p>We use Sniper <ref type="bibr" target="#b19">[20]</ref>-a Pin <ref type="bibr" target="#b61">[62]</ref> based x86 multi-core simulator with an interval core simulation model. Sniper has been validated against several Intel micro-architectures <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. We use CACTI <ref type="bibr" target="#b69">[70]</ref> to obtain cache access times for different cache capacities. We use the McPAT <ref type="bibr" target="#b59">[60]</ref> model built into Sniper to model energy consumption. We implement our compiler analysis techniques using LLVM passes <ref type="bibr" target="#b56">[57]</ref>. We evaluate our approach by modeling a parallel shared memory system with 8 cores as described in Table <ref type="table" target="#tab_11">I</ref>. We run our workloads end-to-end and report the performance numbers by ignoring initialization cost, i.e., reading a graph from a file and populating data structures. We use the region-of-interest (ROI) utility from Sniper to only profile the core algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Irregular Workloads</head><p>We use unmodified versions of the following workloads and run through our compiler pass for analysis.</p><p>Algorithms. We use five graph algorithms from the GAP benchmark suite (GAPBS) <ref type="bibr" target="#b15">[16]</ref> for evaluation-Betweenness Centrality (bc), Breadth-First Search (bfs) <ref type="foot" target="#foot_2">3</ref> , Connected Components (cc), PageRank (pr), and Single-Source Shortest Path (sssp). We also use Sparse Matrix-Vector multiplication (spmv) and Symmetric Gauss-Seidel smoother (symgs) from the HPCG benchmark suite <ref type="bibr" target="#b28">[29]</ref> as representative sparse linear algebra applications. Additionally, we use Conjugate Gradient (cg) and Integer Sort (is) from the NAS parallel benchmark suite <ref type="bibr" target="#b11">[12]</ref> as representative computational fluid dynamics applications. We choose these algorithms as they exhibit single-valued and/or ranged indirections.</p><p>Data sets. As inputs to the graph algorithms, we use realworld graph data sets from SNAP <ref type="bibr" target="#b58">[59]</ref> and UF's sparse matrix collection <ref type="bibr" target="#b26">[27]</ref> as shown in Table <ref type="table" target="#tab_12">II</ref>. We selected these data sets as they represent real-world graph data and offer diversity in total size as well as number of vertices and edges. The primary reasons for avoiding the use of the graph generators kron and urand from GAPBS are (a) they are synthetic data sets, and (b) they are severely bound by synchronization overheads when evaluated on our simulation infrastructure. Unless shown individually, results for each graph algorithm is averaged over all data sets. For non-graph algorithms, we use input generators from benchmark suites; data set sizes for the linear algebra and fluid dynamics kernels are 2M?2M, and 33M (for is) and 75k (for cg), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Design Space Exploration</head><p>We perform design space exploration on Prodigy to understand the trade-off between performance and hardware complexity. Fig. <ref type="figure" target="#fig_1">12</ref> shows the effect of PFHR file size on the overall performance normalized to a baseline of 4 registers. The figure illustrates two key findings. First, there is up to 30% performance difference between the performance-optimal configuration and the baseline PFHR file size. difference is attributed to structural hazards in the PFHR filewhile issuing a prefetch, if the entire PFHR file is busy, the prefetch is dropped. We choose the size of PFHR file to be 16 for our design since it offers a reasonable trade-off between performance and storage area requirement. Second, increasing the number of PFHRs beyond 8 for cc hurts its performance since the benefits of timely prefetches are overshadowed by untimely prefetches that pollute the cache system. Dynamically adapting prefetch aggressiveness according to the usefulness of prefetched cache lines might help improve the performance of such workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prefetching Potential</head><p>To estimate the potential prefetch coverage of Prodigy, Fig. <ref type="figure" target="#fig_3">13</ref> evaluates the fraction of LLC misses, for a nonprefetching baseline, that Prodigy can prefetch. We evaluate this using DIG-annotated application binaries, disabling the prefetcher, and classifying LLC miss addresses based on whether they are within or outside the data structure address bounds annotated by the DIG. The figure shows that, on average, 96.4% of LLC misses can be prefetched. In other words, ideal prefetching and caching resources would convert an average of 96.4% of DRAM accesses into cache hits, which sets the upper bound for our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect on Performance</head><p>Prodigy vs. no-prefetching: Fig. <ref type="figure" target="#fig_0">14</ref> shows the CPI stacks and speedups of Prodigy across all the workloads normalized to a non-prefetching baseline. For each workload, the first and second bars correspond to the CPIs of baseline and Prodigy, respectively. The figure shows the breakdown of execution time in terms of no-stalls and stalls because of DRAM and cache accesses, branch mispredictions, dependent instructions, and others. Prodigy achieves a significant average speedup of 2.6? compared to a non-prefetching baseline.</p><p>We see that Prodigy gains most of its performance by decreasing the DRAM stalls by an average of 80.3%. Notably, the DRAM stall portion of the baseline non-graph workloads is 88.4% of the overall CPI, leading to substantial savings and speedups. Assuming that software communicates the correct workload semantics to the prefetcher, it mostly fetches useful data. The primary inefficiency stems from issuing untimely prefetches. We address this challenge by prefetching for the next few work queue items and dropping prefetch sequences after detecting that the core has caught up. This heuristic allows us to avoid cache pollution by modulating the number of requested cache blocks while also freeing PFHRs for more useful work if their prefetch sequences would only partially hide the memory latency. Note that the pr implementation uses both CSC and CSR graph data structures that achieves a similar speedup as other algorithms that only use CSR format. Furthermore, as a result of reduction in DRAM stalls, Prodigy slightly increases the cache stall portion of the CPI stack. This is due to converting DRAM accesses into cache hits that increases the aggregate time spent on cache accesses. Additionally, mostly for graph workloads, Prodigy reduces the branch segment of the CPI stack by 65.3% on average as a side effect of reducing DRAM stalls. This is especially evident in bfs, pr, and sssp due to the prevalence of load data dependent branches. For example, in bfs, a vertex is only added onto the work queue after loading its visited list entry and verifying that it has not been traversed yet. This finding is consistent with prior work <ref type="bibr" target="#b88">[89]</ref>.</p><p>Prefetch Usefulness: Fig. <ref type="figure" target="#fig_4">15</ref> classifies the usefulness of prefetched data into four categories-demanded and resident in the L1/L2/L3 cache and evicted from the cache hierarchy without being demanded. The figure shows that data brought in by 32.9-85.8% of prefetch requests is demanded before it is evicted, which shows the accuracy of our prefetcher. On average, our prefetcher achieves an accuracy of 62.7%. Furthermore, most of these cache hits are found in the L1D cache, which incurs the lowest latency of the load operations. Note that since Prodigy benefits from static analysis information provided by software, the fraction of evicted data can further be reduced by using an intelligent caching policy (e.g., stream buffers or scratchpads <ref type="bibr" target="#b0">[1]</ref>) since eviction is a consequence of imperfect timeliness. Fig. <ref type="figure" target="#fig_5">16</ref> shows the percentage of prefetchable LLC misses (blue portion of the bar in Fig. <ref type="figure" target="#fig_3">13</ref>) that Prodigy converts into cache hits. On average, Prodigy converts 85.1% of prefetchable LLC misses to cache hits. Significance of ranged indirection: For graph algorithms, ranged indirection is responsible for prefetching 35.4-75.9% (55.3% on average) of all data (not shown because of space limitation). The fraction of data prefetched using ranged indirection depends both on the position of indirection types in a prefetch sequence and the amount of data available to prefetch. For example, a major source of single-valued indirection in bfs is at a prefetch depth of four. At this depth, secondary effects, like squashing of prefetch sequences and PFHR unavailability, limit prefetching opportunities. Prior work <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b98">[99]</ref> only prefetch single-valued indirection and fail to capture a significant prefetching opportunity.</p><p>Prodigy vs. hardware prefetchers: Next we compare the performance of Prodigy with the state-of-the-art hardware prefetchers including GHB-based G/DC data prefetcher <ref type="bibr" target="#b71">[72]</ref>, Ainsworth and Jones' prefetcher <ref type="bibr" target="#b5">[6]</ref>, DROPLET <ref type="bibr" target="#b14">[15]</ref>, and IMP <ref type="bibr" target="#b98">[99]</ref>. Notably, the benefits of different prefetching solutions are highly sensitive to architectural parameters, graph traversal algorithm and design of their data structures, and input data sets. Hence, we present a comparison using the parameters from our simulation framework as well as a comparison with the best reported results on commonly evaluated algorithms from each prior work.</p><p>Prodigy outperforms the baseline and a GHB-based G/DC data prefetcher <ref type="bibr" target="#b71">[72]</ref> (not shown because of space limitations) by 2.6? on average. GHB-based G/DC is known to predict inaccurate prefetch addresses for irregular memory accesses due to the lack of spatial locality, polluting the cache. Therefore, when Prodigy is enabled by software, other traditional prefetchers (e.g., GHB, stride, stream) are disabled.</p><p>Fig. <ref type="figure" target="#fig_6">17</ref> shows the performance comparison of various prefetchers using our simulation framework. Prodigy outperforms Ainsworth and Jones' prefetcher<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> by 1.5?. We have verified with the authors <ref type="bibr" target="#b3">[4]</ref> that our implementation and results are correct. The difference compared to <ref type="bibr" target="#b5">[6]</ref> can be attributed to inaccurate prefetch timeliness. On average, 62.7% of Prodigy's prefetches are demanded by the core versus only 44.6% for <ref type="bibr" target="#b5">[6]</ref>. Also, unlike Prodigy, initiating one prefetch sequence in <ref type="bibr" target="#b5">[6]</ref> sometimes only partially hides the memory latency if the core catches up with the prefetcher. Furthermore, Prodigy is more flexible in that it can adapt with different combinations of data structures and indirection patterns, whereas Ainsworth and Jones' graph prefetcher aims to prefetch for BFS-like access patterns. While an extension of <ref type="bibr" target="#b5">[6]</ref> is presented in <ref type="bibr" target="#b4">[5]</ref>, it incurs significant area overhead of 32KB of storage vs. 0.8KB for Prodigy.</p><p>Compared to DROPLET <ref type="bibr" target="#b14">[15]</ref>, Prodigy achieves a 1.6? speedup on average for two reasons. First, DROPLET only prefetches a subset of data structures, i.e., edge list and visited list-like arrays exhibiting single-valued indirection, compared to Prodigy, which prefetches other graph data structures as well. Second, we notice that DROPLET MPP misses several prefetching opportunities because it can only trigger further prefetches from prefetch requests serviced from DRAM, while much of the prefetched data are present in the cache hierarchy.</p><p>Prodigy achieves an average speedup of 2.3? compared to IMP<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b98">[99]</ref>, because IMP can only detect streaming accesses to data structures that perform A[B <ref type="bibr">[i]</ref>] type prefetching and it only supports up to two levels of indirection. Extending both DROPLET and IMP to prefetch additional data structures would require significant effort because they do not support ranged indirection and DROPLET design is specific to a subset of graph data structures.</p><p>While Prodigy shows a significant speedup over prior work on our simulation environment, we could not reproduce similar results reported in the prior publications despite obtaining evaluation artifacts from the authors. We believe that this discrepancy is attributed to the difference in simulation environment, architecture parameters, and benchmark implementations. To offer better justice to prior work, we also compare Prodigy with the best reported speedups of hardware prefetchers from their original publications. Table <ref type="table" target="#tab_16">III</ref> shows a comparison of best reported speedups over a non-prefetching baseline for optimal algorithm-data set combination for both Prodigy and prior work. The comparison shows that even compared to the best- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speedup (x)</head><p>Figure <ref type="figure" target="#fig_7">18</ref>. Speedup of Prodigy compared to a non-prefetching baseline on reordered graph data sets using HubSort <ref type="bibr" target="#b13">[14]</ref>.</p><p>reported speedups, Prodigy still outperforms the state-of-the-art hardware prefetchers. Prodigy vs. software prefetching: We compare the performance of Prodigy with a software prefetching technique <ref type="bibr" target="#b7">[8]</ref> for indirect memory accesses. To make our evaluation consistent with <ref type="bibr" target="#b7">[8]</ref>, we evaluated the performance of software prefetching on an Intel Broadwell microarchitecture and validated our results with authors of <ref type="bibr" target="#b2">[3]</ref>. Our findings show that for pr, performing a pure software-based prefetching <ref type="bibr" target="#b7">[8]</ref> achieves an average speedup of 7.6% compared to an average speedup of 2? for our approach (not shown due to space limitation). This is because Prodigy benefits from both static analysis information from software and dynamic run-time information from hardware to perform efficient prefetching. We do not report the results on other graph algorithms since we noticed that the compiler pass of <ref type="bibr" target="#b7">[8]</ref> is not able to detect dynamically allocated array sizes, and conservatively avoids placing prefetch instructions to prevent faults <ref type="bibr" target="#b2">[3]</ref>.</p><p>Graph reordering: We also evaluate the performance benefits of Prodigy on reordered graphs using HubSort <ref type="bibr" target="#b13">[14]</ref>. Fig. <ref type="figure" target="#fig_7">18</ref> presents the speedup of Prodigy compared to a nonprefetching baseline (both using graph reordering) for graph algorithms. The figure shows even after benefiting from added locality because of graph reordering, irregular memory accesses can still limit the performance, and Prodigy can further improve this performance by 2.3? on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effect on Energy</head><p>Fig. <ref type="figure" target="#fig_0">19</ref> shows the breakdown of energy consumption for Prodigy normalized to the baseline. Prodigy reduces energy consumption across all categories with an average reduction of 1.6?. We primarily attribute the energy reduction to the static energy savings of the core, cache, and DRAM due to the reduced workload execution time. Accelerating long-latency memory operations also saves energy by reducing the number of instructions executed and memory accesses performed before recovering from mispredicted branches <ref type="bibr" target="#b88">[89]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overhead Analysis</head><p>Prodigy's hardware consists of a finite-state machine, whose area is dominated by the storage structures discussed in We estimate the area overhead in terms of storage area requirements assuming 48-bit physical and 64-bit virtual address spaces. We calculate that the largest DIG used by our workloads has 11 nodes and 11 edges for bc. For a plausible extension to store larger DIGs, we conservatively assume 16entry DIG tables. Moreover, based on Fig. <ref type="figure" target="#fig_1">12</ref>, we use 16 PFHRs for our design. Using these parameters, we estimate the storage requirements of DIG tables and PFHRs to be 0.53KB and 0.26KB, respectively, totaling to just 0.8KB. Assuming this storage area to be dominant, we project our prefetcher to have a negligible area overhead of 0.004% compared to an entire CPU chip. Compared to Prodigy, other work has area overheads of 1.4? <ref type="bibr" target="#b98">[99]</ref>, 2? <ref type="bibr" target="#b5">[6]</ref>, 9.7? <ref type="bibr" target="#b14">[15]</ref>, and 40? <ref type="bibr" target="#b4">[5]</ref>.</p><p>In terms of the software overhead, adding one-time prefetch API calls slightly increases the size of program binaries. Because these calls are executed only once, they translate into a negligible dynamic instruction count increase. To add these API calls, our compiler analysis performs a linear scan of a program's static instructions. The average compilation time added to our benchmarks is less than one second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion on Scalability</head><p>Because of the irregular memory access patterns of evaluated workloads, cores are mostly stalled to receive responses from the memory system. Based on the baseline memory bandwidth utilization results and a bandwidth limit of 100GB/s, increasing the number of cores to around 40 will fully saturate the memory bandwidth, at which point, the benefits from prefetching will be limited. Our evaluation shows a more cost-effective design point where an 8-core system used with Prodigy can saturate the memory bandwidth while consuming 5? less transistor area and less static energy compared to a 40-core system without prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitations of Prodigy</head><p>A subset of irregular algorithms exhibiting singlevalued/ranged indirection patterns also incorporate additional run-time information to issue load operations. For example, triangle counting algorithm in GAPBS <ref type="bibr" target="#b15">[16]</ref> intelligently avoids redundant computation by examining only neighbors with higher vertex IDs than the source vertex (i.e., branch-dependent loads). While Prodigy supports prefetching for indirect memory accesses, it does not account for this additional control-flow information for prefetching. Similar trends might be observed for ordered graph algorithms <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b102">[103]</ref> because node priority is not accounted for prefetching. In such cases, Prodigy might prefetch inaccurate vertices, and we envision using a mechanism that disables the prefetcher when it detects cache thrashing <ref type="bibr" target="#b87">[88]</ref>. Additionally, the storage cost of hardware structures (i.e., DIG tables and PFHR file) was chosen to fit the needs of the workloads evaluated in this paper. It is possible that other workloads with more DIG nodes/edges would require greater storage and PFHR resources. We leave the study of incorporating additional prefetching information and larger workload analysis for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>There is a rich body of work alleviating the memory access bottleneck for various workloads, especially through prefetching. This work employs a unique synergy of both hardware and software optimizations through the novel DIG representation. We divide the related work in different categories and discuss how our work is different.</p><p>Decouple access execute (DAE) architectures <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b89">[90]</ref> use decoupled memory access and execute streams to reduce memory latency and communicate between them using architectural queues. While we use a separate prefetching unit for accelerating memory accesses, we still use a single thread with coupled access and execute streams with no additional requirement of queues for communication.</p><p>Helper threads <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b101">[102]</ref> propose using a separate thread to speculatively prefetch data to reduce memory latency of the main thread. Run-ahead execution <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b70">[71]</ref> and some other architectures <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b103">[104]</ref> utilize additional or unused hardware resources to prefetch useful data for the main thread. Helper threads dedicate extra physical cores to perform prefetching that reduces compute throughput. Unlike Prodigy, runahead execution has to re-execute instructions after longlatency load-instructions.</p><p>More recently, several graph algorithm-based hardware prefetchers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref> have been proposed that assume graph data structure knowledge at hardware and prefetch for accesses falling in these data structures. Accelerating irregular workloads using hardware prefetchers <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b98">[99]</ref> has been long studied that cover other types of data structures and memory access patterns containing linked lists, binary trees, hash joins in application domains such as geometric and scientific computations, high-performance computing, and databases. Furthermore, several temporal prefetchers <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref> and non-temporal prefetchers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b85">[86]</ref> are also investigated for these workloads. These approaches however, when applied in the graph processing context, can either prefetch for a subset of data structures or incur high complexity and cost for generality. Given our compact DIG representation, our approach benefits covering all the data structures having data-dependent indirect accesses at a negligible hardware cost.</p><p>A class of prefetchers <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b97">[98]</ref> focuses on linked data structure traversals using pointers. They have limited applicability for graph algorithms, mainly because of the prevalence of ranged indirection as shown in the Section VI-C. Prodigy on the other hand, can cover all types of indirection present in graph algorithms.</p><p>Software prefetching <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b90">[91]</ref> is another technique to reduce the memory latency of both regular and irregular workloads where data structures are known at compile-time. However, software prefetching could significantly increase the size of the application binary and workloads with dynamically initialized and sized data structures are difficult to prefetch purely in software. Additionally, direct memory access (DMA) engines are used to move data around without explicit CPU instructions. Prodigy that reacts to hardware events is orthogonal to a DMA engine, which is primarily software controlled and used for peripheral devices.</p><p>Several domain-specific architectures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b66">[67]</ref>- <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref> have been proposed for accelerating graph processing applications. These architectures are orthogonal to our software-aided hardware prefetching work for CPUs; they either work as stand-alone accelerators, as near/in-memory processing engines, or as scheduling/intelligent caching aid to the processor core. Many of these architectures use some form of hardware prefetching support, and our lowcost prefetcher can be integrated within these architectures to further enhance their performance.</p><p>Prefetch throttling mechanisms <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b93">[94]</ref> use dynamic information such as prefetch coverage/accuracy, cache pollution, and/or bandwidth utilization to monitor the aggressiveness of prefetches. These mechanisms can be applied to our approach to reduce prefetch-induced cache pollution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>This paper presented Prodigy, a hardware-software co-design approach to improve the memory latency of data-indirect irregular workloads. We proposed a compact representation, called the Data Indirection Graph (DIG), that efficiently abstracts an irregular algorithm's data structure layout and traversal patterns. This representation is constructed using static compiler analysis and code generation techniques, and communicated to the hardware. A programmable hardware prefetcher uses this information to cater its prefetches to irregular algorithms' memory access patterns. This approach benefits from (a) static program analysis from software to capture the irregular nature of memory accesses, and (b) dynamic run-time information from hardware to make adaptive prefetching decisions. We showed that our system is versatile and works for different sparse data representations. We evaluated the benefits of our system using a variety of irregular algorithms on real-world large-scale data sets and showed a 2.6? average performance improvement, 1.6? energy savings, and a negligible storage cost of 0.8KB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of our design and contributions. Prodigy software efficiently communicates key data structures and algorithmic traversal patterns, encoded in the proposed compact representation called the Data Indirection Graph (DIG), to the hardware for informed prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Reduction in ((a)) memory stalls and ((b)) speedup of different approaches normalized to a non-prefetching baseline for the PageRank algorithm on the livejournal data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. BFS algorithm: (a) pseudo-code for a parallel implementation of BFS, and (b) a toy example of BFS traversal on a graph stored in a compressed sparse row (CSR) format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Proposed Data Indirection Graph (DIG) representation-(a) example representation for BFS, (b) data structure memory layout and algorithmic traversal information captured by a DIG node and a weighted DIG edge respectively; two unique data-dependent indirection patterns supported by Prodigy-(c) single-valued indirection, and (d) ranged indirection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Annotated BFS source code to construct the DIG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. An example C program (a) and (b), translated into LLVM IR (c) and instrumented with our API calls to register DIG nodes and edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Pseudocode of Prodigy's compiler analyses for (a) node identification, (b) single-valued indirection, (c) ranged indirection, and (d) runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Prefetching algorithm initiates prefetch sequences between prefetch bounds j and k and advances a prefetch sequence using software-defined indirection types. The superscripts denote a demand (D) or a prefetch (P) access.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Prodigy operation: (a) prefetch sequence initialization, and (b) prefetch sequence advance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 (</head><label>11</label><figDesc>Fig.11(a) shows Prodigy's operation in the first phase. Upon observing a load request 1 that falls in the trigger data structure (i.e., workQueue), a prefetch sequence is initialized. Based on the prefetch-lookahead distance of (let us assume) 2 communicated via a trigger edge as described in Section IV-C, Prodigy computes memory address 0x108 (i.e., 0x100+2?4) to prefetch. Lastly, this address is translated to a physical address using the TLB and issued for prefetching 2 . A new PFHR is allocated for tracking this prefetch request. Fig.11(b) shows the second prefetching phase, where demand and prefetch requests are serviced with their data resident in the cache. Upon receiving the demand request, the core traverses through other data structures 3 ld 0x124 (0x11c+2?4; using index of 2 and data size of 4). Note that further load requests do not trigger prefetch sequences until another access to workQueue. Upon prefetch fills, Prodigy finds the PFHR entry keeping track of this request using a CAM look-up. Once identified, a source DIG node corresponding to this prefetch fill, its outgoing edges, and data indirection type are found by indexing into the edge and edge index tables. Using the single-valued indirection w0 and prefetched data, next prefetch address of 0x12C is computed. Lastly, a prefetch request is sent 4 by translating its address using the TLB and a new PFHR is allocated; this process repeats until a leaf DIG node is encountered. A new PFHR is only allocated for prefetch addresses belonging to non-leaf DIG nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Compiler analysis Run application Software Hardware Instrumented application binary Application source code Add DIG representation</head><label></label><figDesc></figDesc><table><row><cell>System overview</cell><cell cols="2">Contributions</cell></row><row><cell></cell><cell>Data Indirection</cell><cell>Instrumented binary</cell></row><row><cell></cell><cell>Graph (DIG)</cell><cell>int bfs() { [?]</cell></row><row><cell></cell><cell></cell><cell>populateDS(?) regNode(?) regTrigEdge(?) regTravEdge(?) // perform trav</cell><cell>bfs.cc</cell></row><row><cell></cell><cell></cell><cell>[?]</cell></row><row><cell></cell><cell></cell><cell>}</cell></row><row><cell></cell><cell>Programmable</cell></row><row><cell></cell><cell>Prefetcher</cell><cell>Adjustable prefetch</cell></row><row><cell></cell><cell></cell><cell>distance</cell></row><row><cell></cell><cell></cell><cell>Adapts to core's</cell></row><row><cell></cell><cell></cell><cell>execution pace</cell></row><row><cell></cell><cell></cell><cell>Low-cost</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>workQ offsetList edgeList visited w0 w1 w0 node_id base_addr capacity data_size workQ offsetList src_base_addr dest_base_addr edge_type w0 for(i=0; i&lt;a_size</head><label></label><figDesc></figDesc><table><row><cell>workQ</cell><cell>: 0 : 0x10 : 100 : 4</cell><cell cols="3">; ++i) tmp += b[a[i]]; 5 30</cell><cell cols="4">for(i=0; i&lt;a_size; ++i) for(j=a[i]; j&lt;a[i+1]; ++j) tmp += b[j]; 0 30</cell></row><row><cell></cell><cell>: 0x10</cell><cell>i</cell><cell>3 0 1</cell><cell>478 32 56</cell><cell>i</cell><cell>1 4 4</cell><cell>j</cell><cell>478 32 56</cell></row><row><cell></cell><cell>: 0x1A0 : w0</cell><cell></cell><cell>2</cell><cell>367 4</cell><cell></cell><cell>5</cell><cell></cell><cell>367 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Information captures in a node and an edge of DIG Single-valued indirection type represented as w0 (c) Single-valued indirection type represented as w0 (c) Ranged indirection type represented as w1 (d) Ranged indirection type represented as w1 (d) Proposed DIG representation for BFS (a) Proposed DIG representation for BFS (a) (b) w2</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Figure 9. Memory structures used in Prodigy-(a) nodetable, (b) edge index table, and (c) edge table for storing the DIG representation, (d) prefetch status handling register (PFHR) file tracking progress for live prefetch sequences and issuing non-blocking prefetches.</figDesc><table><row><cell>(a)</cell><cell cols="2">Node ID 0</cell><cell cols="3">Base Address 0x00010</cell><cell>Bound Address 0x0019C</cell><cell>Data Size Trigger 4 true</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell cols="2">0x001A0</cell><cell>0x00330</cell><cell>4</cell><cell>false</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell></cell><cell cols="2">0x00334</cell><cell>0x00B00</cell><cell>4</cell><cell>false</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell></cell><cell cols="2">0x00B04</cell><cell>0x00C90</cell><cell>4</cell><cell>false</cell></row><row><cell>(b)</cell><cell cols="3">0 Edge Index</cell><cell>(c)</cell><cell cols="2">0x00010 Src Node Addr</cell><cell>0x001A0 Dest Node Addr</cell><cell>0 Edge Type</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="2">0x001A0</cell><cell>0x00334</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">0x00334</cell><cell>0x00B04</cell><cell>0</cell></row><row><cell cols="2">(d)</cell><cell>Free</cell><cell cols="2">Node ID</cell><cell cols="2">Prefetch Trigger Addr</cell><cell>Outstanding Prefetch Addr</cell><cell>Offset Bitmap</cell></row><row><cell></cell><cell></cell><cell>false</cell><cell></cell><cell>2</cell><cell cols="2">0x00020</cell><cell>0x00468</cell><cell>01010000</cell></row><row><cell></cell><cell></cell><cell>true</cell><cell></cell><cell>0</cell><cell cols="2">0x00108</cell><cell>0x00108</cell><cell>01000000</cell></row><row><cell></cell><cell></cell><cell>false</cell><cell></cell><cell>1</cell><cell cols="2">0x00080</cell><cell>0x00200</cell><cell>00001000</cell></row><row><cell></cell><cell></cell><cell>false</cell><cell></cell><cell>2</cell><cell cols="2">0x00188</cell><cell>0x00A00</cell><cell>01111100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE I BASELINE</head><label>I</label><figDesc>SYSTEM CONFIGURATION.</figDesc><table><row><cell>Component</cell><cell>Modeled Parameters</cell></row><row><cell>Core</cell><cell>8-OoO cores, 4-wide issue, 128-entry ROB, load/store queue</cell></row><row><cell></cell><cell>size = 48/32 entries, 2.66GHz frequency</cell></row><row><cell>Cache Hierarchy</cell><cell>Three-level inclusive hierarchy, write-back caches, MESI</cell></row><row><cell></cell><cell>coherence protocol, 64B cache line, LRU replacement</cell></row><row><cell>L1 I/D Cache</cell><cell>32KB/core private, 4-way set-associative, data/tag access</cell></row><row><cell></cell><cell>latency = 2/1 cycles</cell></row><row><cell>L2 Cache</cell><cell>256KB/core private, 8-way set-associative, data/tag access</cell></row><row><cell></cell><cell>latency = 4/1 cycles</cell></row><row><cell>L3 Cache</cell><cell>2MB/core slice shared, 16-way set-associative, data/tag access</cell></row><row><cell></cell><cell>latency = 27/8 cycles</cell></row><row><cell>Main Memory</cell><cell>DDR3 DRAM, access latency = 120 cycles, memory</cell></row><row><cell></cell><cell>controller queuing latency modeled</cell></row><row><cell cols="2">correct data for each core; this prevents any significant increase</cell></row><row><cell cols="2">in NoC/coherence traffic. The only exception is present at the</cell></row><row><cell cols="2">data structure boundaries, which are rarely accessed. Timeliness</cell></row><row><cell cols="2">in presence of synchronization is maintained by selectively</cell></row><row><cell cols="2">dropping prefetch sequences based on each core's execution</cell></row><row><cell>pace.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE II REAL</head><label>II</label><figDesc>-WORLD GRAPH DATA SETS USED FOR EVALUATION.</figDesc><table><row><cell>Graph</cell><cell>Number of</cell><cell>Number of</cell><cell>Size</cell><cell>? LLC</cell></row><row><cell></cell><cell>vertices</cell><cell>edges</cell><cell>(in MB)</cell><cell>capacity</cell></row><row><cell>pokec (po)</cell><cell>1.6M</cell><cell>30.6M</cell><cell>132.0</cell><cell>16.5</cell></row><row><cell>livejournal (lj)</cell><cell>4.8M</cell><cell>69.0M</cell><cell>300.0</cell><cell>37.5</cell></row><row><cell>orkut (or)</cell><cell>3.1M</cell><cell>117.2M</cell><cell>485.2</cell><cell>60.6</cell></row><row><cell>sk-2005 (sk)</cell><cell>50.6M</cell><cell>1930.3M</cell><cell>7749.6</cell><cell>968.7</cell></row><row><cell>webbase-2001 (wb)</cell><cell>118.1M</cell><cell>1019.9M</cell><cell>4791.6</cell><cell>598.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Figure 12. Design space exploration on the PFHR file size. Performance of each configuration is normalized to 4 entries.</figDesc><table><row><cell>0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Speedup (x)</cell><cell>bc</cell><cell>bfs</cell><cell>cc</cell><cell>4</cell><cell>pr</cell><cell>sssp spmv symgs cg Workload 8 16 32</cell><cell>is</cell></row><row><cell>0 20 40 60 80 100 LLC Misses (%)</cell><cell></cell><cell cols="5">Workload bc bfs cc pr sssp spmv symgs cg is avg Prefetchable Non-prefetchable</cell><cell></cell></row><row><cell cols="8">Figure 13. Classification of LLC miss addresses into potentially prefetchable</cell></row><row><cell cols="6">and non-prefetchable addresses.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The performance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Figure 15. Location of prefetched data in the cache hierarchy when it is demanded. Blue is better.</figDesc><table><row><cell>0.0 0.2 0.4 0.6 0.8 1.0 1.2 Normalized CPI</cell><cell cols="3">bc-lj bc-or bc-po bc-sk bc-wb bfs-lj bfs-or bfs-po bfs-sk bfs-wb cc-lj cc-or cc-po cc-sk cc-wb pr-lj pr-or pr-po pr-sk pr-wb sssp-lj sssp-or sssp-po sssp-sk Workload sssp-wb spmv symgs cg is Other-stall Dependency-stall Branch-stall Cache-stall DRAM-stall No-stall Speedup</cell><cell>0 1 2 3 4 5 6 Speedup (x) 7 8</cell></row><row><cell cols="4">Figure 14. CPI stack comparison and speedup achieved by Prodigy against</cell></row><row><cell cols="4">a non-prefetching baseline. Left bar: CPI stack of baseline; right bar: CPI</cell></row><row><cell cols="4">stack of Prodigy normalized to baseline. Lower is better for CPI, higher for</cell></row><row><cell cols="2">speedup.</cell><cell></cell></row><row><cell cols="2">0 20 40 60 80 100 Usefulness (%) Prefetch</cell><cell>bc bfs cc L1 prefetch hit L2 prefetch hit</cell><cell>pr sssp spmv symgs cg Workload L3 prefetch hit Prefetch eviction before demanded is avg</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>bc bfs cc pr sssp spmv symgs cg is avgFigure 16. Percentage of prefetchable main memory accesses (as shown in Fig.13) converted to cache hits. Blue is better.</figDesc><table><row><cell cols="2">0 20 40 60 80 100 LLC Misses (%) Prefetchable</cell><cell cols="2">Saved by prefetching</cell><cell>Not saved by prefetching</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Workload</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline Ainsworth &amp; Jones</cell><cell>DROPLET IMP</cell><cell>This work</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>7.3x 5.8x 8.4x 5.0x</cell></row><row><cell>0 1 2 3 Speedup (x)</cell><cell cols="2">bc bfs cc</cell><cell cols="2">pr sssp spmv symgs cg Workload</cell><cell>is gm</cell></row><row><cell cols="5">Figure 17. Performance comparison of a non-prefetching baseline, Ainsworth</cell></row><row><cell cols="5">and Jones' prefetcher [6], DROPLET [15], IMP [99], and Prodigy (this</cell></row><row><cell cols="5">work). Higher is better. Ainsworth &amp; Jones and DROPLET are graph-specific</cell></row><row><cell cols="5">approaches, and hence are omitted from non-graph workloads.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE III AVERAGE</head><label>III</label><figDesc>SPEEDUP COMPARISON OVER NO PREFETCHING. Best-performing input data sets used as reported in prior work.</figDesc><table><row><cell></cell><cell cols="2">Common algorithms</cell><cell>Prior work</cell><cell></cell><cell cols="2">Prodigy</cell></row><row><cell></cell><cell cols="2">bc,bfs,bc,pr</cell><cell cols="2">Ainsworth &amp; Jones [6]</cell><cell>2.4?</cell><cell>2.8?</cell></row><row><cell cols="3">bc,bfs,bc,pr,sssp</cell><cell cols="2">DROPLET [15]</cell><cell>1.9?</cell><cell>2.9?</cell></row><row><cell cols="3">bfs,pr,spmv,symgs</cell><cell cols="2">IMP [99]</cell><cell>1.8?</cell><cell>4.6?</cell></row><row><cell>0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5</cell><cell>bc</cell><cell>bfs</cell><cell>cc Workload pr</cell><cell>sssp</cell><cell>gm</cell></row></table><note><p>* *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>These structures include DIG tables (i.e., node table, edge table, and edge index table) and PFHRs. AlthoughProdigy reads data values for prefetching, this is done by snooping on the data response buses, rather than adding or sharing ports on the cache. This limits the performance impact and area overhead. Prodigy might increase the D-TLB contention, however, this is a known issue for prefetchers operating in the virtual address space.</figDesc><table><row><cell>0.0 0.2 0.4 0.6 0.8 1.0 Normalized Energy</cell><cell>bc-lj bc-or bc-po bc-sk bc-wb bfs-lj bfs-or bfs-po bfs-sk bfs-wb cc-lj cc-or cc-po cc-sk cc-wb pr-lj pr-or pr-po pr-sk pr-wb sssp-lj sssp-or sssp-po sssp-sk Workload sssp-wb spmv symgs cg is Core Cache DRAM Others</cell></row><row><cell cols="2">Figure 19. Normalized energy comparison of a non-prefetching baseline (first</cell></row><row><cell cols="2">bar) and Prodigy (second bar). Lower is better.</cell></row><row><cell cols="2">Section IV-A.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We define a prefetch fill as the cache line brought into the cache as a response to a prefetch request.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>An alternate implementation of work queue uses dual buffering with two frontier data structures (current and next); this paper focuses on a sliding queue based work queue structure that is conceptually same as frontiers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For a fair comparison with prior work, we only use a top-down implementation of the bfs algorithm.Prodigy can also adapt to direction-optimizing BFS by re-configuring the DIG during run-time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We used open-sourced artifacts of for the evaluation of<ref type="bibr" target="#b5">[6]</ref>, and verified the presented results with the authors<ref type="bibr" target="#b3">[4]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We used the artifacts provided by the authors for evaluating IMP.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank the anonymous reviewers for their insightful feedback. The material is based on research sponsored by <rs type="funder">Air Force Research Laboratory</rs> (AFRL) and <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> under agreement number <rs type="grantNumber">FA8650-18-2-7864</rs>. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of <rs type="affiliation">Air Force Research Laboratory (AFRL</rs>) and <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> or the <rs type="institution">U.S. Government</rs>. This material is also based upon work supported by the <rs type="funder">National Science Foundation (NSF)</rs> under Grant No. <rs type="grantNumber">NSF-XPS-1628991</rs> and under <rs type="grantName">Graduate Research Fellowship Grant</rs> No. <rs type="grantNumber">NSF-DGE-1256260</rs>. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the NSF.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xWzMcfh">
					<idno type="grant-number">FA8650-18-2-7864</idno>
				</org>
				<org type="funding" xml:id="_RH92Scj">
					<idno type="grant-number">NSF-XPS-1628991</idno>
					<orgName type="grant-name">Graduate Research Fellowship Grant</orgName>
				</org>
				<org type="funding" xml:id="_4EYpuRZ">
					<idno type="grant-number">NSF-DGE-1256260</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Heterogeneous memory subsystem for natural graph analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Addisie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IISWC</title>
		<imprint>
			<biblScope unit="page" from="134" to="145" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scalable processing-in-memory accelerator for parallel graph processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Private communication to verify Ainsworth and Jones</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017 results, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<title level="m">Private communication to verify Ainsworth and Jones</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICS 2016/ASPLOS 2018 results</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An event-triggered programmable prefetcher for irregular workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph prefetching using data structure knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Software prefetching for indirect memory accesses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
		<imprint>
			<date type="published" when="2017-02">Feb 2017</date>
			<biblScope unit="page" from="305" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Software prefetching for indirect memory accesses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="305" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">S2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">x86 computer architecture simulators: A comparative study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sawalha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<biblScope unit="page" from="638" to="645" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data prefetching by dependence graph precomputation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The nas parallel benchmarks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="63" to="73" />
			<date type="published" when="1991-09">Sep. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bingo spatial data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">When is graph reordering an optimization? studying the effect of lightweight graph reordering across applications and input graphs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IISWC</title>
		<imprint>
			<biblScope unit="page" from="203" to="214" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis and optimization of the memory hierarchy for graph processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
			<biblScope unit="page" from="373" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The GAP Benchmark Suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03619[cs.DC</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dspatch: Dual spatial pattern prefetcher</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="531" to="544" />
			<pubPlace>MICRO, New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The effectiveness of decoupling</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Software prefetching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="40" to="52" />
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sniper: Exploring the level of abstraction for scalable and accurate parallel multi-core simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2011-11">Nov 2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An evaluation of high-level mechanistic core models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tolerating memory latency through softwarecontrolled pre-execution in simultaneous multithreading processors</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A general framework for prefetch scheduling in linked data structures and its application to multi-chain prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="280" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic speculative precomputation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="306" to="317" />
			<date type="published" when="2001-12">Dec 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speculative precomputation: long-range prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A stateless, content-directed data prefetching mechanism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The university of florida sparse matrix collection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Julienne: A framework for parallel graph algorithms using work-efficient bucketing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dhulipala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPAA</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="293" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward a new metric for ranking high performance computing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Heroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sandia Report, SAND2013-4744</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page">150</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving data cache performance by preexecuting instructions under a cache miss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009-02">Feb 2009</date>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prefetch-aware shared-resource management for multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Coordinated control of multiple prefetchers in multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="316" to="326" />
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scale-free brain functional networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Eguiluz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18102</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The implementation of the cilk-5 multithreaded language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Loop-aware memory prefetching using code block working sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fuchs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="533" to="544" />
			<pubPlace>MICRO, Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A performance-correctness explicitlydecoupled architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2008-11">Nov 2008</date>
			<biblScope unit="page" from="306" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Computing the shortest path: A* search meets graph theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harrelson</surname></persName>
		</author>
		<idno>MSR-TR-2004-24</idno>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pipe: A vlsi decoupled architecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="1985-06">Jun. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graphicionado: A high-performance and energyefficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Eie: Efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Memory-side prefetching for linked data structures for processor-in-memory systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="448" to="463" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Access map pattern matching for data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="499" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking belady&apos;s algorithm to accommodate prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="110" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
			<biblScope unit="page" from="247" to="259" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic helper threaded prefetching on the sun ultrasparc/spl reg/ cmp processor</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="12" to="104" />
			<date type="published" when="2005-11">Nov 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Program balance and its impact on high performance risc architectures</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="1995-01">Jan 1995</date>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Prefetching using markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="263" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A prefetching technique for irregular accesses to linked data structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="206" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Resource conscious prefetching for irregular applications in multicores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hagersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAMOS</title>
		<imprint>
			<date type="published" when="2014-07">July 2014</date>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kill the program counter: Reconstructing program behavior in the processor cache hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="737" to="749" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Meet the walkers accelerating index traversals for in-memory databases</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="468" to="479" />
			<date type="published" when="2013-12">Dec 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-chain prefetching: Effective exploitation of inter-chain memory parallelism for pointer-chasing codes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kohout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="268" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spare register aware prefetching for graph algorithms on gpus</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Lakshminarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2014-02">Feb 2014</date>
			<biblScope unit="page" from="614" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Llvm: A compilation framework for lifelong program analysis &amp; transformation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Prefetch-aware dram controllers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO. Washington</title>
		<meeting><address><addrLine>DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Mcpat: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009-12">Dec 2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spaid: software prefetching in pointer-and callintensive environments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="231" to="236" />
			<date type="published" when="1995-11">Nov 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pin: Building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The effects of memory latency and finegrain parallelism on astronautics zs-1 performance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mangione-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Third Annual Hawaii International Conference on System Sciences</title>
		<imprint>
			<date type="published" when="1990-01">Jan 1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Measurement and analysis of online social networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMC</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Tolerating latency through software-controlled prefetching in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="106" />
			<date type="published" when="1991-06">Jun. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cache-Guided Scheduling: Exploiting Caches to Maximize Locality in Graph Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AGP</title>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Exploiting Locality in Graph Analytics through Hardware-Accelerated Traversal Scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
	<note>in MICRO-51</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">PHI: Architectural Support for Synchronizationand Bandwidth-Efficient Commutative Scatter Updates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
	<note>in MICRO-52</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Cacti 6.0: A tool to understand large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>in HP laboratories</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Runahead execution: an alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003-02">Feb 2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2004-02">Feb 2004</date>
			<biblScope unit="page" from="96" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Prefedge: Ssd prefetcher for large-scale graph traversal</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nilakant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SYSTOR</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Criticality aware tiered cache hierarchy: A fundamental relook at multi-level cache hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="96" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Energy efficient architecture for graph analytics accelerators</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<idno>1999-66</idno>
	</analytic>
	<monogr>
		<title level="m">Stanford InfoLab</title>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Semantic locality and context-based prefetching using reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2014-02">Feb 2014</date>
			<biblScope unit="page" from="626" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dependence based prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The evicted-address filter: A unified mechanism to address both cache pollution and thrashing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PACT</title>
		<imprint>
			<biblScope unit="page" from="355" to="366" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mitigating prefetcher-caused pollution using informed caching policies for prefetched blocks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Oscar: Optimizing scratchpad reuse for graph processing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Singapura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPEC</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A simulation study of decoupled architecture computers</title>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="692" to="702" />
			<date type="published" when="1986-08">Aug 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Decoupled access/execute computer architectures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="112" to="119" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">GraphR: Accelerating Graph Processing Using ReRAM</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018-02">Feb 2018</date>
			<biblScope unit="page" from="531" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2007-02">Feb 2007</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Load latency tolerance in dynamically scheduled processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="148" to="159" />
			<date type="published" when="1998-12">Dec 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Compiling and optimizing for decoupled architectures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Topham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing &apos;95:Proceedings of the 1995 ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="1995-12">Dec 1995</date>
			<biblScope unit="page" from="40" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A compiler-assisted data prefetch controller</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Vander Wiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1999 IEEE International Conference on Computer Design: VLSI in Computers and Processors (Cat. No.99CB37040)</title>
		<meeting>1999 IEEE International Conference on Computer Design: VLSI in Computers and Processors (Cat. No.99CB37040)</meeting>
		<imprint>
			<date type="published" when="1999-10">Oct 1999</date>
			<biblScope unit="page" from="372" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Speedup graph processing by graph ordering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1813" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Practical off-chip meta-data for temporal memory streaming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009-02">Feb 2009</date>
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">PACMan: Prefetch-Aware Cache Management for high performance caching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="442" to="453" />
			<date type="published" when="2011-12">Dec 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Efficient metadata management for irregular data prefetching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Temporal prefetching without the off-chip metadata</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="996" to="1008" />
			<pubPlace>MICRO, New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Alleviating irregularity in graph analytics acceleration: A hardware/software co-design approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="615" to="628" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A programmable memory hierarchy for prefetching linked data structures</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISHPC</title>
		<imprint>
			<biblScope unit="page" from="160" to="174" />
			<date type="published" when="2002">2002</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>London, UK, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">IMP: Indirect memory prefetcher</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICRO</title>
		<imprint>
			<biblScope unit="page" from="178" to="190" />
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Minnow: Lightweight offload engines for worklist management and worklist-directed prefetching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Graphp: Reducing communication for pim-based graph processing with efficient data partition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018-02">Feb 2018</date>
			<biblScope unit="page" from="544" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Accelerating and adapting precomputation threads for effcient prefetching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Optimizing ordered graph algorithms with graphit</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="158" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Dual-core execution: Building a highly scalable single-thread instruction window</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
