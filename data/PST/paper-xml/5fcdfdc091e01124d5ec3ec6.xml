<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Adversarially Robust Representation Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-28">28 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiarong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yangya@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junru</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
							<email>wangchunping02@xinye.com</email>
							<affiliation key="aff3">
								<orgName type="department">FinVolution Group</orgName>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangang</forename><surname>Lu</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
							<affiliation key="aff6">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Adversarially Robust Representation Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-28">28 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2012.02486v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>graph representation learning, robustness</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require prespecified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs, a common mathematical abstraction for modeling pairwise interactions between objects, are widely applied in numerous domains, including bioinformatics, social networks, chemistry, and finance. Owing to their prevalence, deep learning on graphs, such graph representation adversarial risk S = (A, X ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p U x N 1 b e V 0 k b d 5 t r Q l h + w G o 3 n 7 5 Q = " &gt; A A A C E X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S J U k D J T B d 0 I F T c u K / Y F 7 V A y a d q G Z h 4 k G a E M 8 w t u / B U 3 L h R x 6 8 6 d f 2 O m H X y 0 H g i c e 8 6 9 5 N 7 j h p x J Z V m f x s L i 0 v L K a m 4 t v 7 6 x u b V t 7 u w 2 Z B A J Q u s k 4 I F o u V h S z n x a V 0 x x 2 g o F x Z 7 L a d M d X a V + 8 4 4 K y Q K / p s Y h d T w 8 8 F m f E a y 0 1 D W L H Q + r I c E 8 v k 3 Q B f o p L 5 N j 9 F 2 0 k q O u W b B K 1 g R o n t g Z K U C G a t f 8 6 P Q C E n n U V 4 R j K d u 2 F S o n x k I x w m m S 7 0 S S h p i M 8 I C 2 N f W x R 6 U T T y 5 K 0 K F W e q g f C P 1 8 h S b q 7 4 k Y e 1 K O P V d 3 p j v K W S 8 V / / P a k e q f O z H z w 0 h R n 0 w / 6 k c c q Q C l 8 a A e E 5 Q o P t Y E E 8 H 0 r o g M s c B E 6 R D z O g R 7 9 u R 5 0 i i X 7 J N S + e a 0 U K l l c e R g H w 6 g C D a c Q Q W u o Q p 1 I H A P j / A M L 8 a D 8 W S 8 G m / T 1 g U j m 9 m D P z D e v w C G d J z h &lt; / l a t e x i t &gt; Z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + X U z a I a O B F 7 K 2 e y Z X Z p Y m z / r t d 4 = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C G 5 c V + s L O U D J p p g 3 N Z I Y k I 5 S h v + H G h S J u / R l 3 / o 2 Z d h b a e i B w O O d e 7 s k J E s G 1 c Z x v V N r Y 3 N r e K e 9 W 9 v Y P D o + q x y d d H a e K s g 6 N R a z 6 A d F M c M k 6 h h v B + o l i J A o E 6 w X T u 9 z v P T G l e S z b Z p Y w P y J j y U N O i b G S 5 0 X E T C g R 2 e M c D 6 s 1 p + 4 s g N e J W 5 A a F G g N q 1 / e K K Z p x K S h g m g 9 c J 3 E + B l R h l P B 5 h U v 1 S w h d E r G b G C p J B H T f r b I P M c X V h n h M F b 2 S Y M X 6 u + N j E R a z 6 L A T u Y Z 9 a q X i / 9 5 g 9 S E t 3 7 G Z Z I a J u n y U  Y k Q j h E R q Q j q E c h U T 5 y e S f F B 4 a p Q f 7 Q p r i G k 7 U 3 x M J C p U a h 4 H p z E 5 U s 1 4 m / u d 1 Y t 0 / 9 x P K o 1 g T j q e L + j G D W s A s H N i j k m D N x o Y g L K m 5 F e I h k g h r E 2 H R h O D O v j x P m p W y e 1 K u X J + W q v U 8 j g L Y B w f g G L j g D F T B F a i B B s D g A T y B F / B q P V r P 1 p v 1 P m 1 d s P K Z P f A H 1 s c 3 U Z C c 6 g = = &lt; / l a t e x i t &gt; as graph neural networks (GNNs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>, have recently undergone rapid development, making major progress in various analytical tasks, including node classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>, link prediction <ref type="bibr" target="#b23">[24]</ref>, and graph classification <ref type="bibr" target="#b43">[44]</ref>. However, most deep learning models on graphs are trained with task-specific labels in an end-to-end manner for a particular task. This motivates some recent efforts to pre-train an expressive graph encoder on unlabeled data and further feed the learned representations to (supervised/unsupervised) off-the-shelf machine learning models for relevant downstream tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref>. The pre-training models on graphs enable the learned representations to be directly applicable to different applications with a simple and inexpensive machine learning model attached after the encoded representations. Despite the promising results achieved by deep learning models on graphs, recent studies have shown that these models are vulnerable to adversarial attacks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49]</ref>. In other words, even imperceptible perturbations on graph topology and node attributes can significantly affect the learned graph representation, thereby degrading the performance of downstream tasks <ref type="bibr" target="#b5">[6]</ref>. This so-called adversarial vulnerability has given rise to tremendous concerns regarding the utilization of deep learning models on graphs, especially in security-critical applications such as drug discovery <ref type="bibr" target="#b13">[14]</ref> and financial surveillance <ref type="bibr" target="#b28">[29]</ref>. However, the adversarial vulnerability of pre-training models on graphs is far overlooked. In this work, we show that graph pre-training models also suffer from the adversarial vulnerability problem. Actually, owing to the complicated and deep structure, the graph encoder is more vulnerable to adversarial attacks than the simple machine learning models used for downstream tasks in a graph pre-training pipeline <ref type="bibr" target="#b35">[36]</ref>. As Figure <ref type="figure" target="#fig_2">1</ref> shows, once the graph encoder is vulnerable to adversarial attacks, the adversarial risk would propagate to every downstream task via the perturbed representations.</p><formula xml:id="formula_0">J g K b G K c F 4 B H X D F q x M w S Q h W 3 W T G d E E W o s T V V b A n u 6 p f X S b d R d 6 / q j Y f r W r N d 1 F G G M z i</formula><formula xml:id="formula_1">I P M c X V h n h M F b 2 S Y M X 6 u + N j E R a z 6 L A T u Y Z 9 a q X i / 9 5 g 9 S E t 3 7 G Z Z I a J u n y U J g K b G K c F 4 B H X D F q x M w S Q h W 3 W T G d E E W o s T V V b A n u 6 p f X S b d R d 6 / q j Y f r W r N d 1 F G G M z i</formula><p>Most efforts targeted on this adversarial vulnerability problem focus on supervised, end-to-end models designed for a particular application scenario <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>. However, the dependency on the supervised information largely limits the scope of their application and usefulness. For example, these models do not perform well on downstream tasks in which training labels are missing, e.g., community detection in social networks. In addition, training multiple models for different downstream tasks is both costly and insecure <ref type="bibr" target="#b11">[12]</ref>. In contrast, robust unsupervised pre-training models can easily handle the above issues: adversarial attacks are identified and blocked before propagating to downstream tasks. Moreover, these models are applicable to a more diverse group of applications, including node classification, link prediction, and community detection. And yet, robust pre-training models under the unsupervised setting remains largely unexplored.</p><p>There are many interesting yet challenging questions in this new field of research. Conventionally, the robustness of a model is defined based on the label space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b49">50]</ref>, which is not the case in our setting. Thus the first difficulty we meet is to quantify the robustness of an unsupervised model (without the knowledge of the true or predicted labels).</p><p>To overcome the above challenge, in this paper, we first introduce the graph representation vulnerability (GRV), an information theoretic-based measure used to quantify the robustness of a graph encoder. We then formulate an optimization problem to study the trade-off between the expressive power of a graph encoder and its robustness to adversarial attacks, measured in GRV. However, how to efficiently compute or approximate the objective of the optimization problem becomes the next issue. First, it remains a big problem on how to describe the ability of the attack strategies or the boundary of perturbations, because adversarial attacks on graphs perturb both the discrete graph topology and the continuous node attributes. Second, the rigorous definition of the objective is intractable.</p><p>To handle the above issues, we first quantify the ability of adversarial attacks using Wasserstein distance between probability distributions, and provide a computationally efficient approximation for it. We then adopt a variant of projected gradient descent method to solve the proposed optimization problem efficiently. A sub-optimal solution for the problem gives us a well-qualified, robust graph representation encoder.</p><p>Last but not least, we explore several interesting theoretical connections between the proposed measure of robustness (GRV) and the classifier robustness based on the label space. To show the practical usefulness of the proposed model, we apply the learned representations to three different downstream tasks, namely, node classification, link prediction, and community detection. Experimental results reveal that under adversarial attacks, our model beats the best baseline by an average of +1.8%, +1.8%, and +45.8% on node classification, link prediction, and community detection task, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND NOTATIONS</head><p>In most cases, we use upper-case letters (e.g., ? and ? ) to denote random variables and calligraphic letters (e.g., X and Y) to denote their support, while the corresponding lower-case letters (e.g., ? and ?) indicate the realizations of these variables. We denote the random variables of the probability distributions using subscripts (e.g., ? ? and ? ? ) and the corresponding empirical distributions with hat accents (e.g., ?? and ?? ). We use bold upper-case letters to represent matrices (e.g., A). When indexing the matrices, A ? ? denotes the element at the ?-th row and the ?-th column, while A ? represents the vector at the ?-th row. We use (X, ?) to denote the metric space, where ? : X ? X ? R is a distance function on X. We further denote by M (X) the set of all probability measures on X.</p><p>We assume a generic unsupervised graph representation learning setup. In brief, we are provided with an undirected and unweighted graph G = (V, E) with the node set</p><formula xml:id="formula_2">V = {? 1 , ? 2 , ..., ? |V | } and edge set E ? V ? V = {? 1 , ? 2 , ..., ? |E | }.</formula><p>We are also provided with the adjacency matrix A ? {0, 1} |V |?|V | of the graph G, a symmetric matrix with elements A ? ? = 1 if (? ? , ? ? ) ? E or ? = ?, and A ? ? = 0 otherwise. We augment G with the node attribute matrix X ? R |V |?? if nodes have attributes. Accordingly, we define our input as ? = (?, ?) ? S; thus, we can conceive of ? as the attribute matrix and ? as the adjacency matrix of G under a transductive learning setting, while ? and ? are the adjacency matrix and attribute matrix respectively of a node's subgraph under an inductive learning setting. We further define an encoder ? : S ? Z, which maps an input ? = (?, ?) ? S to a representation ? (?, ?) ? Z, and a simple machine learning model ? : Z ? Y that maps a representation ? ? Z to a label ? (?) ? Y. We go on to define ? = ? ? ? as their composition, such that (? ? ?)(?, ?) = ? (? (?, ?)). A table of main notations is attached in the Appendix. Mutual information. Recall that the mutual information between two random variables ? and ? is a measure of the mutual dependence between them, and is defined as the Kullback-Leibler (KL) divergence between the joint distribution ? (?, ?) and the product of the marginal distributions ? (?)? (?):</p><formula xml:id="formula_3">I(? ; ? ) = ? KL ? (?, ?)?? (?)? (?) = ? Y ? X ? (?, ?) log ? (?, ?) ? (?)? (?) ????.</formula><p>More specifically, it quantifies the "amount of information" obtained about one random variable through observing the other random variable. The successful application of mutual information to various unsupervised graph representation learning tasks has been demonstrated by many authors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Admissible perturbations on graphs. The Wasserstein distance can be conceptualized as an optimal transport problem: we wish to move transport the mass with probability distribution ? ? into another distribution ? ? ? at the minimum cost. Formally, the ?-th Wasserstein distance between ? ? and ? ? ? is defined as</p><formula xml:id="formula_4">? ? = (? ? , ? ? ? ) = inf ? ?? (? ? ,? ? ? ) ? S 2 ? (?, ? ? ) ?? (?, ? ? ) 1/?</formula><p>, where ?(? ? , ? ? ? ) denotes the collection of all measures on S ? S with marginal ? ? and ? ? ? , respectively. The choice of ?-Wasserstein distance (i.e., ? = ?) is conventional in learning graph representations <ref type="bibr" target="#b3">[4]</ref>.</p><p>Based on ?-Wasserstein distance, we can quantify the ability of the adversarial attacks. An attack strategy is viewed as a perturbed probability distribution around that of ? = (?, ? ), and then all possible attack strategies stay in a ball around the genuine distribution ? ? :</p><formula xml:id="formula_5">B ? (? ? , ?) = {? ? ? ? M (?) : ? ? (? ? , ? ? ? ) ? ? },</formula><p>where ? &gt; 0 is a pre-defined budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPHS REPRESENTATIONS ROBUST TO ADVERSARIAL ATTACKS</head><p>In a widely adopted two-phase graph learning pipeline, The first step is to pre-train a graph encoder ? (without the knowledge of true labels), which maps the joint input space S (i.e., the graph topology A and node attributes X) into some, usually lower-dimensional, representation space Z. Then the encoded representation is used to solve some target downstream tasks.</p><p>In this section, we explain how to obtain a well-qualified graph representation robust to adversarial attacks. We first propose a measure to quantify the robustness without label information in ?3.1. In ?3.2, we formulate an optimization problem to explore the trade-off between the expressive power and the robustness of the graph encoder. We then describe every component in the proposed optimization problem, and explain how we obtain a sub-optimal solution efficiently in ?3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantifying the Robustness of Graph Representations</head><p>In this section, we propose the graph representation vulnerability (GRV) to quantify the robustness of an encoded graph representation. Intuitively, the learned graph representation is robust if its quality does not deteriorate too much under adversarial attacks. Now we introduce in detail how to measure the quality of representations using MI, and how to describe the difference of representation quality before and after adversarial attacks. The use of mutual information. A fundamental challenge to achieving a qualified graph representation is the need to find a suitable objective that guides the learning process of the graph encoder. In the case of unsupervised graph representation learning, the commonly used objectives are random walk-based <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref> or reconstruction-based <ref type="bibr" target="#b23">[24]</ref>. These objectives impose an inductive bias that neighboring nodes or nodes with similar attributes have similar representations. However, the inductive bias is easy to break under adversarial attacks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>, leading to the vulnerability of random walk-based and reconstruction-based encoders. As an alternative solution, we turn to maximize the MI between the input attributed graph and the representation output by the encoder, i.e., I(?; ? (?)). In our case, maximizing the MI I(?; ? (?)) encourages the representations to be maximally informative about the input graph and to avoid the above-mentioned inductive bias.</p><p>Graph representation vulnerability. In addition to the measure of the quality of a graph representation, we also need to describe the robustness of a representation. Intuitively, an encoder is robust if the MI before and after the attack stay close enough. Thus, we propose the graph representation vulnerability (GRV) to quantify this difference:</p><formula xml:id="formula_6">GRV ? (?) = I(?; ? (?)) - inf ? ? ? ?B (? ? ,?) I(? ? ; ? (? ? )),<label>(1)</label></formula><p>where ? = (?, ? ) is the random variable following the benign data distribution, and ? ? = (? ? , ? ? ) follows the adversarial distribution.</p><p>The first term I(?; ? (?)) in ( <ref type="formula" target="#formula_6">1</ref>) is the MI between the benign graph data and the encoded representation, while the term I(? ? , ? (? ? )) uses the graph data after attack. The attack strategy ? ? ? that results in the minimum MI is called the worst-case attack, and is defined as</p><formula xml:id="formula_7">? ? ? = argmin ? ? ? ?B (? ? ,?) I(? ? ; ? (? ? )).</formula><p>Hence by definition, the graph representation vulnerability (GRV) describes the difference of the encoder's behavior using benign data and under the worst-case adversarial attack. A lower value of GRV ? (?) implies a more robust encoder to adversarial attacks. Formally, an encoder is called (?, ?)-robust if GRV ? (?) ? ?.</p><p>An analogy to the graph representation vulnerability (GRV) has been studied in the image domain <ref type="bibr" target="#b47">[48]</ref>. However, the extension of <ref type="bibr" target="#b47">[48]</ref> to the graph domain requires nontrivial effort. An image is considered to be a single continuous space while a graph is a joint space S = (A, X), consisting of a discrete graph-structure space A and a continuous feature space X. Moreover, the perturbations on the joint space (A, X) is difficult to track because a minor change in the graph topology or node attributes will propagate to other parts of the graph via edges. This is different in the image domain, where the distributions of all the pixels are assumed to be i.i.d. Therefore, the discrete nature of graph topology and joint space (A, X) make the worst-case adversarial attack extremely difficult to estimate. Thus, the optimization method we apply is substantially different from that in <ref type="bibr" target="#b47">[48]</ref>; see ?3.2 and ?3.3. Furthermore, more complicated analysis is needed to verify our approach in theory; see ?4 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization Problem</head><p>The trade-off between model robustness and the expressive power of encoder has been well-studied by many authors <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47]</ref>. In our case, this trade-off can be readily explored by the following optimization problem</p><formula xml:id="formula_8">maximize ? 1 (?) = I(?; ? (?)) -?GRV ? (?),<label>(2)</label></formula><p>where the optimization variable is the learnable parameters ? of the encoder ?, and ? &gt; 0 is a pre-defined parameter. However, in practice, the "most robust" encoder is usually not the desired one (as it sacrifices too much in the encoder's expressive power). An intuitive example for the "most robust" encoder is the constant map, which always outputs the same representation whatever the input is. Hence, a "robust enough" encoder would be sufficient, or even better. To this end, we add a soft-margin ? to GRV, and obtain the following optimization problem</p><formula xml:id="formula_9">maximize ? 2 (?) = I(?; ? (?)) -? max {GRV ? (?), ? }. (3)</formula><p>The second term is positive if GRV ? &gt; ? and the constant ? otherwise. As a result, when the encoder is sufficiently robust, the second term in ? 2 vanishes, and thus Problem (3) turns to the standard MI maximization using benign data. Furthermore, when ? = 1, Problem (3) can be divided into two simple sub-problems, depending on the value of GRV ? (?):</p><formula xml:id="formula_10">? ? ? ? ? ? ? max ? inf ? ? ? ?B ? (? ? ,?) I(? ? ; ? (? ? )), if GRV ? &gt; ? max ? I(?; ? (?)), otherwise.<label>(4)</label></formula><p>In this case (? = 1), when GRV ? (?) &gt; ?, the problem maximizes the MI under the worst-case adversarial attack. In other words, the robust encoder tries to maintain the mutual dependence between the graph data and the encoded representation, under all kinds of adversarial attacks. When the encoder is sufficiently robust (i.e., GRV ? (?) ? ?), the problem turns to maximize the expressive power of graph encoder. GNN as the parameterized encoder. The graph neural network (GNN) has been extensively used as an expressive function for parameterizing the graph encoder <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. In this paper, we adopt a one-layer GNN:</p><formula xml:id="formula_11">? (A, X) = ? ( D-1/2 ? D-1/2 X?),</formula><p>where ? is the adjacency matrix with self-loops, D is the corresponding degree matrix, ? is the ReLU function, and ? is the learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximate Solution</head><p>In this section, we discuss in detail how to obtain a sub-optimal solution for Problem (4). Overall, the algorithm we use is a variant of the classical gradient-based methods, as presented in Algorithm 1.</p><p>In every iteration, we first find out the distribution of the worstcase adversarial attack, and thus we can calculate the value of GRV. If GRV is larger than ?, we try to enhance the robustness in this iteration, and apply one gradient descent step for the first subproblem in Problem (4). Otherwise when GRV ? (?) &lt; ?, the encoder is considered robust enough, and thus we focus on improving the expressive power by one gradient descent move in the second subproblem in (4). As a small clarification, the stopping criterion in Algorithm 1 follows the famous early-stopping technique <ref type="bibr" target="#b30">[31]</ref>. However, there are still many kinds of difficulties in implementing the algorithm. First of all, the mutual information I(?, ? (?)) is extremely hard to compute, mainly because ? = (?, ? ) is a joint random variable involving a high-dimensional discrete variable ?. In addition, the search space of the adversarial attacks, B ? (? ? , ?), is intractable to quantify: There is no conventional or well-behaved choice for the distance metric ? in such a complicated joint space, and even when we know the metric, the distance between two random variables is difficult to calculate. Apart from the above challenges, the classical, well-known projected gradient descent algorithm does not work in the joint space ? = (?, ? ), and thus the worst-case adversarial attack ? ? ? is no way to find. Therefore, in this section, we further address the above issues in detail and explain every component in Algorithm 1. MI estimation. Directly computing I(?; ? (?)) in Problem ( <ref type="formula" target="#formula_10">4</ref>) is intractable, especially for a joint distribution ? = (?, ? ) which includes a high-dimensional discrete random variable ?. Some authors propose to maximize the average MI between a high-level "global" representation and local regions of the input, and show Algorithm 1 Optimization algorithm.</p><p>Input: Graph G = (A, X), learning rate ?. Output: Graph encoder parameters ?. ? ? ? ? argmin ? ? ? ?B (? ? ,?) I(? ? ; ? (? ? )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>GRV ? (?) ? I(?; ? (?)) -I(? ? ; ? (? ? )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if GRV ? (?) &gt; ? then 6:</p><p>? ? ? -?? ? I(? ? ; ? (? ? )). ? ? ? -?? ? I(?; ? (?)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>end if 10: end while Return: ?.</p><p>great improvement in the quality of representations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>. Inspired by recent work Deep Graph Infomax <ref type="bibr" target="#b38">[39]</ref>, we use a noisecontrastive type objective as an approximation of the mutual information (? ; ? (?)):</p><formula xml:id="formula_12">? enc (?, ?) = E ? [log D (?, ? G )] + E S [log (1 -D (z, ? G ))] ,<label>(5)</label></formula><p>where ? denotes the local representation; ? G = sigmoid (E ? (?)) represents the global representation; S is the random variable of negative examples, and z represents the realization of ? ( S). The critic function D (?, ? G ) represents the probability score assigned to a pair of local and global representations obtained from the natural samples (i.e., the original graph), while D (z, ? G ) is that obtained from negative samples. Common choices for the critic function D include bilinear critics, separable critics, concatenated critics, or even inner product critics <ref type="bibr" target="#b36">[37]</ref>. Here, we select the learnable bilinear critic as our critic function; i.e., D ? = sigmoid(? ? ?? G ), where ? is a learnable scoring matrix. Finally, in practice, the expectation over an underlying distribution is typically approximated by the expectation of the empirical distribution over ? independent samples</p><formula xml:id="formula_13">{(? ? , ? ? )} ? ? [?] .</formula><p>Adversarial distribution estimation. Besides the estimation of MI, another challenge involved in solving Problem ( <ref type="formula" target="#formula_10">4</ref>) is how to find the worst-case adversarial distribution ? ? ? ? B ? (? ? , ?). Here, we divide the difficulties in find ? ? ? into three categories, and explain in detail how we solve them one by one. First, it is difficult to choose an appropriate metric ? on the joint input space S = (A, X) that faithfully measures the distance between each pair of point elements. For example, given any pair of points ? 1 = (? 1 , ? 1 ) and ? 2 = (? 2 , ? 2 ) in the joint metric space (A, ? A ) and (X, ? X ), an intuitive choice for the distance between ? 1 and ? 2 would be the ? ? -norm ? ? A (? 1 , ? 2 ), ? X (? 1 , ? 2 ) ? ? . However, this intuition fails in our case because the changes in graph topology and that in node attributes are not in the same order of magnitude. Thereby, we have to consider the perturbations in A and X separately. With a little abuse of notation, we redefine the perturbation bound as follows:</p><formula xml:id="formula_14">B ? (? ? , ? ? , ?, ?) = {(? ? ? , ? ? ? ) ? M (A) ? M (X) | ? ? (? ? , ? ? ? ) ? ?,? ? (? ? , ? ? ? ) ? ?},</formula><p>where the small positive numbers ? and ? play the role of perturbation budget now. This is indeed a subset of the previous search space B (? ? , ?).</p><p>Moreover, although the search space has been restricted, the ?-Wasserstein constrained optimization problem remains intractable: We still have no clue about the underlying probability distribution.</p><p>Similar to what we did to estimate MI, we turn to replace the real data distribution with an empirical one. Suppose we have a set of i.i.d. samples {(? ? , ? ? )} ? ? [?] (note that ? = 1 under a transductive learning setting, based on which we can compute the empirical distribution ( ?? , ?? )). The empirical search space is defined as</p><formula xml:id="formula_15">B {? ? } ? ?=1 , {? ? } ? ?=1 , ?, ? = ( ?? ? , ?? ? ) ?? ? ? -? ? ? 0 ? ?, ?? ? ? -? ? ? ? ? ?, ? ? [?] ,</formula><p>where ?? ? and ?? ? are the empirical distributions computed from the perturbed samples</p><formula xml:id="formula_16">{(? ? ? , ? ? ? )} ? ? [?] .</formula><p>Here we use the cardinality (i.e., ? 0 -norm) to measure the change in graph topology (i.e., ?), and the ? ? -norm to measure the change in continuous node attributes (i.e., ?). (When node attributes are discrete, or even binary, we can also use ? 0 -norm for them.) Finally, we notice that the empirical space B {? ? } ? ?=1 , {? ? } ? ?=1 , ?, ? is again a subset of B ? ( ?? , ?? , ?, ?). The last, yet the most challenging difficulty is how to efficiently find the worst-case adversarial attack. We know that he projected gradient descent (PGD) method can be used to find adversarial examples in the image domain <ref type="bibr" target="#b26">[27]</ref>. However, the idea that works well for continuous optimization problems is not directly applicable in our case as the graph topology is a kind of Boolean variables. Inspired by <ref type="bibr" target="#b42">[43]</ref>, as a remedy for the discrete case, we adopt a graph PGD attack for graph topology. We first find a convex hull of the discrete feasible set, and apply the projected gradient method. A binary sub-optimal solution ? ? is then recovered using random sampling. This variant of PGD helps us identify the worst-case adversarial example efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL CONNECTION TO LABEL SPACE</head><p>We expect our robust graph encoder is able to block perturbations on graphs and benefits the downstream tasks. To this end, in this section, we establish a provable connection between the robustness of representations (measured by our proposed GRV) and the robustness of the potential model built upon the representations. Despite the generalization of our framework, we take node classification tasks as an example in this section. First, we introduce a conventional robustness, adversarial gap (AG), to measure the robustness of downstream node classifier. Then, we explore some interesting theoretical connections between GRV and AG. Adversarial gap. We here introduce a conventional robustness measure adversarial gap (AG) for node classification, which is built on the label space. Definition 1. Suppose we are under inductive learning setting, then ? and ? are the adjacency matrix and attribute matrix respectively of a node's subgraph. Let (S, ?) denote the input metric space and Y be the set of labels. For node classification model ? : S ? Y, we define the adversarial risk of ? with the adversarial budget ? ? 0 as follows:</p><formula xml:id="formula_17">AdvRisk ? (?) = E ? (?,?) [? ? ? = (? ? , ? ? ) ? B (?, ?) s.t. ?(? ? , ? ? ) ? ?]</formula><p>Based on AdvRisk ? (?), the adversarial gap is defined to measure the relative vulnerability of a given model ? w.r.t ? as follows:</p><p>AG ? (?) = AdvRisk ? (?) -AdvRisk 0 (?).</p><p>The smaller the value of AdvRisk or AG is, the more robust ? is. Table <ref type="table">1</ref> briefly summarize the robustness measures, including AG, RV and GRV. The traditional model robustness, adversarial gap (i.e., AG ? (?) and AG ? (?)), is based on the label space Y, while the MI-based robustness measures (i.e., RV * (?) and GRV * (?)) is built upon the representation space Z. The prior work <ref type="bibr" target="#b47">[48]</ref>, which defines RV ? (?) on a single input space X in the image domain has shown that RV ? (?) has a clear connection with classifier robustness. The graph representation venerability GRV ? (?), however, defined as it is on a joint input space (A, X) in the graph domain, is different from images due to the existence of both discrete and continuous input data structures. In what follows, we explore some interesting theoretic conclusions that an inherent relationship exists between the graph representation vulnerability GRV ? (?) and the adversarial gap AG ? (?); this is based on some certain assumptions that is more aligned with the graph representation learning.</p><p>In exploring the GRV's connection to the label space Y, one solution could be to simply assume that one of the input random variables (i.e., ? or ? ) and the label random variable ? are independent. We first consider the two special cases as follows:</p><p>? Here, we first work on these two special cases under the relevant assumptions. We then illustrate a more general case in which ? is dependent on both ? and ? . Detailed proofs of the following theorems can be found in the Appendix. Special cases. We simplify the GNN-based encoder architecture as ? = ? ? ?? to obtain a tractable surrogate model. Thus, the representation of each node depends only on its one-hop neighbors, so we can obtain the corresponding column of A directly to compute the representation for each node. Additionally, inspired by <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref> that have defined the perturbations on the intermediate representations, Table <ref type="table">1</ref>: Summary of robustness measures. Here, the adversarial gap (AG) is the robustness measure built on the label space Y, while representation vulnerability (RV) and graph representation vulnerability (GRV) are MI-based measures built on the representation space Z. The subscript ? denotes the perturbation budget of ? (i.e., the image) on the image domain, while the subscript ? denotes the perturbation budget of (?, ?) on the graph domain. to denote a column of A and ? = X. The subscript ? of GRV, AdvRisk and AG represents that they are defined via B ? (? ? ? ? , ?), while F = {? : ? ? ? ?} denotes the set of non-trivial downstream classifiers, ? * = arg min ? ? F AdvRisk ? (? ? ?) is the optimal classifier built upon ?, and ? ? is the binary entropy function. Moreover, when indexing ? and ?, ? ? denotes the ?-th entry of ? and ? ? denotes the ?-th row of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness measure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain</head><p>Theorem 4.1 (Topology-aware). Let (A, ? ? ? 0 ) and (X, ? ? ? ? ) be the input metric spaces, Y = {-1, +1} be the label space and Z = {-1, +1} be the representation space. The set of encoders with ? ? R |V | is as follows:</p><formula xml:id="formula_18">E = {? : (?, ?) ? S ? ? sgn[? ? ??]| ??? 2 = 1}. (<label>6</label></formula><formula xml:id="formula_19">)</formula><p>Assume that all samples (?, ?) ? ? ?? are generated from ? u.a.r.</p><formula xml:id="formula_20">? ? {-1, +1}, ? ? i.i.d.</formula><p>? Bernoulli(0.5+??(?-0.5)) and ? ? i.i.d.</p><p>? N (0, ? 2 ? ? ) where ? = 1, 2, . . . , |? | and 0 &lt; ? &lt; 1. Then, given ? ? 0, for any ? ? E, we have the following:</p><formula xml:id="formula_21">GRV ? (?) = 1 -? ? (0.5 + AG ? (? * ? ?)).</formula><p>Next, consider a simpler case in which ? </p><p>Theorem 4.1 reveals an explicit connection between GRV ? (?) and AG ? (? * ? ?) achieved by the best classifier in the topologyaware case. We note that ? ? (? ) is concave on (0, 1) and that the maximum of ? ? is attained uniquely at ? = 0.5. Thus, lower GRV is the sufficient and necessary condition of a smaller AG. Theorem 4.2 (Attribute-aware). Let (A, ? ? ? 0 ) and (X, ? ? ? ? ) be the input metric spaces, Y = {-1, +1} be the label space and Z = {-1, +1} be the representation space. Suppose that the set of encoders is as in <ref type="bibr" target="#b5">(6)</ref>. Assume that the samples (?, ?) ? ? ?? are generated from ? N (? ? ?, ? 2 ? ? ) where ? = 1, 2, . . . , |? |. Then, given ? ? 0, for any ? ? E, we have:</p><formula xml:id="formula_23">GRV ? (?) = 1 -? ? (0.5 -AG ? (? * ? ?)).<label>(8)</label></formula><p>Next, consider a simpler case in which ? u.a.r.</p><formula xml:id="formula_24">? ? {-1, +1}, ? ? ?.?.?. ? N (? ? ?, ? 2 ? ? ) but ? ? {0, 1} |? | , |? | ?=1 ? ? = ? 0 + ? 1 , where ? 0 = |? |/4 + ? ? (? -|? |/4), ? 1 = |? |/4 + ? ? (? -|? |/4) and ? + ? = |? |/2, 0 ? ?, ? ? |? |/2,</formula><p>?, ? ? Z; that is, ? ? ? will aggregate ? 0 samples with ? = +1 and ? 1 samples with ? = -1. Further suppose that the set of encoders is as presented in <ref type="bibr" target="#b5">(6)</ref>. Then, given ? ? 0, (7) also holds for any ? ? E.</p><p>Similarly, we have GRV ? ? AG ? in Theorem 4.2. Note that Theorems 4.1 and 4.2 still hold when ? contains self-loops. General case. In the general case, we can extend <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">Theorem 3.4]</ref> to the graph domain. Regardless of the encoder, the theorem below provides a general lower bound of adversarial risk over any downstream classifiers that involves both MI and GRV. We restate the theorem below. Theorem 4.3. <ref type="bibr" target="#b47">[48]</ref>. Let (S, ?) be the input metric space, Z be the representation space and Y be the label space. Assume that the distribution of labels ? ? over Y is uniform and ? is the random variable following the joint distribution of inputs ? ?? . Further suppose that F is the set of downstream classifiers. Given ? ? 0, inf</p><formula xml:id="formula_25">? ? F AdvRisk ? (? ? ?) ? 1 - ? (?; ? (?)) -GRV ? (?) + log 2 log |Y|</formula><p>holds for any encoder ?.</p><p>Theorem 4.3 suggests that lower adversarial risk over all downstream classifiers cannot be achieved without either lower GRV or higher MI between ? and ? (?). It turns out that jointly optimizing the objective of maximizing ? (?; ? (?)) and that of minimizing GRV ? (?) enables the learning of robust representations. Note that Theorem 4.3 also holds in the graph classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we demonstrate that our model capable of learning highly-qualified representations that are robust to adversarial attacks. In the experiments, we train our model in a fully unsupervised manner, then apply the output representations to three graph learning tasks: namely, node classification, link prediction, and community detection. We demonstrate that compared with non-robust and other robust graph representation models, the proposed model produces robust representations to defend adversarial attacks ( ? 5.2). Furthermore, the superiority of our model still hold under different strengths of attacks (in ?5.3) and under various attach strategies ( ?5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Datasets. We conduct experiments on three benchmark datasets: Cora, Citeseer and Polblogs. The first two of these are well-known citation networks in which nodes are documents and edges are the citation links between two documents. Polblogs is a network of US politics weblogs, where nodes are blogs and the connection between two blogs forms an edge. As Polblogs is a dataset without node attributes, the identity matrix is used to create its node attributes. Baselines. The baseline models roughly fall into two categories.</p><p>? Non-robust graph representation learning: 1) Raw: raw features concatenating the graph topology and the node attributes (graph topology (only) for Polblogs); 2) DeepWalk <ref type="bibr" target="#b29">[30]</ref>: a random walk-based unsupervised representation learning method that only considers the graph topology; 3) DeepWalk+X: concatenating the Deepwalk embedding and the node attributes; 4) GAE <ref type="bibr" target="#b23">[24]</ref>: variational graph auto-encoder, an unsupervised representation learning method; and 5) DGI <ref type="bibr" target="#b38">[39]</ref>: another unsupervised representation learning method based on MI. ? Defense models: 1) Dwns_AdvT <ref type="bibr" target="#b7">[8]</ref>: a defense model designed for Deepwalk; 2) RSC <ref type="bibr" target="#b2">[3]</ref>: a robust unsupervised representation learning method via spectral clustering; 3) DGI-EdgeDrop <ref type="bibr" target="#b33">[34]</ref>: a defense model that works by dropping 10% of edges during training DGI; 4) DGI-Jaccard <ref type="bibr" target="#b41">[42]</ref>: DGI applied to a pruned adjacency matrix in which nodes with low Jaccard similarity are forced to be disconnected; and 5) DGI-SVD <ref type="bibr" target="#b9">[10]</ref>: DGI applied to a low-rank approximation of the adjacency matrix obtained by truncated SVD.</p><p>We also include Ours-soft, an variant of our model which removes soft margin on GRV. Implementation details. In the training phase, we adopt the graph PGD attack to construct adversarial examples of ? while the PGD attack <ref type="bibr" target="#b26">[27]</ref> to construct adversarial examples of ?. We set the hyperparameters ? = 5e-3, ? = 0.4|E|, and ? = 0.1. For Polblogs, we do not perform attacks on the constructed node attributes.</p><p>In evaluation, we use the same attack strategy as in the training phase. Note that DeepWalk and RSC both require the entire graph, and thus we have to retrain them using polluted data. Due to the imperceptible constraint on adversarial attacks, we set ? = 0.2|E| during evaluation. The evaluation is performed on three downstream tasks, and we explain the detailed settings below.</p><p>? Node classification: logistic regression is used for evaluation, and only accuracy score is reported as the test sets are almost balanced. For Cora and Citeseer, we use the same dataset splits as in <ref type="bibr" target="#b24">[25]</ref>, but do not utilize the labels in the validation set. For Polblogs, we allocate 10% of the data for training and 80% for testing. ? Link prediction: logistic regression is used to predict whether a link exists or not. Following conventions, we generate the positive test set by randomly removing 10% of existing links and form the negative test set by randomly sampling the same number of nonexistent links. The training set consists of the remaining 90% of existing links and the same number of additionally sampled nonexistent links. We use the area under the curve (AUC) as the evaluation metric on the link prediction task.</p><p>? Community detection: following the basic schemes for community detection based on graph representation learning, we apply the learned representations to the K-means algorithm. The normalized mutual information (NMI) is used as the evaluation metric here.</p><p>We run 10 trials for all the experiments and report the average performance and standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance on Downstream Tasks</head><p>After adversarial attacks on graph topology and node attributes, our model's performance drops by an average of 13.6%, 1.0% and 47.3% on the node classification, link prediction and community detection task, respectively. It's worth noting that, in community detection, adversarial attacks can cause dramatic influence on model performance because the community detection task itself is very sensitive to the graph topology. Table <ref type="table" target="#tab_2">2</ref> summarizes the performance of different models in three downstream tasks. From the table we see that our model beats the best baseline by an average of +1.8% on the node classification task, +1.8% on the link prediction task and +45.8% on the community detection task. The difference between the performance of our model and that of those non-robust graph learning models indicates the importance of defending adversarial attacks. Moreover, our model still stands out with huge lead when compared with existing defense models. Last but not least, the ablation study, i.e., comparing the last two rows in Table <ref type="table" target="#tab_2">2</ref>, shows the superiority of the soft margin on GRV. With this penalty, the model focuses on the representation capability on clean data when the encoder is robust enough (i.e., RV ? ? ?), while carefully balances the trade-off between the performance on clean data and the robustness to polluted ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance under Different Rates of Perturbation</head><p>We further compare our model with several strong competitors under various perturbation rates. We use the node classification task and the Cora dataset as an illustrative example. We vary the strength   </p><formula xml:id="formula_26">V 1 g d x y R u E I g f V V D A l m o C I P T M G a 8 = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 0 9 S w X 5 A G 8 p m O 2 m X b j Z x d y O U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K n R 4 m m o t Y 9 s s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S C K V h g m r d 9 d z E + B l V h j O B 0 1 I v 1 Z h Q N q Z D 7 F o q a Y T a z + b 3 T s m Z V Q Y k j J U t a c h c / T 2 R 0 U j r S R T Y z o i a k V 7 2 Z u J / X j c 1 4 b W f c Z m k B i V b L A p T Q U x M Z s + T A V f I j J h Y Q p n i 9 l b C R l R R Z m x E J R u C t / z y K m n V q t 5 F t X Z / W a n f 5 X E U 4 Q R O 4 R</formula><formula xml:id="formula_27">V 1 g d x y R u E I g f V V D A l m o C I P T M G a 8 = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 0 9 S w X 5 A G 8 p m O 2 m X b j Z x d y O U 0 D / h x Y M i X v 0 7 3 v w 3 b t s c t P X B w O O 9 G W b m B Y n g 2 r j u t 1 N Y W 9 / Y 3 C p u l 3 Z 2 9 / Y P y o d H L R 2 n i m G T x S J W n Y B q F F x i 0 3 A j s J M o p F E g s B 2 M b 2 Z + + w m V 5 r F 8 M J M E / Y g O J Q 8 5 o 8 Z K n R 4 m m o t Y 9 s s V t + r O Q V a J l 5 M K 5 G j 0 y 1 + 9 Q c z S C K V h g m r d 9 d z E + B l V h j O B 0 1 I v 1 Z h Q N q Z D 7 F o q a Y T a z + b 3 T s m Z V Q Y k j J U t a c h c / T 2 R 0 U j r S R T Y z o i a k V 7 2 Z u J / X j c 1 4 b W f c Z m k B i V b L A p T Q U x M Z s + T A V f I j J h Y Q p n i 9 l b C R l R R Z m x E J R u C t / z y K m n V q t 5 F t X Z / W a n f 5 X E U 4 Q R O 4 R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>Increasing perturbation rate &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S F A 2 6 H c z K S P D y + k 0   of adversarial attacks on the graph topology and the node attributes by choosing different perburation rates ? and ?, respectively. As shown in Figure <ref type="figure" target="#fig_11">2</ref>, the performance of our model is consistently superior to other competitors, both on average and in worst-case. Note that the strong competitor DGI generates negative samples in the training phase, and this might explain the robustness of the DGI model. Comparably, the high standard deviation of DGI-SVD might be attributed to the continuous low-rank approximation of the adjacency matrix: the output of truncated SVD is no longer a 0-1 matrix, which violates the discrete nature of graph topology.</p><formula xml:id="formula_28">1 A 8 + X i D 1 c p k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V 0 G P B i y e p Y D + g D W W z 2 b R r N 7 t h d y K U 0 v / g x Y M i X v 0 / 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g B j 3 v 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l l G Z p q x J l V C 6 E x L D B J e s i R w F 6 6 S a k S Q U r B 2 O b m Z + + 4 l p w 5 V 8 w H H K g o Q M J I 8 5 J W i l V i 9 i A k m / X P G q 3 h z u K v F z U o E c j X 7 5 q x c p m i V M I h X E m K 7 v p R h M i E Z O B Z u W e p l h K a E j M m B d S y V J m A k m 8 2 u n 7 p l V I j d W 2 p Z E d 6 7 + n p i Q x J h x E t r O h O D Q L H s z 8 T + v m 2 F 8 H U y 4 T D N k k i 4 W x Z l w U b m z 1 9 2 I a 0 Z R j C 0 h V H N 7 q 0 u H R B O K N q C S D c F f f n m V t G p V / 6 J a u 7 + s 1 O / y O I p w A q d w D j 5 c Q R 1 u o Q F N o P A I z / A</formula><formula xml:id="formula_29">1 A 8 + X i D 1 c p k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B I v g q S R V 0 G P B i y e p Y D + g D W W z 2 b R r N 7 t h d y K U 0 v / g x Y M i X v 0 / 3 v w 3 b t s c t P X B w O O 9 G W b m h a n g B j 3 v 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l l G Z p q x J l V C 6 E x L D B J e s i R w F 6 6 S a k S Q U r B 2 O b m Z + + 4 l p w 5 V 8 w H H K g o Q M J I 8 5 J W i l V i 9 i A k m / X P G q 3 h z u K v F z U o E c j X 7 5 q x c p m i V M I h X E m K 7 v p R h M i E Z O B Z u W e p l h K a E j M m B d S y V J m A k m 8 2 u n 7 p l V I j d W 2 p Z E d 6 7 + n p i Q x J h x E t r O h O D Q L H s z 8 T + v m 2 F 8 H U y 4 T D N k k i 4 W x Z l w U b m z 1 9 2 I a 0 Z R j C 0 h V H N 7 q 0 u H R B O K N q C S D c F f f n m V t G p V / 6 J a u 7 + s 1 O / y O I p w A q d w D j 5 c Q R 1 u o Q F N o P A I z / A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance under Other Attack Strategies</head><p>In practice we do not know which kind of attack strategies the malicious users is going to use. Thus it is interesting and important to know the performance of our model across different types of adversarial attacks. We adapt some common attack strategies to the unsupervised setting and use them as baselines. 1) Degree: flip edges based on the sum of the degree centrality of two end nodes; 2) Betw: flip edges based on the sum of the betweenness centrality of two end nodes; 3) Eigen: flip edges based on the sum of the eigenvector centrality of two end nodes; and 4) DW <ref type="bibr" target="#b0">[1]</ref>: a blackbox attack method designed for DeepWalk. We set the size of the sampled candidate set to 20K, as suggested in <ref type="bibr" target="#b0">[1]</ref>.</p><p>This time we consider the node classification task on Polblogs dataset for illustration. This choice is convincing because all the above attack strategies only vary the graph topology, which is the only information we know about Polblogs dataset. Results in Table <ref type="table" target="#tab_3">3</ref> shows the outstanding performance of our model as its superiority persists in three attack strategies out of four. Comparison between Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref> shows that the graph PGD attack via MI is the most effective attack strategy used here. This observation verifies the idea of our model: We learn from the worst adversarial example (i.e., the one deteriorates the performance most).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>Unsupervised graph representation learning. The goal of unsupervised graph representation learning is to learn an encoder that maps the input graph into a low-dimensional representation space. Currently, the most popular algorithms for unsupervised graph representation learning mainly rely on matrix factorization <ref type="bibr" target="#b39">[40]</ref>, random walks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>, and adjacency matrix reconstruction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. One alternative approach that has recently been advocated is that of adopting the MI maximization principle <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. This type of methods have achieved massive gain in standard metrics across unsupervised representation learning on graphs, and is even competitive with supervised learning schemes. However, these MI-based graph embeddings usually do not perform well with noisy or adversarial data. This prompts further consideration of MI-based robust representation learning on graphs.</p><p>Additionally, there exist some unsupervised graph learning models targeting the adversarial vulnerability problem. One line of them tried to denoise the perturbed input based on certain hypothesis of the crafted attacks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref>, such as the fact that attackers tend to link two nodes with different features. Another trend focused on the robustness of a particular model, like Deepwalk <ref type="bibr" target="#b7">[8]</ref> and spectral clustering <ref type="bibr" target="#b2">[3]</ref>. However, most of them can only operate on certain task, but cannot be applied to various downstream tasks. Robust models on graphs. Owing to the surge of adversarial attacks on graphs, several countermeasures have been proposed. Compared with pre/post-processing approaches, which are supported by empirical observations on specific attacks or models (mostly on GNNs), such as input denoising <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>, adversarial detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45]</ref> and certifiable robustness guarantees on GNNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50]</ref>, several recent attempts have been made to formulate the graph defense problem as a minimax adversarial game <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. These approaches often resort to adversarial training for optimization due to its excellent performance; however, they typically require additional label information and are tailored to attacks on GNNs. On the other hand, another line of works that utilize a similar adversarial training strategy, do not depend on label information. For instance, one cheap method of this kind involves randomly dropping edges during adversarial training <ref type="bibr" target="#b6">[7]</ref>. Moreover, in cases where no label information is exposed, there are some existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">46]</ref> that have applied adversarial training to unsupervised representation learning (for example, DeepWalk and autoencoders). However, the robustness of unsupervised representation learning via MI on graphs remains an inherent blind spot.</p><p>The work that most closely resembles ours is that of <ref type="bibr" target="#b47">[48]</ref>, which develops a notion of representation vulnerability based on the worstcase MI in the image domain. However, it cannot address the robustness in the graph domain, mainly caused by the joint input space and the discrete nature of graph topology. Moreover, the adversarial perturbations of edges or node attributes are easy to propagate to other neighbors via the relational information on a graph, which makes the robustness even harder to enhance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we study unsupervised adversarially robust representation learning on graphs. We propose the graph representation vulnerability (GRV) to quantify the robustness of an unsupervised graph encoder. Then we formulate an optimization problem to study the trade-off between the expressive power of the encoder and its robustness to adversarial attacks. After that we propose an approximate solution which relies on a reduced empirical search space. We further build sound theoretical connections between GRV and one example downstream task, node classification. Extensive experimental results demonstrate the effectiveness of our method on blocking perturbations on input graphs, sregardless of the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Notations Notation Description</head><p>G, A, X The input graph, the adjacency matrix and the node attribute matrix of G ?, ?</p><p>The random variable representing structural information and its realization ? , ?</p><p>The random variable representing attributes and its realization ?, ?</p><p>The random variable (?, ? ) and its realization (?, ?) A, X, S, Z, Y The input space w.r.t graph topology, node attributes, their joint, representations and labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>The probability distribution of ? ? ? ?</p><p>The adversarial probability distribution of ? ? ? (?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The empirical distribution of ? ? (?)</p><formula xml:id="formula_30">? ?</formula><p>The adversarial empirical distribution of ? ? ?, ? , ?</p><p>The encoder function, the classifier function and their composition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>We conduct all experiments on a single machine of Linux system with an Intel Xeon E5 (252GB memory) and a NVIDIA TITAN GPU (12GB memory). All models are implemented in PyTorch 1 version 1.4.0 with CUDA version 10.0 and Python 3.7. Implementations of our model. We train our proposed model using the Adam optimizer with a learning rate of 1e-3 and adopt early stopping with a patience of 20 epochs. We choose the onelayer GNN as our encoder and set the dimension of its last layer as 512. The weights are initialized via Xavier initialization.</p><p>In the training phase, the step size of the graph PGD attack is set to be 20 and the step size of PGD attack is set to be 1e-5. The iteration numbers of both attackers are set to be 10. In the testing phase, the step size of PGD attack is set to 1e-3. The iteration numbers are set to 50 for both attacks. Others attacker parameters are the same as that in the training phase. When evaluating the learned representations via the logistic regression classifier, we set its learning rate as 1e-2 and train 100 epochs. Implementations of baselines. For all the baselines, we directly adopt their implementations and keep all the hyperparameters as the default values in most cases. Specifically, for GAE, we adopt their graph variational autoencoder version. Since RSC and DGI-SVD are models designed for noisy graphs, we grid search their most important hyperparameters when adopting RSC on benign examples. For RSC on benign examples, the number of clusters is 128, 200, and 100 on Cora, Citeseer, and Polblogs, respectively. For DGI-SVD on benign examples, the rank is set to be 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Results</head><p>Empirical Connection between GRV and AG In ?4, we established a theoretical connection between the graph representation vulnerability (GRV) and adversarial gap (AG) under different assumptions. Here, we also conduct experiments to corroborate   The performance of our model against the perturbations performed on a single input space A or X compared with that on the joint input space under increasing attack rate.</p><formula xml:id="formula_31">Q Z B F 4 s G K c c 6 x r M U c J 9 J o J p P D C F U M n M r p i M i C d U m q 6 I J w V 1 + e Z W 0 a 1 X 3 s l p 7 u C r X b / M 4 C u g E n a I K c t E 1 q q N 7 1 E A t R F G K n t E</formula><formula xml:id="formula_32">Z M W q U Y = " &gt; A A A B + n i c b V B N S N A E N U r q / U j C R a h H i x J F f R Y C K e K v Q L m h A m d L M J u x u l x P U L x U e o v e a / c d v m o K P B h v z T A z L g Y l c q v C v r G l Z x u S z u d / Y J Y P O z J O B Y E i V k s e g G W w C i H t q K K Q S R g K O A Q T c Y z g M I S W P e U p M E v A g P O Q p w U p L v l k + d E x P t P q r B A q f + W b F r t l z W K v E y U k F W j p c i E k a A V e E Y S n j p o L N C U c J g W n J T C Q k m Y z y E v q Y c R y C b H D r V y s A K Y G L K u u / p I c C T l J A p Z T V S C M / E / r + q N r L K E S B Z w s F o U p s R s z X K w B l Q A U W y i C S a C l s t M s I C E X T K u k Q n O W X V m n X n M u a v X y r j L o + j i I R C a o i B h B r p F T d R G B D i Z / S K o w n V N z W r Q U j n z l C f B / g A u f Z N T &lt; / l a t e x i t &gt;</formula><p>whether a similar connection still holds in more complicated scenarios. Again, we take the node classification task on the Cora dataset as our illustrative example. We compare some metrics of three kinds of encoders: GNNs of which the last layers have dimensions 512, 384, and 256, respectively. The left of Figure <ref type="figure">3</ref> presents a positive correlation between the adversarial gap and the value of GRV. This numerical results shows that GRV is indeed a good indicator for the robustness of graph representations. Finally, as a supplementary experiment, the right of Figure <ref type="figure">3</ref> plots the prediction accuracy under polluted data versus our approximation of the objective function ? 2 (?) = I(?; ? (?)) -GRV ? (?) (with ? = 1). The figure shows a positive correlation between these two quantities, which verify the use of ? 2 (?) to enhance the adversarial accuracy. Sensitivity of the budget hyperparameters. The budget hyperparameters ? and ? determine the number of changes made to the original graph topology and node attributes respectively when finding the worst-case adversarial distribution, and are thus important hyperparameters in our proposed model. We use grid search to find their suitable values in our model through numerical experiments on Cora in Figure <ref type="figure">4</ref>. Better performance can be obtained when ? = 0.3 and ? = 0.15. We further observe that when ? and ? are small, the budgets are not sufficient enough to find the worst-case adversarial distribution; while when ? and ? are big, introducing too much adversarial attack will also lead to a decrease in the performance to some extent. Defending perturbations on the joint input space is more challenging. In all the above-mentioned experiments, we further evaluate our model's robustness against the perturbations performed on the joint space (A, X). Here, we consider the perturbations performed on a single space A or X (i.e., Ours-A/X: our model against perturbations performed on A or X) under increasing attack rate as the setting of ? 5.1, and present their results in Figure <ref type="figure">5</ref>. We can see that when facing with the perturbations on the joint input space, the performance drops even more. This indicates that defending perturbations on the joint input space is more challenging, that is why we focus on the model robustness against perturbations on the joint input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proofs of Theorems</head><p>We only show the proof details of Theorem 4.1. Theorem 4.2 follows in a similar way. Proof of Theorem 4.1. For simplicity, we denote ? := |? |. Let ? ? be the random variable following the Binomial distribution according to ? (i.e., ? ? = ? ?=1 ? ? ? ?(?, 0.5 + ?(? -0.5))) and its realization ? ? = ? ?=1 ? ? . Let ? be the random variable following the Gaussian distribution according to A and X (i.e., ? = ? ? ? ? N (0, ? ? ? 2 ? )), then its realization ? = ? ? ? is exactly the aggregation operator of GNNs. We first compute the explicit formulation of the representation vulnerability GRV ? (?). Note that I (? ; ? ) = ? (? ) -? (? |? ) = ? (? ) -? (? |? ). For any given ? ? E, we have because ? (? (?)|?) = 0, and the distribution of ? = ? ? ?, informally defined as ? ? 0.5N (0, ? +1 ? 2 ? ) + 0.5N (0, ? -1 ? 2 ? )), is symmetric w.r.t. 0. We thus have, ? ? (? (?)) = -? ??? ? (?? ? 0) log ? ??? ? (?? ? 0) -? ??? ? (?? &lt; 0) log ? ??? ? (?? &lt; 0) = ? ? (0.5). We note that the binary entropy function ? ? (? ) = -? log(? )-(1-? ) log(1-? ) is concave on (0, 1) and that the maximum of ? ? is attained uniquely at ? = 0.5. To obtain the infimum of ? ? (? (? ? )), we should either maximize or minimize ? ? ? ?? ? ? (? ? ? ? 0). Bound of ? ? ? ?? ? ? (? ? ? ? 0). Compute AG ? . Given the formulation of GRV ? , we further aim to establish its connection to AG ? . Here we induce the detailed formulation of AG ? . In our case, the only two classifiers to be discussed are ? 1 (?) = ? and ? 2 (?) = -?.</p><p>For given ? ? E, we have AdvRisk ? (? 1 ? ?) as: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>H S 3 D h B p p w D y 3 oA I U E n u E V 3 l C K X t A 7 + l i O l l C x c w p / g D 5 / A P c D k b A = &lt; / l a t e x i t &gt; Y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y i b i o A h X q o g h U i r 8 3 M p 9 J n o v U n Q = " &gt; A A A B 8 3 i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a q o M u C G 5 c V + p L O U D J p p g 3 N Z I Y k I 5 S h v + H G h S J u / R l 3 / o 2 Z d h b a e i B w O O d e 7 s k J E s G 1 c Z x v V N r Y 3 N r e K e 9 W 9 v Y P D o + q x y d d H a e K s g 6 N R a z 6 A d F M c M k 6 h h v B + o l i J A o E 6 w X T u 9 z v P T G l e S z b Z p Y w P y J j y U N O i b G S 5 0 X E T C g R 2 e M c D 6 s 1 p + 4 s g N e J W 5 A a F G g N q 1 / e K K Z p x K S h g m g 9 c J 3 E + B l R h l P B 5 h U v 1 S w h d E r G b G C p J B H T f r b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>H S 3 D h B p p w D y 3 oA I U E n u E V 3 l C K X t A 7 + l i O l l C x c w p / g D 5 / A P V 9 k a 8 = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " M q d b b 7 W p X z Z t Y Q e z q v O p m W 6 2 U Z c = " &gt; A A A C D 3 i c b V D L S s N A F J 3 4 r P U V d e l m s C i u S l I F x V X B j c u K f W E T y m Q 6 b Y d O Z s L M R C k h f + D G X 3 H j Q h G 3 b t 3 5 N 0 7 a g N p 6 4 M L h n H u 5 9 5 4 g Y l R p x / m y F h a X l l d W C 2 v F 9 Y 3 N r W 1 7 Z 7 e p R C w x a W D B h G w H S B F G O W l o q h l p R 5 K g M G C k F Y w u M 7 9 1 R 6 S i g t f 1 O C J + i A a c 9 i l G 2 k h d + 4 h c Q C 9 E e o g RS 2 5 S 6 E k 6 G G o k p b j / 0 W / T r l 1 y y s 4 E c J 6 4 O S m B H L W u / e n 1 B I 5 D w j V m S K m O 6 0 T a T 5 D U F D O S F r 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of a graph pre-training pipeline under adversarial attacks. If the graph encoder is vulnerable to the attacks, the adversarial risk would propagate to every downstream task via the perturbed graph representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Topology-aware: given ? ? ? , ? (? |?, ? ) = ? (? |?) ? Attribute-aware: given ? ? ? , ? (? |?, ? ) = ? (? |? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>u.a.r. ? ? {-1, +1} and ? ? i.i.d. ? Bernoulli(0.5 + ? ? (? -0.5)) hold, but ? ? = 1 ? , ? = 1, . . . , |? | and the set of encoders follows such that E = {? : (?, ?) ? ? sgn[(? ? ? -0.5|? |1 ? ? )?] | ??? 2 = 1}, which can be regarded as the non-attribute case. Then, given ? ? 0, for any ? ? E, we have 1 -? ? (0.5 -0.5AG ? (? * ? ?)) ? GRV ? (?) ? 1 -? ? (0.5-AG ? (? * ? ?)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc>? {-1, +1}, ? ? ?.?.?. ? Bernoulli(0.5) and ? ? ?.?.?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>w 8 u I I 6 3 E I D m s B A w D O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 9 R u 5 A w &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>w 8 u I I 6 3 E I D m s B A w D O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 9 R u 5 A w &lt; / l a t e x i t &gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>K b 4 5 y X p x 3 5 2 P R W n D y m W P 4 A + f z B 5 Z 2 j y w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S F A 2 6 H c z K S P D y + k 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of different models under various perturbation rates ? and ?. The downstream task is node classification and we use the Cora dataset for illustration. The shaded area indicates the standard deviation (?0.1) over 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>1 https://github.com/pytorch/pytorch Natural ACC-Adversarial ACC (%) t e x i t s h a 1 _ b a s e 6 4 = " F p b 2 f 6 / U R q v D J d o 2 X P m 3 P z Z EP 2 o = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 L B a h H i x J F f R Y 9 O K x Q r + g C W G z n b Z L N 5 u w u x F q 6 C / x 4 k E R r / 4 U b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w o Q z p R 3 n 2 1 p b 3 9 j c 2 i 7 s F H f 3 9 g 9 K 9 u F R W 8 W p p N C i M Y 9 l N y Q K O B P Q 0 k x z 6 C Y S S B R y 6 I T j u 5 n f e Q S p W C y a e p K A H 5 G h Y A N G i T Z S Y J c u P O A 8 q F W 8 5 g g 0 O Q / s s l N 1 5 s C r x M 1 J G e V o B P a X 1 4 9 p G o H Q l B Ol e q 6 T a D 8 j U j P K Y V r 0 U g U J o W M y h J 6 h g k S g / G x + + B S f G a W P B 7 E 0 J T S e q 7 8 n M h I p N Y l C 0 x k R P V L L 3 k z 8 z + u l e n D j Z 0 w k q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>r e r O e r B f r 3 f p Y t K 5 Z + c w x + g P r 8 w d c p Z I / &lt; / l a t e x i t &gt; Adversarial Acc Approximated `2(?) &lt; l a t e x i t s h a _ b a s e = " h m M M b d F S e l k K H L y I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Left. Connection between GRV and AG. Right. Connection between adversarial accuracy and our approximation of ? 2 (?). Filled points, half-filled points, and unfilled points indicate our models with ? = 0.4, ? = 0.1, our model with ? = 0.1, ? = 0.025, and the DGI model, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>GRV</head><label></label><figDesc>? (?) = I (?; ? (?))inf ? ? ?? ? ? ?B? (? ? ? ? ,? ) I ? ? ; ? ? ? = ? ? (0.5)inf ? ? ? ?B? (? ? ,? )? ? (? (? ? )),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>?</head><label></label><figDesc>:=AdvRisk ? (? 1 ? ?) = ? (?,?) ?? ?? [? ? ? ? B (?, ?), s.t. sgn(? ? ?) ? ? ] =? (?,?) ?? ?? [ min ? ? ?B (?,? ) ? ? ? ? ? ? 0] =? (?,?) ?? ?? [? ? ?? ?min ???B (0,? ) ? ? ???].Because |???| ? ? ??? ? ,min ?? ?B (0,?) ? ? ??? = ? ??? ? holds for any ?. We have? = 1 2 ? ??N (0,? + ? 2 ? ),? + ?? (?,? ) (?? ? ? ?? ? ? ) + 1 2 ? ??N (0,? -? 2 ? ),? + ?? (?,?) (?? ? -? ? 1/2 + ? 2 /2.We also have AdvRisk ?=0 (?1 ? ?) as ? := AdvRisk ?=0 (? 1 ? ?) = ? (?,?)?? ?? [? ? ?? ? 0] = 1/2. Thus, AG ? (? 1 ? ?) = ? -? = ? 2 /2. Similarly, for given ? ? E, we have AdvRisk ? (? 2 ? ?) as ? := AdvRisk ? (? 2 ??) ? 1/2 + ? 2 /2. We also have AdvRisk ?=0 (? 2 ??) = 1/2. We can get AG ? (? 2 ? ?) = ? -? = ? 2 /2.As a result, we have AG ? (?1 ? ?) = AG ? (? 2 ? ?) = ? 2 /2.Connection between GRV and AG. Now we aim to find the connection between AG ? and GRV ? . Given their formulations derived above, It is easy to show that? 1 + ? 2 = 1 is equivalent to 1/2 -? 1 /2 = ? 2 /2 and |? 1 /2 -1/2| = |? 2 /2|. Then we have, GRV ? (?) = ? ? (1/2) -? ? (? 2 /2 + 1/2) = ? ? (1/2) -? ? (1/2 + AG ? (? * ? ?)),which completes the proof. Simpler case. Consider a simpler case that ? u.a.r. ? ? {-1, +1} and ? ? i.i.d. ? Bernoulli(0.5 +? ? (? -0.5)) hold but ? ? = 1 ? , ? = 1, 2, . . . ?, and the set of encoders follows that E = {? : (?, ?) ? ? sgn[(? ? ? -0.5?1 ? ? )?] | ??? 2 = 1}. This simpler case removes the randomness in ?. We let ? ? = ? ? ? -0.5?1 ? ? ? (? ? -0.5?) ? , then its realization ? = ? ? ? -0.5?1 ? ? . We again use the De Moivre-Laplace Central</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>we opt to define the adversarial distribution w.r.t ? ? ? ? instead of that w.r.t ? ? and ? ? respectively. This assumption is reasonable owing to our focus on the robustness of our model rather than the real attack strategies. Accordingly, we assume that the set of adversarial distributions is B ? (? ? ? ? , ?) = {? ? ?? ? ? ? M (H ) : ? ? (? ? ? ? , ? ? ?? ? ? ) ? ? } where H = {? ? ? : ?? ? A, ? ? X} in the following two theorems.In Theorems 4.1 and 4.2, we use ? ? {0, 1} |V |</figDesc><table><row><cell></cell><cell></cell><cell>Input space</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>space</cell></row><row><cell>AG ? (?)</cell><cell>Image</cell><cell>Single X</cell><cell>Y</cell></row><row><cell>AG ? (?)</cell><cell>Graph</cell><cell>Joint (A, X)</cell><cell>Y</cell></row><row><cell>RV ? (?)</cell><cell>Image</cell><cell>Single X</cell><cell>Z</cell></row><row><cell>GRV ? (?)</cell><cell>Graph</cell><cell>Joint (A, X)</cell><cell>Z</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of results for the node classification, link prediction and community detection tasks using polluted data.</figDesc><table><row><cell></cell><cell cols="3">Node classification (Acc%)</cell><cell cols="3">Link prediction (AUC%)</cell><cell>Community detection (NMI%)</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cora</cell><cell cols="2">Citeseer Polblogs</cell><cell>Cora</cell><cell cols="2">Citeseer Polblogs</cell><cell>Cora</cell><cell>Citeseer Polblogs</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Raw</cell><cell cols="6">57.4?3.0 49.7?1.6 73.9?0.9 60.5?0.1 50.2?0.5 89.0?0.4</cell><cell>9.7?7.5</cell><cell>1.0?0.5</cell><cell>0.2?0.1</cell></row><row><cell>DeepWalk</cell><cell cols="7">56.2?1.1 16.5?0.9 80.4?0.5 55.4?0.8 50.3?0.3 89.2?0.7 34.6?0.6 11.1?1.0</cell><cell>0.4?0.5</cell></row><row><cell>DeepWalk + X</cell><cell cols="2">59.3?0.4 26.5?0.5</cell><cell>-</cell><cell cols="2">55.9?0.6 50.9?0.3</cell><cell>-</cell><cell>34.2?3.7 11.1?1.3</cell><cell>-</cell></row><row><cell>GAE</cell><cell cols="7">14.0?1.2 16.2?1.1 49.9?1.2 52.4?1.4 50.9?1.8 50.5?1.3 10.9?2.1</cell><cell>1.4?1.7</cell><cell>9.2?1.0</cell></row><row><cell>DGI</cell><cell cols="7">69.3?2.8 53.2?2.2 75.2?2.4 68.6?0.4 57.6?2.1 91.2?1.1 30.3?3.5</cell><cell>8.5?3.8</cell><cell>6.0?5.6</cell></row><row><cell>Dwns_AdvT</cell><cell cols="7">59.2?1.2 25.0?1.0 80.7?0.5 56.0?0.7 50.7?0.4 89.5?0.8 35.0?0.7 11.5?1.0</cell><cell>0.9?0.7</cell></row><row><cell>RSC</cell><cell cols="6">46.9?3.5 34.0?2.2 58.9?1.7 52.5?0.4 57.2?0.2 61.5?0.4</cell><cell>4.9?0.7</cell><cell>1.8?0.4</cell><cell>4.4?4.3</cell></row><row><cell cols="8">DGI-EdgeDrop 56.0?4.3 49.0?4.5 79.8?1.7 66.2?0.8 61.3?0.9 89.3?1.6 30.1?6.8 7.34?0.8</cell><cell>9.0?7.8</cell></row><row><cell>DGI-Jaccard</cell><cell cols="7">69.4?2.8 57.1?1.3 79.3?0.8 63.8?0.8 57.6?1.0 84.7?0.9 16.4?1.1</cell><cell>6.1?0.6</cell><cell>12.9?0.0</cell></row><row><cell>DGI-SVD</cell><cell cols="7">68.1?8.0 56.1?16.4 81.6?0.7 60.1?0.8 54.7?1.3 85.2?0.7 16.2?0.9</cell><cell>6.5?0.8</cell><cell>13.0?0.0</cell></row><row><cell>Ours-soft</cell><cell cols="7">69.4?0.7 57.5?2.0 79.7?2.1 68.1?0.3 58.2?1.3 90.3?0.5 39.2?8.8 23.5?1.9 12.6?9.6</cell></row><row><cell>Ours</cell><cell cols="7">70.7?0.9 58.4?1.4 82.7?2.2 69.2?0.4 59.8?1.3 91.8?0.4 41.4?4.7 23.6?2.8 14.8?2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Defense against different attackers on Polblogs for the node classification task.</figDesc><table><row><cell>Model</cell><cell>Attacker</cell><cell>Degree</cell><cell>Betw</cell><cell>Eigen</cell><cell>DW</cell></row><row><cell>Raw</cell><cell></cell><cell cols="4">87.4?0.3 84.1?0.8 86.4?0.6 87.9?0.4</cell></row><row><cell cols="2">DeepWalk</cell><cell cols="4">87.8?0.9 83.5?1.2 84.3?1.0 87.7?0.9</cell></row><row><cell cols="2">DeepWalk + X</cell><cell cols="4">85.8?2.7 82.7?2.1 85.0?1.1 88.3?0.9</cell></row><row><cell>GAE</cell><cell></cell><cell cols="4">83.7?0.9 81.0?1.6 81.5?1.4 85.4?1.1</cell></row><row><cell>DGI</cell><cell></cell><cell cols="4">86.6?1.1 84.8?1.2 84.8?1.0 86.4?1.1</cell></row><row><cell cols="2">Dwns_AdvT</cell><cell cols="4">88.0?1.0 84.1?1.3 84.6?1.0 88.0?0.8</cell></row><row><cell>RSC</cell><cell></cell><cell cols="4">52.1?1.3 51.9?0.7 51.4?0.5 52.6?1.1</cell></row><row><cell cols="6">DGI-EdgeDrop 87.1?0.3 87.0?0.6 80.5?0.5 86.3?0.3</cell></row><row><cell cols="2">DGI-Jaccard</cell><cell cols="4">82.1?0.3 80.7?0.4 80.6?0.3 82.2?0.2</cell></row><row><cell cols="2">DGI-SVD</cell><cell cols="4">86.5?0.2 85.6?0.2 86.1?0.2 85.3?0.3</cell></row><row><cell cols="2">Ours-soft</cell><cell cols="4">88.5?0.7 85.7?1.5 86.2?0.4 88.7?0.7</cell></row><row><cell>Ours</cell><cell></cell><cell cols="4">89.3?0.7 86.3?1.2 86.7?0.4 89.0?0.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>To achieve the bound of ? ? ? ?? ? ? (? ? ? ? 0), we first consider the bound of |???| where ?? = ? ? -? = (? +??) ? (? +??) -? ? ?. According to ? ? ? ? B ? (? ? , ?), we can get ??? ? ? ? ? holds almost surely w.r.t. the randomness of ? and the transport map defined by ?-Wasserstein distance. Then, according to the H?lder's inequality, we have |???| ? ???? ? ??? ? ? ? ??? ? , which indicates ? ??? ? (|???| ? ? ??? ? ) ? 1. We have, Compute GRV ? . Next, we will induce the more detailed formulations of the two bounds above. The lower bound is? =? ??N (0,?? 2 ? ),? ?? ? (?? -? ?? ? ? ? 0) =? ? ?N (0,1) ? ? ?? ? (? ? ? ?? ? ? / ? ?? ?? ? 2 ).Then according to De Moivre-Laplace Central Limit Theorem, we use Gaussian distribution to approximate Binomial distribution ? ? ( e.g., ? + ? N (??, ???) where ? = 1 -?). We have, ? ? ?N (0,1) [? ? + ?? (?,? ) (?? ? + ? ?? ? 2 ? ? ?? ? ? ) + ? ? -?? (?,1-? ) (? ? ? -? ?? ? 2 ? ? ?? ? ? ) ] ? 1 2 ? ? ?N (0,1),? &gt;0 [? ? ?N (0,1) (? ? ?-/2 (0 ? ? 1 ? 1). ? 2 ??? 2 ? /? 2 ? 2 ??? 2 2 ? ???. Similarly, we have ? ? 1/2+? 2 /2 (0 ? ? 2 ? 1). Thus, GRV ? (?) = ? ? (1/2) -? ? (max{ |? 1 /2 -1/2 |, |? 2 /2| } + 1/2).</figDesc><table><row><cell>? =</cell><cell>1 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>??</cell><cell>?? ?</cell><cell>) + ? ? ?N (0,1) (? ? ?-??</cell><cell>? ??</cell><cell>)</cell></row><row><cell cols="2">=:? 1 where ? =</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>? ??? ? (?? -? ?? ? ? ? 0) ? ? ? ? ? ?? ? ? (? ? ? ? 0) ? ? ??? ? (?? + ? ?? ? ? ? 0 ? ).</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Limit Theorem to approximate ? ? ,</p><p>We notice that ??-?/2+??-?/2 = 0 and ? ? satisfies the conditions of Gaussian Mixture model in <ref type="bibr" target="#b47">[48]</ref>. Thus, we can directly reuse <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">Theorem 3.4</ref>] by replacing ? * with ?? -?/2 and ? * with diag ? (???), which completes the proof. ?</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Certifiable robustness to graph perturbations</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ?-wasserstein distance: Local solutions and existence of optimal transport maps</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><forename type="middle">De</forename><surname>Pascale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petri</forename><surname>Juutinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Can adversarial network attack be defended</title>
		<author>
			<persName><forename type="first">Jinyin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05994</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05730</idno>
		<title level="m">A survey of adversarial learning on graph</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial training methods for network embedding</title>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">All you need is low (rank) defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">Negin</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient and Robust Automated Machine Learning</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial attacks and defenses in images, graphs and text: A review</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Hao-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu Debayan Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Tang Anil</surname></persName>
		</author>
		<author>
			<persName><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJAC</title>
		<imprint>
			<biblScope unit="page" from="151" to="178" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Vassilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Berberidis</surname></persName>
		</author>
		<author>
			<persName><surname>Giannakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09589</idno>
		<title level="m">Graph-SAC: Detecting anomalies in large-scale graphs</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Certified Robustness of Graph Convolution Networks for Graph Classification under Topological Attacks</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hongwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Peruri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
	<note>Xianfeng Tang, Suhang Wang, and Jiliang Tang</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational graph auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motifs in temporal networks</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial Training Methods for Network Embedding</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">DropEdge: Towards Deep Graph Convolutional Networks on Node Classification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A boundary tilting persepective on the phenomenon of adversarial examples</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04429</idno>
		<title level="m">GraphDefense: Towards robust graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial examples on graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Characterizing malicious edges targeting on graph neural networks</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Gunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenReview</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep network representations with adversarially regularized autoencoders</title>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08573</idno>
		<title level="m">Theoretically principled trade-off between robustness and accuracy</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="7472" to="7482" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning adversarially robust representations via Worst-Case mutual information maximization</title>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11609" to="11618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Certifiable robustness and robust training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="246" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
