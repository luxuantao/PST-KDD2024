<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-16">January 16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Foivos</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data61</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<region>Floreat WA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<addrLine>1 2 3 5 10 11 12 13</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Caccetta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data61</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<region>Floreat WA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CSIRO Agriculture &amp; Food</orgName>
								<address>
									<settlement>St Lucia</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<addrLine>1 2 3 5 10 11 12 13</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ICRAR</orgName>
								<orgName type="institution" key="instit2">The University of Western Australia</orgName>
								<address>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<addrLine>1 2 3 5 10 11 12 13</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Conv2d</surname></persName>
							<affiliation key="aff3">
								<address>
									<addrLine>1 2 3 5 10 11 12 13</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-16">January 16, 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1904.00592v3[cs.CV]</idno>
					<note type="submission">Preprint submitted to -</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural network</term>
					<term>loss function</term>
					<term>architecture</term>
					<term>data augmentation</term>
					<term>very high spatial resolution ResBlock-a (32</term>
					<term>d={1</term>
					<term>3</term>
					<term>15</term>
					<term>31}) ResBlock-a (64</term>
					<term>d={1</term>
					<term>3</term>
					<term>15</term>
					<term>31}) ResBlock-a (128</term>
					<term>d={1</term>
					<term>3</term>
					<term>15}) ResBlock-a (256</term>
					<term>d={1</term>
					<term>3</term>
					<term>15}) ResBlock-a (512</term>
					<term>d={1}) ResBlock-a (32</term>
					<term>d={1</term>
					<term>3</term>
					<term>15</term>
					<term>31}) ResBlock-a (64</term>
					<term>d={1</term>
					<term>3</term>
					<term>15</term>
					<term>31}) ResBlock-a (128</term>
					<term>d={1</term>
					<term>3</term>
					<term>15}) ResBlock-a (256</term>
					<term>d={1</term>
					<term>3</term>
					<term>15}) ResBlock-a (512</term>
					<term>d={1})</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes. The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9% over all classes for our best model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic labelling of very high resolution (VHR) remotelysensed images, i.e., the task of assigning a category to every pixel in an image, is of great interest for a wide range of urban applications including land-use planning, infrastructure management, as well as urban sprawl detection <ref type="bibr" target="#b49">(Matikainen and Karila, 2011;</ref><ref type="bibr" target="#b79">Zhang and Seto, 2011;</ref><ref type="bibr" target="#b45">Lu et al., 2017;</ref><ref type="bibr" target="#b20">Goldblatt et al., 2018)</ref>. Labelling tasks generally focus on extracting one specific category, e.g., building, road, or certain vegetation types <ref type="bibr" target="#b37">(Li et al., 2015;</ref><ref type="bibr" target="#b13">Cheng et al., 2017;</ref><ref type="bibr" target="#b73">Wen et al., 2017)</ref>, or multiple classes all together <ref type="bibr" target="#b55">(Paisitkriangkrai et al., 2016;</ref><ref type="bibr" target="#b35">Längkvist et al., 2016;</ref><ref type="bibr" target="#b41">Liu et al., 2018;</ref><ref type="bibr" target="#b47">Marmanis et al., 2018)</ref>.</p><p>Extracting spatially consistent information in urban environments from remotely-sensed imagery remains particularly challenging for two main reasons. First, urban classes often display a high within-class variability and a low between-class variability. On the one hand, man-made objects of the same semantic class are often built in different materials and with different structures, leading to an incredible diversity of colors, sizes, shapes, and textures. On the other hand, semantically-different man-made objects can present similar characteristics, e.g., cement rooftops, cement sidewalks, and cement roads. There-1 foivos.diakogiannis@data61.csiro.au fore, objects with similar spectral signatures can belong to completely different classes. Second, the intricate three-dimensional structure of urban environments is favourable to interactions between these objects, e.g., through occlusions and cast shadows.</p><p>Circumventing these issues requires going beyond the sole use of spectral information and including geometric elements of the urban class appearance such as pattern, shape, size, context, and orientation. Nonetheless, pixel-based classifications still fail to satisfy the accuracy requirements because they are affected by the salt-and-pepper effect and cannot fully exploit the rich information content of VHR data <ref type="bibr" target="#b52">(Myint et al., 2011;</ref><ref type="bibr" target="#b39">Li and Shao, 2014)</ref>. GEographic Object-Based Imagery Analysis (GEOBIA) is an alternative image processing approach that seeks to group pixels into meaningful objects based on specified parameters <ref type="bibr" target="#b8">(Blaschke et al., 2014)</ref>. Popular image segmentation algorithm in remote sensing include watershed segmentation <ref type="bibr" target="#b70">(Vincent and Soille, 1991)</ref>, multi-resolution segmentation <ref type="bibr" target="#b5">(Baatz and Schäpe, 2000)</ref> and mean-shift segmentation <ref type="bibr" target="#b14">(Comaniciu and Meer, 2002)</ref>. In addition, GEOBIA also allows to compute additional attributes related to the texture, context, and shape of the objects, which can then be added to the classification feature set. However, there is no universallyaccepted method to identify the segmentation parameters that provide optimal pixel grouping, which implies the GEOBIA is still highly interactive and includes subjective trial-and-error methods and arbitrary decisions. Furthermore, image segmentation might fail to simultaneously address the wide range of object sizes that one typically encounters in urban landscapes ranging from finely structure objects such as cars and trees to larger objects such as buildings. Another drawback is that GEO-BIA relies on pre-selected features for which the maximum attainable accuracy is a priori unknown. While several methods have been devised to extract and select features, these methods are not themselves learned from the data, and are thus potentially sub-optimal.</p><p>In recent years, deep learning methods and Convolutional Neural Networks (CNNs) in particular <ref type="bibr" target="#b36">(LeCun et al., 1989)</ref> have surpassed traditional methods in various computer vision tasks, such as object detection, semantic, and instance segmentation (see <ref type="bibr" target="#b61">Rawat and Wang, 2017</ref>, for a comprehensive review). Some of the key advantages of CNN-based algorithms is that they provide end-to-end solutions, that require minimal feature engineering which offer greater generalization capabilities. They also perform object-based classification, i.e., they take into account features that characterize entire image objects, thereby reducing the salt-and-pepper effect that affects conventional classifiers.</p><p>Our approach to annotate image pixels with class labels is object-based, that is, the algorithm extracts characteristic features from whole (or parts of) objects that exist in images such as cars, trees, or corners of buildings and assigns a vector of class probabilities to each pixel. In contrast, using standard classifiers such as random forests, the probability of each class per pixel is based on features inherent in the spectral signature only. Features based on spectral signatures contain less information than features based on objects. For example, looking at a car we understand not only it's spectral features (color) but also how these vary as well as the extent these occupy in an image. In addition, we understand that it is more probable a car to be surrounded by pixels belonging to a road, and less probable to be surrounded by pixels belonging to buildings. In the field of computer vision, there is a vast literature on various modules used in convolutional neural networks that make use of this idea of "per object classification". These modules, such as atrous convolutions <ref type="bibr" target="#b10">(Chen et al., 2016)</ref> and pyramid pooling <ref type="bibr" target="#b26">(He et al., 2014;</ref><ref type="bibr" target="#b81">Zhao et al., 2017a)</ref>, boost the algorithmic performance on semantic segmentation tasks. In addition, after the residual networks era <ref type="bibr" target="#b27">(He et al., 2015)</ref> it is now possible to train deeper neural networks avoiding to a great extent the problem of vanishing (or exploding) gradients.</p><p>Here, we introduce a novel Fully Convolutional Network (FCN) for semantic segmentation, termed ResUNet-a . This network combines ideas distilled from computer vision applications of deep learning, and demonstrates competitive performance. In addition, we describe a modeling framework consisting of a new loss function that behaves well for semantic segmentation problems with class imbalance as well as for regression problems. In summary, the main contributions of this paper are the following:</p><p>1. A novel architecture for understanding and labeling very high resolution images for the task of semantic segmen-tation. The architecture uses a UNet <ref type="bibr" target="#b62">(Ronneberger et al., 2015)</ref> encoder/decoder backbone, in combination with, residual connections <ref type="bibr" target="#b28">(He et al., 2016)</ref>, atrous convolutions <ref type="bibr" target="#b10">(Chen et al., 2016</ref><ref type="bibr" target="#b11">(Chen et al., , 2017))</ref>, pyramid scene parsing pooling <ref type="bibr" target="#b81">(Zhao et al., 2017a)</ref> and multi tasking inference <ref type="bibr" target="#b63">(Ruder, 2017,</ref> we present two variants of the basic architecture, a single task and a multi-task one). 2. We analyze the performance of various flavours of the Dice coefficient for semantic segmentation. Based on our findings, we introduce a variant of the Dice loss function that speeds up the convergence of semantic segmentation tasks and improves performance. Our results indicate that the new loss function behaves well even when there is a large class imbalance. This loss can also be used for continuous variables when the target domain of values is in the range [0,1].</p><p>In addition, we also present a data augmentation methodology, where the input is viewed in multiple scales during training by the algorithm, that improves performance and avoids overfitting. The performance of ResUNet-a was tested using the Potsdam data set made available through the ISPRS competition (ISPRS). Validation results show that ResUNet-a achieves state-of-the-art results. This article is organized as follows. In section 2 we provide a short review of related work on the topic of semantic segmentation focused on the field of remote sensing. In section 3, we detail the model architecture and the modeling framework. Section 4 describes the data set we used for training our algorithm. In section 5 we provide an experimental analysis that justifies the design choices for our modeling framework. Finally, section 6 presents the performance evaluation of our algorithm and comparison with other published results. Readers are referred to sections Appendix A for a description of our software implementation and hardware configurations, and to section Appendix C for the full error maps on unseen test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of semantic segmentation has attracted significant interest in the latest years, not only in the field of computer vision community but also in other disciplines (e.g. biomedical imaging, remote sensing) where automated annotation of images is an important process. In particular, specialized techniques have been developed over different disciplines, since there are task-specific peculiarities that the community of computer vision does not have to address <ref type="bibr">(and vice versa)</ref>.</p><p>Starting from the computer vision community, when first introduced, Fully Convolutional Networks (hereafter FCN) for semantic segmentation <ref type="bibr" target="#b44">(Long et al., 2014)</ref>, improved the state of the art by a significant margin (20% relative improvement over the state of the art on the PASCAL VOC <ref type="bibr" target="#b19">(Everingham et al., 2010)</ref> 2011 and 2012 test sets). The authors replaced the last fully connected layers with convolutional layers. The original resolution was achieved with a combination of upsampling and skip connections. Additional improvements have been presented with the use of deeplab models <ref type="bibr" target="#b10">(Chen et al., 2016</ref><ref type="bibr" target="#b11">(Chen et al., , 2017))</ref>, that first showcased the importance of atrous convolutions for the task of semantic segmentation. Their model uses also a conditioned random field as a post processing step in order to refine the final segmentation. A significant contribution in the field of computer vision came from the community of biomedical imaging and in particular, the U-Net architecture <ref type="bibr" target="#b62">(Ronneberger et al., 2015)</ref> that introduced the encoder-decoder paradigm, for upsampling gradually from lower size features to the original image size. Currently, the state of the art on the computer vision datasets is considered to be mask-rcnn <ref type="bibr" target="#b25">(He et al., 2017)</ref>, that performs various tasks (object localization, semantic segmentation, instance segmentation, pose estimation etc). A key element of the success of this architecture is its multitasking nature.</p><p>One of the major advantages of CNNs over traditional classification methods (e.g. random forests), is their ability to process input data in multiple context levels. This is achieved through the downsampling operations that summarizes features. However, this advantage in feature extraction needs to be matched with a proper upsampling method, to retain information from all spatial resolution contexts and produce fine boundary layers. There has been a quick uptake of the approach in the remote sensing community and various solutions based on deep learning have been presented recently (e.g. <ref type="bibr" target="#b65">Sherrah, 2016;</ref><ref type="bibr" target="#b4">Audebert et al., 2016</ref><ref type="bibr" target="#b3">Audebert et al., , 2017</ref><ref type="bibr" target="#b2">Audebert et al., , 2018;;</ref><ref type="bibr" target="#b35">Längkvist et al., 2016;</ref><ref type="bibr" target="#b37">Li et al., 2015;</ref><ref type="bibr" target="#b39">Li and Shao, 2014;</ref><ref type="bibr" target="#b71">Volpi and Tuia, 2017;</ref><ref type="bibr" target="#b41">Liu et al., 2018</ref><ref type="bibr">Liu et al., , 2017a,b;,b;</ref><ref type="bibr">Pan et al., 2018a,b;</ref><ref type="bibr" target="#b48">Marmanis et al., 2016</ref><ref type="bibr" target="#b47">Marmanis et al., , 2018;;</ref><ref type="bibr" target="#b73">Wen et al., 2017;</ref><ref type="bibr" target="#b82">Zhao et al., 2017b)</ref>. A comprehensive review of deep learning applications in the field of remote sensing can be found in <ref type="bibr">Zhu et al. (2017)</ref>; <ref type="bibr" target="#b46">Ma et al. (2019)</ref>; <ref type="bibr" target="#b23">Gu et al. (2019)</ref>.</p><p>Discussing in more detail some of the most relevant approaches to our work, <ref type="bibr" target="#b65">(Sherrah, 2016)</ref> utilized the FCN architecture, with a novel no-downsampling approach based on atrous convolutions to mitigate this problem. The summary pooling operation was traded with atrous convolutions, for filter processing at different scales. The best performing architectures from their experiments were the ones using pretrained convolution networks. The loss used was categorical crossentropy. <ref type="bibr" target="#b42">Liu et al. (2017a)</ref> introduced the Hourglass-shape network for semantic segmentation on VHR images, which included an encoder-decoder style network, utilizing inception like modules. Their encoder-decoder style departed from the UNet backbone, in that they did not use features from all spatial contexts of the encoder in the decoder branch. Also, their decoder branch is not symmetric to the encoder. The building blocks of the encoder are inception modules. Feature upsampling takes place with the use of transpose convolutions. The loss used was weighted binary cross entropy.</p><p>Emphasizing on the importance of using the information from the boundaries of objects, <ref type="bibr" target="#b47">Marmanis et al. (2018)</ref> utilized the Holistically Ne-sted Edge Detection network <ref type="bibr">(Xie and Tu, 2015, HED)</ref> for predicting boundaries of objects. The loss used for the boundaries was an Euclidean distance regression loss. The estimated boundaries were then concatenated with image features and provided them as input into another CNN segmentation network, for the final classification of pixels. For the CNN segmentation network, they experimented with two architectures, the SegNet <ref type="bibr" target="#b6">(Badrinarayanan et al., 2015)</ref> and a Fully Convolutional Network presented in <ref type="bibr" target="#b48">Marmanis et al. (2016)</ref> that uses weights from pretrained architectures. One of the key differences in our approach for boundary detection with <ref type="bibr" target="#b47">Marmanis et al. (2018)</ref>, is that the boundary prediction happens at the end of our architecture, therefore the request for boundary prediction affects all features since the boundaries are strongly correlated with the extent of the predicted classes. In contrast, in <ref type="bibr" target="#b47">Marmanis et al. (2018)</ref>, the boundaries are fed as input to the segmentation branch of their network, i.e. the segmentation part of their network uses them as additional input. Another difference is that we do not use weights from pretrained networks. <ref type="bibr" target="#b58">Pan et al. (2018b)</ref> presented the Dense Pyramid Network. The authors incorporated group convolutions to process independently the Digital Surface Model from the true orthophoto, presenting an interesting data fusion approach. The channels created from their initial group convolutions were shuffled, in order to enhance the information flow between channels. The authors, utilized a DenseNet <ref type="bibr" target="#b29">(Huang et al., 2016)</ref> architecture as their feature extractor. In addition, a Pyramid Pooling layer was used at the end of their encoder branch, before constructing the final segmentation classes. In order to overcome the class imbalance problem, they chose to use the Focal loss function <ref type="bibr" target="#b40">(Lin et al., 2017)</ref>. In comparison with our work, the authors did not use a symmetric encoder-decoder architecture. The building blocks of their model were DenseNet units which are known to be more efficient than standard residual units <ref type="bibr" target="#b29">(Huang et al., 2016)</ref>. The pyramid pooling operator used in the end of their architecture, before the final segmentation map, is at different scales than the one used in ResUNet-a. <ref type="bibr" target="#b41">Liu et al. (2018)</ref> introduced the CASIA network, which consists of a pretrained deep encoder, a set of self-cascaded convolutional units and a decoder part. The encoder part is deeper than the decoder part. The upscaling of the lower level features takes place with a resize operation followed by a convolutional residual correction term. The self-cascaded units, consist of a sequential multi-context aggregation layer, that aggregates features from higher receptive fields to local receptive fields. In a similar idea to our approach, the CASIA network uses features from multiple contexts, however these are evaluated at a different depth of the network and fused together in a completely different way. The architecture achieved state of the art performance on the ISPRS Potsdam and Vaihingen data. The loss function they used was the normalized cross entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The ResUNet-a framework</head><p>In this section, we introduce the architecture of ResUNet-a in full detail (section 3.1), a novel loss function design to achieve faster convergence and higher performance (section 3.2), data augmentation methodology (section 3.3) as well as the methodology we followed on performing inference on large images (section 3.4). The training strategy and software implementation characteristics can be found in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>Our architecture combines the following set of modules encoded in our models:</p><p>1. A UNet <ref type="bibr" target="#b62">(Ronneberger et al., 2015)</ref> backbone architecture, i.e., the encoder-decoder paradigm, is selected for smooth and gradual transitions from the image to the segmentation mask. 2. To achieve consistent training as the depth of the network increases, the building blocks of the UNet architecture were replaced with modified residual blocks of convolutional layers <ref type="bibr" target="#b28">(He et al., 2016)</ref>. Residual blocks remove to a great extent the problem of vanishing and exploding gradients that is present in deep architectures. 3. For better understanding across scales, multiple parallel atrous convolutions <ref type="bibr" target="#b10">(Chen et al., 2016</ref><ref type="bibr" target="#b11">(Chen et al., , 2017) )</ref> with different dilation rates are employed within each residual building block. Although it is not completely clear why atrous convolutions perform well, the intuition behind their usage is that they increase the receptive field of each layer. The rationale of using these multiple-scale layers is to extract object features at various receptive field scales.</p><p>The hope is that this will improve performance by identifying correlations between objects at different locations in the image. 4. In order to enhance the performance of the network by including background context information we use the pyramid scene parsing pooling <ref type="bibr" target="#b81">(Zhao et al., 2017a)</ref> layer. In shallow architectures, where the last layer of the encoder has a size no less than 16x16 pixels, we use this layer in two locations within the architecture: after the encoder part (i.e., middle of the network) and the second last layer before the creation of the segmentation mask. For deeper architectures, we use this layer only close to the last output layer. 5. In addition to the standard architecture that has a single segmentation mask layer as output, we also present two models where we perform multi-task learning. The algo- rithm learns simultaneously four complementary tasks.</p><p>The first is the segmentation mask. The second is the common boundary between the segmentation masks that is known to improve performance for semantic segmentation <ref type="bibr" target="#b7">(Bertasius et al., 2015;</ref><ref type="bibr" target="#b47">Marmanis et al., 2018)</ref>. The third is the distance transform 2 <ref type="bibr" target="#b9">(Borgefors, 1986)</ref> of the segmentation mask. The fourth is the actual colored image, in HSV color space. That is, the identity transform of the content, but in a different color space.</p><p>We term our network ResUNet-a because it consists of residual building blocks with multiple atrous convolutions and a UNet backbone architecture. We present two basic architectures, ResUNet-a d6 and ResUNet-a d7, that differ in their depth, i.e. the total number of layers. In ResUNet-a d6 the encoder part consists of six ResBlock-a building blocks followed by a PSPPooling layer. In ResUNet-a d7 the encoder consists of seven ResBlock-a building blocks. For each of the d6 or d7 models, there are also three different output possibilities: a single task semantic segmentation layer, a multi-task layer (mtsk), and a conditioned multi-task output layer (cmtsk). The difference between the mtsk and cmtsk output layers is how the various complementary tasks (i.e. the boundary, the distance map, and the color) are used for the determination of the main target task, which is the semantic segmentation prediction. In the 2 The result of the distance transform on a binary segmentation mask is a gray level image, that takes values in the range [0,1], where each pixel value corresponds to the distance to the closest boundary. In OpenCV this transform is encoded in cv::distance transform.</p><p>following we present in detail these models, starting from the basic ResUNet-a d6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">ResUNet-a</head><p>The ResUNet-a d6 network consists of stacked layers of modified residual building blocks (ResBlock-a), in an encoderdecoder style (UNet). The input is initially subjected to a convolution layer of kernel size (1, 1) to increase the number of features to the desired initial filter size. A (1, 1) convolution layer was used in order to avoid any information loss from the initial image by summarizing features across pixels with a larger kernel. Then follow the residual blocks. In each residual block (Fig. <ref type="figure" target="#fig_0">1b</ref>), we used as many as three in parallel atrous convolutions in addition to the standard set of two convolutions of the residual network architecture, i.e., there were up to four parallel branches of sets of two stacked convolutional layers. After the convolutions, the output is added to the initial input in the spirit of residual building blocks. We decided to sum the various atrous branches (instead of concatenating them) because it is known that the residual blocks of two successive convolutional layers demonstrate constant condition number of the Hessian of the loss function, irrespective of the depth of the network <ref type="bibr" target="#b38">Li et al. (2016)</ref>. Therefore the summation scheme is easier to train (in comparison with the concatenation of features). In the encoder part of the network, the output of each of the residual blocks is downsampled with a convolution of kernel size of one and stride of two. At the end of both the encoder and the decoder part, there exists a PSPooling operator <ref type="bibr" target="#b81">(Zhao et al., 2017a)</ref>. In the PSPPooling operator (Fig. <ref type="figure" target="#fig_0">1c</ref>), the initial input is split in channel (feature) space in 4 equal partitions. Then we perform max pooling operation in successive splits of the input layer, in 1, 4, 16 and 64 partitions. Note that in the middle layer (Layer 13 has size: [batch size]×1024 × 8 × 8), the split of 64 corresponds to the actual total size of the input (so we have no additional gain with respect to max pooling from the last split). In Fig. <ref type="figure" target="#fig_0">1a</ref> we present the full architecture of ResUNet-a (see also Table <ref type="table" target="#tab_0">1</ref>). In the decoder part, the upsampling is being done with the use of nearest neighbours interpolation followed by a normed convolution with a kernel size of one. By normed convolution, denoted with Conv2DN, we mean a set of a single 2D convolution followed by a BatchNorm layer. This approach for increasing the resolution of the convolution features was used in order to avoid the chequerboard artifact in the segmentation mask <ref type="bibr" target="#b54">(Odena et al., 2016)</ref>. The combination of layers from the encoder and decoder parts is being performed with the Combine layer (Table <ref type="table" target="#tab_1">2</ref>). This module concatenates the two inputs and subjects them to a normed convolution that brings the number of features to the desired size.</p><p>The ResUNet-a d7 model is deeper than the corresponding d6 model, by one resunet building block both in the encoder and decoder parts. We have tested two versions of this deeper architecture that differ in the way the pooling takes place in the middle of the network. In version 1 (hereafter d7v1) the PSPPooling Layer (Layer 13) is replaced with one additional building block, that is a standard resnet block (see Table <ref type="table" target="#tab_2">3</ref> for details). There is, of course, a corresponding increase in the layers of the decoder part as well, by one additional residual build-ing block. In more detail (Table <ref type="table" target="#tab_2">3</ref>), the PSPPooling layer in the middle of the network is replaced by a standard residual block at a lower resolution. The output of this layer is subjected to a MaxPooling2D(kernel=2, stride=2) operation the output of which is rescaled to its original size and then concatenated with the original input layer. This operation is followed by a standard convolution that brings the total number of features (i.e. the number of channels) to their original number before the concatenation. In version 2 (hereafter d7v2), again the Layer 12 is replaced with a standard resnet block. However, now the MaxPooling operation following this layer is replaced with a smaller PSPPooling layer that has three parallel branches, performing pooling in 1/1, 1/2, 1/4 scales of the original filter (Fig. <ref type="figure" target="#fig_0">1c</ref>). The reason for this is that the filters in the middle of the d7 network cannot sustain 4 parallel pooling operations due to their small size (therefore, we remove the 1/8 scale pooling), for an initial input image of size 256x256.</p><p>With regards to the model complexity, ResUNet-a d6 has ∼ 52M trainable parameters for an initial filter size of 32. ResUNet-a d7 that has greater depth has ∼ 160M parameters for the same initial filter size. The number of parameters remains almost identical for the case of the multi-task models as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Multitasking ResUNet-a</head><p>This version of ResUNet-a replaces the last layer (Layer 31) with a multitasking block (Fig. <ref type="figure" target="#fig_1">2</ref>). The multiple tasks are complementary. These are (a) the prediction of the semantic segmentation mask, (b) the detection of the common boundaries between classes, (c) the reconstruction of the distance map and (e) the reconstruction of the original image in HSV color space. Our choice of using a different color space than the original input was guided by the principle that: (a) we wanted to avoid the identity transform in order to exclude the algorithm recovering trivial solutions and (b) the HSV (or HSL) colorspace matches closely the human perception of color A. <ref type="bibr" target="#b0">Vadivel (2005)</ref>. It is important to note that these additional labels are derived using standard computer vision libraries from the initial image and segmentation mask, without the need for additional information (e.g. separately annotated boundaries). A software implementation for this is given in Appendix B. The idea here is that all these tasks are complementary and should help the target task that we are after. Indeed, the distance map provides information for the topological connectivity of the segmentation mask as well as the extent of the objects (for example if we have an image with a "car" (object class) on a "road" (another object class), then the ground truth of the mask of the "road" will have a hole exactly to the location of the pixels corresponding to the "car" object). The boundary helps in better understanding the extent of the segmentation mask. Finally, the colorspace transformation provides additional information for the correlation between color variations and object extent. It also helps to keep "alive" the information of the fine details of the original image to its full extent until the final output layer. The rationale here is similar with the idea behind the concatenation of higher order features (first layers) with lower order features that exist in the UNet backbone architecture: the encoder layers have finer details about the original image as closely as  they are to the original input. Hence, the reason for concatenating them with the layers of the decoder is to keep the fine details necessary until the final layer of the network that is ultimately responsible for the creation of the segmentation mask. By demanding the network to be able to reconstruct the original image, we are making sure that all fine details are preserved<ref type="foot" target="#foot_0">3</ref> (an example of input image, ground truth and inference for all the tasks in the conditioned multitasking setting can be seen in Fig. <ref type="figure" target="#fig_13">13</ref>).</p><p>We present two flavours of the algorithm whose main difference is how the various tasks are used for the target output that we are interested in. In the simple multi-task block (bottom right block of Fig 2 <ref type="figure">)</ref>, the four tasks are produced simultaneously and independently. That is, there is no direct usage of the three complementary tasks (boundary, distance, and color) in the construction of the target task that is the segmentation. The motivation here is that the different tasks will force the algorithm to identify new meaningful features that are correlated with the output we are interested in and can help in the performance of the algorithm for semantic segmentation. For the distance map, as well as the color reconstruction, we do not use the PSPPooling layer. This is because it tends to produce large squared areas with the same values (due to the pooling operation) and the depth of the convolution layers in the logits is not sufficient to diminish this.</p><p>The second version of the algorithm uses a conditioned inference methodology. That is, the network graph is constructed in such a way so as to take advantage of the inference of the previous layers (top right block of Fig <ref type="figure" target="#fig_1">2</ref>). We first predict the distance map. The distance map is then concatenated with the output of the PSPPooling layer and is used to calculate the boundary logits. Then both the distance map and the prediction of the boundary are concatenated with the PSPPooling layer and the result is provided as input to the segmentation logits for the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss function</head><p>In this section, we introduce a new variant of the family of Dice loss functions for semantic segmentation and regression problems. The Dice family of losses is by no means the only option for the task of semantic segmentation. Other interesting loss functions for the task of semantic segmentation are the focal loss <ref type="bibr" target="#b40">Lin et al. 2017</ref>, see also <ref type="bibr" target="#b58">Pan et al. (2018b)</ref> for an application on VHR images, the boundary loss <ref type="bibr" target="#b32">(Kervadec et al., 2018)</ref>, and the Focal Tversky loss <ref type="bibr" target="#b1">(Abraham and Khan, 2018)</ref>. A list of many other available loss functions can be found in <ref type="bibr" target="#b69">Taghanaki et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Introducing the Tanimoto loss with complement</head><p>When it comes to semantic segmentation tasks, there are various options for the loss function. The Dice coefficient <ref type="bibr" target="#b17">(Dice;</ref><ref type="bibr" target="#b67">Sørensen, 1948)</ref>, generalized for fuzzy binary vectors in a multiclass context <ref type="bibr" target="#b51">(Milletari et al., 2016</ref>, see also <ref type="bibr" target="#b15">Crum et al. 2006;</ref><ref type="bibr" target="#b68">Sudre et al. 2017)</ref>, is a popular choice among practitioners. It has been shown that it can increase performance over the cross entropy loss <ref type="bibr" target="#b53">(Novikov et al., 2017)</ref>. The Dice coefficient can be generalized to continuous binary vectors in two ways: either by the summation of probabilities in the denominator or by the summation of their squared values. In the literature, there are at least three definitions which are equivalent <ref type="bibr" target="#b15">(Crum et al., 2006;</ref><ref type="bibr" target="#b51">Milletari et al., 2016;</ref><ref type="bibr" target="#b18">Drozdzal et al., 2016;</ref><ref type="bibr" target="#b68">Sudre et al., 2017)</ref>:</p><formula xml:id="formula_0">D 1 (p, l) = 2 i p i l i i p i + i l i (1) D 2 (p, l) = 2 i p i l i i (p 2 i + l 2 i )</formula><p>(2)</p><formula xml:id="formula_1">D 3 (p, l) = i p i l i i (p 2 i + l 2 i ) − i (p i l i )<label>(3)</label></formula><p>where p ≡ {p i }, p i ∈ [0, 1] is a continuous variable, representing the vector of probabilities for the i-th pixel, and l ≡ {l i } are the corresponding ground truth labels. For binary vectors, l i ∈ {0, 1}. In the following we will represent (where appropriate) for simplicity the set of vector coordinates, p ≡ {p i }, with their corresponding tensor index notation, i.e p ≡ {p i } → p i . These three definitions are numerically equivalent, in the sense that they map the vectors (p i , l i ) to the continuous domain [0, 1], i.e. D(p i , l i ) : 2 → [0, 1]. The gradients however, of these loss functions behave differently for gradient based optimization, i.e., for deep learning applications, as demonstrated in <ref type="bibr" target="#b51">Milletari et al. (2016)</ref>. In the remainder of this paper, we call Dice loss, the loss function with the functional form with the summation of probabilities and labels in the denominator (Eq. 1). We also use the name Tanimoto for the D 3 loss function (Eq. 3) and designate it with the letter T ≡ D 3 .</p><p>We found empirically that the loss functions containing squares in the denominator behave better in pointing to the ground truth irrespective of the random initial configuration of weights. In addition, we found that we can achieve faster training convergence by complementing the loss with a dual form that measures the overlap area of the complement of the regions of interest. That is, if p i measures the probability of the ith pixel to belong in class l i , the complement loss is defined as</p><formula xml:id="formula_2">T (1− p i , 1−l i ),</formula><p>where the subtraction is performed element-wise, e.g.</p><formula xml:id="formula_3">1 − p i = {1 − p 1 , 1 − p 2 , . . . , 1 − p n } etc.</formula><p>The intuition behind the usage of the complement in the loss function comes from the fact that the numerator of the Dice coefficient, i p i l i , can be viewed as an inner product between the probability vector, p = {p i } and the ground truth label vector, l = {l i }. Then, the part of the probabilities vector, p i , that corresponds to the elements of the label vector, l i , that have zero entries, does not alter the value of the inner product<ref type="foot" target="#foot_1">4</ref> . We, therefore, propose that the best flow of gradients (hence faster training) is achieved using as a loss function the average of T (p i , l i ) with its complement, T (1 − p i , 1 − l i ):</p><formula xml:id="formula_4">T (p i , l i ) = T (p i , l i ) + T (1 − p i , 1 − l i ) 2 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Experimental comparison with other Dice loss functions</head><p>In order to justify these choices, we present an example with a single 2D ground truth vector, (l = (1, 0)), and a vector of probabilities p = (p x , p y ) ∈ [0, 1] 2 . We consider the following six loss functions:</p><p>1. the Dice coefficient, D 1 (p i , l i ) ((Eq. 1)) 2. the Dice coefficient with its complement:</p><formula xml:id="formula_5">D1 (p i , l i ) = (D 1 (p i , l i ) + D 1 (1 − p i , 1 − l i ))/2</formula><p>3. The Dice coefficient D 2 (p i , l i ) (Eq. 2). 4. the Dice coefficient with its complement, D2 . 5. the Tanimoto coefficient, T (p i , l i ) (Eq. 3). 6. the Tanimoto coefficient with its complement, T (p i , l i ) (Eq. 4).</p><p>In Fig. <ref type="figure" target="#fig_2">3</ref> we plot the gradient field of the various flavours of the family of Dice loss functions (top panels), as well as the Laplacians of these (i.e. their 2nd order derivatives, bottom panels). The ground truth is marked with a black dot. What is important in these plots is that for a random initialization of the weights for a neural network, the loss function will take a (random) value in the area within [0, 1] 2 . The quality of the loss function then, as a suitable criterion for training deep learning models, is whether the gradients, from every point of the area in the plot, direct the solution towards the ground truth point. Intuitively we also expect that the behavior of the gradients is even better, if the local extrema of the loss on the ground truth, is also a local extremum of the Laplacian of the loss. As it is evident from the bottom panels of Fig. <ref type="figure" target="#fig_2">3</ref> this is not the case for all loss functions.</p><p>In more detail, in Fig. <ref type="figure" target="#fig_2">3</ref>, we plot the gradient field of the Dice loss functions and the corresponding Laplacian fields. In the top row are shown the gradient fields of the three different functional form of the Dice loss and the form with their complements. From left to right we have the Dice coefficient based loss with summation of probabilities in the denumerator, D 1 (p, l), its complement, D1 (p, l), the Dice loss with summation of squares in the denominator, D 2 (p, l), its complement, D2 (p, l), and the third form of the Dice loss with summation of squares that also includes a subtraction term, D 3 (p, l), and its complement, D3 (p, l). From the gradient flow of the D1 loss, it is evident that for a random initialization of the network weights (which is the case in deep learning) that corresponds to some random point (p x , p y ) of the loss landscape, the gradients of the loss with respect to p x , p y will not necessarily direct to the ground truth point in (1, 0). In this respect, the generalized Dice loss with complement, D1 behaves better. However, the gradient flow lines do not pass through the ground truth point for all possible pairs of values (p x , p y ). For the case of the loss functions D 2 , D 3 and their complements, the gradient flow lines pass through the ground truth point, but these are not straight lines. Their forms with complement, D2 , D3 , have gradient lines flowing straight towards the ground truth irrespective of the (random) initialization point. The Laplacians of these loss functions are in the corresponding bottom panels of Fig. <ref type="figure" target="#fig_2">3</ref>. It is clear that the extremum of the Laplacian operator is closer to the ground truth values only for the cases where we consider the loss functions with complement. Interestingly, the Laplacian of the Tanimoto functional form (D 3 ) has extremum values closer to the ground truth point in comparison with the D 2 functional form. In summary, the Tanimoto loss with complement has gradient flow lines that are straight lines (geodesics, i.e. they follow the shortest path) pointing to the ground truth from any random initialization point, and the second order derivative has extremum on the location of the ground truth. This demonstrates, according to our opinion, the superiority of the Tanimoto with complement as a loss function, among the family of loss functions based on the Dice coefficient, for training deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Tanimoto with complement as a regression loss</head><p>It should be stressed, that if we restrict the output of the neural network in the range [0, 1] (with the use of softmax or sigmoid activations) then the Tanimoto loss can be used to recover also continuous variables in the range [0, 1]. In Fig. <ref type="figure" target="#fig_3">4</ref> we present an example of this, for a ground truth vector of l = (0.25, 0.85). In the top panels, we plot the gradient flow of the Tanimoto (left) and Tanimoto with complement (right) functions. In the bottom panels, we plot the corresponding functions obtained after applying the Laplacian operator to the loss functions. This is an appealing property for the case of multi-task learning, where one of the complementary goals is a continuous loss function. The reason being that the gradients of these components will have similar magnitude scale and the training will be equally balanced to all complementary tasks. In contrast, when we use different functional form functions for different tasks in the optimization, we have to explicitely balance the gradients of the different components, with the extra  <ref type="formula" target="#formula_4">4</ref>), this work). The "ground truth", (0.5, 0.5) is represented with a black dot. It is clear that Tanimoto with complement has gradient flow (i.e. gradient magnitudes and direction) that is symmetric around the ground truth point, thus making it suitable for continuous regression problems. In contrast the Dice loss D 1 is not suitable for this usage, while D 2 has a clear assymetry that affects the gradients magnitude around the ground truth.</p><p>cost of having to find the additional hyperparameter(s). For example, assuming we have two complementary tasks described by two different functional form functions, L 1 and L 2 , then the total loss must be balanced with the usage of some (unknown) hyperparameter a that needs to be calculated:</p><formula xml:id="formula_6">L total = L 1 + aL 2 .</formula><p>In Fig. <ref type="figure" target="#fig_4">5</ref> we plot the gradient flow for three different members of the Dice family loss functional forms. From left to right we plot the standard Dice loss with summation in the denominator (D 1 , Eq. ( <ref type="formula">1</ref>), <ref type="bibr" target="#b68">Sudre et al. 2017)</ref>, the Dice loss with squares in the denominator (D 2 , Eq. ( <ref type="formula">2</ref>), <ref type="bibr" target="#b51">Milletari et al. 2016</ref>) and Tanimoto with complement (Eq, (4) that we introduce in this work. It is clear that the Tanimoto with complement has the highest degree of symmetric gradients in both magnitude and direction around the ground truth point (for this example, the "ground truth" is (p x , p y ) = (0.5, 0.5)). In addition, it also has steeper gradients as this is demonstrated from the distance of isocontours. The above help achieving faster convergence in problems with gradient descent optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Generalization to multiclass imbalanced problems</head><p>Following the Dice loss modification of Sudre et al. ( <ref type="formula">2017</ref>) for including weights per class, we generalize the Tanimoto loss for multi-class labelling of images:</p><formula xml:id="formula_7">T (p iJ , l iJ ) = N class J=1 w J N pixels i=1 p iJ l iJ N class J=1 w J N pixels i=1 p 2 iJ + l 2 iJ − p iJ l iJ . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Here w J are the weights per class J, p iJ is the probability of pixel i belonging to class J and l iJ is the label of pixel i belonging to class J. Weights are derived following the inverse "volume" weighting scheme per <ref type="bibr" target="#b15">Crum et al. (2006)</ref>:</p><formula xml:id="formula_9">w J = V −2 J ,<label>(6)</label></formula><p>where V J is the total sum of true positives per class J, V J = N pixels i=1 l iJ . In the following we will exclusively use the weighted Tanimoto (Eq. 5) with complement, T (p iJ , l iJ ) = (T (p iJ , l iJ ) + T (1 − p iJ , 1 − l iJ ))/2, and we will refer to it simply as the Tanimoto loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data augmentation</head><p>To avoid overfitting, we relied on geometric data augmentation so that, in each iteration, the algorithm never sees the exact same set of images (i.e. the batch of images is always different). Each pair of image and ground truth mask are rotated at a random angle, with a random centre and zoomed in/out according to a random scale factor. The parts of the image that are left out from the frame after the transformation are filled in with reflect padding. This data augmentation methodology is particularly useful for aerial images of urban areas due to the high degree of reflect symmetry these areas have by design. We also used random reflections in x, y directions as an additional data augmentation routine.</p><p>The regularization approach is illustrated in Fig. <ref type="figure" target="#fig_5">6</ref> for a single datum of the ISPRS Potsdam data set (top row, FoV×4 dataset). From left to right, we show the false color infrared image of a 256x256 image patch, the corresponding digital elevation model, and the ground truth mask. In rows 2-4, we provide examples of the random transformations of the original image. By exposing the algorithm to different perspectives of the same objects scenery, we encode the prior knowledge that the algorithm should be able to identify the objects for all possible affine transformations. That is, we make the segmentation task invariant in affine transformations. This is quite similar to the functionality of the Spatial Transformer Network <ref type="bibr" target="#b31">(Jaderberg et al., 2015)</ref>, with the difference that this information is hardcoded in the data rather than the internal layers of the network. It should be noted that several authors report performance gains when they use inputs viewed at different scales, e.g., <ref type="bibr" target="#b4">Audebert et al. (2016)</ref> and <ref type="bibr" target="#b76">Yang et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference methodology</head><p>In this section, we detail the approach we followed for performing inference over large true orthophoto that exceeds the 256x256 size of the image patches we use during training.</p><p>As detailed in the introduction, FCNs such as ResUNet-a use contextual information to increase their performance. In practice, this means that in a single 256x256 window for inference, the pixels that are closer to the edges will not be classified as confidently as the ones close to the center because more contextual information is available to central pixels. Indeed, contextual information for the edge pixels is limited since there is no information outside the boundaries of the image patch. To further improve the performance of the algorithm and provide seamless segmentation masks, the inference is enhanced with multiple overlapping inference windows. This is like deciding on the classification result from multiple views (sliding windows) of the same objects. This type of approach is also used for large-scale land cover classification to combine classification in a seamless map <ref type="bibr" target="#b34">(Lambert et al., 2016;</ref><ref type="bibr" target="#b72">Waldner et al., 2017)</ref>.</p><p>Practically, we perform multiple overlapping windows passes over the whole tile and store the class probabilities for each pixel and each pass. The final class probability vector ( pi (x, y)) is obtained using the average of all the prediction views. The sliding window has size equal to the tile dimensions (256x256), however, we step through the whole image in strides of 256/4 = 64 pixels, in order to get multiple inference probabilities for each pixel. In order to account for the lack of information outside the tile boundaries, we pad each tile with reflect padding at a size equal to 256/2 = 128 pixels <ref type="bibr" target="#b62">(Ronneberger et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data and preprocessing</head><p>We sourced data from the ISPRS 2D Semantic Labelling Challenge and in particular the Potsdam data set (ISPRS). The data consist of a set of true orthophoto (TOP) extracted from a larger mosaic, and a Digital Surface Model (DSM). The TOP consists of the four spectral bands in the visible (VIS; red (R), green (G), and blue (G) and in the near infrared (NIR) and the ground sampling distance is 5 cm. The normalized DSM layer provides information on the height of each pixel as the ground elevation was subtracted. The four spectral bands (VISNIR) and the normalized DSM were stacked (VISNIR+DSM) to be used to train the semantic segmentation models. The labels consist of six classes, namely impervious surfaces, buildings, cars, low vegetation, trees, and background.</p><p>Unlike conventional pixel-based (e.g. random forests) or GEOBIA approaches, CNNs have the ability to "see" image objects in their contexts, which provides additional information to discriminate between classes. Thus, working with large image patches maximizes the competitive advantage of CNNs, however, limits to the maximum patch size are dictated by memory restrictions of the GPU hardware. We have created two versions of the training data. In the first version, we resampled the image tiles to half their original resolution and extracted image patches of size 256x256 pixels to train the network. The reduction of the original tile size to half was decided with the mindset that we can include more context information per image patch. This resulted in image patches with four times larger Field of View (hereafter FoV) for the same 256x256 patch size. We will refer to this dataset as (FoV×4) as it includes 4 times larger Field of View (area) in a single 256x256 image patch (in comparison with 256x256 image patches extracted directly from the original unscaled dataset). In the second version of the training data, we kept the full resolution tiles and again extracted image patches of size 256x256 pixels. We will refer to this dataset as FoV×1. The 256x256 image patch size was the maximum size that the memory capacity of our hardware configuration could handle (see Appendix A) so as to process a meaningfully large batch of datums. Each of the 256x256 patches used for training was extracted from a sliding window swiped over the whole tile at a stride, i.e., step, of 128 pixels. This approach guarantees that all pixels at the edge of a patch become central pixels in subsequent patches. After slicing the original images, we split<ref type="foot" target="#foot_2">5</ref> the 256x256 patches into a training set, a validation set, and a test set with the following ratios: 0.8-0.1-0.1.</p><p>The purpose of the two distinct datasets is: the FoV×4 is useful in order to understand how much (if any) the increased context information improves the performance of the algorithm. It also allows us to perform more experiments much faster due to the decreased volume of data. The FoV×4 dataset is approximately ∼50GB, with ∼10k of pairs of images, masks. The FoV×1 has volume size of ∼250GB, and ∼40k pairs of images, masks. In addition, the FoV×4 is a useful benchmark on how the algorithm behaves with a smaller amount of data than the one provided. Finally, the FoV×1 version is used in order to compare the performance of our architecture with other published results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Architecture and Tanimoto loss experimental analysis</head><p>In this section, we perform an experimental analysis of the ResUNet-a architecture as well as the performance of the Tanimoto with complement loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Accuracy assessment</head><p>For each tile of the test set, we constructed the confusion matrix and extracted the several accuracy metrics such as the overall accuracy (OA), the precision, the recall, and the F1score (F 1 ):</p><formula xml:id="formula_10">OA = T P + T N FP + FN (7) precision = T P T P + FP (8) recall = T P T P + FN (9) F 1 = 2 • precision • recall precision + recall (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where T P, FP, FN, and T N are the is true positive, false positive, false negative and true negative classifications, respectively.</p><p>In addition, for the validation dataset (for which we have ground truth labels), we use the Matthews Correlation Coefficient (hereafter MCC, Matthews 1975):</p><formula xml:id="formula_12">MCC = T P × T N − FP × FN √ (T P + FP)(T P + FN)(T N + FP)(T N + FN)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Architecture ablation study</head><p>In this section, we design two experiments in order to evaluate the performance of the various modules that we use in the ResUNet-a architecture. In these experiments we used the FoV×1 dataset, as this is the dataset that will be the ultimate testbed of ResUNet-a performance against other modeling frameworks. Our metric for understanding the performance gains of the various models tested is the model complexity and training convergence: if model A has greater (or equal) number of parameters than model B, and model A converges faster to optimality than model B, then it is most likely that will also achieve the highest overall score.</p><p>In the first experiment we test the convergent properties of our architecture. In this, we are not interested in the final performance (after learning rate reduction and finetuning) which is a very time consuming operation, but how ResUNet-a behaves during training for the same fixed set of hyperparameters and epochs. We start by training a baseline model, a modified ResUNet <ref type="bibr" target="#b80">(Zhang et al., 2017)</ref> where in order to keep the number of parameters identical with the case with atrous, we use the same ResUNet-a building blocks with dilation rate equal to 1 for all parallel residual blocks (i.e. there are no atrous convolutions). This is similar in philosophy with the wide residual networks <ref type="bibr" target="#b77">(Zagoruyko and Komodakis, 2016)</ref>, however, there are no dropout layers. Then, we modify this baseline by increasing the dilation rate, thus adding atrous convolutions (model: ResUNet + Atrous). It should be clear that the only difference between the models ResUNet and ResUNet + Atrous is that the latter has different dilation rates than the former, i.e. they have identical number of parameters. Then we add PSPPooling in both the middle and the end of the framework (model: ResUNet + Atrous + PSP), and finally we apply the conditioned multitasking, i.e. the full ResUNet-a model (model: ResUNet + Atrous + PSP + CMTSK). The differences in performance of the convergence rates is incremental with each module addition. This performance difference can be seen in Fig. <ref type="figure" target="#fig_6">7</ref> and is substantial. In Fig. <ref type="figure" target="#fig_6">7</ref>  Next, we are interested in evaluating the importance of the PSPPooling layer. In our experiments we found this layer to be more important in the middle of the network than before the last output layers. For this purpose, we train two ResUNet-a d7 models, the d7v1 and d7v2, that are identical in all aspects except that the latter has a PSPPooling layer in the middle. Both models are trained with the same fixed set of hyperparameters (i.e. no learning rate reduction takes place during training). In Fig <ref type="figure">9</ref> we show the convergence evolution of these networks. It is clear that the model with the PSPPooling layer in the middle (i.e. v2) converges much faster to optimality, despite the fact that it has greater complexity (i.e. number of parameters) than the model d7v1.</p><p>In addition to the above, we have to note that when the network performs erroneous inference, due to the PSPPooling layer in the end, this may appear in the form of square blocks, indicating the influence of the pooling area in square subregions of the output. The last PSPPooling layer is in particular problematic when dealing with regression output problems. This is the reason why we did not use it in the evaluation of color and distance transform modules in the multitasking networks. In Fig. <ref type="figure">8</ref> we present two examples of erroneous inference of the last PSPPooling layer that appear in the form of square blocks. The first row corresponds to a zoom in region of tile 6 14, and the bottom row to a zoom in region of tile 6 15. From left to right: RGB bands of input image, error map, and inference map. From the boundary of the error map It can be seen that the boundary of the error map has areas that appear in the form of square blocks. That is, the effect of the pooling operation in various scales can dominate the inference area.</p><p>Comparing ResUNet-a-mtsk and ResUNet-a-cmtsk models (on the basis of the d7v1 feature extractor), we find that the latter demonstrates smaller variance in the values of the loss function (and in consequence, the performance metric) during training. In Fig. <ref type="figure" target="#fig_9">10</ref> we present an example of the comparative training evolution of the ResUNet-a d7v1 mtsk versus the ResUNet-a d7v1 cmtsk models. It is clear that the con- ditioned inference model demonstrates smaller variance, and that, despite the random fluctuations of the MCC coefficient, the median performance of the conditioned multitasking model is higher than the median performance of the simple multitasking model. This helps in stabilizing the gradient updates and results slightly better performance. We have also found that the inclusion of the identity reconstruction of the input image (in HSV colorspace) helps further to reduce the variance of the performance metric.</p><p>Our conclusion is that the greatest gain in using the conditioned multi-task model is in faster and consistent convergence to optimal values, as well as better segmentation of the boundaries (in comparison with the single output models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance evaluation of the proposed loss function</head><p>In order to demonstrate the performance difference between the Dice loss as defined in <ref type="bibr" target="#b15">Crum et al. (2006)</ref> and the Tanimoto loss, and the Tanimoto with complement (Eq. 4) we train three identical models with the same set of hyper-parameters. The weighting scheme is the same for all losses. In this experiment we used the FoV×4 dataset, in order to complete it shorter time. As the loss function cannot be responsible for overfitting (only the model capacity can lead to such behavior) our results persist also with the larger FoV×1 dataset. In Fig. <ref type="figure" target="#fig_11">11</ref> we plot the Matthews correlation coefficient (MCC). In this particular example, we are not interested in achieving maximum performance by reducing the learning rate and pushing the boundaries of what the model can achieve. We are only interested to compare the relative performance for the same training epochs between the different losses with an identical set of fixed hyperparameters. It is evident that the Dice loss stagnates to lower values, while the Tanimoto loss with complement converges faster to an optimal value. The difference in performance is significant: the Tanimoto loss with complement achieves for the same number of epochs an MCC = 85.99, while the Dice loss stagnates at MCC = 80.72. The Tanimoto loss without  complement (Eq. 5) gives a similar performance with the Tanimoto with complement, however, it converges relatively slower and demonstrates greater variance. In all experiments we performed, the Tanimoto with complement gave us the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and discussion</head><p>In this section, we present and discuss the performance of ResUNet-a. We also compare the efficiency of our model with results from architectures of other authors. We present results in both the FoV×4 and FoV×1 versions of the ISPRS Potsdam dataset. It should be noted that the ground truth masks of the test set were made publicly available on June 2018. Since then, the ISPRS 2D semantic label online test results are not being updated. The ground truth labels used to calculate the performance scores are the ones with the eroded boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Design of experiments</head><p>In Section 5 we tested the convergence properties of the various modules that ResUNet-a uses. In this section, our goal is to train the best convergent models until optimality and compare their performance. To this aim we document the following set of representative experiments: (a) ResUNet-a d6 vs ResUNet-a d6 cmtsk. The relative comparison of this will give us the performance boost between single task and conditioned multitasking models, keeping everything else the same. (b) ResUNet-a d7v1 mtsk vs ResUNet-a d7v1 conditioned mtsk. Here we are trying to see if conditioned multitasking improves performance over simple multitasking. In order to reduce computation time, we train these models with the FoV×4 dataset. Finally, we train the two best models, ResUNet-a d6 cmtsk and ResUNet-a d7v2 cmtsk on the FoV×1 dataset, in order to see if there are performance differences due to different Field of Views as well as compare our models with the results from other authors.   <ref type="table" target="#tab_3">4</ref>). The worst result (excluding the class "Background") for this model comes in the class "Trees", where it seems that ResUNet-a d6 systematically under segments the area close to their boundary. This is partially owed to the fact that we reduced the size of the original image, and fine details required for the detailed extent of trees cannot be identified by the algorithm. In fact, even for a human, the annotated boundaries of trees are not always clear (e.g. see Fig. <ref type="figure" target="#fig_12">12</ref>). The ResUNet-a d6 cmtsk model provides a significant performance boost over the single task ResUNet-a d6 model, for the classes "Bulding", "LowVeg" and "Tree". In these classes it also outperforms the deeper models d7v1 (which, however, do not include the PSPPooling layer at the end of the encoder). This is due to the explicit requirement for the algorithm to reconstruct also the boundaries and the distance map and use them to further refine the segmentation mask. As a result, the algorithm gains a "better understanding" of the fine details of objects, even if in some cases it is difficult for humans to clearly identify their boundaries.</p><p>The ResUNet-a d7v1 cmtsk model demonstrates slightly increased performance over all of the tested models (Table <ref type="table" target="#tab_3">4</ref>, although differences are marginal for the FoV×4 dataset, and vary between classes). In addition, there are some annotation errors to the dataset that eventually prove to be an upper bound to the performance. In Fig. <ref type="figure" target="#fig_12">12</ref> we give an example of inference on 256x256 patches of images on unseen test data. In Fig. <ref type="figure" target="#fig_13">13</ref> we provide an example of the inference performed by ResUNet-a d7v1 cmtsk for all the predictive tasks (boundary, distance transform, segmentation, and identity reconstruction). In all rows, the left column corresponds to the same ground truth image. In the first row, from left to right: input image, ground truth segmentation mask, inference segmentation mask. Second row, middle and right: ground truth boundary and inference heat map of the confidence of the algorithm for characterizing pixels as boundaries. The more faint the boundaries are, the less confident is the algorithm for their characterization as boundaries. Third row, middle and right: distance map and inferred distance map. Last row, middle: reconstructed image in HSV space. Right image: average error over all channels between the original RGB image and the reconstructed one. The reconstruction is excellent suggesting that the Tanimoto loss can be used for identity mappings, whenever these are required (as a means of regularization or for Generative Adversarial Networks training <ref type="bibr" target="#b21">(Goodfellow et al., 2014)</ref>, e.g. <ref type="bibr">Zhu et al. (2017)</ref>).</p><p>Finally, in Table <ref type="table" target="#tab_3">4</ref>, we provide a relative comparison between models trained in the FoV×4 and FoV×1 versions of the datasets. Clearly, there is a performance boost when using the higher resolution dataset (FoV×1) for the classes that require finer details. However, for the class "Building" the score is actually better with the wider Field of View (FoV×4, model d6 cmtsk) dataset. It is clear that finer details are present especially for the class "Trees" and "LowVeg", that improve the performance of the algorithm over the FoV×4 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison with other modeling frameworks</head><p>In this section, we compare the performance of the ResUNet-a modeling framework with a representative sample of (peer reviewed) alternative convolutional neural network models. For this comparison we evaluate the models trained on the FoV×1 dataset. The modeling frameworks we compare against ResUNet-a have published results on the ISPRS website. These are: UZ 1 <ref type="bibr" target="#b71">(Volpi and Tuia, 2017)</ref>, RIT L7 <ref type="bibr" target="#b43">(Liu et al., 2017b</ref><ref type="bibr">), RIT 4 (Piramanayagam et al., 2018)</ref>, DST 5 <ref type="bibr" target="#b65">(Sherrah, 2016)</ref>, CAS Y3 (ISPRS), CASIA2 <ref type="bibr" target="#b41">(Liu et al., 2018)</ref>, DPN MFFL <ref type="bibr" target="#b58">(Pan et al., 2018b)</ref>, and HSN+OI+WBP <ref type="bibr" target="#b42">(Liu et al., 2017a)</ref>. To the best of our knowledge, at the time of writing this manuscript, these consist of the best performing models in the competition. For comparison, we provide the F1-score per class over all test tiles, the average F1-score over all classes over all test tiles, and the overall accuracy. Note that the average F1-score was calculated using all classes except the "Background" class. The overall accuracy, for ResUNet-a , was calculated including the "Background" category.</p><p>In Table <ref type="table" target="#tab_4">5</ref> we provide the comparative results, per class, as well as the average F1 score and overall accuracy for the ResUNet-a d6 cmtsk and ResUNet-a d7v2 cmtsk models as well as results from other authors. ResUNet-a d6 performs very well in accordance with other state of the art modeling frameworks, and it ranks overall 3rd (average F1). It should be stressed that for the majority of the results, the performance differences are marginal. Going deeper, the ResUNet-a d7v2 model rank 1st among the representative sample of competing models, in all classes, thus clearly demonstrating the improvement over the state of the art. In Table <ref type="table" target="#tab_6">6</ref> we provide the confusion matrix, over all test tiles, for this particular model.</p><p>It should be noted that some of the contributors (e.g., CA-SIA2, RIT 4, DST 5) in the ISPRS competition used networks with pre-trained weights on external large data sets (e.g. Ima-geNet, <ref type="bibr" target="#b16">Deng et al., 2009</ref>) and fine-tuning, i.e. a methodology called transfer learning <ref type="bibr" target="#b56">(Pan and Yang, 2010</ref>, see also <ref type="bibr" target="#b59">Penatti et al. (2015)</ref>, <ref type="bibr">Xie et al. (2015)</ref> for remote sensing applications). In particular, CASIA2, that has the 2nd highest overall score, used as a basis a state of the art pre-trained ResNet101 <ref type="bibr" target="#b28">(He et al., 2016)</ref> network. In contrast, ResUNet-a was trained from random weights initialization only on the ISPRS Potsdam data set. Although it has been demonstrated that such a strategy does not influence the final performance, i.e. it is possible to achieve the same performance without pre-trained weights <ref type="bibr" target="#b24">(He et al., 2018)</ref>, this comes at the expense of a very long training time.</p><p>To visualize the performance of ResUNet-a , we generated error maps that indicate incorrect (correct) classification in red (green). All summary statistics and error maps were created using the software provided on the ISPRS competition website. For all of our inference results, we used the ground truth masks with eroded boundaries as suggested by the curators of the IS-PRS Potsdam data set (ISPRS). This allows interested readers to have a clear picture of the strengths and weaknesses of our algorithm in comparison with online published results<ref type="foot" target="#foot_3">6</ref> . In Fig. <ref type="figure" target="#fig_15">15</ref> we provide the input image (left column), the error map between the inferred and ground truth masks (middle column) and the inference (right column) for a sample of four test tiles. In Appendix C we present the evaluation results for the rest of the test TOP tiles, per class. In all of these figures, for each row, from left to right: original image tile, error map and inference using our best model (ResUNet-a-cmtsk d7v2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this work, we present a new deep learning modeling framework, for semantic segmentation of high resolution aerial images. The framework consists of a novel multitasking deep learning architecture for semantic segmentation and a new variant of the Dice loss that we term Tanimoto.</p><p>Our deep learning architecture, ResUNet-a, is based on the encoder/decoder paradigm, where standard convolutions are replaced with ResNet units that contain multiple in parallel atrous convolutions. Pyramid scene parsing pooling is included in the middle and end of the network. The best performant variant of our models are conditioned multitasking models which predict among with the segmentation mask also the boundaries of the various classes, the distance transform (that provides information for the topological connectivity of the objects) as well as the identity reconstruction of the input image. The additionally inferred tasks, are re-used internally into the network before the final segmentation mask is produced. That is, the final segmentation mask is conditioned on the inference result of the boundaries of the objects as well as the distance transform of their segmentation mask. We show experimentally that the conditioned multitasking improves the performance of the inferred semantic segmentation classes. The ground truth labels   We analyze the performance of various flavours of the Dice loss and introduce a novel variant of this as a loss function, the Tanimoto loss. This loss can also be used for regression problems. This is an appealing property that makes this loss useful for the case of multitasking problems in that it results in balanced gradients for all tasks during training. We show experimentally that the Tanimoto loss speeds up the training convergence and behaves well under the presence of heavily imbalanced data sets.</p><p>The performance of our framework is evaluated on the 2D semantic segmentation ISPRS Potsdam data set. Our best model, ResUNet-a d7v2 achieves top rank performance in comparison with other published results (Table <ref type="table" target="#tab_4">5</ref>) and demonstrates a clear improvement over the state of the art. The combination of ResUNet-a conditioned multitasking with the proposed loss function is a reliable solution for performant semantic segmentation tasks.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the ResUNet-a d6 network. (a) The left (downward) branch is the encoder part of the architecture. The right (upward) branch is the decoder. The last convolutional layer has as many channels as there are distinct classes. (b) Building block of the ResUNet-a network. Each unit within the residual block has the same number of filters with all other units. Here d 1 , . . . , d n designate different dilation rates, (c) Pyramid scene parsing pooling layer. Pooling takes place in 1/1, 1/2, 1/4 and 1/8 portions of the original image.</figDesc><graphic url="image-2.png" coords="4,343.68,292.29,187.87,161.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-task output layer of the ResUNet-a network. Referring to the example of ResUNet-a d6, Layers 29, 30 and 31 are replaced with one of two variants of the multi-task layer. The first one, the conditioned multitask layer, combines the various intermediate products progressively so as the final segmentation layer to take a "decision" based on inference from previous results. The simple multi-task layer keeps the tasks independent.</figDesc><graphic url="image-3.png" coords="5,37.61,81.22,251.06,235.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Contour plots of the gradient flow (top row) and Laplacian operator (bottom row) for the various versions of the Dice loss functions (Eq 1-3), D i , as well as the functional forms with complements, Di . The black dot corresponds to the ground truth value (1, 0). From left to right, for the top row, we have the gradient flow of the generalized loss functions D 1 , D1 , D 2 , D2 and D 3 , D3 . The bottom panels are the corresponding Laplacian operators of these. The numerical values of the isocontours on the images describe numerically the colorscheme with darker values corresponding to smaller values.</figDesc><graphic url="image-10.png" coords="8,66.51,189.92,63.56,65.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top panels: gradient flow of the Tanimoto (top left) and Tanimoto with complement (top right) loss functions for a continuous target value. Bottom panels: corresponding Laplacian of the gradients. The "ground truth", (0.25, 0.85) is represented with a black dot. The numerical values of the isocontours on the images describe numerically the colorscheme with darker values corresponding to lower values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Gradient flow of the Dice family of losses for three different functional forms. From left to right: standard Dice loss (D 1 , Eq. (1), Sudre et al. 2017, Dice loss with squares in the denominator (D 2 , Eq. (2),<ref type="bibr" target="#b51">Milletari et al. 2016)</ref>, Tanimoto with complement (Eq, (4), this work). The "ground truth", (0.5, 0.5) is represented with a black dot. It is clear that Tanimoto with complement has gradient flow (i.e. gradient magnitudes and direction) that is symmetric around the ground truth point, thus making it suitable for continuous regression problems. In contrast the Dice loss D 1 is not suitable for this usage, while D 2 has a clear assymetry that affects the gradients magnitude around the ground truth.</figDesc><graphic url="image-16.png" coords="9,406.41,85.87,67.21,67.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of data augmentation on image patches of size 256x256 (ground sampling distance 10cm -FoV×4 dataset). Top row: original image, subsequent rows: random rotations with respect to (random) center and at a random scale (zoom in/out). Reflect padding was used to fill the missing values of the image after the transformation.</figDesc><graphic url="image-22.png" coords="10,307.39,374.68,250.28,80.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Convergence performance of the ResUNet-a architecture. Starting from a baseline wide-ResUNet, we add components keeping all training hyperparameters identical.</figDesc><graphic url="image-26.png" coords="12,77.28,108.24,177.84,105.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>we plot the Matthews correlation coefficient (MCC) for all models. The MCC was calculated using the success rate over all classes. The baseline ResUNet requires approximately 120 epochs to achieve the same performance level that ResUNet-a -cmtsk achieves in epoch ∼ 40. The mere change from simple (ResUNet) to atrous convolutions (model ResUNet+ Atrous) almost doubles the convergence rate. The inclusion of the PSP module (both middle and end) provides additional learning capacity, however, it also comes with training instability. This is fixed by adding the conditioned multitasking module in the final model. Clearly, each module addition: (a) increases the complexity of the model since it increases the total number of parameters and (b) it improves the convergence performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Example of PSPPooling erroneous inference behaviour for segmentation tasks. The error appears in the form of (parts of) squares blocks. For each row, from left to right: RGB bands of input image, error map, and segmentation mask. The top row corresponds to a zoom in region of tile 6 14 and the bottom to a region of tile 6 15. Each image patch is of size 1256×1256 and corresponds to a ground sampling distance of 5cm.</figDesc><graphic url="image-32.png" coords="12,451.56,155.27,67.87,67.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Training evolution of the conditioned vs the standard multi-task models for the ResUNet-a d7v1 family of models. The conditioned model (cmtsk) is represented with a red solid line, while the standard multi-task (mtsk) one with a dashed blue line. The mtsk demonstrates higher variance during training especially closer to the final convergence.</figDesc><graphic url="image-35.png" coords="13,79.46,109.99,174.78,108.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Training evolution of the same model using three different loss functions. The Tanimoto with complement loss (solid red line, this work), the Tanimoto (solid green line), and the Dice loss (dashed blue line, Sudre et al., 2017).</figDesc><graphic url="image-37.png" coords="13,346.27,123.93,177.84,89.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: ResUNet-a d7v1 cmtsk inference on unseen test patches of size 256x256 (FoV×4 -ground sampling distance 10cm ). From left to right: rgb image, digital elevation map, ground truth, and prediction.</figDesc><graphic url="image-52.png" coords="14,43.55,563.87,103.95,103.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: ResUNet-a d7v1 cmtsk all tasks inference on unseen test patches of size 256x256 for the FoV×4 dataset (ground sampling distance 10cm). From left to right, top row: input image, ground truth segmentation mask, predicted segmentation mask. Second row: input image, ground truth boundaries, predicted boundaries (confidence). Third row: input image, ground truth distance map, inferred distance map. Bottom row: input image, reconstructed image, difference between input and predicted image.</figDesc><graphic url="image-56.png" coords="15,37.61,81.22,251.06,273.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Same as Fig. 13 for the ResUNet-a d7v2 cmtsk trained on the FoV×1 dataset (256×256 image patches, ground sampling distance 5cm).It is clear that finer details are present especially for the class "Trees" and "LowVeg", that improve the performance of the algorithm over the FoV×4 dataset.</figDesc><graphic url="image-57.png" coords="15,306.60,383.14,251.05,271.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure15: ResUNet-a best model results for tiles 2-13, 2-14, 3-13 and 3-14. From left to right, input image, difference between ground truth and predictions, inference map. Image resolution:6k×6k, ground sampling distance of 5cm.</figDesc><graphic url="image-67.png" coords="17,38.10,589.23,154.16,154.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure C. 17 :</head><label>17</label><figDesc>Figure C.17: ResUNet-a best model results for tiles 4-13, 4-14, 4-15, 5-13. From left to right, input image, difference between ground truth and predictions, inference map. Image resolution:6k×6k, ground sampling distance of 5cm.</figDesc><graphic url="image-80.png" coords="22,38.10,589.23,154.16,154.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure</head><label></label><figDesc>Figure C.18: As Fig. C.17 for tiles5-14, 5-15, 6-13, 6-14    </figDesc><graphic url="image-93.png" coords="23,352.43,589.23,154.16,154.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of the ResUNet-a layers for the d6 model. Here f stands for the number of output channels (or features, the input number of features is deduced from the previous layers). k is the convolution kernel size, d is the dilation rate, and s the stride of the convolution operation. In all convolution operations we used appropriate zero padding to keep the dimensions of the produced feature maps equal to the input feature map (unless downsampling).</figDesc><table><row><cell>Layer #</cell><cell>Layer Type</cell></row><row><cell>1</cell><cell>Conv2D(f=32, k=1, d=1, s=1)</cell></row><row><cell>2</cell><cell>ResBlock-a(f=32, k=3, d={1,3,15,31}, s=1)</cell></row><row><cell>3</cell><cell>Conv2D(f=64, k=1, d=1, s=2)</cell></row><row><cell>4</cell><cell>ResBlock-a(f=64, k=3, d={1,3,15,31}, s=1)</cell></row><row><cell>5</cell><cell>Conv2D(f=128, k=1, d=1, s=2)</cell></row><row><cell>6</cell><cell>ResBlock-a(f=128, k=3, d={1,3,15}, s=1)</cell></row><row><cell>7</cell><cell>Conv2D(f=256, k=1, d=1, s=2)</cell></row><row><cell>8</cell><cell>ResBlock-a(f=256, k=3, d={1,3,15}, s=1)</cell></row><row><cell>9</cell><cell>Conv2D(f=512, k=1, d=1, s=2)</cell></row><row><cell>10</cell><cell>ResBlock-a(f=512, k=3, d=1, s=1)</cell></row><row><cell>11</cell><cell>Conv2D(f=1024, k=1, d=1, s=2)</cell></row><row><cell>12</cell><cell>ResBlock-a(f=1024, k=3, d=1, s=1)</cell></row><row><cell>13</cell><cell>PSPPooling</cell></row><row><cell>14</cell><cell>UpSample (f=512)</cell></row><row><cell>15</cell><cell>Combine (f=512, Layers 14 &amp; 10)</cell></row><row><cell>16</cell><cell>ResBlock-a(f=512, k=3, d=1, s=1)</cell></row><row><cell>17</cell><cell>UpSample (f=256)</cell></row><row><cell>18</cell><cell>Combine (f=256, Layers 17 &amp; 8)</cell></row><row><cell>19</cell><cell>ResBlock-a(f=256, k=3, d=1, s=1)</cell></row><row><cell>20</cell><cell>UpSample (f=128)</cell></row><row><cell>21</cell><cell>Combine (f=128, Layers 20 &amp; 6)</cell></row><row><cell>22</cell><cell>ResBlock-a(f=128, k=3, d=1, s=1)</cell></row><row><cell>23</cell><cell>UpSample (f=64)</cell></row><row><cell>24</cell><cell>Combine (f=64, Layers 23 &amp; 4)</cell></row><row><cell>25</cell><cell>ResBlock-a(f=64, k=3, d=1, s=1)</cell></row><row><cell>26</cell><cell>UpSample (f=32)</cell></row><row><cell>27</cell><cell>Combine (f=32, Layers 26 &amp; 2)</cell></row><row><cell>28</cell><cell>ResBlock-a(f=32, k=3, d=1, s=1)</cell></row><row><cell>29</cell><cell>Combine (f=32, Layers 28 &amp; 1)</cell></row><row><cell>30</cell><cell>PSPPooling</cell></row><row><cell>31</cell><cell>Conv2D (f = NClasses, k=1, d=1, s=1)</cell></row><row><cell>32</cell><cell>Softmax(dim = 1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Details of the Combine(Input1,Input2) layer.</figDesc><table><row><cell>Layer #</cell><cell>Layer Type</cell></row><row><cell>1</cell><cell>Input1</cell></row><row><cell>2</cell><cell>ReLU(Input1)</cell></row><row><cell>3</cell><cell>Concat(Layer 2,Input2)</cell></row><row><cell>3</cell><cell>Conv2DN(k=1, d=1, s=1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Details of the replacement of the middle PSPPooling layer (Layer 13 from Table1) for the ResUNet-a d7 model.</figDesc><table><row><cell>Layer #</cell><cell>Layer Type</cell></row><row><cell>input</cell><cell>Layer 12</cell></row><row><cell>A</cell><cell>Conv2D(f=2048, k=1, d=1, s=2)(input)</cell></row><row><cell>B</cell><cell>ResBlock-a(f=2048, k=3, d=1, s=1)(Layer A)</cell></row><row><cell>C</cell><cell>MaxPooling(kernel=2, stride=2)(Layer B)</cell></row><row><cell>D</cell><cell>UpSample(Layer C)</cell></row><row><cell>E</cell><cell>Concat(Layer D, Layer B)</cell></row><row><cell>F</cell><cell>Conv2D(f=2048,kernel=1)(Layer E)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Potsdam comparison of results (F1 score and overall accuracy -OA) for the various ResUNet-a models trained on the FoV×4 and FoV×1 datasets. Highest score is marked with bold. The average F1-score was calculated using all classes except the "Background" class. The overall accuracy, was calculated including the "Background" category.</figDesc><table><row><cell>Methods</cell><cell>DataSet</cell><cell cols="4">ImSurface Building LowVeg Tree</cell><cell>Car</cell><cell>Avg. F1</cell><cell>OA</cell></row><row><cell>ResUNet-a d6</cell><cell>(FoV×4)</cell><cell>92.7</cell><cell>97.1</cell><cell>86.4</cell><cell>85.8</cell><cell>95.8</cell><cell>91.6</cell><cell>90.1</cell></row><row><cell>ResUNet-a d6 cmtsk</cell><cell>(FoV×4)</cell><cell>91.4</cell><cell>97.6</cell><cell>87.4</cell><cell>88.1</cell><cell>95.3</cell><cell>91.9</cell><cell>90.1</cell></row><row><cell>ResUNet-a d7v1 mtsk</cell><cell>(FoV×4)</cell><cell>92.9</cell><cell>97.2</cell><cell>86.8</cell><cell>87.4</cell><cell>96.0</cell><cell>92.1</cell><cell>90.6</cell></row><row><cell cols="2">ResUNet-a d7v1 cmtsk (FoV×4)</cell><cell>92.9</cell><cell>97.2</cell><cell>87.0</cell><cell>87.5</cell><cell>95.8</cell><cell>92.1</cell><cell>90.7</cell></row><row><cell>ResUNet-a d6 cmtsk</cell><cell>(FoV×1)</cell><cell>93.0</cell><cell>97.2</cell><cell>87.5</cell><cell>88.4</cell><cell>96.1</cell><cell>92.4</cell><cell>91.0</cell></row><row><cell cols="2">ResUNet-a d7v2 cmtsk (FoV×1)</cell><cell>93.5</cell><cell>97.2</cell><cell>88.2</cell><cell>89.2</cell><cell>96.4</cell><cell>92.9</cell><cell>91.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Background</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ImSurf</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Car</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Building</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LowVeg</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tree</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Potsdam comparison of results (based on per class F1 score) with other authors. Best values are marked with bold, second best values are underlined, third best values are in square brackets. Models trained with FoV×1 were trained on 256x256 patches extracted from the original resolution images.</figDesc><table><row><cell>Methods</cell><cell cols="3">ImSurface Building LowVeg</cell><cell>Tree</cell><cell>Car</cell><cell>Avg. F1</cell><cell>OA</cell></row><row><cell>UZ 1 (Volpi and Tuia, 2017)</cell><cell>89.3</cell><cell>95.4</cell><cell>81.8</cell><cell>80.5</cell><cell>86.5</cell><cell>86.7</cell><cell>85.8</cell></row><row><cell>RIT L7 (Liu et al., 2017b)</cell><cell>91.2</cell><cell>94.6</cell><cell>85.1</cell><cell>85.1</cell><cell>92.8</cell><cell>89.8</cell><cell>88.4</cell></row><row><cell>RIT 4 (Piramanayagam et al., 2018)</cell><cell>92.6</cell><cell>97.0</cell><cell>86.9</cell><cell>87.4</cell><cell>95.2</cell><cell>91.8</cell><cell>90.3</cell></row><row><cell>DST 5 (Sherrah, 2016)</cell><cell>92.5</cell><cell>96.4</cell><cell>86.7</cell><cell>88.8</cell><cell>94.7</cell><cell>91.7</cell><cell>90.3</cell></row><row><cell>CAS Y3 (ISPRS)</cell><cell>92.2</cell><cell>95.7</cell><cell>87.2</cell><cell>87.6</cell><cell>95.6</cell><cell>91.7</cell><cell>90.1</cell></row><row><cell>CASIA2 (Liu et al., 2018)</cell><cell>93.3</cell><cell>97.0</cell><cell>[87.7]</cell><cell>[88.4]</cell><cell>96.2</cell><cell>92.5</cell><cell>91.1</cell></row><row><cell>DPN MFFL (Pan et al., 2018b)</cell><cell>92.4</cell><cell>[96.4]</cell><cell>87.8</cell><cell>88.0</cell><cell>95.7</cell><cell>92.1</cell><cell>90.4</cell></row><row><cell>HSN+OI+WBP (Liu et al., 2017a)</cell><cell>91.8</cell><cell>95.7</cell><cell>84.4</cell><cell>79.6</cell><cell>88.3</cell><cell>87.9</cell><cell>89.4</cell></row><row><cell>ResUNet-a d6 cmtsk (FoV×1)</cell><cell>[93.0]</cell><cell>97.2</cell><cell>87.5</cell><cell cols="2">[88.4] [96.1]</cell><cell>[92.4]</cell><cell>[91.0]</cell></row><row><cell>ResUNet-a d7v2 cmtsk (FoV×1)</cell><cell>93.5</cell><cell>97.2</cell><cell>88.2</cell><cell>89.2</cell><cell>96.4</cell><cell>92.9</cell><cell>91.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Potsdam summary confusion matrix over all test tiles for ground truth masks that do not include the boundary. The results correspond to the best model, ResUNet-a-cmtsk d7v2, trained on the FoV×1 dataset. The overall accuracy achieved is 91.5% are used during training for the boundaries, as well as the distance transform, can be both calculated very easily from the ground truth segmentation mask using standard computer vision software (OpenCV, see Section Appendix B for a Python implementation).</figDesc><table><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Predicted</cell><cell>Reference</cell><cell>ImSurface</cell><cell>Building</cell><cell>LowVeg</cell><cell>Tree</cell><cell>Car</cell><cell>Clutter/Background</cell></row><row><cell>ImSurface</cell><cell></cell><cell>0.9478</cell><cell>0.0085</cell><cell>0.0247</cell><cell>0.0117</cell><cell>0.0002</cell><cell>0.0071</cell></row><row><cell>Building</cell><cell></cell><cell>0.0115</cell><cell>0.9765</cell><cell>0.0041</cell><cell>0.0025</cell><cell>0.0001</cell><cell>0.0053</cell></row><row><cell>LowVeg</cell><cell></cell><cell>0.0317</cell><cell>0.0057</cell><cell>0.9000</cell><cell>0.0532</cell><cell>0.0000</cell><cell>0.0095</cell></row><row><cell>Tree</cell><cell></cell><cell>0.0223</cell><cell>0.0036</cell><cell>0.0894</cell><cell>0.8807</cell><cell>0.0016</cell><cell>0.0024</cell></row><row><cell>Car</cell><cell></cell><cell>0.0070</cell><cell>0.0016</cell><cell>0.0002</cell><cell>0.0091</cell><cell>0.9735</cell><cell>0.0086</cell></row><row><cell cols="2">Clutter/Background</cell><cell>0.2809</cell><cell>0.0844</cell><cell>0.1200</cell><cell>0.0172</cell><cell>0.0090</cell><cell>0.4885</cell></row><row><cell cols="2">Precision/Correctness</cell><cell>0.9220</cell><cell>0.9679</cell><cell>0.8640</cell><cell>0.9030</cell><cell>0.9538</cell><cell>0.7742</cell></row><row><cell cols="2">Recall/Completeness</cell><cell>0.9478</cell><cell>0.9765</cell><cell>0.9000</cell><cell>0.8807</cell><cell>0.9735</cell><cell>0.4885</cell></row><row><cell>F1</cell><cell></cell><cell>0.9347</cell><cell>0.9722</cell><cell>0.8816</cell><cell>0.8917</cell><cell>0.9635</cell><cell>0.5990</cell></row></table><note>that</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">However, the color reconstruction on its own does not guarantee that the network learns meaningful correlations between classes and colors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">As a simple example, consider four dimensional vectors, say p = (p 1 , p 2 , p 3 , p 4 ) and l = (1, 1, 0, 0). The value of the inner product term is p•l = p 1 + p 2 , and therefore the information contained in p 3 and p 4 entries is not apparent to the numerator of the loss. The complement inner product provides information for these terms: (1 − p) • (1 − l) = (1 − p) • (0, 0, 1, 1) = 2 − (p 3 + p 4 ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Making sure there is no overlap between the image patches of the training, validation and test sets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">For comparison, competition results can be found online.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">http://www.bsf-swissphoto.com/unternehmen/ueber uns bsf.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">A a short tutorial on manual gradient aggregation with the gluon API in the mxnet framework can be found online.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors acknowledge the support of the Scientific Computing team of CSIRO, and in particular Peter H. Campbell and Ondrej Hlinka. Their contribution was substantial in overcoming many technical difficulties of distributed GPU computing. The authors are also grateful to John Taylor for his help in understanding and implementing distributed optimization using Horovod <ref type="bibr" target="#b64">(Sergeev and Balso, 2018)</ref>. The authors acknowledge the support of the mxnet community, and in particular Thomas Delteil, Sina Afrooze and Thom Lane. The authors acknowledge the provision of the Potsdam ISPRS dataset by BSF Swissphoto 7 . The authors acknowledge the contribution of the anonymous referees, whos questions helped to improve the quality of the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Software implementation and training characteristics</head><p>ResUNet-a was built and trained using the mxnet deep learning library <ref type="bibr" target="#b12">(Chen et al., 2015)</ref>, under the GLUON API. Each of the models trained on the FoV×4 dataset was trained with a batch size of 256 on a single node containing 4 NVIDIA Tesla P100 GPUs in CSIRO HPC facilities. Due to the complexity of the network, the batch size in a single GPU iteration cannot be made larger than ∼ 10 (per GPU). In order to increase the batch size we used manual gradient aggregation 8 . For the models trained on the FoV×1 dataset we used a batch size of 480 in order to speed up the computation. These were trained in a distributed scheme, using the ring allreduce algorithm, and in particular it's implementation on Horovod <ref type="bibr" target="#b64">(Sergeev and Balso, 2018)</ref> for the mxnet <ref type="bibr" target="#b12">(Chen et al., 2015)</ref> deep learning library. The optimal learning rate for all runs was set by the methodology developed in <ref type="bibr" target="#b66">Smith (2018)</ref>. In particular, by monitoring the loss error during training for a continuously increasing learning rate, starting from a very low value. An example is shown in Fig. <ref type="bibr">A.16:</ref> The optimal learning rate is approximately the point of steepest decent of the loss functions. This process was complete in approximately 1 epoch and it can be applied in a distributed scheme as well. We found it more useful than the linear learning rate scaling that is used for large batch size <ref type="bibr" target="#b22">(Goyal et al., 2017)</ref> in distributed optimization. For all models , we used the Adam (Kingma and Ba, 2014) optimizer, with an initial learning rate of 0.001 (initial learning rate can also be set higher for this dataset, see Fig <ref type="figure">. A</ref>.16), momentum parameters (β 1 , β 2 ) = (0.9, 0.999). The learning rate was reduced by an order of magnitude whenever the validation loss stopped decreasing. Overall we reduced the learning rate 3 times. We have also experimented with smaller batch sizes. In particular, with a batch size of 32, the training is unstable. This is owed mainly to the fact that we used 4 GPUs for training, therefore the batch size per GPU is 8, and this is not sufficient for the Batch Normalization layers that use only the data per GPU for the estimation of running means of their parameters. When we experimented with synchronized Batch Normalization layers <ref type="bibr" target="#b30">(Ioffe and Szegedy, 2015;</ref><ref type="bibr" target="#b78">Zhang et al., 2018)</ref>, this increased the stability of the training dramatically even with a batch size as small as 32. However, due to the GPU synchronization, this was a slow operation that proved to be impractical for our purposes.</p><p>A software implementation for the ResUNet-a models that relate to this work can be found on github 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Boundary and distance transform from segmentation mask</head><p>The boundaries and distance transform can be estimated efficiently from the segmentation ground truth mask by the python software routines listed here. The input labels is a binary image, with 1 designating on class and 0 off class pixels. The shape of the labels is two dimensional (i.e. it is a single channel image, of shape (Height,Width) -no channel dimension). In a multiclass context the segmentation mask must be provided in one-hot encoding and applied iteratively per channel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Inference results</head><p>In this section, we present classification results and error maps for all the test TOP tiles of the Potsdam ISPRS dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human color perception in the hsv space and its application in histogram generation for image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vadivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Sural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K M</forename></persName>
		</author>
		<idno type="DOI">10.1117/12.586823</idno>
		<idno>doi:10.1117/12.586823</idno>
		<ptr target="https://doi.org/10.1117/12.586823" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A novel focal tversky loss function with improved attention u-net for lesion segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07842</idno>
		<ptr target="http://arxiv.org/abs/1810.07842" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond rgb: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Segment-before-detect: Vehicle detection and classification through semantic segmentation of aerial images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefvre</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9040368</idno>
		<ptr target="http://www.mdpi.com/2072-4292/9/4/368" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Remote Sensing 9</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantic segmentation of earth observation data using multimodal and multi-scale deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06846</idno>
		<ptr target="http://arxiv.org/abs/1609.06846" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multiresolution segmentation: An optimization approach for high quality multi-scale image segmentation (ecognition)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schäpe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<ptr target="http://arxiv.org/abs/1511.00561" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02674</idno>
		<ptr target="http://arxiv.org/abs/1511.02674" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geographic object-based image analysis-towards a new paradigm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Addink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Feitosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Der Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Werff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Coillie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="180" to="191" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distance transformations in digital images. Comput. Vision Graph. Image Process</title>
		<author>
			<persName><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0734-189X(86)80047-0</idno>
		<ptr target="http://dx.doi.org/10.1016/" />
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="344" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<ptr target="http://arxiv.org/abs/1606.00915" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<ptr target="http://arxiv.org/abs/1706.05587" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="3322" to="3337" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized overlap measures for evaluation and validation in medical image analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Crum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L G</forename><surname>Hill</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/tmi/tmi25.html#CrumCH06" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1451" to="1461" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">R09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
		<idno type="DOI">10.2307/1932409</idno>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04117</idno>
		<ptr target="http://arxiv.org/abs/1608.04117" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using landsat and nighttime lights for supervised pixelbased image classification of urban land cover</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goldblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Stuhlmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Clinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Serrano-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="253" to="275" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
				<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<ptr target="http://arxiv.org/abs/1706.02677" />
		<title level="m">Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR abs/1706.02677</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on deep learning-driven remote sensing image scene understanding: Scene classification, scene retrieval and sceneguided object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3390/app9102110</idno>
		<ptr target="https://www.mdpi.com/2076-3417/9/10/2110" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<ptr target="http://arxiv.org/abs/1811.08883" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<ptr target="http://arxiv.org/abs/1703.06870" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Mask R-CNN. CoRR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4729</idno>
		<ptr target="http://arxiv.org/abs/1406.4729" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<ptr target="http://arxiv.org/abs/1608.06993" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167.ISPRS</idno>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/tests.html" />
	</analytic>
	<monogr>
		<title level="m">International society for photogrammetry and remote sensing (isprs) and bsf swissphoto: Wg3 potsdam overhead data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<ptr target="http://arxiv.org/abs/1506.02025" />
		<title level="m">Spatial transformer networks. CoRR abs/1506.02025</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bouchtiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07032</idno>
		<title level="m">Boundary loss for highly unbalanced segmentation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cropland mapping over sahelian and sudanian agrosystems: A knowledge-based approach using proba-v time series at 100-m</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Defourny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">232</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 8</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Classification and segmentation of satellite orthoimagery using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Längkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiselev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Remote Sensing 8, 329</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.4.541</idno>
		<idno>doi:10.1162/neco.1989.1.4.541</idno>
		<ptr target="https://doi.org/10.1162/neco.1989.1.4.541" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust rooftop extraction from visible band images using higher order crf</title>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Femiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4483" to="4495" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Demystifying resnet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object-based land-cover mapping with high resolution aerial photography at a county scale in midwestern usa</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="11372" to="11390" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<ptr target="http://arxiv.org/abs/1708.02002" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="78" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deligiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munteanu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9060522</idno>
		<ptr target="http://www.mdpi.com/2072-4292/9/6/522" />
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note>Remote Sensing 9</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of very-high-resolution aerial imagery and lidar with fullyconvolutional neural networks and higher-order crfs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piramanayagam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<ptr target="http://arxiv.org/abs/1411.4038" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint dictionary learning for multispectral change detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="884" to="897" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2019.04.015</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs.2019.04.015" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Semantic segmentation of aerial images with an ensemble of cnns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Segment-based land cover mapping of a suburban areacomparison of high-resolution remotely sensed datasets using classification trees and test field points</title>
		<author>
			<persName><forename type="first">L</forename><surname>Matikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1777" to="1804" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of t4 phage lysozyme</title>
		<author>
			<persName><forename type="first">B</forename><surname>Matthews</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-2795(75)90109-9</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/0005279575901099" />
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta (BBA) -Protein Structure</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="442" to="451" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04797</idno>
		<ptr target="http://arxiv.org/abs/1606.04797" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery. Remote sensing of environment</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Myint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossman-Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Weng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1145" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fully convolutional architectures for multi-class segmentation in chest radiographs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hladuvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08816</idno>
		<ptr target="http://arxiv.org/abs/1701.08816" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://distill.pub/2016/deconv-checkerboard/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantic labeling of aerial and satellite imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2009.191</idno>
		<idno>doi:10.1109/TKDE.2009.191</idno>
		<ptr target="http://dx.doi.org/10.1109/TKDE.2009.191" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Semantic labeling of high resolution aerial imagery and lidar data with fine segmentation network. Remote Sensing 10</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marinoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10050743</idno>
		<ptr target="http://www.mdpi.com/2072-4292/10/5/743" />
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">High-resolution aerial imagery semantic labeling with dense pyramid network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18113774</idno>
		<ptr target="http://www.mdpi.com/1424-8220/18/11/3774" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Dos Santos</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2015.7301382</idno>
		<idno>doi:10.1109/CVPRW.2015.7301382</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Supervised classification of multisensor remotely sensed images using a deep learning framework. Remote Sensing 10</title>
		<author>
			<persName><forename type="first">S</forename><surname>Piramanayagam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schwartzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Koehler</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10091429</idno>
		<ptr target="http://www.mdpi.com/2072-4292/10/9/1429" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco_a_00990</idno>
		<idno>pMID: 28599112</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<ptr target="http://arxiv.org/abs/1505.04597" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<ptr target="http://arxiv.org/abs/1706.05098" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<ptr target="http://arxiv.org/abs/1606.02585" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyperparameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<ptr target="http://arxiv.org/abs/1803.09820" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Skr</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03237</idno>
		<ptr target="http://arxiv.org/abs/1707.03237" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07655</idno>
		<title level="m">Deep semantic segmentation of natural and medical images: A review</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Watersheds in digital spaces: an efficient algorithm based on immersion simulations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="583" to="598" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of subdecimeter resolution images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">National-scale cropland mapping based on spectraltemporal features and outdated land cover information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Löw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Newby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Defourny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2017">2017. e0181911</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Semantic classification of urban trees using very high resolution satellite imagery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1413" to="1424" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06375</idno>
		<ptr target="http://arxiv.org/abs/1504.06375" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Transfer learning from deep features for remote sensing and poverty mapping</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00098</idno>
		<ptr target="http://arxiv.org/abs/1510.00098" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Building extraction in very high resolution imagery by dense-attention networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10111768</idno>
		<ptr target="http://www.mdpi.com/2072-4292/10/11/1768" />
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Wide residual networks</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Mapping urbanization dynamics at regional and global scales using multi-temporal dmsp/ols nighttime light data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Seto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="2320" to="2329" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10684</idno>
		<ptr target="http://arxiv.org/abs/1711.10684" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Contextually guided very-high-resolution imagery classification with semantic segments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2017.08.011</idno>
		<ptr target="https://doi.org/10.1016/j.isprsjprs" />
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="48" to="60" />
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<ptr target="http://arxiv.org/abs/1703.10593" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2017.2762307</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
