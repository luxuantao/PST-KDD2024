<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Marios</forename><surname>Anthimopoulos</surname></persName>
							<email>marios.anthimopoulos@artorg.unibe.ch</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Stergios</forename><surname>Christodoulidis</surname></persName>
							<email>ster-gios.christodoulidis@artorg.unibe.ch</email>
						</author>
						<author>
							<persName><forename type="first">Lukas</forename><surname>Ebner</surname></persName>
							<email>lukas.ebner@insel.ch</email>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Geiser</surname></persName>
							<email>thomas.geiser@insel.ch</email>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Christe</surname></persName>
							<email>andreas.christe@insel.ch</email>
						</author>
						<author>
							<persName><forename type="first">Stavroula</forename><surname>Mougiakakou</surname></persName>
							<email>stavroula.mougiakakou@artorg.unibe.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Semantic Segmentation of Pathological Lung Tissue with Dilated Fully Convolutional Networks</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ARTORG Center for Biomedical Engineering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<postCode>3008</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Emergency Medicine</orgName>
								<orgName type="institution">Bern University Hospital &quot;Inselspital&quot;</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">ARTORG Center for Biomedical Engi-neering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<postCode>3008</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">University Clinic for Pneumonology</orgName>
								<orgName type="institution" key="instit2">Bern University Hospital &quot;Inselspital&quot;</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Diagnostic</orgName>
								<orgName type="department" key="dep2">Interven-tional and Pediatric Radiology</orgName>
								<orgName type="institution">Bern University Hospital &quot;Inselspital&quot;</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Department of Diagnostic</orgName>
								<orgName type="department" key="dep2">Interven-tional and Pediatric Radiology</orgName>
								<orgName type="institution">Bern University Hospital &quot;Inselspital&quot;</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">ARTORG Center for Biomedical En-gineering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<postCode>3008</postCode>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB98CC545BC038BA975729E35854FA8E</idno>
					<idno type="DOI">10.1109/JBHI.2018.2818620</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interstitial lung disease</term>
					<term>Fully convolutional neural networks</term>
					<term>Dilated convolutions</term>
					<term>Texture segmentation</term>
					<term>Semi-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Early and accurate diagnosis of interstitial lung diseases (ILDs) is crucial for making treatment decisions, but can be challenging even for experienced radiologists. The diagnostic procedure is based on the detection and recognition of the different ILD pathologies in thoracic CT scans, yet their manifestation often appears similar. In this study, we propose the use of a deep purely convolutional neural network for the semantic segmentation of ILD patterns, as the basic component of a computer aided diagnosis (CAD) system for ILDs. The proposed CNN, which consists of convolutional layers with dilated filters, takes as input a lung CT image of arbitrary size and outputs the corresponding label map. We trained and tested the network on a dataset of 172 sparsely annotated CT scans, within a crossvalidation scheme. The training was performed in an end-toend and semi-supervised fashion, utilizing both labeled and nonlabeled image regions. The experimental results show significant performance improvement with respect to the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>disease is described as idiopathic. The diagnosis of ILD is mostly performed by radiologists and is usually based on the assessment of the different ILD pathologies in high resolution computed tomography (HRCT) thoracic scans. Early diagnosis is crucial for making treatment decisions, while misdiagnosis may lead to life-threatening complications <ref type="bibr" target="#b0">[1]</ref>. Extensive research has been conducted on the development of computeraided diagnosis (CAD) systems, which are able to support clinicians and improve their diagnostic performance. The basic characteristics of such a system are the automatic detection and recognition of the pathological lung tissue. Pathological tissue is usually manifested as various textural patterns in the CT scan. This ILD pattern recognition procedure is traditionally performed by a local texture classification scheme that slides across the images and outputs a map of pathologies, which is later used to reach a final diagnosis. For texture recognition, a great variety of handcrafted image features and machine learning classifiers have been utilized.</p><p>Recently, deep artificial neural networks (ANN) and, in particular, deep convolutional neural networks (CNNs) have gained a lot of attention after their impressive results in the ImageNet Large Scale Visual Recognition Competition in 2012 <ref type="bibr" target="#b1">[2]</ref>. Networks of this kind have existed for decades <ref type="bibr" target="#b2">[3]</ref>, but have only recently managed to achieve adequate performance mainly due to the large volumes of available annotated data, the massive parallelization capabilities of GPUs and a few design tricks. The potential benefits of deep learning techniques in medical image analysis have also been investigated recently and the first results have been promising <ref type="bibr" target="#b3">[4]</ref>. In <ref type="bibr" target="#b4">[5]</ref> , we designed, trained and tested a deep CNN as a fixed-scale, local texture classifier that outperformed traditional methods in ILD pattern classification. However, training large networks on medical images can often be challenging due to the lack of databases that are adequately sized to satisfy the needs of these models. Medical data are scarce and collecting them is a difficult and time consuming process, while their annotation has to be performed by multiple specialists to ensure its validity. To this end, in <ref type="bibr" target="#b5">[6]</ref>, we investigated the potential use of general texture image databases, in a multi-source transfer learning scheme that yielded significant improvements in performance. One remaining limitation in these works is the local nature of the classifier that requires rigorous scanning of the input image with a sliding window, and simultaneous aggregation of the results in the output. This classification scheme can be significantly time-consuming while it may also ignore less local but useful information, if the input size is not appropriately configured.</p><p>In this study, we propose the use of a deep fullyconvolutional network for the problem of ILD pattern recognition that uses dilated convolutions and is trained in an endto-end and semi-supervised manner. The proposed CNN takes as input a lung HRCT image of arbitrary size and outputs the corresponding label map, thus avoiding the limitations of a sliding window model. Additionally, the utilization of nonlabeled image regions in the learning procedure, permits robust training of larger models and proves to be particularly useful when using databases with sparse annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we provide a short review of the recent advances in deep learning for computer vision, followed by a brief overview of previous studies on ILD pattern classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep CNNs for Computer Vision</head><p>Although CNNs have existed for decades <ref type="bibr" target="#b2">[3]</ref>, they only became widely popular after their remarkable success in the ImageNet challenge of 2012 <ref type="bibr" target="#b1">[2]</ref>. The winning approach of the competition <ref type="bibr" target="#b6">[7]</ref>, also known as AlexNet, was a deep CNN with five convolutional and three dense layers, each followed by a rectified linear unit (ReLU), while increased strides were used for the max pooling and convolution operations to gradually down-sample the feature maps. Dropout <ref type="bibr" target="#b7">[8]</ref> and data augmentation were also utilized, in order to prevent overfitting. Since then, the proposed deep CNNs have led to continuous improvements in the results on ImageNet and other datasets, mainly by enhancing their architecture and by increasing their depth and width. The VGG network <ref type="bibr" target="#b8">[9]</ref> reduced the size of the kernels to 3×3, while increasing the number of layers to 19. GoogleNet <ref type="bibr" target="#b9">[10]</ref> used consecutive inception modules, where different convolutional and pooling operations are performed in parallel with their outputs merged. This approach drastically reduced the number of parameters while improving the results. ResNet <ref type="bibr" target="#b10">[11]</ref> introduced skip connections between layers which permitted the training of networks with hundreds of layers and pushed the limits of deep CNNs even further. The batch normalization (BN) technique <ref type="bibr" target="#b11">[12]</ref> also supported these developments, by regularizing and accelerating the training procedure.</p><p>Many of the already proposed CNNs have recently been adapted to perform semantic segmentation, rather than just image classification. The term semantic segmentation refers to the task of assigning a class label to every pixel of an image. A simple approach to this task is to use any fixedscale classification method under a sliding window scheme and then to aggregate the results to build a label map. However, this could be highly inefficient as local image features would have to be recalculated multiple times for adjacent positions of the input window. Luckily, the convolutional layers (with the appropriate padding) in a typical CNN produce feature maps that maintain spatial correspondence with the input image. Therefore, input images of any size can be fed to the network and each pixel can be classified, on the basis of the values of the respective feature map position. This can be achieved by utilizing convolutional layers of size 1 × 1 that serve as local dense layers and, these networks are therefore often referred to as fully convolutional (FCNs). However, the spatial correspondence between the input and output of a CNN, can be disrupted by the use of downsampling operations such, as strided pooling and convolution. Down-sampling of the feature maps is often used to increase the receptive field of the network with respect to the input, as well as to reduce the amount of computational load. In order to restore the original size of the input, researchers have used encoder-decoder architectures, where the encoder usually adopts a well-known architecture such as VGG <ref type="bibr" target="#b8">[9]</ref> and the decoder reverses the process by mapping the feature representation back to the input data space. To this end, upsampling operations and transposed convolution <ref type="bibr" target="#b12">[13]</ref> (also known as fractionally strided convolution or "deconvolution") have been used for semantic segmentation <ref type="bibr" target="#b13">[14]</ref>. Alternatively, in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>, max unpooling has been used as the inverse operation of each max pooling layer, where the pairs of pooling/unpooling layers are coupled by transferring the max indices from the encoder to the decoder. In <ref type="bibr" target="#b16">[17]</ref>, a similar architecture was proposed for biomedical image segmentation, with additional skip connections that concatenate the feature maps of an encoding layer to the feature maps of the samescale decoding layer.</p><p>Recently, some CNNs for semantic segmentation have been proposed that use dilated convolutions to increase the receptive field, instead of downsampling the feature maps. Dilated convolution, also called àtrous, is the convolution with kernels that have been dilated by inserting zero holes (àtrous in French) between the non-zero values of a kernel. This was originally proposed for efficient wavelet decomposition in a scheme also known as "algorithme àtrous" <ref type="bibr" target="#b17">[18]</ref>. Figure <ref type="figure">1</ref> shows examples of kernels with different dilation rates. Dilated convolution can increase the receptive field without increasing the number of parameters, as opposed to normal convolution. Moreover, feature maps are densely computed on the original image resolution without the need for downsampling. In <ref type="bibr" target="#b18">[19]</ref>, a CNN module with dilated convolutions was designed to aggregate multiscale contextual information and improve the performance of state-of-the-art semantic segmentation systems. The module has eight convolutional layers with exponentially increasing dilation rates (i.e. 1, 1, 2, 4, 8, 16), resulting in an exponential increase in the receptive field, while the number of parameters is only grown linearly. Similarly, expansion of the receptive field was achieved in <ref type="bibr" target="#b19">[20]</ref> by integrating dilated convolutions in a bottleneck module that was designed for efficiency. In <ref type="bibr" target="#b20">[21]</ref>, the àtrous spatial pyramid pooling (ASPP) scheme is proposed that uses multiple parallel dilated convolutional layers, in order to capture information from multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ILD Pattern Classification</head><p>Over the last twenty years, numerous approaches have been proposed for the problem of ILD pattern recognition, which is generally regarded as a texture classification problem. Most of the proposed methods involve hand-crafted texture features which are fed to machine learning classifiers, and locally recognize lung tissue within a sliding window framework. In one of the early studies <ref type="bibr" target="#b21">[22]</ref>, the adaptive multiple feature method (AMFM) was proposed, which utilizes a combination of graylevel histograms, co-occurrence and run-length matrices, as well as fractal analysis parameters. For the classification, a Bayesian classifier was used. In <ref type="bibr" target="#b22">[23]</ref>, a filter bank of Gaussian and Laplacian kernels was applied on the input images and the histogram moments of the responses were fed to a linear discriminant classifier. The simple, yet powerful, Local Binary Pattern (LBP) descriptor has also been proposed <ref type="bibr" target="#b23">[24]</ref>, combined with a k-nearest neighbors classifier. In <ref type="bibr" target="#b24">[25]</ref>, a random forest classifier was utilized that was trained on local DCT features. More recently, some proposed methods have adopted unsupervised feature extraction techniques, such as bag of features <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and sparse representation models <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>Lately, a few methods have been proposed that utilize CNNs for lung pattern classification. Most of these are still designed under a patch-wise scheme where a square patch is fed to the CNN, while the output consists of the probabilities for this patch to belong to each class. Although the strong descriptive capabilities of modern CNNs are commonly attributed to their depth, the first studies utilized rather shallow architectures. A modified RBM that resembles a convolutional layer was used in <ref type="bibr" target="#b29">[30]</ref>, whereas in <ref type="bibr" target="#b30">[31]</ref>, a CNN was proposed with one convolutional and three fully-connected layers. More recently, some attempts have also been made to utilize deeper architectures. In our previous work <ref type="bibr" target="#b4">[5]</ref>, a CNN with five convolutional and three dense layers was designed and trained on ILD data, while in <ref type="bibr" target="#b5">[6]</ref> its performance was improved using knowledge transfer from other domains. In another study <ref type="bibr" target="#b31">[32]</ref>, a CNN with three convolutional and one dense layer was fed with rotational invariant Gabor-LBP representations of lung tissue patches. Finally, in <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b33">[34]</ref>, the authors utilized well established pretrained CNN architectures such as AlexNet and GoogleNet which were further finetuned for detecting possible "presence/absence" of pathologies at a slice level. However, these architectures were designed to classify natural color images with size 224 × 224, so the authors had to resize the images and artificially generate three channels by applying different Hounsfield unit (HU) windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIALS AND METHODS</head><p>This section presents the proposed fully convolutional neural network for semantic lung tissue segmentation. Prior to this, we describe the materials used for training and testing the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Materials</head><p>For the purposes of this study, we compiled a dataset of 172 HRCT scans, each corresponding to a unique ILD or healthy subject. The dataset contains 109 cases from the publicly available multimedia database of interstitial lung diseases <ref type="bibr" target="#b34">[35]</ref> by the Geneva University Hospital (HUG), along with 63 cases from Bern University Hospital -"Inselspital" (INSEL), as collected by the authors. The scans were acquired between 2003 and 2015 using different scanners and acquisition protocols. The INSEL scans are volumetric, while the HUG scans have a 10-15mm spacing. The slice thickness is 1-2mm for both datasets.</p><p>Two experienced radiologists from INSEL annotated or reannotated ILD typical pathological patterns, as well as healthy tissue in both databases <ref type="foot" target="#foot_0">1</ref> . A lung field segmentation mask was also provided for each case. In total six types of tissue were considered: normal, ground glass opacity, micronodules, consolidation, reticulation and honeycombing. It should be emphasized that these annotations do not cover the entire lung field, but only the most typical manifestations of the listed ILD patterns. This protocol was followed in both databases since it permits the annotation of more scans for the same effort and thus increases data diversity. On the other hand, sparse annotations also introduce challenges. Non-annotated lung areas have to be excluded from both supervised training and evaluation. Another challenging characteristic of the databases is the uneven distribution of the considered classes across the cases. Table <ref type="table" target="#tab_0">I</ref> provides statistics for the entire dataset, while figure <ref type="figure">2</ref> presents a sample CT lung slice along with the given annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Methods</head><p>In this study, we propose the use of a deep purely convolutional network for the problem of lung tissue semantic segmentation. The network is inspired by <ref type="bibr" target="#b18">[19]</ref> and consists of solely convolutional layers that use dilated kernels to increase the receptive field, instead of downsampling the feature maps. This kind of network has been shown to be suitable for similar dense prediction problems that require high resolution precision. The proposed network (Fig. <ref type="figure">3</ref>) has 13 convolutional layers and a total receptive field of 287×287. Specifically, each of the first ten layers has 32 kernels of size 3×3 and dilation rates 1, 1, 2, 3, 5, 8, 13, 21, 34 and 55, respectively. We chose not to increase the dilation rates exponentially, as is commonly done, in order to avoid extreme gridding problems that have been reported in several studies <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Instead, Fig. <ref type="figure">2:</ref> A typical slice with annotations. The white border line denotes the lung field segmentation, the blue denotes healthy tissue, the purple micronodules and the red the honeycombing pattern.</p><p>we use the first terms of the Fibonacci sequence as dilation rates; this mitigates the gridding problem by providing a less steep dilation rate increase and thus denser sampling.</p><p>The output of the first 10 layers, as well as the input of the network, are concatenated, thus leading to 1+10×32 = 321 feature maps, which are passed through a dropout layer with a rate of 0.5 and fed to the rest of the network. This concatenation is allowed by the lack of pooling layers and the appropriate zero padding for each convolution and brings several benefits. It permits the aggregation of features from all different scales and levels of abstraction, while it also facilitates the flow of gradients thought the network and therefore allows faster training. The last three layers have 1×1 kernels and play the role of locally dense layers that reduce the feature dimensionality for each pixel from 321 to 128, 32 and finally 6, which is the number of classes considered. The output is converted into a probability distribution by the softmax function.</p><p>A BN layer follows each convolution and is based on the batch statistics in both training and test time. This is permitted, as the batch size is one, so there is always a full batch during inference. This approach has been proposed before, under the term instance normalization (InstanceNorm) <ref type="bibr" target="#b37">[38]</ref>, and has exhibited good performance in texture synthesis, image stylization and image to image translation <ref type="bibr" target="#b38">[39]</ref>. InstanceNorm provides invariance to intensity and contrast shifts, which makes the features adaptive for each slice and could mitigate problems caused by different CT scanners and reconstruction kernels. We also found that adding the normalized activations to the non-normalized ones, before passing them through the ReLU function (Fig. <ref type="figure">4</ref>) substantially improves the results. This instance normalization skip connection cancels the mean normalization of activations (when the trainable parameters have not been trained), while it performs a kind of feature contrast enhancement which reduces the importance of variance shift without providing complete invariance to the latter.</p><p>The network was trained by minimizing the categorical cross entropy using the Adam optimizer <ref type="bibr" target="#b39">[40]</ref> with a learning rate of 0.0001. The dense nature of the considered classification problem combined with the sparse available annotations, resulted in two issues. Firstly, large parts of the dataset were not annotated, and so could not be used for either supervised training or testing. Secondly, the distribution of the considered classes in the dataset was highly imbalanced, a fact that can be challenging for any classification method. We tackled both problems by scaling the considered loss and accuracy with appropriate weighting schemes computed for each set. All pixels corresponding to annotated areas were assigned a weight inversely proportional to the number of samples of its class in the specific set. In this way, all classes contributed equally to the considered metrics. Furthermore, we employed a semi-supervised learning technique to additionally exploit non-labeled areas of the data. We added an extra term to the supervised loss function, which corresponds to the entropy of the network's output on the areas that do not participate in the supervised learning. This entropy minimization technique has been used in different applications such as in <ref type="bibr" target="#b40">[41]</ref> yielding significant improvements in performance. Similarly in <ref type="bibr" target="#b41">[42]</ref> the technique of pseudo-labelling was introduced, where the network classifies non-annotated regions and then uses them as ground truth for fine-tuning. Semi-supervised learning techniques of this kind are based on the cluster assumption i.e. samples from the same class tend to form compact clusters. By minimizing the entropy of the network's output, the decision boundaries are driven away from areas densely populated by learning samples. If the cluster assumption holds and there is no large overlap between the classes this method may increase the network's generalization ability. It acts equivalently to manifold learning and includes self-learning as a special case, as it increases the confidence of the classifier. The influence of the semi-supervised term is controlled by an appropriate weight, which is scaled relatively with the proportion of the unlabeled regions versus the annotated ones. Hence, the loss for a pixel x with output ŷ is:</p><formula xml:id="formula_0">L(x, ŷ) =    - C i=1 w i s y i log( ŷi ), when y is given -αw u C i=1 ŷi log( ŷi ), otherwise<label>(1)</label></formula><p>where y is the true label in one-hot encoding, C is the number of classes, w i s is the supervised weight for class i (which is inversely proportional to the number of samples of the class), α is a scaler and w u the unsupervised weight.</p><p>The training procedure stops when the network does not significantly improve its performance on the validation set for 50 epochs. The performance is assessed in terms of weighted (balanced) accuracy, while an improvement is considered significant if the relative increase in performance is at least </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP AND RESULTS</head><p>In this section, we first present the setup of the experiments conducted, followed by the corresponding results that justify the algorithmic choices of the proposed method and compare it to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Given the relatively small size of the dataset with respect to the diversity of the problem, we adopted a 5-fold cross validation (CV) scheme to ensure the validity of the results.</p><p>data splitting was performed per scan, so tissue from one case was never present in more than one set. Specifically, the 172 scans of the dataset were divided into five non-overlapping sets, with one of them having 36 and the rest 34 scans. Every time a model was tested on a specific set, the rest of the data were used for training. On average over all folds, the number of slices was 2060 for training and 515 for testing. As principal performance metric, we used the balanced accuracy (Eq. 2), averaged over the five folds.</p><formula xml:id="formula_1">BACC = 1 N N i=1 c i n i (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where N is the number of classes, c i is the number of correctly classified samples of class i and n i is the total number of samples of class i. Since the slices were only sparsely annotated, the accuracy was calculated over the areas of the scans where a ground truth was available.</p><p>In order to avoid extreme class imbalances between the different sets, data splitting was performed using a simple hill climbing technique that maximizes the entropy of the class distribution for the five sets. The methods started from an arbitrary split and then randomly swapped two cases between two sets in an attempt to find a more balanced solution. If the new solution has a higher class distribution entropy (averaged over the 5 sets), we retained it and repeated the procedure until no further improvement was possible.</p><p>To minimize the number of computations and memory requirements, we discarded part of the data that lack annotations. Hence, we cropped the left and right lung on each slice and used only the ones with relevant annotations as inputs of the 2168-2194 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.  networks. This is permitted by the fully-convolutional nature of the tested networks that do not require a fixed input size.</p><p>For the cropping, we utilized the available lung mask, while a margin of 32 pixels was added on each side to provide context that could be useful to the networks. The proposed method was implemented in Python 2 using the Keras framework 3 with the Theano <ref type="bibr" target="#b42">[43]</ref> back-end. All experiments were performed under Linux OS on a machine with CPU Intel Core i7-5960X @ 3.50GHz, GPU NVIDIA GeForce Titan X, and 128 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Table II presents a comparison between different network configurations. The bold line corresponds to the proposed CNN, while the rest correspond to models that differ from the proposed in only one aspect, as specified in the first column. The rest of the columns provide the number of model parameters, the average inference time per (single-lung) slice, and the average balanced accuracy across the five validation sets.</p><p>The proposed model achieved top performance with accuracy nearly equal to 82% and inference time 58ms. The use of 64 kernels per layer instead of 32 did indeed improve the results, yet not significantly enough and with higher inference times, whereas the network with 16 kernels performed notably worse. On reducing the dilated convolutional layers from 10 to 9, we observed a relatively small reduction in the accuracy. However, we chose to keep 10 layers, since the difference in memory and time requirements was also small and because the resulting receptive field was comparable with that of the state of the art networks used for comparison. The use of semisupervised learning yielded an improvement of nearly 1.5%, with no additional requirements in computational resources. We also performed an experiment with exponential increase in the dilation rates of the consecutive layers, similarly to <ref type="bibr" target="#b18">[19]</ref> i.e. 1, 1, 2, 4, 8, 16, 32 and 64. The resulting model was smaller and faster, since 2 fewer layers were required to achieve similar receptive field, however the accuracy decreased by almost 3%. In the case where the convolutions were not dilated, the network performed poorly, because of the radical decrease of the receptive field. The accuracy of the proposed  model without any normalization was substantially poorer, probably because it could not properly handle the contrast differences among the scans caused by different CT scanners and reconstruction kernels. The use of instance normalization improved the performance by adaptively normalizing the feature contrast for each input. However, this kind of normalization also normalizes the mean intensity that could be a useful feature. By adding the InstanceNorm skip connection (Fig. <ref type="figure">4</ref>), the accuracy improved even further. We speculate this is because the mean normalization is diminished, while the resulting variance normalization is only partially invariant to contrast shifts. Finally, omitting the concatenation of the first 10 layers also resulted in significant impairment of the results, which was expected since only 32 features are considered. In Fig. <ref type="figure" target="#fig_2">5</ref> the accuracy curves for different values of w u are presented. These curves are generated by averaging over the five folds the best accuracies achieved this far by each model in each epoch. The curve for the model without the unsupervised learning was also included for comparison. The best performing configuration proved to be the one with w u = 0.1, which we utilized for the training of the proposed model.</p><p>Table <ref type="table" target="#tab_3">III</ref> presents a comparison between the proposed network and three previous studies. It has to be noted that all models used the same unsupervised weight (w u = 0.1) and whenever batch normalization was performed, this was based on batch statistics (instance normalization) since this yielded the best results. Fig. <ref type="figure" target="#fig_4">6</ref> illustrates a few segmentation results for each of the models in Table <ref type="table" target="#tab_3">III</ref>.</p><p>The first line of the table refers to our previous work <ref type="bibr" target="#b4">[5]</ref>, which has been converted into a fully convolutional network so it can accept arbitrarily sized images for input. Its low accuracy is probably due to the small receptive field (33×33) and the extensive pooling. This architecture was sufficient to describe the local texture of the 32×32 single-class patches in <ref type="bibr" target="#b4">[5]</ref>, but could not capture higher level structure that is present in the whole-lung dataset of this study. The results of the model in Fig. <ref type="figure" target="#fig_4">6</ref> show its noisy output near the lung boundaries or between patterns, where context information could be useful. Segnet <ref type="bibr" target="#b14">[15]</ref> and U-net <ref type="bibr" target="#b16">[17]</ref> yielded better results, with the latter being slightly faster and substantially more accurate. Both models have a very high number of parameters and large enough receptive fields to capture any relevant information. The superior performance of U-net could be attributed to its skip connections that allow features from the lower scales to directly contribute to its output. Indeed, Fig. <ref type="figure" target="#fig_4">6</ref> illustrates the more detailed results of U-net as opposed to the overly smoothed areas produced by Segnet. Finally, the proposed network yielded the best results, while being faster and having far fewer parameters. The output examples in Fig. <ref type="figure" target="#fig_4">6</ref> indicate that the proposed model manages to keep a better balance between fine details and smooth border among the different classes. Even thought it is really difficult to visually assess the performance of the system for the different classes, there are a few examples in Fig. <ref type="figure" target="#fig_4">6</ref> with wrong classifications on which we can comment. Firstly, parts of the bronchovascular tree in the third row were recognized as consolidation because of their similar densities, while accentuated terminal bronchial parts, that might be physiological as well, caused the erroneous classification of healthy areas into reticulation, in the first row. Some mistakes however are also attributed in the limited number of annotated classes. For example in row 6, there are emphysematic areas (dark area in the center of the lung) that have been annotated as healthy due to their similar density. Figure <ref type="figure" target="#fig_3">7</ref> shows the confusion matrix of the proposed model. As expected, many of the misclassifications occur between reticulation and honeycombing due to their similar textural appearance. Moreover, healthy tissue is often confused with reticulation probably because of the 2D sections of the bronchovascular tree that could resemble reticular patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this study, we proposed and evaluated a deep CNN for the semantic segmentation of pathological lung tissue on HRCT slices. The CNN is designed under a fully convolutional scheme and thus can handle variable input sizes, while it was trained in an end-to-end and semi-supervised fashion. The main characteristic of the proposed network is the use of dilated convolutions along with an instance variance normalization scheme, and multi-scale feature fusion. The training and testing of the network was performed using a cross validation scheme on a dataset of 172 cases, whereas the split of the dataset into folds was performed per case. The proposed network surpassed the highest performance in previous studies, and is much more efficient in terms of memory and computation. Future work includes the modification of the model to consider the 3D nature of lung patterns, and to account for the bronchovascular tree. The former could be achieved by a direct extension of the architecture to 3D, similarly to 3D U-Net <ref type="bibr" target="#b43">[44]</ref> and V-Net <ref type="bibr" target="#b44">[45]</ref> or by employing a multi-planar view aggregation scheme, also referred to as 2.5D, <ref type="bibr" target="#b45">[46]</ref>. Alternatively, a 3D post processing scheme could be used to refine the 2D segmentation output using conditional random fields or deformation models <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Finally, the result of a bronchovascular segmentation method could be utilized by the network to reduce false alarms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Fig. 1 :</head><label>31</label><figDesc>Fig. 1: Dilated convolution kernels. D denotes the dilation rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig.3: The architecture of the proposed network. Each gray box corresponds to a block like the one presented in Fig.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Accuracy curves for different values of w u .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Confusion matrix of the proposed model as calculated over the cross validation scheme. The numbers represent percentages of pixels across all validation images.</figDesc><graphic coords="7,355.70,67.18,156.79,156.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Output examples for the models of Table III. From left to right: Ground Truth, ILD-CNN, Segnet, U-net, Proposed. Each example has a different pattern annotated. From top to bottom: Healthy (Blue), Ground Glass Opacity (Purple), Micronodules (Green), Consolidation (Yellow), Reticulation (Orange) and Honeycombing (Red).</figDesc><graphic coords="8,48.96,560.58,514.06,86.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,59.29,56.07,230.39,230.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Data statistics across the considered classes i.e. Healthy (H), Ground Glass Opacity (GGO), Micronodules (MN), Consolidation (Cons), Reticulation (Ret) and Honeycombing (HC).</figDesc><table><row><cell></cell><cell>H</cell><cell>GGO</cell><cell>MN</cell><cell>Cons</cell><cell>Ret</cell><cell>HC</cell><cell>Totals</cell></row><row><cell>#Pixels×10 5</cell><cell>92.5</cell><cell>27.7</cell><cell>35.8</cell><cell>7.08</cell><cell>28.2</cell><cell>20.1</cell><cell>211.4</cell></row><row><cell>#Cases</cell><cell>66</cell><cell>82</cell><cell>15</cell><cell>46</cell><cell>81</cell><cell>47</cell><cell>172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Citation information: DOI 10.1109/JBHI.2018.2818620, IEEE Journal of Biomedical and Health Informatics</figDesc><table><row><cell>JBHI-00843-2017.R1</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison of the different network configurations</figDesc><table><row><cell>Network configuration</cell><cell>Number of parameters ×10 5</cell><cell>Average inference time ms</cell><cell>CV balanced accuracy %</cell></row><row><cell>w/o dilated convolutions</cell><cell>1.30</cell><cell>51</cell><cell>68.0</cell></row><row><cell>w/o concatenation</cell><cell>0.93</cell><cell>53</cell><cell>72.6</cell></row><row><cell>w/o InstanceNorm</cell><cell>1.29</cell><cell>38</cell><cell>77.9</cell></row><row><cell>w/o InstanceNorm skip</cell><cell>1.30</cell><cell>57</cell><cell>78.6</cell></row><row><cell>16 kernels/layer</cell><cell>0.47</cell><cell>51</cell><cell>79.2</cell></row><row><cell>Exponential dilation [19]</cell><cell>1.03</cell><cell>48</cell><cell>79.5</cell></row><row><cell>Purely supervised</cell><cell>1.30</cell><cell>58</cell><cell>80.6</cell></row><row><cell>9 dilated layers</cell><cell>1.18</cell><cell>53</cell><cell>81.3</cell></row><row><cell>Proposed</cell><cell>1.30</cell><cell>58</cell><cell>81.8</cell></row><row><cell>64 kernels/layer</cell><cell>4.23</cell><cell>82</cell><cell>82.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Comparison with previous studies</figDesc><table><row><cell></cell><cell>Number of</cell><cell>Average</cell><cell>CV balanced</cell></row><row><cell>Network</cell><cell>parameters</cell><cell>inference time</cell><cell>accuracy</cell></row><row><cell></cell><cell>×10 5</cell><cell>ms</cell><cell>%</cell></row><row><cell>ILD-CNN [5]</cell><cell>0.9</cell><cell>237</cell><cell>72.2</cell></row><row><cell>Segnet [15]</cell><cell>335</cell><cell>111</cell><cell>73.6</cell></row><row><cell>U-net [17]</cell><cell>310</cell><cell>88</cell><cell>77.5</cell></row><row><cell>Proposed</cell><cell>1.3</cell><cell>58</cell><cell>81.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>ITK-snap was used for the annotation process, http://www.itksnap.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>JBHI-00843-2017.R1</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2168-2194 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JBHI.2018.2818620, IEEE Journal of Biomedical and Health Informatics JBHI-00843-2017.R1</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manuscript received</head> March 21, 2018   <p>This research was carried out within the framework of the IntACT research project, supported by Bern University Hospital,"Inselspital" and the Swiss National Science Foundation (SNSF) under Grant 156511.</p><p>M. Anthimopoulos and S. Christodoulidis contributed equally to this work. The asterisk indicates the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The diagnosis, assessment and treatment of diffuse parenchymal lung disease in adults</title>
		<author>
			<persName><forename type="first">B</forename><surname>Society</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thorax</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<biblScope unit="page">S1</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11">Nov 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique</title>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1159" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lung pattern classification for interstitial lung diseases using a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Anthimopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christodoulidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mougiakakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1216" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multisource transfer learning with convolutional neural networks for lung pattern analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Christodoulidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anthimopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mougiakakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="84" />
			<date type="published" when="2017-01">Jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2670313" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<ptr target="http://arxiv.org/abs/1505.04597" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
	<note>in Wavelets</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno>abs/1606.02147</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computer recognition of regional lung disease patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Uppaluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Hunninghake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of respiratory and critical care medicine</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="648" to="654" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis in high resolution ct of the lungs</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Sluimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Van Waes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3081" to="3090" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quantitative analysis of pulmonary emphysema using local binary patterns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="559" to="569" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classification of interstitial lung disease patterns using local dct features and random forest</title>
		<author>
			<persName><forename type="first">M</forename><surname>Anthimopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christodoulidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mougiakakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2014-08">Aug 2014</date>
			<biblScope unit="page" from="6040" to="6043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A texton-based approach for the classification of lung parenchyma in ct images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gangeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="595" to="602" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using multiscale visual words for lung texture classification and retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI International Workshop on Medical Content-Based Retrieval for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="69" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification of diffuse lung diseases patterns by a sparse representation based method on hrct images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="5457" to="5460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiscale sparse representation of highresolution computed tomography (hrct) lung images for diffuse lung disease classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning features for tissue classification with the classification restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Workshop on Medical Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Medical image classification with convolutional neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 13th International Conference on Control Automation Robotics Vision (ICARCV)</title>
		<imprint>
			<date type="published" when="2014-12">Dec 2014</date>
			<biblScope unit="page" from="844" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiscale rotation-invariant convolutional neural networks for lung texture classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Holistic interstitial lung disease detection using deep convolutional neural networks: Multi-label learning and unordered pooling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mollura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05616</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building a reference multimedia database for interstitial lung diseases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Platon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Poletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized medical imaging and graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="238" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<idno>abs/1702.08502</idno>
		<ptr target="http://arxiv.org/abs/1702.08502" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno>abs/1705.09914</idno>
		<ptr target="http://arxiv.org/abs/1705.09914" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1701.02096</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1611.07004</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2976040.2976107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing Systems, ser. NIPS&apos;04</title>
		<meeting>the 17th International Conference on Neural Information Processing Systems, ser. NIPS&apos;04<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d u-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
	<note>in 3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving computer-aided detection using convolutional neural networks and random view aggregation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1181" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network and 3d deformable approach for tissue segmentation in musculoskeletal magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samsonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kijowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2379" to="2391" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic liver and lesion segmentation in ct using cascaded fully convolutional neural networks and 3d conditional random fields</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E A</forename><surname>Elshaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ettlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tatavarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Armbruster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danastasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
