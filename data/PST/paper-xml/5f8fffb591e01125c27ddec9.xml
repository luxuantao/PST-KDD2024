<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLAG: ADVERSARIAL DATA AUGMENTATION FOR GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-19">19 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
							<email>guohao.li@kaust.edu.sa</email>
							<affiliation key="aff1">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff1">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
							<email>taylor@usna.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">US Naval Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FLAG: ADVERSARIAL DATA AUGMENTATION FOR GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-19">19 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.09891v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation helps neural networks generalize better, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on augmenting graph topological structures by adding/removing edges, we offer a novel direction to augment in the input node feature space for better performance. We propose a simple but effective solution, FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training, and boosts performance at test time. Empirically, FLAG can be easily implemented with a dozen lines of code and is flexible enough to function with any GNN backbone, on a wide variety of large-scale datasets, and in both transductive and inductive settings. Without modifying a model's architecture or training setup, FLAG yields a consistent and salient performance boost across both node and graph classification tasks. Using FLAG, we reach state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets. We open source our implementation at https://github.com/devnkong/FLAG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) have emerged as powerful architectures for learning and analyzing graph representations. The Graph Convolutional Network (GCN) <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2016</ref>) and its variants have been applied to a wide range of tasks, including visual recognition <ref type="bibr" target="#b42">(Zhao et al., 2019;</ref><ref type="bibr" target="#b29">Shen et al., 2018)</ref>, meta-learning <ref type="bibr" target="#b10">(Garcia &amp; Bruna, 2017)</ref>, social analysis <ref type="bibr" target="#b25">(Qiu et al., 2018;</ref><ref type="bibr" target="#b20">Li &amp; Goldwasser, 2019)</ref>, and recommender systems <ref type="bibr" target="#b38">(Ying et al., 2018)</ref>. However, the training of GNNs on large-scale datasets usually suffers from overfitting, and realistic graph datasets often involve a high volume of out-of-distribution test nodes <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>, posing significant challenges for prediction problems.</p><p>One promising solution to combat overfitting in deep neural networks is data augmentation <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref>, which is commonplace in computer vision tasks. Data augmentations apply label-preserving transformations to images, such as translations and reflections. As a result, data augmentation effectively enlarges the training set while incurring negligible computational overhead. However, it remains an open problem how to effectively generalize the notion of data augmentation to GNNs. Transformations on images rely heavily on image structures, and it is challenging to design low-cost transformations that preserve semantic meaning for non-visual tasks like natural language processing <ref type="bibr" target="#b34">(Wei &amp; Zou, 2019)</ref> and graph learning. Generally speaking, graph data for machine learning comes with graph structure (or edge features) and node features. In the limited cases where data augmentation can be done on graphs, it generally focuses exclusively on the graph structure by adding/removing edges <ref type="bibr" target="#b26">(Rong et al., 2019)</ref>. To date, there is no study on how to manipulate graphs in node feature space for enhanced performance.</p><p>In the meantime, adversarial data augmentation, which happens in the input feature space, is known to boost neural network robustness and promote resistance to adversarially chosen inputs <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b23">Madry et al., 2017)</ref>. Despite the wide belief that adversarial training harms standard Preprint generalization and leads to worse accuracy <ref type="bibr" target="#b30">(Tsipras et al., 2018;</ref><ref type="bibr" target="#b0">Balaji et al., 2019)</ref>, recently a growing amount of attention has been paid to using adversarial perturbations to augment datasets and ultimately alleviate overfitting. For example, <ref type="bibr" target="#b32">Volpi et al. (2018)</ref> showed adversarial data augmentation is a data-dependent regularization that could help generalize to out-of-distribution samples, and its effectiveness has been verified in domains including computer vision <ref type="bibr" target="#b36">(Xie et al., 2020)</ref>, language understanding <ref type="bibr" target="#b43">(Zhu et al., 2019;</ref><ref type="bibr" target="#b16">Jiang et al., 2019)</ref>, and visual question answering <ref type="bibr" target="#b9">(Gan et al., 2020)</ref>. Despite the rich literature about adversarial training of GNNs for security purposes <ref type="bibr" target="#b44">(Zügner et al., 2018;</ref><ref type="bibr" target="#b4">Dai et al., 2018;</ref><ref type="bibr" target="#b1">Bojchevski &amp; Günnemann, 2019;</ref><ref type="bibr" target="#b41">Zhang &amp; Zitnik, 2020)</ref>, it remains unclear how to effectively and efficiently improve GNNs' clean accuracy using adversarial augmentation.</p><p>Present work. We propose FLAG, Free Large-scale Adversarial Augmentation on Graphs, to tackle the overfitting problem. While existing literature focuses on modifying graph structures to augment datasets, FLAG works purely in the node feature space by adding gradient-based adversarial perturbations to the input node features with graph structures unchanged. FLAG leverages "free" methods <ref type="bibr" target="#b27">(Shafahi et al., 2019)</ref> to conduct efficient adversarial training so that it is highly scalable to large-scale datasets. We verify the effectiveness of FLAG on the Open Graph Benchmark (OGB) <ref type="bibr" target="#b15">(Hu et al., 2020)</ref>, which is a collection of large-scale, realistic, and diverse graph datasets for both node and graph property prediction tasks. We conduct extensive experiments across OGB datasets by applying FLAG to prestigious GNN models, which are <ref type="bibr">GCN, GraphSAGE, GAT, and GIN (Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b31">Veličković et al., 2017;</ref><ref type="bibr" target="#b37">Xu et al., 2019)</ref> and show that FLAG brings consistent and significant improvements. For example, FLAG lifts the test accuracy of GAT on ogbn-products by an absolute value of 2.31%. DeeperGCN <ref type="bibr" target="#b21">(Li et al., 2020</ref>) is another strong baseline that achieves top performance on several OGB benchmarks. FLAG enables DeeperGCN to generalize further and reach new state-of-the-art performance on ogbg-molpcba and ogbg-ppa. FLAG is simple (adding just a dozen lines of code), general (can be directly applied to any GNN model), versatile (works in both transductive and inductive settings), and efficient (able to bring salient improvement at tractable or even no extra cost). Our main contributions are summarized as follows:</p><p>• We propose adversarial perturbations as a data augmentation in the input node feature space to efficiently boost GNNs' performance. The resulting FLAG framework is a scalable and flexible augmentation scheme for GNNs, which is easy to implement and applicable to any GNN architecture for both node and graph classification tasks. • We advance the state-of-the-art on a number of large-scale OGB datasets, often by large margins. • We provide a detailed analysis and deep insights on the effects adversarial augmentation has on GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Graph Neural Networks (GNNs). We denote a graph as G(V, E) with initial node features x v for v ∈ V and edge features e uv for (u, v) ∈ E. GNNs are built on graph structures to learn representation vectors h v for every node v ∈ V and a vector h G for the entire graph G. The k-th iteration of message passing, or the k-th layer of GNN forward computation is:</p><formula xml:id="formula_0">h (k) v = COMBINE (k) h (k−1) v , AGGREGATE (k) h (k−1) v , h (k−1) u , euv : u ∈ N (v) ,<label>(1)</label></formula><p>where h</p><formula xml:id="formula_1">(k)</formula><p>v is the embedding of node v at the k-th layer, e uv is the feature vector of the edge between node u and v, N (v) is node v's neighbor set, and h (0) v = x v . COMBINE(•) and AGGREGATE(•) are functions parameterized by neural networks. To simplify, we view the holistic message passing pipeline as an end-to-end function f θ (•) built on graph G:</p><formula xml:id="formula_2">H (K) = f θ (X; G), (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where X is the input node feature matrix. After K rounds of message passing we get the final-layer node matrix H (K) . To obtain the representation of the entire graph h G , the permutation-invariant READOUT(•) function pools node features from the final iteration K as:</p><formula xml:id="formula_4">h G = READOUT h (K) v | v ∈ V ,<label>(3) Preprint</label></formula><p>Additionally from the spectral convolution point of view, the k-th layer of GCN is:</p><formula xml:id="formula_5">I + D − 1 2 AD − 1 2 → D− 1 2 Ã D− 1 2 , S = D− 1 2 Ã D− 1 2 ,<label>(4)</label></formula><formula xml:id="formula_6">H (k+1) = σ SH (k) Θ (k) ,<label>(5)</label></formula><p>where H (k) is the node feature matrix of the k-th layer with H 0 = X, Θ k is the trainable weight matrix of layer k, and σ is the activation function. D and A denote the diagonal degree matrix and adjacency matrix, respectively. Here, we view S as a normalized adjacency matrix with self-loops added.</p><p>Adversarial training. Standard adversarial training seeks to solve the min-max problem as:</p><formula xml:id="formula_7">min θ E (x,y)∼D max δ p ≤ L (f θ (x + δ), y) , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where D is the data distribution, y is the label, • p is some p -norm distance metric, is the perturbation budget, and L is the objective function. <ref type="bibr" target="#b23">Madry et al. (2017)</ref> showed that this saddlepoint optimization problem could be reliably tackled by Stochastic Gradient Descent (SGD) for the outer minimization and Projected Gradient Descent (PGD) for the inner maximization. In practice, the typical approximation of the inner maximization under an l ∞ -norm constraint is as follows,</p><formula xml:id="formula_9">δ t+1 = Π δ ∞≤ (δ t + α • sign (∇ δ L (f θ (x + δ t ), y))) ,<label>(7)</label></formula><p>where perturbation δ is updated iteratively, and Π δ ∞ ≤ performs projection onto the -ball in the l ∞ -norm. For maximum robustness, this iterative updating procedure usually loops M times, which makes PGD computationally expensive. While there are M forward and backward steps within the process, θ gets updated just once using the final δ M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD: FLAG</head><p>Adversarial training is a form of data augmentation. By hunting for and stamping out small perturbations that cause the classifier to fail, one may hope that adversarial training should be beneficial to standard accuracy <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b30">Tsipras et al., 2018;</ref><ref type="bibr" target="#b24">Miyato et al., 2018)</ref>. With an increasing amount of attention paid to leverage adversarial training for better clean performance in varied domains <ref type="bibr" target="#b36">(Xie et al., 2020;</ref><ref type="bibr" target="#b43">Zhu et al., 2019;</ref><ref type="bibr" target="#b9">Gan et al., 2020)</ref>, we conduct the first study on how to effectively generalize GNNs using adversarial data augmentation. Here we introduce FLAG, Free Large-scale Adversarial Augmentation on Graphs, to best exploit the power of adversarial augmentation. Note that our method differs from other augmentations for graphs in that it happens in the input node feature space.</p><p>Augmentation for "free". We leverage the "free" adversarial training method <ref type="bibr" target="#b27">(Shafahi et al., 2019)</ref> to craft adversarial data augmentations. PGD is a strong but inefficient way to solve the inner maximization of (6). While computing the gradient for the perturbation δ, free training simultaneously computes the model parameter θ's gradient. This "free" parameter gradient is then used to compute the ascent step. The authors proposed to train on the same minibatch M times in a row to simulate the inner maximization in (6), while compensating by performing M times fewer epochs of training. The resulting algorithm yields accuracy and robustness competitive with standard adversarial training, but with the same runtime as clean training.</p><p>Gradient accumulation. When doing "free" adversarial training, the inner/adversarial loop is usually run M times, each time computing both the gradient for δ t and θ t−1 . Rather than updating the model parameters in each loop, <ref type="bibr" target="#b40">Zhang et al. (2019)</ref> proposed to accumulate the gradients for θ t−1 during the inner loop and applied them all at once during the outer/parameter update. The same idea was used by <ref type="bibr" target="#b43">Zhu et al. (2019)</ref>, who proposed FreeLB to tackle this optimization issue on language understanding tasks. FreeLB ran multiple PGD steps to craft adversaries, and meanwhile accumulated the gradients ∇ θ L of model parameters. The gradient accumulation behavior can be approximated as optimizing the objective below: </p><formula xml:id="formula_10">min θ E (x,y)∼D 1 M M −1 t=0 max δt∈It L (f θ (x + δ t ) , y) ,<label>(8)</label></formula><formula xml:id="formula_11">δ 0 ← U (−α, α)</formula><p>initialize from uniform distribution 4:</p><formula xml:id="formula_12">g 0 ← 0 5: for t = 1 . . . M do 6: g t ← g t−1 + 1 M • ∇ θ L (f θ (X + δ t−1 ; G), y) θ gradient accumulation 7: g δ ← ∇ δ L (f θ (X + δ t−1 ; G) , y) 8: δ t ← δ t−1 + α • g δ / g δ F perturbation δ gradient ascent 9:</formula><p>end for 10:</p><formula xml:id="formula_13">θ ← θ − τ • g M model parameter θ gradient descent 11: end for where I t = B x+δ0 (αt) ∩ B x ( ).</formula><p>The gradient accumulation algorithm largely empowers FLAG to further improve GNN with efficient gradient usage for optimization.</p><p>Unbounded attack. Usually on images, the inner maximization is a constrained optimization problem. The largest perturbation one can add is bounded by the hyperparameter , typically 8/255 under the l ∞ -norm. This encourages the visual imperceptibility of the perturbations, thus making defenses realistic and practical. However, graph node features or language word embeddings do not have such straightforward semantic meanings, which makes the selection of highly heuristic. In light of the positive effect of large perturbations on generalization <ref type="bibr" target="#b32">(Volpi et al., 2018)</ref>, and also to simplify hyperparameter search, FLAG drops the projection step when performing the inner maximization. Note that, although the perturbation is not bounded by an explicit , it is still implicitly bounded in the furthest distance that δ can reach, i.e. the step size α times the number of ascending steps M .</p><p>Biased perturbation for node classification. Conventional conv nets treat each test sample independently during inference, whereas this is not the case in transductive graph learning scenarios. When classifying one target node, messages from the whole k-hop neighborhood are aggregated and combined into its embedding. It is natural to believe that a further neighbor should have lower impact, i.e. higher smoothness, on the final decision of the target node, which can also be intuitively reflected by the message passing view of GNNs in (1). To promote more invariance for furtheraway neighbors when doing node classification, we perturb unlabeled nodes with larger step sizes α u than α l for target nodes. We show the effectiveness of this biased perturbation in the ablation study section.</p><p>The overall augmentation pipeline is presented in Algorithm 1. Note that when doing transductive node classification, we use diverse step sizes α l and α u to craft adversarial augmentation for target and unlabeled nodes, respectively. In the following sections, we verify FLAG's effectiveness through extensive experiments. In addition, we provide detailed discussions for a deep understanding of the effects of adversarial augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we demonstrate FLAG's effectiveness through extensive experiments on the Open Graph Benchmark (OGB), which consists of a wide range of challenging large-scale datasets. <ref type="bibr" target="#b28">Shchur et al. (2018)</ref>; <ref type="bibr" target="#b7">Errica et al. (2019);</ref><ref type="bibr" target="#b6">Dwivedi et al. (2020)</ref> showed that traditional graph datasets suffered from problems such as unrealistic and arbitrary data splits, highly limited data sizes, nonrigorous evaluation metrics, and common neglect of cross-validation, etc. In order to empirically study FLAG's effects in a fair and reliable manner, we conduct experiments on the newly released OGB <ref type="bibr" target="#b15">(Hu et al., 2020)</ref> datasets, which have tackled those major issues and brought more realistic challenges to the graph research community. We refer readers to <ref type="bibr" target="#b15">Hu et al. (2020)</ref>  Unless otherwise stated, all of the baseline test statistics come from the official OGB leaderboard website, and we conduct all of our experiments using publicly released implementations without touching the original model architecture or training setup. We report mean and std values from ten runs with different random seeds. Following common practice on this benchmark, we report the test performance associated with the best validation result. We choose the prestigious GCN, GraphSAGE, GAT, and GIN as our baseline models. In addition, we apply FLAG to the recent DeeperGCN model to demonstrate effectiveness. Our implementation always uses M = 3 ascent steps for simplicity. Following <ref type="bibr" target="#b13">Goodfellow et al. (2014)</ref>; <ref type="bibr" target="#b23">Madry et al. (2017)</ref>, we use sign(•) for gradient normalization. We leave exhaustive hyperparameter and normalization search for future research. All training hyperparameters and evaluation results can be found in the Appendix.</p><p>Node Property Prediction. We summarize the results of node classification in Table <ref type="table" target="#tab_1">1</ref>. On ogbn-products, GraphSAGE, GAT, and DeeperGCN all receive promising results with FLAG. We adopt neighbor sampling <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> as the mini-batch algorithm for GraphSAGE and GAT to make the experiments scalable. For DeeperGCN, we follow the original setup by <ref type="bibr" target="#b21">Li et al. (2020)</ref> to randomly split the graph into clusters. Notably, FLAG yields a 2.31% test accuracy lift for GAT, making GAT competitive on the ogbn-products dataset. Because the graph size of ogbn-proteins is small, all models are trained in a full-batch manner. From Table <ref type="table" target="#tab_1">1</ref> we can see that FLAG further enhances the performance of DeeperGCN but harms that of GCN and Graph-SAGE. Considering the dataset's specialty of not having input node features, we provide detailed discussions on the effect of different node feature constructions later. We also do full-batch training on ogbn-arxiv, where FLAG enables GAT and DeeperGCN to reach 73.71% and 72.14% accuracy. Note that the GAT baseline is from the DGL <ref type="bibr" target="#b33">(Wang et al., 2019)</ref> implementation, which differs from vanilla GAT with batch norm and label propagation incorporated. We reveal batch norm's influence in the discussion. ogbn-mag is a heterogeneous network where only "paper" nodes come with node features. We use the neighbor sampling mini-batch algorithm to train R-GCN and report its results in the right part of Table <ref type="table" target="#tab_2">2</ref>. Surprisingly, FLAG can also directly bring nontrivial accuracy improvement without special designs for heterogeneous graphs, which demonstrates its versatility.</p><p>Graph Property Prediction. Table <ref type="table" target="#tab_3">3</ref> summarizes the test scores of GCN, GIN, and DeeperGCN on all four OGB graph property prediction datasets. "Virtual" means the model is augmented with virtual nodes <ref type="bibr" target="#b22">(Li et al., 2017;</ref><ref type="bibr" target="#b12">Gilmer et al., 2017;</ref><ref type="bibr" target="#b15">Hu et al., 2020)</ref>. As adversarial perturbations are crafted by gradient ascent, it would be unnatural to perturb discrete input node features. Following  <ref type="formula">2019</ref>), we firstly project discrete node features into the continuous space and then adversarially augment the hidden embeddings. On ogbg-molhiv, FLAG yields notable improvements, but when GCN has already been hurt by virtual nodes, FLAG appears to exaggerate the harm. Note that the test results on ogbg-molhiv all have relatively high variance compared with others, where randomness in the test result is more severe. On ogbg-molpcba, GIN-Virtual with FLAG receives an absolute value 1.31% test AP value increase, and DeeperGCN is further enhanced to retain its SOTA performance. On ogbg-ppa, FLAG further generalizes DeeperGCN and registers a new state-of-the-art test accuracy of 77.52%. On ogbg-code, FLAG boosts GCN-Virtual to a state-of-the-art test F1 score of 33.16. Besides node classification, FLAG's strong effects on graph classification prove its high versatility. In most cases, FLAG works well with virtual node augmentation to further enhance graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDIES AND DISCUSSIONS</head><p>Effects of biased perturbation. From the left part of Table <ref type="table" target="#tab_2">2</ref>, we see that there is a salient increase of accuracy when using a larger perturbation on unlabeled nodes, which verifies the effectiveness of biased perturbations.</p><p>Comparison with other adversarial training methods. The right part of Table <ref type="table" target="#tab_4">4</ref> shows GAT's performance with different adversarial augmentations. For PGD and Free, we compute 8 ascent steps for the inner-maximization, while for FreeLB and FLAG we compute 3 steps. FLAG outperforms all other methods by a large margin.</p><p>Compatibility with mini-batch methods. Graph mini-batch algorithms are critical to training GNNs on large-scale datasets. We test how different algorithms will work with adversarial data augmentation with GraphSAGE as the backbone. From the left part of sampling <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> and GraphSAINT <ref type="bibr" target="#b39">(Zeng et al., 2019</ref>) can all work with FLAG to further boost performance, while Cluster <ref type="bibr" target="#b3">(Chiang et al., 2019)</ref> suffers an accuracy drop.</p><p>Compatibility with batch norm. The left part of Table <ref type="table" target="#tab_6">5</ref> shows that batch norm works to generalize GAT, and FLAG works to push the improvement further. In the computer vision domain, <ref type="bibr" target="#b36">Xie et al. (2020)</ref> proposed a new batch norm method that makes adversarial training further generalize largescale CNN models. As there is growing attention on using batch norm on GNNs, it will also be interesting to see how to synergize adversarial augmentation with batch norm in future architectures.</p><p>Compatibility with dropout. Dropout is widely used in GNNs. The right part of Table <ref type="table" target="#tab_6">5</ref> shows that, when trained without dropout, GAT accuracy drops steeply by a large margin. What's more, FLAG can further generalize GNN models together with dropout, similar to the phenomenon of image augmentations.</p><p>Towards going "free". FLAG introduces tractable extra training overhead. We empirically show that, when we decrease the total training epochs to make it as fast as the standard GNN training pipeline, FLAG still brings significant performance gains. The left part of Table <ref type="table" target="#tab_2">2</ref> shows that FLAG with fewer epochs still generalizes the baseline. Empirically, on a single Nvidia RTX 2080Ti, 100epoch vanilla GAT takes 88 mins, while FLAG in Table <ref type="table" target="#tab_2">2</ref> takes 91 mins. We note that heuristics like early stopping and cyclic learning rates can further accelerate the adversarial training process <ref type="bibr" target="#b35">(Wong et al., 2020)</ref>, so there are abundant opportunities for further research on adversarial augmentation at lower or even no cost.</p><p>Towards going deep. Over-smoothing stops GNNs from going deep. FLAG shows its ability to boost both shallow and deep baselines, e.g. GCN and DeeperGCN. In the left part of Figure <ref type="figure" target="#fig_0">1</ref>, we show FLAG's effects on generalization when a GNN goes progressively deeper. The experiments are conducted on ogbn-arxiv with GraphSAGE as the backbone, where a consistent improvement is evident.</p><p>What if there's no node feature? One natural question can be raised: what if no input node features are provided? ogbn-proteins is a dataset without input node features. <ref type="bibr" target="#b15">Hu et al. (2020)</ref> proposed to average incoming edge features to obtain initial node features, while <ref type="bibr" target="#b21">Li et al. (2020)</ref> used summation and achieved competitive results. Note that the GCN and GraphSAGE baselines in Table <ref type="table" target="#tab_1">1</ref> use the "mean" node features as input and suffer an accuracy drop with FLAG; DeeperGCN leverages the "sum" and gets further improved. Interestingly, when DeeperGCN is trained with "mean" node features, it receives high invariance, so that even large magnitude perturbations will not change its result. The diverse behavior of adversarial augmentation implies the importance of node feature construction method selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">WHERE DOES THE BOOST COME FROM?</head><p>It is now widely believed that model robustness appears to be at odds with clean accuracy. Despite the proliferation of literature in using adversarial data augmentation to promote standard performance, it is still unsettled where the boost or detriment of adversarial training comes from.</p><p>Data distribution is the key. We conjecture that the diverse effects of adversarial training in different domains stem from differences in the input data distribution rather than model architectures.  CNNs could benefit from adversarial augmentations on MNIST, where the pixel values are closer to discrete distribution than other more natural image datasets. All these observations are consistent with our conjecture that data distribution has more to do with the effect of adversarial augmentation. Like one-hot word embeddings for language models, input node features usually come from discrete spaces, e.g., the bag-of-words binary features in ogbn-products. We believe that adversarial augmentation on discrete vs. continuous input features will lead to different effects. To illustrate, we provide a simple example on the Cora <ref type="bibr" target="#b11">(Getoor, 2005)</ref> dataset. We choose FGSM to craft adversarial augmentation for a GCN. By adding Gaussian noise with standard deviation σ, we simulate node features drawn from a continuous distribution. The result is summarized in the right part of Figure <ref type="figure" target="#fig_0">1</ref>. When σ = 0, the discrete distribution of node features persists. At this moment, a GCN with adversarial augmentation outperforms the non-augmented model. With increased noise level σ, the features are continuously distributed with large support and FGSM starts to harm the clean accuracy, which validates our conjecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Existing graph regularizers mainly focus on augmenting graph structures by modifying edges <ref type="bibr" target="#b26">(Rong et al., 2019;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b2">Chen et al., 2018)</ref>. We propose to effectively augment graph data using adversarial perturbations. On large-scale image classification tasks, <ref type="bibr" target="#b36">Xie et al. (2020)</ref> leveraged adversarial perturbations, along with new batch norm methods, to augment data. <ref type="bibr" target="#b43">Zhu et al. (2019)</ref>; <ref type="bibr" target="#b16">Jiang et al. (2019)</ref> added adversarial perturbations in the embedding space and generalized language models further in the fine-tuning phase. <ref type="bibr" target="#b9">Gan et al. (2020)</ref> showed that VQA model accuracy was further improved by adversarial augmentation. To clarify, FLAG is intrinsically different from the previous graph adversarial training methods <ref type="bibr" target="#b8">(Feng et al., 2019;</ref><ref type="bibr" target="#b5">Deng et al., 2019;</ref><ref type="bibr" target="#b17">Jin &amp; Zhang, 2019)</ref>. <ref type="bibr" target="#b8">Feng et al. (2019)</ref> proposed to reinforce local smoothness to make embeddings within communities similar. All three methods assigned pseudo-labels to test nodes during training time and utilized virtual adversarial training <ref type="bibr" target="#b24">(Miyato et al., 2018)</ref> to make test node predictions similar to their pseudo-labels. This makes them workable for semi-supervised settings, but not for inductive tasks. Besides the original classification loss term, they all introduced KL loss into the final objective functions, which would at least double the GPU memory usage and make training less efficient and less scalable. In contrast, FLAG requires minimal extra space overhead and can directly work in the original training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We propose FLAG (Free Large-scale Adversarial Augmentation on Graphs), a simple, scalable, and general data augmentation method for better GNN generalization. Like widely-used image augmentations, FLAG can be easily incorporated into any GNN training pipeline. FLAG yields consistent improvement over a range of GNN baselines, and reaches state-of-the-art performance on the large-scale ogbg-molpcba, ogbg-ppa, and ogbg-code datasets. Besides extensive experiments, we also provide conceptual analysis to validate adversarial augmentation's different behavior on varied data types. The effects of adversarial augmentation on generalization are still not entirely understood, and we think this is a fertile space for future exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Test accuracy on ogbn-arxiv. Right: Test accuracy on the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 FLAG: Free Large-scale Adversarial Augmentation on Graphs Require: Graph G = (V, E); input feature matrix X; learning rate τ ; ascent steps M ; ascent step size α; training epochs N ; forward function on graph f θ (•) denoted in (2); L(•) as objective function. We omit the READOUT(•) function in (3) for the inductive scenario here. 1: Initialize θ 2: for epoch = 1 . . . N do</figDesc><table><row><cell>3:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>for detailed information on the OGB datasets. Node property prediction test performance on ogbn-products, ogbn-proteins, and ogbn-arxiv datasets. Blank denotes no statistics on the leaderboard.</figDesc><table><row><cell></cell><cell cols="3">ogbn-products ogbn-proteins ogbn-arxiv</cell></row><row><cell>Backbone</cell><cell>Test Acc</cell><cell>Test ROC-AUC</cell><cell>Test Acc</cell></row><row><cell>GCN</cell><cell>-</cell><cell>72.51±0.35</cell><cell>71.74±0.29</cell></row><row><cell>+FLAG</cell><cell>-</cell><cell>71.71±0.50</cell><cell>72.04±0.20</cell></row><row><cell>GraphSAGE</cell><cell>78.70±0.36</cell><cell>77.68 ±0.20</cell><cell>71.49±0.27</cell></row><row><cell>+FLAG</cell><cell>79.36±0.57</cell><cell>76.57±0.75</cell><cell>72.19±0.21</cell></row><row><cell>GAT</cell><cell>79.45±0.59</cell><cell>-</cell><cell>73.65±0.11</cell></row><row><cell>+FLAG</cell><cell>81.76±0.45</cell><cell>-</cell><cell>73.71±0.13</cell></row><row><cell>DeeperGCN</cell><cell>80.98±0.20</cell><cell>85.80±0.17</cell><cell>71.92±0.16</cell></row><row><cell>+FLAG</cell><cell>81.93±0.31</cell><cell>85.96±0.27</cell><cell>72.14±0.19</cell></row><row><cell cols="2">ogbn-products</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>Test Acc</cell><cell></cell><cell>ogbn-mag</cell></row><row><cell>GAT</cell><cell>79.45±0.59</cell><cell>Backbone</cell><cell>Test Acc</cell></row><row><cell>+FLAG</cell><cell>80.64±0.74</cell><cell>R-GCN</cell><cell>46.78±0.67</cell></row><row><cell>+FLAG  †</cell><cell>81.29±0.39</cell><cell>+FLAG</cell><cell>47.37±0.48</cell></row><row><cell>+FLAG  ‡</cell><cell>81.76±0.45</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Left: Test performance on ogbn-products with GAT as baseline. denotes model trained in N/M epochs; † denotes α u = α l ; ‡ denotes α u = 2α l . Right: Test performance on the heterogeneous OGB node property prediction dataset ogbn-mag.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Graph property test performance on ogbg-molhiv, ogbg-molpcba, ogbg-ppa, and ogbg-code datasets. denotes state-of-the-art performance on the OGB leaderboard; denotes the existence of virtual nodes; blank denotes no statistics on the leaderboard.</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">ogbg-molhiv ogbg-molpcba ogbg-ppa ogbg-code</cell></row><row><cell>Backbone</cell><cell cols="2">Test ROC-AUC</cell><cell>Test AP</cell><cell></cell><cell>Test Acc</cell><cell>Test F1</cell></row><row><cell>GCN</cell><cell></cell><cell>76.06±0.97</cell><cell cols="2">20.20±0.24</cell><cell>68.39±0.34</cell><cell>31.63±0.18</cell></row><row><cell>+FLAG</cell><cell></cell><cell>76.83±1.02</cell><cell cols="2">21.16±0.17</cell><cell>68.38±0.47</cell><cell>32.09±0.19</cell></row><row><cell>GCN-Virtual</cell><cell></cell><cell>75.99±1.19</cell><cell cols="2">24.24±0.34</cell><cell>68.57±0.61</cell><cell>32.63±0.13</cell></row><row><cell>+FLAG</cell><cell></cell><cell>75.45±1.58</cell><cell cols="2">24.83±0.37</cell><cell>69.44±0.52</cell><cell>33.16 ±0.25</cell></row><row><cell>GIN</cell><cell></cell><cell>75.58±1.40</cell><cell cols="2">22.66±0.28</cell><cell>68.92±1.00</cell><cell>31.63±0.20</cell></row><row><cell>+FLAG</cell><cell></cell><cell>76.54±1.14</cell><cell cols="2">23.95±0.40</cell><cell>69.05±0.92</cell><cell>32.41±0.40</cell></row><row><cell>GIN-Virtual</cell><cell></cell><cell>77.07±1.49</cell><cell cols="2">27.03±0.23</cell><cell>70.37±1.07</cell><cell>32.04±0.18</cell></row><row><cell>+FLAG</cell><cell></cell><cell>77.48±0.96</cell><cell cols="2">28.34±0.38</cell><cell>72.45±1.14</cell><cell>32.96±0.36</cell></row><row><cell>DeeperGCN</cell><cell></cell><cell>78.58±1.17</cell><cell cols="2">27.81 ±0.38</cell><cell>77.12±0.71</cell><cell>-</cell></row><row><cell>+FLAG</cell><cell></cell><cell>79.42±1.20</cell><cell cols="2">28.42 ±0.43</cell><cell>77.52 ±0.69</cell><cell>-</cell></row><row><cell cols="2">Backbone GraphSAGE w/ NS +FLAG GraphSAGE w/ Cluster +FLAG GraphSAGE w/ SAINT +FLAG</cell><cell cols="2">ogbn-products Test Acc 78.70±0.36 79.36±0.57 78.97±0.33 78.60±0.27 79.08±0.24 79.60±0.19</cell><cell cols="2">Backbone GAT GAT+PGD GAT+Free GAT+FreeLB GAT+FLAG</cell><cell>ogbn-products Test Acc 79.45±0.59 80.96±0.41 79.42±0.84 81.28±0.73 81.76±0.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Left: Test accuracy on ogbn-products with GraphSAGE trained with diverse minibatch algorithms. Right: Test performance on ogbn-products with GAT trained with different adversarial augmentations. Jin &amp; Zhang (2019); Zhu et al. (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>, we see that neighbor</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Left: Test Accuracy on the ogbn-arxiv dataset. Right: Test Accuracy on the ogbn-products dataset.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A FLAG PYTORCH IMPLEMENTATION #M as ascent steps, alpha as ascent step size #X denotes input node features, y denotes labels def flag <ref type="bibr">(model, X, y, optimizer, criterion, M, alpha)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets</title>
		<author>
			<persName><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08051</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced methods for knowledge discovery from complex data</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="189" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent adversarial training of graph convolution networks</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Hongwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Learning and Reasoning with Graph-Structured Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks forpolitical perspective detection in news media</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName><forename type="first">Junying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial training for free!</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Amin Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3358" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Zico</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03994</idno>
		<title level="m">Fast is better than free: Revisiting adversarial training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Zhanxing Zhu, and Bin Dong. You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00877</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gnnguard: Defending graph neural networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08149</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
