<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Nonnegative Matrix Approximations with Bregman Divergences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
							<email>inderjit@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">The Univ. of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
							<email>suvrit@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">The Univ. of Texas at Austin Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Nonnegative Matrix Approximations with Bregman Divergences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A67B7CB88AD111AD15D7EF30631A78F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nonnegative matrix approximation (NNMA) is a recent technique for dimensionality reduction and data analysis that yields a parts based, sparse nonnegative representation for nonnegative input data. NNMA has found a wide variety of applications, including text analysis, document clustering, face/image recognition, language modeling, speech processing and many others. Despite these numerous applications, the algorithmic development for computing the NNMA factors has been relatively deficient. This paper makes algorithmic progress by modeling and solving (using multiplicative updates) new generalized NNMA problems that minimize Bregman divergences between the input matrix and its lowrank approximation. The multiplicative update formulae in the pioneering work by Lee and Seung [11]  arise as a special case of our algorithms. In addition, the paper shows how to use penalty functions for incorporating constraints other than nonnegativity into the problem. Further, some interesting extensions to the use of "link" functions for modeling nonlinear relationships are also discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nonnegative matrix approximation (NNMA) is a method for dimensionality reduction and data analysis that has gained favor over the past few years. NNMA has previously been called positive matrix factorization <ref type="bibr" target="#b12">[13]</ref> and nonnegative matrix factorization <ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b11">[12]</ref>. Assume that a 1 , . . . , a N are N nonnegative input (M -dimensional) vectors. We organize these vectors as the columns of a nonnegative data matrix A a 1 a 2 . . . a N .</p><p>NNMA seeks a small set of K nonnegative representative vectors b 1 , . . . , b K that can be nonnegatively (or conically) combined to approximate the input vectors a i . That is,</p><formula xml:id="formula_0">a n ≈ K k=1 c kn b k , 1 ≤ n ≤ N,</formula><p>where the combining coefficients c kn are restricted to be nonnegative. If c kn and b k are unrestricted, and we minimize n a n -Bc n 2 , the Truncated Singular Value Decomposition (TSVD) of A yields the optimal b k and c kn values. If the b k are unrestricted, but the coefficient vectors c n are restricted to be indicator vectors, then we obtain the problem of hard-clustering (See <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Chapter 8]</ref> for related discussion regarding different constraints on c n and b k ).</p><p>In this paper we consider problems where all involved matrices are nonnegative. For many practical problems nonnegativity is a natural requirement. For example, color intensities, chemical concentrations, frequency counts etc., are all nonnegative entities, and approximating their measurements by nonnegative representations leads to greater interpretability. NNMA has found a significant number of applications, not only due to increased interpretability, but also because admitting only nonnegative combinations of the b k leads to sparse representations. This paper contributes to the algorithmic advancement of NNMA by generalizing the problem significantly, and by deriving efficient algorithms based on multiplicative updates for the generalized problems. The scope of this paper is primarily on generic methods for NNMA, rather than on specific applications. The multiplicative update formulae in the pioneering work by Lee and Seung <ref type="bibr" target="#b10">[11]</ref> arise as a special case of our algorithms, which seek to minimize Bregman divergences between the nonnegative input A and its approximation. In addition, we discuss the use penalty functions for incorporating constraints other than nonnegativity into the problem. Further, we illustrate an interesting extension of our algorithms for handling non-linear relationships through the use of "link" functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problems</head><p>Given a nonnegative matrix A as input, the classical NNMA problem is to approximate it by a lower rank nonnegative matrix of the form BC, where</p><formula xml:id="formula_1">B = [b 1 , ..., b K ] and C = [c 1 , ..., c N ] are themselves nonnegative. That is, we seek the approximation, A ≈ BC, where B, C ≥ 0.<label>(2.1)</label></formula><p>We judge the goodness of the approximation in (2.1) by using a general class of distortion measures called Bregman divergences. For any strictly convex function ϕ : S ⊆ R → R that has a continuous first derivative, the corresponding Bregman divergence D ϕ : S × int(S) → R + is defined as D ϕ (x, y) ϕ(x) -ϕ(y) -∇ϕ(y)(x -y), where int(S) is the interior of set S <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Bregman divergences are nonnegative, convex in the first argument and zero if and only if x = y. These divergences play an important role in convex optimization <ref type="bibr" target="#b1">[2]</ref>. For the sequel we consider only separable Bregman divergences, i.e., D ϕ (X, Y ) = ij D ϕ (x ij , y ij ). We further require x ij , y ij ∈ domϕ ∩ R + .</p><p>Formally, the resulting generalized nonnegative matrix approximation problems are: min</p><formula xml:id="formula_2">B, C≥0 D ϕ (BC, A) + α(B) + β(C),<label>(2.2)</label></formula><p>min</p><formula xml:id="formula_3">B, C≥0 D ϕ (A, BC) + α(B) + β(C). (2.</formula><p>3)</p><p>The functions α and β serve as penalty functions, and they allow us to enforce regularization (or other constraints) on B and C. We consider both (2.2) and (2.3) since Bregman divergences are generally asymmetric. Table <ref type="table">1</ref> gives a small sample of NNMA problems to illustrate the breadth of our formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithms</head><p>In this section we present algorithms that seek to optimize (2.2) and (2.3). Our algorithms are iterative in nature, and are directly inspired by the efficient algorithms of Lee and Seung <ref type="bibr" target="#b10">[11]</ref>. Appealing properties include ease of implementation and computational efficiency.</p><formula xml:id="formula_4">Divergence D ϕ ϕ α β Remarks A -BC 2 F 1 2 x 2 0 0 Lee and Seung [11, 12] A -BC 2 F 1 2 x 2 0 λ1 T C1 Hoyer [10] W ⊙ (A -BC) 2 F 1 2 x 2 0 0 Paatero and Tapper [13] KL(A, BC) x log x 0 0 Lee and Seung [11] KL(A, W BC) x log x 0 0 Guillamet et al. [9] KL(A, BC) x log x c1B T B1 -c ′ C 2 F Feng et al. [8] D ϕ (A, W 1 BCW 2 ) ϕ(x) α(B) β(C)</formula><p>Weighted NNMA (new) Table <ref type="table">1</ref>: Some example NNMA problems that may be obtained from (2.3). The corresponding asymmetric problem (2.2) has not been previously treated in the literature. KL(x, y) denotes the generalized KL-Divergence = i x i log xi yi -x i + y i (also called I-divergence).</p><p>Note that the problems (2.2) and (2.3) are not jointly convex in B and C, so it is not easy to obtain globally optimal solutions in polynomial time. Our iterative procedures start by initializing B and C randomly or otherwise. Then, B and C are alternately updated until there is no further appreciable change in the objective function value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithms for (2.2)</head><p>We utilize the concept of auxiliary functions <ref type="bibr" target="#b10">[11]</ref> for our derivations. It is sufficient to illustrate our methods using a single column of C (or row of B), since our divergences are separable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3.1 (Auxiliary function)</head><formula xml:id="formula_5">. A function G(c, c ′ ) is called an auxiliary function for F (c) if: 1. G(c, c) = F (c),<label>and</label></formula><formula xml:id="formula_6">2. G(c, c ′ ) ≥ F (c) for all c ′ .</formula><p>Auxiliary functions turn out to be useful due to the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.2 (Iterative minimization).</head><p>If G(c, c ′ ) is an auxiliary function for F (c), then F is non-increasing under the update</p><formula xml:id="formula_7">c t+1 = argmin c G(c, c t ). Proof. F (c t+1 ) ≤ G(c t+1 , c t ) ≤ G(c t , c t ) = F (c t ).</formula><p>As can be observed, the sequence formed by the iterative application of Lemma 3.2 leads to a monotonic decrease in the objective function value F (c). For an algorithm that iteratively updates c in its quest to minimize F (c), the method for proving convergence boils down to the construction of an appropriate auxiliary function. Auxiliary functions have been used in many places before, see for example <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>We now construct simple auxiliary functions for (2.2) that yield multiplicative updates. To avoid clutter we drop the functions α and β from (2.2), noting that our methods can easily be extended to incorporate these functions.</p><p>Suppose B is fixed and we wish to compute an updated column of C. We wish to minimize</p><formula xml:id="formula_8">F (c) = D ϕ (Bc, a),<label>(3.1)</label></formula><p>where a is the column of A corresponding to the column c of C. The lemma below shows how to construct an auxiliary function for (3.1). For convenience of notation we use ψ to denote ∇ϕ for the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.3 (Auxiliary function). The function</head><formula xml:id="formula_9">G(c, c ′ ) = ij λ ij ϕ b ij c j λ ij - i ϕ(a i ) + ψ(a i ) (Bc) i -a i ,<label>(3.2</label></formula><p>)</p><formula xml:id="formula_10">with λ ij = (b ij c ′ j )/( l b il c ′ l )</formula><p>, is an auxiliary function for <ref type="bibr">(3.1)</ref>. Note that by definition j λ ij = 1, and as both b ij and c ′ j are nonnegative, λ ij ≥ 0.</p><p>Proof. It is easy to verify that G(c, c) = F (c), since j λ ij = 1. Using the convexity of ϕ, we conclude that if j λ ij = 1 and λ ij ≥ 0, then</p><formula xml:id="formula_11">F (c) = i ϕ j b ij c j -ϕ(a i ) -ψ(a i ) (Bc) i -a i ≤ ij λ ij ϕ b ij c j λ ij - i ϕ(a i ) + ψ(a i ) (Bc) i -a i = G(c, c ′ ).</formula><p>To obtain the update, we minimize G(c, c ′ ) w.r.t. c. Let ψ(x) denote the vector [ψ(x 1 ), . . . , ψ(x n )] T . We compute the partial derivative</p><formula xml:id="formula_12">∂G ∂c p = i λ ip ψ b ip c p λ ip b ip λ ip - i b ip ψ(a i ) = i b ip ψ c p c ′ p (Bc ′ ) i -(B T ψ(a)) p .<label>(3.3)</label></formula><p>We need to solve (3.3) for c p by setting ∂G/∂c p = 0. Solving this equation analytically is not always possible. However, for a broad class of functions, we can obtain an analytic solution. For example, if ψ is multiplicative (i.e., ψ(xy) = ψ(x)ψ(y)) we obtain the following iterative update relations for b and c (see <ref type="bibr" target="#b6">[7]</ref>)</p><formula xml:id="formula_13">b p ← b p • ψ -1 [ψ(a T )C T ] p [ψ(b T C)C T ] p ,<label>(3.4)</label></formula><formula xml:id="formula_14">c p ← c p • ψ -1 [B T ψ(a)] p [B T ψ(Bc)] p .<label>(3.5)</label></formula><p>It turns out that when ϕ is a convex function of Legendre type, then ψ -1 can be obtained by the derivative of the conjugate function ϕ * of ϕ, i.e., ψ -1 = ∇ϕ * <ref type="bibr" target="#b13">[14]</ref>.</p><p>Note. <ref type="bibr">(3.4</ref>) &amp; (3.5) coincide with updates derived by Lee and Seung <ref type="bibr" target="#b10">[11]</ref>, if ϕ(x) = 1 2 x 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Examples of New NNMA Problems</head><p>We illustrate the power of our generic auxiliary functions given above for deriving algorithms with multiplicative updates for some specific interesting problems.</p><p>First we consider the problem that seeks to minimize the divergence,</p><formula xml:id="formula_15">KL(Bc, a) = i (Bc) i log (Bc) i a i -(Bc) i + a i , B, c ≥ 0. (3.6)</formula><p>Let ϕ(x) = x log x -x. Then, ψ(x) = log x, and as ψ(xy) = ψ(x) + ψ(y), upon substituting in (3.3), and setting the resultant to zero we obtain</p><formula xml:id="formula_16">∂G ∂c p = i b ip log(c p (Bc ′ ) i /c ′ p ) - i b ip log a i = 0, =⇒ (B T 1) p log c p c ′ p = [B T log a -B T log(Bc ′ )] p =⇒ c p = c ′ p • exp [B T log a/(Bc ′ ) ] p [B T 1] p .</formula><p>The update for b can be derived similarly.</p><p>Constrained NNMA. Next we consider NNMA problems that have additional constraints. We illustrate our ideas on a problem with linear constraints. We can solve (3.7) problem using our method by making use of an appropriate (differentiable) penalty function that enforces P c ≤ 0. We consider,</p><formula xml:id="formula_17">F (c) = D ϕ (Bc, a) + ρ max(0, P c) 2 ,<label>(3.8)</label></formula><p>where ρ &gt; 0 is some penalty constant. Assuming multiplicative ψ and following the auxiliary function technique described above, we obtain the following updates for c,</p><formula xml:id="formula_18">c k ← c k • ψ -1 [B T ψ(a)] k -ρ[P T (P c) + ] k [B T ψ(Bc)] k ,</formula><p>where (P c) + = max(0, P c). Note that care must be taken to ensure that the addition of this penalty term does not violate the nonnegativity of c, and to ensure that the argument of ψ -1 lies in its domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks.</head><p>Incorporating additional constraints into (3.6) is however easier, since the exponential updates ensure nonnegativity. Given a = 1, with appropriate penalty functions, our solution to (3.6) can be utilized for maximizing entropy of Bc subject to linear or non-linear constraints on c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nonlinear models with "link" functions. If A ≈ h(BC)</head><p>, where h is a "link" function that models a nonlinear relationship between A and the approximant BC, we may wish to minimize D ϕ (h(BC), A). We can easily extend our methods to handle this case for appropriate h. Recall that the auxiliary function that we used, depended upon the convexity of ϕ. Thus, if (ϕ •h) is a convex function, whose derivative ∇(ϕ •h) is "factorizable," then we can easily derive algorithms for this problem with link functions. We exclude explicit examples for lack of space and refer the reader to <ref type="bibr" target="#b6">[7]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithms using KKT conditions</head><p>We now derive efficient multiplicative update relations for (2.3), and these updates turn out to be simpler than those for (2.2). To avoid clutter, we describe our methods with α ≡ 0, and β ≡ 0, noting that if α and β are differentiable, then it is easy to incorporate them in our derivations. For convenience we use ζ(x) to denote ∇ 2 (x) for the rest of this section.</p><p>Using matrix algebra, one can show that the gradients of D ϕ (A, BC) w.r.t. B and C are,</p><formula xml:id="formula_19">∇ B D ϕ (A, BC) = ζ(BC) ⊙ (BC -A) C T ∇ C D ϕ (A, BC) =B T ζ(BC) ⊙ (BC -A) ,</formula><p>where ⊙ denotes the elementwise or Hadamard product, and ζ is applied elementwise to BC. According to the KKT conditions, there exist Lagrange multiplier matrices Λ ≥ 0 and Ω ≥ 0 such that </p><formula xml:id="formula_20">∇ B D ϕ (A, BC) = Λ, ∇ C D ϕ (A, BC) = Ω,<label>(3.9a</label></formula><formula xml:id="formula_21">b mk ← b mk ζ(BC) ⊙ A C T mk ζ(BC) ⊙ BC C T mk .<label>(3.10)</label></formula><p>Proceeding in a similar fashion we obtain a similar iterative formula for c kn , which is</p><formula xml:id="formula_22">c kn ← c kn [B T ζ(BC) ⊙ A ] kn [B T ζ(BC) ⊙ BC ] kn .<label>(3.11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Examples of New and Old NNMA Problems as Special Cases</head><p>We now illustrate the power of our approach by showing how one can easily obtain iterative update relations for many NNMA problems, including known and new problems. For more examples and further generalizations we refer the reader to <ref type="bibr" target="#b6">[7]</ref>.</p><p>Lee and Seung's Algorithms. Let α ≡ 0, β ≡ 0. Now if we set ϕ(x) = 1 2 x 2 or ϕ(x) = x log x, then (3.10) and <ref type="bibr">(3.11)</ref> reduce to the Frobenius norm and KL-Divergence update rules originally derived by Lee and Seung <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Elementwise weighted distortion.</head><p>Here we wish to minimize W ⊙(A-BC) 2  F . Using X ← √ W ⊙ X, and A ← √ W ⊙ A in (3.10) and (3.11) one obtains</p><formula xml:id="formula_23">B ← B ⊙ (W ⊙ A)C T (W ⊙ (BC))C T , C ← C ⊙ B T (W ⊙ A) B T (W ⊙ (BC)) .</formula><p>These iterative updates are significantly simpler than the PMF algorithms of <ref type="bibr" target="#b12">[13]</ref>.</p><p>The Multifactor NNMA Problem (new). The above ideas can be extended to the multifactor NNMA problem that seeks to minimize the following divergence (see <ref type="bibr" target="#b6">[7]</ref>)</p><formula xml:id="formula_24">D ϕ (A, B 1 B 2 . . . B R ),</formula><p>where all matrices involved are nonnegative. A typical usage of multifactor NNMA problem would be to obtain a three-factor NNMA, namely A ≈ RBC. Such an approximation is closely tied to the problem of co-clustering <ref type="bibr" target="#b2">[3]</ref>, and can be used to produce relaxed coclustering solutions <ref type="bibr" target="#b6">[7]</ref>.</p><p>Weighted NNMA Problem (new). We can follow the same derivation method as above (based on KKT conditions) for obtaining multiplicative updates for the weighted NNMA problem:</p><formula xml:id="formula_25">min D ϕ (A, W 1 BCW 2 ),</formula><p>where W 1 and W 2 are nonnegative (and nonsingular) weight matrices. The work of <ref type="bibr" target="#b8">[9]</ref> is a special case as mentioned in Table <ref type="table">1</ref>. Please refer to <ref type="bibr" target="#b6">[7]</ref> for more details.</p><p>We have looked at generic algorithms for minimizing Bregman divergences between the input and its approximation. One important question arises: Which Bregman divergence should one use for a given problem? Consider the following factor analytic model</p><formula xml:id="formula_26">A = BC + N ,</formula><p>where N represents some additive noise present in the measurements A, and the aim is to recover B and C. If we assume that the noise is distributed according to some member of the exponential family, then minimizing the corresponding Bregman divergence <ref type="bibr" target="#b0">[1]</ref>  Some other open problems involve looking at the class of minimization problems to which the iterative methods of Section 3.2 may be applied. For example, determining the class of functions h, for which these methods may be used to minimize D ϕ (A, h(BC)). Other possible methods for solving both (2.2) and (2.3), such as the use of alternating projections (AP) for NNMA, also merit a study.</p><p>Our methods for (2.2) decreased the objective function monotonically (by construction). However, we did not demonstrate such a guarantee for the updates (3.10) &amp; <ref type="bibr">(3.11)</ref>. Figure <ref type="figure" target="#fig_3">1</ref> offers encouraging empirical evidence in favor of a monotonic behavior of these updates. It is still an open problem to formally prove this monotonic decrease. Preliminary results that yield new monotonicity proofs for the Frobenius norm and KL-divergence NNMA problems may be found in <ref type="bibr" target="#b6">[7]</ref>.  NNMA has been used in a large number of applications, a fact that attests to its importance and appeal. We believe that special cases of our generalized problems will prove to be useful for applications in data mining and machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) λ mk b mk = ω kn c kn = 0. (3.9b) Writing out the gradient ∇ B D ϕ (A, BC) elementwise, multiplying by b mk , and making use of (3.9a,b), we obtain ζ(BC) ⊙ (BC -A) C T mk b mk = λ mk b mk = 0, which suggests the iterative scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>PMF Objectiveϕ(x) = -log x ϕ(x) = x log x -x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Objective function values over 100 iterations for different NNMA problems. The input matrix A was random 20×8 nonnegative matrix. Matrices B and C were 20×4, 4×8, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is appropriate. For e.g., if the noise is modeled as i.i.d. Gaussian noise, then the Frobenius norm based problem is natural. , both problems coincide. For other ϕ, the choice between (2.2) and (2.3) can be guided by computation issues or sparsity patterns of A. Clearly, further work is needed for answering this question in more detail.</figDesc><table><row><cell>Another question is: Which version of the problem we should use, (2.2) or (2.3)? For</cell></row><row><cell>ϕ(x) = 1 2 x 2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use the word approximation instead of factorization to emphasize the inexactness of the process since, the input A is approximated by BC.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Paatero and Tapper <ref type="bibr" target="#b12">[13]</ref> introduced NNMA as positive matrix factorization, and they aimed to minimize W ⊙ (A -BC) F , where W was a fixed nonnegative matrix of weights. NNMA remained confined to applications in Environmetrics and Chemometrics before pioneering papers of Lee and Seung <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> popularized the problem. Lee and Seung <ref type="bibr" target="#b10">[11]</ref> provided simple and efficient algorithms for the NNMA problems that sought to minimize A -BC F and KL(A, BC). Lee &amp; Seung called these problems nonnegative matrix factorization (NNMF), and their algorithms have inspired our generalizations.</p><p>NNMA was applied to a host of applications including text analysis, face/image recognition, language modeling, and speech processing amongst others. We refer the reader to <ref type="bibr" target="#b6">[7]</ref> for pointers to the literature on various applications of NNMA. Srebro and Jaakola <ref type="bibr" target="#b14">[15]</ref> discuss elementwise weighted low-rank approximations without any nonnegativity constraints. Collins et al. <ref type="bibr" target="#b5">[6]</ref> discuss algorithms for obtaining a low rank approximation of the form A ≈ BC, where the loss functions are Bregman divergences, however, there is no restriction on B and C. More recently, Cichocki et al. <ref type="bibr" target="#b3">[4]</ref> presented schemes for NNMA with Csiszár's ϕ-divergeneces, though rigorous convergence proofs seem to be unavailable. Our approach of Section 3.2 also yields heuristic methods for minimizing Csiszár's divergences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSF grant CCF-0431257, NSF Career Award ACI-0093404, and NSF-ITR award IIS-0325116.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering with Bregman Divergences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conf. on Data Mining</title>
		<meeting><address><addrLine>Lake Buena Vista, Florida</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Censor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Zenios</surname></persName>
		</author>
		<title level="m">Parallel Optimization: Theory, Algorithms, and Applications. Numerical Mathematics and Scientific Computation</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Minimum Sum Squared Residue based Co-clustering of Gene Expression data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th SIAM International Conference on Data Mining (SDM)</title>
		<meeting>4th SIAM International Conference on Data Mining (SDM)<address><addrLine>Florida</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Csiszár&apos;s Divergences for Non-Negative Matrix Factorization: Family of New Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Int. Conf. ICA &amp; BSS</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Logistic regression, adaBoost, and Bregman distances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth annual conference on COLT</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Generalization of Principal Components ysis to the Exponential Family</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generalized nonnegative matrix approximations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local nonnegative matrix factorization as a visual representation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-Y.</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Development and Learning</title>
		<meeting>the 2nd International Conference on Development and Learning<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<biblScope unit="page" from="178" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A weighted nonnegative matrix factorization for local representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-negative sparse coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Neural Networks for Signal Processing</title>
		<meeting>IEEE Workshop on Neural Networks for Signal essing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999-10">October 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Positive matrix factorization: A nonnegative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">111-126</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex Analysis</title>
		<imprint>
			<publisher>Princeton Univ. Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weighted low-rank approximations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 20th ICML</title>
		<meeting>of 20th ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Topics in Sparse Approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>The Univ. of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
