<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Engineering in Surveying, Mapping and Remote Sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430072</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9FF7F4D0D32D40A859365E24ED9F670</idno>
					<idno type="DOI">10.1109/TCSVT.2019.2897980</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2019.2897980, IEEE Transactions on Circuits and Systems for Video Technology</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ship Detection</term>
					<term>Saliency Detection</term>
					<term>Coastline Extraction</term>
					<term>Object Location</term>
					<term>CNN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time detection of inshore ships plays an essential role in the efficient monitoring and management of maritime traffic and transportation for port management. Current ship detection methods which are mainly based on remote sensing images or radar images hardly meet real-time requirement due to timeliness of image acquisition. In this paper, we propose to use visual images captured by an on-land surveillance camera network to achieve real-time detection. However, due to the complex background of visual images and the diversity of ship categories, the existing convolution neural network (CNN) based methods are either inaccurate or slow. To achieve high detection accuracy and real-time performance simultaneously, we propose a saliency-aware CNN framework for ship detection, comprising comprehensive ship discriminative features, such as deep feature, saliency map and coastline prior. This model uses CNN to predict the category and the position of ships, and uses the global contrast based salient region detection to correct the location. We also extract coastline information and respectively incorporate it into CNN and saliency detection to obtain more accurate ship locations. We implement our model on Darknet under CUDA 8.0 and CUDNN V5 and use a real-world visual image dataset for training and evaluation. The experimental results show that our model outperforms representative counterparts (Faster R-CNN, SSD, and YOLOv2) in terms of accuracy and speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>transportation. As for marine surveillance, ship detection plays a strong supervisory role in fisheries dumping of pollutants and illegal smuggling. In the navigation safety, ship detection can judge whether there are abnormal sailing behaviors such as landing or stagnation, so as to ensure the safety on the coast and at sea. Real-time detection of ships is also very important and has the ability to proactively alert. The real-time detection system can be connected with other systems, especially the emergency dispatch system, which is helpful for responding to abnormal behaviors and emergencies in time to avoid possible adverse consequences. It can also be integrated with space-time systems to process and analyze previous surveillance videos in real time and make timely decisions. According to the image generation source, images based ship target detection methods are roughly classified into three categories: radar images <ref type="bibr" target="#b0">[1]</ref>, remote sensing images, and visual images. The acquisition and preprocessing of radar images and remote sensing images always takes time and cannot be detected in real time. Compared with other categories, visual images are generally obtained more easily from continuous monitoring video, and so they can be used as real-time detection. However, because the background of visual images is more complicated and less clear, there exists severe interference for foreground detection. Therefore, accurate ship object detection from surveillance video faces huge challenges.</p><p>There appear some ship detection methods based on visual images <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. They usually use ship features, such as the contextual information of the image, the temporal-spatial information of the ship, and the geographical environment prior (e.g., coastline). In recent few years, convolutional neural network (CNN) has achieved great success in natural image classification <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref> and object detection <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b14">[15]</ref>. In contrast to traditional methods using manual pre-defined features, CNN based methods are able to automatically represent and extract discriminative and robust features for object detection. However, there are special difficulties for ship detection task in the marine environment. First, due to waves and floating objects, the background is very complicated so that ships are easily mixed with them and even visually overlap the nearshore buildings. Second, the ships vary in categories and sizes, which range from dozens to hundreds pixels in size and may cross or occlude with each other. Finally, because marine climate and lighting conditions are variable, low visibility weather such as clouds and fog often degrade the acquired video quality. Therefore, CNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Saliency-Aware Convolution Neural Network for Ship Detection in Surveillance Video</head><p>Zhenfeng <ref type="bibr">Shao</ref>  approaches to regular object detection often fail to provide satisfactory ship detection performance.</p><p>Recently, some researchers have used CNN for ship detection in remote sensing images. Li et al. <ref type="bibr" target="#b15">[16]</ref> pioneers to introduce CNN into ship detection of remote sensing images. They propose a novel parallelogram image cropping (PIC) method to generate parallelogram samples, each of which only contains single ship or dock. Lin et al. <ref type="bibr" target="#b16">[17]</ref> propose to divide the detection among the network layers at different depths to combine the advantages of deep network used for location and shallow network used for detecting. However, different from the top-down perspective of the remote sensing images, most of the visual images are in frontal perspectives. In addition, for detection task from the continuous video, they must sacrifice accuracy to guarantee real-time performance.</p><p>To address the ship detection under seashore surveillance video conditions, we develop a novel saliency-aware CNN framework, which is built on the YOLOv2 pipeline. Essentially, our proposed ship detection model follows a classification and localization driven coarse-to-fine idea. It first uses CNN to predict the ship class and the rough position. However, because the onshore buildings and ships are similar in color and appearance, YOLOv2 often judges ashore buildings either as the target ships or as part of the nearshore ships, which results in detection confusion, including false detection (false positive) or inaccurate localization and size of the actual ship region. In a further examination, inaccurate location and size will reduce the confidence score of bounding box, which in turn leads to missed ships (i.e., low recall rate) because YOLOv2 tends to discard low scoring candidates. To this end, considering that the coast surveillance image contains sea areas and land areas, but ships only appear in the sea area, we extract coastline features and incorporate them into CNN to improve the robustness and efficiency of the ship detection. More specifically, only cells (by YOLOv2) at sea are produced and the classification is further examined, but excluding onshore cells. Furthermore, since ships differ much from water in terms of visual saliency, we further incorporate saliency detection technique to refine a more accurate location. Owning to the improved localization, the missed ship suffering from low confidence score is accordingly recalled. At last, as for ship detection in continuous video, because the position of the ship in the video frame is spatially coherent, we use temporal continuity to set the initial observation position of each frame instead of re-traversing the entire video frame. Extensive validations on real-world coast surveillance video datasets (as shown in Fig. <ref type="figure">1</ref>) from Hengqin Island in Zhuhai in China show the proposed model's capability in terms of detection accuracy and speed <ref type="bibr" target="#b17">[18]</ref>.</p><p>The main contributions of this paper are highlighted as follows:</p><p>1) To the best of our knowledge, we are the first to introduce CNN into ship detection in surveillance video.</p><p>2) Based on the YOLOv2 pipeline, we propose a saliency-aware CNN framework to improve the accuracy and robustness of ship detection under complex seashore surveillance conditions, where the ship's category and location are first predicted by CNN and then are refined with saliency detection.</p><p>3) We propose coastline segmentation to reduce the inspection range and further improve the detection efficiency.</p><p>The rest of the paper is organized as follows. In Section II, we introduce the related work of ship detection. In Section III, we give detailed explanations of our proposed model. Section IV illustrates experimental results and comparisons against other state-of-the-art methods. Section V draws a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ship Detection</head><p>Some methods using hand-crafted features are widely studied for ship detection. W. KrÃ¼ger et al. <ref type="bibr" target="#b1">[2]</ref> first use color segmentation and edge detection to detect the sea level feature, and then use the image registration and subtraction to separate the ship from the water. Bao et al. <ref type="bibr" target="#b2">[3]</ref> detect ship with the contextual information and the ship space-time information. They manually determine the mean and variance threshold for each category, and then based on contextual information analyze the regional-level movement of the ship and its corresponding local context for detection. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a new method based on mean shift and the peak of grayscale for ship automatic detection and tracking, but it needs to be optimized in real time. Zhang et al. <ref type="bibr" target="#b4">[5]</ref> detect the horizon line by exploiting the characteristics of discrete cosine transform (DCT) blocks and extract the sea-surface background regions below the horizon. They simply remove the background to obtain ship targets and the results are unreliable. These features leverage human attention mechanisms to obtain the saliency of the ship in the entire image with respective features designed for different ship conditions, but they are not suitable for detecting diverse &gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; 3 category of ships in our scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Network</head><p>Convolutional neural network (CNN) has been successfully applied to object detection <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b14">[15]</ref>. In recent years, the development of deep learning has been driven by the regional proposal method and the regional proposal-based CNN (R-CNN) <ref type="bibr" target="#b8">[9]</ref>. R-CNN is the first network to use the CNN feature for classification. In order to improve efficiency, R. Girshick further proposed Fast R-CNN based on R-CNN <ref type="bibr" target="#b9">[10]</ref>. Fast R-CNN maps the proposal region to the feature map of the last convolutional layer of CNN. In this way, an image needs to be extracted only once which greatly increases speed.</p><p>Based on Fast R-CNN, R. Girshick also proposed the Faster R-CNN <ref type="bibr" target="#b10">[11]</ref>, which is composed of Region Proposal Network (RPN) and Fast R-CNN. Two models share the features and the RPN module tells the Fast R-CNN module where to look.</p><p>The accuracy of the R-CNN framework is getting higher and higher, especially the Faster R-CNN. The bottleneck of the R-CNN framework is that it cannot fully utilize the context information of the local object in the entire image after transforming the decomposition problem into the classification problem of the image local area. Therefore, J. Redmon and R. Girshick <ref type="bibr" target="#b14">[15]</ref> proposed the YOLO (You Only Look Once) network together. The idea is handling object detection problem as regression problem, separating object locations and categories from space. The detection speed of the network is very fast and can achieve real-time video processing, but the accuracy is not high enough. J. Redmon <ref type="bibr" target="#b18">[19]</ref> used a series of methods to improve YOLO and proposed YOLOv2, which improves the accuracy and maintains the speed. However, the accuracy still cannot meet the requirements. Qi <ref type="bibr" target="#b19">[20]</ref> proposed a novel paradigm of deep network to explore various scales of spatial contexts adjusted to pixels at different locations. This model constructs multiple layers of memory cells, whose outputs are hierarchically gated on different scales before recursively feeding to higher layers. Then the pixel labels at different locations are decided based on the spatial contexts of the customized scales. Compared with the general object detection algorithm, it can make full use of the context information to obtain a good pixel-level detection effect. Nevertheless, the required dataset must be fully labeled on individual pixel level, which does not meet the situation of our ship dataset. In the view of the above discussion, we follow the YOLOv2 framework to construct our ship detection pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Saliency Detection</head><p>Salient object detection can help people quickly locate target region of interest in an image. It has been widely used as a preprocess step in many computer vision tasks such as super-pixel segmentation <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>, object recognition <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, image retrieval <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>, etc.</p><p>Inspired by these works, many researchers began to harness saliency information in ship detection. Bi <ref type="bibr" target="#b30">[31]</ref> extracted salient candidate regions across the entire detection scene using a bottom-up visual attention mechanism. Then appearance and neighborhood similarity features are combined to discriminate the selected salient regions. Jiang <ref type="bibr" target="#b31">[32]</ref> used the salient corner features at ship bow to precisely detect in-shore ships and separate multiple docked targets. Lin <ref type="bibr" target="#b32">[33]</ref> implemented a task partitioning model similar to the attention model in FCN <ref type="bibr" target="#b33">[34]</ref> network. With deep path for attention/saliency maps and the shallow path for detection, the integrated FCN can detect ships robustly and simply.</p><p>The above works have all proved that adding salient information to ship detection problem can effectively improve the detection performance in optical remote sensing images. However, few studies have introduced this idea into ship detection in natural images. Walther <ref type="bibr" target="#b34">[35]</ref> proposed a biologically plausible model for forming and attending proto-objects in natural scenes. But this method is hardly generalized to other computer vision tasks, such as image segmentation and object detection. Achanta <ref type="bibr" target="#b35">[36]</ref> adopted a frequency-tuned approach to compute full resolution saliency maps with well-defined boundaries, which uses an image-dependent adaptive threshold to binarize the generated saliency map. Rahtu <ref type="bibr" target="#b36">[37]</ref> firstly generated saliency maps using a statistical framework and local feature contrast in illumination, color, and motion information, and then segmented the salient object with a conditional random field. Goferman <ref type="bibr" target="#b37">[38]</ref> detected context-aware saliency maps based on four principles observed in the psychological literatures. The approach was evaluated in two applications where the context of the dominant objects is just as essential as the objects themselves. Cheng <ref type="bibr" target="#b38">[39]</ref> proposed a method that considers both appearance similarity and spatial distribution of image pixels, which produces perceptually accurate salient region detection. For more detailed literature discussion of some state-of-the-art saliency detection algorithms, we refer readers to <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b40">[41]</ref>.</p><p>In this paper, we adopted a regional contrast-based saliency extraction algorithm <ref type="bibr" target="#b41">[42]</ref> which simultaneously evaluates global contrast differences and spatial coherence. We compared the improvements of this algorithm with previously mentioned methods <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b38">[39]</ref> on the final ship detection results. And experimental results showed that regional contrast-based method can achieve better recall and precision rates and more accurate location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Our proposed model is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Our model mainly consists of CNN and saliency detection, both combined with coastline features. The former is used to predict the category and preliminary position of the ship, and the latter is used to determine the exact position. The model first resizes the input image to a fixed size and passes it to CNN to extract feature maps, where CNN learns from extracted coastline features to exclude onshore feature maps. On the final feature map, we examine the spatial relationship between each cell and the coastline, and only generate a bounding box for the sea part. Then, the corresponding grids of the remaining feature maps generate several anchor boxes of different sizes and output the most likely box category and the position to be corrected. Finally, we use the salient region detection of the proposals to rectify the ship localization generated by CNN. Because the color information of the house is also very salient, we attempt to combine coastline with saliency to reduce the impact of onshore houses on saliency detection. We intersect the coastline with the detection box and only perform saliency detection on the offshore part. The detection result takes the outer rectangle as the position of the ship.</p><p>In the practical application of ship detection in video, we can further leverage temporal continuity to set the initial inspection position for individual frames. We estimate the ship's moving displacement between adjacent frames based on the speed, heading, and interframe interval and then determine the approximate location of the ship in the next frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. YOLOv2 Based Classification</head><p>YOLOv2 integrates bounding box generation, feature extraction, target classification and target position into convolutional neural network. It directly extracts bounding boxes from the image, and predicts the position and probability of ship through the entire image feature. It converts ship detection problems into regression problems, which is truly end-to-end detection. So, we use YOLOv2 to predict classification of ship in images.</p><p>YOLOv2 resizes the input image to 416x416 and the downsampling rate is 32, so finally the feature map covers 13 Ã— 13 cells. As shown in Fig. <ref type="figure">3 (b</ref>), many cells are on the shore, which actually have nothing to do with the ship detection task. The coastline is a very useful feature that distinguishes between sea and land. Therefore, we can take advantages of coastline prior to exclude unnecessary generation of onshore cells, for reducing both computational burden and the interference of onshore buildings to ship detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Coastline Extraction</head><p>We first use the Canny operator to detect edges in the image. Then we use Hough transform to obtain all line segments based on the edges. Suffering from the complex coast background, Hough transform used for extracting line segments usually produces multiple segments rather than unique one. Considering that the generated line segments are mostly concentrated near the coastline and are relatively random, we need to figure out the accurate coastline. We find that the slope ğ‘˜ in the coastline does not exceed 0.15 and the intercept ğ‘ does not exceed 800 from the origin in the upper left corner of the image (1920x1080). Therefore, we calculate ğ‘˜ and ğ‘ of all generated line segments, and then exclude some outliers that impossibly form a reasonable coastline based on this observation.</p><p>Through the above steps, we have excluded the outliers, and then we are to fit a line segment (from the remaining segments) that is closest to the true coastline. Due to the difference in position and slope of the remaining segments, they unevenly contribute to the resulting coastline. To do this, we need to establish a judgment criterion to find the most valuable line segment to fit the final coastline. The position of the line is determined by the slope and intercept, which also make differences on the position, and so we consider the following discriminant function:</p><formula xml:id="formula_0">ğ¿ $ = ğœ† Ã— ğ‘˜ $ ( + (1 -ğœ†) Ã— ğ‘ $ (<label>(1)</label></formula><p>where ğ‘– denotes the ğ‘– -th segment, and ğ¿ $ denotes the cost function. ğ‘˜ / and ğ‘ $ represent the slope and intercept of the ğ‘–-th segment after normalization, and ğœ† indicates the weight parameter.</p><p>Then we arrange the line segments from small to large according to the cost function. Because too small values in cost function have a negative impact on the resulted coastline, we only use the first fraction of ğœŒ as effective line segments for generating the coastline. We set the average ğ‘˜ and ğ‘ of these line segments as the slope and intercept of the extracted coastline. We will move the coastline up 30 pixels, considering that the ship may intersect the coastline. As a concrete example, the coastline extraction results are shown in Fig. <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Bounding Box Regression with Coastline</head><p>After the coastline is extracted, we input the coastline feature into CNN together with the image, which are jointly used to conduct bounding box regression and classification by YOLOv2.During the classification, both in training and testing, coastline features can assist YOLOv2's decision-making to reduce detection time and improve accuracy. Based on coastline feature, we use Eq. ( <ref type="formula">2</ref>) to determine whether the cell is on the shore or not. If the cell satisfies Eq. ( <ref type="formula">2</ref>), we mark it as ashore cell for which we do not generate bounding boxes to reduce the wrong ship classification, like the shown example in Fig. <ref type="figure">5</ref>. During testing stage, each of cells directly generates 5 bounding boxes according to the model. YOLOv2 treats the bounding box with a confidence probability above a certain threshold as the valid detection. The boxes which do not satisfy the threshold are directly discarded. Because several regressed bounding boxes may correspond to the same target, NMS (Non-Maximum Suppression) <ref type="bibr" target="#b42">[43]</ref> is further used to find the most suitable bounding box.</p><p>The labeled bounding boxes are used for ground truth in training. But unlike bounding boxes, the coastlines are not used as ground truth during the training of YOLOv2. They are thus only extracted online by our proposed method, both in training and testing phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Salient Region Detection based Location</head><p>Suffering from complex onshore backgrounds (e.g., similar color and appearance between ashore buildings and nearshore ships), YOLOv2 often judges the ashore building either as the target ship or as part of the nearshore ship. The former leads to false detection (false positive) while the latter results in inaccurate localization and size of the actual ship region. In other words, for the latter, the resulting bounding box does not match the actual area of the ship, which further leads to missing detection due to the scoring mechanism of YOLOv2.</p><p>The rule for YOLOv2 to determine whether the bounding box is available according to its confidence score. When the confidence score of the bounding box is larger than the threshold (typically 0.24), YOLOv2 then calculates its IoU value. Only those with IoU greater than the IoU threshold (usually 0.5) are considered usable. In other words, when the bounding box is considered unavailable, its corresponding object will be missed. Experimentally, we do observe that the missing detection occurs due to the low IoU, with some examples shown in Fig. <ref type="figure" target="#fig_3">6</ref>. In Fig. <ref type="figure" target="#fig_3">6</ref>, some of the bounding boxes are much larger than the ship (with the IoU value being 0.466), others are smaller (with the IoU value being 0.462), and some contain only a portion of the ship (with the IoU value being 0.397). In all of these cases, their IoU values are less than 0.5, so they are abandoned. The reason for these problems is that the localization of YOLOv2 is not very accurate due to the interference of the complex ashore backgrounds. If we can improve the location, the missed ships will be detected correctly or the recall rate will be increased.</p><p>Salient object detection can help people quickly locate target region of interest in an image. So we thus combine the saliency features to rectify the preliminary location generated by YOLOv2. Due to the complexity of the image, we still get poor results when we input the entire image in saliency extraction, as shown in Fig. <ref type="figure">7</ref>. Instead, we use the bounding boxes as the target detection range. Additionally, we do not intend to perform saliency detection on all boxes, but instead choose those that are expected to be corrected. If the IoU is too low, which means that the box intersects with the boat very little, we think this is a completely wrong box, without further correction value. Therefore, we set a low threshold for the IoU and choose those boxes whose IoU values fall between lower limit and 0.5 for further re-examination using saliency detection. With the help of preliminary location given by YOLOv2, we will expand the box slightly and adaptively to contain the complete ship as much as possible for the saliency detection in all conditions.</p><p>We perform salient region detection based on global contrast. The idea of using region based contrast (RC) <ref type="bibr" target="#b41">[42]</ref> to produce saliency maps comes from a common sense that saliency of a region mainly depends on its contrast to its nearby regions. RC first segments the input image into regions through a graph-based super-pixel segmentation algorithm <ref type="bibr" target="#b43">[44]</ref>. Then, the saliency value of each region ğ‘Ÿ S is computed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(ğ‘Ÿ S ) = ğ‘¤ U (ğ‘Ÿ S ) âˆ‘ ğ‘’ W X (Y Z ,Y \ )</head><p>]^X _ ğœ”(ğ‘Ÿ $ )ğ· B (ğ‘Ÿ S , ğ‘Ÿ $ )</p><p>B Z bB \ <ref type="bibr" target="#b4">(5)</ref> where ğ‘¤ U (ğ‘Ÿ S ) is a spatial prior weighting term; ğ· U (ğ‘Ÿ S , ğ‘Ÿ $ ) is the spatial Euclidean distance between centroids of two regions ğ‘Ÿ S and ğ‘Ÿ $ , and ğœ U adjusts the influence of spatial distance weights; ğœ”(ğ‘Ÿ $ ) is the weight of region ğ‘Ÿ $ which is measured by the number of pixels in ğ‘Ÿ $ . The color distance between two regions ğ· B (ğ‘Ÿ S , ğ‘Ÿ $ ) is defined as follows:</p><p>ğ· B (ğ‘Ÿ : , ğ‘Ÿ ( ) = âˆ‘ âˆ‘ ğ‘“eğ‘ :,$ gğ‘“eğ‘ (,h gğ·(ğ‘ :,$ , ğ‘ (,h ) i _ hj: i k $j: <ref type="bibr" target="#b5">(6)</ref> where ğ‘“eğ‘ S,$ g is the probability of the ğ‘–-th color ğ‘ S,$ among all ğ‘› S colors in the ğ‘˜-th region ğ‘Ÿ S , ğ‘˜ = {1, 2}. It plays as a weighting role in the distance computation to emphasize the color differences between dominant colors.</p><p>In saliency detection, because the information of the ashore buildings is also salient, we manage to make use of the coastline features to reduce the impact of the houses, making the saliency area consistent with the real ship area. We process the bounding boxes which intersect the coastline and if the pixels satisfy Eq. ( <ref type="formula">7</ref>), we mark them as ashore pixels and set them to 0.</p><formula xml:id="formula_1">ğ‘˜ Ã— ğ‘ $ -ğ‘ -ğ‘ h â‰¥ 0 (7)</formula><p>where ğ‘˜ and ğ‘ denote the coastline parameters (slope and intercept). ğ‘ $ and ğ‘ h are the horizontal and vertical coordinates of the pixel, respectively. We detect the processed boxes and produce the result which is not affected by the buildings and more robust, as shown in Fig. <ref type="figure">8</ref>. In Fig. <ref type="figure">9</ref>, we then take the outer rectangle of the salient region as the ultimate location of the ship. Finally, to especially confirm the role of saliency refinement, we further show the corresponding improved counterparts on bounding boxes in Fig. <ref type="figure" target="#fig_3">6</ref>, as shown in Fig. <ref type="figure">10</ref>. Accordingly, their IoU values are promoted to 0.610, 0.533 and 0.698 from original 0.466, 0.397 and 0.462, respectively. Since the improved IoU values are all above the IoU threshold 0.5, the bounding boxes will be considered valid and the associated ships will be recalled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>To prove the effectiveness of our proposed method, we designed experiments and evaluated our method quantitatively on our own ship data set. Subjective and objective results are reported in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We use our own new large ship dataset called SeaShips. The dataset currently consists of 11126 images and covers 6 common ship categories (ore carrier, bulk cargo carrier, general cargo ship, container ship, fishing boat, and passenger ship). All the images are from about 5400 real-world video segments, which are acquired by 156 monitoring cameras in the coastline video surveillance system deployed on Hengqin Island in Zhuhai in China. They are carefully selected to mostly cover all possible imaging variations, e.g., different scales, hull parts, illumination, viewpoints, backgrounds, and occlusions. All images are annotated with ship category labels and high-precision bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Test Environment</head><p>We conduct experiments based on learning platform Darknet in Windows 10. All our experiments are performed on a workstation with TitanX GPU cards under CUDA 8.0 and CUDNN V5. Our testing uses only one card.</p><p>Our network structure is modified from darknet19. To train the hyper-parameters, the mini-batch size is set to 16. The base learning rate is 0.0001 when iteration number is low than 20k and steps to 0.00001 when iteration number is low than 26k. The poly learning rate policy is adopted with power 0.9 together with the maximum iteration number 26k. Momentum is 0.9 and weight decay is 0.0001. Data augmentation contains random mirror and rand resizing is between 0.5 and 2.</p><p>During the coastline extraction, we use the grid search method to determine the final ğœ† and ğœŒ . In all candidate parameters, the best performing parameters are the final result by looping through each possibility of the parameters. ğœ† is set to 0.1, 0.3, 0.5, 0.7, 0.9 and ğœŒ is set to 1, 1/3, 1/5, 1/7, 1/9. The final results show that only when ğœ† is equal to 0.3 and ğœŒ is equal to 1/3, the coastline is completely correct. Because the average width of all bounding boxes of ships in training set is in 30 pixels, we move the coastline up 30 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Indicators</head><p>There are some typical quantitative indicators for evaluating an object detection model, which are briefly described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Average Precision</head><p>Given an IoU threshold, there are two indicators called recall and precision. We manually marked the ground truth of ships, whose total number is defined as NP. If the bounding box has an IoU overlapping with the ground truth over 0.5, we mark these as true positive (TP). Each bounding box can only match one ground truth. Therefore, false detections of the same ground truth are defined as false positives (FP). So, recall and precision follow:</p><formula xml:id="formula_2">ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = pq rq (8) ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = pq pquvq<label>(9)</label></formula><p>For each category, we can draw a precision-recall curve according to recall and precision values. AP is the area surrounded by the curve. </p><p>where ğ‘› is the number of classes that need to be detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Frame Per Second(FPS)</head><p>In addition to evaluate the accuracy, we also consider the model speed as one of the evaluation criteria. FPS indicates the number of image frames that models detect in one second. We use this indicator to measure the model speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison with Other Detection Methods</head><p>We compare our method with other detection methods, such as Fast R-CNN <ref type="bibr" target="#b9">[10]</ref>, Faster R-CNN <ref type="bibr" target="#b10">[11]</ref>, SSD <ref type="bibr" target="#b13">[14]</ref>, and YOLOv2 <ref type="bibr" target="#b18">[19]</ref>. For the Fast R-CNN algorithm, we choose the VGG training detection model. For Faster R-CNN, we set a convolutional neural network that has been pre-trained on ImageNet as pre-trained model, then use ZF net <ref type="bibr" target="#b44">[45]</ref> (5 convolutional layers and 3 fully connected layers) and VGG16 net <ref type="bibr" target="#b45">[46]</ref> (13 convolutional layers and 3 fully connected layers) to retrain the detection model. For SSD, we use the MobileNet <ref type="bibr" target="#b46">[47]</ref> and VGG16 net <ref type="bibr" target="#b45">[46]</ref> to retrain the detection model. For YOLO v2, we use our pre-trained weights to retrain the detection model while using some common data enhancement methods to increase the amount of data and improve model robustness such as hue, saturation, and exposure shifts. Our own method uses the parameters described above for training. All experiments were performed on four Titan Xp. We recorded the results of each model according to the previous evaluation indicators, as shown in Table <ref type="table" target="#tab_1">I</ref>. Fig. <ref type="figure" target="#fig_6">11</ref> shows AP performance for each ship with the IoU threshold set to 0.5.</p><p>As can be seen from Table <ref type="table" target="#tab_1">I</ref>, Fast R-CNN is much worse in mAP performance than others by a large margin. The performance of the Faster R-CNN series is significantly better than YOLOv2 and SSD. On average, Faster R-CNN's mAP is 14.52% higher than YOLOv2 and 11.12% higher than SSD. Our approach significantly improves the performance of YOLOv2, narrows the gap with Faster R-CNN, and performs better than the Faster R-CNN on general cargo ships.</p><p>Our proposed model is based on YOLOv2, and the mAP of each category in our model has a good improvement. Among the six categories of ship, ore and container ships can achieve better results. Because these two categories of ships are mainly used to transport cargo such as ore and containers, and these goods have very distinct features that are distinguished from other ships. In contrast, the performance of fishing ships is worse than other categories. The main reason is that fishing boats are generally small, occupying only 70x130 pixels in a 1920x1080 image. Detectors usually have poor detection results of small targets. After many forward convolutions layers, the feature of the small targets becomes blurred, even worse in the YOLOv2 series.</p><p>We use saliency detection to increase 5% on the basis of YOLOv2. For fishing boats, we have increased from 73.3% to 78.3%, which almost achieves the mAP of the SSD method. Although there are still gaps compared with other categories, the results are still good. For the passenger ships, our method has increased by up to 10%. We hold that the color feature of the passenger ships is generally very salient, so the  performance of the saliency detection is particularly good and the accuracy is higher. For general cargo ships which perform well on the original method, our method has also improved notably over the Faster R-CNN.</p><p>In terms of speed, Detector FPS of 24 is considered to real-time detector in object detection. In terms of real-time performance, the detection speed of the YOLOv2 is much better than other methods, and the FPS reaches 83, but the detection effect is not good. Since the SSD and the Faster R-CNN use the end-to-end training method, the detection effect is better, but the FPS is respectively 7 and 15, lower than the requirement. Our method gives FPS of 49, which not only guarantees the real-time performance, but also increases the accuracy by 5% against YOLOv2.</p><p>As shown examples in Fig. <ref type="figure" target="#fig_5">12</ref>, we can see the visual improvement of our proposed method against YOLOv2. Specifically, when ships intersect, the bounding box of YOLOv2 is much larger than the ship, but our method can predict a more accurate box. When the ship is small, YOLOv2 is prone to misdetection, but our method remains good. When the ship is similar to the background, YOLOv2 can easily detect the background as a ship by mistake, but our method can eliminate false detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparison with Other Saliency Detection Methods</head><p>We combined other saliency detection methods with YOLOv2 and compared them with our method. These methods include FT <ref type="bibr" target="#b35">[36]</ref>, SEG <ref type="bibr" target="#b36">[37]</ref>, CA <ref type="bibr" target="#b37">[38]</ref>, and GC <ref type="bibr" target="#b38">[39]</ref>. All experiments use the same dataset and are performed on four Titan Xp. We recorded the results of each model based on previous assessment indicators, shown in Table <ref type="table" target="#tab_2">II</ref>. Fig. <ref type="figure">13</ref> shows AP performance where the IoU threshold for each ship is set to 0.5.</p><p>As we see in Table <ref type="table" target="#tab_2">II</ref>, our method is basically the best for each category of ships. The IoU values have been improved by saliency detection, indicating that the saliency detection has refined the location, with the refinement from 70.69% to 74.53%. For FT <ref type="bibr" target="#b35">[36]</ref>, SEG <ref type="bibr" target="#b36">[37]</ref> and CA <ref type="bibr" target="#b37">[38]</ref>, in addition to fishing boats and passenger ships with salient colors and features, the mAP values of other categories of ships are basically the same as those of the YOLOv2, indicating that these three methods make little improvement of detection and cannot meet the demand at all. FT <ref type="bibr" target="#b35">[36]</ref> is mainly based on local features for detection, without considering global features. It tends to produce small salient objects other than the main ship body, so that the outer rectangle has a large error. The advantage of SEG <ref type="bibr" target="#b36">[37]</ref> is that the feature between the video sequences can be used for saliency detection, but mainly based on local features, so the effect is also not good. CA <ref type="bibr" target="#b37">[38]</ref> combines global features and local features, but it not only extracts the salient region, but also extracts the background information. In fact, it is not suitable for our dataset because our salient region generally occupies the main part of bounding box. GC <ref type="bibr" target="#b38">[39]</ref> considers both global uniformity and    color distribution, so the improvement of detection is almost the same as ours. Nevertheless, its FPS is 40, which is lower than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Comparison with or without Coastline</head><p>In the process of generating the anchor boxes, the number of boxes can be reduced by using the coastline feature, which can save the detection time. Coastline feature can also be used to remove disturbances from shore houses and improve accuracy during saliency detection. We conducted experiments to compare the results of the models with or without coastline feature. The results are shown in Table <ref type="table" target="#tab_3">III</ref>. After adding coastline feature, mAP increases by 1%, and the biggest increase is from the general cargo ship, which is 5%. We think that the color of the general cargo ship's bow is usually white and there are several rows of windows on it, which are very similar to the shore house. Thus, it is easy to falsely detect the house as part of ship. The FPS of model without coastline is 5 higher than the FPS of our method, which is due to the fact that the time required to generate the coastline exceeds the time saved by reducing the amount of anchor generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparison with other ship detection methods</head><p>Since there are no public source codes or executables for ship detection, we compare the proposed algorithm with Zhang et al. <ref type="bibr" target="#b4">[5]</ref>. Because the method cannot detect the category of ship, we separately detect the images of different categories. The results are shown in Table <ref type="table" target="#tab_4">IV</ref>. As can be seen from the table, our method is far better than comparison method for each category. Among six categories of ship, the result for fishing boat is relatively best for the comparison method, which is possibly due to the fact that the fishing boat does not intersect with the coastline, thus less affected by ashore background. Some typical detection results are shown in the Fig. <ref type="figure" target="#fig_7">14</ref>, where the buoy is mistaken as ship by the comparison method. At the same time, its FPS is much lower than ours, which confirms that our approach is comparably effective and fast under complex environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, in order to realize the real-time detection of ships in many application fields, we propose method based on convolution neural network and saliency detection. Our method generates the bounding boxes based on YOLOv2 and proposes saliency detection to predict the location of the ships in the bounding boxes. When the probability of bounding boxes is low, we use salient features to predict more accurate location in combination with the coastline feature. We train the model on the real-world ship dataset built by our own and compare it with other methods. Experimental results prove that our method is able to simultaneously result in high accuracy and fast speed against typical CNN based methods.</p><p>In the future, we will apply the detection model to achieve multi-target tracking of ships.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2019.2897980, IEEE Transactions on Circuits and Systems for Video Technology &gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt; 2 (a) ore carrier (b) bulk cargo carrier (c) general cargo ship (d) container ship (e) fishing boat (f) passenger ship Fig. 1. The ship category in our used ship dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The proposed ship detection pipeline. We first input the image into a convolutional neural network and generate anchor boxes combined with coastline feature. Then we use saliency detection which uses the spatial relationship and color space to produce more accurate ship location in combination with coastline feature.</figDesc><graphic coords="4,80.00,62.47,453.50,171.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Extraction results of the coastline. (a) All detected segments. (b) The final coastline which is up 30 pixels. (a) (b) Fig. 5. Coastline feature in generating feature map. (a) The original feature map. (b) The feature map combined with coastline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1Fig. 6 .</head><label>6</label><figDesc>ğ‘˜ Ì‡Ã— ğ‘– -ğ‘ Ì‡-ğ‘— â‰¥ 0 ğ‘˜ Ì‡Ã— (ğ‘– + 1) -ğ‘ Ì‡-ğ‘— â‰¥ 0(2)where ğ‘˜ Ì‡ and ğ‘ Ì‡ denote the coastline parameters (slope and intercept) transformed into the feature map. ğ‘– and ğ‘— are the numbers of the cells, from 1 to 13. We need to resize the coastline along with the image and get the coastline parameters on the feature map. Since the final feature map size is 13x13, the transformation of k and b obeys ğ‘˜ and ğ‘ refer to the slope and intercept of coastline in original image and ğ‘¤ and â„ denote the width and height of original image, respectively.During training stage, each of cells on the sea predicts 5 detected bounding boxes with their confidence score for containing a specific category of ship. In order to get better and more representative prior boxes, YOLOv2 uses the IoU-based K-means clustering method to train bounding boxes, which can automatically find the width and height of 5 boxes more properly.The IoU represents the overlap ratio of the resulted bounding box to the ground truth. ğ¼ğ‘œğ‘ˆ = ABCA(DD EF âˆ©DD HF ) ABCA(DD EF âˆªDD HF ) Examples on bounding boxes of the missed ships. (a) The bounding box is larger than the ship (the IoU is 0.466). (b)The bounding box contains a portion of the ship (the IoU is 0.397). (c) The bounding box is smaller than the ship (the IoU is 0.462). (a) (b) Fig. 7. Salient detection result of an image. (a) The original image. (b) The salient result of the entire image. ğµğµ KL denotes the ground truth of box, ğµğµ NL denotes the detected bounding box and ğ‘ğ‘Ÿğ‘’ğ‘(â€¢) represents the area operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Coastline feature in salient region. (a) The salient region combined with coastline. (b) The original salient region. Saliency detection result of a bounding box. (a) The salient region. (b) The location of the ship. mproved bounding boxes of missed ships by saliency. (a) The IoU of the box is increased to 0.610. (b) The IoU of the box is increased to 0.533. (c) The IoU of the box is increased to 0.698.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 )</head><label>2</label><figDesc>Mean Average Precision mAP denotes the average values of ğ´ğ‘ƒ $ of each class ğ‘–.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Precision-recall curves for each detector on six ship categories.</figDesc><graphic coords="9,107.23,567.00,397.45,151.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.14. The typical results of our proposed method (right) and Zhang's method<ref type="bibr" target="#b4">[5]</ref> (left).</figDesc><graphic coords="12,86.24,359.55,441.38,137.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Linggang Wang*, Zhongyuan Wang*, Wan Du, and Wenjing Wu</figDesc><table /><note><p>1051-8215 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DETECTION</head><label>I</label><figDesc>RESULTS OF DIFFERENT DETECTORS ON THE SEASHIP DATASET Fast â€¢ means Fast R-CNN. Faster ' means Faster R-CNN.</figDesc><table><row><cell>Model</cell><cell>mAP</cell><cell>ore carrier</cell><cell>bulk cargo carrier</cell><cell>general cargo ship</cell><cell>container ship</cell><cell>fishing boat</cell><cell>passenger ship</cell><cell>FPS (Titan Xp)</cell></row><row><cell cols="3">ğ…ğšğ¬ğ­ ğš (ğ•ğ†ğ†) 0.710 0.771</cell><cell>0.713</cell><cell>0.771</cell><cell>0.868</cell><cell>0.617</cell><cell>0.522</cell><cell>0.5</cell></row><row><cell cols="3">ğ…ğšğ¬ğ­ğğ« ğ› (ğ™ğ…) 0.892 0.905</cell><cell>0.900</cell><cell>0.908</cell><cell>0.909</cell><cell>0.857</cell><cell>0.871</cell><cell>15</cell></row><row><cell cols="3">ğ…ğšğ¬ğ­ğğ« ğ› (ğ•ğ†ğ†) 0.901 0.894</cell><cell>0.903</cell><cell></cell><cell>0.909</cell><cell>0.888</cell><cell>0.906</cell><cell>6</cell></row><row><cell>SSD</cell><cell cols="2">0.794 0.750</cell><cell>0.767</cell><cell>0.877</cell><cell>0.907</cell><cell>0.718</cell><cell>0.744</cell><cell>7</cell></row><row><cell cols="3">YOLOv2 0.830 0.849</cell><cell>0.850</cell><cell>0.881</cell><cell>0.888</cell><cell>0.733</cell><cell>0.781</cell><cell>83</cell></row><row><cell>Ours</cell><cell cols="2">0.874 0.881</cell><cell>0.876</cell><cell>0.917</cell><cell>0.903</cell><cell>0.783</cell><cell>0.886</cell><cell>49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II DETECTION</head><label>II</label><figDesc>RESULTS OF DIFFERENT SALIENCY METHODS ON THE SEASHIP DATASET</figDesc><table><row><cell>Model</cell><cell>IoU</cell><cell>mAP</cell><cell>ore carrier</cell><cell>bulk cargo carrier</cell><cell>general cargo ship</cell><cell>container ship</cell><cell>boat</cell><cell>passenger ship</cell><cell>FPS (Titan Xp)</cell></row><row><cell cols="2">YOLOv2+FT[36] 0.7133</cell><cell>0.835</cell><cell>0.840</cell><cell>0.850</cell><cell>0.880</cell><cell>0.888</cell><cell>0.768</cell><cell>0.781</cell><cell>1.5</cell></row><row><cell cols="2">YOLOv2+SEG[37] 0.7217</cell><cell>0.841</cell><cell>0.840</cell><cell>0.850</cell><cell>0.881</cell><cell>0.888</cell><cell>0.735</cell><cell>0.853</cell><cell>3</cell></row><row><cell cols="2">YOLOv2+CA[38] 0.7162</cell><cell>0.839</cell><cell>0.841</cell><cell>0.857</cell><cell>0.880</cell><cell>0.888</cell><cell>0.742</cell><cell>0.828</cell><cell>0.3</cell></row><row><cell cols="2">YOLOv2+GC[39] 0.7309</cell><cell>0.862</cell><cell>0.872</cell><cell>0.870</cell><cell>0.902</cell><cell>0.907</cell><cell>0.781</cell><cell>0.888</cell><cell>40</cell></row><row><cell>YOLOv2</cell><cell>0.7069</cell><cell>0.830</cell><cell>0.849</cell><cell>0.850</cell><cell>0.881</cell><cell>0.888</cell><cell>0.733</cell><cell>0.781</cell><cell>83</cell></row><row><cell>Ours</cell><cell>0.7453</cell><cell>0.874</cell><cell>0.881</cell><cell>0.876</cell><cell>0.917</cell><cell>0.903</cell><cell>0.783</cell><cell>0.886</cell><cell>49</cell></row></table><note><p>Fig. 13. Precision-recall curves for each salient method with YOLOv2 on six ship categories.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III DETECTION</head><label>III</label><figDesc>RESULTS WITH OR WITHOUT COASTLINE ON THE SEASHIP DATASET</figDesc><table><row><cell>Model</cell><cell>mAP</cell><cell>ore carrier</cell><cell>bulk cargo carrier</cell><cell>general cargo ship</cell><cell>container ship</cell><cell>fishing boat</cell><cell>passenge r ship</cell><cell>FPS (Titan Xp)</cell></row><row><cell>without coastline</cell><cell>0.862</cell><cell>0.872</cell><cell>0.857</cell><cell>0.870</cell><cell>0.902</cell><cell>0.781</cell><cell>0.888</cell><cell>54</cell></row><row><cell>with coastline</cell><cell>0.874</cell><cell>0.881</cell><cell>0.876</cell><cell>0.917</cell><cell>0.903</cell><cell>0.783</cell><cell>0.886</cell><cell>49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV DETECTION</head><label>IV</label><figDesc>RESULTS OF DIFFERENT SHIP DETECTION METHODS</figDesc><table><row><cell>Model</cell><cell>mAP</cell><cell>ore carrier</cell><cell>bulk cargo carrier</cell><cell>general cargo ship</cell><cell>container ship</cell><cell>fishing boat</cell><cell>passenge r ship</cell><cell>FPS (Titan Xp)</cell></row><row><cell>Ours</cell><cell>0.874</cell><cell>0.881</cell><cell>0.876</cell><cell>0.917</cell><cell>0.904</cell><cell>0.783</cell><cell>0.886</cell><cell>49</cell></row><row><cell>Zhang's[5]</cell><cell>0.487</cell><cell>0.414</cell><cell>0.432</cell><cell>0.387</cell><cell>0.462</cell><cell>0.583</cell><cell>0.502</cell><cell>2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>&gt; REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) &lt;</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ship detection is of great value in many application fields, such as ocean surveillance, port management, and navigation safety. In the field of port management, ship detection can monitor and assist in the management of maritime traffic and This work was supported in part by the National key R &amp; D plan on strategic international scientific and technological innovation cooperation special project under Grants 2016YFE0202300, the National Natural Science Foundation of China under Grants 61671332, 41771452, and 41771454, Guangzhou Science and Technology Project under Grant 201604020070, and the Key Research and Development Program of Hubei Province of China under Grant 2016AAA018. Z. Shao, L. Wang, and W. Wu are with the State Key Laboratory for</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Ship Detection And Analysis System Based On Synthetic Aperture Radar Image</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microcomputer Information</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="298" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust layer-based boat detection and multi-target-tracking in maritime environments</title>
		<author>
			<persName><forename type="first">W</forename><surname>KrÃ¼ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Orlov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International WaterSide Security Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context modeling combined with motion analysis for moving ship detection in port surveillance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41114</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic detection and tracking of ship based on mean shift in corrected video sequences</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 2nd International Conference on Image, Vision and Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="449" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ship detection for visual maritime surveillance from non-stationary platforms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ocean Engineering</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015-09">Sept. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inshore ship detection in remote sensing images based on deep features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Signal Processing, Communications and Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully Convolutional Network With Task Partitioning for Inshore Ship Detection in Optical Remote Sensing Images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1665" to="1669" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SeaShips: A Large-Scale Precisely-Annotated Dataset for Ship Detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2593" to="2604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchically Gated Deep Networks for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2267" to="2275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Computer Vision</title>
		<meeting>Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency based image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Multimedia Technology</title>
		<meeting>Int. Conf. on Multimedia Technology</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5068" to="5071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integration of the saliency-based seed extraction and random walks for image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="391" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-based active 3d point cloud segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bjorkman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Intelligent Robots &amp; Systems</title>
		<meeting>IEEE Conf. Intelligent Robots &amp; Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1165" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is bottom-up attention useful for object recognition?</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rutishauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust classification of objects, faces, and flowers using natural image statistics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2472" to="2479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Region-based saliency detection and its application in object recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Circuits &amp; Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="769" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sketch2photo: Internet image montage</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Internet visual media processing: a survey with graphics and vision applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer International Journal of Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Visual Search Inspired Computational Model for Ship Detection in Optical Satellite Images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience &amp; Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In-shore ship extraction from HR optical remote sensing image via salience structure and GIS information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Multispectral Image Processing &amp; Pattern Recognition, MIPPR 2015: Remote Sensing Image Processing, Geographic Information Systems, and Other Applications</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9815</biblScope>
			<biblScope unit="page">98150</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully Convolutional Network With Task Partitioning for Inshore Ship Detection in Optical Remote Sensing Images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience &amp; Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling attention to salient proto-objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks the Official Journal of the International Neural Network Society</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1395</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">ï¼Œ</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmenting Salient Objects from Images and Videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>HeikkilÃ¤</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context-Aware Saliency Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnikmanor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1529" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Salient object detection: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.5878" />
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Global Contrast based Salient Region Detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="416" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient Non-Maximum Suppression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR 2006)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
