<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models</title>
				<funder ref="#_HnBRwyv">
					<orgName type="full">US DARPA KAIROS</orgName>
				</funder>
				<funder ref="#_dZpAAUp #_6VvJnGh #_FAxWvqx #_7VCMfv8 #_5rYdSKn #_9hX7eCm #_DnP6cZ9">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_epMHYyH">
					<orgName type="full">IIS</orgName>
				</funder>
				<funder ref="#_F9B8rdc">
					<orgName type="full">INCAS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-24">24 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>Jiang</surname></persName>
							<email>jimeng@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shivam</forename><surname>Agarwal</surname></persName>
							<email>shivama2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Jin</surname></persName>
							<email>bowenj4@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
							<email>xuanw@vt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-24">24 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.15597v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TAGREAL achieves state-of-the-art performance on two benchmark datasets. We find that TAGREAL has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A knowledge graph (KG) is a heterogeneous graph that encodes factual information in the form of entity-relation-entity triplets, where a relation connects a head entity and a tail entity (e.g., "Miami-located_in-USA") <ref type="bibr" target="#b39">(Wang et al., 2017;</ref><ref type="bibr" target="#b17">Hogan et al., 2021)</ref>. KG <ref type="bibr" target="#b7">(Dai et al., 2020)</ref> plays a central role in many NLP applications, including question answering <ref type="bibr" target="#b16">(Hao et al., 2017;</ref><ref type="bibr" target="#b44">Yasunaga et al., 2021)</ref>, recommender systems <ref type="bibr" target="#b46">(Zhou et al., 2020)</ref>, and drug discovery <ref type="bibr" target="#b47">(Zitnik et al., 2018)</ref>. However, existing works <ref type="bibr" target="#b38">(Wang et al., 2018;</ref><ref type="bibr" target="#b13">Hamilton et al., 2018)</ref> show that most large-scale KGs are incomplete and cannot fully cover the massive real-world knowledge. This challenge motivates KG completion, which aims to find one or more object entities given a subject entity and a relation <ref type="bibr" target="#b22">(Lin et al., 2015)</ref>. For example, in Figure <ref type="figure">1</ref>, our goal is to predict the object entity with "Detroit" as the subject entity and "contained_by" as the relation.</p><p>However, existing KG completion approaches <ref type="bibr">(Trouillon et al., 2016b;</ref><ref type="bibr" target="#b9">Das et al., 2018)</ref> have sev- Michigan (I'm pretty sure!!!)</p><p>Figure <ref type="figure">1</ref>: The quality of hand-crafted prompts can be limited, while prompt mining is a scalable alternative. Support information also helps PLM understand the purpose of prompts. In this example, Canada and Michigan are potentially valid options but given prompt mining and support information retrieval, the model becomes confident about Michigan as the answer here.</p><p>eral limitations <ref type="bibr" target="#b12">(Fu et al., 2019)</ref>. First, their performance heavily depends on the density of the graph. They usually perform well on dense graphs with rich structural information but poorly on sparse graphs which are more common in real-world applications. Second, previous methods (e.g., <ref type="bibr" target="#b4">Bordes et al. (2013)</ref>) assume a closed-world KG without considering vast open knowledge in the external resources. In fact, in many cases, a KG is usually associated with a rich text corpus <ref type="bibr" target="#b3">(Bodenreider, 2004)</ref>, which contains a vast amount of factual data not yet extracted. To overcome these challenges we investigate the task of open knowledge graph completion, where KG can be constructed using new facts from outside the KG. Recent text-enriched solutions <ref type="bibr" target="#b12">(Fu et al., 2019)</ref> focus on using a predefined set of facts to enrich the knowledge graph. Nonetheless, the pre-defined set of facts is often noisy and constricted, that is, they do not provide sufficient information to efficiently update the KG.</p><p>Pre-trained language models (PLMs) <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019a)</ref> have shown to be powerful in capturing factual knowledge implicitly from learning on massive unlabeled texts <ref type="bibr">(Petroni et al., 2019b)</ref>. Since PLMs are superb in text encoding, they can be utilized to facilitate knowledge graph completion with external text information. Recent knowledge graph completion methods <ref type="bibr" target="#b34">(Shin et al., 2020;</ref><ref type="bibr" target="#b27">Lv et al., 2022)</ref> focus on using manually crafted prompts (e.g., "Detroit is located in <ref type="bibr">[MASK]</ref>" in Figure <ref type="figure">1</ref>) to query the PLMs for graph completion (e.g., "Michigan"). However, manually creating prompts can be expensive with limited quality (e.g., PLM gives a wrong answer "Canada" to the query with a handcrafted prompt, as shown in Figure <ref type="figure">1</ref>).</p><p>Building on the above limitations of standard KG and the enormous power of PLMs <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019a)</ref>, we aim to use PLMs for open knowledge graph completion. We propose an end-to-end framework that jointly exploits the implicit knowledge in PLMs and textual information in the corpus to perform knowledge graph completion (as shown in Figure <ref type="figure">1</ref>). Unlike existing works (e.g., <ref type="bibr" target="#b12">(Fu et al., 2019;</ref><ref type="bibr" target="#b27">Lv et al., 2022)</ref>), our method does not require a manually pre-defined set of facts and prompts, which is more general and easier to adapt to real-world applications.</p><p>Our contributions can be summarized as:</p><p>?</p><p>We study the open KG completion problem that can be assisted by facts captured from PLMs. To this end, we propose a new framework TAGREAL that denotes text augmented open KG completion with real-world knowledge in PLMs.</p><p>? We develop prompt generation and information retrieval methods, which enable TAGREAL to automatically create highquality prompts for PLM knowledge probing and search support information, making it more practical especially when PLMs lack some domain knowledge.</p><p>? Through extensive quantitative and qualitative experiments on real-world knowledge graphs such as Freebase<ref type="foot" target="#foot_0">1</ref> we show the applicability and advantages of our framework<ref type="foot" target="#foot_1">2</ref> .</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KG Completion Methods</head><p>KG completion methods can be categorized into embedding-based and PLM-based methods.</p><p>Embedding-based methods represent entities and relations as embedding vectors and maintain their semantic relations in the vector space. TransE <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref> vectorizes the head, the relation and the tail of triples into a Euclidean space. Dist-Mult <ref type="bibr" target="#b42">(Yang et al., 2014</ref>) converts all relation embeddings into diagonal matrices in bilinear models. RotatE <ref type="bibr" target="#b35">(Sun et al., 2019)</ref> presents each relation embedding as a rotation in complex vector space from the head entity to the tail entity.</p><p>In recent years, researchers have realized that PLMs can serve as knowledge bases <ref type="bibr">(Petroni et al., 2019a;</ref><ref type="bibr" target="#b45">Zhang et al., 2020;</ref><ref type="bibr" target="#b0">AlKhamissi et al., 2022)</ref>. PLM-based methods for KG completion <ref type="bibr" target="#b43">(Yao et al., 2019;</ref><ref type="bibr" target="#b20">Kim et al., 2020;</ref><ref type="bibr" target="#b6">Chang et al., 2021;</ref><ref type="bibr" target="#b27">Lv et al., 2022)</ref> start to gain attention. As a pioneer, KG-BERT <ref type="bibr" target="#b43">(Yao et al., 2019)</ref> fine-tunes PLM with concatenated head, relation, and tail in each triple, outperforming the conventional embedding-based methods in link prediction tasks. <ref type="bibr" target="#b27">Lv et al.(2022)</ref> present PKGC, which uses manually designed triple prompts and carefully selected support prompts as inputs to the PLM. Their result shows that PLMs could be used to substantially improve the KG completion performance, especially in the open-world <ref type="bibr" target="#b33">(Shi and Weninger, 2018)</ref> setting. Compared to PKGC, our framework TAGREAL automatically generates prompts of higher quality without any domain expert knowledge. Furthermore, instead of pre-supposing the existence of support information, we search relevant textual information from the corpus with an information retrieval method to support the PLM knowledge probing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Probing using Prompts</head><p>LAMA <ref type="bibr">(Petroni et al., 2019a)</ref> is the first framework for knowledge probing from PLMs. The prompts are manually created with a subject placeholder and an unfilled space for the object. For example, a triple query (Miami, location, ?) may have a prompt "Miami is located in [MASK]" where "&lt;subject&gt; is located in [MASK]" is the template for "location" relation. The training goal is to correctly fill [MASK]with PLM's prediction. Another work, BertNet <ref type="bibr" target="#b15">(Hao et al., 2022)</ref>, proposes an approach applying GPT-3 <ref type="bibr" target="#b5">(Brown et al., 2020</ref>   to automatically generate a weighted prompt ensemble with input entity pairs and a manual seed prompt. It then uses PLM again to search and select top-ranked entity pairs with the ensemble for KG completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prompt Mining Methods</head><p>When there are several relations to interpret, manual prompt design is costly due to the requirement of domain expert knowledge. In addition, the prompt quality could not be ensured. Hence, quality prompt mining catches the interest of researchers. <ref type="bibr" target="#b19">Jiang et al. 2020</ref> propose an approach MINE which searches middle words or dependency paths between the given inputs and outputs in a large text corpus (e.g., Wikipedia). They also propose a reasonable approach to optimize the ensemble of the mined prompts by weighting prompt individuals regarding their performance on the PLM.</p><p>Before the emergence and widespread use of PLMs, textual pattern mining performed a similar function to find reliable patterns for information extraction. For instance, MetaPAD <ref type="bibr" target="#b18">(Jiang et al., 2017)</ref> generates quality meta patterns by context-aware segmentation with the pattern quality function, and TruePIE <ref type="bibr" target="#b21">(Li et al., 2018)</ref> proposes the concept of pattern embedding and a self-training framework, that discovers positive patterns automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose TAGREAL, a PLM-based framework to handle KG completion tasks. In contrast to the previous work, our framework does not rely on handcrafted prompts or pre-defined relevant facts. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we automatically create appropriate prompts and search relevant support information, which are further utilized as templates to explore implicit knowledge from PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Knowledge graph completion is to add new triples (facts) to the existing triple set of a KG. There are two tasks to achieve this goal. The first is triple classification, which is a binary classification task to predict whether a triple (h, r, t) belongs to the KG, where h, r, t denote head entity, relation and tail entity respectively. The second task is link prediction, which targets on predicting either the tail entity t with a query (h, r, ?) or the head entity h with a query (?, r, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt Generation</head><p>Previous studies (e.g., <ref type="bibr" target="#b19">Jiang et al. (2020)</ref>) demonstrate that the accuracy of relational knowledge extracted from PLMs heavily relies on the quality of prompts used for querying. To this end, we develop a comprehensive approach for automatic quality prompt generation given triples in KG as the only input, as shown in Figure <ref type="figure" target="#fig_8">3</ref>. We use textual pattern mining methods to mine quality patterns from large corpora as the prompts used for PLM knowledge probing. As far as we know, we are pioneers in using textual pattern mining methods for LM prompt mining. We believe in the applicability of this approach for the following reasons.</p><p>? Similar data sources. We apply pattern mining on large corpora (e.g., Wikipedia) which are the data sources where most of PLMs are pre-trained.</p><p>? Similar objectives. Textual pattern mining is to mine patterns to extract new information from large corpora; prompt mining is to mine prompts to probe implicit knowledge from PLMs.</p><p>? Similar performance criteria. The reliability of a pattern or a prompt is indicated by how many accurate facts it can extract from corpora/PLMs.</p><p>Sub-corpora mining is the first step that creates the data source for the pattern mining. Specifically, given a KG with a relation set R = (r 1 , r 2 , ..., r k ), we first extract tuples T r i paired by head entities and tail entities for each relation r i ? R from the KG. For example, for the relation r 1 : /business/company/founder, we extract all tuples like &lt;microsoft, bill_gates&gt; in this relation from the KG. For each tuple t j , we then search sentences s t j containing both head and tail from a large corpus (e.g., Wikipedia) and other reliable sources, which is added to compose the sub-corpus C r i . We limit the size of each set to ? </p><formula xml:id="formula_0">? ! " : ? # " : ? $ " : ? % " : ? ? &amp;'# " : ? &amp;'! " : ?&amp; " : Prompt Selection Prompt Optimization ? !,! : [Y], founder of [X] ? !,# : [Y], co-founder of [X] ? ? !,$ : [Y], chairman of [X]</formula><p>Final prompts for relation  </p><formula xml:id="formula_1">? %,! : [X], neighborhood of [Y] ? %,# : [Y] 's [X] neighborhood ? ? %,$ : [X] area of [Y]</formula><formula xml:id="formula_2">L 7 F g D F 7 b V O D 1 f 3 W b 3 B s = " &gt; A A A C E n i c b V B N S 8 N A E J 3 4 W e t X 1 K O X x S L q p S Q q 6 r H o x W M F + w F N D Z v t t l 2 6 2 Y T d j V B C f o M X / 4 o X D 4 p 4 9 e T N f + O m 7 a G 2 P h h 4 v D f D z L w g 5 k x p x / m x F h a X l l d W C 2 v F 9 Y 3 N r W 1 7 Z 7 e u o k Q S W i M R j 2 Q z w I p y J m h N M 8 1 p M 5 Y U h w G n j W B w k / u N R y o V i 8 S 9 H s a 0 H e K e Y F 1 G s D a S b 5 9 4 I d Z 9 g n l a z f x U + m 7 2 k B 5 l y G M C T T m 5 5 t s l p + y M g O a J O y E l m K D q 2 9 9 e J y J J S I U m H C v V c p 1 Y t 1 M s N S O c Z k U v U T T G Z I B 7 t G W o w C F V 7 X T 0 U o Y O j d J B 3 U i a E h q N 1 O m J F I d K D c P A d O Z n q l k v F / / z W o n u X r V T J u J E U 0 H G i 7 o J R z p C e T 6 o w y Q l m g 8 N w U Q y c y s i f S w x 0 S b F o g n B n X 1 5 n t R P y + 5 F + e z u v F S 5 n s R R g H 0 4 g G N w 4 R I q c A t V q A G B J 3 i B N 3 i 3 n q 1 X</formula><formula xml:id="formula_3">K 6 S g x w Q W z k = " &gt; A A A C C n i c b V B N S 8 N A E J 3 4 W e t X 1 K O X 1 S J 4 K o m K e i x 6 8 V j B f k B T y m a 7 a Z d u N m F 3 I 5 S Q s x f / i h c P i n j 1 F 3 j z 3 7 h p c 6 i t D w Y e 7 8 0 w M 8 + P O V P a c X 6 s p e W V 1 b X 1 0 k Z 5 c 2 t 7 Z 9 f e 2 2 + q K J G E N k j E I 9 n 2 s a K c C d r Q T H P a j i X F o c 9 p y x / d 5 n 7 r k U r F I v G g x z H t h n g g W M A I 1 k b q 2 U d e i P W Q Y J 7 W s 1 4 q e 6 M M e U y g W d W u O F V n A r R I 3 I J U o E C 9 Z 3 9 7 / Y g k I R W a c K x U x 3 V i 3 U 2 x 1 I x w m p W 9 R N E Y k x E e 0 I 6 h A o d U d d P J K x k 6 M U o f B Z E 0 J T S a q L M T K Q 6 V G o e + 6 c x P V P N e L v 7 n d R I d X H d T J u J E U 0 G m i 4 K E I x 2 h P B f U Z 5 I S z c e G Y C K Z u R W R I Z a Y a J N e 2 Y T g z r + 8 S J p n V f e y e n 5 / U a n d F H G U 4 B C O 4 R R c u I I a 3 E E d G k D g C V 7 g D d 6 t Z + v V + r A + p 6 1 L V j F z A H 9 g f f 0 C 1 u K a / w = = &lt; / l a t e x i t &gt;</formula><p>Pr k 2 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 + m c S s x W B b x f 8 V z L G u u p e 2 5 v Q X k = " &gt; A A A C C n i c b V B N S 8 N A E J 3 4 W e t X 1 K O X 1 S J 4 K o m K e i x 6 8 V j B f k B T y m a 7 a Z d u N m F 3 I 5 S Q s x f / i h c P i n j 1 F 3 j z 3 7 h p c 6 i t D w Y e 7 8 0 w M 8 + P O V P a c X 6 s p e W V 1 b X 1 0 k Z 5 c 2 t 7 Z 9 f e 2 2 + q K J G E N k j E I 9 n 2 s a K c C d r Q T H P a j i X F o c 9 p y x / d 5 n 7 r k U r F I v G g x z H t h n g g W M A I 1 k b q 2 U d e i P W Q Y J 7 W s 1 4 q e 2 6 G P</p><formula xml:id="formula_4">C b Q r G p X n K o z A V o k b k E q U K D e s 7 + 9 f k S S k A p N O F a q 4 z q x 7 q Z Y a k Y 4 z c p e o m i M y Q g P a M d Q g U O q u u n k l Q y d G K W P g k i a E h p N 1 N m J F I d K j U P f d O Y n q n k v F / / z O o k O r r s p E 3 G i q S D T R U H C k Y 5 Q n g v q M 0 m J 5 m N D M J H M 3 I r I E E t M t E m v</formula><p>b E J w 5 1 9 e J M 2 z q n t Z P b + / q N R u i j h K c A j H c A o u X E E N 7 q A O D S D w B C / w B u / W s / V q f V i f 0 9 Y l q 5 g 5 g D + w v n 4 B e y C a x Q = = &lt; / l a t e x i t &gt; Pr 1 2 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H J 7 g e k n R g e X 4 T X p 1 z 7 + f 4 5 9 A e u 0 = "</p><formula xml:id="formula_5">&gt; A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 C R b B U 0 l U 1 G P R i 8 c K 1 h b S U D b b b b t 0 s x t 2 X 4 Q S + j O 8 e F D E q 7 / G m / / G T Z u D t g 4 s D D P v s f M m S g Q 3 6 H n f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 G p V q y l p U C a U 7 E T F M c M l a y F G w T q I Z i S P B 2 t H 4 N v f b T 0 w b r u Q D T h I W x m Q o + Y B T g l Y K u j H B E</formula><p>S U i a 0 5 7 1 Z p X 9 2 Z w l 4 l f k B o U a P a q X 9 2 + o m n M J F J B j A l 8 L 8 E w I x o 5 F W x a 6 a a G J Y S O y Z A F l k o S M x N m s 8 h T 9 8 Q q f X e g t H 0 S 3 Z n 6 e y M j s T G T O L K T e U S z 6 O X i f 1 6 Q 4 u A 6 z L h M U m S S z j 8 a p M J F 5 e b 3 u 3 2 u G U U x s Y R Q z W 1 W l 4 6 I J h R t S x V b g r 9 4 8 j J 5 P K v 7 l / X z + 4 t a 4 6 a o o w x H c A y n 4 M M V N O A O m t A C C g q e 4 R X e H H R e n H f n Y z 5 a c o q d Q / g D 5 / M H i b W R b g = = &lt; / l a t e x i t &gt; P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 X I + w J j q E G R / m + o h f I G x T E I g v C k = " &gt; A A A B 9 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y q q M e i F 4 9 V 7 A e 0 S 8 m m 2 T Y 0 m 1 2 T b K E s + z u 8 e F D E q z / G m / / G t N 2 D t j 4 Y e L w 3 w 8 w 8 <ref type="table" target="#tab_7">P 8 A y v 8 I b G 6 A W 9 o 4 9 5 a w H l M 4</ref>  </p><formula xml:id="formula_6">P x Z c G 8 f 5 R o W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q 1 F G i K G v Q S E S q 7 R P N B J e s Y b g R r B 0 r R k J f s J Y / u p 3 6 r T F T m k f y 0 U x i 5 o V k I H n A K T F W 8 l L V c z P c 5 R K n D 1 m v X H G q z g x 4 m b g 5 q U C O e q / 8 1 e 1 H N A m Z N F Q Q r T u u E x s v J c p w K l h W 6 i a a x Y S O y I B 1 L J U k Z N p L Z 0 d n + M Q q f R x E y p Y 0 e K b + n k h J q P U k 9 G 1 n S M x Q L 3 p T 8 T + v k 5 j g 2 k u 5 j B P D J J 0 v C h K B T Y S n C e A + V 4 w a M b G E U M X t r Z g O i S L U 2 J x K N g R 3 8 e V l 0 j y r u p f V 8 / u L S u 0 m j 6 M I R 3 A M p + D C F d T g D u r Q A A p</formula><formula xml:id="formula_7">f w B + j z B z C C k b s = &lt; / l a t e x i t &gt; r1 2 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z 6 n O t f Q d w 4 A z B R l R y s y X k v m v s V o = " &gt; A A A B 9 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y q q M e i F 4 9 V 7 A e 0 S 8 m m 2 T Y 0 m 1 2 T b K E s + z u 8 e F D E q z / G m / / G t N 2 D t j 4 Y e L w 3 w 8 w 8 P x Z c G 8 f 5 R o W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q 1 F G i K G v Q S E S q 7 R P N B J e s Y b g R r B 0 r R k J f s J Y / u p 3 6 r T F T m k f y 0 U x i 5 o V k I H n A K T F W 8 l L V G 2 W 4 y y V O H 7 J e u e J U n R n w M n F z U o E c 9 V 7 5 q 9 u P a B I y a a g g W n d c J z Z e S p T h V L C s 1 E 0 0 i w k d k Q H r W C p J y L S X z o 7 O 8 I l V + j i I l C 1 p 8 E z 9 P Z G S U O t J 6 N v O k J i h X v S m 4 n 9 e J z H B t Z d y G S e G S T p f F C Q C m w h P E 8 B 9 r h g 1 Y m I J o Y r b W z E d E k W o s T m V b A j u 4 s v L p H l W d S + r 5 / c X l d p N H k c R j u A Y T s G F K 6 j B H d S h A R S e 4 B l e 4 Q 2 N 0 Q t 6 R x / z 1 g L K Z w 7 h D 9 D n D 4 p 0 k f U = &lt; / l a t e x i t &gt; rk 2 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 X I + w J j q E G R / m + o h f I G x T E I g v C k = " &gt; A A A B 9 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y q q M e i F 4 9 V 7 A e 0 S 8 m m 2 T Y 0 m 1 2 T b K E s + z u 8 e F D E q z / G m / / G t N 2 D t j 4 Y e L w 3 w 8 w 8 P x Z c G 8 f 5 R o W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q 1 F G i K G v Q S E S q 7 R P N B J e s Y b g R r B 0 r R k J f s J Y / u p 3 6 r T F T m k f y 0 U x i 5 o V k I H n A K T F W 8 l L V c z P c 5 R K n D 1 m v X H G q z g x 4 m b g 5 q U C O e q / 8 1 e 1 H N A m Z N F Q Q r T u u E x s v J c p w K l h W 6 i a a x Y S O y I B 1 L J U k Z N p L Z 0 d n + M Q q f R x E y p Y 0 e K b + n k h J q P U k 9 G 1 n S M x Q L 3 p T 8 T + v k 5 j g 2 k u 5 j B P D J J 0 v C h K B T Y S n C e A + V 4 w a M b G E U M X t r Z g O i S L U 2 J x K N g R 3 8 e V l 0 j y r u p f V 8 / u L S u 0 m j 6 M I R 3 A M p + D C F d T g D u r Q A A p P 8 A y v 8 I b G 6 A W 9 o 4 9 5 a w H l M 4 f w B + j z B z C C k b s = &lt; / l a t e x i t &gt; r1 2 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W n o 3 k Q y y i 9 i Y + T 7 k R p E h b F M l v C M = " &gt; A A A B + n i c b V D L T s M w E N z w L O W V w p G L R Y X E q U o A A c c K L h y L 1 J f U R p H j O q 1 V x 4 l s B 1 S F f g o X D i D E l S / h x t / g t D l A y 0 g r j W d 2 5 d 0 J E s 6 U d p x v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 K w d t F a e S 0 B a J e S y 7 A V a U M 0 F b m m l O u 4 m k O A o 4 7 Q T j 2 9 z v P F C p W C y a e p J Q L 8 J D w U J G s D a S b 1 e y 5 t T P p O 9 O U Z 8 J l L / s q l N z Z k D L x C 1 I F Q o 0 f P u r P 4 h J G l G h C c d K 9 V w n 0 V 6 G p W a E 0 2 m 5 n y q a Y D L G Q 9 o z V O C I K i + b r T 5 F J 0 Y Z o D C W p o R G M / X 3 R I Y j p S Z R Y D o j r E d q 0 c v F / 7 x e q s N r L 2 M i S T U V Z P 5 R m H K k Y 5 T n g A Z M U q L 5 x B B M J D O 7 I j L C E h N t 0 i q b E N z F k 5 d J + 6 z m X t b O 7 y + q 9 Z s i j h I c w T G c g g t X U I c 7 a E A L C D z C M 7 z C m / V k v V j v 1 s e 8 d c U q Z g 7 h D 6 z P H 9 8 Z k 8 E = &lt; / l a t e x i t &gt; Tr 1 2 T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e u Q r Q r A Z N D 8 Z B 8 3 M 6 D K u n N K Q G j M = " &gt; A A A B 6 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y q q M e g F 4 / x k Q c k S 5 i d z C Z D Z m e X m V 4 h L P k E L x 4 U 8 e o X e f N v n C R 7 0 G h B Q 1 H V T X d X k E h h 0 H W / n M L S 8 s r q W n G 9 t L G 5 t b 1 T 3 t 1 r m j j V j D d Y L G P d D q j h U i j e Q I G S t x P N a R R I 3 g p G 1 1 O / 9 c i 1 E b F 6 w H H C / Y g O l A g F o 2 i l + + x u 0 i t X 3 K o 7 A / l L v J x U I E e 9 V / 7 s 9 m O W R l w h k 9 S Y j u c m 6 G d U o 2 C S T 0 r d 1 P C E s h E d 8 I 6 l i k b c + N n s 1 A k 5 s k q f h L G 2 p Z D M 1 J 8 T G Y 2 M G U e B 7 Y w o D s 2 i N x X / 8 z o p h p d + J l S S I l d s v i h M J c G Y T P 8 m f a E 5 Q z m 2 h D I t 7 K 2 E D a m m D G 0 6 J R u C t / j y X 9 I 8 q X r n 1 d P b s 0 r t K o + j C A d w C M f g w Q X U 4 A b q 0 A A G A 3 i C F 3 h 1 p P P s v D n v 8 9 a C k 8 / s w y 8 4 H 9 9 y T Y 3 q &lt; / l a t e x i t &gt; R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K M p J y 7 h E U e q e V 3 / p w W G E k d P f l p U = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K x h b a U D b b T b t 0 s w m 7 E 6 G E / g Y v H h T x 6 g / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 7 d R v P X F t R K I e c J z y I K Y D J S L B K F r J z 3 X P m / S q N b f u z k C W i V e Q G h R o 9 q p f 3 X 7 C s p g r Z J I a 0 / H c F I O c a h R M 8 k m l m x m e U j a i A 9 6 x V N G Y m y C f H T s h J 1 b p k y j R t h S S m f p 7 I q e x M e M 4 t J 0 x x a F Z 9 K b i f 1 4 n w + g 6 y I V K M + S K z R d F m S S Y k O n n p C 8 0 Z y j H l l C m h b 2 V s C H V l K H N p 2 J D 8 B Z f X i a P Z 3 X v s n 5 + f 1 F r 3 B R x l O E I j u E U P L i C B t x B E 3 x g I O A Z X u H N U c 6 L 8 + 5 8 z F t L T j F z C H / g f P 4 A y d e O r g = = &lt; / l a t e x i t &gt; r1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N Y b k F H 8 3 b X n P U s o v Q j x p G b 4 P A c g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K x h b a U D b b S b t 0 s w m 7 G 6 G E / g Y v H h T x 6 g / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T A X X x n W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 4 1 E m m G P o s E Y l q h 1 S j 4 B J 9 w 4 3 A d q q Q x q H A V j i 6 n f q t J 1 S a J / L B j F M M Y j q Q P O K M G i v 5 u e q N J r 1 q z a 2 7 M 5 B l 4 h W k B g W a v e p X t 5 + w L E Z p m K B a d z w 3 N U F O l e F M 4 K T S z T S m l I 3 o A D u W S h q j D v L Z s R N y Y p U + i R J l S x o y U 3 9 P 5 D T W e h y H t j O m Z q g X v a n 4 n 9 f J T H Q d 5 F y m m U H J 5 o u i T B C T k O n n p M 8 V M i P G l l C m u L 2 V s C F V l B m b T 8 W G 4 C 2 + v E w e z + r e Z f 3 8 / q L W u C n i K M M R H M M p e H A F D b i D J v j A g M M z v M K b I 5 0 X 5 9 3 5 m L e W n G L m E P 7 A + f w B I g i O 6 A = = &lt; / l a t e x i t &gt; rk &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F q 2 6 Q n b I Z K Q H B a z X d N i F b S C 6 B w Y = " &gt; A A A C D n i c b Z D L S s N A F I Y n 9 V b r L e r S z W A p V J C S q K i 4 K r p x W c F e o A 1 h M p 2 0 Q y c X Z i Z i i H k C N 7 6 K G x e K u H X t z r d x k m a h r T 8 M f P z n H O a c 3 w k Z F d I w v r X S w u L S 8 k p 5 t b K 2 v r G 5 p W / v d E Q Q c U z a O G A B 7 z l I E E Z 9 0 p Z U M t I L O U G e w 0 j X m V x l 9 e 4 d 4 Y I G / q 2 M Q 2 J 5 a O R T l 2 I k l W X r t Y G H 5 B g j l r R S O + G 2 m V 5 A h f U k T h + S + / Q w t w 5 s v W o 0 j F x w H s w C q q B Q y 9 a / B s M A R x 7 x J W Z I i L 5 p h N J K E J c U M 5 J W B p E g I c I T N C J 9 h T 7 y i L C S / J w U 1 p Q z h G 7 A 1 f M l z N 3 f E w n y h I g 9 R 3 V m y 4 v Z W m b + V + t H 0 j 2 3 E u q H k S Q + n n 7 k R g z K A G b Z w C H l B E s W K 0 C Y U 7 U r x G P E E Z Y q w Y o K w Z w 9 e R 4 6 R w 3 z t H F 8 c 1 J t X h Z x l M E e 2 A d 1 Y I I z 0 A T X o A X a A I N H 8 A x e w Z v 2 p L 1 o 7 9 r H t L W k F T O 7 4 I + 0 z x + W k Z x y &lt; / l a t e x i t &gt; Pr 1 : P (y|x, r1) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F M e p R g r T d W s R u 9 3 R p w k Z 0 Y d N e Z 8 = " &gt; A A A B + n i c b V D L T s M w E N z w L O W V w p G L R Y X E q U o A A c e K X j g W i T 6 k N o o c 1 2 2 t O k 5 k O 6 A q 5 F O 4 c A A h r n w J N / 4 G p 8 0 B W k Z a a T y z K + 9 O E H O m t O N 8 W y u r a + s b m 6 W t 8 v b O 7 t 6 + X T l o q y i R h L Z I x C P Z D b C i n A n a 0 k x z 2 o 0 l x W H A a S e Y N H K / 8 0 C l Y p G 4 1 9 O Y e i E e C T Z k B G s j + X Y l b W R + K n 0 3 Q 3 0 m U P 6 y q 0 7 N m Q E t E 7 c g V S j Q 9 O 2 v / i A i S U i F J h w r 1 X O d W H s p l p o R T r N y P 1 E 0 x m S C R 7 R n q M A h V V 4 6 W z 1 D J 0 Y Z o G E k T Q m N Z u r v i R S H S k 3 D w H S G W I / V o p e L / 3 m 9 R A + v v Z S J O N F U k P l H w 4 Q j H a E 8 B z R g k h L N p 4 Z g I p n Z F Z E x l p h o k 1 b Z h O A u n r x M 2 m c 1 9 7 J 2 f n d R r d 8 U c Z T g C I 7 h F F y 4 g j r c Q h N a Q O A R n u E V 3 q w n 6 8 V 6 t z 7 m r S t W M X M I f 2 B 9 / g C q g Z O f &lt; / l a t e x i t &gt; Cr 1 2 C &lt; l a t e x i t s h a _ b a s e = " X I + w J j q E G R / m + o h f I G x T E I g v C k = " &gt; A A A B H i c b V B N S w M x E J U r q / q h B I v g q e y q q M e i F V A e S m m T Y m T b K E s + z u e F D E q z / G m / / G t N D t j Y e L w w w P x Z c G f R o W V b X j e J m a W t Z e v v H / Q F G i K G v Q S E S q R P N B J e s Y b g R r B r R k J f s J Y / u p r T F T m k f y U x i o V k I H n A K T F W l L V c z P c R K n D m v X H G q z g x m b g q U C O e q / e H N A m Z N F Q Q r T u u E x s v J c p w K l h W i a a x Y S O y I B L J U k Z N p L Z d n + M Q q f R x E y p Y e K b + n k h J q P U k G n S M x Q L p T T + v k j g k u j B P D J J v C h K B T Y S n C e A + V w a M b G E U M X t r Z g O i S L U J x K N g R e V l j y r u p f V / u L S u m j M I R A M p + D C F d T g D u r Q A A p P A y v I b G A W o a w H l M f w B + j z B z C C k b s = &lt; / l a t e x i t &gt; r1 2 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 X I + w J j q E G R / m + o h f I G x T E I g v C k = " &gt; A A A B 9 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e y q q M e i F 4 9 V 7 A e 0 S 8 m m 2 T Y 0 m 1 2 T b K E s + z u 8 e F D E q z / G m / / G t N 2 D t j 4 Y e L w 3 w 8 w 8 P x Z c G 8 f 5 R o W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / Q 1 F G i K G v Q S E S q 7 R P N B J e s Y b g R r B 0 r R k J f s J Y / u p 3 6 r T F T m k f y 0 U x i 5 o V k I H n A K T F W 8 l L V c z P c 5 R K n D 1 m v X H G q z g x 4 m b g 5 q U C O e q / 8 1 e 1 H N A m Z N F Q Q r T u u E x s v J c p w K l h W 6 i a a x Y S O y I B 1 L J U k Z N p L Z 0 d n + M Q q f R x E y p Y 0 e K b + n k h J q P U k 9 G 1 n S M x Q L 3 p T 8 T + v k 5 j g 2 k u 5 j B P D J J 0 v C h K B T Y S n C e A + V 4 w a M b G E U M X t r Z g O i S L U 2 J x K N g R 3 8 e V l 0 j y r u p f V 8 / u L S u 0 m j 6 M I R 3 A M p + D C F d T g D u r Q A A p P 8 A y v 8 I b G 6 A W 9 o 4 9 5 a w H l M 4 f w B + j z B z C C k b s = &lt; / l a t e x i t &gt; r1 2 R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D R Z d o q l G v k G L d 3 B q M I f k 4 D J E F q 8 = " &gt; A A A B 8 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K 9 k P a E D b b T b t 0 d x N 2 J 0 I J + R V e P C j i 1 Z / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u w H W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 o m z j V l L V o L G L d D Y l h g i v W A g 6 C d R P N i A w F 6 4 T j 2 6 n f e W L a 8 F g 9 w C R h v i R D x S N O C V j p M T N 5 k E H g 5 U G 1 5 t b d G f A y 8 Q p S Q w W a Q f W r P 4 h p K p k C K o g x P c 9 N w M + I B k 4 F y y v 9 1 L C E 0 D E Z s p 6 l i k h m / G x 2 c I 5 P r D L A U a x t K c A z 9 f d E R q Q x E x n a T k l g Z B a 9 q f i f 1 0 s h u v Y z r p I U m K L z R V E q M M R 4 + j 0 e c M 0 o i I k l h G p u b 8 V 0 R D S h Y D O q 2 B C 8 x Z e X S f</formula><formula xml:id="formula_8">I S + 5 R p V W v n U Y T t U U = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K x h b a U D b b T b t 0 s w m 7 E 6 G E / g Y v H h T x 6 g / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 7 d R v P X F t R K I e c J z y I K Y D J S L B K F r J z 7 H n T X r V m l t 3 Z y D L x C t I D Q o 0 e 9 W v b j 9 h W c w V M k m N 6 X h u i k F O N Q o m + a T S z Q x P K R v R A e 9 Y</formula><p>q m j M T Z D P j p 2 Q E 6 v 0 S Z R o W w r J T P 0 9 k d P Y m H E c 2 s 6 Y 4 t A s e l P x P 6 + T Y X Q d 5 E K l G X L F 5 o u i T B J M y P R z 0 h e a M 5 R j S y j T w t 5 K 2 J B q y t D m U 7 E h e I s v L 5 P H s 7 p 3 W T + / v 6 g 1 b o o 4 y n A E x 3 A K H l x B A + 6 g C T 4 w E P A M r / D m K O f F e X c + 5 q 0 l p 5 g 5 h D 9 w P n 8 A z O W O s A = = &lt; / l a t e x i t &gt; t1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d P W j q X 6 / n V h I Z X 4 Q h S r A K W 9 2 L P E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q q M e i F   for each tuple to mine more generic patterns for future applications.</p><formula xml:id="formula_9">s k 8 k a Q 9 u Z C U H B T d D t Y f V t r E x k = " &gt; A A A B 8 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K 9 k P a E D b b T b t 0 d x N 2 J 0 I J + R V e P C j i 1 Z / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u w H W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 o m z j V l L V o L G L d D Y l h g i v W A g 6 C d R P N i A w F 6 4 T j 2 6 n f e W L a 8 F g 9 w C R h v i R D x S N O C V j p M T N 5 k E H A 8 6 B a c + v u D H i Z e A W p o Q L N o P r V H 8 Q 0 l U w B F c S Y n u c m 4 G d E A 6 e C 5 Z V + a l h C 6 J g M W c 9 S R S Q z f j Y 7 O M c n V h n g K N a 2 F O C Z + n s i I 9 K Y i Q x t p y Q w M o v e V P z P 6 6 U Q X f s Z V 0 k K T N H 5 o i g V G G I 8 / R 4 P u G Y U x M Q S Q j W 3 t 2 I 6 I p p Q s B l V b A j e 4 s v L p H 1 W 9 y 7 r 5 / c X t c Z N E U c Z H a F j d I o 8 d I U a 6 A 4 1 U Q t R J N E z e k V v j n Z e n H f n Y 9 5 a c o q Z Q / Q H z u c P d + O Q 2 g = = &lt; / l a t e x i t &gt; s ti &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u x L / n + 0 c 2 8 V 7 F d k H 7 t 5 r Q C p t J p I = " &gt; A A A B 7 3 i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o m K e h G K X j x W s B / Q h r L Z T t q l m 0 3 c n Q g l 9 E 9 4 8 a C I V / + O N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d p a W V 1 b X 1 g s b x c 2 t 7 Z 3 d 0 t 5 + w 8 S p 5 l D n s Y x 1 K 2 A G p F B Q R 4 E S W o k G F g U S m s H w d u I 3 n 0 A b E a s H H C X g R 6 y v R C g 4 Q y u 1 r m k H B 4 C s W y q 7 F X c K u k i 8 n J R J j l q 3 9 N X p x T y N Q C G X z J i</formula><formula xml:id="formula_10">W c G t E z c g t S g Q N O 3 v / q D i C Q h F Z p w r F T P d W L t p V h q R j j N K v 1 E 0 R i T C R 7 R n q E C h 1 R 5 6 S x 6 h o 6 N M k D D S J o n N J q p v z d S H C o 1 D Q M z m Q d V i 1 4 u / u f 1 E j 2 8 8 l I m 4 k R T Q e a H h g l H O k J 5 D 2 j A J C W a T w 3 B R D K T F Z E x l p h o 0 1 b F l O A u f n m Z t E / r 7 k X 9 7 O 6 8 1 r g u 6 i j D I R z B C b h w C Q 2 4 h S a 0 g M A j P M M r v F l P 1 o v 1 b n 3 M R 0 t W s X M A f 2 B 9 / g C P x Z Q 0 &lt; / l a t e x i t &gt; P r1 [X] and [Y] and [Y], founder of [X] [Y] leaves [X] [Y] co-founded [X] with paul allen ? founder [Y] [Y] says [X] that [X]'s founder [Y]</formula><p>Phrase segmentation and frequent pattern mining are applied to mine patterns from subcorpora as prompt candidates. We use AutoPhrase <ref type="bibr" target="#b32">(Shang et al., 2018)</ref> to segment corpora to more natural and unambiguous semantic phrases, and use FP-Growth algorithm <ref type="bibr" target="#b14">(Han et al., 2000)</ref> to mine frequent appeared patterns to compose a candidate set</p><formula xml:id="formula_11">P ? r i = (p ? 1 , p ? 2 , ..., p ? m</formula><p>). The size of the set is large, as there are plenty of messy textual patterns.</p><p>Prompt selection. To select quality patterns from the candidate set, we apply two textual mining approaches: MetaPAD <ref type="bibr" target="#b18">(Jiang et al., 2017)</ref> and TruePIE <ref type="bibr" target="#b21">(Li et al., 2018)</ref>. MetaPAD applies pattern quality function introducing several criteria of contextual features to estimate the reliability of a pattern. We explain why those features can also be adapted for LM prompt estimation: (1) Frequency and concordance: Since a PLM learns more contextual relations between frequent patterns and entities during the pre-training stage, a pattern occurs more frequently in the background corpus can probe more facts from the PLM. Similarly, if a pattern composed of highly associated sub-patterns appears frequently, it should be considered as a good one as the PLM would be familiar with the contextual relations among the sub-patterns. (2) Informativeness: A pattern with low informativeness (e.g., p ? 1 in Figure <ref type="figure" target="#fig_8">3</ref>) has the weak ability of PLM knowledge probing, as the relation between the subject or object entities cannot be well interpreted by it. (3) Completeness: The completeness of a pattern affects a lot to the PLM knowledge probing especially when any of the placeholders is missing (e.g., p ? m-2 in Figure <ref type="figure" target="#fig_8">3</ref>) so that PLM cannot even give an answer. (4) Coverage: A quality pattern should be able to probe accurate facts from PLM as many as possible. Therefore, patterns like p ? 4 which only suit a few or only one case should have a low quality score. We then apply TruePIE on the prompts (patterns) selected by MetaPAD. TruePIE filters the prompts that have low cosine similarity with the positive samples (e.g., p ? 3 and p ? m-1 are filtered), which matters to the creation of prompt ensemble since we want the prompts in the ensemble to be semantically close to each other so that some poor-quality prompts would not significantly impact the prediction result by PLM. As a result, we create a more reliable prompt ensemble P r i = {p i,1 , p i,2 , ..., p i,n } based on the averaged probabilities given by the prompts:</p><formula xml:id="formula_12">P (y|x, r i ) = 1 n n j=1 P LM (y|x, p i,j ),<label>(1)</label></formula><p>where r i is the i-th relation and p i,j is the j-th prompt in P r i . Beyond prompt selection, a prompt optimization process is also employed. Pointed out by <ref type="bibr" target="#b19">Jiang et al. 2020</ref>, some prompts in the ensemble are more reliable and ought to be weighted more. Thus, we change Equation 1 to:</p><formula xml:id="formula_13">P (y|x, r i ) = n j=1 w i,j P LM (y|x, p i,j ),<label>(2)</label></formula><p>where w i,j is the weight of j-th prompt for i-th relation. In our setting, all weights {w 1,1 , .., w k,n } Query ? ! " : &lt; ?icrosoft, /business/company/founder, ? &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BM25</head><p>"however, microsoft is planning a significant marketing push into the field with a keynote speech by bill_gates, the company 's co-founder and chairman."</p><formula xml:id="formula_14">? # : [Y], founder of [X] ? $ : [Y], co-founder of [X] ? ? % : [Y], chairman of [X]</formula><p>"[CLS] however, microsoft is planning a significant marketing push into the field with a keynote speech by bill_gates, the company 's co-founder and chairman. are learned through PLM to optimize P (y|x, r i ) for r i ? R ahead of the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Support Information Retrieval</head><p>In addition to the prompt mining, we also attach some query-wise and triple-wise support text information to the prompt to help the PLMs understand the knowledge we want to probe as well as to aid in training triple classification ability. As seen in Figure <ref type="figure" target="#fig_9">4</ref>, for the i-th query q r i in relation r, we use BM25 <ref type="bibr" target="#b31">(Robertson et al., 1995)</ref> to retrieve highly ranked support texts with score greater than ? and length shorter than ? from the reliable corpus and randomly select one of them as the support information. To compose the input cloze qr i to the PLM, we concatenate the support text to each prompt in the optimized ensemble we obtained through previous steps, with the subject filled and the object masked.</p><p>[CLS] and [SEP] are the tokens for sequence classification and support information-prompt separation accordingly.</p><p>In the training stage, we search texts using triples rather than queries, and the [MASK] would be filled by the object entities. It is worth noting that support text is optional in TAGREAL, and we leave it blank if no matching data is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>To train our model, we create negative triples in addition to the given positive triples following the idea introduced by PKGC <ref type="bibr" target="#b27">(Lv et al., 2022)</ref>, to handle the triple classification task. We create negative triples by replacing the head and tail in each positive triple with the "incorrect" entity that achieves high probability by the KGE model. We also create random negative samples by randomly replacing the heads and tails The query instances qr i are then used to fine-tune the PLM by updating its parameters. Cross-entropy loss <ref type="bibr" target="#b27">(Lv et al., 2022)</ref> is applied for optimization:</p><formula xml:id="formula_15">L = - ? ?T (y ? log(c 1 ? ) + (1 -y ? ) log(c 0 ? ) M ),<label>(3)</label></formula><p>where c 0 ? , c 1 ? ? [0, 1] are the softmax classification scores of the token [CLS] for the triple ? , y ? is the ground truth label (1/0) of the triple, and</p><formula xml:id="formula_16">M = (|T + |/|T -|)</formula><p>is the ratio between the number of positive and negative triples. After the PLM is fine-tuned with positive/negative triples in training set, it should have a better performance on classifying the triples in the dataset compared to a raw PLM. This capability would enable it to perform KG completion as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference</head><p>Given a query (h, r, ?), we apply the query-wise support information that is relevant to the head entity h and relation r, as we presume that we are unaware of the tail entity (our prediction goal). Then, we make the corresponding query instances containing [MASK], with both support information and prompt ensemble, as shown in Figure <ref type="figure" target="#fig_9">4</ref>. To leverage the triple classification capability of the PLM on link prediction, we replace [MASK] in a query instance with each entity in the known entity set and rank their classification scores in descending order to create a 1-d vector as the prediction result for each query. This indicates that the lower-indexed entities in the vector are more likely to compose a positive triple with the input query. For prompt ensemble, we sum up the scores by entity index before ranking them. The detailed illustration is placed in Appendix E. Model 20% 50% 100% Hits@5 Hits@10 MRR Hits@5 Hits@10 MRR Hits@5 Hits@10 MRR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KGE-based</head><p>TransE <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref> 29 contains more general relations (e.g., "nationality of perso") whereas UMLS-PubMed focuses on biomedical domain-specific relations (e.g., "gene mapped to diseas"). We apply the pre-processed dataset<ref type="foot" target="#foot_2">3</ref> (with training/validation/testing data size 8:1:1) to align the evaluation of our method with the baselines. Due to the imbalanced distribution and noise present in FB60K-NYT10 and UMLS-PubMed, 16 and 8 relations are selected for the performance evaluation, respectively. We place more details of the datasets in Appendix A.</p><p>Compared Methods. We compare our model TAGREAL with four categories of methods. For (1) traditional KG embedding-based methods, we evaluate TransE <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>, DisMult <ref type="bibr" target="#b42">(Yang et al., 2014)</ref>, ComplEx <ref type="bibr">(Trouillon et al., 2016a)</ref>, ConvE <ref type="bibr" target="#b10">(Dettmers et al., 2018)</ref>, TuckER <ref type="bibr" target="#b2">(Bala?evi? et al., 2019)</ref> and RotatE <ref type="bibr" target="#b35">(Sun et al., 2019)</ref> where TuckER is a newly added model. For ( <ref type="formula" target="#formula_13">2</ref>) joint text and graph embedding methods, we evaluate RC-Net <ref type="bibr" target="#b40">(Xu et al., 2014</ref><ref type="bibr">), TransE+LINE (Fu et al., 2019)</ref> and JointNRE <ref type="bibr" target="#b15">(Han et al., 2018)</ref>. For (3) reinforcement learning (RL) based path-finding methods, we evaluate MINERVA <ref type="bibr" target="#b8">(Das et al., 2017)</ref> and CPL <ref type="bibr" target="#b12">(Fu et al., 2019)</ref>. For (4) PLM-based methods, we evaluate PKGC <ref type="bibr" target="#b27">(Lv et al., 2022)</ref> and our method TAGREAL. We keep the reported data of (2) and (3) by <ref type="bibr">Fu et al.2019</ref> while re-evaluating all models in (1) in different settings for more rigorous comparison (see Appendix I for details). PKGC in our setting can be viewed as TAGREAL with manual prompts and without support information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>For FB60K-NYT10, we use LUKE <ref type="bibr" target="#b41">(Yamada et al., 2020)</ref>, a PLM pre-trained on more Wikipedia data with RoBERTa <ref type="bibr">(Liu et al., 2019b)</ref>. For UMLS-PubMed, we use SapBert <ref type="bibr" target="#b23">(Liu et al., 2021</ref>) that pretrained on both UMLS and PubMed with BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. For sub-corpora mining, we use Wikipedia with 6,458,670 document examples as the general corpus and NYT10/PubMed as the reliable sources, and we mine 500 sentences at maximum (? = 500) for each tuple. For the prompt selection, we apply MetaPAD with its default setting, and apply TruePIE with the infrequent pattern penalty, and thresholds for positive patterns and negative patterns reset to {0.5, 0.7, 0.3} respectively. For support information retrieval, we use BM25 to search relevant texts with ? = 0.9 and ? = 100 in the corpora NYT10/PubMed. We follow the same fine-tuning process as PKGC. We use TuckER as the KGE model to create negative triples, and we set M = 30 as the ratio of positive/negative triples. To compare with baselines, we test our model on training sets in the ratios of [20%, 50%, 100%] for FB60K-NYT10 and [20%, 40%, 70%, 100%] for UMLS-PubMed. The evaluation metrics are described in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Comparison</head><p>We show the performance comparison with the state-of-the-art methods in Tables <ref type="table" target="#tab_2">1</ref> and<ref type="table" target="#tab_3">2</ref>. As one can observe, TAGREAL outperforms the existing Model 20% 40% 70% 100% Hits@5 Hits@10 Hits@5 Hits@10 Hits@5 Hits@10 Hits@5 Hits@10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KGE-based</head><p>TransE <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref> 19  <ref type="bibr">(43.23, 47.74) (47.10, 52.02) (48.66, 57.46) (32.95, 44.42) (44.37, 54.96) (51.98, 59.09) (59.99, 61.23) mine+supp (44.54, 49.53) (47.43, 53.87) (49.03, 58.82) (35.56, 45.33) (45.35, 55.44) (53.12, 59.65) (60.27, 61.70) optim+supp (45.59, 51.34) (48.98, 55.64) (50.85, 60.64) (35.83, 46.45) (46.26, 55.99) (53.46, 60.40) (60.68, 62.88)</ref> Table <ref type="table">3</ref>: Ablation study on prompt and support information. Data in brackets denotes Hits@5 (left) and Hits@10 (right). "man", "mine" and "optim" denote TAGREAL with manual prompts, mined prompt ensemble without optimization and optimized prompt ensemble, respectively. "supp" denotes application of support information.</p><p>works in most cases. Given dense training data, KGE-based methods (e.g., RotatE) and RL-based methods (e.g., CPL) can still achieve relatively high performance. However, when the training data is limited, these approaches suffer, whereas PLM-based methods (PKGC and TAGREAL) are not greatly impacted. Our approach performs noticeably better in such cases than the current non-PLM-based ones. This is because the KGE models cannot be trained effectively with inadequate data, and the RL-based path-finding models cannot recognize the underlying patterns given insufficient evidential and general paths in KG. On the other hand, PLMs already possess implicit information that can be used directly, and the negative effects of insufficient data in fine-tuning would be less harsh than in training from scratch. TAGREAL outperforms PKGC due to its ability to automatically mine quality prompts and retrieve support information in contrast to manual annotations which are often limited. Next, we analyze the impacts of support information and prompt generation on the performance of TAGREAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Analysis</head><p>We conduct an ablation study to verify the effectiveness of both automatically generated prompts and retrieved support information. The results are presented in Table <ref type="table">3</ref>, Figure <ref type="figure" target="#fig_12">5</ref> and<ref type="figure" target="#fig_14">6</ref>.    Support Information. As shown in Table <ref type="table">3</ref>, for FB60K-NYT10, support information helps improve Hits@5 and Hits@10 in ranges of [5.2%,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual Prompt</head><p>[Y] is located in <ref type="bibr">[X]</ref>.</p><p>Query: (?, /location/location/contains, alba) "in alba , italy 's truffle capital , in the northwestern province of piedmont , demand for the fungi has spawned a cottage industry of package tours , food festivals and a strip mall of truffle-themed shops . "</p><p>[Y], <ref type="bibr">[X]</ref>   Optim denotes optimized prompt ensemble. Supp denotes support information. The ground truth tail entity , helpful information and optimized prompts (darker for higher weights) are highlighted.</p><p>7.5%] and [3.8%, 5.3%], respectively. For UMLS-PubMed, it helps improve Hits@5 and Hits10 in ranges of [1.9%, 4.94%] and [0.9%, 3.6%], respectively. Although the overlap between UMLS and PubMed is higher than that between FB60K and NYT10 <ref type="bibr" target="#b12">(Fu et al., 2019)</ref>, the textual information in PubMed could not help as much as NYT10 since that: (1) SapBert already possesses adequate implicit knowledge on both UMLS and PubMed so that a large portion of additional support texts might be useless. The lines "u2", "u3", "u4" and "u5" in Figure <ref type="figure" target="#fig_12">5</ref> show that support information helps more when using LUKE as the PLM as it contains less domain-specific knowledge. It also infers that the support information could be generalized to any application, especially when fine-tuning a PLM is difficult in low-resource scenarios <ref type="bibr" target="#b1">(Arase and Tsujii, 2019;</ref><ref type="bibr" target="#b28">mahabadi et al., 2021)</ref>.</p><p>(2) UMLS contains more queries with multiple correct answers than FB60K (see Appendix A), which means some queries are likely "misled" to another answer and thus not counted into the Hits@N metric. Prompt Generation. Almost all of the relations, as shown in Figure <ref type="figure" target="#fig_14">6</ref>, could be converted into better prompts by our prompt mining and optimization, albeit some of them might be marginally worse than manually created prompts due to the following fact. A few of the mined prompts, which are of lower quality than the manually created prompts, may significantly negatively affect the prediction score for the ensemble with equal weighting. Weighting based on PLM reduces such negative effects of the poor prompts for the optimized ensembles and enables them to outperform most handcrafted prompts. In addition, Table <ref type="table">3</ref> shows the overall improvement for these three types of prompts, demonstrating that for both datasets, optimized ensembles outperform equally weighted ensembles, which in turn outperform manually created prompts. Moreover, by comparing line "f1" with line "f2", or line "u1" with line "u3" in Figure <ref type="figure" target="#fig_12">5</ref>, we find a performance gap between PLM with manual prompts and with the optimized ensemble for triple classification, highlighting the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Study</head><p>Figure <ref type="figure" target="#fig_15">7</ref> shows an example of using TAGREAL for link prediction with a query (?, /location/location/ contains, alba) where "piedmont" is the ground truth. By comparing the prediction results in different pairs, we find that both prompt generation and support information could enhance the KG completion performance. With the handcrafted prompt, the PLM simply lists out the terms that have some connections to the subject entity "alba" without being aware that we are trying to find the place it is located in. Differently, with the optimized prompt ensemble, the PLM lists entities that are highly relevant to our target, where "cuneo", "italy", "north-ern_italy" are correct real-world answers, indicating that our intention is well conveyed to the PLM. With the support information, the PLM increases the score of entities that are related to the keywords ("italy", "piedmont") in the text. Moreover, the optimized ensemble removes "texas" and "scotland" from the list and leaves only Italy-related locations. More examples are placed in Appendix H.</p><p>In this study, we proposed a novel framework to exploit the implicit knowledge in PLM for open KG completion. Experimental results show that our method outperforms existing methods especially when the training data is limited. We showed that the optimized prompts with our approach outperform the handcrafted ones in PLM knowledge probing. The effectiveness of the support information retrieval to aid the prompting is also demonstrated. In the future, we may leverage QA model's power to retrieve more reliable support information. Another potential extension is to make our model more explainable by exploring path-finding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Due to the nature of deep learning, our method is less explainable than path-finding-based KG completion methods (e.g., CPL), which provide a concrete reasoning path to the target entity. Composing the path with multiple queries might be an applicable strategy that is worthwhile to investigate in order to extend our work on the KG reasoning task.</p><p>For the link prediction task, we adapt the "recall and re-ranking" strategy from PKGC <ref type="bibr" target="#b27">(Lv et al., 2022)</ref>, which brings a trade-off between prediction efficiency and accuracy. We alleviate the issue by applying different hyper-parameters given different sizes of training data, which is discussed in detail in Appendix C.</p><p>As a common issue of existing KG completion models, the performance of our model also degrades when the input KG contains noisy data. The advantage of our approach in addressing this issue is that it can use both corpus-based textual information and implicit PLM knowledge to reduce noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethical Statements</head><p>In this study, we use two datasets FB60K-NYT10 and UMLS-PubMed, which include the knowledge graphs FB60K and UMLS as well as the text corpora NYT10 and PubMed. The data is all publicly available. Our task is knowledge graph completion, which is performed by finding missing facts given existing knowledge. This work is only relevant to NLP research and will not be put to improper use by ordinary people. latter on extracting facts from PLMs. In this section, we use another example (Figure <ref type="figure">8</ref>) to explain in detail how the textual pattern mining approaches like MetaPAD <ref type="bibr" target="#b18">(Jiang et al., 2017)</ref> and TruePIE <ref type="bibr" target="#b21">(Li et al., 2018)</ref> are implemented to mine quality prompts. In the example, given the relation location/neighborhood/neighborhood_of as the input, we first extract tuples (e.g., &lt;east new york, brooklyn&gt;) in the relation from the KG (i.e., FB60K). Then, we construct a sub-corpus by searching the sentences in a large corpus (e.g., Wikipedia) and the KG-related corpus (i.e. NYT10 for FB60). After the creation of subcorpus, we apply phrase segmentation and frequent pattern mining to mine raw prompt candidates. Since the candidate set is noisy as some prompts with low completeness (e.g., in lower [Y]), low informativeness (e.g., the [Y], <ref type="bibr">[X]</ref>) and low coverage (e.g., <ref type="bibr">[X]</ref>, manhattan, [Y]) are present, we use MetaPAD to handle the prompt filtering with its quality function introducing those contextual features. After the prompts have been processed by MetaPAD, we choose one of them to serve as a seed prompt (for example, <ref type="bibr">[X]</ref> neighborhood of <ref type="bibr">[Y]</ref>) so that other prompts can be compared to it by computing their cosine similarity. As the positive seed prompt is selected manually, we can tell that there is still room for future improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Re-ranking Recalls from KGE Model</head><p>Re-ranking framework.</p><p>According to the inference process we present in Figure <ref type="figure">9</ref>, we fill the placeholder ([MASK]) with each entity (e 1 , e 2 , ..., e n ) in the entity set E. However, as mentioned by <ref type="bibr">Lv et al.2022</ref>, the inference speed of PLM-based models is much slower than that of KGE models, which is a disadvantage of using PLM for KG completion. To address this issue, they use the recalls from KGE models, that is, using KGE models to run KG completion and select X top-ranked entities for each query as the entity set E. Then, they shuffle the set and re-rank those entities using the PLM-based model. In our work, we adapt this re-ranking framework to accelerate the inference and evaluation as our time complexity is Z times as large as PKGC <ref type="bibr" target="#b26">(Lv et al., 2018)</ref> for each case where Z is the size of prompt ensemble. We use the recalls from TuckER <ref type="bibr" target="#b2">(Bala?evi? et al., 2019)</ref>   Limitations. Nonetheless, implementing the re-ranking framework has a trade-off between efficiency and Hits@N performance. When the training data is large (e.g., 100%), the KGE model could be well trained so that the ground truth entity e gt is more likely to be contained in the top X ranked ones. However, when the training data is limited (e.g., 20%), the trained KGE model could not perform well on link prediction, as shown in Table <ref type="table" target="#tab_2">1</ref> and 2. In such a case, there is a probability that e gt is not among the top X entities if we keep using the same X regardless of the size of the training data. To alleviate this side effect, we test and select different values of the hyper-parameter X for different sizes of training data, as presented in Table <ref type="table" target="#tab_7">6</ref>.  To check how much space there is for improvement, we manually add the ground truth entity into the recalls (we should not do this for the evaluation of TAGREAL as we suppose the object entity is unknown) and test the performance of TAGREAL on UMLS-PubMed. The result is shown in Table <ref type="table">7</ref>. By comparing this data with Table <ref type="table">3</ref> for UMLS-PubMed, we find that changing the values of X could not perfectly address the issue. We leave the improvement as one of our major future works. Table <ref type="table">7</ref>: Link prediction of TAGREAL on UMLS-PubMed with ground truth added to the KGE recalls. Data in brackets are Hits@5 (left) and Hits@10 (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Computing Infrastructure &amp; Budget</head><p>We trained and evaluated TAGREAL on 7 NVIDIA RTX A6000 running in parallel as we support multi-GPU computing. Training TAGREAL to a good performance took about 22 and 14 hours on the entire FB60K-NYT10 dataset (with LUKE <ref type="bibr" target="#b41">(Yamada et al., 2020)</ref>) and the entire UMLS-PubMed dataset (with SapBert <ref type="bibr" target="#b23">(Liu et al., 2021)</ref>) respectively. The training time is proportional to the size (ratio) of the training data. The evaluation took about 12 minutes for FB60K-NYT10 with LUKE when hyper-parameter X = 20, and 16 minutes for UMLS-PubMed with SapBert when X = 30.</p><p>The evaluation time is proportional to X, which explains why we applied the re-ranking framework (Appendix C) to improve the prediction efficiency. For the link prediction with equally-weighted or optimized ensembles, we apply the method shown in Figure <ref type="figure">9</ref>. Specifically, for each sentence with [MASK] filled with an entity e i , we calculate its classification score with the fine-tuned PLM. For each query, we get an m ? n matrix where m is the number of prompts in the ensemble, n is the number of entities in the entity set (which is X if the re-ranking framework is applied). For an ensemble that is equally weighted, we simply sum the scores of each entity obtained from the different prompts, whereas for an optimized ensemble, we multiply the weighting of the prompts by the scores before the addition. After sorting the vector in size of 1?n in descending order, we can get the ranking of entities as the result of the link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Link Prediction with Ensemble</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Evaluation Metrics</head><p>Following previous KG completion works <ref type="bibr" target="#b12">(Fu et al., 2019;</ref><ref type="bibr" target="#b27">Lv et al., 2022)</ref>, we use Hits@N and Mean Reciprocal Rank (MRR) as our evaluation metrics. As mentioned in Section 3.5, the prediction of each query (h, r, ?) is a 1-d vector of indices of entities in descending order regarding their scores. Specifically, for a query q i , We record the rank of the object entity t as R i , then we have:</p><formula xml:id="formula_17">Hits@N = Q i=1 R i,in Q and R i,in = 0, R i &gt; N 1, R i ? N,<label>(4)</label></formula><formula xml:id="formula_18">M RR = Q i=1 1 QR i , (<label>5</label></formula><formula xml:id="formula_19">)</formula><p>where Q is the number of queries in evaluation. To exploit the power of PLM, we need to map the code (entity_id) in KG/corpus into the words (Figure <ref type="figure" target="#fig_17">10</ref> shows the performance difference of PLM between using word and using code). For FB60K-NYT10, we use the mapping provided by JointNRE <ref type="bibr" target="#b15">(Han et al., 2018)</ref> <ref type="foot" target="#foot_4">5</ref> , which covers the translation for all entities. For UMLS-PubMed, we jointly use three mappings<ref type="foot" target="#foot_5">6</ref>,<ref type="foot" target="#foot_6">7</ref>,<ref type="foot" target="#foot_7">8</ref> which cover 97.22% of all entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Code Interpretation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Case Study</head><p>In addition to Figure <ref type="figure" target="#fig_15">7</ref>, we show more examples applying TAGREAL on link prediction in Figure <ref type="figure">11</ref>. We can see that the predictions with optimized prompt ensemble outperform those with manual prompts in all the cases, and even outperforms predictions with manual prompts and support information in some cases. In all these examples, the support information aids the PLM knowledge probing in different ways. For the first example, we believe that the PLM captures the words "brother james_murray" and "his wife jenny", and realize that we are talking about the Scottish lexicographer "james_murray" but not the American comedian with the same name, based on our survey. For the second example, the PLM probably captures "glycemic control" which is highly relevant to the disease "hyperglycemia". For the third example, the term "antiemetic" (the drug against vomiting) is likely captured so that the answer "vomiting" could be correctly predicted. Hence, it is not necessary for the support information to include the object</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Detroit is the largest city in the U.S state of Michigan. People from Detroit, [MASK] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TAGREAL Framework. The input and output of each phase are highlighted by red and green, respectively. The dotted arrow indicates the optional process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>co-founded [X] corporation in 1975 with paul allen." ? "[Y] is a co-founder of [X], along with his late childhood friend paul allen." ? "[Y] was a co-founder of [X] , along with akio morita ." ? "[X] founder and waseda alumnus [Y] " Sub-corpora Mining Phrase Segmentation (e.g., with AutoPhrase) &amp; Frequent Pattern Mining (e.g., with FP-Growth) Prompt candidates for relation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>/business/company/founder ? ? /location/neighbor/neighbor_of &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X U b n 6 1 Z b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>6 8 P 6 H L c u W J O Z P f g D 6 + s X m W C e D w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " r F r 2 S T c y M R p b g c b m 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>u s 7 l 3 W z + 8 v a o 2 b I o 4 y O k L H 6 B R 5 6 A o 1 0 B 1 q o h a i S K J n 9 I r e H O 2 8 O O / O x 7 y 1 5 B Q z h + g P n M 8 f I s u Q o g = = &lt; / l a t e x i t &gt; s t1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s W+ I Z x E f J d g C 4 f 3 C P i N 0 7 0 J j 7 N E = " &gt; A A A B 8 H i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K 9 k P a E D b b T b t 0 d x N 2 J 0 I J + R V e P C j i 1 Z / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e m A h u w H W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 o m z j V l L V o L G L d D Y l h g i v W A g 6 C d R P N i A w F 6 4 T j 2 6 n f e W L a 8 F g 9 w C R h v i R D x S N O C V j p M T N 5 k E E w y o N q z a 2 7 M + B l 4 h W k h g o 0 g + p X f x D T V D I F V B B j e p 6 b g J 8 R D Z w K l l f 6 q W E J o W M y Z D 1 L F Z H M + N n s 4 B y f W G W A o 1 j b U o B n 6 u + J j E h j J j K 0 n Z L A y C x 6 U / E / r 5 d C d O 1 n X C U p M E X n i 6 J U Y I j x9 H s 8 4 J p R E B N L C N X c 3 o r p i G h C w W Z U s S F 4 i y 8 v k / Z Z 3 b u s n 9 9 f 1 B o 3 R R x l d I S O 0 S n y 0 B V q o D v U R C 1 E k U T P 6 B W 9 O d p 5 c d 6 d j 3 l r y S l m D t E f O J 8 / d l 6 Q 2 Q = = &lt; / l a t e x i t &gt; s th &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W z n T 8 6 d l 5 c W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>4 8 V T F t o Q 9 l s t + 3 S z S b s T o Q S + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J H C o O t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b/ + g f H j U N H G q G f d Z L G P d D q n h U i j u o 0 D J 2 4 n m N A o l b 4 X j u 5 n f e u L a i F g 9 4 i T h Q U S H S g w E o 2 g l P 8 N e b d o r V 9 y q O w d Z J V 5 O K p C j 0 S t / d f s x S y O u k E l q T M d z E w w y q l E w y a e l b m p 4 Q t m Y D n n H U k U j b o J s f u y U n F m l T w a x t q W Q z N X f E x m N j J l E o e 2 M K I 7 M s j c T / / M 6 K Q 5 u g k y o J E W u 2 G L R I J U E Y z L 7 n P S F 5 g z l x B L K t L C 3 E j a i m j K 0 + Z R s C N 7 y y 6 u k W a t 6 V 9 W L h 8 t K / T a P o w g n c A r n 4 M E 1 1 O E e G u A D A w H P 8 A p v j n J e n H f n Y 9 F a c P K Z Y / g D 5 / M H z m q O s Q = = &lt; / l a t e x i t &gt;t2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 Q p S K n 8 8 0 C c a i p h e c y A 5 7 m + 6 R W E = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l U 1 G P R i 8 c K x h b a U D b b T b t 0 s w m 7 E 6 G E / g Y v H h T x 6 g / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s kY l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R 7 d R v P X F t R K I e c J z y I K Y D J S L B K F r J z 7 E 3 n P S q N b f u z k C W i V e Q G h R o 9 q p f 3 X 7 C s p g r Z J I a 0 / H c F I O c a h R M 8 k m l mx m e U j a i A 9 6 x V N G Y m y C f H T s h J 1 b p k y j R t h S S m f p 7 I q e x M e M 4 t J 0 x x a F Z 9 K b i f 1 4 n w + g 6 y IV K M + S K z R d F m S S Y k O n n p C 8 0 Z y j H l l C m h b 2 V s C H V l K H N p 2 J D 8 B Z f X i a P Z 3 X v s n 5 + f 1 F r 3 B R x l O E I j u E U P L i C B tx B E 3 x g I O A Z X u H N U c 6 L 8 + 5 8 z F t L T j F z C H / g f P 4 A I I e O 5 w = = &lt; / l a t e x i t t e x i t s h a 1 _ b a s e 6 4 = " Q 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>2 5 y b o Z 0 y j 4 B L G x U 5 q I G F 8 y P r Q t l S x C I y f T e 8 d 0 2 O r 9 G g Y a 1 s K 6 V T 9 P Z G x y J h R F N j O i O H A z H s T 8 T + v n W J 4 5 W d C J S m C 4 r N F Y S o p x n T y P O 0 J D R z l y B L G t b C 3 U j 5 g m n G 0 E R V t C N 7 8 y 4 u k c V r x L i p n 9 + f l 6 k 0 e R 4 E c k i N y Q j x y S a r k j t R I n X A i y T N 5 J W / O o / P i v D s f s 9 Y l J 5 8 5 I H / g f P 4 A f 9 + P o Q = = &lt; / l a t e x i t &gt; = ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 / c 1 a 0 I C F j G i j Z D C Y R y 8 U 3 m 6 8 z g = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d D B b B V U l U 1 G X R j c s K 9 g F t C J P p t B 0 6 m Y S Z i V J i P s W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n i D l T 2 n G + r d L K 6 t r 6 R n m z s r W 9 s 7 t n V / f b K k o k o S 0 S 8 U h 2 A 6 w o Z 4 K 2 N N O c d m N J c R h w 2 g k m N 7 n f e a B S s U j c 6 2 l M v R C P B B s y g r W R f L v a D 7 E e E 8 z T Z u a n 0 n c z 3 6 4 5 d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prompt generation process. The solid lines connect the intermediate processes, and the arrows point to the intermediate/final results. Input and output are highlighted in red and green respectively. [X] and [Y] denote head and tail entities respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Support information retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>to enlarge the set of negative training/validation triples. The labeled training triples are assembled as T = T + ? (T - KGE ? T - RAN D ) where T + is the positive set, T - KGE and T - RAN D are two negative sets we created by embedding model-based and random approaches respectively. Then, we transform all training triples of each relation r into sentences with the prompt ensemble P r and the triple-wise support information retrieved by BM25 (if there is any). At the training stage, the [MASK] is replaced by the object entity in each positive/negative triple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance (F1-Score) variation of triple classification w.r.t training time. "man" or "optim" means TAGREAL with manual prompts or optimized prompt ensemble. "supp" denotes support information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Relation-wise KG completion performance (Hits@10) comparison on FB60K-NYT10. Labels on the x-axis are the abbreviations of relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of the link prediction with TAGREAL on FB60K-NYT10. Man denotes manual prompt. Optim denotes optimized prompt ensemble. Supp denotes support information. The ground truth tail entity ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance variation of triple classification w.r.t training time. "code" and "word" denote the representation of KG entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of KG completion on FB60K-NYT10 dataset. Results are averaged values of ten independent runs of head/tail entity predictions. The highest score is highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell>.13</cell><cell>32.67</cell><cell cols="2">15.80 41.54</cell><cell>45.74</cell><cell cols="2">25.82 42.53</cell><cell>46.77</cell><cell>29.86</cell></row><row><cell></cell><cell>DisMult (Yang et al., 2014)</cell><cell>3.44</cell><cell>4.31</cell><cell>2.64</cell><cell>15.98</cell><cell>18.85</cell><cell cols="2">13.14 37.94</cell><cell>41.62</cell><cell>30.56</cell></row><row><cell></cell><cell>ComplEx (Trouillon et al., 2016a)</cell><cell>4.32</cell><cell>5.48</cell><cell>3.16</cell><cell>15.00</cell><cell>17.73</cell><cell cols="2">12.21 35.42</cell><cell>38.85</cell><cell>28.59</cell></row><row><cell></cell><cell>ConvE (Dettmers et al., 2018)</cell><cell>29.49</cell><cell>33.30</cell><cell cols="2">24.31 40.10</cell><cell>44.03</cell><cell cols="2">32.97 50.18</cell><cell>54.06</cell><cell>40.39</cell></row><row><cell></cell><cell>TuckER (Bala?evi? et al., 2019)</cell><cell>29.50</cell><cell>32.48</cell><cell cols="2">24.44 41.73</cell><cell>45.58</cell><cell cols="2">33.84 51.09</cell><cell>54.80</cell><cell>40.47</cell></row><row><cell></cell><cell>RotatE (Sun et al., 2019)</cell><cell>15.91</cell><cell>18.32</cell><cell cols="2">12.65 35.48</cell><cell>39.42</cell><cell cols="2">28.92 51.73</cell><cell>55.27</cell><cell>42.64</cell></row><row><cell></cell><cell>RC-Net (Xu et al., 2014)</cell><cell>13.48</cell><cell>15.37</cell><cell cols="2">13.26 14.87</cell><cell>16.54</cell><cell cols="2">14.63 14.69</cell><cell>16.34</cell><cell>14.41</cell></row><row><cell>Text&amp;KGE-based</cell><cell>TransE+Line (Fu et al., 2019)</cell><cell>12.17</cell><cell>15.16</cell><cell>4.88</cell><cell>21.70</cell><cell>25.75</cell><cell>8.81</cell><cell>26.76</cell><cell>31.65</cell><cell>10.97</cell></row><row><cell></cell><cell>JointNRE (Han et al., 2018)</cell><cell>16.93</cell><cell>20.74</cell><cell cols="2">11.39 26.96</cell><cell>31.54</cell><cell cols="2">21.24 42.02</cell><cell>47.33</cell><cell>32.68</cell></row><row><cell>RL-based</cell><cell>MINERVA (Das et al., 2017) CPL (Fu et al., 2019)</cell><cell>11.64 15.19</cell><cell>14.16 18.00</cell><cell cols="2">8.93 10.87 26.81 25.16</cell><cell>31.54 31.70</cell><cell cols="2">22.24 43.80 23.80 43.25</cell><cell>44.70 49.50</cell><cell>34.62 33.52</cell></row><row><cell>PLM-based</cell><cell>PKGC (Lv et al., 2022) TagReal (our method)</cell><cell>35.77 45.59</cell><cell>43.82 51.34</cell><cell cols="2">28.62 41.93 35.41 48.98</cell><cell>46.70 55.64</cell><cell cols="2">31.81 41.98 38.03 50.85</cell><cell>52.56 60.64</cell><cell>32.11 38.86</cell></row><row><cell cols="2">4 Experiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Datasets and Compared Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Datasets. We use the datasets FB60K-NYT10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">and UMLS-PubMed provided by Fu et al., where</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FB60K and UMLS are knowledge graphs and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">NYT10 and PubMed are corpora. FB60K-NYT10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of KG completion on UMLS-PubMed dataset. Results are averaged values of ten independent runs of head/tail entity predictions. The highest score is highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>.70</cell><cell>30.47</cell><cell>27.72</cell><cell>41.99</cell><cell>34.62</cell><cell>49.29</cell><cell>40.83</cell><cell>53.62</cell></row><row><cell></cell><cell cols="2">DisMult (Yang et al., 2014)</cell><cell>19.02</cell><cell>28.35</cell><cell>28.28</cell><cell>40.48</cell><cell>32.66</cell><cell>47.01</cell><cell>39.53</cell><cell>53.82</cell></row><row><cell></cell><cell cols="3">ComplEx (Trouillon et al., 2016a) 11.28</cell><cell>17.17</cell><cell>24.64</cell><cell>35.15</cell><cell>25.89</cell><cell>38.19</cell><cell>34.54</cell><cell>49.30</cell></row><row><cell></cell><cell cols="2">ConvE (Dettmers et al., 2018)</cell><cell>20.45</cell><cell>30.72</cell><cell>27.90</cell><cell>42.49</cell><cell>30.67</cell><cell>45.91</cell><cell>29.85</cell><cell>45.68</cell></row><row><cell></cell><cell cols="2">TuckER (Bala?evi? et al., 2019)</cell><cell>19.94</cell><cell>30.82</cell><cell>25.79</cell><cell>41.00</cell><cell>26.48</cell><cell>42.48</cell><cell>30.22</cell><cell>45.33</cell></row><row><cell></cell><cell cols="2">RotatE (Sun et al., 2019)</cell><cell>17.95</cell><cell>27.55</cell><cell>27.35</cell><cell>40.68</cell><cell>34.81</cell><cell>48.81</cell><cell>40.15</cell><cell>53.82</cell></row><row><cell></cell><cell cols="2">RC-Net (Xu et al., 2014)</cell><cell>7.94</cell><cell>10.77</cell><cell>7.56</cell><cell>11.43</cell><cell>8.31</cell><cell>11.81</cell><cell>9.26</cell><cell>12.00</cell></row><row><cell>Text&amp;KGE-based</cell><cell cols="2">TransE+Line (Fu et al., 2019)</cell><cell>23.63</cell><cell>31.85</cell><cell>24.86</cell><cell>38.58</cell><cell>25.43</cell><cell>34.88</cell><cell>22.31</cell><cell>33.65</cell></row><row><cell></cell><cell cols="2">JointNRE (Han et al., 2018)</cell><cell>21.05</cell><cell>31.37</cell><cell>27.96</cell><cell>40.10</cell><cell>30.87</cell><cell>44.47</cell><cell>-</cell><cell>-</cell></row><row><cell>RL-based</cell><cell cols="2">MINERVA (Das et al., 2017) CPL (Fu et al., 2019)</cell><cell>11.55 15.32</cell><cell>19.87 24.22</cell><cell>24.65 26.96</cell><cell>35.71 38.03</cell><cell>35.80 37.23</cell><cell>46.26 47.60</cell><cell>57.63 58.10</cell><cell>63.83 65.16</cell></row><row><cell>PLM-based</cell><cell cols="2">PKGC (Lv et al., 2022) TagReal (our method)</cell><cell>31.08 35.83</cell><cell>43.49 46.45</cell><cell>41.34 46.26</cell><cell>52.44 55.99</cell><cell>47.39 53.46</cell><cell>55.52 60.40</cell><cell>55.05 60.68</cell><cell>59.43 62.88</cell></row><row><cell>Condition</cell><cell></cell><cell>FB60K-NYT10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">UMLS-PubMed</cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell>50%</cell><cell>100%</cell><cell>20%</cell><cell></cell><cell>40%</cell><cell>70%</cell><cell></cell><cell>100%</cell></row><row><cell>man</cell><cell cols="9">(35.77, 43.82) (41.93, 46.70) (41.98, 52.56) (31.08, 43.49) (41.34, 52.44) (47.39, 56.52) (55.05, 59.43)</cell></row><row><cell>man+supp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>for both datasets.</figDesc><table><row><cell>relations</cell><cell cols="6">#triples(all) #queries(all) ratio(all) #triples(test) #queries(test) ratio(test)</cell></row><row><cell>FB60K-NYT10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>/people/person/nationality</cell><cell>44186</cell><cell>20215</cell><cell>2.19</cell><cell>4438</cell><cell>2282</cell><cell>1.94</cell></row><row><cell>/location/location/contains</cell><cell>42306</cell><cell>11971</cell><cell>3.53</cell><cell>4244</cell><cell>2373</cell><cell>1.79</cell></row><row><cell>/people/person/place_lived</cell><cell>29160</cell><cell>12760</cell><cell>2.29</cell><cell>3094</cell><cell>2066</cell><cell>1.50</cell></row><row><cell>/people/person/place_of_birth</cell><cell>28108</cell><cell>16341</cell><cell>1.72</cell><cell>2882</cell><cell>2063</cell><cell>1.40</cell></row><row><cell>/people/deceased_person/place_of_death</cell><cell>6882</cell><cell>4349</cell><cell>1.58</cell><cell>678</cell><cell>518</cell><cell>1.31</cell></row><row><cell>/people/person/ethnicity</cell><cell>5956</cell><cell>2944</cell><cell>2.02</cell><cell>574</cell><cell>305</cell><cell>1.88</cell></row><row><cell>/people/ethnicity/people</cell><cell>5956</cell><cell>2944</cell><cell>2.02</cell><cell>592</cell><cell>318</cell><cell>1.86</cell></row><row><cell>/business/person/company</cell><cell>4334</cell><cell>2370</cell><cell>1.83</cell><cell>450</cell><cell>379</cell><cell>1.19</cell></row><row><cell>/people/person/religion</cell><cell>3580</cell><cell>1688</cell><cell>2.12</cell><cell>300</cell><cell>175</cell><cell>1.71</cell></row><row><cell>/location/neighborhood/neighborhood_of</cell><cell>1275</cell><cell>547</cell><cell>2.33</cell><cell>130</cell><cell>91</cell><cell>1.43</cell></row><row><cell>/business/company/founders</cell><cell>904</cell><cell>709</cell><cell>1.28</cell><cell>94</cell><cell>87</cell><cell>1.08</cell></row><row><cell>/people/person/children</cell><cell>821</cell><cell>711</cell><cell>1.15</cell><cell>56</cell><cell>56</cell><cell>1.00</cell></row><row><cell>/location/administrative_division/country</cell><cell>829</cell><cell>498</cell><cell>1.66</cell><cell>88</cell><cell>72</cell><cell>1.22</cell></row><row><cell>/location/country/administrative_divisions</cell><cell>829</cell><cell>498</cell><cell>1.66</cell><cell>102</cell><cell>79</cell><cell>1.29</cell></row><row><cell>/business/company/place_founded</cell><cell>754</cell><cell>548</cell><cell>1.38</cell><cell>80</cell><cell>73</cell><cell>1.10</cell></row><row><cell>/location/us_county/county_seat</cell><cell>264</cell><cell>262</cell><cell>1.01</cell><cell>32</cell><cell>32</cell><cell>1.00</cell></row><row><cell>UMLS-PubMed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>may_be_treated_by</cell><cell>71424</cell><cell>7703</cell><cell>9.27</cell><cell>7020</cell><cell>3118</cell><cell>2.25</cell></row><row><cell>may_treat</cell><cell>71424</cell><cell>7703</cell><cell>9.27</cell><cell>6956</cell><cell>3091</cell><cell>2.25</cell></row><row><cell>may_be_prevented_by</cell><cell>10052</cell><cell>3232</cell><cell>3.11</cell><cell>1014</cell><cell>584</cell><cell>1.74</cell></row><row><cell>may_prevent</cell><cell>10052</cell><cell>3232</cell><cell>3.11</cell><cell>1034</cell><cell>586</cell><cell>1.76</cell></row><row><cell>gene_mapped_to_disease</cell><cell>6164</cell><cell>1732</cell><cell>3.56</cell><cell>596</cell><cell>331</cell><cell>1.80</cell></row><row><cell>disease_mapped_to_gene</cell><cell>6164</cell><cell>1732</cell><cell>3.56</cell><cell>652</cell><cell>357</cell><cell>1.82</cell></row><row><cell>gene_associated_with_disease</cell><cell>536</cell><cell>289</cell><cell>1.85</cell><cell>58</cell><cell>49</cell><cell>1.18</cell></row><row><cell>disease_has_associated_gene</cell><cell>536</cell><cell>289</cell><cell>1.85</cell><cell>48</cell><cell>41</cell><cell>1.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Number of triples (#triples) and queries (#queries) in relations for FB60K-NYT10 and UMLS-PubMed. Triples/queries for both head prediction and tail prediction are counted. "all" and "test" denote the whole dataset and testing data respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Best X for different training sizes</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Prediction with ensemble. e 1 , e 2 , ..., e n denote the indices of entities. p 1 , p 2 , ..., p m denote the prompts in the ensemble.</figDesc><table><row><cell></cell><cell cols="5">Equally Weighted Ensemble</cell><cell></cell><cell cols="2">Optimized ensemble</cell></row><row><cell>? !</cell><cell cols="3">? ! ? " ? # 0.62 0.18 0.40</cell><cell>? ?</cell><cell>? $ 0.53</cell><cell>?0.12</cell><cell cols="2">? ! ? " ? # 0.07 0.02 0.05</cell><cell>? ?</cell><cell>? $ 0.06</cell></row><row><cell></cell><cell>+</cell><cell>+</cell><cell cols="2">+ +</cell><cell>+</cell><cell></cell><cell>+</cell><cell>+</cell><cell>+ +</cell><cell>+</cell></row><row><cell>? "</cell><cell cols="3">0.44 0.31 0.69</cell><cell>?</cell><cell>0.16</cell><cell>?0.39</cell><cell cols="2">0.17 0.12 0.27</cell><cell>?</cell><cell>0.06</cell></row><row><cell>? ? %</cell><cell cols="4">+ + 0.15 0.20 0.77 + + + ? ? + + +</cell><cell>+ + 0.28</cell><cell>?0.27</cell><cell cols="2">+ + 0.04 0.05 0.21 + + + ? ? + + +</cell><cell>+ + 0.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sum</cell><cell></cell><cell></cell><cell></cell><cell>sum</cell></row><row><cell>score</cell><cell cols="3">4.96 2.88 5.87</cell><cell>?</cell><cell>3.79</cell><cell></cell><cell cols="2">0.76 0.58 2.66</cell><cell>?</cell><cell>0.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">argsort</cell><cell></cell><cell></cell><cell>argsort</cell></row><row><cell>rank</cell><cell cols="3">? !!" ? ## ? $</cell><cell>?</cell><cell>? %&amp;</cell><cell>rank</cell><cell cols="2">? $ ? ## ? &amp;'</cell><cell>?</cell><cell>? ($</cell></row><row><cell cols="2">Figure 9:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/thunlp/OpenNRE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our code is available at: https://github.com/ pat-jj/TagReal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/INK-USC/CPL# datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/INK-USC/CPL# datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/thunlp/JointNRE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://evs.nci.nih.gov/ftp1/NCI_ Thesaurus/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://www.ncbi.nlm.nih.gov/books/ NBK9685/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://bioportal.bioontology.org/ ontologies/VANDF</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>9 Acknowledgements Research was supported in part by <rs type="funder">US DARPA KAIROS</rs> Program No. <rs type="grantNumber">FA8750-19-2-1004</rs> and <rs type="funder">INCAS</rs> Program No. <rs type="grantNumber">HR001121C0165</rs>, <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">IIS-19-56151</rs>, <rs type="grantNumber">IIS-17-41317</rs>, and <rs type="funder">IIS</rs> <rs type="grantNumber">17-04532</rs>, and the <rs type="institution">Molecule Maker Lab Institute</rs>: <rs type="programName">An AI Research Institutes program</rs> supported by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2019897</rs>, and the <rs type="institution">Institute for Geospatial Understanding</rs> through an <rs type="grantName">Integrative Discovery Environment (I-GUIDE</rs>) by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2118329</rs>, and <rs type="funder">NSF</rs> Award <rs type="grantNumber">SCH-2205289</rs>, <rs type="grantNumber">SCH-2014438</rs>, <rs type="grantNumber">IIS-2034479</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HnBRwyv">
					<idno type="grant-number">FA8750-19-2-1004</idno>
				</org>
				<org type="funding" xml:id="_F9B8rdc">
					<idno type="grant-number">HR001121C0165</idno>
				</org>
				<org type="funding" xml:id="_dZpAAUp">
					<idno type="grant-number">IIS-19-56151</idno>
				</org>
				<org type="funding" xml:id="_6VvJnGh">
					<idno type="grant-number">IIS-17-41317</idno>
				</org>
				<org type="funding" xml:id="_epMHYyH">
					<idno type="grant-number">17-04532</idno>
					<orgName type="program" subtype="full">An AI Research Institutes program</orgName>
				</org>
				<org type="funding" xml:id="_FAxWvqx">
					<idno type="grant-number">2019897</idno>
					<orgName type="grant-name">Integrative Discovery Environment (I-GUIDE</orgName>
				</org>
				<org type="funding" xml:id="_7VCMfv8">
					<idno type="grant-number">2118329</idno>
				</org>
				<org type="funding" xml:id="_5rYdSKn">
					<idno type="grant-number">SCH-2205289</idno>
				</org>
				<org type="funding" xml:id="_9hX7eCm">
					<idno type="grant-number">SCH-2014438</idno>
				</org>
				<org type="funding" xml:id="_DnP6cZ9">
					<idno type="grant-number">IIS-2034479</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Overview</head><p>We use the datasets FB60K-NYT10 and UMLS-PubMed provided by <ref type="bibr" target="#b12">(Fu et al., 2019)</ref>  4 . They take the following steps to split the data: (1) split the data of each KG (FB60K or UMLS) in the ratio of 8:1:1 for training/validation/testing data. (2) For training data, they keep all triples in any relations.</p><p>(3) For validation/testing data, they only keep the triples in 16/8 relations they concern (see relations in Table <ref type="table">5</ref>). The processed data has {train: 268280, valid: 8765, test: 8918} for FB60K and {train: 2030841, valid: 8756, test: 8689} for UMLS. As for the corpora, there are 742536 and 5645558 documents in NYT10 and PubMed respectively. Table <ref type="table">4</ref>: The number of queries and the ratio of triples/queries for FB60K-NYT10 and UMLS-PubMed Sub-training-set splitting. To split the training data in the ratio of 20%/50% for FB60K-NYT10 or 20%/40%/70% for UMLS-PubMed, we use the same random seeds <ref type="bibr">(55,</ref><ref type="bibr">83,</ref><ref type="bibr">5583)</ref> as <ref type="bibr">Fu et al. used</ref>, and report the results in average.</p><p>Query-triple ratio. Within the relations that we focus on, we calculate the ratio of the triples by the queries (including both (h, r, ?) and (?, r, t)) to indicate the number of correct answers a query may have in average. The result is given in Table <ref type="table">4</ref>. For UMLS-PubMed, as the relations are symmetric in pairs, the number of queries for head and tail predictions are the same. Table <ref type="table">5</ref> presents the counting in a more detailed setting. Both tables show that there are more multi-answer queries in UMLS-PubMed than in FB60K-NYT10, which explains why the support information may not be as helpful in the former as it is in the latter, as revealed by Table <ref type="table">3</ref> and discussed in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Textual Pattern Mining</head><p>The purpose of pattern mining is to find rules that describe particular patterns in the data. Information extraction is a common goal for pattern mining and prompt mining, where the former focuses on extracting facts from massive text corpora and the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reliable patterns (for relation ) output by TruePIE</head><p>&lt;east new york, brooklyn&gt;, &lt;koreatown, manhattan&gt;, &lt;samos, tucson&gt;, &lt;prospect park, minneapolis&gt;, &lt;grand, riverside&gt;, &lt;love field, dallas&gt;, &lt;bayway, elizabeth&gt;, &lt;germantown, philadelphia&gt;, &lt;alphabet city, manhattan&gt;, &lt;upper west side, manhattan&gt;, &lt;cascade, seattle&gt;, &lt;fishtown, philadelphia&gt;, &lt;broad channel, queens&gt;, &lt;herald square, manhattan&gt;, &lt;canal street, buffalo&gt;, &lt;jackson heights, edmonton&gt;, &lt;hegewisch, chicago&gt;, &lt;pearl district, portland&gt;, &lt;tottenville, staten&gt;, &lt;brooklyn heights, brooklyn&gt;, &lt;south harrison, tucson&gt;, &lt;coal harbour, vancouver&gt;, &lt;britannia, ottawa&gt;, &lt;south norwalk, fairfield&gt; ? ? ? ? Head-tail tuples in relation "poiret was born on 20 april 1879 to a cloth merchant in the poor neighborhood of <ref type="bibr">[X]</ref>, <ref type="bibr">[Y]</ref>." " <ref type="bibr">[X]</ref> is one of a few neighborhoods in [Y] that is completely privately owned." "one of [Y]'s most exclusive neighborhoods, <ref type="bibr">[X]</ref> is home to two of the three prestigious 'hill schools'" "the family then moved to the <ref type="bibr">[X]</ref> neighborhood of <ref type="bibr">[Y]</ref>." "she eventually settled in <ref type="bibr">[X]</ref>, <ref type="bibr">[Y]</ref>, where she lived until 1982." "it is also represented within the city of [Y] by the <ref type="bibr">[X]</ref> neighborhood council" ? ? ? ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search in large corpora</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-corpus for relation</head><p>Relation : location/neighborhood/neighborhood_of (in FB60K-NYT10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extract tuples in relation of in FB60K</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p / Z e G g   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual Prompt</head><p>[Y] may be treated by <ref type="bibr">[X]</ref>. Optim :</p><p>"this reportedly allows for less pharmacodynamic variability and within-subject variability than currently available insulin analogs , and a duration of action that is over 24 hours . the lack of proof of carcinogenicity with insulin_degludec is yet another factor that would be taken into consideration when choosing the optimal basal insulin for a diabetic individual . a formulation of insulin insulin_degludec with insulin aspart , insulin_degludec 70% / aspart 30% , may permit improved flexibly of dosing without compromising glycemic control or safety ." entity itself, and including only some text relevant to it could also be helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Re-evaluation of Knowledge Graph Embedding Models</head><p>We find that the performance of some KGE models was underestimated by <ref type="bibr">Fu et al.2019</ref> due to the low embedding dimension set for entity and relation. According to our re-evaluation (Table <ref type="table">8</ref>), many of these models could perform much better with higher dimension, and we report their best performance in Table <ref type="table">1</ref> and 2 based on our experiments. For the previously evaluated models, we use the same code 9,10,11 as Fu et al. used to ensure the fairness of the comparison. For TuckER <ref type="bibr" target="#b2">(Bala?evi? et al., 2019)</ref>, we use the code provided by the author 12 . Same as Fu et al., to make the comparison more rigorous, we do not apply the filtered setting <ref type="bibr" target="#b4">(Bordes et al., 2013;</ref><ref type="bibr" target="#b35">Sun et al., 2019)</ref> of the Hits@N evaluation to all the models including TAGREAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Code for Paper</head><p>The source code of TAGREAL is attached as supplemental material to the submission of this paper. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Badr</forename><surname>Alkhamissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Millicent</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06031</idno>
		<title level="m">A review on language models as knowledge bases</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer finetuning: A BERT case study</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Arase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09590</idno>
		<title level="m">Tensor factorization for knowledge graph completion</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Translating embeddings for modeling multirelational data. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks</title>
		<author>
			<persName><surname>Ting-Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05457</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on knowledge graph embedding: Approaches, applications and benchmarks</title>
		<author>
			<persName><forename type="first">Yuanfei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">750</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Neal Naixue Xiong, and Wenzhong Guo</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collaborative policy learning for open knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woojeong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2672" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM sigmod record</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural knowledge acquisition via mutual attention between knowledge graph and text</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.14268</idno>
	</analytic>
	<monogr>
		<title level="m">Bertnet: Harvesting knowledge graphs from pretrained language models</title>
		<imprint>
			<date type="published" when="2018">2018. 2022</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An endto-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Kirrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Emilio Labra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Gayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Neumaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anisa</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Rula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Schmelzeisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Sequeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447772</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge graphs. ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Metapad: Meta pattern discovery from massive text corpora</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lance M Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="877" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task learning for knowledge graph completion with pre-trained language models</title>
		<author>
			<persName><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1737" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Truepie: Discovering reliable patterns in pattern-based information extraction</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hanratty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1675" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-alignment pretraining for biomedical entity representations</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4228" to="4238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04588</idno>
		<title level="m">Differentiating concepts and instances for knowledge graph embedding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Do pretrained models benefit knowledge graph completion? a reliable evaluation and a reasonable approach</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3570" to="3581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational information bottleneck for effective low-resource fine-tuning</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Okapi at trec-3</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nist Special Publication Sp</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open-world knowledge graph completion</title>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">In International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="2071" to="2080" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards empty answers in sparql: approximating querying with rdf embedding</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="513" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rcnet: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on conference on information and knowledge management</title>
		<meeting>the 23rd ACM international conference on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Luke: Deep contextualized entity representations with entity-aware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<title level="m">Kgbert: Bert for knowledge graph completion</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Empower entity set expansion via language model probing</title>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13897</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interactive recommender system via knowledge graph-enhanced reinforcement learning</title>
		<author>
			<persName><forename type="first">Sijing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Ruiming Tang, Xiuqiang He, and Yong Yu</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty294</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">? ? ? ? Patterns mined with FP-Growth Patterns selected by MetaPAD&apos;s contexual segmentation</title>
		<imprint/>
	</monogr>
	<note>school in [Y], [X] , manhattan, [Y] , the [Y] , [X] , to [X] , [Y] , the. ballpark in [Y] , of [X] , [Y] , [X] in [Y] and , district in [Y], avenue in the [X] neighborhood of [Y] , born in [X] , [Y] , [X] is close to [Y] , located in the [X] neighborhood of [Y] , in the [X] district of [Y] , [Y] &apos;s [X] neighborhood , the [X] area of. in lower [Y], [Y] division, [X] , born in the [X] neighborhood of [Y] , club in [X] , [Y]. district of [Y], [X] neighbors [Y] , [X] , in [Y] , [X] section of [Y] , home in [X] , [Y] , avenue in the [X] neighborhood of [Y] , neighborhood of [X] , [Y] , [X] is adjacent to [Y] , [X] neighborhood in [Y] , [X], close to [Y] , [X] is in [Y] , [X] neighboring city [Y] , in the. street in [X] , [Y] , ? ? FB60K-NYT10 Fu et al.&apos;s setting Our setting (edim, rdim, filter) Ratio: (Hits@5, Hits@10, MRR) (edim, rdim, filter) Ratio: (Hits@5, Hits@10, MRR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Transe</surname></persName>
		</author>
		<author>
			<persName><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">2013. 15.12, 18.83, 12.57</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>20%: (29.13, 32.67, 15.80) 50%: (19.38, 23.20, 13.36) 50%: (41.54, 45.74. 25.82) 100%: (38.53, 43.38, 29.90) 100%: (42.53, 46.77, 29.86</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Dismult</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-01-42">2014. 1.42, 2.55, 1.05</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>3.44, 4.31, 2.64) 50%: (15.23, 19.05, 12.36) 50%: (15.98, 18.85, 13.14) 100%: (32.11, 35.88, 24.95) 100%: (37.94, 41.62, 30.56</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Complex (trouillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1944">2016a. 4.22, 5.97, 3.44</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>4.32, 5.48, 3.16) 50%: (19.10, 23.08, 12.99) 50%: (15.00, 17.73, 12.21) 100%: (32.91, 34.62, 24.67) 100%: (35.42, 38.85, 28.59</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Conve</surname></persName>
		</author>
		<author>
			<persName><surname>Dettmers</surname></persName>
		</author>
		<idno>20%: (20.60, 26.90, 11.96</idno>
		<imprint>
			<date type="published" when="0200">2018. 200, 200</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>24.39, 30.59, 18.51) 50%: (26.52, 29.84, 22.67) 100%: (33.02, 39.78, 24.45) 100%: (31.71, 35.66, 25.58) (600, 600, n/a) 20%: (29.49, 33.30, 24.31) 50%: (40.10, 44.03, 32.97) 100%: (50.18, 54.06, 40.39</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><surname>Bala?evi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>20%: (20.04, 23.02, 16.27) 50%: (24.04, 27.88, 20.21) 100%: (34.54, 38.77, 28.19) (600, 600, n/a) 20%: (29.50, 32.48, 24.44) 50%: (41.73, 45.58, 33.84) 100%: (51.09, 54.80, 40.47</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Rotate</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">600</biblScope>
		</imprint>
	</monogr>
	<note>200, 100, ?) 20%: (9.25, 11.83, 8.04) (100, 50, n/a) 20%: (1.34, 2.13, 1.08) 50%: (25.96, 31.63, 23.34) 50%: (2.54, 4.03, 1.91) 100%: (58.32, 60.66, 51.85) 100%: (5.42, 7.87, 2.09) (200, 100, n/a) 20%: (7.47, 9.14, 5.81) 50%: (21.68, 25.45, 17.35) 100%: (47.96, 52.02, 39.17. 15.91, 18.32, 12.65) 50%: (35.48, 39.42, 28.92) 100%: (51.73, 55.27, 42.64</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">&apos;s setting Our setting (edim, rdim, filter) Ratio</title>
		<author>
			<persName><forename type="first">Umls-Pubmed</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hits@5, Hits@10) (edim, rdim, filter)</title>
		<imprint/>
	</monogr>
	<note>Ratio: (Hits@5, Hits@10</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Transe</surname></persName>
		</author>
		<author>
			<persName><surname>Bordes</surname></persName>
		</author>
		<idno>%: (19.70, 30.47) 40%: (26.86, 38.08) 40%: (27.72, 41.99) 70%: (31.32, 43.58) 70%: (34.62, 49.29) 100%: (32.28, 45.52) 100%: (40.83, 53.62</idno>
		<imprint>
			<date type="published" when="2013-07-12">2013. 7.12, 11.17</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Dismult</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno>%: (19.02, 28.35) 40%: (26.90, 38.35) 40%: (28.28, 40.48) 70%: (31.65, 44.98) 70%: (32.66, 47.01) 100%: (32.80, 47.50) 100%: (39.53, 53.82</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><surname>Complex (trouillon</surname></persName>
		</author>
		<idno>%: (11.28, 17.17) 40%: (23.77, 34.15) 40%: (24.64, 35.15) 70%: (30.04, 43.60) 70%: (25.89</idno>
		<imprint>
			<date type="published" when="1958">2016a. 18.18, 19.58</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>31.84, 46.57) 100%: (34.54, 49.30</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Conve</surname></persName>
		</author>
		<author>
			<persName><surname>Dettmers</surname></persName>
		</author>
		<idno>42.04) 40%: (27.90, 42.49) 70%: (31.01, 45.81) 70%: (30.67, 45.91) 100%: (30.35</idno>
		<imprint>
			<date type="published" when="0200">2018. 200, 200</date>
			<biblScope unit="volume">45</biblScope>
		</imprint>
	</monogr>
	<note>200, 200, n/a) 20%: (20.45, 30.72) 40%: (28.01. %: (29.85, 45.68) (600, 600, n/a) 20%: (20.26, 30.29) 40%: (26.85, 41.57) 70%: (26.97, 42.44) 100%: (25.43, 41.58</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><surname>Bala?evi?</surname></persName>
		</author>
		<idno>20%: (19.94, 30.82) 40%: (25.79, 41.00) 70%: (26.48</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
	<note>5.13, 8.06) 40%: (20.48, 31.20) 70%: (29.. 30.22, 45.33) (600, 600, n/a) 20%: (18.84, 27.94) 40%: (24.57, 37.79) 70%: (25.50, 41.32) 100%: (24.41, 40.56</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Rotate</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno>%: (17.95, 27.55) 40%: (8.65, 13.21) 40%: (27.35, 40.68) 70%: (14.90, 21.67) 70%: (34.81, 48.81) 100%: (20.75, 27.82) 100%: (40.15, 53.82</idno>
		<imprint>
			<date type="published" when="1950-06-03">2019. 200. 4.03, 6.50</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Table 8: Performance of knowledge graph embedding models on FB60K-NYT10 and UMLS-PubMed. &quot;edim&quot; and &quot;rdim&quot; denotes the embedding size of entity and relation respectively</title>
	</analytic>
	<monogr>
		<title level="m">Best setting for each model is highlighted in bold</title>
		<imprint/>
	</monogr>
	<note>filter&quot; denotes the application of the filtered setting</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
