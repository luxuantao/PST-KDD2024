<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching for BurgerFormer with Micro-Meso-Macro Space Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Longxing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shun</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jilin</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research Center for Intelligent Computing Systems</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sci-ences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Computer Archi-tecture</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci-ence and Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Searching for BurgerFormer with Micro-Meso-Macro Space Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the success of Transformers in the computer vision field, the automated design of vision Transformers has attracted significant attention. Recently, MetaFormer found that simple average pooling can achieve impressive performance, which naturally raises the question of how to design a search space to search diverse and high-performance Transformer-like architectures. By revisiting typical search spaces, we design micro-meso-macro space to search for Transformer-like architectures, namely Burger-Former. Micro, meso, and macro correspond to the granularity levels of operation, block and stage, respectively. At the microscopic level, we enrich the atomic operations to include various normalizations, activation functions, and basic operations (e.g., multi-head self attention, average pooling). At the mesoscopic level, a hamburger structure is searched out as the basic Burg-erFormer block. At the macroscopic level, we search for the depth, width, and expansion ratio of the network based on the multi-stage architecture. Meanwhile, we propose a hybrid sampling method for effectively training the supernet. Experimental results demonstrate that the searched BurgerFormer architectures achieve comparable even superior performance compared with current state-of-the-art Transformers on the ImageNet and COCO datasets. The codes can be available at https://github.com/xingxing-123/BurgerFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the emergence of landmark works such as Vision Transformer (ViT) <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref>, DeiT <ref type="bibr" target="#b44">(Touvron et al., 2021b)</ref> and Swin <ref type="bibr" target="#b29">(Liu et al., 2021)</ref>, Transformers have made encouraging performance advances in the Computer Vision (CV) field such as recognition, detection, and segmentation. As a result, combining Transformer and Neural Architecture Search (NAS) <ref type="bibr" target="#b6">(Chen et al., 2021b;</ref><ref type="bibr">a)</ref> has also gained increasing attention. Recently, MetaFormer <ref type="bibr" target="#b54">(Yu et al., 2022)</ref> leverages simple average pooling to achieve impressive performance, which naturally raises the question of how to design a search space to search high-performance Transformer-like architectures.</p><p>Recalling the development of NAS, the design of search space plays a key role. In the early stage, the NAS-RL <ref type="bibr">(Zoph &amp; Le, 2017)</ref> search space is proposed to find out optimal hyper-parameters of the entire network. Afterwards, NASNet <ref type="bibr">(Zoph et al., 2018)</ref> proposes a search space containing 13 operations for searching cells and then stacks cells into a network. The search space is further compacted to 8 operations in DARTS <ref type="bibr" target="#b28">(Liu et al., 2019)</ref>. ProxylessNAS <ref type="bibr" target="#b3">(Cai et al., 2019)</ref> and FBNet <ref type="bibr" target="#b47">(Wu et al., 2019)</ref> search spaces borrow from the block design of MobileNet <ref type="bibr" target="#b38">(Sandler et al., 2018)</ref>, which means the search spaces contain various MB-Conv blocks. Similarly, in Transformer NAS, the search space of AutoFormer <ref type="bibr" target="#b6">(Chen et al., 2021b)</ref> is borrowed from the block design of Transformer.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we classify the typical search space design according to the granularity of operations, blocks, and stages from the micro, meso, and macro perspectives. In the NASNet and DARTS search spaces, diverse and rich atomic operations belong to micro design. The ProxylessNAS, FBNet, AutoFormer, and ViT-ResNAS <ref type="bibr" target="#b26">(Liao et al., 2021)</ref> search spaces contain MobileNet or Transformer blocks which belong to meso design. In the RegNet <ref type="bibr" target="#b21">(Ilija et al., 2020)</ref>, FBNet, AutoFormer and ViT-ResNAS search spaces, the search for hyper-parameters such as depth and width of the stage belongs to macro design.</p><p>In this paper, we aim to design a search space at the micromeso-macro level to search for more efficient Transformerlike architectures. At the microscopic level, we use richer atomic operations, including various normalizations, activation functions, and basic operations (e.g., multi-head self  attention, average pooling). At the mesoscopic level, we design a combined Norm-Op-Norm-Act operation structure and propose the hamburger structure to search for diverse Transformer-like blocks. At the macroscopic level, we search for the depth, width, and expansion ratio based on the multi-stage architecture.</p><p>Our major contributions are as follows:</p><p>1. We propose a micro-meso-macro search space for the first time and propose a Norm-Op-Norm-Act and Hamburger structure to offer more diverse operations and block design. Based on the proposed search space, highperformance Transformer-like architecture BurgerFormer can be searched.</p><p>2. We utilized One-Shot NAS for searching and propose a hybrid sampling method to effectively train the supernet.</p><p>3. Experiments on the ImageNet and COCO datasets show that the searched BurgerFormers match even outperform state-of-the-art Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Architecture Search. NAS is designed to automatically search for efficient neural architectures to reduce the cost of manual trial and error. Early NAS utilized reinforcement learning (Zoph &amp; Le, 2017) and evolutionary algorithms <ref type="bibr" target="#b37">(Real et al., 2019)</ref> to search for convolutional neural network and recurrent neural network. These methods had an immense resource overhead because hundreds of individual networks needed to be trained from scratch, resulting in thousands of GPU days even on small-scale datasets. Subsequently, ENAS <ref type="bibr" target="#b34">(Pham et al., 2018)</ref> proposed the weight sharing strategy, which reduced the search time to a few GPU days as only one supernet needs to be trained. Based on the strategy, differentiable NAS methods <ref type="bibr" target="#b28">(Liu et al., 2019;</ref><ref type="bibr" target="#b9">Chu et al., 2020;</ref><ref type="bibr" target="#b51">Xu et al., 2020;</ref><ref type="bibr" target="#b7">Chen et al., 2019)</ref> make the search space continuous and adopt architecture parameters to select operations. Single-Path methods <ref type="bibr" target="#b15">(Guo et al., 2020;</ref><ref type="bibr" target="#b10">Chu et al., 2021;</ref><ref type="bibr" target="#b53">Yu et al., 2020)</ref> further reduce the resource consumption by sampling and training single-path sub-networks from the supernet, and then searching sub-networks based on validation set accuracy.</p><p>Vision Transformer. Dosovitskiy et al <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref> introduced Transformer to the computer vision field, which achieved competitive performance compared to Convolutional Neural Networks (CNNs). Subsequently, DeiT <ref type="bibr" target="#b44">(Touvron et al., 2021b)</ref> proposed data-efficient ViT and a teacher-student distillation strategy. Swin-Transformer <ref type="bibr" target="#b29">(Liu et al., 2021)</ref> and PVT <ref type="bibr" target="#b46">(Wang et al., 2021)</ref> used multi-stage strategy to successfully apply Transformer to different vision tasks such as detection and segmentation. TNT <ref type="bibr" target="#b17">(Han et al., 2021)</ref> proposed that attention to finer-grained patches can effectively improve performance. <ref type="bibr">ConViT (d'Ascoli et al., 2021)</ref> introduced a soft inductive bias of CNN into ViT to bridge the gap between CNN and Transformer. CeiT <ref type="bibr" target="#b55">(Yuan et al., 2021)</ref> combined the benefits of CNN in extracting low-level features and locality with the advantage of Transformer in extracting long-range dependencies. MLP-Mixer <ref type="bibr" target="#b42">(Tolstikhin et al., 2021)</ref> and ResMLP <ref type="bibr" target="#b43">(Touvron et al., 2021a)</ref> used MLP instead of the self-attention module, still achieving comparable results. Moreover, MetaFormer <ref type="bibr" target="#b54">(Yu et al., 2022)</ref> discovered that using the average pooling operation instead of the self-attention module also achieved promising performance.</p><p>Vision Transformer NAS. AutoFormer <ref type="bibr" target="#b6">(Chen et al., 2021b)</ref> is the first algorithm to automatically search ViT for embedding dimension, depth, MLP ratio, and the number of heads. ViTAS <ref type="bibr" target="#b40">(Su et al., 2021)</ref> searched ViT models based on the weight-sharing mechanism. GLiT <ref type="bibr" target="#b5">(Chen et al., 2021a)</ref> introduced locality modules into the search space, proposed a hierarchical search strategy, and searched ViT from both global and local levels. ViT-ResNAS <ref type="bibr" target="#b26">(Liao et al., 2021)</ref> proposed residual spatial reduction and multi-architectural sampling techniques for searching a multi-stage ViT architecture. <ref type="bibr" target="#b31">(Minghao et al., 2021)</ref>   Error to search the architecture and search space. NASViT <ref type="bibr" target="#b8">(Chengyue et al., 2022)</ref> designed a hybrid space based on MBConv and Transformer blocks and developed ViTs from 200M to 800M FLOPs. These works focus on the design of the block or stage granularity, lacking the diversity of block structures and a comprehensive consideration of all three granularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Search Space Design</head><p>In this section, we will first introduce the micro-meso-macro search space design of BurgerFormer in detail. Then we will briefly discuss the size of the proposed search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Micro Search Space Design</head><p>The micro search space corresponds to the operation granularity. Rich candidate operations facilitate the search for different mesoscopic structures. For example, eight operations can build about 10 9 normal or reduction cells in the DARTS search space. To improve the richness of operations, our micro search space contains varied normalizations, activation functions, and basic operations.</p><p>Normalization and Activation Function. Normalization and activation are two types of atomic operations. Normalization can accelerate training and improve the generalization, while activation functions can enhance the nonlinear fitting ability of the network. Proper selection of the two types of atomic operations can improve performance. Previous NAS methods directly utilize fixed combinations of normalization and activation, e.g., Conv-BN-ReLU. However, existing Transformers and CNNs already employ different normalization and activation functions, which inspires us to search varied normalization and activation functions. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> , our micro search space includes layer normalization (LN) <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>, batch normalization (BN) <ref type="bibr" target="#b23">(Ioffe &amp; Szegedy, 2015)</ref>, group normalization (GN) <ref type="bibr" target="#b48">(Wu &amp; He, 2018)</ref>, activation functions of ReLU6 <ref type="bibr" target="#b1">(Agarap, 2018)</ref>, GELU <ref type="bibr" target="#b19">(Hendrycks &amp;</ref><ref type="bibr" target="#b19">Gimpel, 2016), and</ref><ref type="bibr">SiLU (Ramachandran et al., 2018)</ref>.</p><p>Basic Operation. Basic operations refer to micro operations other than normalizations and activation functions. Following Transformer-like architectures, the basic operations of BurgerFormer contain Multi-Head Self Attention (MHSA) <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref>, Spatial MLP <ref type="bibr" target="#b43">(Touvron et al., 2021a)</ref>, and Pooling <ref type="bibr" target="#b54">(Yu et al., 2022)</ref>, where the pool- ing size is 3. In addition, we extend the 3x3 depthwise convolution (Dwise 3x3) and 1x1 convolution (Conv 1x1) to introduce more locality to the network. Specifically, MHSA, Spatial MLP, and Pooling are formulated as follows:</p><formula xml:id="formula_0">M HSA(Y ) = Concat(head 1 , . . . , head h )W o , head i = Sof tmax( (Y W Q i )(Y W K i ) T √ d i )Y W V i ,<label>(1)</label></formula><formula xml:id="formula_1">Spatial M LP (Y ) = (Y T W ) T + b,<label>(2)</label></formula><formula xml:id="formula_2">P ooling(X i,j,k ) = 1 9 3 p,q=1 X i,j+p−2,k+q−2 −X i,j,k ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">W Q , W K , W V , W o ∈ R c×c</formula><p>, and W ∈ R wh×wh are the weights, X ∈ R c×w×h are 2D feature maps, and Y ∈ R n×d are 1D tokens, respectively. c, w, h, n, d represent channel, width, height, number of tokens, and embedding dimension, respectively. Following DeiT <ref type="bibr" target="#b44">(Touvron et al., 2021b)</ref> source code, the token dimension in each head is R (c/head) , so W o ∈ R head * (c/head)×c = R c×c . Please note that the Pooling operator is from MetaFormer, where the subtraction is only used as a preprocessing method for the subsequent residual connection. In practice, pooling and convolutions require 2D feature maps, while MHSA and Spatial MLP require 1D tokens, so two data formats need to be converted. For the conversion of a 2D feature map to a 1D token, we first flatten the spatial dimension of</p><formula xml:id="formula_4">X ∈ R c×w×h to obtain X ∈ R c×(wh) , then transpose X to get Y ∈ R (wh)×c = R n×d .</formula><p>Conversion of a 1D token to a 2D feature map is an inverse process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meso Search Space Design</head><p>Good blocks in the meso search space are halfway to successfully search for high-performance macro architecture.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref> Finally, we symmetrically add Norm-Op-Norm-Act before and after the FFN to search for more diverse meso blocks. Since the block structure resembles a hamburger, we call it hamburger block. Meanwhile, we vividly call the three Norm-Op-Norm-Act structures from input to output as the first bread, the meat, and the second bread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Macro Search Space Design</head><p>The macro search space corresponds to the design of the stage granularity. Referring from PVT <ref type="bibr" target="#b46">(Wang et al., 2021)</ref> and Swin <ref type="bibr" target="#b29">(Liu et al., 2021)</ref>, we design the multi-stage search space for searching optimal depth, width, and expansion ratio, which mean the number of hamburger blocks, the channels of 2D feature map or the embedding dimension of 1D tokens, and the expansion ratio between the two 1x1 convolutions, respectively. The search ranges of these hyperparameters are shown in Table <ref type="table" target="#tab_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Search Space Size</head><p>In order to reduce resource consumption, we have adjusted the search space as follows. First, we did not search MHSA and Spatial MLP in the first and the second stages, because the complexity of these two operations is quadratic to hw, which is not suitable for use in large resolution. Second, the basic operations in the meat part of the burger contain only skip and 3x3 depthwise convolution. Third, we search for hamburger blocks for each stage. There are about 4.5×10 28 architectures in the search space. The range of parameters is from 0.5M to 41.6M , and the range of FLOPs is from 0.2G to 7.4G. . Hybrid sampling method. Meso sampling is sampling one hamburger block for each stage. Macro sampling is sampling the minimum, maximum, and random sub-networks in the macro search space, whose blocks are determined by the meso sampling. The supernet weights are updated with the accumulated gradients from the sampled three sub-networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">One-Shot NAS</head><p>One-Shot NAS is a search strategy based on a weight sharing mechanism. The strategy only needs to train one supernet N and then evaluates the accuracy of the sub-networks A that inherit supernet weights, which greatly reduces the search cost compared with training thousands of stand-alone networks. For keeping consistency and reducing memory usage, the supernet is usually optimized by uniformly sampling the sub-networks:</p><formula xml:id="formula_5">W * A = arg min A L train (N (A, W )), α * = arg max a∈A Acc val (N (α, W * )), s.t. Resource(N (α, W α * )) ≤ C,<label>(4)</label></formula><p>where L train is the training loss, Acc val is the validation accuracy, and C is the resource constriant (e.g., FLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Supernet Training with Hybrid Sampling</head><p>In One-Shot NAS, it is common practice to sample a subnetwork (SPOS) <ref type="bibr" target="#b15">(Guo et al., 2020)</ref> in each iteration and train the supernet. However, SPOS suffers from low-accuracy sub-networks in our search space. Sandwich sampling <ref type="bibr" target="#b53">(Yu et al., 2020)</ref> can improve the accuracy of sub-networks, but we observe that the maximum model in the search space has poor performance and slow convergence speed. The convergence ability of the maximum model is usually worse than the minimum model, so it is not suitable to use the sandwich sampling directly. More details can be referred to Appendix A. In order to better train the supernet, we propose a hybrid sampling method that combines SPOS and sandwich sampling.</p><p>As shown in Fig. <ref type="figure">4</ref>, hybrid sampling consists of two steps, i.e., meso sampling and macro sampling. In the meso sampling step, for each Norm-Op-Norm-Act structure, we first randomly select normalization, activation, and basic operation from the candidates, then randomly select pre-Op normalization and post-Op normalization. Note that normalization and activation will be set to skip when the basic operation is skip, and the residual connections next to the Norm-Op-Norm-Act structure will be removed in first and second bread. After three independent samplings, a specific hamburger block is determined per stage. In the macro sampling step, we randomly select one hyper-parameter from the candidate depth, width, and expansion ratio for each stage. In order to better train the supernet, we additionally sample the maximum macro architecture and the minimum macro architecture of each stage after confirming the hamburger block. The three sampled paths are trained separately, and then the gradients are accumulated to update the supernet weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evolutionary Search</head><p>After training supernet, we utilize an evolutionary algorithm to search sub-networks by evaluating performance with the inherited weights from the supernet. During evolutionary iteration, we maintain the population of sub-networks. In the first iteration, P 0 sub-networks are randomly selected.</p><p>In subsequent iterations, parent sub-networks are randomly selected from the sub-network population to generate child sub-networks by crossover and mutation. Crossover is the exchange of architecture parameters with a certain probability P c , and mutation randomly changes the architecture parameters with a certain probability P m . During search, sub-networks that satisfy the resource constraints will be selected. The whole process will be repeated T iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first show the searched architectures and compare them with state-of-the-art transformers. In addition, we transfer the searched Burgerformer to the COCO dataset to verify its transferability. We then conduct an ablation study of search methods and hamburger blocks. Finally, the searched architectures and visualization are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on ImageNet</head><p>Searching Settings. ImageNet <ref type="bibr" target="#b32">(Olga et al., 2015)</ref> contains 1.28M training images and 50, 000 validation images. We split 25, 000 images from the training set as the validation set for searching. In the supernet training phase, we trained the supernet using AdamW <ref type="bibr" target="#b22">(Ilya &amp; Frank, 2019)</ref> optimizer with learning rate 1e − 3 and weight decay 0.05. we turned off the normalization statistics because of varying sampled architectures. The data augmentation and other techniques are essentially the same as for retraining, except that stochastic depth is not used. The epochs are 120 and the warmup epochs are 10. Experiments are performed on eight V100s with a batch size of 32 per GPU. In the searching phase, our settings follow <ref type="bibr" target="#b26">(Liao et al., 2021)</ref>. The initial population size P 0 is 500 and the number of iterations T is 20, The number of parents and child networks are 75 and 150, respectively. The crossover and mutation probability is 0.3. We select the Top-5 architectures and validate them on ImageNet-100 <ref type="bibr" target="#b52">(Yonglong et al., 2020)</ref>, and then take the optimal architecture and retrain it on ImageNet.</p><p>Retraining Settings. Our implementations follow DeiT <ref type="bibr" target="#b44">(Touvron et al., 2021b)</ref> and MetaFormer <ref type="bibr" target="#b54">(Yu et al., 2022)</ref>. Models are optimized using AdamW with learning rate 1e − 3 and weight decay 0.05 and batch size 1, 024. Data augmentations include MixUp <ref type="bibr" target="#b20">(Hongyi et al., 2018)</ref>, Cut-Mix <ref type="bibr" target="#b39">(Sangdoo et al., 2019</ref><ref type="bibr">), CutOut (Zhun et al., 2020)</ref> and RandAugment <ref type="bibr" target="#b13">(Ekin Dogus et al., 2020)</ref>. We alse use stochasic depth <ref type="bibr" target="#b14">(Gao et al., 2016)</ref> and layerscale <ref type="bibr" target="#b20">(Hugo et al., 2021)</ref>. Label Smoothing <ref type="bibr" target="#b41">(Szegedy et al., 2016)</ref> is set to 0.1. The training epochs are 300 and the warmup epochs are 10. Retraining is also conducted on eight V100s.</p><p>Results. The experimental results are shown in Table <ref type="table" target="#tab_3">2</ref>. We searched BurgerFormer under four different levels of FLOPs and compared it with state-of-the-art Transformers. Burg-erFormers are competitive even superior to either manually designed or automatically searched Transformers under similar FLOPs. For example, BurgerFormer-Tiny has only 1.0G FLOPs, but the accuracy is higher than the manually designed DeiT-T and the automatically searched AutoFormer-Tiny by 5.8% and 3.3%. Meanwhile, BurgerFormer-Base outperforms Swin-T by 1.4% points with even fewer FLOPs (3.9G vs. 4.5G) and parameters (26M vs. 29M ). In addition, although BurgerFormer-Large has the same Top-1 accuracy as Swin-S, it has 25% and 28% fewer FLOPs and parameters, respectively. BurgerFormer-Large also outperforms the pure CNN model RegNetY-8G and the hy-brid CNN-Transformer model BossNAS-T1. Furthermore, BurgerFormer consistently outperforms the automatically searched Transformer-like model, indicating the effectiveness of our micro-meso-macro designed search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on COCO</head><p>Setting. We conduct experiments on COCO benchmark <ref type="bibr" target="#b27">(Lin et al., 2014)</ref> to verify the transferability of Burger-Former architectures. COCO benchmark contains 118K training images (train2017) and 5K validation images (val2017). We employ BurgerFormer as backbone for two standard detectors, i.e., <ref type="bibr">RetinaNet (TsungYi et al., 2017)</ref> and Mask R-CNN <ref type="bibr" target="#b24">(Kaiming et al., 2017)</ref>. The backbone is initialized with the weights pre-trained on ImageNet, while the added layers are initialized by Xavier <ref type="bibr" target="#b49">(Xavier &amp; Yoshua, 2010)</ref>. The detectors are trained with AdamW <ref type="bibr" target="#b22">(Ilya &amp; Frank, 2019)</ref> optimizer and an initial learning rate of 1e−4. Following <ref type="bibr" target="#b45">(TsungYi et al., 2017;</ref><ref type="bibr" target="#b24">Kaiming et al., 2017)</ref>, we utilized 1× training schedule where the detectors are trained with 12 epochs. The training images are resized to 800 pixels (short side) by no more than 1,333 pixels (long side). The testing images have a short side of 800 pixels.</p><p>Results. As shown in Table <ref type="table" target="#tab_4">3</ref>, under a similar volume of parameters, BurgerFormer significantly outperforms ResNet50 and PoolFormer-S24. When compared with powerful handcrafted baselines such as PVT-Small and Swin-T, Burger-Former achieves competitive results, which validates the transferability of the searched model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>Search Method. In Table <ref type="table" target="#tab_5">4</ref>, we compared random search, SPOS, Sandwich, and our hybrid sampling search method on ImageNet. For a fair comparison, all models are constrained to 1G FLOPs and are trained under the same settings. The experimental results demonstrate that our hybrid sampling search method outperforms random search,  SPOS and sandwich by 2.4 points, 1.0 points and 0.5 points, respectively, validating the effectiveness of the proposed method.</p><p>Hamburger Blocks. and remove the residual connections, to check its impact on performance. Table <ref type="table" target="#tab_6">5</ref> shows that each part contributes to the performance with similar FLOPs, but the impact of the meat part is somewhat larger, which we attribute to that this part locates in the inverse bottleneck structure thus has a greater impact on the performance. We can see the hamburger structure offers more diversity to find a better architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion</head><p>Searched Architecture. Fig. <ref type="figure">6</ref> visualizes the structure of BurgeFormer-Tiny, where d, w, and r denote depth, width, and expansion ratio, respectively. Overall, local operations such as pooling and convolution dominate in BurgerFormer-Tiny, while there is only one global operation (i.e., Spatial MLP) in the third stage. Interestingly, the hamburger block of the fourth stage resembles a depthwise separable convolution but does not use an activation function with a different normalization. Meanwhile, the second stage uses three depthwise convolutions to obtain a larger perceptual field. More discussions can be referred to Appendix B.</p><p>Visualization. Fig. <ref type="figure">7</ref> visualizes learned features of DeiT-Small and BurgerFormer-Base on ImageNet. For a fair comparison, both methods use the same visualization technique <ref type="bibr" target="#b36">(Ramprasaath R. et al., 2017)</ref>. The visualized heat map is calculated by multiplying the output feature map by its gradient and then scaling it to the size of the input image. As shown in Fig. <ref type="figure">7</ref>, BurgerFormer locates objects more accurately than DeiT, which indicates that the searched architectures have a better ability to extract features.</p><p>Micro-Meso-Macro Design. In this work, we present a micro-meso-macro schema for designing search spaces. An intuitive question is whether three granularities are needed.</p><p>We believe that the co-design of the three granularities is beneficial and necessary. Micro multivariate operations can constitute diverse local and global operators, and the appropriate pairing of these is helpful to improve performance; Meso hamburger block covers the common structure of Mobilenet and Transformer block (e.g., inverse bottleneck, residuals); macro-level design can significantly affect FLOPs and parameters, and thus the model capacity. Recently, ConvNeXt <ref type="bibr" target="#b30">(Liu et al., 2022)</ref> manually improved ResNet from three granularities to outperform Swin <ref type="bibr" target="#b29">(Liu et al., 2021)</ref>, so the automatic search for three granularities should be a direction worth thinking about and exploring.</p><p>Limitation. Limited by hardware resources, the ranges of FLOPs and parameters of the searched architectures are relatively small (e.g., maximum FLOPs &lt; 8G), and we conjecture that BurgerFormer searched under larger FLOPs or higher volume of parameters will have stronger representation capability. In addition, because all of the three granularities are involved in the micro-meso-macro search space, the search cost is high, e.g., training of the supernet takes 11 days on eight V100s. Therefore, reducing the search cost is a direction for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In   Table <ref type="table">6</ref>. The tokens, patch embedding and MHSA head number settings of each stage.</p><p>Table <ref type="table">6</ref> shows the tokens, patch embedding, and MHSA head number settings of each stage. "K × K, stride S, C" in the patch embedding column indicates the kernel size K, the stride S and the output channels C of convolution. Following <ref type="bibr" target="#b26">(Liao et al., 2021)</ref>, we add three additional 3x3 convolutions into the patch embedding of stage one. Since the three convolutions already have a large receptive field, we set the kernel size three in the first patch embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. A taxonomy of search space designs. Micro, Meso, and Macro correspond to operation, block, and stage granularity, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The micro-meso-macro search space. The top left is the micro search space, which contains candidate activation functions, normalizations and basic operations. Skip indicates no data processing. The top right is the meso search space, which includes the Hamburger and the Norm-Op-Norm-Act structure. Norm, Op, and Act represent normalization, basic operation, and activation function, respectively. The lower part is the macro search space, which produces a multi-stage architecture.</figDesc><graphic url="image-1.png" coords="3,79.85,253.41,77.84,58.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Transformer block vs. Hamburger block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure4. Hybrid sampling method. Meso sampling is sampling one hamburger block for each stage. Macro sampling is sampling the minimum, maximum, and random sub-networks in the macro search space, whose blocks are determined by the meso sampling. The supernet weights are updated with the accumulated gradients from the sampled three sub-networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure6. The architecture of BurgerFormer-Tiny. "d2, w32, r4" means that depth, width, and expansion ratio are 2, 32, and 4, respectively.</figDesc><graphic url="image-2.png" coords="8,102.63,290.94,58.17,58.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The architectures of BurgerFormer-Small, BurgerFormer-Base, and BurgerFormer-Large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9</head><label>9</label><figDesc>Fig.9visualizes the structure of BurgerFormer-Small, BurgerFormer-Base and BurgerFormer-Large. At the microscopic level, global operators (i.e., Spatial MLP, MHSA) always exist in stage three or stage four. And the searched architectures prefer MHSA to Spatial MLP when the model size grows. Besides, an interesting observation is that Pooling, Spatial MLP, and MHSA can be followed by an activation function, which is rarely considered in previous architecture designs. At the mesoscopic and macroscopic levels, all three parts of the hamburger block tend to be leveraged and the architectures become deeper and wider as the model size grows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>proposed S3 space based on ViT and Swin, and leveraged weight sharing and E-T</figDesc><table><row><cell>Activation</cell><cell>ReLU6</cell><cell cols="2">GELU</cell><cell>SiLU</cell><cell>Skip</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Norm-Op-Norm-Act</cell></row><row><cell>Normalization</cell><cell>BN</cell><cell></cell><cell>GN</cell><cell>LN</cell><cell>Skip</cell><cell>Hamburger</cell><cell></cell><cell>Conv 1x1</cell><cell>Act</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell cols="2">Norm-Op-Norm-Act</cell><cell>Norm</cell></row><row><cell></cell><cell>MHSA</cell><cell cols="3">Spatial MLP</cell><cell>Pooling</cell><cell></cell><cell></cell><cell>Conv 1x1</cell><cell>Op</cell></row><row><cell>Basic Operation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Dwise 3x3</cell><cell cols="2">Conv 1x1</cell><cell>Skip</cell><cell></cell><cell cols="2">Norm-Op-Norm-Act</cell><cell>Norm</cell></row><row><cell></cell><cell cols="3">Micro Seach Space</cell><cell></cell><cell></cell><cell cols="3">Meso Search Space</cell></row><row><cell></cell><cell></cell><cell cols="2">Stage 1</cell><cell></cell><cell>Stage 2</cell><cell>Stage 3</cell><cell></cell><cell>Stage 4</cell></row><row><cell></cell><cell>Patch Embedding</cell><cell></cell><cell></cell><cell></cell><cell>Patch Embedding</cell><cell>Patch Embedding</cell><cell></cell><cell>Patch Embedding</cell><cell>classifier</cell></row><row><cell></cell><cell cols="4">depth, width, ratio</cell><cell>depth, width, ratio</cell><cell cols="2">depth, width, ratio</cell><cell>depth, width, ratio</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Macro Search Space</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Macro Search Space. Depth is the number of blocks per stage. Width refers to the channels of 2D feature maps or the embedding dimension of 1D tokens. Ratio is the expansion ratio between the two 1x1 convolutions.Act, and Op represent normalization, activation function, and basic operation, respectively. Please note that both the pre-Op normalization and the post-Op normalization are determined by searching, so the two normalizations can be different. Secondly, on the one hand, adding convolution to feed-forward network (FFN) can improve performance<ref type="bibr" target="#b55">(Yuan et al., 2021)</ref>, on the other hand, we observe that both of the MobileNet and Transformer blocks have inverse bottleneck structures. Therefore, we split the FFN into two 1x1 convolutions and add Norm-Op-Norm-Act between them.</figDesc><table><row><cell>Stage</cell><cell>Depth</cell><cell>Width</cell><cell>Ratio</cell></row><row><cell>1</cell><cell>{1, 2, 3, 4}</cell><cell>{32, 64, 96}</cell><cell>{1, 2, 4, 6}</cell></row><row><cell>2</cell><cell>{1, 2, 3, 4}</cell><cell>{64, 96, 128}</cell><cell>{1, 2, 4, 6}</cell></row><row><cell>3</cell><cell cols="3">{1, 2, 3, 4, {128, 192, 256, 320} {1, 2, 4, 6} 5, 6, 7, 8}</cell></row><row><cell>4</cell><cell cols="3">{1, 2, 3, 4} {128, 256, 384, 512} {1, 2, 4, 6}</cell></row></table><note>, we design the hamburger block based on three modifications to the classical Transformer block. Firstly, the combination of LN and MHSA is abstracted as a Norm-Op-Norm-Act structure for search, where Norm,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art models on ImageNet. "Param." is the volume of parameters. "acc." is accuracy. The gray color highlights the searched BurgerFormer architectures. We group different models according to their FLOPs.</figDesc><table><row><cell></cell><cell cols="5">Params. (M) FLOPs (G) Top-1 acc. (%) Top-5 acc. (%) Design Type</cell></row><row><cell>DeiT-Ti (Touvron et al., 2021b)</cell><cell>6</cell><cell>1.3</cell><cell>72.2</cell><cell>91.1</cell><cell>Manual</cell></row><row><cell>TNT-Ti (Han et al., 2021)</cell><cell>6</cell><cell>1.4</cell><cell>73.9</cell><cell>-</cell><cell>Manual</cell></row><row><cell>CeiT-T (Yuan et al., 2021)</cell><cell>6</cell><cell>1.2</cell><cell>76.4</cell><cell>-</cell><cell>Manual</cell></row><row><cell>AutoFormer-tiny (Chen et al., 2021b)</cell><cell>6</cell><cell>1.3</cell><cell>74.7</cell><cell>92.6</cell><cell>Auto</cell></row><row><cell>GLiT-Tiny (Chen et al., 2021a)</cell><cell>7</cell><cell>1.4</cell><cell>76.3</cell><cell>91.1</cell><cell>Auto</cell></row><row><cell>ViTAS-DeiT-A (Su et al., 2021)</cell><cell>7</cell><cell>1.4</cell><cell>75.6</cell><cell>92.5</cell><cell>Auto</cell></row><row><cell>BurgerFormer-Tiny</cell><cell>10</cell><cell>1.0</cell><cell>78.0</cell><cell>93.7</cell><cell>Auto</cell></row><row><cell>ConVi-Ti+ (d'Ascoli et al., 2021)</cell><cell>10</cell><cell>2.0</cell><cell>76.7</cell><cell>93.6</cell><cell>Manual</cell></row><row><cell>PVT-Tiny (Wang et al., 2021)</cell><cell>13</cell><cell>1.9</cell><cell>75.1</cell><cell>-</cell><cell>Manual</cell></row><row><cell>PoolFormer-S12 (Yu et al., 2022)</cell><cell>12</cell><cell>2.0</cell><cell>77.2</cell><cell></cell><cell>Manual</cell></row><row><cell>BurgerFormer-Small</cell><cell>14</cell><cell>2.1</cell><cell>80.4</cell><cell>95.0</cell><cell>Auto</cell></row><row><cell>Deit-S (Touvron et al., 2021b)</cell><cell>22</cell><cell>4.7</cell><cell>79.9</cell><cell>-</cell><cell>Manual</cell></row><row><cell>Swin-T (Liu et al., 2021)</cell><cell>29</cell><cell>4.5</cell><cell>81.3</cell><cell>-</cell><cell>Manual</cell></row><row><cell>CvT-13 (Haiping et al., 2021)</cell><cell>20</cell><cell>4.5</cell><cell>81.6</cell><cell>-</cell><cell>Manual</cell></row><row><cell>TNT-S (Han et al., 2021)</cell><cell>24</cell><cell>5.2</cell><cell>81.5</cell><cell>95.7</cell><cell>Manual</cell></row><row><cell>PVT-Small (Wang et al., 2021)</cell><cell>25</cell><cell>3.8</cell><cell>79.8</cell><cell>-</cell><cell>Manual</cell></row><row><cell>ViL-Small (Pengchuan et al., 2021)</cell><cell>25</cell><cell>4.9</cell><cell>82.0</cell><cell>-</cell><cell>Manual</cell></row><row><cell>ResMLP-S12 (Touvron et al., 2021a)</cell><cell>31</cell><cell>6.0</cell><cell>79.4</cell><cell>-</cell><cell>Manual</cell></row><row><cell>Twins-PCPVT-S (Xiangxiang et al., 2021)</cell><cell>24</cell><cell>3.8</cell><cell>81.2</cell><cell>-</cell><cell>Manual</cell></row><row><cell>PoolFormer-S36 (Yu et al., 2022)</cell><cell>31</cell><cell>5.2</cell><cell>81.4</cell><cell>-</cell><cell>Manual</cell></row><row><cell>RegNetY-4G (Ilija et al., 2020)</cell><cell>21</cell><cell>4.0</cell><cell>80.0</cell><cell>-</cell><cell>Auto</cell></row><row><cell>AutoFormer-small (Chen et al., 2021b)</cell><cell>23</cell><cell>5.1</cell><cell>81.7</cell><cell>-</cell><cell>Auto</cell></row><row><cell>GLiT-Small (Chen et al., 2021a)</cell><cell>25</cell><cell>4.4</cell><cell>80.5</cell><cell>-</cell><cell>Auto</cell></row><row><cell>ViTAS-DeiT-B (Su et al., 2021)</cell><cell>23</cell><cell>4.9</cell><cell>80.2</cell><cell>95.1</cell><cell>Auto</cell></row><row><cell>S3-T (Minghao et al., 2021)</cell><cell>28</cell><cell>4.7</cell><cell>82.1</cell><cell>95.8</cell><cell>Auto</cell></row><row><cell>ViT-ResNAS-Medium (Liao et al., 2021)</cell><cell>97</cell><cell>4.5</cell><cell>82.4</cell><cell>-</cell><cell>Auto</cell></row><row><cell>BurgerFormer-Base</cell><cell>26</cell><cell>3.9</cell><cell>82.7</cell><cell>96.2</cell><cell>Auto</cell></row><row><cell>Swin-S (Liu et al., 2021)</cell><cell>50</cell><cell>8.7</cell><cell>83.0</cell><cell>-</cell><cell>Manual</cell></row><row><cell>Twins-PCPVT-B (Xiangxiang et al., 2021)</cell><cell>44</cell><cell>6.4</cell><cell>82.7</cell><cell>-</cell><cell>Manual</cell></row><row><cell>CvT-21 (Haiping et al., 2021)</cell><cell>32</cell><cell>7.1</cell><cell>82.5</cell><cell>-</cell><cell>Manual</cell></row><row><cell>BoTNet-S1-59 (A. et al., 2021)</cell><cell>34</cell><cell>7.3</cell><cell>81.7</cell><cell>-</cell><cell>Manual</cell></row><row><cell>RegNetY-8G (Ilija et al., 2020)</cell><cell>39</cell><cell>8.0</cell><cell>81.7</cell><cell>-</cell><cell>Auto</cell></row><row><cell>BossNet-T1 (Changlin et al., 2021)</cell><cell>-</cell><cell>7.9</cell><cell>82.2</cell><cell>95.8</cell><cell>Auto</cell></row><row><cell>BurgerFormer-Large</cell><cell>36</cell><cell>6.5</cell><cell>83.0</cell><cell>96.8</cell><cell>Auto</cell></row></table><note>Figure 5. Comparison between BurgerFormer and efficient vision transformers, in terms of ImageNet Top-1 over FLOPs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art models on COCO.</figDesc><table><row><cell>BackBone</cell><cell cols="4">RetinaNet 1x Param. (M) AP b AP b 50 AP b 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>ResNet50 (He et al., 2016)</cell><cell>37.7</cell><cell>36.3</cell><cell>55.3</cell><cell>38.6</cell><cell>19.3</cell><cell>40.0</cell><cell>48.8</cell></row><row><cell>PVT-Small (Wang et al., 2021)</cell><cell>34.2</cell><cell>40.4</cell><cell>61.3</cell><cell>43.0</cell><cell>25.0</cell><cell>42.9</cell><cell>55.7</cell></row><row><cell>PoolFormer-S24 (Yu et al., 2022)</cell><cell>31.1</cell><cell>38.9</cell><cell>59.7</cell><cell>41.3</cell><cell>23.3</cell><cell>42.1</cell><cell>51.8</cell></row><row><cell>BurgerFormer-Base</cell><cell>35.9</cell><cell>41.2</cell><cell>61.3</cell><cell>43.9</cell><cell>24.2</cell><cell>44.4</cell><cell>55.4</cell></row><row><cell>BackBone</cell><cell cols="6">Mask R-CNN 1x Param. (M) AP b AP b 50 AP b 75 AP M AP M 50</cell><cell>AP M 75</cell></row><row><cell>ResNet50 (He et al., 2016)</cell><cell>44.2</cell><cell>38.0</cell><cell>58.6</cell><cell>41.4</cell><cell>34.4</cell><cell>55.1</cell><cell>36.7</cell></row><row><cell>PVT-Small (Wang et al., 2021)</cell><cell>44.1</cell><cell>40.4</cell><cell>62.9</cell><cell>43.8</cell><cell>37.8</cell><cell>60.1</cell><cell>40.3</cell></row><row><cell>PoolFormer-S24 (Yu et al., 2022)</cell><cell>41.0</cell><cell>40.1</cell><cell>62.2</cell><cell>43.4</cell><cell>37.0</cell><cell>59.1</cell><cell>36.9</cell></row><row><cell>Swin-T (Liu et al., 2021)</cell><cell>48.0</cell><cell>43.7</cell><cell>66.6</cell><cell>47.7</cell><cell>39.8</cell><cell>63.3</cell><cell>42.7</cell></row><row><cell>BurgerFormer-Base</cell><cell>45.9</cell><cell>44.0</cell><cell>65.6</cell><cell>48.1</cell><cell>40.2</cell><cell>62.7</cell><cell>43.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of Random Search, SPOS, Sandwich and our hybrid sampling search method on ImageNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Table5analyzes the effect of each Norm-OP-Norm-Act in hamburger blocks. We choose the Transformer-Tiny model as the baseline. We remove the part corresponding to burger, i.e., replace the part with skip Ablation study of Hamburger blocks.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs (G) Top-1 acc. (%)</cell></row><row><cell>BurgerFormer-Tiny</cell><cell>1.0</cell><cell>78.0</cell></row><row><cell>w/o the first bread</cell><cell>0.85</cell><cell>77.7</cell></row><row><cell>w/o the second bread</cell><cell>0.82</cell><cell>77.2</cell></row><row><cell>w/o the meat</cell><cell>0.84</cell><cell>76.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>this work, we design a new search space for searching BurgerFormer architectures from a micro-meso-macro perspective. Meanwhile, we propose a hybrid search strategy to train supernet effectively. Experiments on ImageNet and COCO datasets verify the effectiveness of the BurgerFormer architecture. While demonstrating better performance than state-of-the-art Transformers, this work indicates that good search space is very critical for finding out powerful visual Transformer-like architectures. In the future, we will further explore the Transformer-like search space design and experiment with other computer vision tasks. Zhun, Z., Liang, Z., Guoliang, K., Shaozi, L., and Yi, Y.Random erasing data augmentation. In Association for the Advancement of Artificial Intelligence, 2020. Zoph, B. and Le, Q. V. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017. Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning transferable architectures for scalable image recognition. In Conference on Computer Cision and Pattern Recognition, 2018.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by the National Key R&amp;D Program of China under Grant No. 2018AAA0102701, and in part by the National Natural Science Foundation of China under Grant No. 62176250.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Sampling Methods</head><p>The figure indicates that the sub-networks accuracy of the SPOS method is generally lower than that of the hybrid sampling method, especially when the FLOPs are larger than 3G. Fig. <ref type="figure">8</ref> shows the retraining accuracy along training epochs of the maximum and minimum models on ImageNet-100. The training configuration is the same as that on ImageNet except that the batch size is 128. The experimental results demonstrate that the maximum model has much worse convergence ability than the minimum model before 200 epochs. Furthermore, the Top-1 accuracy of the maximum model is not the upper bound in the search space, so it is not suitable to train the supernet directly using the sandwich sampling method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsung-Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jonathon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units (relu)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Changlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guangrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiefeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiaodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiaojun</surname></persName>
		</author>
		<author>
			<persName><surname>Bossnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural architecture search for global and local image transformer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><surname>Glit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nasvit: Neural architecture search for efficient vison transformers with gradient conflict-aware supernet training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chengyue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dilin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xinlei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhicheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yuandong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Fairnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ascoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ekin Dogus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Barret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jonathon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Haiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mengchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hongyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moustapha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herv'e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018. 2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ilija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raj Prateek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Georgia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename></persName>
		</author>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning (ICML 2000)</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Machine Learning (ICML 2000)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-stage vision transformers</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00642</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">A convnet for the 2020s</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Searching the search space of vision transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Minghao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Houwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jianlong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Haibin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Olga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Andrej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pengchuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jianwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jianfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhruv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Con-ference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sangdoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dongyoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Seong Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanghyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Junsuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13700</idno>
		<title level="m">Vision transformer architecture search</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tsungyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piotr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fbnet: Hardwareaware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiangxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haibing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiaolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huaxia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chunhua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yonglong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dilip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Phillip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bignas: Scaling up neural architecture search with big singlestage models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Cision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
