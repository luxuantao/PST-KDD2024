<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word matching using single closed contours for indexing handwritten historical documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-07-29">29 July 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">O</forename><surname>R I G I Na L Pa P E R</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomasz</forename><surname>Adamek</surname></persName>
							<email>adamekt@eeng.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
							<email>oconnorn@eeng.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
							<email>alan.smeaton@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word matching using single closed contours for indexing handwritten historical documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-07-29">29 July 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">59E4CEE11C41AD70BC3AB1E07E3B1542</idno>
					<idno type="DOI">10.1007/s10032-006-0024-y</idno>
					<note type="submission">Received: 14 February 2005 / Revised: 9 March 2006 / Accepted: 28 May 2006 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Historical manuscripts</term>
					<term>Holistic word recognition</term>
					<term>Contour matching</term>
					<term>Annotation</term>
					<term>Indexing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective indexing is crucial for providing convenient access to scanned versions of large collections of historically valuable handwritten manuscripts. Since traditional handwriting recognizers based on optical character recognition (OCR) do not perform well on historical documents, recently a holistic word recognition approach has gained in popularity as an attractive and more straightforward solution (Lavrenko et al. in proc. document Image Analysis for Libraries (DIAL'04), pp. [278][279][280][281][282][283][284][285][286][287] 2004). Such techniques attempt to recognize words based on scalar and profile-based features extracted from whole word images. In this paper, we propose a new approach to holistic word recognition for historical handwritten manuscripts based on matching word contours instead of whole images or word profiles. The new method consists of robust extraction of closed word contours and the application of an elastic contour matching technique proposed originally for general shapes (Adamek and O'Connor  in IEEE Trans Circuits Syst Video Technol 5 : 2004). We demonstrate that multiscale contour-based descriptors can effectively capture intrinsic word features avoiding any segmentation of words into smaller subunits. Our experiments show a recognition accuracy of 83%, which considerably exceeds the performance of other systems reported in the literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is an increasing need to digitally preserve and provide access to historical document collections <ref type="bibr" target="#b2">[3]</ref>. However, the application of existing technology to this challenging problem exposes numerous problems. Off-the-shelf optical character recognition (OCR) or commercial document processing systems are often not able to cope with problems peculiar to historical manuscripts such as poor quality of the manuscript itself, noise, stained paper, contrast variations, faded ink, and differences in pressure of the writing instrument. Therefore, recently a significant research effort has been devoted specifically to the analysis of historical documents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>.</p><p>The approach described in this paper is motivated by the work of Lavrenko, Rath and Manmatha <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> who promote the idea of holistic word recognition for handwritten historical documents. Their main focus is on achieving reasonable recognition accuracy, which enables retrieval of handwritten pages from a user-supplied ASCII query. They argue that, although currently it is not feasible to produce near-perfect recognition results, satisfactory retrieval can still be performed using the noisy outputs of recognizers <ref type="bibr" target="#b7">[8]</ref>. In their word spotting approach for indexing historical handwritten manuscripts, pages are segmented into words which are then matched as images and grouped into clusters which contain all instances of the same word. A partial index is constructed for the collection by tagging a number of the resulting clusters.</p><p>There has been a large number of approaches to handwritten text recognition proposed in the past. Although, typically they cannot be directly used for word spotting in historical documents some solutions and observations are still relevant to the work presented here.</p><p>There has been extensive research devoted to isolated handwritten character recognition, which in theory could be applied to word recognition <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. A comprehensive review of handwriting recognition approaches considering mainly on-line methods can be found in <ref type="bibr" target="#b8">[9]</ref>. Many approaches to alphanumerics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and Chinese character recognition are based on some form of elastic matching. Importantly, some studies <ref type="bibr" target="#b8">[9]</ref> of the character recognition task reported that elastic matching can halve the error rate of linear matching.</p><p>The approaches intended solely for word recognition can be classified as either analytical or holistic. The approaches in the first category proceed by first identifying the characters and then building word interpretation. Typical approaches adopt an over-segmentation methodology followed by character model based recognition using dynamic programming (DP) <ref type="bibr" target="#b11">[12]</ref>. On the other hand, the holistic approaches attempt to recognize the word as a whole without preliminary letter identification. An excellent discussion of the holistic paradigm can be found in <ref type="bibr" target="#b12">[13]</ref>. The review presented here is primarily based on this discussion.</p><p>Madhvanath and Govindaraja <ref type="bibr" target="#b12">[13]</ref> identifies three levels of word features which can be used by holistic recognizers, namely: low-, intermediate-, and highlevel. Examples of typical low-level features include stroke direction distribution <ref type="bibr" target="#b13">[14]</ref>, and various profilebased features <ref type="bibr" target="#b4">[5]</ref>. An early approach in this category, presented in <ref type="bibr" target="#b13">[14]</ref>, used elastic matching with eight direction codes to recognize ten cursively written key words. Typical representative of intermediate-level features are edges, end-points, concavities, diagonal, and horizontal strokes <ref type="bibr" target="#b14">[15]</ref>. High-level features include ascenders, descenders, loops, i-dots, t-bars, and length <ref type="bibr" target="#b15">[16]</ref>. For example, the approach in <ref type="bibr" target="#b16">[17]</ref> relied on detection of features such as gaps between words, word length, ascenders, and descenders for verification of handwritten phrases (lexicon reduction). A DP algorithm was used to match the predicted features with the extracted image features.</p><p>It should be noted that the introduction of intermediate-or high-level features is mainly dictated by the need for constructing synthetic word models from character models (ASCII), since in many applications the words or phrases to be verified or recognized are not known a priori, i.e., they are not available in the training collection. On the other hand, the word spotting in historical documents, as presented in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, can use a stricter holistic paradigm and directly match the unknown words with annotated words from the training collection. Consequently, word spotting does not necessarily require extraction of intermediate-or high-level features.</p><p>To summarize, many approaches to handwritten text recognition currently reported in the literature are not suitable for word spotting in historical documents due to their (1) limitation to single alphanumerics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, (2) reliance on character recognition <ref type="bibr" target="#b11">[12]</ref>, (3) limitation to applications with small lexicons <ref type="bibr" target="#b13">[14]</ref>, or (4) weak discriminatory power suitable for lexicon reduction rather than for word recognition <ref type="bibr" target="#b16">[17]</ref>.</p><p>Given that traditional handwriting recognizers, analytical or holistic, do not perform well on historical documents, recently an increasing research effort has been devoted specifically to the analysis of historical documents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. To the best of the authors' knowledge all approaches available currently in literature are based on the holistic paradigm. So far, the most advanced approaches in this area were proposed by Lavrenko, Rath and Manmatha <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Since the quality of historical documents is often significantly degraded it is crucial to select the right features for word matching. The feature set proposed by Rath and Manmatha <ref type="bibr" target="#b4">[5]</ref> includes smoothed versions of the word images and various profile-based features. For example, the shape of a word is captured by upper and lower word profiles extracted by traversing the upper (lower) boundary of a word's bounding box and recording for each image column the distance to the nearest "ink" pixel in that column. In order to capture the "inner" structure of a word these features were extended by the number of background to "ink" transitions. It appears that the biggest limitation of the above set of features is their strong dependency on good normalization, mainly skew/slant angle normalization and baseline<ref type="foot" target="#foot_0">1</ref> detection.</p><p>In their early approach profile-based features were compared using dynamic time warping (DTW) <ref type="bibr" target="#b5">[6]</ref>. They demonstrated on two different data sets from the George Washington collection that their approach outperforms competing matching techniques, including the shape context approach <ref type="bibr" target="#b10">[11]</ref> which is currently the best classifier for handwritten digits. They showed in <ref type="bibr" target="#b0">[1]</ref> that application of statistical language models can further improve word recognition accuracy as a postprocessing step. A simple Hidden Markov Model with one state for each word resulted in over 10% improvement in recognition accuracy. Recently, they proposed a complete search engine for historical manuscript images <ref type="bibr" target="#b2">[3]</ref>. The main innovation of this system is that it is based on ideas developed in information retrieval, cross-lingual retrieval, and automatic image annotation and retrieval rather than on direct matching of word features. Although the system shows promising results on an impressive collection of 987 page images of George Washington's manuscripts, the objective evaluation was carried out only for 26 queries due to the lack of relevance judgments for such large collection.</p><p>In this work, our primary goal is to investigate the possibility of matching words using their contours instead of whole images or word profiles. We believe that contour-based descriptors can better capture a word's important details and eliminate the need for skew and slant angle normalization. The proposed approach utilizes a rich multiscale contour representation which eliminates several problems related to holistic representation of words. Since multiscale representation implicitly captures details at multiple levels there is no need to choose between low-, intermediate-, and high-level features or their integration. Unlike the case of intermediate-, and high-level features there is no need for making any decision before the actual matching can proceed. Furthermore, elastic contour matching based on a DP technique provides a flexible way to compensate for inter-character and intra-character spacing variations. In this initial approach, we assume that bounding boxes for each word are already known <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. The inner holes in letters and dots are ignored.</p><p>The remainder of this article is organized as follows: the next section describes extraction of a single closed contour for each word. Then, the contour matching algorithm proposed originally for general shapes <ref type="bibr" target="#b1">[2]</ref> is briefly described in Sect. 3. Next, experimental results are presented in Sect. 4. Our outlook on future research is discussed in Sect. 5 and conclusions are formulated in Sect. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contour extraction</head><p>Extraction of a single closed contour for each word is a key component of the proposed approach. The extraction procedure has to deal with the poor quality typical of manuscripts (noise, stained paper, etc.), contrast variations (faded ink, differences in pressure on the writing instrument) and the most challenging problem -disconnected letters. Figure <ref type="figure">1</ref> shows a sample of a typical manuscript under consideration. Contour extraction is performed in five steps: binarization, localization of the main body of lowercase letters, connected components labeling, connecting disconnected letters, and contour tracing.</p><p>All parameters controlling the contour extraction are set empirically using scanned pages of modern handwritten text. It should be noted that theoretically these parameters could be optimized based on the annotated training collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Binarization</head><p>In the first step, the pixels from the input gray level image are classified as either "ink" or "paper" by comparing their values with a threshold. Since the contrast of the input word image can vary considerably, mainly due to differences in pressure on the writing instrument, there is no single optimal threshold that provides acceptable binarization for different parts of the word. Therefore, the optimal threshold has to be estimated individually for each pixel based on its local neighborhood. We chose Niblack's <ref type="bibr" target="#b18">[19]</ref> algorithm where the threshold T for a given pixel is computed using the mean µ and the standard deviation σ of the gray values in a small window centered at the pixel. The threshold is calculated according to the formula proposed by Sauvola et al. <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_0">T = µ 1 -k 1 - σ R (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where k is a constant set to 0.02 and R denotes the dynamics of the standard deviation and is set to 128. The window size was fixed manually to cover approximatively the width of one stroke. However, the above approach does not produce smooth word outlines and as such was modified. Prior to binarization, morphological filtering <ref type="bibr" target="#b20">[21]</ref> is used to create eroded and opened versions of the input imagethe structuring element employed is shown in Fig. <ref type="figure">2</ref>. The dynamic threshold T is calculated based on the eroded image. The binary classification of a single pixel as "ink" or "paper" is made by comparing its value from the opened image with the dynamic threshold calculated for this pixel based on the eroded image. An example of binarization is shown in Fig. <ref type="figure">3</ref>. Typically, weak strokes (in this example top of double "t" and connections between "r" and "s") are detected and even dilated without widening the strong strokes. However, it should be stressed that the above approach is just an adaptation of one of many binarization methods available in the literature -see for example <ref type="bibr" target="#b21">[22]</ref>. Detailed evaluation of the influence of the binarization stage on the overall performance of the system is outside the scope of this article. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Position estimation</head><p>Often binarization alone is not sufficient to obtain a single connected "ink" region for each word. In such cases, separated parts of the word have to be connected. The proposed connecting procedure utilizes the position of the word baseline and the x-height. <ref type="foot" target="#foot_1">2</ref> Detection is performed by analysis of the number of "ink" pixels in each line of the word binary image (see Fig. <ref type="figure" target="#fig_1">4</ref>).</p><p>Let us assume that the origin of the coordinate system lies at the left bottom corner of the word bounding box and that the baseline and the line defining the top of the main body of lowercase letters are vertical. Their vertical coordinates are denoted as y l and y u , respectively. Let x height denote the x-height. Note that y u = y l + x height . Also, let P V (y) be a function representing number of "ink" pixels in line y and y max be a vertical coordinate of the line with the maximum number of "ink" pixels. The y u is located by finding the closest line to y max fulfilling the conditions: y &gt; y max and P V (y) &lt; α u P V (y max ). Analogically y l is located by finding the closest line to y max fulfilling the conditions: y &lt; y max and P V (y) &lt; α l P V (y max ). Parameters α u and α l were set empirically to 0.5 and 0.3, respectively.</p><p>Horizontal boundaries of lowercase letters are detected by analyzing the number of "ink" pixels in each column of the binary word image. Let P H (x) be a function representing number of "ink" pixels in column x counted between y l and y u . The first column for which P H (x)/x height &gt; α x found during scanning the word image from left(right) to right(left) is chosen as the left(right) boundary of lowercase letters. The parameter α x was set empirically to 0.1. Additional rules can be applied to eliminate the residuals of the vertical margin line as illustrated in the example shown in Fig. <ref type="figure" target="#fig_2">5c</ref>.</p><p>It should be noted that the position of the baseline and the length of x-height are not directly used by the matching algorithm, but serve only as guidelines for the extraction of word contours. Moreover, if all "ink" pixels are already connected in the binarization step, the proper contour can be extracted even if the estimated baseline and x-height are inaccurate. Although precise identification of baseline and x-height is not crucial to the presented approach, it should be also noted that, the above procedure would fail to accurately identify the baseline and x-height in the presence of significant skew. In such cases, an alternative method for baseline detection should be investigate like, for example, the method presented in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Connected component labeling</head><p>The connected component labeling algorithm scans the word's binary image and groups "ink" pixels into components based on pixel 8-neighborhood connectivity. Once all groups are determined, each pixel is labeled according to the component it is assigned to. Only components containing a sufficient number of pixels within the area of the main body of lowercase letters are retained for further processing, where the required number of pixels is computed as [0.1x 2  height ]. As a result, small "ink" regions and letter residuals from the line above or below the current word are eliminatedexamples are shown in Fig. <ref type="figure" target="#fig_2">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(fourth column).</head><p>It should be noted that the above procedure removes punctuation and diacritic marks. This may be problematic for languages which utilize many diacritic marks. However, the results presented here indicate that ignoring diacritic marks, especially in the presence of significant noise and stains, is not critical for Western script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Connecting disconnected letters</head><p>Once the relevant components are identified they are sorted based on the horizontal positions of their center of gravity. Then, successive components are connected by adding the best connecting link (synthetic "ink" line) into the binary image. Our observations indicate that disconnections within parts of a letter are rare due to the dynamic thresholding. Therefore, it is reasonable to assume that disconnections occur only between letters. The best candidate for the link between two disconnected components is chosen by the following procedure.</p><p>At first, candidates for the beginning and the end of the link are chosen from the region on the left and right, respectively. The candidates for the beginning(end) of the link are chosen as the first contour pixels from the right(left) in each line of the left(right) region. Then, candidate links are created by pairing selected points from the left and right region. This is followed by link validation. A link is considered as valid only if:</p><p>• Both ends of the link are "strictly" inside of the main body of lowercase letters -see examples (a), (b) and (e) in Fig. <ref type="figure" target="#fig_2">5</ref>; An end of a link is considered to be "strictly" inside of the main body of lowercase letters if its vertical coordinate y fulfils the condition:</p><formula xml:id="formula_2">y l &lt; y &lt; (y u -α c x height )</formula><p>, where value of parameter α c was set empirically to 0.2. • Both ends of the link are considerably above the main body of lowercase letters -see the top of letter 'I' in example 5(h). An end of a link is considered to be considerably above the main body of lowercase letters if its vertical coordinate y fulfils the condition:</p><formula xml:id="formula_3">y &gt; 2y u -y l .</formula><p>The shortest valid link is chosen as the best "ink" line connecting two components. Links connecting "ink" regions are highlighted in red in Fig. <ref type="figure" target="#fig_2">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(fourth column).</head><p>It should be noted that the above procedure does not necessarily need to connect components in exactly the same manner as a human would. First of all, the connection is typically made within the bounding box of lowercase letters and its shape will contribute little discriminatory information compared to the descenders and ascenders, i.e., differences in connection will affect only lower scales of the multiscale representation. Secondly, a word connected in a nonintuitive way should still be recognized if there is at least one example in the annotated collection connected in the same way. Moreover, our observations indicate that, even if the disconnection occurs within parts of a letter the above procedure is able to connect components in a reasonable manner allowing correct recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Contour tracing</head><p>Once all components are connected with the links, the final binary mask for the word can be created -see examples from the fifth column in Fig. <ref type="figure" target="#fig_2">5</ref>. This is followed by a contour tracing procedure which extracts a single ordered contour for each word -see examples from the last column in Fig. <ref type="figure" target="#fig_2">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Contour matching</head><p>In our approach, similarities between word contours are measured using the contour matching technique proposed originally for comparing general shapes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. In <ref type="bibr" target="#b1">[2]</ref>, we introduced a rich shape description method, termed the multiscale convexity concavity (MCC) representation. In this representation, information about the amount of convexity/concavity at different scale levels is stored for each contour point (Fig. <ref type="figure" target="#fig_4">6</ref>). A DTW technique <ref type="bibr" target="#b23">[24]</ref> is used to find an optimal refined alignment along the contours upon which the dissimilarity measure is defined. The approach is robust to several transformations including translation, scaling, rotation, modest occlusion, and symmetric transformation. The method performs particularly well in cases of elastic deformations and where the similarity between curves is weak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MCC representation</head><p>The MCC representation stores information about the amount of convexity/concavity at different scale levels for each contour point. The representation takes form of a 2D matrix where the rows correspond to the different scale levels σ and the columns correspond to contour points (parameter u). Position (σ , u) contains information about the degree of convexity or concavity for the uth contour point at scale level σ -see example in Fig. <ref type="figure" target="#fig_4">6c</ref>.</p><p>The simplified contours at different scale levels are obtained via a curve evolution process similar to that used in <ref type="bibr" target="#b24">[25]</ref>  </p><formula xml:id="formula_4">x σ (u) = x(t)φ σ (u -t)dt, (<label>2</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">φ σ (u) = 1 √ 2πσ 2 e -u 2 /2σ 2<label>(3)</label></formula><p>and similarly for y(u).</p><p>The resulting contour, C σ , becomes smoother with increasing value of σ , until finally the contour is convex. The convexity/concavity measure is defined as the displacement of the contour between two consecutive scale levels. Convex and concave parts are distinguished via a change in sign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching</head><p>The similarity/dissimilarity measure between two contours is based on local distances between corresponding points (computed using their multiscale feature vectors). The correspondence between points is established using well-known DTW <ref type="bibr" target="#b23">[24]</ref> technique.</p><p>When establishing correspondences between two contours, first distances between their contour points are computed using their multiscale feature vectors and stored in an N × N distance table allowing their convenient examination. Each entry in the table stores a distance between a pair of contour points corresponding to the row and the column of this entry. The distances between contour points of both contours are computed in a following way. Define the MCC representation as an σ max × N matrix F = [f σ u ] where f σ u denotes the convexity/concavity measure for contour point u at scale σ . Let contours A and B be represented by F A and F B , respectively. Pairwise distance between contour points u A and u B from A and B, respectively, is defined as weighted sum of absolute differences between their convexity/concavity measure from all scales:</p><formula xml:id="formula_7">d(u A , u B ) = σ max σ =1 p σ • |f A σ u A -f B σ u B | r A σ + r B σ (4)</formula><p>where r σ denotes dynamic range of concavity/convexity measures at scale σ defined as</p><formula xml:id="formula_8">r σ = max u {f σ u } -min u {f σ u }<label>(5)</label></formula><p>Weights p σ control the relative proportions between distances computed using different scales. Optimization of these weights is discussed in Sect. <ref type="bibr">3.3</ref> Finding the optimal match between two contour representations corresponds to finding the lowest cost diagonal path through the table containing pairwise distances between contour point features from both words -see example in Fig. <ref type="figure" target="#fig_5">7</ref>. The optimal path has to begin and end at the cell corresponding to the alignment of the two starting contour points under consideration. In our original approach all possible combinations of starting points are examined to ensure an optimal match. Deviations of the path from a straight diagonal path compensates for elastic deformations between contours, i.e., stretching and shrinking of parts. The detailed DP formulation of the algorithm can be found in <ref type="bibr" target="#b1">[2]</ref>.</p><p>The final dissimilarity between two contours is calculated by normalizing the cost of the optimal path (D min ) found by the matching algorithm by the number of contour points used in the MCC representations: It should be stressed that there is no extra penalty for stretching and shrinking of contour parts during the matching process. Rather, a globally optimal match between two contours is found using a rich multiscale feature for each contour point and dissimilarity is based solely on distances between corresponding points. This is contrary to the majority of other contour matching approaches based on DP where different penalty factors for stretching and shrinking, usually chosen in an ad hoc fashion, drive the matching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MCC-DCT representation</head><p>Although the original MCC representation obtained very encouraging retrieval results <ref type="bibr" target="#b1">[2]</ref>, the adaptation of the weights p σ proven to be difficult due to high correlation between information from different scales. In <ref type="bibr" target="#b22">[23]</ref>, we proposed an alternative version of the MCC representation, termed MCC-DCT, where a 1D discrete cosine transform (DCT) is applied to each multiscale contour point feature vector de-correlating information from different scale levels and placing most of the energy in low frequency coefficients -see example in Fig. <ref type="figure" target="#fig_4">6d</ref>. In addition, we proposed an iterative optimization framework for determining the relative proportions in which information from different DCT coefficients should be combined in the final similarity measure. We showed that such optimization can further improve matching performance for a particular application. The MCC-DCT representation and the optimized matching obtained improved retrieval performance in collections containing general shapes.</p><p>For the purpose of this work, we used the MCC-DCT version of the contour representation. The match-ing procedure was optimized using the shape collection from the MPEG-7 Core Experiment "CE-Shape-1" (part B) <ref type="bibr" target="#b25">[26]</ref>. It should be noted that this dataset contains general shapes and does not contain any examples of word contours. In theory, the matching could be optimized using the annotated collection of handwritten manuscripts, however, this possibility could not be explored in this work due to the lack of a suitable training collection.</p><p>An example of matching word contours using the above method is shown in Fig. <ref type="figure" target="#fig_5">7</ref>. A more detailed description of the MCC representation and the associated matching algorithm can be found in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall results</head><p>For our experiments we used a set of 20 pages from the George Washington collection at the Library of Congress. These contain 4,856 word occurrences of 1,187 unique words. The collection was accurately segmented into words using the algorithm described in <ref type="bibr" target="#b17">[18]</ref>. Ground-truth annotations for the word images were created manually <ref type="bibr" target="#b0">[1]</ref>.</p><p>The contours for all words were extracted according to the procedure described in Sect. 2. Then, for each word contour the MCC-DCT descriptor with 100 equally spaced contour points, each with a feature vector consisting of ten DCT coefficients (σ max = 10), was extracted and stored.</p><p>Each of the 4,856 word occurrences was used as a query and its contour representation was matched against word representations from all manuscript pages except the page with the query word. A 1-nearest neighbor method was used to classify the query word and the classification result was compared against manual annotation. It should be noted that words from the same manuscript page as the query are excluded from matching for comparability reasons with the results reported in <ref type="bibr" target="#b0">[1]</ref>.</p><p>The results are measured in terms of average word error rate (WER). To separate out-of-vocabulary (OOV) errors from mismatches we report two types of WER, one that includes OOV words and one that omits them from the evaluation. In other words, WER that includes OOV errors is the average rate of all incorrectly classified query words -irrespectively of the fact that the error was attributed to the performance of the recognition algorithm or simply there was no such word in the searched collection (outside the manuscript page containing the query word). On the other hand, WER that excludes OOV errors is the average rate of all incorrectly classified words for all queries which had at least one instance of the word in the searched collection (outside the manuscript page containing the query word). Therefore, WER that excludes OOV errors will characterize solely the performance of the used recognition method while WER that includes OOV errors characterize the overall performance of the recognition system which depends also on the size of the annotated collection.</p><p>The lowest WER reported until now in the literature for the George Washington collection was 0.449 when OOV words were included and 0.349 when OOV words were excluded <ref type="bibr" target="#b0">[1]</ref>. It should be noted that this result was obtained after utilizing additional resources to train a statistical language model. Using a 27-dimensional feature vector representation of each word, without any language model postprocessing yielded a WER of 0.603 when OOV words are included and 0.531 when OOV words are excluded. In comparison our method obtained an average WER of 0.306 with OOV words and 0.174 without OOV words. This represents approximately 50% reduction of the non-OOV error rate obtained by our system despite the fact that it is based on contour mapping only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Towards fast recognition</head><p>The contour matching technique described in Sect. 3 was proposed originally for comparing general shapes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. Although the method can be used without any changes, in this section we demonstrate possibilities for further adaptation to the specific task of word matching in order to improve performance and reduce computational load. In addition, a pruning technique for discarding unlikely matches is proposed. All experiments presented in this section are performed using a standard PC with 1.6 GHz Pentium 4 processor. For clarity, only WER without OOV words is considered. Note that the goal of the experiments is to demonstrate the influence of various parameter settings on speed and recognition performance and not a proposal for the final tuning of the algorithm. We are aware the latter case would require use of two collections, one for tuning and one for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Modifications to the matching algorithm</head><p>When matching two shapes, their relative rotation and therefore the circular shift between their contour representations is generally unknown. Hence, all circular shifts have to be examined to ensure an optimal match, implying a total complexity of O(N 3 ), where N is the number of contour points used. In the case of word matching, we can locate a single starting point for each contour in such a way that its position along the contour is consistent among different instances of the same word. Close to optimal matching can then be performed by examining only one circular shift corresponding to the alignment of starting points and therefore reducing the matching complexity to O(N 2 ). We propose to locate the starting point from the part of the contour corresponding to the end of the word, rather than the beginning, due to better shape consistency at this point. The point is located by scanning the main body of lowercase letters and searching for the first "ink" pixel. The scanning starts at the right bottom and is performed from right to left and from bottom to top as shown in Fig. <ref type="figure" target="#fig_6">8</ref>. Utilizing the location of starting points, reduced the average matching time for two words from 6 ms to less than 289 µs whilst maintaining a low average WER of 0.182 (without OOV words).</p><p>In the case of the matching algorithm used, finding the optimal match corresponds to finding the lowest cost diagonal path through the table containing pairwise distances between contour points from both words -as illustrated in Fig. <ref type="figure" target="#fig_5">7</ref>. The path has to begin and end at the cell corresponding to the alignment of the two starting contour points. Deviations of the path from a straight diagonal path compensate for inter-character and intracharacter spacing variations.</p><p>It is common practice in the case of DTW techniques <ref type="bibr" target="#b23">[24]</ref> to restrict the path to lie in the area close to the ideal straight diagonal line in order to speed up the matching process. In our original approach, the path is allowed to deviate from the straight diagonal path by no more than a predefined deviation threshold ±T d N, where T d = 0.04 [reducing the complexity to O(2T d N 2 )]. In most cases when matching general shapes, this restriction imposed on the path's geometry had no affect on the optimal path. In order to investigate the influence of this restriction on the word recognition rate we plotted WER against different values of the parameter T d together with corresponding times required for matching two word contours -see Fig. <ref type="figure">9</ref>. From the plot we can observe that constraining the path to the diagonal Fig. <ref type="figure">9</ref> Word error rate (excluding OOV) as a function of allowed path deviation T d (starting points utilized). Labels at each data point denote times required for matching two word contours for a given value of T d line would result in WER above 0.230 which confirms the requirement of elastic matching. The experiment also verified the linear dependency between T d and computational load of the matching. For the remainder of this article we adopt T d = 0.08 as a good tradeoff between recognition efficiency and matching speed. Although, this setting increases the average matching time to 532 µs whilst the average WER without OOV words is further reduced to 0.165.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Pruning unlikely matches</head><p>Pruning techniques can be used to quickly discard unlikely matches by requiring word images to have similar statistics <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b26">[27]</ref>, pruning of word pairs was performed based on the area and aspect ratio of their bounding boxes. This idea was further extended in <ref type="bibr" target="#b27">[28]</ref> by the additional requirement of two words to have the same number of descenders (strokes below the baseline). In our approach, the pruning statistics are extracted directly from the word contours. Pruning is performed based on contour complexity and the number of descenders and ascenders. For clarity, each pruning rule is first discussed independently and only the best settings for each of the rules is used in the final combined pruning criterion.</p><p>The shape complexity x i of a word contour C i can be defined as the ratio between its perimeter length l i and the square root of its area a i : x i = l i / √ a i . Both can be easily estimated directly from the word contour <ref type="bibr" target="#b28">[29]</ref>. The following rule was used to discard unlikely matches <ref type="bibr" target="#b29">[30]</ref>:  Clearly, increasing τ x in Rule 1 allows for more matches to be processed by the matching algorithm (reducing recognition speed) and potentially leads to lower values of average WER. Table <ref type="table" target="#tab_0">1</ref> demonstrates empirically the effect of pruning using different values of threshold τ x on WER and number of pruned pairs. The results show that setting τ x = 0.3 would prune almost half of the total number of word pairs, which would otherwise have to be processed by the matching algorithm, while maintaining a low average WER of 0.165.</p><p>Word's descenders and ascenders are identified by traversing their contours and labeling contour points as belonging to a descender or an ascender based on the comparison between their vertical positions with the vertical limits of the body of lowercase letters -(see Fig. <ref type="figure">10</ref>). Final numbers of ascenders and descenders are found by counting the number of continuous sequences of points marked as ascenders or descenders. It should be noted that it may not be important whether or not the "real" ascenders and descenders are detected. Reliable pruning should be possible as long as the handwriting style is consistent, i.e., the identification (or missing) of ascenders and descenders is performed in a consistent way. The thresholds τ desc and τ asc control the tolerance for the absolute differences between numbers of descenders and ascenders when pruning word pairs and should have only integer values. Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref> demonstrate the effects of pruning on the average WER and the number of pruned pairs using rules 2 and 3, respectively. Table <ref type="table" target="#tab_1">2</ref> shows that the number of descenders provides very reliable evidence for identifying unlikely matches. Specifically, τ desc = 0 would lead to early discarding of 50% of word pairs without noticeable difference in average WER. In contrast, pruning based on the number of ascenders (Table <ref type="table" target="#tab_2">3</ref>) is less reliable most likely due to the generally more complex shape of the upper parts of the words. Therefore, to ensure that only small proportion of valid matches will be discarded by the Rule 3 we have to allow certain tolerance, e.g., τ asc = 1 would result in WER of 0.164.</p><p>In the last experiment all three pruning rules were combined:</p><p>Rule 4: Two word contours C i and C j are similar only if Rules 1-3 are satisfied.</p><p>Incorporating the above rule and setting τ x = 0.2, τ desc = 0, and τ asc = 1 led to discarding 85% of total matches while maintaining a low average WER of 0.183 (excluding OOV words).</p><p>In summary, the relatively minor adaptation of the matching algorithm and the incorporation of the pruning technique could reduce the average time required to recognize a single word to 0.4 s (for the particular size of the annotated collection) whilst maintaining a low average WER. This corresponds roughly to a 70-fold increase in speed compared to the original approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with CSS</head><p>In the past, different shape matching techniques (typically not requiring parameterization of word contours) have been tested for their potential capabilities to match words. For example, Rath and Manmatha <ref type="bibr" target="#b5">[6]</ref> demonstrated that the shape context approach <ref type="bibr" target="#b10">[11]</ref>, which is currently the best classifier for handwritten digits, performs poorly in terms of recognition precision and speed for word matching.</p><p>The contour extraction procedure presented in Sect. 2 opened a new possibility of utilizing matching techniques requiring ordering of the contour points for the purpose of word matching. In this section, we evaluate the performance of the curvature scale space (CSS) approach <ref type="bibr" target="#b24">[25]</ref>, which was adopted as contour-based shape descriptor by the ISO/IEC MPEG-7 standard. The CSS descriptor is very compact, allows fast matching, and extensive tests using general shape collections <ref type="bibr" target="#b30">[31]</ref> revealed that the method is quite robust with respect to noise, scale, and orientation changes of objects. In the experiment, the CSS extraction and matching was performed using the MPEG-7 eXperimentation Model (XM software v5.6) <ref type="bibr" target="#b31">[32]</ref>.</p><p>To facilitate comparison of the three approaches, their results are collected in Table <ref type="table" target="#tab_3">4</ref>. Adopting the CSS technique for recognition of words from the George Washington collection resulted in relatively high average WER of 0.350 (excluding OOV words). Such poor performance can be explained by two major drawbacks of the CSS technique: the occurrence of ambiguity with regard to concave segments and its inability to represent convex segments. This result confirms the need for utilization of a rich shape descriptor, as discussed in Sect. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future work</head><p>An obvious improvement to further reduce WER would be the optimization of the similarity measure using the framework proposed in <ref type="bibr" target="#b22">[23]</ref> in conjunction with a suitable training collection of handwritten words. Furthermore, currently all contours from the database are re-scaled to a predefined size prior to the extraction of the MCC representations. In the case of word match-  <ref type="bibr" target="#b24">[25]</ref> 0.350 ing, the x-height should be sufficient for size alignment.</p><p>Replacing the size invariance property with x-height invariance could further improve discrimination between words. Incorporation of additional features in order to capture the "inner" structure of a word and dots, e.g., size and centroid of the inner holes <ref type="bibr" target="#b9">[10]</ref>, should also be investigated. The number of contour matchings necessary to classify a single word could be further reduced by developing an appropriate indexing strategy for a training dictionary. We would like to investigate the possibility of reducing WER caused by OOV words by augmenting the training dictionary by synthesizing new word contours based on the initial training set. In theory, this could significantly reduce OOV errors. Finally, it should be stressed that word matching is just a first step in word recognition. Incorporating a statistical language model as a postprocess could further improve the recognition accuracy of the system as was shown in <ref type="bibr" target="#b0">[1]</ref>. Also, it would be interesting to compare different binarization methods discussed recently in <ref type="bibr" target="#b21">[22]</ref> with the variant of Niblack's method used in this work.</p><p>In the future we plan to incorporate this algorithm into a complete indexing system for large collections of handwritten historical documents, such as those used for experimentation purposes here, but also illuminated Gaelic manuscripts. Furthermore, in the latter case, a specific challenge we intend to investigate is the differentiation of multiple scribes within a single document based on characterizing the shapes of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this article, a new method for word recognition for historical manuscripts has been proposed. Extensive experimentation conducted using the George Washington collection shows that systems based on word contour matching can significantly outperform existing techniques. Specifically, it has been shown that on a set of 20 pages, the average recognition accuracy was 83%. Adaptation of the contour matching to the specific task of word recognition in order to improve performance and reduce computational load together with a simple pruning technique was also discussed. Moreover, preliminary investi-gation of potential simplifications allows us to speculate that additional development would further improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 Fig. 2 Fig. 3</head><label>123</label><figDesc>Fig. 1 Manuscript sample (2770277)</figDesc><graphic coords="4,203.26,56.79,340.35,126.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Position estimation</figDesc><graphic coords="5,200.76,56.96,340.35,169.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig.<ref type="bibr" target="#b4">5</ref> Intermediate results of the contour extraction process (columns in order from left to right): input gray-scale image, "ink"/"paper" identification by applying dynamic threshold, localization of the main body of lowercase letters and removal of margin line residual, connected components alg.(gray levels) and best connecting links (red lines), binary mask, word contour</figDesc><graphic coords="6,203.26,56.51,340.35,309.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to extract CSS images. Let us assume a size normalized closed contour C represented by N contour points and parameterized by arc length u: C(u) = x(u), y(u) , where u ∈ 0, N . The coordinate functions of C are convolved with a Gaussian kernel φ σ of width σ ∈ {1, 2 • • • σ max }:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Extraction of MCC representation</figDesc><graphic coords="7,113.14,57.27,368.67,369.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Matching example</figDesc><graphic coords="8,53.58,56.91,232.83,189.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Location of starting point</figDesc><graphic coords="9,340.76,56.91,170.43,81.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 1 :</head><label>101</label><figDesc>Fig. 10 Identification of descenders and ascenders</figDesc><graphic coords="10,308.70,202.97,232.83,126.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Effect of pruning based on global contour complexity</figDesc><table><row><cell>τ x</cell><cell>No. of Pruned pairs No. of total pairs</cell><cell>(%)</cell><cell>WER (excluding OOV words)</cell></row><row><cell cols="2">0.1 80</cell><cell></cell><cell>0.215</cell></row><row><cell cols="2">0.2 63</cell><cell></cell><cell>0.170</cell></row><row><cell cols="2">0.3 48</cell><cell></cell><cell>0.165</cell></row><row><cell cols="2">0.4 37</cell><cell></cell><cell>0.165</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell></cell><cell>. . .</cell></row><row><cell cols="2">∞ 0</cell><cell></cell><cell>0.165</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Effect of pruning based on number of descenders</figDesc><table><row><cell>τ desc</cell><cell>No. of pruned pairs No. of total pairs</cell><cell>[%]</cell><cell>WER (excluding OOV words)</cell></row><row><cell>0</cell><cell>50</cell><cell></cell><cell>0.180</cell></row><row><cell>1</cell><cell>9</cell><cell></cell><cell>0.164</cell></row><row><cell>2</cell><cell>2</cell><cell></cell><cell>0.165</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell></cell><cell>. . .</cell></row><row><cell>∞</cell><cell>0</cell><cell></cell><cell>0.165</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Effect of pruning based on number of ascenders Let DESC i and ASC i be the estimated number of descenders and ascenders of word contour C i . The following two rules are used to discard unlikely matches: Rule 2: Two word contours C i and C j are similar only if |DESC i -DESC j | ≤ τ desc . Rule 3: Two word contours C i and C j are similar only if |ASC i -ASC j | ≤ τ asc .</figDesc><table><row><cell>τ asc</cell><cell>No. of pruned pairs No. of total pairs</cell><cell>[%]</cell><cell>WER (excluding OOV words)</cell></row><row><cell>0</cell><cell>73</cell><cell></cell><cell>0.204</cell></row><row><cell>1</cell><cell>30</cell><cell></cell><cell>0.164</cell></row><row><cell>2</cell><cell>9</cell><cell></cell><cell>0.165</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell></cell><cell>. . .</cell></row><row><cell cols="2">∞ 0</cell><cell></cell><cell>0.165</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>WER (excluding OOV) words for the</cell><cell>Method</cell></row><row><cell>discussed methods</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Baseline -imaginary line upon which a line of text rests.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>x-height -distance between the baseline and tops of the main body of lower case letters (see Fig.4).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the Center for Intelligent Information Retrieval from University of Massachusetts for providing the annotated collection of George Washingtons manuscripts.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Holistic word recognition for handwritten historical documents</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Document Image Analysis for Libraries (DIAL&apos;04)</title>
		<meeting>Document Image Analysis for Libraries (DIAL&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multiscale representation method for non-rigid shapes with a single closed contour</title>
		<author>
			<persName><forename type="first">T</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol., special issue on Audio and Video Analysis for Multimedia Interactive Services</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A search engine for historical manuscript images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIGIR Conference (SIGIR&apos;04)</title>
		<meeting>SIGIR Conference (SIGIR&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004-07">July (2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transcript mapping for historic handwritten document images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Tomai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 8th International Workshop on Frontiers in Handwriting Recognition</title>
		<meeting>8th International Workshop on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2002-08">August. 2002</date>
			<biblScope unit="page" from="413" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Features for word spotting in historical manuscripts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference IC-DAR&apos;03</title>
		<meeting>Conference IC-DAR&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="218" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word image matching using dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference. CVPR&apos;03</title>
		<meeting>Conference. CVPR&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document analysis systems vi</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings 6th International Workshop DAS&apos;04</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Marinai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</editor>
		<meeting>6th International Workshop DAS&apos;04<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Sept. Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic retrieval of ocr degraded text using n-grams</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st European Conference. on Research and Advanced Technology for Digital Libraries</title>
		<meeting>the 1st European Conference. on Research and Advanced Technology for Digital Libraries<address><addrLine>Pisa, Italy, Sept</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The state of the art in on-line handwriting recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Teppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wakahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="787" to="808" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognition of handwritten digits based on contour information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="255" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Character model word recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Favata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Workshop on Frontiers in Handwriting Recognition</title>
		<meeting><address><addrLine>Essex, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">Sept. 1996. 1996</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The role of holistic paradigms in handwritten word recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madhvanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="164" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Word-level recognition of cursive script</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. C</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="175" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Handwritten word recognition-the approach proved by practice</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dzuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Filatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gershuny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop Frontiers in Handwriting Recognition</title>
		<meeting>the Sixth International Workshop Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chaincode contour processing for handwritten word recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madhvanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="928" to="932" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Holistic verification of handwritten phrases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madhvanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1344" to="1356" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scale space technique for word segmentation in handwritten manuscripts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srimal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Scale-Space. Theories in Computer Vision</title>
		<meeting>the 2nd International Conference on Scale-Space. Theories in Computer Vision<address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">Sept. 1999. 1999</date>
			<biblScope unit="page" from="22" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<title level="m">An Introduction to Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive document binarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sauvola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seppänen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haapakoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Document Analysis and Recognition</title>
		<meeting>International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Morphological grayscale reconstruction in image analysis: applications and efficient algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="201" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparison of binarization methods for historical archive documents</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Downton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Document Analysis and Recognition (ICDAR&apos;2005)</title>
		<meeting>the 7th International Conference on Document Analysis and Recognition (ICDAR&apos;2005)<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08">Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-scale representation and optimal matching of non-rigid shapes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Content-Based Multimedia Indexing, CBMI&apos;05</title>
		<meeting>the 4th International Workshop on Content-Based Multimedia Indexing, CBMI&apos;05<address><addrLine>Riga, Latvia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">June (2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance tradeoff in dynamic time warping algorithms for isolated word recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theory of multiscale, curvature-based shape representation for planar curves</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mackworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="789" to="805" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Description of core experiments for MPEG-7 motion/shape</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeannin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bober</surname></persName>
		</author>
		<idno>MPEG-7</idno>
		<ptr target="ISO/IEC/JTC1/SC29/WG11/MPEG99/N2690" />
		<imprint>
			<date type="published" when="1999-03">March (1999</date>
			<publisher>Seoul</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word spotting: Indexing handwritten archives</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intelligent Multimedia Information Retrieval Collection</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting>the Intelligent Multimedia Information Retrieval Collection</meeting>
		<imprint>
			<publisher>AAAI/MIT Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Indexing George Washington&apos;s handwritten manuscripts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Partridge</surname></persName>
		</author>
		<idno>MM-34</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Center for Intelligent Information Retrieval, University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep., Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient contour-based shape representation and matching</title>
		<author>
			<persName><forename type="first">T</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM SIGMM International Workshop on Multimedia Information Retrieval (MIR&apos;03)</title>
		<meeting>the 5th ACM SIGMM International Workshop on Multimedia Information Retrieval (MIR&apos;03)<address><addrLine>Berkeley, CA, Nov</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient shape matching through model-based shape recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="215" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shape descriptors for non-rigid shapes with a single closed contour</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lakämper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Eckhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference. On Computer Vision and Pattern Recognition (CVPR&apos;00)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="424" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="www.lis.ei.tum.de/research/bv/topics/mmdb/e_mpeg7.html" />
		<title level="m">ISO/IEC MPEG7 eXperimentation model (XM software)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
