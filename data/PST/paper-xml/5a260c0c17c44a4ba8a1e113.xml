<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Protein Interface Prediction using Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
							<email>fout@colostate.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
							<email>jonbyrd@colostate.edu</email>
						</author>
						<author>
							<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Colorado State University Fort Collins</orgName>
								<address>
									<postCode>80525</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Colorado State University Fort Collins</orgName>
								<address>
									<postCode>80525</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Colorado State University Fort Collins</orgName>
								<address>
									<postCode>80525</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Colorado State University Fort Collins</orgName>
								<address>
									<postCode>80525</postCode>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Protein Interface Prediction using Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the prediction of interfaces between proteins, a challenging problem with important applications in drug discovery and design, and examine the performance of existing and newly proposed spatial graph convolution operators for this task. By performing convolution over a local neighborhood of a node of interest, we are able to stack multiple layers of convolution and learn effective latent representations that integrate information across the graph that represent the three dimensional structure of a protein of interest. An architecture that combines the learned features across pairs of proteins is then used to classify pairs of amino acid residues as part of an interface or not. In our experiments, several graph convolution operators yielded accuracy that is better than the state-of-the-art SVM method in this task. † denotes equal contribution 31st Conference on Neural Information Processing Systems (NIPS 2017),</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many machine learning tasks we are faced with structured objects that can naturally be modeled as graphs. Examples include the analysis of social networks, molecular structures, knowledge graphs, and computer graphics to name a few. The remarkable success of deep neural networks in a wide range of challenging machine learning tasks from computer vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and speech recognition <ref type="bibr" target="#b11">[12]</ref> to machine translation <ref type="bibr" target="#b23">[24]</ref> and computational biology <ref type="bibr" target="#b3">[4]</ref>, has resulted in a resurgence of interest in this area. This success has also led to the more recent interest in generalizing the standard notion of convolution over a regular grid representing a sequence or an image, to convolution over graph structures, making these techniques applicable to the wide range of prediction problems that can be modeled in this way <ref type="bibr" target="#b7">[8]</ref>.</p><p>In this work we propose a graph convolution approach that allows us to tackle the challenging problem of predicting protein interfaces. Proteins are chains of amino acid residues that fold into a three dimensional structure that gives them their biochemical function. Proteins perform their function through a complex network of interactions with other proteins. The prediction of those interactions, and the interfaces through which they occur, are important and challenging problems that have attracted much attention <ref type="bibr" target="#b9">[10]</ref>. This paper focuses on predicting protein interfaces. Despite the plethora of available methods for interface prediction, it has been recently noted that "The field in its current state appears to be saturated. This calls for new methodologies or sources of information to be exploited" <ref type="bibr" target="#b9">[10]</ref>. Most machine learning methods for interface prediction use hand-crafted features that come from the domain expert's insight on quantities that are likely to be useful and use standard machine learning approaches. Commonly used features for this task include surface accessibility, sequence conservation, residue properties such as hydrophobicity and charge, and various shape descriptors (see Aumentado et al. <ref type="bibr" target="#b5">[6]</ref> for a review of the most commonly used features for this task).</p><p>The task of object recognition in images has similarities to interface prediction: Images are represented as feature values on a 2D grid, whereas the the solved crystal structure of a protein can be thought of as a collection of features on an irregular 3D grid corresponding to the coordinates of its atoms. In both cases, we are trying to recognize an object within a larger context. This suggests that approaches that have proven successful in image classification can be adapted to work for protein structures, and has motivated us to explore the generalization of the convolution operator to graph data. In fact, several techniques from computer vision have found their way into the analysis of protein structures, especially methods for locally describing the shape of an object, and various spectral representations of shape (see e.g. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>).</p><p>In this work we evaluate multiple existing and proposed graph convolution operators and propose an architecture for the task of predicting interfaces between pairs of proteins using a graph representation of the underlying protein structure. Our results demonstrate that this approach provides state-of-theart accuracy, outperforming a recent SVM-based approach <ref type="bibr" target="#b1">[2]</ref>. The proposed convolution operators are not specific to interface prediction. They are applicable to graphs with arbitrary size and structure, do not require imposing an ordering on the nodes, allow for representing both node and edge features, and maintain the original graph structure, allowing multiple convolution operations without the need to downsample the graph. Therefore we expect it to be applicable to a variety of other learning problems on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods for Graph Convolution</head><p>In this work we consider learning problems over a collection of graphs where prediction occurs at the node level. Nodes and edges have features that are associated with them, and we denote by x i the feature vector associated with node i and A ij the feature vector associated with the edge between nodes i and j, where for simplicity we have omitted indexing over graphs.</p><p>We describe a framework that allows us to learn a representation of a local neighborhood around each node in a graph. In the domains of image, audio, or text data, convolutional networks learn local features by assigning an ordering to pixels, amplitudes, or words based on the structure inherent to the domain, and associating a weight vector/matrix with each position within a receptive field. The standard notion of convolution over a sequence (1D convolution) or an image (2D convolution) relies on having a regular grid with a well-defined neighborhood at each position in the grid, where each neighbor has a well-defined relationship to its neighbors, e.g. "above", "below", "to the left", "to the right" in the case of a 2D grid. On a graph structure there is usually no natural choice for an ordering of the neighbors of a node. Our objective is to design convolution operators that can be applied to graphs without a regular structure, and without imposing a particular order on the neighbors of a given node. To summarize, we would like to learn a mapping at each node in the graph which has the form: z i = σ W (x i , {x n1 , . . . , x n k }), where {n 1 , . . . , n k } are the neighbors of node i that define the receptive field of the convolution, σ is a non-linear activation function, and W are its learned parameters; the dependence on the neighboring nodes as a set represents our intention to learn a function that is order-independent. We present the following two realizations of this operator that provides the output of a set of filters in a neighborhood of a node of interest that we refer to as the "center node":</p><formula xml:id="formula_0">z i = σ W C x i + 1 |N i | j∈Ni W N x j + b ,<label>(1)</label></formula><p>where N i is the set of neighbors of node i, W C is the weight matrix associated with the center node, W N is the weight matrix associated with neighboring nodes, and b is a vector of biases, one for each filter. The dimensionality of the weight matrices is determined by the dimensionality of the inputs and the number of filters. The computational complexity of this operator on a graph with n nodes, a neighborhood of a node is the set of neighboring nodes in the protein structure; each node has features computed from its amino acid sequence and structure, and edges have features describing the relative distance and angle between residues. Right: Schematic description of the convolution operator which has as its receptive field a set of neighboring residues, and produces an activation which is associated with the center residue. neighborhood of size k, F in input features and F out output features is O(kF in F out n). Construction of the neighborhood is straightforward using a preprocessing step that takes O(n 2 log n).</p><p>In order to provide for some differentiation between neighbors, we incorporate features on the edges between each neighbor and the center node as follows:</p><formula xml:id="formula_1">z i = σ W C x i + 1 |N i | j∈Ni W N x j + 1 |N i | j∈Ni W E A ij + b ,<label>(2)</label></formula><p>where W E is the weight matrix associated with edge features.</p><p>For comparison with order-independent methods we propose an order-dependent method, where order is determined by distance from the center node. In this method each neighbor has unique weight matrices for nodes and edges:</p><formula xml:id="formula_2">z i = σ W C x i + 1 |N i | j∈Ni W N j x j + 1 |N i | j∈Ni W E j A ij + b .<label>(3)</label></formula><p>Here W N j /W E j are the weight matrices associated with the j th node or the edges connecting to the j th nodes, respectively. This operator is inspired by the PATCHY-SAN method of Niepert et al. <ref type="bibr" target="#b15">[16]</ref>. It is more flexible than the order-independent convolutional operators, allowing the learning of distinctions between neighbors at the cost of significantly more parameters.</p><p>Multiple layers of these graph convolution operators can be used, and this will have the effect of learning features that characterize the graph at increasing levels of abstraction, and will also allow information to propagate through the graph, thereby integrating information across regions of increasing size. Furthermore, these operators are rotation-invariant if the features have this property.</p><p>In convolutional networks, inputs are often downsampled based on the size and stride of the receptive field. It is also common to use pooling to further reduce the size of the input. Our graph operators on the other hand maintain the structure of the graph, which is necessary for the protein interface prediction problem, where we classify pairs of nodes from different graphs, rather than entire graphs. Using convolutional architectures that use only convolutional layers without downsampling is common practice in the area of graph convolutional networks, especially if classification is performed at the node or edge level. This practice has support from the success of networks without pooling layers in the realm of object recognition <ref type="bibr" target="#b22">[23]</ref>. The downside of not downsampling is higher memory and computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work.</head><p>Several authors have recently proposed graph convolutional operators that generalize the notion of convolution over a regular grid. Spectral graph theory forms the basis for several of these methods <ref type="bibr" target="#b7">[8]</ref>, in which convolutional filters are viewed as linear operators on the eigenvectors of the graph Laplacian (or an approximation thereof <ref type="bibr" target="#b12">[13]</ref>). Our protein dataset consists of multiple graphs with no natural correspondence to each other, making it difficult to apply methods based on the graph Laplacian. In what follows we describe several existing spatial graph convolutional methods, remarking on the aspects which resemble or helped inspire our implementation.</p><p>In their Molecular Fingerprint Networks (MFNs), Duvenaud et al. <ref type="bibr" target="#b8">[9]</ref> proposed a spatial graph convolution approach similar to Equation ( <ref type="formula" target="#formula_0">1</ref>), except that they use a single weight matrix for all nodes in a receptive field and sum the results, whereas we distinguish between the center node and the neighboring nodes, and we average over neighbors rather than sum over them. Furthermore, their graphs do not contain edge features, so their convolution operator does not make use of them. MFNs were designed to generate a feature representation of an entire molecule. In contrast, our node level prediction task motivates distinguishing between the center node, whose representation is being computed, and neighboring nodes, which provide information about the local environment of the node. Averaging is important in our problem to allow for any size of neighborhood.</p><p>Schlichtkrull et al. <ref type="bibr" target="#b18">[19]</ref> describe Relational Graph Convolutional Networks (RGCNs), which consider graphs with a large number of binary edge types, where a unique neighborhood is defined by each edge type. To reduce the total number of model parameters, they employ basis matrices or block diagonal constraints to introduce shared parameters between the representations of different edge/neighborhood types. That aspect of the method is not relevant to our problem, and without it, Equation (1) closely resembles their convolution operator. Schütt et al. <ref type="bibr" target="#b20">[21]</ref> define Deep Tensor Neural Networks (DTNNs) for predicting molecular energies. This version of graph convolution uses the node and edge information from neighbors to produce an additive update to the center node:</p><formula xml:id="formula_3">z i = x i + 1 |N i | j∈Ni σ W (W N x j + b N ) (W E A ij + b E ) ,<label>(4)</label></formula><p>where denotes the elementwise product, W , W N , and W E are weights matrices, and b N and b E are bias vectors. Edge information is incorporated similarly to Equation (2), with the difference in how the edge and node signals are combined-their choice being elementwise product rather than sum. Another difference is that DTNN convolution forces the output of a layer to have the same dimensionality as its input; our approach does not require that, allowing the networks to have varying numbers of filters across convolutional layers.</p><p>Rather than operate on fixed neighborhoods, Atwood and Towsley <ref type="bibr" target="#b4">[5]</ref> take a different spatial convolution approach in their Diffusion-Convolutional Neural Networks (DCNNs), and apply multiple steps (or "hops") of a diffusion operator that propagates the value of an individual feature across the graph. A node after k hops will contain information from all nodes that have walks of length k ending at that node. If X is a data matrix where each row corresponds to a node, and each column to a different feature, then the representation of X after a k hop convolution is:</p><formula xml:id="formula_4">Z k = σ(w k P k X),<label>(5)</label></formula><p>where w k is the k-hop vector of weights, and P k is the transition matrix raised to power k. Rather than stack multiple convolution layers, the authors apply the diffusion operator using multiple hop numbers. In our work we use this method with an adjacency matrix whose entries are an exponentially decreasing function of the distance between nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proteins as graphs.</head><p>In this work we represent a protein as a graph where each amino acid residue is a node whose features represent the properties of the residue; the spatial relationships between residues (distances, angles) are represented as features of the edges that connect them (see Figure <ref type="figure" target="#fig_0">1</ref>). The neighborhood of a node used in the convolution operator is the set of k closest residues as determined by the mean distance between their atoms. Before going into the details of the node and edge features we describe the neural network architecture.</p><p>Pairwise classification architecture. In the protein interface prediction problem, examples are composed of pairs of residues, one from a ligand protein and one from a receptor protein, i.e., our task is to classify pairs of nodes from two separate graphs representing those proteins. More formally, our data are a set of N labeled pairs {((l i , r i ), y i )} N i=1 , where l i is a residue (node) in the ligand, r i  is a residue (node) in the receptor protein, and y i ∈ {−1, 1} is the associated label that indicates if the two residues are interacting or not. The role of ligand/receptor is arbitrary, so we would like to learn a scoring function that is independent of the order in which the two residues are presented to the network. In the context of SVM-based methods this can be addressed using pairwise kernels, building the invariance into the representation (see e.g. <ref type="bibr" target="#b1">[2]</ref>). To create an order-invariant model in a setting which requires an explicit feature representation. We considered two approaches. One is to construct explicit features that are order invariant by taking the sum and element-wise products of the two feature vectors. Note that pairwise kernels implicitly use all products of features, which we avoid by taking the element wise product. Another approach is to present each example to the model in both possible orders, (l i , r i ) and (r i , l i ), and average the two predictions; the feature representation of an example is the concatenation of the features of the two residues <ref type="bibr" target="#b2">[3]</ref>. In preliminary experiments both approaches yielded similar results, and our reported results use the latter.</p><p>Our network architecture is composed of two identical "legs" which learn feature representations of the ligand and receptor proteins of a complex by applying multiple layers of graph convolution to each. The weights between the two legs are shared. We then merge the legs by concatenating residue representations together to create the representation of residue pairs. The resulting features are then passed through one or more fully-connected layers before classification (see Figure <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Data.</p><p>In our experiments we used the data from Version 5 of the Docking Benchmark Dataset, which is the standard benchmark dataset for assessing docking and interface prediction methods <ref type="bibr" target="#b24">[25]</ref>. These complexes are a carefully selected subset of structures from the Protein Data Bank (PDB). The structures are generated from x-ray crystallography or nuclear magnetic resonance experiments and contain the atomic coordinates of each amino acid residue in the protein. These proteins range in length from 29 to 1979 residues with a median of 203.5. For each complex, DBD includes both bound and unbound forms of each protein in the complex. Our features are computed from the unbound form since proteins can alter their shape upon binding, and the labels are derived from the structure of the proteins in complex. As in previous work <ref type="bibr" target="#b1">[2]</ref>, two residues from different proteins are considered part of the interface if any non-Hydrogen atom in one is within 6Å of any non-Hydrogen atom in the other when in complex.</p><p>For our test set we used the 55 complexes that were added since version 4.0 of DBD, and separated the complexes in DBD 4.0 into training and validation sets. In dividing the complexes into training and validation we stratified them by difficulty and type using the information provided in DBD.</p><p>Because in any given complex there are vastly more residue pairs that don't interact than those that do, we downsampled the negative examples in the training set to obtain a 10:1 ratio of negative and positive examples. Final models used for testing were trained using the training and validation data, with the 10:1 ratio of positive to negative examples. Dataset sizes are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Node and edge features. Each node and edge in the graph representing a protein has features associated with it that are computed from the protein's sequence and structure. For the node features we used the same features used in earlier work <ref type="bibr" target="#b1">[2]</ref>, as summarized next. Protein sequence alone can be a good indicator of the propensity of a residue to form an interface, because each amino acid exhibits unique electrochemical and geometric properties. Furthermore, the level of conservation of a residue in alignments against similar proteins also provides valuable information, since surface residues that participate in an interface tend to be more conserved than surface residues that do not. The identity and conservation of a residue are quantified by 20 features that capture the relative frequency of each of the 20 amino acids in alignments to similar proteins. Earlier methods used these features by considering a window of size 11 in sequence centered around the residue of interest and concatenating their features <ref type="bibr" target="#b1">[2]</ref>. Since we are explicitly representing the structure of a protein, each node contains only the sequence features of the corresponding residue. In addition to these sequence-based features, each node contains several features computed from the structure. These include a residue's surface accessibility, a measure of its protrusion, its distance from the surface, and the counts of amino acids within 8Å in two directions-towards the residue's side chain, and in the opposite direction.</p><p>The primary edge feature is based on the distance between two residues, calculated as the average distance between their atoms. The feature is a Radial Basis Function (RBF) of this distance with a standard deviation of 18Å (chosen on the validation set). To incorporate information regarding the relative orientation of two residues, we calculate the angle between the normal vectors of the amide plane of each residue. Note that DCNNs use residue distances to inform the diffusion process. For this we used an RBF kernel over the distance, with a standard deviation optimized as part of the model selection procedure. All node and edge features were normalized to be between 0 and 1, except the residue conservation features, which were standardized.</p><p>Training, validation, and testing. The validation set was used to perform an extensive search over the space of possible feature representations and model hyperparameters, to select the edge distance feature RBF kernel standard deviation (2 to 32), negative to positive example ratio (1:1 to 20:1), number of convolutional layers (1 to 6), number of filters <ref type="bibr">(8 to 2000)</ref>, neighborhood size (2 to 26), pairwise residue representation (elementwise sum/product vs concatenation), number of dense layers after merging (0 to 4), optimization algorithm (stochastic gradient descent, RMSProp, ADAM, Momentum), learning rate (0.01 to 1), dropout probability (0.3 to 0.8), minibatch size (64 or 128 examples), and number of epochs (50 to 1000). This search was conducted manually and not all combinations were tested. Automatic model selection as in Bergstra et al. <ref type="bibr" target="#b6">[7]</ref> failed to outperform the best manual search results.</p><p>For testing, all classifiers were trained for 80 epochs in minibatches of 128. Weight matrices were initialized as in He et al. <ref type="bibr" target="#b10">[11]</ref> and biases initialized to zero. Rectified Linear Units were employed on all but the classification layer. During training we performed dropout with probability 0.5 to both dense and convolutional layers (except for DCNN, where performance was better when trained without dropout). Negative examples were randomly sampled to achieve a 10:1 ratio with positive examples, and the weighted cross entropy loss function was used to account for the class imbalance.</p><p>Training was performed using stochastic gradient descent with a learning rate of 0.1. Test results were computed by training the model on the training and validation sets using the model hyperparameters that yielded best validation performance. The convolution neighborhood (i.e. receptive field) is defined as a fixed-size set of residues that are closest in space to a residue of interest, and 21 yielded the best performance in our validation experiments. We implemented our networks in TensorFlow <ref type="bibr" target="#b0">[1]</ref> v1.0.1 to make use of rapid training on GPUs. Training times vary from roughly 17-102 minutes depending on convolution method and network depth, using a single NVIDIA GTX 980 or GTX TITAN X GPU. No Convolution 0.812 (0.007) 0.810 (0.006) 0.808 (0.006) 0.796 (0.006) Diffusion (DCNN) (2 hops) <ref type="bibr" target="#b4">[5]</ref> 0.790 (0.014) ---Diffusion (DCNN) (5 hops) <ref type="bibr" target="#b4">[5]</ref>) 0.828 (0.018) ---Single Weight Matrix (MFN <ref type="bibr" target="#b8">[9]</ref>) 0.865 (0.007) 0.871 (0.013) 0.873 (0.017) 0.869 (0.017) Node Average (Equation <ref type="formula" target="#formula_0">1</ref>) 0.864 (0.007) 0.882 (0.007) 0.891 (0.005) 0.889 (0.005) Node and Edge Average (Equation <ref type="formula" target="#formula_1">2</ref>) 0.876 (0.005) 0.898 (0.005) 0.895 (0.006) 0.889 (0.007) DTNN <ref type="bibr" target="#b20">[21]</ref> 0.867 (0.007) 0.880 (0.007) 0.882 (0.008) 0.873 (0.012) Order Dependent (Equation <ref type="formula" target="#formula_2">3</ref>) 0.854 (0.004) 0.873 (0.005) 0.891 (0.004) 0.889 (0.008) To determine the best form of graph convolution for protein interface prediction, we implemented the spatial graph convolution operators described in the Related Work section. The MFN method required modification to work well in our problem, namely averaging over neighbors rather than summing. For each graph convolution method, we searched over the hyperparameters listed above using the same manual search method; for the DCNN this also included the number of hops. Diffusion convolution is a single layer method as presented in the original publication; and indeed, stacking multiple diffusion convolutional layers yielded poor results, so testing was conducted using only one layer for that method.</p><p>To demonstrate the effectiveness of graph convolution we examine the effect of incorporating neighbor information by implementing a method that performs no convolution (referred to as No-Convolution), equivalent to Equation (1) with no summation over neighbors. The PAIRpred SVM method <ref type="bibr" target="#b1">[2]</ref> was trained by performing five fold cross validation on the training and validation data to select the best kernel and soft margin parameters before evaluating on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Results comparing the accuracy of the various graph convolution methods are shown in Table <ref type="table" target="#tab_1">2</ref>. Our first observation is that the proposed graph convolution methods, with AUCs around 0.89, outperform the No Convolution method, which had an AUC of 0.81, showing that the incorporation of information from a residue's neighbors improves the accuracy of interface prediction. This matches the biological intuition that the region around a residue should impact its binding affinity. We also observe that the proposed order-independent methods, with and without edge features (Equations ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>) ) and the order-dependent method (Equation (3) performed at a similar level, although the order-independent methods do so with fewer layers and far fewer model parameters than the order-dependent method. These methods exhibit improvement over the state-of-the-art PAIRPred method which yielded an AUC of 0.863.</p><p>The MFN method, which is a simpler version of the order-independent method given in Equation (1) performed slightly worse. This method uses the same weight matrix for the center node and its neighbors, and thereby does not differentiate between them. Its lower performance suggests this is an important distinction in our problem, where prediction is performed at the node level. This convolution operator was proposed in the context of a classification problem at the graph level. The DTNN approach is only slightly below the top performing methods. We have observed that the other convolutional methods perform better when the number of filters is increased gradually in subsequent network layers, a feature not afforded by this method.</p><p>Among the convolutional methods, the diffusion convolution method (DCNN) performed the worst, and was similar in performance to the No Convolution method. The other convolution methods performed best when employing multiple convolutional layers, suggesting that the networks are indeed learning a hierarchical representation of the data. However, networks with more than four layers performed worse, which could be attributed to the relatively limited amount of labeled protein interface data. Finally, we note that the extreme class imbalance in the test set produces a very poor area under the precision-recall curve, with no method achieving a value above 0.017.</p><p>To better understand the behavior of the best performing convolutional method we visualize the best performing test complex, PDB ID 3HI6 (see figure <ref type="figure" target="#fig_3">3</ref>). The figure shows that the highest predictions are in agreement with the true interface. We also visualize two convolutional filters to demonstrate their ability to learn aspects of the complex that are useful for interface prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We have examined the performance of several spatial graph convolutional methods in the problem of predicting interfaces between proteins on the basis of their 3D structure. Neighborhood-based convolution methods achieved state-of-the-art performance, outperforming diffusion-based convolution and the previous state-of-the-art SVM-based method. Among the neighborhood-based methods, order-independent methods performed similarly to an order-dependent method, and we identified elements that are important for the performance of the order-indpendent methods.</p><p>Our experiments did not demonstrate a big difference with the inclusion of edge features. There were very few of those, and unlike the node features, they were static: our networks learned latent representations only for the node features. These methods can be extended to learn both node and edge representations, and the underlying convolution operator admits a simple deconvolution operator which lends itself to be used with auto-encoders.</p><p>CNNs typically require large datasets to learn effective representations. This may have limited the level of accuracy that we could attain using our purely supervised approach and the relatively small The features learned by deep convolutional architectures for image classification have demonstrated a great degree of usefulness in classification tasks different than the ones they were originally trained on (see e.g. <ref type="bibr" target="#b21">[22]</ref>). Similarly, we expect the convolution operators we propose and the resulting features to be useful in many other applications, since structure information is useful for predicting a variety of properties of proteins, including their function, catalytic and other functional residues, prediction of protein-protein interactions, and protein interactions with DNA and RNA.</p><p>In designing our methodology we considered the question of the appropriate level at which to describe protein structure. In classifying image data, CNNs are usually applied to the raw pixel data <ref type="bibr" target="#b14">[15]</ref>. The analogous level of description for protein structure would be the raw 3D atomic coordinates, which we thought would prove too difficult. Using much larger training sets and unsupervised learning can potentially allow the network to begin with features that are closer to the raw atomic coordinates and learn a more detailed representation of the geometry of proteins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graph convolution on protein structures. Left: Each residue in a protein is a node in a graph where the</figDesc><graphic url="image-1.png" coords="3,155.32,82.37,117.46,125.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of the pairwise classification architecture. Each neighborhood of a residue in the twoproteins is processed using one or more graph convolution layers, with weight sharing between legs of the network. The activations generated by the convolutional layers are merged by concatenating them, followed by one or more regular dense layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: PyMOL<ref type="bibr" target="#b19">[20]</ref> visualizations of the best performing test complex (PDB ID 3HI6). Upper left: Ligand (red) and receptor (blue), along with the true interface (yellow). Upper right: Visualization of predicted scores, where brighter colors (cyan and orange) represent higher scores. Since scores are for pairs of residues, we take the max score over all partners in the partner protein. Bottom row: Activations of two filters in the second convolutional layer, where brighter colors indicate greater activation and black indicates activation of zero. Lower left: A filter which provides high activations for buried residues, a useful screening criterion for interface detection. Lower right: Filter which gives high activations for residues near the interface of this complex.</figDesc><graphic url="image-5.png" coords="8,255.50,165.51,257.88,145.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>number of labeled training examples. Unsupervised pre-training would allow us to use the entire Protein Data Bank which contains close to 130,000 structures (see http://www.rcsb.org/).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of complexes and examples in the Docking Benchmark Dataset. Positive examples are residue pairs that participate in the interface, negative examples are pairs that do not. For training we downsample the negative examples for an overall ratio of 10:1 of negative to positive examples; in validation and testing all the negative examples are used.</figDesc><table><row><cell cols="4">Data Partition Complexes Positive examples Negative examples</cell></row><row><cell>Train</cell><cell>140</cell><cell>12,866 (9.1%)</cell><cell>128,660 (90.9%)</cell></row><row><cell>Validation</cell><cell>35</cell><cell cols="2">3,138 (0.2%) 1,874,322 (99.8%)</cell></row><row><cell>Test</cell><cell>55</cell><cell cols="2">4,871 (0.1%) 4,953,446 (99.9%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Median area under the receiver operating characteristic curve (AUC) across all complexes in the test set for various graph convolutional methods. Results shown are the average and standard deviation over ten runs with different random seeds. Networks have the following number of filters for 1, 2, 3, and 4 layers before merging, respectively: (256), (256, 512), (256, 256, 512), (256, 256, 512, 512). The exception is the DTNN method, which by necessity produces an output which is has the same dimensionality as its input. Unlike the other methods, diffusion convolution performed best with an RBF with a standard deviation of 2Å. After merging, all networks have a dense layer with 512 hidden units followed by a binary classification layer. Bold faced values indicate best performance for each method.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowedgements</head><p>This work was supported by the National Science Foundation under grant no DBI-1564840.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Python code is available at https://github.com/fouticus/pipgcn, data can be downloaded from: https://zenodo.org/record/1127774, and the accompanying poster can be found at: https://zenodo.org/record/1134154.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Yuan Yu, and Xiaoqiang Zheng</publisher>
		</imprint>
	</monogr>
	<note>TensorFlow: Large-scale machine learning on heterogeneous systems</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PAIRpred: Partner-specific prediction of interacting residues from sequence and structure</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Fayyaz Ul Amir Afsar Minhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1142" to="1155" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partner-aware prediction of interacting residues in protein-protein complexes from sequence data</title>
		<author>
			<persName><forename type="first">Shandar</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Mizuguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e29104</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for computational biology</title>
		<author>
			<persName><forename type="first">Christof</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanel</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leopold</forename><surname>Parts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Stegle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular systems biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">878</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1993">1993-2001, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithmic approaches to protein-protein interaction site prediction</title>
		<author>
			<persName><forename type="first">Tristan</forename><forename type="middle">T</forename><surname>Aumentado-Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Murgita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms for Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithms for hyperparameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data. IEEE Sig</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Magazine</title>
				<meeting>Magazine</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progress and challenges in predicting protein interfaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Esmaielbeiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Nebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Deane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015-01">January. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imageNet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
				<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Protein surface representation and comparison : New approaches in structural proteomics</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Sael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological Data Mining</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="89" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast protein tertiary structure retrieval based on global surface shape similarity</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Sael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raif</forename><surname>Rustamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1259" to="1273" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The PyMOL molecular graphics system, version 1.8</title>
		<author>
			<persName><forename type="first">Llc</forename><surname>Schrödinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11">November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: The all convolutional net</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Updates to the integrated protein-protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2</title>
		<author>
			<persName><forename type="first">Thom</forename><surname>Vreven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">H</forename><surname>Moal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vangone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mieczyslaw</forename><surname>Kastritis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Torchala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chaleil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Jiménez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><surname>Fernandez-Recio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3031" to="3041" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
