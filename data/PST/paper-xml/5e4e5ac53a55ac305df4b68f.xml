<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-27">27 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-27">27 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.08155v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns generalpurpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both "bimodal" data of NL-PL pairs and "unimodal" data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large pre-trained models such as ELMo <ref type="bibr" target="#b18">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b21">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, XL-Net <ref type="bibr" target="#b30">(Yang et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b14">(Liu et al., 2019)</ref> have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, *Work is done during internship at Microsoft Research Asia. Contact: dutang@microsoft.com 1 Harbin Institute of Technology, Harbin,China 2 Sun Yat-sen University, China 3 Microsoft Research Asia, Beijing, China 4 Microsoft Search Technology Center Asia, Beijing, China. such as masked language modeling, which predicts the original masked word from an artificially masked input sequence. The success of pre-trained models in NLP also drives a surge of multi-modal pre-trained models, such as ViLBERT <ref type="bibr" target="#b15">(Lu et al., 2019)</ref> for language-image and VideoBERT <ref type="bibr" target="#b24">(Sun et al., 2019)</ref> for language-video, which are learned from bimodal data such as language-image pairs with bimodal self-supervised objectives.</p><p>In this work, we present CodeBERT, a bimodal pre-trained model for natural language (NL) and programming language (PL) like Python, Java, JavaScript, etc. CodeBERT captures the semantic connection between natural language and programming language, and produces general-purpose representations that can broadly support NL-PL understanding tasks (e.g. natural language code search) and generation tasks (e.g. code documentation generation). It is developed with the multi-layer Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, which is adopted in a majority of large pre-trained models. In order to make use of both bimodal instances of NL-PL pairs and large amount of available unimodal codes, we train CodeBERT with a hybrid objective function, including standard masked language modeling <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and replaced token detection <ref type="bibr" target="#b2">(Clark et al., 2020)</ref>, where unimodal codes help to learn better generators for producing better alternative tokens for the latter objective.</p><p>We train CodeBERT from Github code repositories in 6 programming languages, where bimodal datapoints are codes that pair with function-level natural language documentations <ref type="bibr" target="#b5">(Husain et al., 2019)</ref>. Training is conducted in a setting similar to that of multilingual BERT <ref type="bibr" target="#b20">(Pires et al., 2019)</ref>, in which case one pre-trained model is learned for 6 programming languages with no explicit markers used to denote the input programming language. We evaluate CodeBERT on two downstream NL-PL tasks, including natural language code search and code documentation generation. Results show that fine-tuning the parameters of CodeBERT achieves state-of-the-art performance on both tasks. To further investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and test CodeBERT in a zero-shot scenario, i.e. without fine-tuning the parameters of CodeBERT. We find that CodeBERT consistently outperforms RoBERTa, a purely natural language-based pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The contributions of this work are as follows:</head><p>• We propose CodeBERT, which to the best of our knowledge is the first large NL-PL pre-trained model.</p><p>• We present a hybrid learning objective that supports the use of both bimodal data of NL-PL pairs and easily accessed unimodal data, e.g. codes without paired natural language documentation.</p><p>• We demonstrate that CodeBERT achieves state-of-theart performance on natural language code search and code documentation generation. We further create a dataset to investigate the probing ability of NL-PL pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pre-Trained Models in NLP</head><p>Large pre-trained models <ref type="bibr" target="#b18">(Peters et al., 2018;</ref><ref type="bibr" target="#b21">Radford et al., 2018;</ref><ref type="bibr" target="#b3">Devlin et al., 2018;</ref><ref type="bibr" target="#b30">Yang et al., 2019;</ref><ref type="bibr" target="#b14">Liu et al., 2019;</ref><ref type="bibr" target="#b22">Raffel et al., 2019)</ref> have brought dramatic empirical improvements on almost every NLP task in the past few years. Successful approaches train deep neural networks on largescale plain texts with self-supervised learning objectives. One of the most representative neural architectures is Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, which is also the one used in this work. It contains multiple self-attention layers, and can be conventionally learned with gradient decent in an endto-end manner as every component is differentiable. The terminology "self-supervised" means that supervisions used for pre-training are automatically collected from raw data without manual annotation. Dominant learning objectives are language modeling and its variations. For example, in GPT <ref type="bibr" target="#b21">(Radford et al., 2018)</ref>, the learning objective is language modeling, namely predicting the next word w k given the preceding context words {w 1 , w 2 , ..., w k−1 }. As the ultimate goal of pre-training is not to train a good language model, it is desirable to consider both preceding and following contexts to learn better general-purpose contextual representations. This leads us to the masked language modeling objective used in BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, which learns to predict the masked words of a randomly masked word sequence given surrounding contexts. Masked language modeling is also used as one of the two learning objectives for training CodeBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-Modal Pre-Trained Models</head><p>The remarkable success of the pre-trained model in NLP drives the development of multi-modal pre-trained model that learns implicit alignment between inputs of different modalities. These models are typically learned from bimodal data, such as pairs of language-image or pairs of language-video. For example, ViLBERT <ref type="bibr" target="#b15">(Lu et al., 2019)</ref> learns from image caption data, where the model learns by reconstructing categories of masked image region or masked words given the observed inputs, and meanwhile predicting whether the caption describes the image content or not. Similarly, VideoBERT <ref type="bibr" target="#b24">(Sun et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CodeBERT</head><p>We describe the details about CodeBERT in this section, including the model architecture, the input and output representations, the objectives and data used for training Code-BERT, and how to fine-tune CodeBERT when it is applied to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>We follow BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr" target="#b14">(Liu et al., 2019)</ref>, and use multi-layer bidirectional Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> as the model architecture of Code-BERT. We will not review the ubiquitous Transformer architecture in detail. We develop CodeBERT by using exactly the same model architecture as RoBERTa-base, which includes 12 layers. Each layer has 12 self-attention heads, and the size of each head is 64. The hidden dimension is 768 and the inner hidden size of the feed-forward layer is 3072.</p><p>The total number of model parameters is 125M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Input/Output Representations</head><p>In the pre-training phase, we set the input as the concatenation of two segments with a special separator token, namely</p><formula xml:id="formula_0">[CLS], w 1 , w 2 , ..w n , [SEP ], c 1 , c 2 , ..., c m , [EOS].</formula><p>One segment is natural language text, and another is code from a certain programming language.</p><p>[CLS] is a special token in front of two segments, whose final hidden representation is considered as the aggregated sequence represen-tation for classification or ranking. Following the standard way of processing text in Transformer, we regard a natural language text as a sequence of words, and split it as Word-Piece <ref type="bibr" target="#b29">(Wu et al., 2016)</ref>. We regard a piece of code as a sequence of tokens.</p><p>The output of CodeBERT includes (1) contextual vector representation of each token, for both natural language and code, and (2) the representation of [CLS], which works as the aggregated sequence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pre-Training Data</head><p>We train CodeBERT with both bimodal data, which refers to parallel data of natural language-code pairs, and unimodal data, which stands for codes without paired natural language texts and natural language without paired codes.</p><p>We use datapoints from Github repositories, where each bimodal datapoint is an individual function with paired documentation, and each unimodal code is a function without paired documentation. Specifically, we use a recent large dataset provided by <ref type="bibr" target="#b5">Husain et al. (2019)</ref>, which includes 2.1M bimodal datapoints and 6.4M unimodal codes across six programming languages (Python, Java, JavaScript, PHP, Ruby, and Go). Data statistics is shown in Table <ref type="table" target="#tab_1">1</ref>. <ref type="foot" target="#foot_0">1</ref>The data comes from publicly available open-source nonfork GitHub repositories and are filtered with a set of constraints and rules. For example, (1) each project should be used by at least one other project, (2) each documentation is truncated to the first paragraph, (3) documentations shorter than three tokens are removed, (4) functions shorter than three lines are removed, and (5) function names with substring "test" are removed. An example of the data is given in Figure <ref type="figure" target="#fig_0">1</ref> <ref type="foot" target="#foot_1">2</ref> .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pre-Training CodeBERT</head><p>We describe the two objectives used for training CodeBERT here. The first objective is masked language modeling (MLM), which has proven effective in literature <ref type="bibr" target="#b3">(Devlin et al., 2018;</ref><ref type="bibr" target="#b14">Liu et al., 2019;</ref><ref type="bibr" target="#b24">Sun et al., 2019)</ref>. We apply masked language modeling on bimodal data of NL-PL pairs. The second objective is replaced token detection (RTD), which further uses a large amount of unimodal data, such as codes without paired natural language texts.</p><p>Objective #1: Masked Language Modeling (MLM) Given a datapoint of NL-PL pair (x = {w, c}) as input, where w is a sequence of NL words and c is a sequence of PL tokens, we first select a random set of positions for both NL and PL to mask out (i.e. m w and m c , respectively), and then replace the selected positions with a special [M ASK] token. Following <ref type="bibr" target="#b3">Devlin et al. (2018)</ref>, 15% of the tokens from x are masked out.</p><formula xml:id="formula_1">m w i ∼ unif{1, |w|} for i = 1 to |w| (1) m c i ∼ unif{1, |c|} for i = 1 to |c| (2) w masked = REPLACE(w, m w , [M ASK])<label>(3)</label></formula><formula xml:id="formula_2">c masked = REPLACE(c, m c , [M ASK]) (4) x = w + c (5)</formula><p>The MLM objective is to predict the original tokens which are masked out, formulated as follows, where p D1 is the discriminator which predicts a token from a large vocabulary.</p><formula xml:id="formula_3">L MLM (θ) = i∈m w ∪m c −log p D1 (x i |w masked , c masked ) (6)</formula><p>Objective #2: Replaced Token Detection (RTD)  pre-trained model for natural language. We adapt it in our scenario, with the advantage of using both bimodal and unimodal data for training. Specifically, there are two data generators here, an NL generator p Gw and a PL generator p Gc , both for generating plausible alternatives for the set of randomly masked positions.</p><p>ŵi ∼ p Gw (w i |w masked ) for i ∈ m w (7)</p><p>ĉi ∼ p Gc (c i |c masked ) for i ∈ m c (8)</p><formula xml:id="formula_4">w corrupt = REPLACE(w, m w , ŵ) (9) c corrupt = REPLACE(c, m c , ĉ) (10) x corrupt = w corrupt + c corrupt (11)</formula><p>The discriminator is learned to determine whether a word is the original one or not, which is a binary classification problem. It is worth noting that the RTD objective is applied to every position in the input, and it differs from GAN (generative adversarial network) in that if a generator happens to produce the correct token, the label of that token is "real" instead of "fake" <ref type="bibr" target="#b2">(Clark et al., 2020)</ref>. The loss function of RTD with regard to the discriminator parameterized by θ is given below, where δ(i) is an indicator function and p D2 is the discriminator that predicts the probability of the i-th word being real.</p><formula xml:id="formula_5">L RTD (θ) = |w|+|c| i=1 δ(i)log p D2 (x corrupt , i)+ 1 − δ(i) 1 − log p D2 (x corrupt , i)<label>(12)</label></formula><formula xml:id="formula_6">δ(i) = 1, if x corrupt i = x i . 0, otherwise.<label>(13)</label></formula><p>There are many different ways to implement the generators.</p><p>In this work, we implement two efficient n-gram language models <ref type="bibr" target="#b7">(Jurafsky, 2000)</ref> with bidirectional contexts, one for NL and one for PL, and learn them from corresponding unimodel datapoints, respectively. The approach is easily generalized to learn bimodal generators or use more complicated generators like Transformer-based neural architecture learned in a joint manner. We leave these to future work.</p><p>The PL training data is the unimodal codes as shown in Table <ref type="table" target="#tab_1">1</ref>, and the NL training data comes from the documentations from bimodal data. One could easily extend these two training datasets to larger amount. The final loss function are given below.</p><formula xml:id="formula_7">min θ L MLM (θ) + L RTD (θ)<label>(14)</label></formula><p>We train CodeBERT on one NVIDIA DGX-2 machine using FP16. It combines 16 interconnected NVIDIA Tesla V100 with 32GB memory. We use the following set of hyperparameters to train models: batchsize is in {256,512} and learning rate is in {1e-4,5e-4}. We use Adam <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref> to update the parameters and set the number of warmup steps as 10K. We set the batch size as 2,048 and max length as 512. Training 1,000 batches of data costs 600 minutes with MLM objective, 120 minutes with RTD objective. The final model is trained with 25K batches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Fine-Tuning CodeBERT</head><p>We have different settings to use CodeBERT in downstream NL-PL tasks. For example, in natural language code search, we feed the input as the same way as the pre-training phase and use the representation of [CLS] to measure the semantic relevance between code and natural language query, while in code-to-text generation, we use an encoder-decoder framework and initialize the encoder of a generative model with CodeBERT. Details are in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We present empirical results in this section to verify the effectiveness of CodeBERT. We first describe the use of CodeBERT in natural language code search ( §4.1), in a way that model parameters of CodeBERT are fine-tuned. After that, we present the NL-PL probing task ( §4.2), and evaluate CodeBERT in a zero-shot setting where the parameters of CodeBERT are clamped. Finally, we evaluate CodeBERT on a generation problem, i.e. code documentation generation ( §4.3), and further evaluate on a programming language which is never seen in the training phase ( §4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Natural Language Code Search</head><p>Given a natural language as the input, the objective of code search is to find the most semantically related code from a collection of codes. We conduct experiments on the Code-SearchNet corpus <ref type="bibr" target="#b5">(Husain et al., 2019)</ref> <ref type="foot" target="#foot_2">3</ref> . Data statistics of the training/validation/testing data splits for six programming languages are given in Table <ref type="table" target="#tab_4">3</ref>. We follow the official evaluation metric to calculate the Mean Reciprocal Rank (MRR) for each pair of test data (c, w) over a fixed set of 999 distractor codes. We further calculate the macro- average MRR for all languages as an overall evaluation metric. It is helpful to note that this metric differs from the AVG metric in the original paper, where the answer is retrieved from candidates from all six languages. We fine-tune a language-specific model for each programming language<ref type="foot" target="#foot_3">4</ref> . We train each model with a binary classification loss function, where a sof tmax layer is connected to the representation of <ref type="bibr">[CLS]</ref>. Both training and validation datasets are created in a way that positive and negative samples are balanced. Negative samples consist of balanced number of instances with randomly replaced NL (i.e. (c, ŵ)) and PL (i.e. (ĉ, w)).</p><p>In the fine-turning step, we set the learning rate as 1e-5, the batch size as 64, the max sequence length as 200 and the max fine-tuning epoch as 8. We use Adam to update the parameters. We choose the model performed best on the development set, and use that to evaluate on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparisons</head><p>Table <ref type="table" target="#tab_3">2</ref> shows the results of different approaches on the CodeSearchNet corpus. The first four rows are reported by <ref type="bibr" target="#b5">Husain et al. (2019)</ref>, which are joint em- beddings of NL and PL <ref type="bibr" target="#b4">(Gu et al., 2018;</ref><ref type="bibr" target="#b16">Mitra et al., 2018)</ref>. NBOW represents neural bag-of-words. CNN, BIRNN and SELFATT stand for 1D convolultional neural network <ref type="bibr" target="#b9">(Kim, 2014)</ref>, bidirectional GRU-based recurrent neural network <ref type="bibr" target="#b1">(Cho et al., 2014)</ref>, and multi-head attention <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, respectively.</p><p>We report the remaining numbers in Table <ref type="table" target="#tab_3">2</ref>. We train all these pre-trained models by regarding codes as a sequence of tokens. We also continuously train RoBERTa only on codes from CodeSearchNet with masked language modeling. Results show that CodeBERT consistently performs better than RoBERTa and the model pre-trained with code only. CodeBERT (MLM) learned from scratch performs better than RoBERTa. Unsurprisingly, initializing CodeBERT with RoBERTa improves the performance.</p><p>We further give a learning curve of different pre-trained models in the fine-tuning process. From Figure <ref type="figure" target="#fig_2">3</ref>, we can see that CodeBERT performs better at the early stage, which reflects that CodeBERT provides good initialization for learning downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NL-PL Probing</head><p>In the previous subsection, we show the empirical effectiveness of CodeBERT in a setting that the parameters of CodeBERT are fine-tuned in downstream tasks. In this subsection, we further investigate what type of knowledge is learned in CodeBERT without modifying the parameters.</p><p>Task Formulation and Data Construction Following the probing experiments in NLP <ref type="bibr" target="#b19">(Petroni et al., 2019;</ref><ref type="bibr" target="#b27">Talmor et al., 2019)</ref>, we study NL-PL probing here. Since there is no existing work towards this goal, we formulate the problem of NL-PL probing and create the dataset by ourselves. Given an NL-PL pair (c, w), the goal of NL-PL probing is to test model's ability to correctly predict/recover the masked token of interest (either a code token c i or word token w j ) among distractors. There are two major types of distractors: one is the whole target vocabulary used for the masked language modeling objective <ref type="bibr" target="#b19">(Petroni et al., 2019)</ref>, and another one has fewer candidates which are filter or curated based on experts' understanding about the ability to be tested <ref type="bibr" target="#b27">(Talmor et al., 2019)</ref>. We follow the second direction and formulate NL-PL probing as a multi-choice question answering task, where the question is cloze-style in which a certain token is replaced by [M ASK] and distractor candidate answers are curated based on our expertise. Specifically, we evaluate on the NL side and PL side, respectively. To ease the effort of data collection, we collect data automatically from NL-PL pairs in both validation and testing sets of CodeSearchNet, both of which are unseen in the pre-training phase. To evaluate on the NL side, we select NL-PL pairs whose NL documentations include one of the six keywords (max, maximize, min, minimize, less, greater), and group them to four candidates by merging first two keywords and the middle two keywords. The task is to ask pre-trained models to select the correct one instead of three other distractors. That is to say, the input in this setting includes the complete code and a masked NL documentation. The goal is to select the correct answer from four candidates. For the PL side, we select codes containing keywords max and min, and formulate the task as a twochoice answer selection problem. Here, the input includes complete NL documentation and a masked PL code, and the goal is to select the correct answer from two candidates. Since code completion is an important scenario, we would like to test model's ability in predicting the correct token merely based on preceding PL contexts. Therefore, we add an additional setting for PL side, where the input includes the complete NL documentation and preceding PL codes. Data statistics is given in the top two rows in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparisons</head><p>Results are given in Table <ref type="table" target="#tab_5">4</ref>. We report accuracy, namely the number of correctly predicted instances over the number of all instances, for each programming language. Since datasets in different programming languages are extremely unbalanced, we report the accumulated metric with the same way. We use CodeBERT (MLM) here because its output layer naturally fits for probing. Results show that CodeBERT performs better than baselines on almost all languages on both NL and PL probing. The numbers with only preceding contexts are lower than that with bidirectional contexts, which suggests that code completion is challenging. We leave it as a future work.</p><p>We further give a case study on PL-NL probing. Figure <ref type="figure">4</ref> illustrates the example of a python code 5 . We mask NL token and PL token separately, and report the predicted probabilities of RoBERTa and CodeBERT. We can see that RoBERTa fails in both cases, whereas CodeBERT makes the correct prediction in both NL and PL settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Code Documentation Generation</head><p>Although the pre-training objective of CodeBERT does not include generation-based objectives <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref>, we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. We use BLEU-4 score <ref type="bibr" target="#b17">(Papineni et al., 2002)</ref> as our evaluation metric. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin   5 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 &amp; Och, 2004).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Training Details</head><p>We compare our model with several baselines, including a RNN-based sequence-tosequence model with attention mechanism <ref type="bibr" target="#b25">(Sutskever et al., 2014)</ref>, the Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, RoBERTa and the model pre-trained on code only. We use Transformer with 6 layers, 768 dimensional hidden states and 12 attention heads as our decoder in all settings. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and stay hyperparameters consistent. We set the max length of input and inference as 256 and 64, respectively. We use the Adam optimizer to update model parameters. The learning rate and the batch size are 5e-5 and 64, respectively. We tune hyperparameters and perform early stopping on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Table <ref type="table">5</ref> shows the results with different models for the code-to-documentation generation task. As we can see, models pre-trained on programming language outperform RoBERTa, which illustrates that pre-trainning models on programming language could improve code-to-NL generation. Besides, results in the Table show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3% BLEU score over RoBERTa overall and achieve the state-of-the-art performance on the majority of programming languages. These results show that our pre-trainning objectives (MLM and RTD) are effective for CodeBERT on code-to-NL generation tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present CodeBERT, which to the best of our knowledge is the first large bimodal pre-trained model for natural language and programming language. We train CodeBERT on both bimodal and unimodal data, and show that fine-tuning CodeBERT achieves state-of-the-art performance on downstream tasks including natural language code search and code-to-documentation generation. To further investigate the knowledge embodied in pre-trained models, we formulate the task of NL-PL probing and create a dataset for probing. We regard the probing task as a cloze-style answer selection problem, and curate distractors for both NL and PL parts. Results show that, with model parameters fixed, CodeBERT performs better than RoBERTa and a continuously trained model using codes only.</p><p>There are many potential directions for further research on this field. First, one could learn better generators with bimodal evidence or more complicated neural architecture to improve the replaced token detection objective. Second, the loss functions of CodeBERT mainly target on NL-PL understanding tasks. Although CodeBERT achieves strong BLEU scores on code-to-documentation generation, the CodeBERT itself could be further improved by generation-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of the NL-PL pair, where NL is the first paragraph (filled in red) from the documentation (dashed line in black) of a function.</figDesc><graphic url="image-1.png" coords="3,306.49,66.59,236.14,126.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. An illustration about the replaced token detection objective. Both NL and code generators are language models, which generate plausible tokens for masked positions based on surrounding contexts. NL-Code discriminator is the targeted pre-trained model, which is trained via detecting plausible alternatives tokens sampled from NL and PL generators. NL-Code discriminator is used for producing general-purpose representations in the fine-tuning step. Both NL and code generators are are thrown out in the fine-tuning step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Learning curve of different pre-trained models in the fine-tuning step. We show results on Python and Java.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the dataset used for training CodeBERT.</figDesc><table><row><cell cols="3">TRAINING DATA bimodal DATA unimodal CODES</cell></row><row><cell>GO</cell><cell>319,256</cell><cell>726,768</cell></row><row><cell>JAVA</cell><cell>500,754</cell><cell>1,569,889</cell></row><row><cell>JAVASCRIPT</cell><cell>143,252</cell><cell>1,857,835</cell></row><row><cell>PHP</cell><cell>662,907</cell><cell>977,821</cell></row><row><cell>PYTHON</cell><cell>458,219</cell><cell>1,156,085</cell></row><row><cell>RUBY</cell><cell>52,905</cell><cell>164,048</cell></row><row><cell>ALL</cell><cell>2,137,293</cell><cell>6,452,446</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results on natural language code retrieval. Baselines include four joint embeddings (first group) of NL and PL, RoBERTa, and RoBERTa which is continuously trained with masked language modeling on codes only (second group). PT stands for pre-training. We train CodeBERT (third group) with different settings, including using different initialization ( from scratch (INIT=SCRATCH) or initialized with the parameters of RoBERTa) and using different learning objectives (MLM, RTD, or the combination of both).</figDesc><table><row><cell>MODEL</cell><cell>RUBY</cell><cell>JAVASCRIPT</cell><cell>GO</cell><cell>PYTHON</cell><cell>JAVA</cell><cell>PHP</cell><cell>MA-AVG</cell></row><row><cell>NBOW</cell><cell>0.4285</cell><cell>0.4607</cell><cell>0.6409</cell><cell>0.5809</cell><cell cols="2">0.5140 0.4835</cell><cell>0.5181</cell></row><row><cell>CNN</cell><cell>0.2450</cell><cell>0.3523</cell><cell>0.6274</cell><cell>0.5708</cell><cell cols="2">0.5270 0.5294</cell><cell>0.4753</cell></row><row><cell>BIRNN</cell><cell>0.0835</cell><cell>0.1530</cell><cell>0.4524</cell><cell>0.3213</cell><cell cols="2">0.2865 0.2512</cell><cell>0.2580</cell></row><row><cell>SELFATT</cell><cell>0.3651</cell><cell>0.4506</cell><cell>0.6809</cell><cell>0.6922</cell><cell cols="2">0.5866 0.6011</cell><cell>0.5628</cell></row><row><cell>ROBERTA</cell><cell>0.6245</cell><cell>0.6060</cell><cell>0.8204</cell><cell>0.8087</cell><cell cols="2">0.6659 0.6576</cell><cell>0.6972</cell></row><row><cell>PT W/ CODE ONLY (INIT=SCRATCH)</cell><cell>0.5712</cell><cell>0.5557</cell><cell>0.7929</cell><cell>0.7855</cell><cell cols="2">0.6567 0.6172</cell><cell>0.6632</cell></row><row><cell>PT W/ CODE ONLY (INIT=ROBERTA)</cell><cell>0.6612</cell><cell>0.6402</cell><cell>0.8191</cell><cell>0.8438</cell><cell cols="2">0.7213 0.6706</cell><cell>0.7260</cell></row><row><cell>CODEBERT (MLM, INIT=SCRATCH)</cell><cell>0.5695</cell><cell>0.6029</cell><cell>0.8304</cell><cell>0.8261</cell><cell cols="2">0.7142 0.6556</cell><cell>0.6998</cell></row><row><cell>CODEBERT (MLM, INIT=ROBERTA)</cell><cell>0.6898</cell><cell>0.6997</cell><cell>0.8383</cell><cell>0.8647</cell><cell cols="2">0.7476 0.6893</cell><cell>0.7549</cell></row><row><cell>CODEBERT (RTD, INIT=ROBERTA)</cell><cell>0.6414</cell><cell>0.6512</cell><cell>0.8285</cell><cell>0.8263</cell><cell cols="2">0.7150 0.6774</cell><cell>0.7233</cell></row><row><cell cols="2">CODEBERT (MLM+RTD, INIT=ROBERTA) 0.6926</cell><cell>0.7059</cell><cell>0.8400</cell><cell>0.8685</cell><cell cols="2">0.7484 0.7062</cell><cell>0.7603</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Data statistics about the CodeSearchNet Corpus for natural language code search.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Statistics of the data for NL-PL probing and the performance of different pre-trained models. Accuracies (%) are reported. Best results in each group are in bold.</figDesc><table><row><cell></cell><cell>RUBY</cell><cell>JAVASCRIPT</cell><cell>GO</cell><cell>PYTHON</cell><cell>JAVA</cell><cell>PHP</cell><cell>ALL</cell></row><row><cell cols="2">NUMBER OF DATAPOINTS FOR PROBING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PL (2 CHOICES)</cell><cell>38</cell><cell>272</cell><cell>152</cell><cell>1,264</cell><cell>482</cell><cell>407</cell><cell>2,615</cell></row><row><cell>NL (4 CHOICES)</cell><cell>20</cell><cell>65</cell><cell>159</cell><cell>216</cell><cell>323</cell><cell>73</cell><cell>856</cell></row><row><cell>PL PROBING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROBERTA</cell><cell>73.68%</cell><cell>65.07%</cell><cell>71.05%</cell><cell>59.02%</cell><cell>62.03%</cell><cell>70.02%</cell><cell>62.83%</cell></row><row><cell>PRE-TRAIN W/ CODE ONLY</cell><cell>84.21%</cell><cell>81.62%</cell><cell>91.45%</cell><cell>75.16%</cell><cell>85.27%</cell><cell>83.05%</cell><cell>80.00%</cell></row><row><cell>CODEBERT (MLM)</cell><cell>81.58%</cell><cell>86.40%</cell><cell cols="5">92.11% 79.19% 91.08% 90.42% 84.67%</cell></row><row><cell cols="2">PL PROBING WITH PRECEDING CONTEXT ONLY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROBERTA</cell><cell>71.05%</cell><cell>51.84%</cell><cell>51.32%</cell><cell>55.06%</cell><cell>42.12%</cell><cell>52.58%</cell><cell>51.97%</cell></row><row><cell>PRE-TRAIN W/ CODE ONLY</cell><cell>63.16%</cell><cell>49.26%</cell><cell cols="4">59.51% 56.96% 59.13% 58.72%</cell><cell>57.05%</cell></row><row><cell>CODEBERT (MLM)</cell><cell>60.53%</cell><cell>52.21%</cell><cell cols="5">59.51% 61.16% 57.68% 61.92% 59.58%</cell></row><row><cell>NL PROBING</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROBERTA</cell><cell>45.00%</cell><cell>72.31%</cell><cell>47.17%</cell><cell>67.59%</cell><cell>50.77%</cell><cell>61.64%</cell><cell>56.78%</cell></row><row><cell>PRE-TRAIN W/ CODE ONLY</cell><cell>55.00%</cell><cell>61.54%</cell><cell>55.97%</cell><cell>65.74%</cell><cell>53.25%</cell><cell>61.64%</cell><cell>58.29%</cell></row><row><cell>CODEBERT (MLM)</cell><cell>60.00%</cell><cell>83.08%</cell><cell cols="5">64.15% 72.69% 61.61% 75.34% 67.64%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Case study on python language. Masked tokens in NL (in blue) and PL (in yellow) are separately applied. Predicted probabilities of RoBERTa and CodeBERT are given.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">masked NL token</cell></row><row><cell cols="5">"Transforms a vector np.arange(-N, M, dx) to np.arange( min (|vec|),</cell></row><row><cell cols="2">max(N,M),dx)]"</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">def vec_to_halfvec(vec):</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">d = vec[1:] -vec[:-1]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">if ((d/d.mean()).std() &gt; 1e-14) or (d.mean() &lt; 0):</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">raise ValueError('vec must be np.arange() in increasing order')</cell></row><row><cell cols="2">dx = d.mean()</cell><cell cols="2">masked PL token</cell><cell></cell></row><row><cell cols="2">lowest = np.abs(vec). min ()</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">highest = np.abs(vec).max()</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">return np.arange(lowest, highest + 0.1*dx, dx).astype(vec.dtype)</cell></row><row><cell></cell><cell>max</cell><cell>min</cell><cell>less</cell><cell>greater</cell></row><row><cell>NL</cell><cell>Roberta CodeBERT (MLM) 39.38% 96.24%</cell><cell>3.73% 60.60%</cell><cell>0.02% 0.02%</cell><cell>0.01% 0.0003%</cell></row><row><cell>PL</cell><cell>Roberta CodeBERT (MLM) 0.001% 95.85%</cell><cell>4.15% 99.999%</cell><cell>--</cell><cell>--</cell></row><row><cell cols="2">Figure 4.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Code-to-NL generation on C# language.</figDesc><table><row><cell>MODEL</cell><cell>BLEU</cell></row><row><cell>MOSES (KOEHN ET AL., 2007)</cell><cell>11.57</cell></row><row><cell>IR</cell><cell>13.66</cell></row><row><cell>SUM-NN (RUSH ET AL., 2015)</cell><cell>19.31</cell></row><row><cell>2-LAYER BILSTM</cell><cell>19.78</cell></row><row><cell>TRANSFORMER (VASWANI ET AL., 2017)</cell><cell>19.68</cell></row><row><cell>TREELSTM (TAI ET AL., 2015)</cell><cell>20.11</cell></row><row><cell>CODENN (IYER ET AL., 2016)</cell><cell>20.53</cell></row><row><cell>CODE2SEQ (ALON ET AL., 2019)</cell><cell>23.04</cell></row><row><cell>ROBERTA</cell><cell>19.81</cell></row><row><cell>PRE-TRAIN W/ CODE ONLY</cell><cell>20.65</cell></row><row><cell>CODEBERT (RTD)</cell><cell>22.14</cell></row><row><cell>CODEBERT (MLM)</cell><cell>22.32</cell></row><row><cell>CODEBERT (MLM+RTD)</cell><cell>22.36</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Since we will evaluate on the natural language code search task, we only use the training data of<ref type="bibr" target="#b5">Husain et al. (2019)</ref> to train CodeBERT with no access to the dev or testing data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The source of the illustrating example comes from https://github.com/apache/spark/blob/ 618d6bff71073c8c93501ab7392c3cc579730f0b/ python/pyspark/rdd.py#L125-L138</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/github/CodeSearchNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We have fine-tuned a multi-lingual model for six programming languages, but find that it performs worse that fine-tuning a language-specific model for each programming language.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">https://github.com/sriniiyer/codenn</note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CODE SEARCH TRAINING DEV TESTING GO 635,635 28,483 14,291 JAVA 908,886 30,655 26,909 JAVASCRIPT 247,773 16,505 6,483 PHP 1,047,406 52,029 28,391 PYTHON 824,342 46,213 22,176 RUBY 97,580 4,417 2,279</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We would like to evaluate CodeBERT on the programming language which is never seen in the pre-training step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN <ref type="bibr" target="#b6">(Iyer et al., 2016)</ref> 6 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. The length of target document in this task is about 10 on average. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. To reliably evaluate models, the dataset extends the test set by asking human to provide two additional titles for code snippets from the test set, making a total of three reference titles for each code snippet. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as <ref type="bibr" target="#b6">Iyer et al. (2016)</ref>.</p><p>Since state-of-the-art methods use RNN as their decoder, we choose a 2-layer GRU <ref type="bibr" target="#b1">(Cho et al., 2014</ref>) with an attention mechanism as our decoder for a comparison. We fine-tune models using a grid search with the following set of hyper-parameters: batchsize is in {32, 64} and learning rate is in {2e-5, 5e-5}. We report the number when models achieve best performance on the development set.</p><p>Table <ref type="table">6</ref> shows that our model with MLM and RTD pretraining objectives achieves 22.36 BLEU score and improves by 2.55 points over RoBERTa, which illustrates CodeBERT could generalize better to other programming language which is never seen in the pre-training step. However, our model achieve slightly lower results than code2seq <ref type="bibr" target="#b0">(Alon et al., 2019)</ref>. The main reason could be that code2seq makes use of compositional paths in its abstract syntax tree (AST) while CodeBERT only takes original code as the input. We have trained a version of CodeBERT by traversing the tree structure of AST following a certain order, but applying that model does not bring improvements on generation tasks. This shows a potential direction to improve CodeBERT by incorporating AST. related learning objectives. How to successfully incorporate AST into the pre-training step is also an attractive direction. Third, we plan to apply CodeBERT to more NL-PL related tasks, and extend it to more programming languages. Flexible and powerful domain/language adaptation methods are necessary to generalize well.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<title level="m">Generating sequences from structured representations of code. International Conferenceon Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">{ELECTRA}: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep code search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="933" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speech &amp; language processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pre-trained contextual embedding of source code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00059</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</title>
				<meeting>the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Orange: a method for evaluating automatic evaluation metrics for machine translation</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
				<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">501</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pretraining task-agnostic visiolinguistic representations for visionand-language tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Vilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An introduction to neural information retrieval</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="126" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
				<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01502</idno>
		<title level="m">How multilingual is multilingual bert?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Videobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01766</idno>
		<title level="m">A joint model for video and language representation learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13283</idno>
		<title level="m">olmpicson what language model pre-training captures</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
