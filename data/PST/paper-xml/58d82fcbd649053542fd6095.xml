<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Recurrent Neural Network for Protein Function Prediction from Sequence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xueliang</forename><forename type="middle">Leon</forename><surname>Liu</surname></persName>
							<email>xliu@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Wyss Institute for Biologically Inspired Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Systems Biology</orgName>
								<orgName type="department" key="dep2">Harvard Medical School</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Recurrent Neural Network for Protein Function Prediction from Sequence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>artificial neural network</term>
					<term>protein function</term>
					<term>CRISPR</term>
					<term>P450</term>
					<term>biomagnetism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As highthroughput biological sequencing becomes faster and cheaper, the need to extract useful information from sequencing becomes ever more paramount, often limited by lowthroughput experimental characterizations. For proteins, accurate prediction of their functions directly from their primary aminoacid sequences has been a long standing challenge. Here, machine learning using artificial recurrent neural networks (RNN) was applied towards classification of protein function directly from primary sequence without sequence alignment, heuristic scoring or feature engineering.</p><p>The RNN models containing longshorttermmemory (LSTM) units trained on public, annotated datasets from UniProt achieved high performance for inclass prediction of four important protein functions tested, particularly compared to other machine learning algorithms using sequencederived protein features. RNN models were used also for outofclass predictions of phylogenetically distinct protein families with similar functions, including proteins of the CRISPRassociated nuclease, ferritinlike iron storage and cytochrome P450 families. Applying the trained RNN models on the partially unannotated UniRef100 database predicted not only candidates validated by existing annotations but also currently unannotated sequences. Some RNN predictions for the ferritinlike iron sequestering function were experimentally validated, even though their sequences differ significantly from known, characterized proteins and from each other and cannot be easily predicted using popular bioinformatics methods. As sequencing and experimental characterization data increases rapidly, the machinelearning approach based on RNN could be useful for discovery and prediction of homologues for a wide range of protein functions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As the cost of DNA sequencing is decreasing drastically over the last decade, the volume of biological sequences particularly for new proteins is also increasing rapidly.</p><p>Discovering the functions of these new proteins not only could allow one to better understand their roles in their native contexts, but also utilize them in synthetic biology to assembled new biological circuits and pathways for useful applications such as production of valuable compounds or treating disease. However, the experimental characterization of proteins' properties such as structure and function can be slow and resourcedemanding using techniques such as xray crystallography, cryoTEM, or functional assays, significantly outpaced by sequencing. A predictive pipeline that can accurately translate primary sequence to function would allow filtering of the vast sequence dataset to an experimentally manageable subset of highconfidence candidates of highest interest toward a particular function or application is greatly desired.</p><p>Currently there are several popular methods for extracting useful information from primary sequences and infer functional information based on comparison of new sequences to existing sequences of known function. For example, BLAST performs sequence alignment with heuristic scoring. Multiple sequence alignments can be used to build models that capture conservation patterns (i.e. profiles or motifs), such as Position Specific Score Matrices (PSSM) <ref type="bibr" target="#b0">1</ref> or Hidden Markov Models 2 . These profiles can be used to iteratively search in a database search (e.g. PSIBLAST, jackhmmer) to detect remote homologies, allowing the discovery of protein clusters or families that are evolutionarily related. New query sequences may be aligned to existing profiles for identification of function. The alignment scores and "Evalue" can help indicate the degree of homology between the new sequence and existing sequences. Powerful and popular as these existing approaches are for protein function annotation directly from sequence, there may still be limited in classifying sequences coding for proteins with similar function or structure but are very distant in evolutionary scale or have come to adopt similar function via convergent evolution. For instance the proteases have independently evolved the "catalytic triad" active site in 23 protein superfamilies. The "catalytic triad" consists an acidbasenucleophile configuration of three amino acids arranged in spatial proximity but can be distant on the sequence. Given the difficulty in accurate prediction of threedimensional protein folding, the "catalytic triad" is difficult to predict based on sequence alignment. Here I present the application of machine learning using recurrent neural networks, recently gaining popularity and successes for natural languages processing, to capture highdimensional, complex patterns in biological sequences in order to predict protein functions, potentially beyond the capability of current methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>The deep learning recurrent neural network (RNN) model for protein function prediction is trained on a large set of protein sequences with certain known functions as labels. The training process tunes the parameters of the network by minimizing prediction errors (categorical entropy). After validating good prediction performance of the trained network using a "test" dataset of randomly chosen sequences of proteins with known functions but have never been seen during training, new sequences with unknown functions are fed to the network to make predictions of function (Figure <ref type="figure" target="#fig_2">1a</ref>).</p><p>The predicted function is eventually validated by experimental assay. Furthermore, RNN models could predict certain phylogenetically distinct "outofclass" protein families with similar function, albeit with worse sensitivity and selectivity.</p><p>The recurrent neural network (RNN) model contains one or more sets of "bi directional" recurrent layers with longshort term memory (LSTM) neurons processing the input sequence one residue or character at a time (Figure <ref type="figure" target="#fig_2">1b</ref>). The forward layer scans the protein sequences from the N towards the Cterminus and reversed for the backward layer, allowing the network to make use of context on both sides of each position rather than just what was seen before in a single direction. Each residue in the input protein sequence is converted into a "onehot" vector whose elements are all 0 except at the position of the amino acid it corresponds to, where it is set to 1. Each LSTM neuron in each recurrent layer uses input "i", output "o", gate "g", and forget "f" gates to modulate the input vector and update the neuron's internal cell state "c" and hidden state "h". The gates apply matrices, whose elements are adjustable parameters to be learned, on the input and hidden state vectors at each recurrent step/layer and subsequently normalize the results with nonlinear activation functions (Methods).</p><p>Intuitively, given each new input vector (i.e. sequence residue), the gates control what and how much to add to and output from the hidden state memory, which encodes sequence patterns relevant toward particular protein function (Figure <ref type="figure" target="#fig_2">1c</ref>). These features of the LSTM architecture allow the RNN to maintain, over many recurrent iterations, the magnitudes of both the relevant signals in feedforward propagation as well as the error gradients in backpropagation, thereby resolving the issues of loss of contextual memory and vanishing/exploding gradients that have limited the usefulness of traditional RNNs in processing long sequences (e.g. hundreds of units/iterations).</p><p>The outputs from the last LSTM neurons of the forward and reverse hidden layers are eventually fed to a fullyconnected layer of artificial neurons, where each neuron represents one functional class and outputs via the "softmax" activation function the probability that the input sequence represents a particular functional class. The number of recurrent hidden layers, LSTM neurons in each layer, hidden units (i.e. hidden state vector dimension) in each LSTM neuron, and the architecture of each LSTM neuron (e.g. "peephole" connections") are hyperparameters that can be optimized. For example, stacking several recurrent layers by feeding the output of each LSTM neuron in one recurrent layer as input into an adjacent recurrent layer, or increasing the number of neurons and hidden units, enable more complex or hierarchical representations at the risk of overfitting. Furthermore, the number of output neurons, which represents the number of functions to be simultaneously considered (i.e. multiplex), can be varied. In this work, a single set of bidirectional recurrent layers was utilized for inclass predictions, and up to three sets were used for training toward outofclass predictions.</p><p>As protein sequences vary widely in length, the number of LSTM neurons in the recurrent layer was capped, typically at 333 representing a maximum of 333 amino acids sequence or around 1 kilobase of DNA. For proteins smaller than 333 residues the sequence was prepadded with 0's up to a 333digit sequence, where the digits 1 to 20 represents the 20 canonical amino acids. For functions with mostly large proteins such as the CRISPRassociated nucleases, up to 800 Nterminal amino acids were input for training, and subsequently the same RNN model was trained on up to 800 C terminal amino acids. There are 128 hidden units in each LSTM neuron (i.e. the hidden state is represented by a 128 element vector). A high dimensional hidden state vector can encode more information to represent more complex functionrelated sequence features. This can be an advantage compared to some sequential models (e.g. hidden Markov model) with limited number of internal states at the expense of interpretability.</p><p>Additionally, the multiple nonlinear operators of the LSTM (e.g. activation functions) allow complex updating of the hidden state memory. Adding to this flexibility, the probability of "Dropout", the random severing of connections between layers, was consistently set to 0.5. Unlike previous artificial neural network based methods, the LSTM model here does not limit itself to learning short "profiles" or motifs of predefined length <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> (e.g. 21 amino acid window 3 ) but instead learns from the entire sequence up to a maximum length (e.g. 333, 500 or 800 from each terminus) in order to capture potentially longrange patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-class Model Training and Validation</head><p>For training, protein aminoacid sequences were obtained from the UniProt database and directly used as inputs into the neural network without any feature extraction. For prediction of a particular protein function, the positive class contains all sequences that match the function in UniProt by keyword. The negative class contains all nonmatching sequences in SwissProt (the manually reviewed database within UniProt with currently around 550K sequences). Of the combined dataset, 80% was randomly selected and employed as "trainset" for training the neural network, and the remaining 20% was used as the "testset" to evaluate the trained model's performance on yetunseen dataset. As the negative class generally greatly outnumbers the positive class, it was divided into 4 or more chunks to train against the positive class sequentially for classbalance. Each chunk of the negative set combined with the positive set was trained for at least 5 epochs (i.e. passes over the entire dataset) during which the categorical entropy of the predicted output compared against the expected output was minimized via the ADAM optimizer <ref type="bibr" target="#b5">6</ref> with the minibatch sampling size set to 64. Ten percent of the total data within the training dataset was used to monitor the network losses and changes in prediction accuracy during each training step.</p><p>Furthermore after each chunk had been trained, the prediction performance on the "testset" was evaluated to calculate the accuracy, precision, recall and F1 (Fmeasure)</p><p>for the positive and negative classes. The "testset" data was initially selected and mixed at random without applying classbalance in order to mimic reallife operations when the positive class is heavily underrepresented.</p><p>Four functional classes were picked to test the performance of the RNN predictive model: iron sequestering proteins (class "Ferritin"), cytochrome P450 proteins (class "P450"), serine and cysteine proteases (class "Protease") and Gprotein coupled receptors (class "GPCR"). Iron sequestration is crucial for cells to maintain iron homeostasis and protect against ROS generation from ironcatalyzed Fenton reactions <ref type="bibr" target="#b6">7</ref> .</p><p>Currently wellknown iron sequesters across domains of life are ferritins and dps (DNA binding protein from starved cells) proteins which form protein cages in cells that sequester and mineralize iron into inorganic nanoparticles. In addition to detoxification, the iron oxide nanoparticles synthesized could potentially be utilized towards noninvasive applications in biology such as a reporter or contrast agent for magnetic resonance imaging <ref type="bibr" target="#b6">7</ref> . The iron sequestration and magnetic properties of the proteins can be experimentally validated using cellular assays <ref type="bibr" target="#b7">8</ref> . P450 proteins are also ubiquitous across kingdoms of life and are enzymes that act on a variety of substrates carrying out important tasks including detoxifying drugs in humans. Gprotein coupled receptors are important transmembrane proteins for cellular signal transduction and are targets for many drugs. Lastly, serine and cysteine proteases cleave peptide bonds in proteins to break them down and represent a prime example of molecularscale convergent evolution, where different organisms independently evolved the "catalytic triad" for performing the peptide cleavage function with otherwise little homology at the overall protein sequence level.</p><p>High performance of predictions on the randomly leftout "testset" data not seen by the model during training was obtained for all four classes of protein functions (Figure <ref type="figure" target="#fig_3">2a</ref>). Even though accuracy is nearly 100% for all predictors, it is not the most informative measure as the negative class of proteins not possessing a particular function vastly outnumbers the positive class and a predictor could achieve high accuracy by simply only predicting negatives. But despite such challenge of "finding needle in a haystack", all functional predictors were able to achieve close to unity precision and recall in identifying the correct sequences from the "testset", with F1 measure close to unity. The receiver operating characteristic (ROC) plots for the True Positive Rate (sensitivity) versus False Positive Rate as a function of the classification threshold (between 0 and 1) and their Area Under the Curve (AUC) close to unity also demonstrates the model's ability to make strong discrimination of the positive class distribution from that of the negative class in the tested dataset (Figure <ref type="figure" target="#fig_3">2b</ref>). However, it is important to note that these metrics do not readily apply to prediction on arbitrary datasets, particularly large databases where class imbalance (ratio of negatives to positives) is extreme due to the negligible fraction of total proteins that have one specific function, and RNN performance may be negatively impacted. Also very low false positive rate (e.g. 1E6) would be needed to avoid large number of false positives when searching a large database (e.g. 54 million sequences in UniRef100). Lastly, as anticipated, the prediction performance in precision and recall decreases as the cutoff length of the input sequence or equivalently the depth of the bidirectional recurrent layer was decreased as demonstrated for the "Ferritin" class (Figure <ref type="figure" target="#fig_3">2c, d</ref>), even though reducing neural network depth increases training speed. Allowing input sequence length greater than 333 amino acids significantly increases processing and memory requirements without yielding noticeable increases in prediction performance for the four protein functions of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database Search and Prediction</head><p>The trained and performancevalidated models were used to predict whether a new sequence without assigned function could possess a potential function. Currently, stateofart tools for remote homology search include PSIBlast, DeltaBlast and in particular jackhmmer (part of HMMER 2 ) which utilizes Hidden Markov Models. For comparison, HMMER and the RNN models were run on the same comprehensive UniRef100 sequence database containing numerous uncharacterized or unannotated proteins sequences. For each of the four functional classes (Ferritin, P450, Protease, GPCR), a representative or important member was used (FTNA_ECOLI, CP21A_HUMAN, SEPR_HUMAN, FFAR2_HUMAN), respectively, as initial seed for iterative HMMER (jackhmmer) search on the UniRef100 database, and at least 5 iterations were run with a reporting cutoff threshold of evalue E=10.0 (default).</p><p>Separately, the trained RNN models also predicted thousands of new hits from the UniRef100 database for each function (Figure <ref type="figure" target="#fig_4">3a</ref>, "Predict") in addition to the thousands of sequences that were used for training each model. Upon comparing the lists of outputs from HMMER to the RNN models discounting the alreadyannotated sequences used for training, there were still thousands of new, unique sequences predicted by the RNN model that were not shared by the HMMER output (Figure <ref type="figure" target="#fig_4">3a</ref>, "Unique"). As a check, the majority of additional sequences predicted by the RNN model have some identification of the correct family or gene ontology in a public database obtained by other sequence or structure homology detection techniques (Figure <ref type="figure" target="#fig_4">3b</ref>). However, there is a further subset of the predicted sequences that are unannotated and uncharacterized in UniProt (Figure <ref type="figure" target="#fig_4">3a</ref>, "No Annot."). For the "Ferritin" class, the "No Annot." sequences predicted by the RNN show numerous lineages after multiple sequence alignment by Clustal Omega (using EMBLEBI server), suggesting a set of diverse, dissimilar sequences not sharing obvious sequence patterns identifiable by alignment (Figure <ref type="figure" target="#fig_4">3c</ref>). The statistics of the domains of origin for the predicted proteins reveal certain domain biases for function, such as bacteria for "Ferritin" class or eukaryote for P450 and GPCR, as expected (Figure <ref type="figure" target="#fig_4">3d</ref>). Similar biases could be seen for the "Ferritin" and "P450" classes in the taxonomy of the organisms of origin for the predicted proteins (Figure <ref type="figure" target="#fig_4">3e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Validation of Predicted Function</head><p>To validate the functional prediction by the RNN model of sequences without characterization or annotation in UniProt, I experimentally characterized the iron sequestration properties of ten "unique" candidates predicted by the RNN model for iron sequestration proteins. The ten sequences were selected from diverse domains of life and vary widely in their amino acid lengths and composition (Figure <ref type="figure" target="#fig_5">4a</ref>). The candidates were named after their biological contexts. Homology search with these sequences as seeds using popular bioinformatics tools such as BLAST and jackhmmer using their web servers on the latest protein databases (NCBI nr, Reference Proteomes) yielded mostly proteins of unknown (only "predicted" or "hypothetical") and uncharacterized function. However, some functional homologues were identified. For the "fungi" candidate, both webbased BLAST and jackhmmer were able to detect "ferritinlike" homologues, corroborating the RNN prediction. On the other hand, candidates "human", "mouse", "potato", "cyano", "gut" or their BLAST/jackhmmer homologues showed few entry names suggestive of other functions such as "Alternative protein NCAM1 (neural cell adhesion molecule)" for the "human" candidate and "polyhomeotic like protein" for "mouse" candidate. This could have new implications for the biological activity, particularly of iron sequestration, for these uncharacterized sequences. The remaining candidates "lancelet", "virus", "algae" and "archaea" yielded no hint of protein function.</p><p>The DNA sequences encoding all 10 uncharacterized proteins were codon optimized, synthesized, and cloned into vectors in E. coli cells and expressed highly using a rhamnoseinducible, high copy number vector. The E. coli cells simultaneously contain a fluorescent, genetic iron sensor based on the E. coli fiu promoter that has been validated to detect intracellular iron depletion (Chapter 3). Using calibration by iron chelator bipyridine, the fluorescence values could be converted to equivalent intracellular free iron concentrations. After induction of recombinant protein expression during exponential growth phase followed by overnight growth to saturation in LB media supplemented with 100μM Fe (II) sulfate, the cells were characterized for their fluorescence by the green fluorescent protein (GFP) reporter. All ten proteins showed statistically significant increases in fluorescence, or equivalently decreases in cellular free iron concentrations upon protein expression relative to no expression/induction (P value &lt; 0.05 by twotailed Student's ttest) (Figure <ref type="figure" target="#fig_5">4b</ref>). However, the protein derived from "potato" did not dramatically change the concentrations compared to the others. To determine the proteins' ability to not only bind and sequester iron but also to bio mineralize similar to the ferritins and dps proteins, I measured the retention level of the proteinexpressing cells in highgradient magnetic separation columns, as iron based minerals could increase magnetic moment of the cells. Some of the proteins tested, particularly "algae", "human" and "archaea", demonstrated increased magnetic retention compared to the uninduced control (Figure <ref type="figure" target="#fig_5">4c</ref>). The expression of some of these proteins including "algae", "archaea", "virus", and the nonsequestering "potato" were clearly observed by SDSPAGE gel (Figure <ref type="figure" target="#fig_5">4d</ref>). The inability to observe bands for candidates "human" and "mouse" may be due to their very low molecular weight (predicted &lt;10kD). Furthermore, the impact of mutations to the predicted sequences on the desired ironsequestering function could be analyzed using the same trained RNN model in silico in the manner of "saturation mutagenesis" where residue position of a sequence is mutated to every other base. The resulting impacts are illustrated in heat maps with the residue positions along the sequence along the horizontal axis and the 20 canonical amino acids along the vertical axis. The negative impacts are illustrated as red and positive impacts as green. In this manner, residues "conserved" for function are easily identified by the "red columns" (Figure <ref type="figure" target="#fig_5">4e</ref>). Furthermore, a "red row" at proline illustrates the potential helixbreaking and structuredisrupting property of proline, a chemical property that the RNN model has learned only from sequence information without a priori chemical knowledge. Further experimental testing of such mutations could enable further validation and optimization of the RNN model. Lastly, homology modeling of some of the predicted candidates using ITASSER 9</p><p>, the top structure prediction method in the CASP competition in 2012 and 2014, reveals diverse structures. Therefore, the model is not predictive of a particular protein fold or structure but other sequencebased features associated with function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of RNN to Other Machine Learning Methods</head><p>For machinelearning benchmark, the performance of the RNN model was compared against other popular machine learning classification models, particularly logistic regression and random forest which are known for speed, robustness and often good predictability. Furthermore, both algorithms are capable of modelling nonlinear relationships as would be expected between protein sequences and functions that would not be accurately captured by other fast machine learning methods such as linear regression. For all of these models, a set of features or independent variables are required. Using the same dataset for each of the four functional classes, 51 ProtParam features (Table <ref type="table">S1</ref>) were extracted or calculated for each sequence and vectorized.</p><p>These features include simple amino acid composition and length as well as biochemically relevant properties such as isoelectric point, molecular weight, stability index, hydrophobicity and grand average of hydropathicity (gravy). The logistic regression and random forest models were each trained using "gridsearch" over a range of values for their model hyperparameters, such as alpha for logistic regression, and the parameter values that produced the best prediction results were selected.</p><p>Comparing the "inclass" prediction performance on the four functional classes by all the machine learning methods, logistic regression was by far the fastest to train but also the least predictive (Figure <ref type="figure">5</ref>). While random forest was slower, it achieved much better performance but still outclassed by the near perfect performance of the RNN model on the same dataset. Nonetheless, the "feature importance" of random forest models calculated for the four predictors on the 51 features reveals different biases toward different functional classes (Figure <ref type="figure">5</ref>). The RNN model could not be simply interpreted based on these predefined features, but their bestinclass performance without "feature engineering", like in other successful deeplearning applications, demonstrate their potential to represent and capture nontrivial and difficulttoquantify patterns or relationship between sequence information and protein function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-of-class Training and Prediction</head><p>Lastly "outofclass" prediction performance was tested, whereby the RNN models were trained on sequences from certain protein families and tested on other functionally homologous but phylogenetically distinct families. One drawback of the random splitting of UniProt dataset into train and test sets employed so far is that the two sets could contain highly similar or even identical sequences that represent homologous proteins from closely related species. Furthermore, the ability to discover proteins with homologous function that are distant in evolution from what are already known could be valuable both for studying sequence evolution as well as mining for novel proteins for particular applications like genomeediting. Here I conducted "outof class" prediction test on three functions, "GenomeEdit", "Ferritin", and "P450". The Taking into account the different length distributions of the protein families, the maximum recurrent depth (i.e. sequencelength) was capped at 333 for "Ferritin", 500 for P450, and 800 for "GenomeEdit". To remove possible false positives in the training sets, sequences shorter than 10 amino acids or longer than 1000 amino acids for "Ferritin", "P450" functions, or 2000 amino acids for "GenomeEdit", were filtered out before training. As the "GenomeEdit" Cas9 or Cpf1 enzyme sequences are typically over 1000 amino acids long, the RNN was trained scanning over up to the first 800 amino acids from the Nterminus and subsequently from the Cterminus. Overall, prediction performance varied more substantially among the outofclass predictors compared to the previous random, inclass prediction performance (Table <ref type="table">1</ref>). Decent detection sensitivities were achieved with the leftout P450 families and for detecting bfr after training on nonhaem ferritins and dps. However, sensitivity/recall was low (0.13) for detection of the 12member caged dps from RNNs trained only on the 24member caged nonhaem ferritins and bfr. Tripling the number of recurrent layers by feeding the output sequence of one layer as input into the next, which produced a slower but deeper model with potential to encode more complex sequence patterns, increased sensitivity for detecting dps from 0.13 to 0.36 without decreasing precision. Lastly, prediction performance on Cpf1 from an RNN trained on Cas9 yielded sensitivity/recall of 0.59 after training on both N and C terminal residues (up to 800 amino acids) and averaging the prediction probabilities of processing the sequence from its two termini for final classification. Interestingly, classifying using predicted probabilities for only the N or Cterminal residues (up to 800) significantly decreased precision (i.e. increased false positives), suggesting that multiple features along the entire sequence length (e.g. the binding and nuclease domains) may be required toward accomplishing the "Genome</p><p>Edit" function and that many other proteins may exist with only a subset of those features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In summary, this study has shown that recurrent neural network (RNN) based on LSTM can be trained to classify certain protein functions with high level of accuracy from input amino acid sequences alone. Experimental validation of the predicted iron sequestering or mineralizing proteins including some currently not easily identified by other bioinformatics methods confirm the accuracy and utility of the model for prediction.</p><p>Compared against popular sequence prediction and analysis tools such as BLAST and HMMER, the RNN model currently has several potential benefits but also limitations. One important benefit is the potential to capture obscure sequencefunction relationships, allowing predictions of very remote homologies. Unlike most sequence search tools, RNN models do not explicitly rely on sequence alignments or heuristic scoring functions or similarity measures. The memory or internal state of the LSTM neuron processing entire protein sequences, unlike other machine learning methods that employ short, predefined motif windows <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> , allows selective retention of important sequence features across long distances <ref type="bibr" target="#b13">14</ref> . For instance, residues that make up an active site of an enzyme may be separated by large gaps in the protein sequence, but are in proximity of each other in 3dimensional space. Despite much advances in recent years, the folded structure of proteins still cannot be reliably predicted from their primary amino acid sequences, which limits the prediction of protein function most often highly related to the structure. In this work, four important functional classes were selected which includes as their members proteins across domains of life that share little homology, or have converged upon the same function without common evolutionary origin as in the catalytic triad of the proteases. The ability of the RNN model to accurately make predictions for all of these functional class from only primary sequence without structural information suggests that the RNN could represent complex patterns in the protein sequence that encode for function. However, it is important to note that the "inclass" performance measures obtained from testing on randomly selected sequences from a small and predominantly reviewed dataset (fewer than 1 million sequences) may not hold for testing on arbitrary databases (e.g. UniRef100 with 54 million sequences). In the larger databases, the proportion of members with particular function (i.e. the positive class) can be extremely small. As a result, very high performance is demanded, with false positive rate approaching zero to avoid large number of false positive predictions. The "inclass" performance, though respectable, will require calibration on the same test databases for comparisons against current stateofart (e.g. BLAST). Furthermore, the "inclass" predictors' performance may partially benefit from the high similarity or possible redundancy of sequences representing homologous proteins in closely related species randomly partitioned into the training and testing sets. The "outofclass" predictors tested on phylogenetically distinct families showed lower performance as expected. Therefore, while the RNN models can be sensitive toward new protein families with functional homology, further optimizations are necessary to improve their sensitivity and selectivity particularly for this difficult task of discovering new protein families with related functions in the large and growing sequence databases.</p><p>As a "deeplearning" model, RNN with LSTM has found success in several domains related to sequence learning, particularly language recognition and modelling, that surpassed the performance of other machine learning models particularly for learning directly from raw data <ref type="bibr" target="#b14">15</ref> . However, a current limitation of using RNN with particularly deep layers (e.g. long sequences) is the training and processing speed. This is mainly because of the large number of variables in a deep neural network model which requires training with large datasets and many operations on large matrices in the iterative optimization steps using the relatively slow gradient based, backpropagation techniques. Building Position Specific Scoring Matrices (PSSM) for PSIBLAST or hidden Markov models for HMMER as well as searching against those models can be performed faster currently on the public servers.</p><p>Besides currently limited computing power, another limitation at the present is the data itself. While there is abundant data for accurate training for the functions of iron mineralizing proteins, cytochrome P450s, proteases and GPCRs, there are some functions of interest that at the present do not yet have sufficient data size to produce highly predictive models. For example, in the last few years there has been exploding amount of interest and applications of oligonucleotidetargeted nucleases for genome editing across a variety of systems. The CRISPR (Clustered Regularly Interspaced Short Palindromic Repeats) system originated from bacteria Streptococcus pyogenes has been particularly successful in efficient genome editing across a variety of cell types including human cell lines <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref> . And in recent years new systems of similar function are continuously discovered via bioinformatics techniques for remotehomology prediction such as PSIBLAST 1</p><p>. It is of great interest to discover the whole diversity of oligonucleotidetargeted nucleases for future enhancement of genome editing applications. While there may already be some that are homologous to the known CRISPR systems by sequence or structure, there are potentially more in Nature with more remote homology not detectable by the PSIBLAST or HMMER. The deep learning approach here employing RNN has the potential to detect those remote candidates. However, the main challenge currently is the limited amount of public data for creating the training set, as fewer than 10,000 CRISPR/Cas9 like nucleases have been identified. Additionally, unlike the ironmineralizing ferritins or P450s, these guided nucleases so far identified are mostly large proteins with relatively long sequences of more than 1000 amino acids. Long sequences have been particularly challenging for RNN training due to the exploding or vanishing gradient issue with backpropagation.</p><p>The use of LSTM neurons allowing selective retention and forgetting of information has ameliorated the issue, but training very long sequences would require significantly more computational processing power and memory. Given significantly more computational resources and time, results here have shown that deeper RNN models could be trained on the currently available dataset to make reasonable predictions (Table <ref type="table">1</ref>). However, both sensitivity and selectivity could be optimized with training on the growing volume of experimental data in order to more accurately and precisely discover new functional candidates or protein families and demonstrate utility and power of the RNN predictors over the current stateofart (e.g. PSIBLAST).</p><p>Despite current limitations in speed and data availability for certain functional prediction applications, RNNbased deeplearning models have the potential to overcome these obstacles quickly in the coming years to become more widely applicable enabled by three trends. On the speed side, both the cost and performance of computing are improving rapidly, particularly due to the design and deployment of highlyparallelized processing architectures (e.g. graphic computing units) that are particularly well suited and have been increasingly dedicated toward training deep neural networks. On the data side, increasingly large volumes of data are collected from automated, highthroughput experimentation. In the field of synthetic biology, first the cost of sequencing and now of synthesis of DNA has been decreasing dramatically.</p><p>Large throughput sequencing, particularly of hardtoculture environmental samples in metagenomics, has rapidly increased the database of sequences available for mining new proteins and new functions. Meanwhile, the accessibility of DNA synthesis has made it possible to quickly test new sequences of interest in relevant biological contexts and obtain valuable data such as those related to protein functions. As more validation data become available, the deeplearning model can be further trained to become more powerful at predicting desired functions. As the RNN is agnostic to the specific biological nature of the sequence, it can be potentially useful for analyzing other biological sequences besides aminoacids (e.g. RNA). Furthermore, as RNN can be a generative model, it can be trained on proteins of a particular functional class with an autoencoder and use the decoder to write new protein sequences that may possess that function. This is currently done for translating human languages <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> due to the abundance of data. It may be foreseeably applied to protein sequences in the future as the amount of data increases, but it will be significantly more challenging due to requiring the RNN model to learn and remember not only sufficient patterns for classification of certain functions but also everything else that makes a functional protein, as often even few mutations unrelated to a particular function could cause proteins to misfold. At the very least, much deeper RNN models (with numerous stacked recurrent layers) and large hidden state vectors that are capable of storing more information, along with ample training dataset for not only particular function but also for other essential aspects such as proper protein folding, will be necessary to accomplish de novo protein "writing". Lastly on the theoretical side, the convergence of artificial neural networks (ANN) research with the field of neuroscience where it first drew its inspiration could lead to potentially better model or computing architectures that improve both the speed and accuracy of the artificial recurrent neuralnet based predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Modelling</head><p>All computational models were written in Python and processed on the Harvard Odyssey computing cluster at Harvard University using a combination of CPU and GPU computing nodes. The recurrent neural network models were built upon the Google Tensorflow backend. The logistic regression and random forest models were built using the Python scikitlearn packages. HMMER v3.1b1 (jackhmmer tool) was deployed and executed also on the Odyssey cluster. Protein sequence and function data were obtained directly from the UniProt databases (www.uniprot.org)</p><p>For each LSTM Neuron in the RNN, its input "i", output "o", gate "g", forget "f", cell state "c" and hidden state "h" values at time "t" are determined by the following equations <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18</ref> : The performances reported in the main text and figures consider the proteins containing a particular function, the minority class, as "Positive" whereas the rest are "Negative".</p><formula xml:id="formula_0">= ( ( ) + ℎ + ) = ( ) + ℎ + = tanh ( ) + ℎ + = ∘ + ∘ = ( ( ) + ℎ + ) ℎ = ∘ tanh( ) ( ) =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLAST and HMMER search of experimentally validated RNN predictions</head><p>Default setting were used for NCBI BLAST and EMBLEBI jackhmmer on their web servers for searching the ten RNN predictions that were experimentally validated for possible functional homologs. Specifically for NCBI BLAST, the NCBI nonredundant protein sequences database was used for blastp. For jackhmmer run on the EMBLEBI server, the Reference Proteomes was used, and the CutOff thresholds were set at default values such that Significance Evalues was 0.01 for sequence and 0.03 for hit, while the Report Evalues were 1 for both Sequence and Hit. Jackhmmer was iterated until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construction of expression vectors for predicted protein candidates</head><p>Candidate genes for experimental validation were first synthesized as geneBlocks (gBlocks) according to their sequences. The Nterminal six methionine repeat sequence of "human" was synthesized with only the last methionine due to DNA synthesis difficulty of ATG repeats and the possibility of product from translational start at the last methionine. The gBlocks were then cloned into a high copynumber plasmid (pUC origin of replication) with rhamnose inducible promoter (rhaPBAD, with native E. coli transcription factors RhaS and RhaR) and kanamycin resistance cassette via Gibson Assembly. The DNA plasmid was verified by Sanger Sequencing (Genewiz) and transformed into E. coli BW25113 cells via electroporation. Protein expression was induced in cells by adding rhamnose to cell culture (maximum 0.2%) during logphase growth (OD600~0.4). DNA sequences of the most relevant genes and constructs can be found in Table <ref type="table">S2</ref> in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iron level characterization by genetic sensor</head><p>For the genetic iron sensor, the E. coli fiu promoter was cloned along with a superfolder GFP (sfGFP) reporter via Gibson Assembly into a low copy (p15A origin), chloramphenicolresistance plasmid compatible with the ferritinexpressing plasmid. Iron levels were measured for cells containing the proteinexpression and iron sensor plasmids by taking the GFP fluorescence of the culture of cells (488nm excitation by laser, 512nm emission) in 96well plate format using the BioTek NEO platereader. For calibration, known concentrations of iron sequesterer bipyridine were added to cell cultures. The fluorescence measured were normalized to culture density by dividing by OD600 measured by the same platereader. The increase in normalized fluorescence of the cells was plotted against the increase in bipyridine (or consequent decrease in free iron) and modeled to determine the conversion between fluorescence reading and free iron concentration <ref type="bibr" target="#b7">8</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Magnetic Column Retention characterization</head><p>A highgradient magnetic column (Miltenyi LD columns) was sandwiched between two neodymium permanent magnets (K&amp;J Magnetics Inc., BX8C4N52) to create high magnetic field gradients inside the column. The column was first wetted by passage of 2ml of PBS 1X buffer. Then 500μl of cells resuspended in PBS 1X buffer were added and flowed through by gravity into the elution tube, followed by addition of 3ml of PBS 1X buffer to wash through any unbound cells into the elution tube. Once dry, the column was removed from the magnets, and 3ml of PBS buffer was pushed through the column to extract the magnetically retained cells into a separate retention tube. Measuring OD600 of the elution and retention tubes allow estimation of cell counts and the percentage of total cells retained by the magnetic column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDS gel analysis of protein expression</head><p>E. coli cells were resuspended in SDS Buffer (NuPAGE LDS Buffer) with reducing agent, followed by two cycles of boiling at 95°C for 5 minutes and vigorous vortex to lyse cells and denature proteins. The lysate was centrifuged to pellet cell debris, and the protein suspension was diluted and added to NuPAGE 412% BisTris gel with MES buffer. Empty lanes in the gel were filled with equal volume of SDS buffer. After running at 200V for 35 minutes, the gel was removed and stained with Coomassie Orange dye for one hour and subsequently imaged for dye fluorescence on a Typhoon Imager.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>10 .</head><label>10</label><figDesc>negative set for both training and testing was again the reviewed SwissProt database excluding members containing function of interest. For the "GenomeEdit" function, RNN was trained on the InterPro Cas9 family of proteins (IPR028629, 1201 sequences) as positive set and tested on the InterPro Cpf1 family of proteins (IPR027620, 55 sequences) Both Cas9 and Cpf1 are guided nucleases associated with the CRISPR locus 1112,13 . Cpf1 was discovered more recently and confer benefits such as not requiring a "tracrDNA" for targeting and potentially higher specificity. Due to the scarcity of the positive training set (Cas9 family) relative to the set of negatives (&gt;550,000 in SwissProt outside of Cas9 and Cpf1 family), the negative set was divided into 100 chunks and sequentially trained with the same positive set (Cas9 family). Such classbalancing or undersampling during training was not applied during testing on the Cpf1 to more closely simulate the naturally small fraction of positives in a database. For the "Ferritin" function, RNN was trained on the InterPro nonhaem ferritin family (IPR001519) along with either the haemcontaining bacterioferritin family bfr (IPR002024) or the DNA binding protein dps family (IPR002177) as positives, and tested on the remaining un trained family. The dps differs from the ferritins or bacterioferritins prominently in assembling a cage of 12 rather than 24 monomers. The "P450" function is represented by 6 different sequence clusters/families in InterPro: Bclass (IPR002397), Eclass CYP24Amitochondrial (IPR002949), EclassgroupI (IPR002401), EclassgroupII (IPR002402), EclassgroupIV (IPR002403) and mitochondrial (IPR002399). Either the Bclass (31205 sequences) or EclassgroupII (2314 sequences) was treated as the testset, with training of RNN using the combination of the other families as positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 1 +F1 = 2 x</head><label>12</label><figDesc>exp (− ) , where W represents weight matrix, b represents constant bias, D represents dropout (sets value to zero with probability p, p=0 in this study), ∘ represents elementwise multiplication (Hadamard product), and tanh represents hyperbolic tangent function. For evaluation of machine learning performance, the metrics are computed from the number of True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN) as follows: Accuracy = (TP + TN) / (TP + TN + FP + FN) Precision = TP / (TP + FP) Recall = TP / (TP + FN) Precision x Recall / (Precision + Recall) True Positive Rate (Sensitivity) = TP / (TP + FN) = Recall False Positive Rate = FP / (FP + TN) The Receiver Operating Characteristic (ROC) is plotted for the True Positive Rate against the False Positive Rate as classification threshold is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FiguresFigure 1</head><label>1</label><figDesc>Figures</figDesc><graphic coords="32,72.00,130.44,450.96,188.52" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 RNN</head><label>2</label><figDesc>Figure 2 RNN model achieves high prediction performance on randomly left-out testing data (a) High prediction performance is achieved for all four tested classes: ironsequestering (Ferritin), cytochrome P450, protease (serine and cysteine) and G protein coupled receptor (GPCR). (b) Receiveroperating characteristic (ROC) of the four separate models demonstrate high AreaUndertheCurve (AUC). For the "Ferritin" class, prediction precision (c) and recall (d) both improve to close to unity as the length of amino acid sequence shown the network increases, saturating around 333 letters.</figDesc><graphic url="image-3.png" coords="33,72.00,72.00,450.96,277.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Trained RNN model predicts new annotations (a) Table listing for each function, the number of sequences used for training the RNN model, the number of additional sequences it predicted as positive in the UniRef100 database ("Predict"), the number of sequences not included in output of jackhmmer (iterative HMMER search) using representative starting sequence ("Unique"), and the number of sequences without any function or family annotation on UniProt and linked databases (Gene3D, InterPro, PROSITE, Pfam, SUPFAM) ("No Annot."). (b) Among the "Predict" sequences, high percentages agree with manually curated SwissProt annotation for expected gene ontology of each class. Agreement is worse for the automatic annotations in TrEMBL database particularly for "Ferritin" and "GPCR" functions. (c) Clustal Omega multiple sequence alignment of the "No Annot." sequences for "Ferritin" function shows diverse lineages. (d) Taxonomy of "Predict" proteins reveals expected bias for functional class. (d) Taxonomy of the organism of origin for the "Predict" proteins for "Ferritin" (left) showing greater species diversity among bacteria (red) and "P450" (right) showing greater diversity among eukaryotic species (green).</figDesc><graphic coords="34,72.00,72.00,451.08,255.00" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Experimental validation of predicted iron sequestering proteins (a) List of ten proteins picked from diverse biological contexts without annotation in UniProt predicted by RNN model to contain "Ferritin" like function (b) after cloning and expressing the proteins in E. coli with genetic iron sensor, the majority of the tested proteins demonstrated decreased cellular iron particularly for "algae", "human", "archaea" and "mouse". (P value &lt; 0.05 by twotailed Student's ttest. Three biological replicates in one experiment. Iron sensor functionality has been verified with controls</figDesc><graphic url="image-6.png" coords="35,72.00,72.00,451.08,538.20" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The author would like to acknowledge Professors Sean Eddy, Debora Marks and Pamela Silver at Harvard University for helpful discussions and feedback. The author would like to acknowledge the Harvard Odyssey Computing Cluster for providing the computational resources for this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest</head><p>The author declares no conflict of interest for this work.  The average of the predictions on up to 800 amino acids in the N and C termini significantly increased precision and F1, suggesting several important features throughout the entire sequence that are necessary for function.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gapped BLAST and PSIBLAST:a new generation of protein database search programs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HMMER web server: Interactive sequence similarity searching</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast modelbased protein homology detection without alignment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1728" to="1736" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Networks for FullScale Protein Sequence Classification: Sequence Encoding with Singular Value Decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mclarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="177" to="193" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting the sequence specificities of DNA and RNAbinding proteins by deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Biotechnol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980[cs.LG]1-15</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ferritin: A versatile building block for bionanotechnology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santos Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Rev</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1653" to="1701" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Engineering GeneticallyEncoded Mineralization and Magnetism via Directed Evolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep38019</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">38019</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ITASSER: a unified platform for automated protein structure and function prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Protoc</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="725" to="738" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">InterPro: The integrative protein signature database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="211" to="215" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cpf1 is a single RNAguided endonuclease of a Class 2 CRISPR Cas system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zetsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="759" to="771" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Programmable DualRNA-Guided DNA Endonuclease in Adaptive Bacterial Immunity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="page" from="816" to="822" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiplex Genome Engineering Using CRISPR/Cas Systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="page" from="819" to="824" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04069</idno>
		<title level="m">LSTM: A Search Space Odyssey</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation By Jointly Learning To Align and Translate</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01919</idno>
		<title level="m">Convolutional LSTM networks for subcellular localization of proteins</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Several candidates also gave rise to increased cellular magnetism (magnetic column retention) due to possible iron biomineralization compared to uninduced cells (3 biological replicates in one experiment.) (d) Bands for overexpressed proteins could be clearly observed for &quot;virus&quot;, &quot;algae&quot;, &quot;archaea&quot; and &quot;potato&quot; (did not demonstrate significant iron sequestration or magnetism) (e) in silico &quot;saturation mutagenesis&quot; of selected sequences using RNN model to predict effects of mutations on desired function (red=bad, yellow=neural, green=good), with residue position along horizontal axis and the 20 canonical amino acids along vertical axis. RNN model identifies key positions conserved for function (e.g. vertical arrow), and also the potentially structurebreaking mutations by mutation to proline (horizontal arrow) (f) structural homology models of protein candidates &quot;mouse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>replicated more than three times in lab. (top), &quot;archaea&quot; (middle) and &quot;algae&quot; (bottom) using I TASSER server (the top method in the recent CASP 2012, 2014 protein structure prediction competitions), showing diverse predicted structures</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
