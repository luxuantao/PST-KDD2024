<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yingmei</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">Microsoft STCA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
							<email>jpei@cs.sfu.ca</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">Microsoft STCA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingxing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
							<email>zywu@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<email>djiang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NLP Group</orgName>
								<orgName type="institution">Microsoft STCA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating noise in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various augmented methods. Those models provide supervision signals to each other. The experimental results show that our method outperforms the existing state of the art by 3.05 and 4.24 percentage points on two benchmark datasets, respectively. The code will be made open sourced on github.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spoken language understanding (SLU) is a key component in task-oriented dialogue systems. SLU consists of two subtasks: intent detection and slot tagging <ref type="bibr" target="#b36">(Wang et al., 2005;</ref><ref type="bibr">Tur and De Mori, 2011)</ref>. Although promising progress has been achieved on SLU in English <ref type="bibr" target="#b17">(Liu and Lane, 2016;</ref><ref type="bibr" target="#b26">Peng et al., 2020;</ref><ref type="bibr" target="#b11">Huang et al., 2020)</ref>, those methods need large amounts of training data, and thus cannot be applied to low-resource languages where zero or few training data is available.</p><p>In this paper, we target at the extreme setting for cross-lingual SLU where no labeled data in target languages is assumed, which is critical for industry practice, since annotating a large SLU dataset with high quality for every language is simply infeasible.</p><p>Existing cross-lingual transfer learning methods mainly build on pre-trained cross-lingual word embeddings <ref type="bibr" target="#b29">(Ruder et al., 2019)</ref> or contextual models <ref type="bibr" target="#b37">(Wu and Dredze, 2019;</ref><ref type="bibr" target="#b10">Huang et al., 2019;</ref><ref type="bibr" target="#b13">Lample and Conneau, 2019;</ref><ref type="bibr" target="#b1">Conneau et al., 2020)</ref>, which represent texts with similar meaning in different languages close to each other in a shared vector space. Those approaches often show good performance on intent detection. The results on slot tagging, however, are often unsatisfactory, especially for distant languages, which are dramatically different from English in scripts, morphology, or syntax <ref type="bibr" target="#b34">(Upadhyay et al., 2018;</ref><ref type="bibr" target="#b31">Schuster et al., 2019;</ref><ref type="bibr" target="#b15">Li et al., 2020a)</ref>.</p><p>Several studies <ref type="bibr" target="#b31">(Schuster et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2020b;</ref><ref type="bibr" target="#b15">Li et al., 2020a)</ref> show that adding translated data into the fine-tuning process of pretrained models can improve the results of cross-lingual SLU substantially when no golden-labeled training data in target languages is available. For example, machine translation can be employed to translate the training data in English into target languages, and some alignment methods, such as attention weights <ref type="bibr" target="#b31">(Schuster et al., 2019)</ref>, fastalign <ref type="bibr" target="#b5">(Dyer et al., 2013)</ref> or giza++ <ref type="bibr" target="#b25">(Och and Ney, 2003)</ref>, can be further applied to label the translated data. Another approach to alleviate the problem of data scarcity is to automatically generate training data. Recently, some methods for monolingual SLU <ref type="bibr" target="#b0">(Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b12">Kumar et al., 2020;</ref><ref type="bibr" target="#b40">Zhao et al., 2019;</ref><ref type="bibr" target="#b26">Peng et al., 2020)</ref> automatically label domain-specific data or use pre-trained language models to generate additional data.</p><p>However, the synthesized training data derived from both the translation approach and the data generation approach may be quite noisy and may contain errors in label. For the translation approach, both the translation process and the alignment process may generate errors <ref type="bibr" target="#b39">(Xu et al., 2020;</ref><ref type="bibr" target="#b16">Li et al., 2020b)</ref>. For the data generation approach, it is often hard to control a right tradeoff between generating correct but less diverse data and generalizing more diverse data but with more noise. Moreover, generating synthetic training data across languages further adds challenges to the robustness of the arXiv:2109.01583v1 [cs.CL] 3 Sep 2021 generation methods.</p><p>To filter out noise in the synthesized training data, a few methods are proposed, such as the mask mechanism <ref type="bibr" target="#b15">(Li et al., 2020a)</ref>, the soft alignment method <ref type="bibr" target="#b39">(Xu et al., 2020)</ref>, the unsupervised adaptation method <ref type="bibr" target="#b16">(Li et al., 2020b)</ref>, the rule-based filtering method <ref type="bibr" target="#b26">(Peng et al., 2020)</ref>, the classifier based filtering method <ref type="bibr" target="#b0">(Anaby-Tavor et al., 2020)</ref> and the language model score based filtering method <ref type="bibr" target="#b32">(Shakeri et al., 2020)</ref>. These methods rely on either adhoc rules or extra models. Although they have shown promising results, each of them considers only a single source for data augmentation. It is still challenging to differentiate noisy instances from useful ones, since all those instances are sampled from the same distribution generated by the same method.</p><p>In this paper, we regard both the translation approach and the generation approach as data augmentation methods. We tackle the problem of reducing the impact of noise in augmented data sets. We develop a principled method to learn from multiple noisy augmented data sets for cross-lingual SLU, where no golden-labeled target language data exists. Our major technical contribution consists of a series of denoising methods including instance relabeling, co-training and instance re-weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First,</head><p>motivated by the self-learning method <ref type="bibr" target="#b41">(Zoph et al., 2020)</ref>, we design a model-ensemble-based instance relabeling approach to correct the noisy labels of augmented training data in low resource languages. As teacher models can not always generate correct labels, the original self-learning method tends to suffer from accumulated errors caused by model predictions. To alleviate the problem of accumulated errors, in our instance relabeling approach, we use crowd-intelligence from multiple models to derive the more reliable labels of pseudo training instances. Besides, we filter out noisy instances based on co-training and instance re-weighting strategies to reduce the impact of incorrect predictions on subsequent training. Our training strategy does not follow a traditional teacher-student manner. Instead, we use model predictions in the last epoch as pseudo labels in the current epoch to compute loss which saves training time.</p><p>In order to filter out noisy instances, we adopt a co-training mechanism, which uses selected instances from the other models to train the current model. Different from the co-teaching method <ref type="bibr" target="#b9">(Han et al., 2018)</ref> where two models are trained with the same data, we propose multiple models should be trained with multiple different noisy augmented data where noise may be largely independent. It is because deep neural networks have a high capacity to fit noisy labels. When two models are trained with the same data, we tend to obtain two similar models. The co-teaching method gradually becomes a naive selection method with two models ensembled. In our co-training method, by employing very different data generation methods, we hope to attain that the noise from different sources may be largely independent and models can learn different knowledge from them. Therefore, the instances that pass the screening process of the other models can serve as the supervision signals to the current model which alleviates the problem of accumulated errors caused by selection bias.</p><p>Last, we further propose an instance reweighting technique to adjust the weights of training instances adaptively. As we do not have real training data in target languages, we can use the consistency among the soft labels predicted by different models to predict the reliability of the instances. Intuitively, if the predictions of different models are highly inconsistent on an instance, the instance may contain much noise. The larger deviation, the more uncertainty, and the less weight. This idea further increases the robustness of the selected training instances.</p><p>We conduct extensive experiments on two public datasets. The experiment results clearly indicate that, by consciously considering multiple noisy data sources derived from very different augmentation methods, our approach is more effective than using any single source. Our methods improve the state of the art (SOTA) by 3.05 and 4.24 percentage points on the two benchmark datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The cross-lingual spoken language understanding methods can be divided into two main categories: the model transfer methods and the data transfer methods.</p><p>The model transfer methods build on pre-trained cross-lingual models to learn language agnostic representations, such as MUSE <ref type="bibr" target="#b14">(Lample et al., 2017</ref><ref type="bibr">), CoVE (McCann et al., 2017)</ref>, mBERT <ref type="bibr" target="#b37">(Wu and Dredze, 2019)</ref>, XLM <ref type="bibr" target="#b13">(Lample and Conneau, 2019)</ref>, Unicoder <ref type="bibr" target="#b10">(Huang et al., 2019)</ref>, and XLM-R <ref type="bibr" target="#b1">(Conneau et al., 2020)</ref>. The English training data is applied to fine-tune the pre-trained models and then the fine-tuned models are directly applied to target languages <ref type="bibr" target="#b20">(Liu et al., 2020b;</ref><ref type="bibr" target="#b31">Schuster et al., 2019;</ref><ref type="bibr" target="#b27">Qin et al., 2020;</ref><ref type="bibr" target="#b34">Upadhyay et al., 2018;</ref><ref type="bibr" target="#b15">Li et al., 2020a)</ref>. To better align embeddings between source and target languages, <ref type="bibr" target="#b19">Liu et al. (2019)</ref> use domain-related word pairs and employ a latent variable model to cope with the variance of similar sentences across different languages. <ref type="bibr" target="#b20">Liu et al. (2020b)</ref> and <ref type="bibr" target="#b27">Qin et al. (2020)</ref> use parallel word pairs to construct code-switching data for fine-tuning. Their methods encourage the model to align similar words in different languages into the same space and attend to keywords. <ref type="bibr" target="#b21">Liu et al. (2020c)</ref> propose a regularization approach to align word-level and sentence-level representations across languages without any external resource.</p><p>The data transfer methods construct pseudolabeled data in target languages. These methods usually employ machine translators to translate training instances in a source language into target languages and then apply alignment methods, such as attention weights <ref type="bibr" target="#b31">(Schuster et al., 2019)</ref>, fastalign <ref type="bibr" target="#b5">(Dyer et al., 2013)</ref>, or giza++ <ref type="bibr" target="#b25">(Och and Ney, 2003)</ref>, to project slot labels to the target language side. The derived training instances are combined with the training data in the source language to fine-tune the pre-trained cross-lingual models. Previous studies <ref type="bibr" target="#b34">(Upadhyay et al., 2018;</ref><ref type="bibr" target="#b31">Schuster et al., 2019;</ref><ref type="bibr" target="#b15">Li et al., 2020a)</ref> show that adding translated training data can significantly improve the model performance, especially on languages which are distant from the source language.</p><p>The data generation approaches can also construct additional training data. Some methods <ref type="bibr" target="#b35">(Wang and Yang, 2015;</ref><ref type="bibr" target="#b23">Marivate and Sefara, 2020;</ref><ref type="bibr" target="#b7">Gao et al., 2020)</ref> make slight changes to the original training instances through word replacement or paraphrases. More sophisticated methods generate training data through large-scale neural networks, such as generative adversarial networks <ref type="bibr" target="#b8">(Goodfellow et al., 2020)</ref>, variational autoencoders <ref type="bibr" target="#b2">(Doersch, 2016;</ref><ref type="bibr" target="#b3">dos Santos Tanaka and Aranha, 2019;</ref><ref type="bibr" target="#b30">Russo et al., 2020)</ref>, and pre-trained language models <ref type="bibr">(Wu et al., 2019;</ref><ref type="bibr" target="#b0">Anaby-Tavor et al., 2020;</ref><ref type="bibr" target="#b12">Kumar et al., 2020;</ref><ref type="bibr" target="#b26">Peng et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we define the problem and then propose our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Solution Framework</head><p>The SLU task aims to parse user queries into a predefined semantic representation format. Formally, given an utterance x = {x i } L i=1 with a sequence of L tokens, a SLU model targets to produce an intent label y I for the whole utterance and a sequence of slot labels</p><formula xml:id="formula_0">y S = {y S i } L i=1</formula><p>, where y S i is the slot label for the i th token x i . Here, we target at the extreme cross-lingual setting where only some training data D src in English (or, in general, a rich-resource source language) and some development data D dev in English exist. Besides, some annotated data D test in target languages is used as the test set. Cross-lingual SLU is to learn a model by leveraging D src to perform well on D test , using D dev for parameter tuning.</p><p>We add a special token [CLS] in front of each input sequence. Then we feed x into an encoder M enc to obtain the contextual hidden representation H = {h i } L i=0 , that is, H = M enc (x; Θ), where Θ denotes the parameters of the encoder.</p><p>We take h 0 as the sentence representation for intent classification and take h i (1 ≤ i ≤ L) as the token representations for slot filling. We apply linear transformation and the softmax operation to obtain the intent probability distribution p I (x; Θ) and the slot probability distribution p S i (x; Θ), that is,</p><formula xml:id="formula_1">p I (x; Θ) = softmax(W I • h0 + b I ) p S i (x; Θ) = softmax(W S • hi + b S )<label>(1)</label></formula><p>where The overall architecture of our proposed method is shown in Figure <ref type="figure" target="#fig_1">1</ref>. It consists of two major modules, the data augmentation module and the denoising module.</p><formula xml:id="formula_2">p I ∈ R 1×|C I | , p S i ∈ R 1×|C S | , C I is the set of intent labels, C S is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Data Augmentation Module</head><p>In this module, we augment training data in target languages via translation and generation. The left part of Figure <ref type="figure" target="#fig_1">1</ref> shows the details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Translation</head><p>We use Google Translator to translate the training corpus D src in the source language (English) to the target languages. In addition to translation, we also need some word alignment methods to project the slot labels to the target language side. We try giza++ <ref type="bibr" target="#b25">(Och and Ney, 2003)</ref> and fastalign <ref type="bibr" target="#b5">(Dyer et al., 2013)</ref> to obtain word alignment information and find that the pseudo slot labels projected by giza++ generally lead to better performance (about 2% increase in F1 on the SNIPS dataset). Thus, in the rest of the paper, we use Google translator and giza++ to produce translated data. Denote by D trans the translated training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Generation</head><p>To further increase the diversity of synthesized training data, we leverage multilingual BART (mBART) <ref type="bibr" target="#b18">(Liu et al., 2020a)</ref> as the generator to synthesize additional target language training corpus. Specifically, we first fine-tune the pretrained mBART model on the translated training data D trans by adopting the denoising objective <ref type="bibr" target="#b18">(Liu et al., 2020a)</ref> -the cross-entropy loss between the decoder's output and the original input. The input to mBART consists of the dialog act and the utterance in D trans , defined by</p><formula xml:id="formula_3">X = [I; (s 1 , v 1 ), ..., (s p , v p ); (x 1 , ..., x L )] (2)</formula><p>where I is the intent and (s i , v i ) p i=1 are the slotvalue pairs. Here, v i in the target language is obtained by word alignment between utterances in the source and the target language. Following <ref type="bibr" target="#b18">Liu et al. (2020a)</ref>, we apply text infilling as the injected noise in the fine-tuning stage.</p><p>After fine-tuning, we apply the same noise function to the input data X and leverage the finetuned mBART to generate m candidates for each instance. To increase diversity of generated data, the top-p sampling strategy <ref type="bibr" target="#b6">(Fan et al., 2018</ref>) is adopted. Each generated instance consists of "an utterance, the corresponding intent and slot-value pairs". Then, we perform preliminary data selection by filtering out the generated utterances not containing the required slot values. Last, we randomly sample a instances from the candidate set for each input to construct the generated corpus D gen .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Denoising Module</head><p>To tackle the noisy label problem introduced by the data augmentation module, we design a denoising module shown in Algorithm 1.</p><p>At the initialization stage, we first train K models using the augmented data derived from K different augmentation methods. All models are optimized by the cross-entropy loss function computed using the original labels of intent and slots. For the k-th model (k ≤ K),</p><formula xml:id="formula_4">L k (x) = − 1 L L j=1 y S j logp S j (x; Θ k )−y I logp I (x; Θ k ) (3)</formula><p>where x is a training utterance, y I is the intent label, y S j is the slot label of the j-th word in the utterance, and p I (x; Θ k ) and p S j (x; Θ k ) are the predicted probability distributions of the intent and the slot, respectively.</p><p>To keep our discussion simple, in this paper, we mainly consider using the training corpora derived from machine translation and generation. Thus, we maintain K = 2 SLU networks with the same structure in the training process. M 1 and M 2 are trained using different training corpora D 1 = {D src , D trans } and D 2 = {D src , D trans , D gen }, respectively. Based on our epxeriments, D gen only is too noisy thus we combine with D trans . Our training framework in general can handle more than 2 models. We present experimental results with more than 2 models in Section 4.4.</p><p>After the initialization stage, each model has learned some knowledge from each augmented training data. Since there exists noise in the augmented training data (such as D trans and D gen ), we step into the relabeling stage, which combines a series of strategies: instance relabeling, co-training and instance re-weighting to reduce the impact of the noise. </p><formula xml:id="formula_5">y I = 1 K K k=1 p I (x; Θ k ); y S j = 1 K K k=1 p S j (x; Θ k ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Co-Training</head><p>Heuristically, instances with small losses are more likely to have cleaner labels. When noise from different augmentation methods is more or less independent, each model can learn instances with small cross-entropy losses from the other models. Specifically, when K = 2, in each batch of the training data, each network discards the instances with larger losses computed by Equation 3 by a ratio of δ and then teach the remaining instances to another one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Instance Re-weighting</head><p>Another way to reduce the impact of noisy instances is to assign different weights to different instances, the more noisy an instance, the less weight it is associated. We design a re-weighting mechanism to implement this idea. The intuition is that if the predictions by multiple models are quite inconsistent, the instance may likely be noisy. Technically, we design an uncertainty based weight to re-weight the training instances. The larger the deviation, the more uncertainty and the less the weight. Specifically, the uncertainty of each instance is defined as:</p><formula xml:id="formula_6">u = 1 K K k=1 V ar(p(x; Θ k ), m(x))<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">V ar(p(x; Θ k ), m(x)) = (p I (x; Θ k ) − 1 K K k=1 p I (x; Θ k )) 2 + 1 L L l=1 (p S l (x; Θ k ) − 1 K K k=1 p S l (x; Θ k )) 2 (6)</formula><p>We further compute weight by w = e −u and incorporate this weight into Equation 3 to obtain:</p><formula xml:id="formula_8">L k (x) = −w[ 1 L L j=1 y S j logp S j (x; Θ k )+ y I logp I (x; Θ k )]<label>(7)</label></formula><p>which is the new training objective during the relabeling stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report our experiments on two benchmark datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>We evaluate the effectiveness of our proposed approach over five languages on two benchmark datasets: SNIPS <ref type="bibr" target="#b31">(Schuster et al., 2019)</ref> and MTOP <ref type="bibr" target="#b15">(Li et al., 2020a)</ref>. The details of datasets are provided in Appendix.</p><p>For generation, we generate m = 10 candidates for each input and randomly sample a = 1 from each candidate set to construct D gen . Our SLU model is based on the pre-trained XLM-R large model, which has L = 24 layers and two additional task-specific linear layers for intent classification and slot filling. More implementation details including hyper-parameters are described in Appendix.</p><p>Following the previous works <ref type="bibr" target="#b31">(Schuster et al., 2019;</ref><ref type="bibr" target="#b15">Li et al., 2020a)</ref>, we use F1 score to measure the slot filling quality and use accuracy score to evaluate the intent classification quality on the SNIPS dataset and use Exact Match Accuracy on the MTOP dataset.</p><p>We employ the following SOTA baselines in two groups. The first group is the model transfer methods, including Multi.CoVe <ref type="bibr" target="#b31">(Schuster et al., 2019)</ref>; Transferable Latent Variable <ref type="bibr" target="#b19">(Liu et al., 2019)</ref>; Attention-Informed Mixed <ref type="bibr" target="#b20">(Liu et al., 2020b)</ref>; CoSDA-ML <ref type="bibr" target="#b27">(Qin et al., 2020)</ref>; LR&amp;ALVM <ref type="bibr" target="#b21">(Liu et al., 2020c);</ref><ref type="bibr">and EN (Li et al., 2020a)</ref>. The second group is the data transfer methods, including EN+Trans. <ref type="bibr" target="#b15">(Li et al., 2020a)</ref> and EN+Trans.+mask <ref type="bibr" target="#b15">(Li et al., 2020a)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> reports the results of our approach and the SOTA baselines. As the translator used in <ref type="bibr" target="#b15">Li et al. (2020a)</ref> is not publicly available, we use Google translator instead, which leads to some results on some languages slightly different from reported by <ref type="bibr" target="#b15">Li et al. (2020a)</ref>.</p><p>Our method outperforms the SOTA baselines and achieves new SOTA performance. Our method improves the Exact Match Accuracy on MTOP from 65.59 to 69.83, the F1 score on SNIPS from 81.44 to 84.49, and the accuracy on SNIPS from 98.64 to 98.71. These results clearly demonstrate the effectiveness of our proposed method.</p><p>One interesting finding is that the performances on Spanish and French become slightly worse after adding the translated data. It is because the noise introduced by the machine translation and alignment processes may hurt the performance. Our method introduces the denoising training approach, which is able to handle the noise of synthesized data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Other Denoising Methods</head><p>To verify the effectiveness of our approach, we conduct experiments with two kinds of denoising approaches used by the previous works. First, we consider the classifier based selection approach. Following Anaby-Tavor et al. ( <ref type="formula">2020</ref>), we train an extra classifier using the corpus in English and the translated corpus, and filter out noisy data according to the probability scores predicted by the classifier. Second, we consider the LM based selection approach. Following <ref type="bibr" target="#b32">Shakeri et al. (2020)</ref>, we use the language model score as the indicator to select high-quality data.</p><p>For fair comparisons, for each noise filtering baseline, we also fine-tune two pre-trained XLM-R large models using different random seeds and take the ensembled model as the final model. Additionally, to remove the effect of instance relabeling method, we also apply it to baselines similar to our approach. Table <ref type="table" target="#tab_1">2</ref> shows the comparison results on the MTOP dataset. Our approach outperforms those two methods (w/ relabeling) by 0.91 and 1.32 percentage points, respectively. This suggests that the gain of our method is not from the simple ensemble of two models. Instead, our method could indeed effectively remove the noise of synthesized data, outperforming previous noise filtering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To validate the contribution of each component in our method, we conduct the ablation study on the MTOP dataset. We consider several ablation options. ( <ref type="formula" target="#formula_1">1</ref> As shown in Table <ref type="table" target="#tab_2">3</ref>, compared with the performance of approach using translated and source language data, the performance of approach using generated data and source language data is reduced by 3.11 percentage points without denoising strategies. This is due to the much noise introduced by the generation process, which hurts the performance. When combining with our denoising approach, the approach with generated training corpus is superior to the approach without generated corpus by 0.47 percentage points. We consider that multiple augmented data sets increase data diversity and lead to better supervision signals. Table <ref type="table" target="#tab_2">3</ref> also shows that removing any of the other components generally leads to clear performance drop. It confirms that all of the proposed techniques contribute to the cross-lingual setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Instance Relabeling</head><p>To better understand the effect of the instance relabeling strategy, in Figure <ref type="figure" target="#fig_3">2</ref>, we record the Exact Match Accuracy of our method with or without the relabeling strategy on the MTOP test set after each training epoch. The performance of our method with relabeling strategy keeps improving and is consistently better than the baseline during  the relabeling stage. It demonstrates that the relabeling method indeed corrects many label errors in the noisy training data and the corrected labels contribute to the performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effect of Number of Models</head><p>We explore the effect of the number K of models (XLM-R). Specifically, we conduct experiments using one or three models. In the setting of one model, that is K = 1, we only train one network with all training corpora D = {D src , D trans , D gen }, and adopt the instance relabeling and instance filtering strategies. In the setting of K = 3, three models are trained using {D src , D trans }, {D src , D gen } and {D src , D trans , D gen }, respectively.</p><p>The results shown in Table <ref type="table" target="#tab_4">5</ref> indicate that our method can effectively extend beyond two models. When the number of networks increases, the performance improves. The intuition is that more models can produce more reliable predictions, and thus can lead to better instance relabeling as well as instance filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effect of Filtering Rate in Co-Training</head><p>To study the impact of the co-training strategy, we conduct experiments with different filtering rates on the MTOP dataset. Table <ref type="table" target="#tab_5">6</ref> shows the results with regard to different filtering rates and different training corpora. For both approaches using or not using the generated corpus, as the filtering rate increases, the performance improves as well. This demonstrates that the filtering strategy can in-deed filter out noisy instances effectively. However, further increasing the filtering rate degrades the performance. It is mainly because of the excessive drop of useful information contained in the training data. Another finding is that the best filtering rate for training corpus {D src , D trans , D gen } is larger than that for {D src , D trans }. The explanation may be that the generated corpus D gen has more diverse data than the translated corpus D trans , but may also contain more noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Case Study</head><p>We conduct case analysis of the instance relabeling results on the MTOP dataset to examine the capability of our approach. We statistically analyze the differences between the original labels and the modified labels after relabeling stage.</p><p>We find that our instance relabeling method effectively corrects wrong labels of the synthesized data, including intent label and slot label as shown in Table <ref type="table" target="#tab_3">4</ref>. Specifically, there are four types of label modifications: 1) Intent Change: the intent label of an utterance is modified; 2) Slot Change: the slot type of a text span is modified; 3) Boundary Change: the BIO boundaries of a slot are modified; and 4) Slot and Boundary Change: both the slot type and the BIO boundaries are modified. For the MTOP dataset, intent labels of 4.99% of the translated and generated data are modified and the slot labels of 33.10% of those data sets are modified.</p><p>From the case study, we can see that the synthesized data indeed contains much noise and our relabeling strategy is able to greatly reduce the negative impact of the noise by correcting different types of label errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a denoising training approach where multiple models trained from various augmented methods provide supervision signals to each other. Extensive experimental results show that the proposed method outperforms the previous approaches, and can certainly alleviate the noisy label problem. Our proposed method is independent of the backbone network (e.g., XLM-R model) and the task. As future work, we plan to investigate the performance of our method on different cross-lingual tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the set of slot labels under the BIO annotation schema (Ramshaw and Marcus, 1999), W I ∈ R |C I |×d and W S ∈ R |C S |×d are the output matrices, and b I and b S are the biases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of data augmentation module (left) and denoising module (right).</figDesc><graphic url="image-1.png" coords="4,93.55,70.86,408.20,175.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Denoising ModuleData: M k (1 ≤ k ≤ K): SLU networks; E all : the number of fine-tuning epochs; E: the number of initialization epochs; Dsrc, Dtrans, Dgen: training corpus; Result: M k (1 ≤ k ≤ K) // Initialization Stage 1 for e = 1; e &lt; E; e + + do 2 shuffle training corpus D k from {Dsrc, Dtrans, Dgen, ...} 3 update M k with L k (D k ) 4 end // Relabeling Stage 5 for e = E; e ≤ E all ; e + + do 6 shuffle training corpus D = {Dsrc, Dtrans, Dgen, ...} into B mini-batches; 7 for j = 1; j &lt;= B; j + + do 8 Fetch j th mini-batch D from D // Co-Training 9 D k = argmin D :|D |≥(1−δ)|D | K\k i L i (D ) // Instance Re-weighting 10 update M k with L k (D k ) In the relabeling stage, model training and instance relabeling are conducted iteratively. Motivated by the idea of model ensemble, we use the ensemble of model predictions to correct label errors in a self-learning manner. Specifically, all models are trained using all training corpora D = {D src , D trans , D gen }. The slot labels and the intent labels of the training instances in D trans and D gen are modified to the corresponding ensemble predicted probability distributions, which are used as the pseudo-truth labels to compute the loss in the next epoch. That is,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Metric trend per epoch on MTOP w/ or w/o instance relabeling strategy.</figDesc><graphic url="image-2.png" coords="6,306.14,305.78,218.27,102.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) w/o generation removes the generated training data. (2) w/o instance relabeling keeps the intent and slot labels of data unchanged throughout the training process. (3) w/o co-training trains models using all training data without filtering. (4) w/o instance re-weighting skips the instance reweighting strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Trans. + Gen. + Denoise 86.19/99.37 82.79/98.17 84.49/98.71 71.58 71.53 74.05 69.66 62.31 69.83 Comparison results between our approach and prior SOTA methods. The top block rows are baselines that belong to model transfer methods. The middle block rows are XLM-R large model based approaches, w/ or w/o translated training data. Methods with reimp are re-implemented in this paper with different translator and alignment tool. EN, Trans. and Gen. denote source language data D src , translated target language data D trans and generated target language data D gen , respectively. Denoise denotes proposed denoising module.</figDesc><table><row><cell></cell><cell cols="3">SNIPS (F1/Intent Accuracy)</cell><cell></cell><cell cols="4">MTOP (Exact Match Accuracy)</cell></row><row><cell></cell><cell>es</cell><cell>th</cell><cell>Average</cell><cell>es</cell><cell>fr</cell><cell>de</cell><cell>hi</cell><cell>th</cell><cell>Average</cell></row><row><cell>Multi. CoVe</cell><cell cols="3">19.25/53.89 35.62/70.70 27.44/62.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Transferable Latent Variable</cell><cell cols="3">65.79/90.20 32.24/73.47 49.02/81.84</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Attention-Informed Mixed</cell><cell cols="3">73.89/87.88 27.12/73.46 50.51/80.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CoSDA-ML</cell><cell cols="3">80.40/94.80 37.30/76.80 58.85/85.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LR&amp;ALVM</cell><cell cols="3">72.49/92.31 33.28/75.77 52.89/84.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EN</cell><cell cols="3">84.20/97.70 46.00/90.40 65.10/94.05</cell><cell cols="5">69.10 65.40 64.00 55.00 43.80</cell><cell>59.46</cell></row><row><cell>EN + Trans.</cell><cell cols="3">73.40/98.30 50.90/96.60 62.15/97.45</cell><cell cols="5">74.50 72.60 64.70 58.30 56.50</cell><cell>65.32</cell></row><row><cell>EN + Trans. + mask</cell><cell cols="3">83.00/98.00 50.20/96.60 66.60/97.30</cell><cell cols="5">74.60 72.20 65.70 62.50 53.20</cell><cell>65.64</cell></row><row><cell>EN reimp</cell><cell cols="3">84.69/97.67 39.34/89.42 62.02/93.55</cell><cell cols="5">68.38 70.48 66.15 54.81 38.36</cell><cell>59.63</cell></row><row><cell>EN + Trans. reimp</cell><cell cols="3">83.58/99.34 79.29/97.93 81.44/98.64</cell><cell cols="5">70.25 67.27 70.39 64.22 55.80</cell><cell>65.59</cell></row><row><cell>EN +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with other denoising methods on the MTOP dataset.</figDesc><table><row><cell></cell><cell>es</cell><cell>fr</cell><cell>de</cell><cell>hi</cell><cell>th</cell><cell>Average</cell></row><row><cell>Classifier based selection</cell><cell cols="6">71.45 69.21 71.68 66.37 60.43</cell><cell>67.83</cell></row><row><cell>LM based selection</cell><cell cols="6">70.41 68.78 70.95 65.94 58.44</cell><cell>66.90</cell></row><row><cell cols="7">Classifier based selection w/ relabeling 72.01 70.06 73.29 69.01 60.25</cell><cell>68.92</cell></row><row><cell>LM based selection w/ relabeling</cell><cell cols="6">71.38 69.28 72.95 67.73 61.19</cell><cell>68.51</cell></row><row><cell>EN + Trans. + Gen. + Denoise</cell><cell cols="6">71.58 71.53 74.05 69.66 62.31</cell><cell>69.83</cell></row><row><cell></cell><cell>es</cell><cell>fr</cell><cell>de</cell><cell>hi</cell><cell>th</cell><cell>Average</cell></row><row><cell>EN + Trans.</cell><cell cols="6">70.25 67.27 70.39 64.22 55.80 65.59</cell></row><row><cell>EN + Gen.</cell><cell cols="6">66.71 65.99 70.84 63.55 45.32 62.48</cell></row><row><cell>EN + Trans. + Gen.</cell><cell cols="6">69.21 66.93 71.46 65.48 56.89 65.99</cell></row><row><cell cols="7">EN + Trans. + Gen. + Denoise 71.58 71.53 74.05 69.66 62.31 69.83</cell></row><row><cell>w/o Gen.</cell><cell cols="6">71.08 70.28 72.67 70.05 62.71 69.36 ↓</cell></row><row><cell>w/o instance relabeling</cell><cell cols="6">71.98 70.15 72.78 69.37 60.87 69.03 ↓</cell></row><row><cell>w/o co-training</cell><cell cols="6">71.71 69.56 73.15 68.94 61.41 68.95 ↓</cell></row><row><cell>w/o instance re-weighting</cell><cell cols="6">72.18 70.84 73.99 68.55 62.82 69.68 ↓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the MTOP dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Cases of relabeling results on the MTOP dataset.</figDesc><table><row><cell>Type</cell><cell cols="2">Source Lang</cell><cell></cell><cell>Utterance</cell><cell cols="2">Original Labels Modified Labels</cell></row><row><cell>Intent</cell><cell>Gen.</cell><cell>fr</cell><cell></cell><cell>je suis tout disponible</cell><cell>get_available</cell><cell>set_available</cell></row><row><cell>Intent</cell><cell>Gen.</cell><cell>de</cell><cell cols="2">verschieben sie die erinnerung in den alarm</cell><cell cols="2">update_reminder delete_reminder</cell></row><row><cell>Slot</cell><cell>Trans.</cell><cell>es</cell><cell cols="2">¿cómo es el clima en colorado en esta época del año?</cell><cell>None</cell><cell>location</cell></row><row><cell>Boundary</cell><cell>Trans.</cell><cell>es</cell><cell></cell><cell>resumen de noticias [de tayer]</cell><cell>data_time</cell><cell>data_time</cell></row><row><cell>Slot&amp;Boundary</cell><cell>Gen.</cell><cell>de</cell><cell cols="2">lesen sie mir [die heutigen] schlagzeilen</cell><cell>news_source</cell><cell>data_time</cell></row><row><cell></cell><cell></cell><cell cols="3">One model Two models Three models</cell><cell></cell></row><row><cell cols="2">EN+Trans.+Gen.+Denoise</cell><cell>67.57</cell><cell>69.83</cell><cell>70.04</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance of approaches with different number of models on the MTOP dataset.</figDesc><table><row><cell></cell><cell>0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>EN+Trans.+Denoise</cell><cell cols="5">68.32 68.69 69.36 69.07 68.75</cell></row><row><cell cols="6">EN+Trans.+Gen.+Denoise 68.95 68.77 69.12 69.83 69.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance with different filtering rates and training corpus.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Jian Pei's research is supported in part by the NSERC Discovery Grant program. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Datasets</head><p>We evaluate the effectiveness of our proposed approach over five target languages on two benchmark datasets: SNIPS <ref type="bibr" target="#b31">(Schuster et al., 2019)</ref> and MTOP <ref type="bibr" target="#b15">(Li et al., 2020a)</ref>. Statistics of used data are detailed in Table <ref type="table">7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>For generation, we fine-tune mBART pre-trained on 25 languages with 0.3 dropout, 0.2 label smoothing, 2500 warm-up steps, 3×10 −5 maximum learning rate, and 1024 tokens in each batch. For text filling, we mask 35% of the words in each instance by randomly sampling a span length according to a Poisson distribution (λ = 3.5). Then we append to each instance an end-of-sentence token (&lt; /S &gt;) and the corresponding language id symbol (&lt; LID &gt;). We don't search the best parameters for generation but use the default values in open-source code * . The final models are selected based on validation likelihood.</p><p>For SLU, we use XLM-R large model with about 550M parameters as the backbone network. In the fine-tuning process, we set the batch size as 128, fine-tuning epochs E all = 10, initialization epochs E = 4 and 0.1 dropout for two benchmark datasets. The maximum filtering rates are δ = 0.2 and δ = 0.3 for the SNIPS and MTOP datasets, respectively. The learning rates are 2 × 10 −5 and 5 × 10 −5 for SNIPS and MTOP datasets, respectively. We select the best hyper-parameters by searching a combination of batch size, learning rate, the number of fine-tuning epochs, the number of initialization epochs and the filtering ratio with the following range: batch size {32, 64, 128}, learning rate {1, 2, 3, 4, 5} × 10 −5 , fine-tuning epochs {5, 10, 15}, initialization epochs {2, 3, 4}, filtering ratio {10%, 20%, 30%, 40%}. The models are saved by performance on the English development corpus and translated target language development corpus. The models are trained using mini-batch back-propagation, and the AdamW <ref type="bibr" target="#b22">(Loshchilov and Hutter, 2019)</ref> optimizer is used for optimization. We fine-tune the models on two V100-32GB GPUs which lasts about 4 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Effect of Generated Data Size</head><p>To further analyze the effect of generated data, we randomly sample a = {0, 1, 2, 3} instances from the candidate set for each input to construct the generated corpus. As shown in Table <ref type="table">8</ref>, by increasing the size of the generated corpus, the performance improves. However, when the data size reaches a certain scale, e.g., a = 3 in our experiments, the performance slightly regresses, but still outperforms the baseline without generated data. This suggests that the augmentation module indeed increases the diversity of training data and then improves the performance. Though increasing noise limits the growth of the improvement, our approach is robust enough to achieve comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Variance Analysis</head><p>We conduct 5 runs of training and calculate the mean and standard deviation (Stdev) values for our approach and the baseline on the MTOP dataset.</p><p>The results are listed in Table <ref type="table">9</ref>. Besides, we also conduct a two-sided statistically significant t-test with the significance threshold 0.05 to compare the baseline with our method. The results show that the variance of our approach is similar to that of the baseline. Moreover, with p-value = 4.5 × 10 −8 , our method outperforms the baseline with statistical significance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Zwerdling</surname></persName>
		</author>
		<title level="m">Do not have enough data? deep learning to the rescue! In AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7383" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tutorial on variational autoencoders</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno>CoRR, abs/1606.05908</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Henrique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoiti</forename><surname>Dos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santos</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claus</forename><surname>Aranha</surname></persName>
		</author>
		<title level="m">Data augmentation using gans</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">1904.09135</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
				<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Silin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Paraphrase augmented task-oriented dialog generation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Federated learning for spoken language understanding</title>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.310</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee on Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3467" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/2003.02245</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09335</idno>
		<title level="m">Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual adaptation for sequence tagging and beyond</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot cross-lingual dialogue systems with transferable latent variables</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="1297" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020b. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8433" to="8440" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-lingual spoken language understanding with regularized representation alignment</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.587</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020c. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="7241" to="7251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving short text classification through global augmentation methods</title>
		<author>
			<persName><forename type="first">Vukosi</forename><surname>Marivate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tshephisho</forename><surname>Sefara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Data augmentation for spoken language understanding via pretrained models</title>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>CoRR, abs/2004.13952</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual NLP</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/533</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3853" to="3860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing using very large corpora</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="157" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of cross-lingual word embedding models</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="569" to="631" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Control, generate, augment: A scalable framework for multi-attribute text generation</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nora</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudiu</forename><surname>Cristian Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.33</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20">2020. 16-20 November 2020</date>
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer learning for multilingual task oriented dialog</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rushin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3795" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multilingual synthetic question and answer generation for cross-lingual reading comprehension</title>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Sanjay Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<idno>CoRR, abs/2010.12008</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<title level="m">2011. Spoken Language Understanding: Systems for Extracting Semantic Information from Speech</title>
				<imprint>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">(almost) zero-shot cross-lingual spoken language understanding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dilek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6034" to="6038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets</title>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
				<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2557" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spoken language understanding &quot;&quot; an introduction to the statistical framework</title>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="31" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional bert contextual augmentation</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end slot alignment and recognition for crosslingual NLU</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Batool</forename><surname>Haider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5052" to="5063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data augmentation with atomic templates for spoken language understanding</title>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1375</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3635" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
