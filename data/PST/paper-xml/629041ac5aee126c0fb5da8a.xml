<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Transformers Effective for Time Series Forecasting?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-26">26 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
							<email>alzeng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muxi</forename><surname>Chen</surname></persName>
							<email>mxchen21@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@idea.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are Transformers Effective for Time Series Forecasting?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-26">26 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.13504v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been a surge of Transformer-based solutions for the time series forecasting (TSF) task, especially for the challenging long-term TSF problem. Transformer architecture relies on self-attention mechanisms to effectively extract the semantic correlations between paired elements in a long sequence, which is permutation-invariant and "anti-ordering" to some extent. However, in time series modeling, we are to extract the temporal relations among an ordering set of continuous points. Consequently, whether Transformer-based techniques are the right solutions for "long-term" time series forecasting is an interesting problem to investigate, despite the performance improvements shown in these studies. In this work, we question the validity of Transformer-based TSF solutions. In their experiments, the compared (non-Transformer) baselines are mainly autoregressive forecasting solutions, which usually have a poor long-term prediction capability due to inevitable error accumulation effects. In contrast, we use an embarrassingly simple architecture named DLinear that conducts direct multi-step (DMS) forecasting for comparison. DLinear decomposes the time series into a trend and a remainder series and employs two one-layer linear networks to model these two series for the forecasting task. Surprisingly, it outperforms existing complex Transformer-based models in most cases by a large margin. Therefore, we conclude that the relatively higher long-term forecasting accuracy of Transformer-based TSF solutions shown in existing works has little to do with the temporal relation extraction capabilities of the Transformer architecture. Instead, it is mainly due to the non-autoregressive DMS forecasting strategy used in them. We hope this study also advocates revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at https://github.com/cure-lab/DLinear. * Equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series are ubiquitous in today's data-driven world. Given historical data, time series forecasting (TSF) is a long-standing task that has a wide range of applications, including but not limited to traffic flow estimation, energy management, and financial investment.</p><p>Over the past several decades, TSF solutions have undergone a progression from traditional statistical methods (e.g., ARIMA <ref type="bibr" target="#b1">[1]</ref>) and machine learning techniques (e.g., GBRT <ref type="bibr" target="#b10">[10]</ref>) to deep learningbased solutions, e.g., recurrent neural networks (RNNs) <ref type="bibr" target="#b15">[15]</ref> and temporal convolutional networks (TCNs) <ref type="bibr" target="#b3">[3]</ref>. At the same time, we are dealing with increasingly complex and diverse time series data, ranging from univariate time series to multivariate time series and today's big-time series in many applications, requiring advanced deep neural networks for temporal relation extraction.</p><p>Transformer <ref type="bibr" target="#b25">[25]</ref> is arguably the most successful sequence modeling architecture, which demonstrates unparalleled performances in various artificial intelligence applications, such as natural language processing <ref type="bibr" target="#b6">[6]</ref>, speech recognition <ref type="bibr" target="#b7">[7]</ref>, and motion analysis <ref type="bibr" target="#b19">[19]</ref>. Recently, there has also been a surge of Transformer-based solutions for time series analysis, as surveyed in <ref type="bibr" target="#b26">[26]</ref>. Some notable models for the TSF task include: LogTrans <ref type="bibr" target="#b16">[16]</ref> (NeurIPS 2019), Informer <ref type="bibr" target="#b28">[28]</ref> (AAAI 2021 Best paper), Autoformer <ref type="bibr" target="#b27">[27]</ref> (NeurIPS 2021), Pyraformer <ref type="bibr" target="#b18">[18]</ref> (ICLR 2022 Oral), and the recent FEDformer <ref type="bibr" target="#b29">[29]</ref> (ICML 2022). Most of the above works focus on the less explored long-term time series forecasting (LTSF) problem, demonstrating considerable prediction accuracy improvements over traditional methods. However, in their experiments, all the compared (non-Transformer) baselines perform autoregressive forecasting <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b23">23]</ref>, which are known to suffer from significant error accumulation effects. More importantly, the main working power of the Transformer architecture is from its multi-head self-attention mechanism, which has a remarkable capability of extracting semantic correlations between paired elements in a long sequence (e.g., words in texts or 2D patches in images), and this procedure is permutation-invariant, i.e., regardless of the order. However, for time series analysis, we are mainly interested in modeling the temporal dynamics among a continuous set of points, wherein the order itself often plays the most crucial role. Based on the above analysis, we pose the following intriguing question: Are Transformers really effective for long-term time series forecasting?</p><p>To answer this question, we present an embarrassingly simple network named DLinear as a baseline for comparison. DLinear decomposes the time series into a trend and a remainder series and employs two one-layer linear networks to model these two series with direct multi-step (DMS) forecasting. We conduct extensive experiments on nine widely-used benchmarks, including several real-life applications: traffic, energy, economics, weather, and disease predictions.</p><p>Our results show that DLinear outperforms existing complex Transformer-based models in most cases by a large margin. In particular, for the Exchange-Rate dataset that does not have obvious periodicity, the prediction errors of the state-of-the-art method <ref type="bibr" target="#b29">[29]</ref> are more than twice larger than those of DLinear. Moreover, we find that, in contrast to the claims in existing works, most of them fail to extract temporal relations from long sequences, i.e., the forecasting errors are not reduced (sometimes even increased) with the increase of look-back window sizes. Finally, we also conduct various ablation studies on existing Transformer-based TSF solutions to study the impact of various design elements in them.</p><p>With the above, we conclude that the temporal modeling capabilities of Transformers for time series are exaggerated, at least for the time series forecasting problem. At the same time, while DLinear achieves a better prediction accuracy compared to existing works, it merely serves as a simple baseline for future research on the challenging long-term TSF problem. With our findings, we also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.</p><p>The remainder of this paper is organized as follows. Sec. 2 presents the preliminaries on time series forecasting. Then, we discuss existing Transformer-based solutions in Sec. 3. Next, Sec. 4 details the baseline DLinear architecture. Experimental results are then shown in Sec. 5. Finally, Sec. 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TSF Problem Formulation</head><p>For time series containing C variates, given historical data X = {X t 1 , ..., X t C } L t=1 , wherein L is the look-back window size and X t i is the value of the i th variate at the t th time step. The time series forecasting task is to predict the values X = { Xt 1 , ..., Xt C } L+T t=L+1 at the T future time steps. When T &gt; 1, we can learn a single-step forecaster and iteratively apply it to obtain multi-step predictions, known as iterated multi-step (IMS) forecasting <ref type="bibr" target="#b22">[22]</ref>. Alternatively, we can directly optimize the multi-step forecasting objective at once, known as direct multi-step (DMS) forecasting <ref type="bibr" target="#b4">[4]</ref>.</p><p>Compared to DMS forecasting results, IMS predictions have smaller variance thanks to the autoregressive estimation procedure, but they inevitably suffer from error accumulation effects. Consequently, IMS forecasting is preferable when there is a highly-accurate single-step forecaster and T is relatively small. In contrast, DMS forecasting generates relatively more accurate prediction results when it is hard to obtain an unbiased single-step forecasting model or T is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Transformer-Based TSF Solutions</head><p>As a long-standing problem that has a wide range of applications, statistical approaches (e.g., autoregressive integrated moving average (ARIMA) <ref type="bibr" target="#b1">[1]</ref>, exponential smoothing <ref type="bibr" target="#b11">[11]</ref>, and structural models <ref type="bibr" target="#b13">[13]</ref>) for time series forecasting have been used from the 1970s onward.</p><p>Generally speaking, the parametric models used in statistical methods require significant domain expertise to build. To relieve this burden, many machine learning techniques such as gradient boosting regression tree (GBRT) <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b9">9]</ref> gain popularity, which learns the temporal dynamics of time series in a data-driven manner.</p><p>However, these methods still require manual feature engineering and model designs.</p><p>With the powerful representation learning capability of deep neural networks (DNNs) from abundant data, various deep learning-based TSF solutions are proposed in the literature, achieving better forecasting accuracy than traditional techniques in many cases. Besides Transformers, the other two popular DNN architectures are also applied for time series forecasting:</p><p>? Recurrent neural networks (RNNs) based methods (e.g., <ref type="bibr" target="#b20">[20]</ref>) summarize the past information compactly in internal memory states and recursively update themselves for forecasting. ? Convolutional neural networks (CNNs) based methods (e.g., <ref type="bibr" target="#b3">[3]</ref>), wherein convolutional filters are used to capture local temporal features.</p><p>RNN-based TSF methods belong to IMS forecasting techniques. Depending on whether the decoder is implemented in an autoregressive manner, there are either IMS or DMS forecasting techniques for CNN-based TSF methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b17">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transformer-Based LTSF Solutions</head><p>Transformer-based models <ref type="bibr" target="#b25">[25]</ref> have achieved unparalleled performances in many long-standing AI tasks in natural language processing and computer vision fields, thanks to the effectiveness and efficiency of the multi-head self-attention mechanism. This has also triggered lots of research interests in Transformer-based time series modeling techniques, as surveyed in <ref type="bibr" target="#b26">[26]</ref>. In particular, a large amount of research works are dedicated to the TSF task (e.g., <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref>). Considering the ability to capture long-range dependencies with Transformer models, most of them focus on the less-explored long-term forecasting problem (T 1).</p><p>When applying the vanilla Transformer model to the LTSF problem, it has some limitations, including the quadratic time/memory complexity with the original self-attention scheme and error accumulation caused by the autoregressive decoder design. Informer <ref type="bibr" target="#b28">[28]</ref> addresses these issues and proposes a novel Transformer architecture with reduced complexity and a DMS forecasting strategy. Later, more Transformer variants introduce various time series features into their models for performance or efficiency improvements <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29]</ref>. We summarize the design elements of existing Transformerbased TSF solutions as follows (see Figure <ref type="figure" target="#fig_1">1</ref>).</p><p>Time series decomposition: For data preprocessing, normalization with zero-mean is common in TSF. Besides, Autoformer <ref type="bibr" target="#b27">[27]</ref> first applies seasonal-trend decomposition behind each neural block, which is a standard method in time series analysis to make raw data more predictable <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b12">12]</ref>. Specifically, they use a moving average kernel on the input sequence to extract the trend-cyclical component of the time series. The difference between the original sequence and the trend component is regarded as the seasonal component. On top of the decomposition scheme of Autoformer, FEDformer <ref type="bibr" target="#b29">[29]</ref> further proposes the mixture of experts strategy to mix the trend components extracted by moving average kernels with various kernel sizes.</p><p>Input embedding strategies: The self-attention layer in the Transformer architecture cannot preserve the positional information of the time series. However, local positional information, i.e. the ordering of   <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29]</ref>. time series, is important. Besides, global temporal information, such as hierarchical timestamps (week, month, year) and agnostic timestamps (holidays, and events), is also informative <ref type="bibr" target="#b28">[28]</ref>. To enhance the temporal context of time-series inputs, a practical design in the SOTA Transformer-based methods is injecting several embeddings, like a fixed positional encoding, a channel projection embedding, and learnable temporal embeddings into the input sequence. Moreover, temporal embeddings with a temporal convolution layer <ref type="bibr" target="#b16">[16]</ref> or learnable timestamps <ref type="bibr" target="#b27">[27]</ref> are introduced.</p><p>Self-attention schemes: Transformers rely on the self-attention mechanism to extract the semantic dependencies between paired elements. To reduce the O L 2 time and memory complexity of the vanilla Transformer, two strategies are used for efficiency improvements. On the one hand, LogTrans <ref type="bibr" target="#b16">[16]</ref> and Pyraformer <ref type="bibr" target="#b18">[18]</ref> explicitly introduce a sparsity bias into the self-attention scheme. To be specific, LogTrans <ref type="bibr" target="#b16">[16]</ref> uses a Logsparse mask to reduce the computational complexity to O (LlogL) while Pyraformer <ref type="bibr" target="#b18">[18]</ref> adopts pyramidal attention that captures hierarchically multi-scale temporal dependencies with an O (L) time and memory complexity. On the other hand, Informer <ref type="bibr" target="#b28">[28]</ref> and FEDformer <ref type="bibr" target="#b29">[29]</ref> use the low-rank property in the self-attention matrix. Informer <ref type="bibr" target="#b28">[28]</ref> proposes a ProbSparse self-attention mechanism and a self-attention distilling operation to decrease the complexity to O (LlogL) and FEDformer <ref type="bibr" target="#b29">[29]</ref> designs a Fourier enhanced block and a wavelet enhanced block with random selection to obtain O (L) complexity. Lastly, Autoformer <ref type="bibr" target="#b27">[27]</ref> designs a series-wise auto-correlation mechanism to replace the original self-attention layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoders:</head><p>The original Transformer decoder outputs sequences in an autoregressive manner, resulting in a slow inference speed and error accumulation effects, especially for long-term predictions. Informer <ref type="bibr" target="#b28">[28]</ref> designs a generative-style decoder for DMS forecasting. Other Transformer variants employ similar DMS strategies. For instance, Pyraformer <ref type="bibr" target="#b18">[18]</ref> uses a fully-connected layer concatenating spatio-temporal axes as the decoder. Autoformer <ref type="bibr" target="#b27">[27]</ref> sums up two refined decomposed features from trend-cyclical components and the stacked auto-correlation mechanism for seasonal components to get the final prediction. FEDformer <ref type="bibr" target="#b29">[29]</ref> also uses a decomposition scheme with the proposed frequency attention block to decode the final results.</p><p>The premise of Transformer models is the semantic correlations between paired elements, while the self-attention mechanism itself is permutation-invariant. Considering the raw numerical data in time series (e.g., stock prices or electricity values), there are hardly any point-wise semantic correlations between them. In time series modeling, we are mainly interested in the temporal relations among a continuous set of points, and the order of these elements instead of the paired relationship plays the most crucial role. While employing positional encoding and using tokens to embed sub-series facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. Due to the above observations, we are interested in revisiting the effectiveness of Transformer-based LTSF solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">An Embarrassingly Simple Baseline for LTSF</head><p>In the experiments of existing Transformer-based LTSF solutions (T 1), all the compared (non-Transformer) baselines are IMS forecasting techniques, which are known to suffer from significant error accumulation effects. We hypothesize that the performance improvements shown in these works are largely due to the DMS strategy used in them. To validate this hypothesis, we present an embarrassingly simple linear model with time series decomposition, named DLinear, as a baseline for comparison. There are mainly two observations that inspire our design.</p><p>First, an one-layer linear network is arguably the simplest network to aggregate historical information for future prediction. Second, from the experiments in previous works <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29]</ref>, decomposition can largely enhance the performance of Transformer-based methods in time series forecasting, which is model-agnostic and may boost other models, such as a linear model. Accordingly, DLinear is a combination of a decomposition scheme and a linear network. It first decomposes a time-series data into a trend component X t ? R L?C and a remainder data X s = X -X t . Then, two one-layer linear networks are applied to the two series.  The overall structure of DLinear is shown in Figure <ref type="figure" target="#fig_3">2</ref> (a). The whole process is: Although DLinear is simple, it has some compelling characteristics:</p><formula xml:id="formula_0">X = H s + H t , where H s = W s X s ? R T ?C and H t = W t X t ? R T ?C</formula><p>? An O(1) maximum signal traversing path length: The shorter the path, the better the dependencies are captured <ref type="bibr" target="#b18">[18]</ref>, making DLinear capable of capturing both short-range and long-range temporal relations. ? High-efficiency: As each branch has only one linear layer, it costs much lower memory and fewer parameters and has a faster inference speed than existing Transformers (see Table <ref type="table" target="#tab_8">6</ref>). ? Interpretability: After training, we can visualize weights from the seasonality and trend branches to have some insights on the predicted values <ref type="bibr" target="#b8">[8]</ref>. ? Easy-to-use: DLinear can be obtained easily without tuning model hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Dataset. We conduct extensive experiments on nine widely-used real-world datasets, including ETT (Electricity Transformer Temperature) <ref type="bibr" target="#b28">[28]</ref> (ETTh1, ETTh2, ETTm1, ETTm2), Traffic, Electricity, Weather, ILI, Exchange-Rate <ref type="bibr" target="#b15">[15]</ref>. All of them are multivariate time series. We leave the detailed data descriptions in the supplementary material. Evaluation metric. Following previous works <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29]</ref>, we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) as the core metrics to compare performance.</p><p>Compared methods. We include six state-of-the-art Transformer-based methods: FEDformer <ref type="bibr" target="#b29">[29]</ref>, Autoformer <ref type="bibr" target="#b27">[27]</ref>, Informer <ref type="bibr" target="#b28">[28]</ref>, Pyraformer <ref type="bibr" target="#b18">[18]</ref>, LogTrans <ref type="bibr" target="#b16">[16]</ref>, and Reformer <ref type="bibr" target="#b14">[14]</ref>. Besides, we  include the naive DMS method: Closest Repeat (Repeat-C), which naively repeats the last value in the look-back window, as another simple baseline. Since there are two variants of FEDformer, we compare with the one with better accuracy (FEDformer-f via Fourier transform) shown in their paper. More implementation details are in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Transformers</head><p>To verify whether existing Transformer-based solutions are effective for TSF tasks, we present both quantitative and qualitative comparisons with DLinear.</p><p>Quantitative results. In Table <ref type="table" target="#tab_1">2</ref>  The recent FEDformer achieves relatively high forecasting accuracy, especially for the ETT benchmark, as shown in Table <ref type="table" target="#tab_2">3</ref>. This might be due to the fact that FEDformer does not rely much on the self-attention mechanism in Transformers. Rather, it employs classical time series analysis techniques such as Fourier transformation, which plays an important role in temporal feature extraction.</p><p>It is worth noting that, while FEDformer outperforms DLinear in some cases, it is achieved under the setting of T = 96. We study the impact of different look-back window sizes (see Sec. 5.3), and our results show that DLinear continues to improve with increased T , eventually outperforming FEDformer by a large margin.</p><p>Another interesting observation is that even though the naive Repeat-C method shows worse results when predicting long-term seasonal data (e.g., Electricity and Traffic), it surprisingly outperforms all the Transformer-based methods on the Exchange-Rate (over 30%) and Weather (over 10%) dataset. This is mainly caused by the wrong prediction of trends in Transformer-based solutions, resulting in significant accuracy degradation (see Figure <ref type="figure" target="#fig_5">3(b)</ref>).</p><p>Qualitative results. As shown in Figure <ref type="figure" target="#fig_5">3</ref>, we plot the prediction results on three selected time series   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">More Analyses on Transformer-Based Solutions</head><p>Impact of the look-back window size. The size of the look-back window has a high impact on forecasting accuracy as it determines how much we can learn from historical data. Generally speaking, a powerful TSF model with a strong temporal relation extraction capability should be able to achieve better results with larger look-back window sizes.</p><p>To make a fair comparison, we follow the experimental settings in Autoformer <ref type="bibr" target="#b27">[27]</ref> and FEDformer <ref type="bibr" target="#b29">[29]</ref> and set a fixed look-back window size (36 for ILI and 96 for other datasets) in previous experiment. To study the impact of look-back window sizes, we conduct experiments with L ? {24, 48, 72, 96, 120, 144, 168, 192, 336, 504, 672, 720} to forecast either short-term time series (24 steps) or long-term time series (720 steps). Figure <ref type="figure">5</ref>: The MSE results (Y-axis) of models with different look-back window sizes (X-axis) of the Long Forecasting (720 time steps) and the Short Forecasting (24 time steps) on four datasets.</p><p>Figure <ref type="figure">5</ref> demonstrates the MSE results on four datasets. Similar to the observations from previous studies <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b26">26]</ref>, the performance of existing Transformer-based models will deteriorate or keep stable when the look-back window sizes increase. In contrast, the performances of DLinear are significantly boosted with the increase of look-back window size. The input size 96 is suitable for most Transformers, and FEDformer can obtain a comparable performance with DLinear under this setting. Note that, we provide more quantitative results in the supplementary materials, and our conclusion holds in almost all cases. Impact of training data size. Some may argue that the poor performance of Transformer-based solutions is due to the small sizes of the benchmark datasets. Unlike computer vision or natural language processing tasks, TSF is performed on given time series and it is difficult to scale up the training data size. At the same time, the size of the training data would indeed have a significant impact on the model performance. Therefore, we conduct an experiment on the Traffic dataset, comparing the performance of the model trained with a full dataset (17,544*0.7 hours), named Ori., and that trained with a shortened dataset (8,760 hours, i.e., 1 year), called Short. As can be observed in Table <ref type="table" target="#tab_6">4</ref>, the prediction errors with reduced training data are lower in most cases. While we cannot conclude that we should use less data for training, it demonstrates that the training data size is not the limiting factor for the performances of Autoformer and FEDformer.</p><p>Impact of different embedding strategies. In Table <ref type="table" target="#tab_7">5</ref>, we study the benefits brought by embedding position/timestamp information into Transformer-based methods. As can be observed, the forecasting errors of Informer are largely increased without the positional embedding or timestamp embeddings, since Informer uses a single time step as a token for self-attention, indicating the necessity of introducing temporal information in the model. Rather than using a single time step in each token, FEDformer <ref type="bibr" target="#b29">[29]</ref> and Autoformer <ref type="bibr" target="#b27">[27]</ref> input a sequence of timestamps to introduce more positional information. They can achieve comparable or even better performance without the fixed positional embedding (wo/Pos.). Without timestamp embedding, the performance of Autoformer declines rapidly due to the loss of global temporal information. However, thanks to the frequency enhanced module proposed in FEDformer, it suffers less from removing any of the specific position/timestamp embeddings. Efficiency comparison. Existing Transformer-based TSF methods focus on reducing the O L 2 complexity of the vanilla Transformer. Although they prove to be able to improve the theoretical time and memory complexity, it is unclear whether 1) the actual inference time and memory cost on devices are improved, and 2) the memory issue is unacceptable and urgent for today's GPU (e.g., we use one NVIDIA Titan Xp here). -? is modified into the same one-step decoder, which is implemented in the source code from Autoformer.</p><p>- * 236.7M parameters of Pyraformer come from its linear decoder.</p><p>In Table <ref type="table" target="#tab_8">6</ref>, we compare both the theoretical and practical efficiency. Interestingly, compared with the vanilla Transformer (with the same DMS decoder), most Transformer variants incur similar or even worse inference time and parameters in practice, due to the additional design elements introduced in their model. Moreover, the memory cost of the vanilla Transformer is practically acceptable, even for output length L = 720, which weakens the importance of deriving a memory-efficient Transformer-based method. As can be observed in Figure <ref type="figure">6</ref>, with increased look-back window sizes, the performance of DLinear is significantly boosted for most datasets, while this is not the case for Transformer-based TSF solutions. The results of Exchange-Rate and ILI datasets do not show improved results with a long look-back window, and we attribute it to the lack of seasonal information in these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison under a Long Look-Back Window</head><p>In the main paper, we follow the settings of existing Trasnformer-based TSF solutions and fix the look-back window size as 96 for a fair comparison. The results of FEDformer are on par with those of DLinear on some datasets, i.e., Traffic, Electricity, and ETTh1. In this experiment, we set the look-back window size to be 336. As shown in Since the SOTA methods FEDformer and Autoformer obtain better performance with an input length L = 96 as shown in Figure <ref type="figure">6</ref>, we keep their best settings for comparison. FEDformer-f and FEDformer-w represent the FEDformer with fourier enhanced blocks and wavelet enhanced blocks, respectively. The best results are highlighted in red bold and the second best results are highlighted with a blue underline.</p><p>144 (1 day is 144 * 10 minutes). Besides, the visualization can also reveal the periodicity of different temporal scales. For Traffic data, as shown in Figure <ref type="figure">7</ref>(d), the model gives high weights to the latest time step of the look-back window for the 0,23,47...719 forecasting steps. Among these forecasting time steps, the 0, 167, 335, 503, 671 time steps have higher weights. Note that, 24 time steps are a day and 168 time steps is a week. This indicates that Traffic has a daily periodicity and a weekly periodicity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The pipeline of existing Transformer-based TSF solutions. In (a) and (b), the solid boxes are essential operations and the dotted boxes are applied optionally. (c) and (d) are distinct for different methods [16, 28, 27, 18, 29].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>History ? timesteps Future ? timesteps (a) The whole structure of DLinear (b) One Linear Layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the Decomposition Linear Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>are the decomposed trend and remainder features. W s ? R T ?L and W t ? R T ?L are two linear layers, as illustrated in Figure2(b).Note that, if the variates of the dataset have different characteristics, i.e., different seasonality and trend, DLinear with shared weights across different variates might not perform well. Therefore, we have two designs in DLinear. We name the DLinear network that every variate shares the same linear layer as DLinear-S and the DLinear network that has a linear layer for each variate individually as DLinear-I. We use DLinear-S by default.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the long-term forecasting output (Y-axis) of five models with an input length L=96 and output length T =336 (X-axis) on Electricity, Exchange-Rate, and ETTh2, respectively. The interpretability of DLinear. Because DLinear is a linear model, its weights can directly reveal how DLinear works. Figure 4(a) and (b) visualize the weights of the trend and the remainder layers on the Exchange-Rate dataset. Due to the lack of periodicity and seasonality in financial data, it is hard to observe clear patterns, but the trend layer reveals greater weights of information closer to the outputs, representing their larger contributions to the predicted values.In contrast, for periodical data, Figure4(d) shows a clear seasonality. We can count the interval/periodicity of adjacent purple lines (weights with high value) as 24 (for one day). Specifically, the value of the first output time step strongly depends on the 1 th , 25 th , 49 th ... time steps of the input. A similar phenomenon appears in the trend layer shown in Figure4(c).</figDesc><graphic url="image-16.png" coords="7,215.87,588.01,71.85,71.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the weights(T*L) of DLinear. (a) and (c) are weights in the remainder layer. (b) and (d) are weights in the trend layer. Models are trained with a look-back window L=96 (X-axis) and a forecasting length T=96 (Y-axis) on the Exchange-Rate and Electricity datasets, respectively.</figDesc><graphic url="image-14.png" coords="7,117.72,588.01,71.85,71.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the nine benchmark datasets.</figDesc><table><row><cell>Datasets</cell><cell>ETTh1&amp;ETTh2</cell><cell>ETTm1 &amp;ETTm2</cell><cell>Traffic</cell><cell>Electricity</cell><cell>Exchange-Rate</cell><cell>Weather</cell><cell>ILI</cell></row><row><cell>Variates</cell><cell>7</cell><cell>7</cell><cell>862</cell><cell>321</cell><cell>8</cell><cell>21</cell><cell>7</cell></row><row><cell>Timesteps</cell><cell>17,420</cell><cell>69,680</cell><cell>17,544</cell><cell>26,304</cell><cell>7,588</cell><cell>52,696</cell><cell>966</cell></row><row><cell>Granularity</cell><cell>1hour</cell><cell>5min</cell><cell>1hour</cell><cell>1hour</cell><cell>1day</cell><cell>10min</cell><cell>1week</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Long-term forecasting errors in terms of MSE and MAE, the lower the better. Among them, four datasets are with look-back window size L = 96 and forecasting horizon T ? {96, 192, 336, 720}. For the ILI dataset, L = 36 and T ? {24, 36, 48, 60}. Repeat-C repeats the last value of the look-back window. The best results are highlighted in red bold and the second best results are highlighted with a blue underline.</figDesc><table><row><cell cols="2">Methods Metric</cell><cell>96</cell><cell>Electricity 192 336 720</cell><cell>96</cell><cell>Exchange-Rate 192 336 720</cell><cell>96</cell><cell>Traffic 192 336 720</cell><cell>96</cell><cell>Weather 192 336 720</cell><cell>24</cell><cell>36</cell><cell>ILI</cell><cell>48</cell><cell>60</cell></row><row><cell>DLinear-S*</cell><cell cols="14">MSE 0.194 0.193 0.206 0.242 0.078 0.159 0.274 0.558 0.650 0.598 0.605 0.645 0.196 0.237 0.283 0.345 2.398 2.646 2.614 2.804 MAE 0.276 0.280 0.296 0.329 0.197 0.292 0.391 0.574 0.396 0.370 0.373 0.394 0.255 0.296 0.335 0.381 1.040 1.088 1.086 1.146</cell></row><row><cell>DLinear-I*</cell><cell cols="14">MSE 0.184 0.184 0.197 0.234 0.084 0.157 0.236 0.626 0.647 0.602 0.607 0.646 0.164 0.209 0.263 0.338 3.015 2.737 2.577 2.821 MAE 0.270 0.273 0.289 0.323 0.216 0.298 0.379 0.634 0.403 0.375 0.377 0.398 0.237 0.282 0.327 0.380 1.192 1.036 1.043 1.091</cell></row><row><cell>FEDformer</cell><cell cols="14">MSE 0.193 0.201 0.214 0.246 0.148 0.271 0.460 1.195 0.587 0.604 0.621 0.626 0.217 0.276 0.339 0.403 3.228 2.679 2.622 2.857 MAE 0.308 0.315 0.329 0.355 0.278 0.380 0.500 0.841 0.366 0.373 0.383 0.382 0.296 0.336 0.380 0.428 1.260 1.080 1.078 1.157</cell></row><row><cell>Autoformer</cell><cell cols="14">MSE 0.201 0.222 0.231 0.254 0.197 0.300 0.509 1.447 0.613 0.616 0.622 0.660 0.266 0.307 0.359 0.419 3.483 3.103 2.669 2.770 MAE 0.317 0.334 0.338 0.361 0.323 0.369 0.524 0.941 0.388 0.382 0.337 0.408 0.336 0.367 0.395 0.428 1.287 1.148 1.085 1.125</cell></row><row><cell>Informer</cell><cell cols="14">MSE 0.274 0.296 0.300 0.373 0.847 1.204 1.672 2.478 0.719 0.696 0.777 0.864 0.300 0.598 0.578 1.059 5.764 4.755 4.763 5.264 MAE 0.368 0.386 0.394 0.439 0.752 0.895 1.036 1.310 0.391 0.379 0.420 0.472 0.384 0.544 0.523 0.741 1.677 1.467 1.469 1.564</cell></row><row><cell>Pyraformer*</cell><cell cols="14">MSE 0.386 0.378 0.376 0.376 1.748 1.874 1.943 2.085 0.867 0.869 0.881 0.896 0.622 0.739 1.004 1.420 7.394 7.551 7.662 7.931 MAE 0.449 0.443 0.443 0.445 1.105 1.151 1.172 1.206 0.468 0.467 0.469 0.473 0.556 0.624 0.753 0.934 2.012 2.031 2.057 2.100</cell></row><row><cell>LogTrans</cell><cell cols="14">MSE 0.258 0.266 0.280 0.283 0.968 1.040 1.659 1.941 0.684 0.685 0.734 0.717 0.458 0.658 0.797 0.869 4.480 4.799 4.800 5.278 MAE 0.357 0.368 0.380 0.376 0.812 0.851 1.081 1.127 0.384 0.390 0.408 0.396 0.490 0.589 0.652 0.675 1.444 1.467 1.468 1.560</cell></row><row><cell>Reformer</cell><cell cols="14">MSE 0.312 0.348 0.350 0.340 1.065 1.188 1.357 1.510 0.732 0.733 0.742 0.755 0.689 0.752 0.639 1.130 4.400 4.783 4.832 4.882 MAE 0.402 0.433 0.433 0.420 0.829 0.906 0.976 1.016 0.423 0.420 0.420 0.423 0.596 0.638 0.596 0.792 1.382 1.448 1.465 1.483</cell></row><row><cell>Repeat-C*</cell><cell cols="14">MSE 1.588 1.595 1.617 1.647 0.081 0.167 0.305 0.823 2.723 2.756 2.791 2.811 0.259 0.309 0.377 0.465 6.587 7.130 6.575 5.893 MAE 0.946 0.950 0.961 0.975 0.196 0.289 0.396 0.681 1.079 1.087 1.095 1.097 0.254 0.292 0.338 0.394 1.701 1.884 1.798 1.677</cell></row></table><note><p><p><p>-Methods * are implemented by us; Other results are from FEDformer</p><ref type="bibr" target="#b29">[29]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Long-term forecasting errors on four ETT benchmarks with look-back window size L = 96 and forecasting horizon T ? {96, 192, 336, 720}. The best results are highlighted in red bold and the second best results are highlighted with a blue underline.</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">DLinear-S*</cell><cell cols="2">FEDformer</cell><cell cols="2">Autoformer</cell><cell cols="2">Informer</cell><cell cols="2">Pyraformer*</cell><cell cols="2">LogTrans</cell><cell cols="2">Reformer</cell></row><row><cell cols="2">Metric</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell></row><row><cell>ET T h1</cell><cell>96 192 336 720</cell><cell>0.386 0.437 0.481 0.519</cell><cell>0.400 0.432 0.459 0.516</cell><cell>0.376 0.420 0.459 0.506</cell><cell>0.419 0.448 0.465 0.507</cell><cell>0.449 0.500 0.521 0.514</cell><cell>0.459 0.482 0.496 0.512</cell><cell>0.865 1.008 1.107 1.181</cell><cell>0.713 0.792 0.809 0.865</cell><cell>0.664 0.790 0.891 0.963</cell><cell>0.612 0.681 0.738 0.782</cell><cell>0.878 1.037 1.238 1.135</cell><cell>0.740 0.824 0.932 0.852</cell><cell>0.837 0.923 1.097 1.257</cell><cell>0.728 0.766 0.835 0.889</cell></row><row><cell>ET T h2</cell><cell>96 192 336 720</cell><cell>0.295 0.452 0.504 0.577</cell><cell>0.352 0.462 0.490 0.538</cell><cell>0.346 0.429 0.496 0.463</cell><cell>0.388 0.439 0.487 0474</cell><cell>0.358 0.456 0.482 0.515</cell><cell>0.397 0.452 0.486 0.511</cell><cell>3.755 5.602 4.721 3.647</cell><cell>1.525 1.931 1.835 1.625</cell><cell>0.645 0.788 0.907 0.963</cell><cell>0.597 0.683 0.747 0.783</cell><cell>2.116 4.315 1.124 3.188</cell><cell>1.197 1.635 1.604 1.540</cell><cell>2.626 11.12 9.323 3.874</cell><cell>1.317 2.979 2.769 1.697</cell></row><row><cell>ET T m1</cell><cell>96 192 336 720</cell><cell>0.345 0.380 0.413 0.474</cell><cell>0.372 0.389 0.413 0.453</cell><cell>0.379 0.426 0.445 0.543</cell><cell>0.419 0.441 0.459 0.490</cell><cell>0.505 0.553 0.621 0.671</cell><cell>0.475 0.496 0.537 0.561</cell><cell>0.672 0.795 1.212 1.166</cell><cell>0.571 0.669 0.871 0.823</cell><cell>0.543 0.557 0.754 0.908</cell><cell>0.510 0.537 0.655 0.724</cell><cell>0.600 0.837 1.124 1.153</cell><cell>0.546 0.700 0.832 0.820</cell><cell>0.538 0.658 0.898 1.102</cell><cell>0.528 0.592 0.721 0.841</cell></row><row><cell>ET T m2</cell><cell>96 192 336 720</cell><cell>0.183 0.260 0.336 0.415</cell><cell>0.273 0.325 0.367 0.423</cell><cell>0.203 0.269 0.325 0.421</cell><cell>0.287 0.328 0.366 0.415</cell><cell>0.255 0.281 0.339 0.433</cell><cell>0.339 0.340 0.372 0.432</cell><cell>0.365 0.533 1.363 3.379</cell><cell>0.453 0.563 0.887 1.338</cell><cell>0.435 0.730 1.201 3.625</cell><cell>0.507 0.673 0.845 1.451</cell><cell>0.768 0.989 1.334 3.048</cell><cell>0.642 0.757 0.872 1.328</cell><cell>0.658 1.078 1.549 2.631</cell><cell>0.619 0.827 0.972 1.242</cell></row><row><cell cols="8">-Methods* are implemented by us; Other results are from FEDformer [29].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>datasets with Transformer-based solutions and DLinear: Electricity (Sequence 1951, Variate 36), Exchange-Rate (Sequence 676, Variate 3), and ETTh2 ( Sequence 1241, Variate 2), where these data have different temporal patterns. When the input length is 96 steps and the output horizon is 336 steps, Transformers [28, 27, 29] fail to capture the scale and bias of the future data on Electricity and ETTh2. Moreover, they can hardly predict a proper trend on aperiodic data such as Exchange-Rate. These phenomena further indicate the inadequacy of existing Transformers for the TSF task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of forecasting errors (MSE) with different training sizes.</figDesc><table><row><cell cols="2">Methods DLinear</cell><cell>FEDformer Autoformer</cell></row><row><cell cols="3">Dataset Ori. Short Ori. Short Ori. Short</cell></row><row><cell>96</cell><cell cols="2">0.650 0.631 0.587 0.568 0.613 0.594</cell></row><row><cell>192</cell><cell cols="2">0.598 0.582 0.604 0.584 0.616 0.621</cell></row><row><cell>336</cell><cell cols="2">0.605 0.589 0.621 0.601 0.622 0.621</cell></row><row><cell>720</cell><cell cols="2">0.645 0.628 0.626 0.608 0.660 0.650</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The impact of different embedding strategies on Transformer-based methods with look-back window size L = 96 and forecasting horizon T ? {96, 192, 336, 720}. The metric used is MSE.</figDesc><table><row><cell>Methods</cell><cell>Embedding</cell><cell>96</cell><cell>Electricity 192 336 720</cell><cell>96</cell><cell>Traffic 192 336 720</cell></row><row><cell></cell><cell>All</cell><cell cols="4">0.189 0.198 0.210 0.248 0.597 0.606 0.627 0.649</cell></row><row><cell>FEDformer</cell><cell>wo/Pos. wo/Temp.</cell><cell cols="4">0.193 0.201 0.214 0.246 0.587 0.604 0.621 0.626 0.209 0.213 0.222 0.265 0.613 0.623 0.650 0.677</cell></row><row><cell></cell><cell cols="5">wo/Pos.-Temp. 0.203 0.211 0.222 0.250 0.613 0.622 0.648 0.663</cell></row><row><cell></cell><cell>All</cell><cell cols="4">0.193 0.227 0.252 0.252 0.629 0.647 0.676 0.638</cell></row><row><cell>Autoformer</cell><cell>wo/Pos. wo/Temp.</cell><cell cols="4">0.193 0.201 0.214 0.246 0.613 0.616 0.622 0.660 0.206 0.243 0.303 0.302 0.681 0.665 0.908 0.769</cell></row><row><cell></cell><cell cols="5">wo/Pos.-Temp. 0.215 0.308 0.506 0.729 0.672 0.811 1.133 1.300</cell></row><row><cell></cell><cell>All</cell><cell cols="4">0.274 0.296 0.300 0.373 0.719 0.696 0.777 0.864</cell></row><row><cell>Informer</cell><cell>wo/Pos. wo/Temp.</cell><cell cols="4">0.436 0.599 0.599 0.670 1.035 1.186 1.307 1.472 0.332 0.357 0.363 0.407 0.754 0.780 0.903 1.259</cell></row><row><cell></cell><cell cols="5">wo/Pos.-Temp. 0.524 0.651 0.801 0.954 1.038 1.351 1.491 1.512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of method efficiency on the Electricity dataset with a look-back window size of 96 and forecasting horizon of 720 steps. MACs are the number of multiply-accumulate operations. The inference time is an average result of 5 runs.</figDesc><table><row><cell>Method</cell><cell cols="2">MACs Parameter Time Memory</cell><cell>Time</cell><cell cols="2">Memory Test Step</cell></row><row><cell>DLinear</cell><cell>0.04G 139.7K</cell><cell>0.4ms 687MiB</cell><cell>O (L)</cell><cell>O (L)</cell><cell>1</cell></row><row><cell cols="4">Transformer? 4.03G 13.61M 26.8ms 6091MiB O L 2</cell><cell>O L 2</cell><cell>1</cell></row><row><cell>Informer</cell><cell cols="4">3.93G 14.39M 49.3ms 3869MiB O (LlogL) O (LlogL)</cell><cell>1</cell></row><row><cell cols="5">Autoformer 4.41G 14.91M 164.1ms 7607MiB O (LlogL) O (LlogL)</cell><cell>1</cell></row><row><cell cols="3">Pyraformer 0.80G 241.4M  *  3.4ms 7017MiB</cell><cell>O (L)</cell><cell>O (L)</cell><cell>1</cell></row><row><cell cols="3">FEDformer 4.41G 20.68M 40.5ms 4143MiB</cell><cell>O (L)</cell><cell>O (L)</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>6</head><label></label><figDesc>Conclusion and Future WorkConclusion. This work questions the effectiveness of emerging favored Transformer-based solutions for long-term time series forecasting. We introduce an embarrassingly simple linear model DLinear as a DMS forecasting baseline for comparison. Our results show that DLinear outperforms existing Transformer-based solutions on nine widely-used benchmarks in most cases, often by a large margin.Future work. DLinear model has limited model capacity, and it merely serves a simple yet competitive baseline with strong interpretability for future research. For example, the one-layer linear network cannot capture the temporal dynamics caused by change points<ref type="bibr" target="#b24">[24]</ref>. Consequently, we believe there is much potential for new model designs to tackle the challenging LTSF problem on complex time series. Moreover, with Transformers' questionable temporal relation extraction capability, we advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., classification and anomaly detection) in the future. The MSE results (Y-axis) of models with different look-back window sizes (X-axis) of the long-term forecasting (e.g., 720-time steps) and the short-term forecasting (e.g., 24 time steps) on different benchmarks. 720}, which represents {4, 8, 12, 16, 20, 24, 28, 32, 56, 84, 112, 120} hours. The forecasting steps is {24, 720}, which are {4, 120} hours. For weekly granularity dataset (ILI), we set the look-back window size as {26, 52, 78, 104, 130, 156, 208}, which represents {0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4} years. The forecasting steps is {26, 208}, which are {0.5, 4} years.</figDesc><table><row><cell>0.8 0.9 1.0</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>1.2 1.4</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>1.0 1.2</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>4 5</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell></row><row><cell>0.3 0.4 0.5 0.6 0.7</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>0.6 0.8 1.0</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>0.2 0.4 0.6 0.8</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>1 2 3</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell></row><row><cell cols="7">(a) 24 steps-ETTh1</cell><cell cols="7">(b) 720 steps-ETTh1</cell><cell cols="7">(c) 24 steps-ETTh2</cell><cell cols="7">(d) 720 steps-ETTh2</cell></row><row><cell>0.55</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>1.2</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell></cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell></cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell></row><row><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.20 0.25 0.30 0.35 0.40 0.45</cell><cell>24</cell><cell>36</cell><cell>48</cell><cell>60</cell><cell>72</cell><cell>144 288</cell><cell>0.4 0.5 0.6 0.7 0.8 0.9 1.0</cell><cell>24</cell><cell>36</cell><cell>48</cell><cell>60</cell><cell>72</cell><cell>144 288</cell><cell>0.10 0.15 0.20 0.25</cell><cell>24</cell><cell>36</cell><cell>48</cell><cell>60</cell><cell>72</cell><cell>144 288</cell><cell>1 2 3</cell><cell>24</cell><cell>36</cell><cell>48</cell><cell>60</cell><cell>72</cell><cell>144 288</cell></row><row><cell cols="7">(e) 24 steps-ETTm1</cell><cell cols="7">(f) 576 steps-ETTm1</cell><cell cols="7">(g) 24 steps-ETTm2</cell><cell cols="7">(h) 576 steps-ETTm2</cell></row><row><cell>0.35 0.40 0.45 0.50</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>1.2 1.4 1.6</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>0.7 0.8 0.9</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>1.2 1.4</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell></row><row><cell>0.10 0.15 0.20 0.25 0.30</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>0.4 0.6 0.8 1.0</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>0.4 0.5 0.6</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>0.4 0.6 0.8 1.0</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell></row><row><cell cols="7">(i) 24 steps-Weather</cell><cell cols="7">(j) 720 steps-Weather</cell><cell cols="7">(k) 24 steps-Traffic</cell><cell cols="7">(l) 720 steps-Traffic</cell></row><row><cell>0.2 0.4 0.6 0.8 1.0 1.2 1.4</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell cols="2">Autoformer FEDformer</cell><cell>Pyraformer DLinear</cell><cell>1.0 1.5 2.0 2.5 3.0</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell>Autoformer FEDformer</cell><cell></cell><cell>Pyraformer DLinear</cell><cell>3 4 5 6 7</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell>Autoformer FEDformer</cell><cell></cell><cell>Pyraformer DLinear</cell><cell>3 4 5 6 7 8</cell><cell cols="2">Transformer Informer</cell><cell></cell><cell>Autoformer FEDformer</cell><cell></cell><cell>Pyraformer DLinear</cell></row><row><cell>0.0</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>0.5</cell><cell cols="6">24 48 72 96 120 144 168 192 336 504 672 720</cell><cell>2</cell><cell>26</cell><cell>52</cell><cell>78</cell><cell cols="3">104 130 156 208</cell><cell></cell><cell>26</cell><cell>52</cell><cell>78</cell><cell cols="3">104 130 156 208</cell></row><row><cell cols="7">(m) 24 steps-Exchange</cell><cell cols="7">(n) 720 steps-Exchange</cell><cell></cell><cell cols="5">(o) steps-ILI</cell><cell></cell><cell></cell><cell cols="5">(p) 60 steps-ILI</cell><cell></cell></row><row><cell cols="3">Figure 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>, the performance of DLinear can largely surpass the SOTA Transformer-based methods (FEDformer and Autoformer). Specifically, DLinear outperforms FEDformer by over 40% on Exchange rate, around 30% on Traffic, Electricity, and Weather, and around 25% on ETTm1 and Weather.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Multivariate long sequence time-series forecasting results on nine benchmarks with a long look-back window size L = 336 of DLinear and forecasting horizon T ? {96, 192, 336, 720}.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix:</head><p>In this appendix, we provide detailed experimental settings in Section A, more comparisons under different look-back window sizes in Section B, more analyses about the impact of decomposition on our DLinear model, and the visualization of DLinear on other benchmarks in Section C. We also append our code to reproduce the results shown in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data Descriptions</head><p>We use nine wildly-used benchmarks in the main paper. We detail them in the following.</p><p>? ETT (Electricity Transformer Temperature) <ref type="bibr" target="#b28">[28]</ref> 2 consists of two hourly-level datasets (ETTh) and two 15-minute-level datasets (ETTm). Each of them contains seven oil and load features of electricity transformers from July 2016 to July 2018.</p><p>? Traffic 3 describes the road occupancy rates. It contains the hourly data recorded by the sensors of San Francisco freeways from 2015 to 2016.</p><p>? Electricity 4 collects the hourly electricity consumption of 321 clients from 2012 to 2014.</p><p>? Exchange-Rate <ref type="bibr" target="#b15">[15]</ref> 5 collects the daily exchange rates of 8 countries from 1990 to 2016.</p><p>? Weather 6 includes 21 indicators of weather, such as air temperature, and humidity. Its data is recorded every 10 min for 2020 in Germany.</p><p>? ILI 7 describes the ratio of patients seen with influenza-like illness and the total number of the patients. It includes the weekly data from the Centers for Disease Control and Prevention of the United States from 2002 to 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>For existing Transformer-based TSF solutions: the implementation of Autoformer <ref type="bibr" target="#b27">[27]</ref>, Informer <ref type="bibr" target="#b28">[28]</ref>, and the vanilla Transformer <ref type="bibr" target="#b25">[25]</ref> are all taken from the Autoformer work <ref type="bibr" target="#b27">[27]</ref>; the implementation of FEDformer <ref type="bibr" target="#b29">[29]</ref> and Pyraformer <ref type="bibr" target="#b18">[18]</ref> are from their respective code repository. We also adopt their default hyper-parameters to train the models. For DLinear, to obtaining a smooth weight with a clear pattern, we initialize the weights of the linear layers in DLinear as 1/L rather than random initialization. That is, we use the same weight for every forecasting time step in the look-back window at the start of training. For more hyper-parameters of DLinear, please refer to our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Comparison with Transformers</head><p>In Figure <ref type="figure">5</ref>  In this experiment, we explore the impact of the decomposition operation of DLinear on different benchmarks. Accordingly, we remove the decomposition scheme, and the remained model is simply a one-layer linear network to transform the historical time series into the forecasting results. In Table <ref type="table">8</ref>, we observe that the decomposition scheme is beneficial when there is a clear trend in the specific dataset, e.g., Exchange Rate, ILI, and ETT, and it is not effective for those datasets without a clear trend such as Traffic and Weather, which is expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Weight Visualization</head><p>The weight visualization of DLinear can reveal certain characteristics in the data used for forecasting.</p><p>In addition to the visualizations given in the main paper, we visualize the trend and remainder weights of other datasets with a fixed input length of 96 and four different forecasting horizons.</p><p>In-96, Out-96 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exchange-Rate</head><p>In-36, Out-24 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Methods</forename><surname>Dlinear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-S</forename><surname>Dlinear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-I</forename><surname>Fedformer-F Fedformer-W</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Autoformer</forename><surname>Metric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mse Mae Mse Mae Mse Mae Mse Mae Mse</forename><surname>Mae</surname></persName>
		</author>
		<idno>ETTh1 96 0.375 0.399 0.377 0.397 0.376 0.419 0.395 0.424 0.449 0.459 192 0.405 0.416 0.413 0.421 0.420 0.448 0.469 0.470 0.500 0.482 336 0.439 0.443 0.440 0.439 0.459 0.465 0.530 0.499 0.521 0.496 720 0.472 0.490 0.476 0.481 0.506 0.507 0.598 0.544 0.514 0.512</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stock price prediction using the arima model</title>
		<author>
			<persName><forename type="first">Adewumi</forename><forename type="middle">O</forename><surname>Adebiyi A Ariyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">K</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><surname>Ayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 UKSim-AMSS 16th International Conference on Computer Modelling and Simulation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="106" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Computation and Language</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Direct multi-step estimation and forecasting</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Chevillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Surveys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="785" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stl : A seasonal-trend decomposition procedure based on loess</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Cleveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Office Statistics</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-tosequence model for speech recognition</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A granular time series approach to long-term forecasting and trend forecasting</title>
		<author>
			<persName><forename type="first">Ruijun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Witold</forename><surname>Pedrycz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">387</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3253" to="3270" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Shereen</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Thyssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Samer Jomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02118</idno>
		<title level="m">Do we really need deep learning models for time series forecasting? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><surname>Jerome H Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exponential smoothing: The state of the art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Everette</surname></persName>
		</author>
		<author>
			<persName><surname>Gardner</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Time series analysis</title>
		<author>
			<persName><forename type="first">James</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamilton</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Princeton university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Harvey</surname></persName>
		</author>
		<title level="m">Forecasting, structural time series models and the kalman filter</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international acm sigir conference on research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Yao Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Time series is a special sequence: Forecasting with sample convolution and interaction</title>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuxia</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09305</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting</title>
		<author>
			<persName><forename type="first">Shizhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schahram</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal motion prediction with stacked transformers</title>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7577" to="7586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for time series forecasting</title>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Petneh?zi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00069</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepar: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recursive and direct multi-step forecasting: the best of both worlds</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Souhaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><forename type="middle">J</forename><surname>Taieb</surname></persName>
		</author>
		<author>
			<persName><surname>Hyndman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Forecasting at scale. PeerJ Prepr</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Letham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An evaluation of change point detection algorithms</title>
		<author>
			<persName><forename type="first">Gerrit Jj</forename><surname>Van Den Burg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06222</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07125</idno>
		<title level="m">Transformers in time series: A survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting</title>
		<author>
			<persName><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
