<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting DocRED -Addressing the Overlooked False Negative Problem in Relation Extraction</title>
				<funder ref="#_V3zuYFe">
					<orgName type="full">Swift</orgName>
				</funder>
				<funder ref="#_EB2FcQw">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-25">25 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
							<email>qingyu.tan@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country>of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
							<email>lu.x@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Lidong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University</orgName>
								<address>
									<country>of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
						</author>
						<title level="a" type="main">Revisiting DocRED -Addressing the Overlooked False Negative Problem in Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-25">25 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.12696v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The DocRED dataset is one of the most popular and widely used benchmarks for documentlevel relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., the false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-ofthe-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we propose different metrics to comprehensively evaluate the document-level RE task. 1 * Equal contribution. Qingyu Tan and Lu Xu are under the Joint PhD Program between Alibaba and NUS/SUTD. 1 We make our dataset publicly available at https:// github.com/tonytan48/Re-DocRED.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of relation extraction (RE) is related to knowledge bases (KB). Most popular relation extraction datasets are constructed from knowledge triples in knowledge bases. For example, the TACRED dataset <ref type="bibr" target="#b30">(Zhang et al., 2017)</ref> is constructed by the TAC Knowledge Base Population challenge. The NYT10 <ref type="bibr" target="#b21">(Riedel et al., 2013)</ref> dataset matched the Freebase Knowledge base <ref type="bibr" target="#b2">(Bollacker et al., 2008)</ref> to the New York Times corpus <ref type="bibr" target="#b22">(Sandhaus, 2008)</ref>. Wiki20 <ref type="bibr" target="#b11">(Gao et al., 2021)</ref> and Do-cRED <ref type="bibr" target="#b28">(Yao et al., 2019)</ref>   From the highlighted evidence , it can be inferred that "I Knew You Were Trouble" was produced by both "Max Martin" and "Shellback", but in the previous incomplete DocRED dataset, only the first producer is labeled. from the knowledge bases for a given piece of text. The retrieved triples are based on co-occurrence only, and they may not be related to the context. Thereafter, most existing works <ref type="bibr" target="#b28">(Yao et al., 2019;</ref><ref type="bibr">Alt et al., 2020)</ref> focus on reducing false positives.</p><p>However, the false negative problem in the relation extraction datasets is overlooked. Without resolving this issue, the annotations of the datasets are incomplete. Recent efforts on addressing the false negative problem are from the model perspective <ref type="bibr" target="#b5">(Chen et al., 2021;</ref><ref type="bibr" target="#b15">Hao et al., 2021)</ref>, which aims to denoise the false negative data during training. The challenge for these approaches is that both development and test sets can be incomplete at the same time. Without a complete annotated dataset, the current evaluation is ill-defined. Several reannotation works revised the existing sentencelevel relation extraction datasets. <ref type="bibr">Alt et al. (2020)</ref> re-annotated a small amount of challenging samples in the development and test sets of the TA-CRED dataset <ref type="bibr" target="#b30">(Zhang et al., 2017)</ref>. <ref type="bibr" target="#b24">Stoica et al. (2021)</ref> extended this work and re-annotated the training, development, and test sets of TACRED with a semantically refined label space. Besides, <ref type="bibr" target="#b11">Gao et al. (2021)</ref> re-annotated the test sets of two distantly supervised RE datasets. Even though their discussions and examples emphasized the efforts on correcting the false positive samples, all the revised versions of the datasets have a significant increase of positive samples. That is, many samples that are previously labeled as no_relation (NA) are re-annotated with relation labels in the revised datasets. We show the detailed statistics in Table <ref type="table">1</ref>, which indicates that the false negative problem is prevalent in sentence-level relation extraction. Compared to the sentence-level task, documentlevel relation extraction is more susceptible to the false negative problem. This is primarily because document-level RE involves significantly more entity pairs in a raw text, as shown in Table <ref type="table">1</ref>. Note that the objective of the RE task is to determine the relation types for all entity pairs, and the number of entity pairs is quadratic in the number of entities.</p><p>In this paper, we address the false negative problem in DocRED. We find that the false negative problem originates from two sources. First, although the Wikidata knowledge base provides a good starting point for annotation, it is highly sparse and far from complete. There are many relation triplets that are not in the Wikidata KB. For example, in Figure <ref type="figure" target="#fig_1">1</ref>, the article reflects that the song "I Knew You Were Trouble" was produced by both "Max Martin" and "Shellback", but "Shellback" is not included in both the knowledge base and the DocRED dataset. Second, the DocRED dataset employed the recommend-revise annotation scheme. However, the additional relation triples from the RE model and human annotations do not cover the remaining ground-truth relation triples, and a detailed discussion is given in Section 2. With the incomplete annotated development and test sets, the previous evaluation does not necessarily provide a fair reference. Therefore, we propose to revise the DocRED dataset to recover the incomplete annotations through an iterative approach with a human in the loop. Specifically, we use multiple state-of-the-art document-level RE models to generate relation candidates and ask human annotators to examine the recommended triples. The details of our annotation process are given in Section 3.</p><p>Most recently, <ref type="bibr" target="#b17">Huang et al. (2022)</ref> also identify the false negative issue in DocRED <ref type="bibr" target="#b28">(Yao et al., 2019)</ref>, and they combat the problem by annotating 96 documents from scratch with two expert annotators. However, annotating relation triples from scratch is different from revising recommended triples, and it is difficult to scale up to a dataset of a larger size. We provide a comprehensive analysis between our approach and the annotatingfrom-scratch approach in Appendix A. Compared to <ref type="bibr" target="#b17">Huang et al. (2022)</ref>, our approach is better in the following aspects. First, our dataset is of significantly larger size (4,053 vs 96). Second, the precision of our annotation is higher. Third, our evaluation dataset contains more triples per document than <ref type="bibr" target="#b17">Huang et al. (2022)</ref>, indicating that our dataset better tackles the incompleteness problem of DocRED. Fourth, our dataset annotation approach is more scalable and can be extended to an arbitrary number of relation types.</p><p>Overall, our contributions can be summarized as follows:</p><p>? We identify the overwhelming but neglected false negative problem in relation extraction.</p><p>? We show that the false negative problem is the cause of performance bottleneck on many relation extraction datasets, and we also provide a high-quality revised version of the documentlevel relation extraction dataset, Re-DocRED.</p><p>? Moreover, we propose different metrics to have a more comprehensive evaluation for document-level RE research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background of the DocRED Dataset</head><p>The DocRED dataset <ref type="bibr" target="#b28">(Yao et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problems of the DocRED Dataset</head><p>Based on our empirical analysis of the topperforming models <ref type="bibr">(Tan et al., 2022;</ref><ref type="bibr" target="#b29">Zhang et al., 2021;</ref><ref type="bibr">Zhou et al., 2021b)</ref> on the DocRED leaderboard, many predicted triples are correct but are not annotated in the DocRED dataset. Therefore, it is important to review the dataset and identify the true bottleneck of document-level RE. We identify incomplete annotation as the major issue in the DocRED dataset.</p><p>Incomplete Annotation As mentioned in Section 2.1, it is difficult to annotate a document-level RE dataset from scratch. The DocRED dataset is mainly created by human filtering the recommended relation triples from distantly supervised data and predictions from the RE model. It is worth noting that the construction procedure of the DocRED dataset relies on the underlying assumption that the combination of the recommended triples from the distantly supervised data and the 2 By computing their reported statistics, the sum of the filtered triples is 15.1 per document, which is already more than the reported average number of triples (12.5) in the DocRED dataset.</p><p>RE model contains almost all the ground-truth relation triples in the documents. This assumption is not true since the relations in the Wikidata knowledge base are sparse and incomplete. In the original DocRED production process, only the distantly supervised data is used to train the RE model, which may lead to low-quality prediction. Furthermore, the performance of the previously used RE model is significantly worse than the recent approaches based on pre-trained language models. Therefore, the recommended relation triples from the RE model are likely not enough for covering the ground-truth relation triples for a given text.</p><p>By relying heavily on the relation triple candidates generated from the above two methods, the annotation of the previous DocRED dataset is incomplete. While the recommend-revise scheme is the major source of incompleteness of the original DocRED dataset <ref type="bibr" target="#b28">(Yao et al., 2019)</ref>, a secondary source of incompleteness comes from logical inconsistency. There are many inverse relation pairs in DocRED. For example, if entity A is annotated as a "sibling" of entity B, then entity B is also a "sibling" of entity A. The lack of inverse relation also contributes to the incompleteness problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Preliminary Analysis</head><p>To identify the difficulty of relation extraction, we conduct a preliminary analysis on two evaluation tasks: Relation Extraction (RE) and Positive Relation Classification (PRC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings of Our Analysis</head><p>Given a text T and a set of n entities {e 1 , ..., e n }, the objective of the RE task is to identify a relation type r ? R for each entity pair (e i , e j ). e i and e j denote two different entities, and R is a predefined set of relation types, including no_relation. Under the PRC setting, we do not use any entity pair in no_relation, and the model is trained and evaluated with only entity pairs that have some pre-defined relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Such a setting allows us to assess the difficulty of differentiating the predefined relation types. We use the top-performing models for our preliminary experiments. For the sentence-level task, we use the typed entity marker <ref type="bibr" target="#b32">(Zhou and Chen, 2021</ref>) and RoBERTa-large <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> as our baseline.</p><p>For the document-level task, we use the ATLOP <ref type="bibr">(Zhou et al., 2021b)</ref> as the baseline model, and we use RoBERTa-large as the encoder for DocRED and XLM-R-base <ref type="bibr" target="#b8">(Conneau et al., 2020)</ref> for Ha-cRED.</p><p>Datasets We compare the preliminary investigation results on two sentence-level RE datasets: TACRED <ref type="bibr" target="#b30">(Zhang et al., 2017)</ref> and Re-TACRED <ref type="bibr" target="#b24">(Stoica et al., 2021)</ref>, and two document-level RE datasets: DocRED <ref type="bibr" target="#b28">(Yao et al., 2019)</ref> and Ha-cRED <ref type="bibr" target="#b6">(Cheng et al., 2021)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Preliminary Results of Positive Relation Classification</head><p>By assuming that there is a relation between the entity pairs, the positive relation classification (PRC) task shows the difficulty of classifying the relation types. Note that document-level PRC is a multi-label classification problem. Hence, precision and recall are not necessarily the same. From Table <ref type="table" target="#tab_1">2</ref>, we can see that all the models perform well. The performance of sentence-level RE and document-level RE are comparable, even though document-level RE has a significantly longer context and requires cross-sentence reasoning. This shows that the difficulty of positive relation classification is not severely affected by sentence boundary or context length. Another finding from Table 2 is that the revised version of TACRED has marginally higher performance than the original version. This is expected as the revised version receives an extra round of human annotation. The performance on Incomplete Re-TACRED is only marginally worse than the Re-TACRED, which shows that positive relation classification can achieve a comparatively high performance despite the reduction of training instances. Besides, it is worth noting that the performance on HacRED is higher than the performance on DocRED, even though HacRED claims that it is a semantically harder dataset. Although the baseline models are not exactly the same, we can still infer that the difficulty level of classifying the positive relation types on sentence-level and document-level datasets are not significantly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Preliminary Results of Relation Extraction</head><p>Compared to the setting of positive relation classification, the standard relation extraction task includes all the negative no_relation samples during training. We compare the performance of the previous best approaches on the sentence-level and document-level RE datasets in Table <ref type="table">3</ref>. We observe that the performance on the standard RE task is lower than that on the PRC task. For a specific dataset, the performance on PRC is the upper bound of RE performance, since the evaluation of PRC ignores no_relation. However, with a good quality annotation, the performance of relation extraction and positive relation classification should not have a large gap. For the sentence-level dataset, we can see that the performance on the revised ver- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revising DocRED with an Iterative Approach</head><p>In this section, we describe our iterative humanin-the-loop approach to revising DocRED. The goal is to add the previously unrecognized relation triples to the DocRED dataset. Our iterative approach consists of three steps in each iteration:</p><p>(1) Training scorer models; (2) Scoring the unrecognized triples; and (3) Human filtration. The following sections elaborate the details of each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our Iterative Approach</head><p>Step 1 -Training Scorer Models Even though the annotation of the previous DocRED is incomplete, we can still train neural models on such data.</p><p>We adopt three top-performing models on the current DocRED leaderboard: KD-DocRE <ref type="bibr">(Tan et al., 2022)</ref>, DocuNET <ref type="bibr" target="#b29">(Zhang et al., 2021)</ref>, and AT-LOP <ref type="bibr">(Zhou et al., 2021b)</ref>. We split the datasets into four disjoint parts. We then train each model in a cross-validation manner, i.e., when three parts are used for training, the remaining one part is used for evaluation.</p><p>Step 2 -Scoring the Unrecognized Triples In this step, we aim to generate a large number of relation triple candidates, so that they could cover the missing annotations in the previous DocRED. With the trained scorer models, we can predict the scores for all the enumerated relation triples. To control the number of relation triple candidates for the next step, we define a threshold score to remove the less confident predictions. The predicted relation triples from all the models are then merged together. Due to the different characteristics of these models, we could generate a large and diverse pool of relation triple candidates for the next step.</p><p>Step 3 -Human Filtration After the relation triple candidates are generated from the previous step, each triple candidate will be annotated by humans. The human annotators are asked to read the document and check whether the triples can be inferred from the document. Unlike the annotation process of the original DocRED, we did not ask the annotators to explicitly label the supporting evidence of their judgment. This is mainly because the evidence annotations have a marginal effect on RE and they are hard to obtain during inference time. Each triple will be annotated by two annotators, and a third annotator will resolve the conflicting annotations.</p><p>The above three steps form one round of our iterative approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Revised Re-DocRED</head><p>We conducted two rounds of annotation in total. For the first round, we annotated 4,053 documents that include all training and evaluation documents. On average, we recommended 11.9 triples for each document and 9.4 triples were accepted, with an acceptance rate of 79.0%. The Fleiss Kappa <ref type="bibr" target="#b11">(Fleiss, 1971</ref>) coefficient for round 1 annotation is 0.73, which is considered substantial agreement. To further improve the recall on the evaluation dataset, we conducted a second round of annotation for the 1,000 evaluation documents. We used the annotated 3,053 training samples from round 1 for round 2 training. In this round, 14.1 triples were recommended and only 6.0 triples were accepted, with an acceptance rate of only 42.55%. The Fleiss Kappa for round 2 annotation is 0.66. More details are given in Appendix B. After human annotation, we also add relation triples by manually defining logic rules. In this way, we are able to resolve the problem of logical inconsistency (described in Section 2.2) in the DocRED dataset. These rules mainly consist of inverse relations and co-occurring relations. On average, we added 6.4 triples for each document. See Appendix E for more details.</p><p>Since our main goal is to address the false negative problem in the DocRED dataset, we keep all the existing annotated triples in the original Do-cRED dataset. Overall, our training documents contain 28.1 triples on average, with 9.4 triples added from human annotation and 6.2 triples from logical rules. Our evaluation documents contain 34.7 triples on average, with 15.4 triples added from human annotation and 6.7 triples from logical rules. We divide the 1,000 evaluation documents into 500 development and 500 test documents. The average number of triples of the evaluation documents is higher than that of the training documents. This indicates that the evaluation data has more complete annotation compared to the training data. The detailed statistics of the Re-DocRED dataset are shown in Table <ref type="table" target="#tab_4">4</ref>. The average number of triples per document is significantly higher for Re-DocRED compared to the original DocRED. There are 12.3 triples per document in the original DocRED and 34.7 triples per document in Re-DocRED. This shows that there are approximately 64.6% triples missing in the original Do-cRED dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison on Relation Extraction</head><p>To compare the previous DocRED and our Re-DocRED, we evaluate 4 different approaches on the two datasets. Apart from the three models that are used during our annotation process (Section 3.1), we also compare the performance with an additional approach, JEREX <ref type="bibr" target="#b10">(Eberts and Ulges, 2021)</ref>. Table <ref type="table" target="#tab_6">5</ref> shows the experimental results, and the reported metrics are micro-averaged F1 scores and Ign_F1 scores. The latter refers to the F1 score that ignores the triples that appear in the training set. According to the statistics in Table <ref type="table">1</ref>, even though our revised Re-DocRED dataset contains many more relation triples, we observe that all the baseline models demonstrate significant performance improvement on both development and test sets. Compared to DocRED, the performance of the baseline models on Re-DocRED increased by more than 11 F1 points. When these models are pre-trained with distantly supervised data, we observe consistent performance improvement on Re-DocRED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison on Positive Relation Classification</head><p>Following the experimental setting in Section 2.3.1, we compare the positive relation classification performance between the original DocRED and our Re-DocRED with ATLOP <ref type="bibr">(Zhou et al., 2021b)</ref> in and Table <ref type="table" target="#tab_7">6</ref>. We observe that the performance on Re-DocRED is comparable to the original version. This indicates that our added triples are of comparable quality to the original DocRED data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">More Analysis</head><p>Additional Evaluation Metrics As mentioned in Section 2, document-level relation extraction is a challenging task. Hence, it is necessary to have various performance evaluation metrics so as to conduct a comprehensive evaluation. On top of the standard F1 and Ign_F1 evaluation metrics, we define four additional metrics to assess the models. (1). Freq. F1, which only considers the 10 most common relation types in the training set of Re-DocRED, where these frequent relation types account for 60% of the relation triples. (2). LT F1, which only considers the long-tail (the remaining 86) relation types. (3). Intra F1, which evaluates on relation triples that appear in the same sentence. (4). Inter F1, which evaluates on cross-sentence relation triples.</p><p>We show the comprehensive evaluation results in Table <ref type="table" target="#tab_8">7</ref>. We observe that there exists a relatively large gap between the Freq. F1 and LT F1 metrics, and the difference is around 6-8 F1 points. Such  behavior shows that the frequent relation types are easier to be recognized compared to the long-tail relations. Furthermore, we also find that the performance on triples that appear in the same sentence (Intra F1) is better than that on the cross-sentence relation triples (Inter F1), by around 2-5 F1 points. This is because it is harder to encode long-distance interactions. Therefore, a promising future direction is to match the performance on the long-tail relation types to the frequent types, and also improve the model's representation capability to capture long-distance interactions so as to reduce the differences between the inter-sentence and intrasentence relation triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Distant Supervision Pre-training</head><p>To further analyze the effects of distant supervision pre-training, we examine the performance of the KD-DocRE model with and without the pretraining step, and Table <ref type="table" target="#tab_9">8</ref> shows the results. Under both settings, recall of the long-tail classes is significantly lower than their corresponding precision. Moreover, by comparing the performance of long-tail classes, we observe that pre-training with distantly supervised data improves the precision of the long-tail classes. We show additional error analysis and case studies in Appendix D.</p><p>6 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Relation Extraction</head><p>Relation extraction (RE) is an important task in information extraction and knowledge graph completion. There is a series of RE datasets built over the past decades, and they have significantly advanced the research on RE. The ACE 2005 dataset <ref type="bibr" target="#b25">(Walker et al., 2006)</ref> and SemEval 2010 Task 8 <ref type="bibr" target="#b16">(Hendrickx et al., 2010)</ref> are two sentence-level RE datasets created by human annotation. However, these two datasets have a relatively small number of relation types and instances. The large-scale TACRED <ref type="bibr" target="#b30">(Zhang et al., 2017)</ref> dataset is created based on the 2009-2014 TAC knowledge base population (KBP) challenges and crowd-sourced human annotations. FewRel <ref type="bibr" target="#b14">(Han et al., 2018)</ref> and FewRel 2.0 <ref type="bibr" target="#b12">(Gao et al., 2019)</ref> have been proposed to study the transferability and few-shot capability of RE models. However, early relation extraction datasets mainly focus on sentence-level RE, whereas many relations can only be expressed by multiple sentences. The document-level relation extraction task has been proposed to build RE systems that are able to extract relations from multiple entities and sentences. <ref type="bibr" target="#b28">Yao et al. (2019)</ref> have created the DocRED dataset by distant supervision from Wikipedia articles and the Wikidata knowledge base, then sampled 5,053 documents for human annotation. The annotation strategy of DocRED is mainly based on machine recommendation and human filtering. With a similar approach, <ref type="bibr" target="#b6">Cheng et al. (2021)</ref> has created a Chinese document-level RE dataset that focuses on hard relation cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Machine-Assisted Data Generation</head><p>Since labeled data is expensive to obtain for complex NLP tasks, there is much research on generating labeled data in an automatic fashion. Distant supervision was first used by <ref type="bibr" target="#b20">Mintz et al. (2009)</ref> to generate large amounts of relation extraction data without human efforts. Prior work on automatic data generation mainly relies on rule-based pattern matching <ref type="bibr">(Lehmann et al., 2015)</ref> and web crawling <ref type="bibr" target="#b4">(Buck et al., 2014)</ref>. These types of rulebased methods are susceptible to noise propagation. With the rapid development of pre-trained language models (PLMs; <ref type="bibr" target="#b9">Devlin et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019 ;</ref><ref type="bibr" target="#b3">Brown et al., 2020)</ref>, much recent research explores methods that leverage PLMs for automatic data generation <ref type="bibr" target="#b1">(Anaby-Tavor et al., 2020;</ref><ref type="bibr">Zhou et al., 2021a;</ref><ref type="bibr" target="#b27">Yang et al., 2020;</ref><ref type="bibr" target="#b18">Kumar et al., 2020)</ref>. However, these methods typically depend on a certain set of supervised source data. Another line of work utilizes manually designed prompts and instructions to generate data in an unsupervised manner <ref type="bibr" target="#b23">(Schick and Sch?tze, 2021;</ref><ref type="bibr" target="#b7">Chia et al., 2022)</ref>. Although these methods improve the performance of certain downstream tasks, the quality of the machine-generated data still does not match human annotation. To mitigate noise from the machine-generated data, <ref type="bibr" target="#b26">West et al. (2021)</ref> generate a large amount of commonsense knowledge data and employ human annotators to filter the generated candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In conclusion, this paper identifies the causes and effects of the overwhelming but neglected false negative problem in relation extraction. We show that the false negative problem is the cause of the performance bottleneck on many RE datasets. We have also provided a high-quality revised version of the document-level RE dataset DocRED. Moreover, we have proposed different metrics to achieve a more comprehensive evaluation for documentlevel RE. We have also conducted a thorough error analysis on state-of-the-art RE models.</p><p>A Our Machine-Guided Annotation vs.</p><p>Annotation From Scratch</p><p>In this section, we compare our Re-DocRED dataset with a concurrent work <ref type="bibr" target="#b17">(Huang et al., 2022)</ref> on revising the DocRED dataset. Our work uses machine-guided annotation methods, whereas their work asks the annotators to annotate from scratch (denoted as "From Scratch"). As mentioned in <ref type="bibr" target="#b17">Huang et al. (2022)</ref>, annotating from scratch is an extremely challenging task. This is mainly due to the quadratic complexity of the document-level RE task. Suppose there are N entities in one document and the label space of interest contains R relations. The search space for human annotation is N * (N -1) * R. In particular, for an average case of DocRED (N =20, R=96), annotators will need to make around 36,480 classification decisions for one document. In contrast, for machineguided generation, annotators will only need to make decisions on the recommended candidates, averaging only 25.5 decisions per document. This is primarily due to the pattern recognition capability of deep neural models, which significantly reduces the search space for human annotators. The size of our re-annotated dataset is much larger, and we have conducted significantly more experimental analysis.</p><p>We examined and analyzed all the annotated 96 documents of "From Scratch". We found that there are several types of systematic errors in <ref type="bibr" target="#b17">Huang et al. (2022)</ref> and we show the error types in Table <ref type="table" target="#tab_11">10</ref>. Firstly, annotation from scratch is susceptible to annotators' misunderstanding of relation definition (25.5%). For the first example in Table <ref type="table" target="#tab_10">9</ref>, the relation between architect and its designed building is architect 3 , whereas annotators in <ref type="bibr" target="#b17">Huang et al. (2022)</ref> deem such relation as creator 4 . This is imprecise as the architect relation was not in the label space of DocRED, therefore, it is not possible to find this relation between the architect and its design. Secondly, annotating from scratch is susceptible to human commonsense bias (19.0%). This is primarily due to human's memorization of popular entities, such as countries and geographical locations (example 3 in Table <ref type="table" target="#tab_10">9</ref>). The third major error type is due to the slippery slope logical fallacy, as shown by example 4 in   <ref type="formula">2022</ref>) are already experts in English and the annotators went through discussion after annotation. However, there are still a considerable number of errors from their dataset. We believe that this is due to the complex nature of this annotation task. In the meantime, we also conducted human-evaluation on our Re-DocRED dataset and compared the precision of the two datasets in Table 11. We can see that our Re-DocRED dataset has significantly higher precision for the added triples. Moreover, we compare the unit price and unit time for different annotation strategies in Table <ref type="table" target="#tab_13">12</ref>. We can see that annotating from scratch costs three times more than our machine-guided annotation.</p><p>Hence, by comparing the two approaches for annotating document-level relation extraction datasets, we conclude that:</p><p>1. Even though the annotation "From Scratch" is conducted by human experts, there are still missing triples in the annotated 96 documents. That is, annotating from scratch does not completely eliminate the incompleteness problem when the number of entities N and relation types R are large.</p><p>2. Annotation from scratch is not as precise as recommend-revise. As Table <ref type="table" target="#tab_10">9</ref> shows, human annotation of <ref type="bibr" target="#b17">Huang et al. (2022)</ref> contains several types of systematic errors.</p><p>3. Annotating from scratch is hard to scale. According to <ref type="bibr" target="#b17">Huang et al. (2022)</ref> and feedback from our annotators, it takes more than 30 minutes by experts to annotate one document. Then the two experts will still spend extra time discussing and resolving the conflicts.</p><p>4. The recommend-revise scheme is able to mitigate the false negative problem and is easier to scale up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Relation Annotation</head><p>As mentioned in Section 3.1, we use three topperforming models on the DocRED leaderboard for relation candidate generation, and the three models are: (1) <ref type="bibr">KD-DocRE (Tan et al., 2022)</ref>, (2) DocuNET <ref type="bibr" target="#b29">(Zhang et al., 2021)</ref>, and (3) AT-LOP <ref type="bibr">(Zhou et al., 2021b)</ref>. To obtain relation triple candidates for all 4,053 documents, split the human-annotated subset into 4 different splits (Table <ref type="table" target="#tab_4">14</ref>), with the first split as the original DocRED development set. This is to ensure that the number of training samples is comparable to the original DocRED when any three of the splits are used for training our scorer models, and then the remaining one is used for prediction. Following the training paradigm of Tan et al. ( <ref type="formula">2022</ref>), we first pre-train each model with the distantly supervised data in DocRED. Then, the pre-trained model is continuetrained on any three splits of DocRED, and we then use the trained models to make predictions on the remaining split set. Therefore, it requires four times of training and inference so that we can get the predictions for all the four split sets. To further increase the number of relation candidates, we set the dynamic threshold to 0.9 of the models for the Adaptive Threshold class <ref type="bibr">(Zhou et al., 2021b</ref>) for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Coreferential Error Annotation</head><p>Coreferential Errors Besides the major problem of incomplete annotation, coreferential errors are also detrimental to the evaluation of DocRED. Note that an entity in the DocRED dataset can have multiple mention appearances in a document. If some mentions that are referring to the same entity are not included in the entity cluster, a redundant entity cluster will be formed. This kind of coreferential error will affect the relation predictions involving the redundant entity. Since the complexity of the DocRE task is quadratic in the number of entities, it is important to make sure that the coreferential annotations are correct. Errors in the coreferential annotation can be propagated and amplified during relation extraction.</p><p>Coreferential Annotation As mentioned above, there are a considerable number of entities of Do-cRED that have the same surface names but refer to different entities. In such cases, we examined all entities that contain the same surface name and entity types in the DocRED dataset. For these entities, annotators will need to decide whether: (1) the two entities are coreferential to each other, (2) the overlapping mentions are wrongly grouped to a certain entity cluster, and (3) the two mentions are indeed referring to two separate entities. As a result, we merged 102 coreferential entity pairs in the evaluation documents and 122 pairs in the training documents. Therefore, the average number of entities per document of Re-DocRED is slightly lower than the original DocRED.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>originated from distant supervision from the Wikidata knowledge base and Wikipedia articles. By exploiting distant supervision, relation triple candidates can be retrieved DocRED: (I Knew You Were Trouble, Max Martin, producer); (Taylor Swift, the United States, country of citizenship) ... Re-DocRED: (I Knew You Were Trouble, Shellback, producer); (Locked Out of Heaven, Bruno Mars, performer) ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: A sample document in our revised DocRED dataset. From the highlighted evidence , it can be inferred that "I Knew You Were Trouble" was produced by both "Max Martin" and "Shellback", but in the previous incomplete DocRED dataset, only the first producer is labeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Preliminary experimental results of positive relation classification.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Sentence level</cell><cell></cell><cell></cell></row><row><cell>TACRED</cell><cell cols="3">93.38 93.38 93.38</cell></row><row><cell>Re-TACRED</cell><cell cols="3">96.83 96.83 96.83</cell></row><row><cell cols="4">Incomplete Re-TACRED 96.53 96.53 96.53</cell></row><row><cell>Document level</cell><cell></cell><cell></cell></row><row><cell>DocRED</cell><cell cols="3">93.84 89.68 91.71</cell></row><row><cell>HacRED</cell><cell cols="3">95.87 90.13 94.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics of our Re-DocRED dataset and the DocRED dataset<ref type="bibr" target="#b28">(Yao et al., 2019)</ref>.</figDesc><table><row><cell></cell><cell cols="3">Re-DocRED</cell><cell cols="2">DocRED</cell></row><row><cell></cell><cell cols="5">Train Dev Test Train Dev</cell></row><row><cell># Documents</cell><cell cols="5">3,053 500 500 3,053 1,000</cell></row><row><cell>Avg. # Entities</cell><cell cols="4">19.4 19.4 19.6 19.5</cell><cell>19.6</cell></row><row><cell>Avg. # Triples</cell><cell cols="4">28.1 34.6 34.9 12.5</cell><cell>12.3</cell></row><row><cell>Avg. # Sentences</cell><cell>7.9</cell><cell>8.2</cell><cell>7.9</cell><cell>7.9</cell><cell>8.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Experimental results using the original DocRED and our revised Re-DocRED. For DocRED the reported results are using the same splits of development and test sets as Re-DocRED.</figDesc><table><row><cell>Training</cell><cell>Test</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>DocRED</cell><cell>DocRED</cell><cell cols="3">93.84 89.68 91.71</cell></row><row><cell cols="5">Re-DocRED Re-DocRED 94.48 90.12 92.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Positive relation classification performance with the original DocRED and our revised Re-DocRED (using ATLOP).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparison under different metrics.</figDesc><table><row><cell>Models</cell><cell></cell><cell>Dev</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell cols="2">Ign_F1</cell><cell>F1</cell><cell>Ign_F1</cell><cell>F1</cell><cell cols="2">Freq. F1 LT F1 Intra F1 Inter F1</cell></row><row><cell>ATLOP</cell><cell></cell><cell cols="4">76.88 77.63 76.94 77.73</cell><cell>80.79</cell><cell>72.47</cell><cell>80.18</cell><cell>75.13</cell></row><row><cell cols="2">DocuNET</cell><cell cols="4">77.53 78.16 77.27 77.92</cell><cell>81.16</cell><cell>73.41</cell><cell>79.91</cell><cell>76.64</cell></row><row><cell cols="6">KD-DocRE 77.92 78.65 77.63 78.35</cell><cell>80.97</cell><cell>74.42</cell><cell>79.57</cell><cell>77.26</cell></row><row><cell cols="6">+ Pre-trained with distantly supervised data</cell><cell></cell></row><row><cell>ATLOP</cell><cell></cell><cell cols="4">78.15 79.13 78.42 79.39</cell><cell>82.09</cell><cell>75.16</cell><cell>80.28</cell><cell>78.33</cell></row><row><cell cols="2">DocuNET</cell><cell>78.4</cell><cell cols="3">78.95 78.51 79.02</cell><cell>81.98</cell><cell>74.37</cell><cell>80.18</cell><cell>78.01</cell></row><row><cell cols="6">KD-DocRE 79.31 79.88 80.09 80.62</cell><cell>82.76</cell><cell>76.35</cell><cell>81.78</cell><cell>79.61</cell></row><row><cell></cell><cell>Type</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell><cell></cell></row><row><cell>KD-DocRE</cell><cell cols="4">Freq. 87.23 75.55 80.97 LT 80.90 68.90 74.42</cell><cell></cell><cell></cell></row><row><cell cols="5">KD-DocRE Freq. 89.76 76.78 82.76</cell><cell></cell><cell></cell></row><row><cell cols="2">+Pre-training LT</cell><cell cols="3">86.14 68.53 76.35</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Detailed analysis of KD-DocRE on the test set of Re-DocRED.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>The numbers within brackets were falsely identified as</figDesc><table><row><cell>3 https://www.wikidata.org/wiki/</cell></row><row><cell>Property:P84</cell></row><row><cell>4 https://www.wikidata.org/wiki/</cell></row><row><cell>Property:P170</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Common error types of<ref type="bibr" target="#b17">Huang et al. (2022)</ref>.</figDesc><table><row><cell></cell><cell cols="3">Added Triples Errors Precision</cell></row><row><cell>Scratch</cell><cell>681</cell><cell>54</cell><cell>92.4</cell></row><row><cell>Re-DocRED</cell><cell>733</cell><cell>17</cell><cell>97.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Error rates of Re-DocRED and the Scratch dataset<ref type="bibr" target="#b17">(Huang et al., 2022)</ref> based on examining 20 randomly sampled documents. We observe that Re-DocRED has higher precision.the date of birth and date of death, whereas the passage is about a renowned high school. It can be inferred that the numbers behind the alumni names are indicating the time periods that they were in this school. This error arises because most date of birth and date of death are described by brackets and numbers. However, such a pattern does not necessarily mean all numbers in brackets are indicating such relations.It is worth noting that the annotators inHuang  et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Costs and unit time required for different annotation strategies.</figDesc><table><row><cell></cell><cell cols="2">Unit Price Unit Time</cell></row><row><cell cols="2">Annotating from Scratch 48 CNY</cell><cell>40 mins</cell></row><row><cell>One Round of Revision</cell><cell>7.8 CNY</cell><cell>10 mins</cell></row><row><cell cols="3">Two Rounds of Revision 15.8 CNY 15 mins</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>"<rs type="projectName">I Knew You Were Trouble</rs> " is a song recorded by American singer -songwriter <rs type="person">Taylor Swift</rs> for her fourth studio album , Red ( 2012 ) . It was released on October 9 , 2012 , in the <rs type="institution">United States by Big Machine Records</rs> as the third promotional single from the album . Later , " <rs type="projectName">I Knew You Were Trouble</rs> " was released as the third single from Red on November 27 , 2012 , in the United States . It was written by <rs type="funder">Swift</rs> , <rs type="person">Max Martin</rs> and <rs type="person">Shellback</rs> , with the production handled by the latter two ... It later peaked at number two in January 2013 , blocked from the top spot by <rs type="programName">Bruno Mars' " Locked Out of Heaven " . At the inaugural YouTube Music Awards in 2013 , " I Knew You Were Trouble</rs> " won the award for YouTube phenomenon . . .</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_EB2FcQw">
					<orgName type="project" subtype="full">I Knew You Were Trouble</orgName>
				</org>
				<org type="funded-project" xml:id="_V3zuYFe">
					<orgName type="project" subtype="full">I Knew You Were Trouble</orgName>
					<orgName type="program" subtype="full">Bruno Mars&apos; &quot; Locked Out of Heaven &quot; . At the inaugural YouTube Music Awards in 2013 , &quot; I Knew You Were Trouble</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Common Model Error Examples</head><p>In this section, similar to <ref type="bibr">Tan et al. (2022)</ref>, we split the union of ground-truth triples and predicted triples into four categories: (1) Correct (C), where predicted triples are in the ground truth. ( <ref type="formula">2</ref>) Wrong (W), where the output relation type is wrong but the predicted head and tail entities are in the ground truth. (3) Missed (MS), where the model predicts no relation for a pair of head and tail entities but there is some relation in the ground truth. (4) More (MR), where the model predicts an extraneous relation for a pair of head and tail entities that is not in the ground truth. The performance breakdown is shown in Table <ref type="table">15</ref>. We observe that the majority of the errors were in the MS and MR categories. On the other hand, we found that predictions on popular relations tend to fall under the MR category. We further show this popularity bias pattern  Error Type 2: Missing Triples (MS) Error Cause: Fail to find long-tail relations Document: CBBC ( short for Children 's BBC ) is a British children 's television strand owned by the BBC and aimed for children aged from 6 to 12 . BBC programming aimed at under six year old children is broadcast on the CBeebies channel . CBBC broadcasts from 7 am to 9 pm on the digital CBBC Channel , available on most UK digital platforms . The CBBC brand was used for the broadcast of children 's programmes on BBC One on weekday afternoons and on BBC Two mornings until these strands were phased out in 2012 and 2013 respectively , as part of the BBC 's " Delivering Quality First " cost -cutting initiative . CBBC programmes were also broadcast in high definition alongside other BBC content on BBC HD , generally at afternoons on weekends , unless the channel was covering other events . This ended when BBC HD closed on 26 March 2013 , but CBBC HD launched on 10 December 2013 . Table <ref type="table">13</ref>: Examples of the two most common error types. We use blue and green to color the entities and relations, respectively. Split 0 Split 1 Split 2 Split 3 1,000 1,000 1,000 1,053 with examples in Table <ref type="table">13</ref>. We can see that popular relation patterns tend to be under the MR category. From the first example, "Nicki Minaj" is a popular artist. But the song "Trophies" was not performed by "Nicki Minaj". In contrast, the model wrongly predicts that this song is also performed by her. On the contrary, in example 4, the cue that "BBC HD" dissolved on "Mar 26, 2013" is obvious, whereas the model failed to find this relation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details of Logical Rules</head><p>In this section, we show the logical rules that we used. After examining the DocRED dataset <ref type="bibr" target="#b28">(Yao et al., 2019)</ref>, we found that there are two types of logical inconsistencies. The first type is the incompleteness of inverse relations, and the sec-ond is the inconsistency in co-occurring relations. The inverse relations are logical relations that can be implied by reversing the direction of relation triplets. For example, if entity 1 is the participant of an event (entity 2), this event should have a participant relation with entity 1. We show all the inverse relation pairs that we used in Table <ref type="table">17</ref>. Besides inverse relations, we also added triples by co-occurring rules. This is mainly because we found that these relations are logically correlated and their co-occurrence is inconsistent in the original DocRED dataset. For example, when describing the relation between entities and wars, there are two involved relations: relation conflict 5 and participant of 6 . For such cases, we deem that the two relations shall all be present when conflict is present. We show the list of co-occurring relations in Table <ref type="table">16</ref>.</p><p>Relation Co-occuring relation country located in conflict participant of </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
	<note>References Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Ateret</forename><surname>Anaby-Tavor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Goldbraich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Tepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naama</forename><surname>Zwerdling</surname></persName>
		</author>
		<title level="m">Do not have enough data? deep learning to the rescue! In Proceedings of AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">H-FND: hierarchical false-negative denoising for distant supervision relation extraction</title>
		<author>
			<persName><forename type="first">Jhih-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Kang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2579" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HacRED: A largescale relation extraction dataset toward hard cases in practical applications</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">RelationPrompt: leveraging prompts to generate synthetic data for zero-shot relation triplet extraction</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09101</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An end-to-end model for entity-level relation extraction using multiinstance learning</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Manual evaluation matters: Reviewing test protocols of distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Joseph L Fleiss ; Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhuo</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyue</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="1971">1971. 2021</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1306" to="1318" />
		</imprint>
	</monogr>
	<note>Measuring nominal scale agreement among many raters</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FewRel 2.0: Towards more challenging few-shot relation classification</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1649</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6251" to="6256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">More data, more relations, more context and more openness: A review and outlook for relation extraction</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AACL</title>
		<meeting>AACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="745" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowing false negatives: An adversarial training method for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Kailong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.761</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9661" to="9672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Does recommend-revise produce reliable annotations? an analysis on missing instances in DocRED</title>
		<author>
			<persName><forename type="first">Quzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data augmentation using pre-trained transformer models</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</title>
		<editor>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</editor>
		<meeting>the 2nd Workshop on Life-long Learning for Spoken Language Systems<address><addrLine>S?ren Auer</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>et al. 2015. Dbpedia -a largescale, multilingual knowledge base extracted from wikipedia. Semantic web</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times annotated corpus. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating datasets with pretrained language models</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive focal loss and knowledge distillation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI. Qingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou Ng</title>
		<meeting>AAAI. Qingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou Ng</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
		</imprint>
	</monogr>
	<note>Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">ACE 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07178</idno>
		<title level="m">Symbolic knowledge distillation: from general language models to commonsense models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative data augmentation for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Yiben</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Ping</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DocRED: a large-scale document-level relation extraction dataset</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03618</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13655</idno>
		<title level="m">MELM: data augmentation with masked entity language modeling for cross-lingual NER</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An improved baseline for sentence-level relation extraction</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01373</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">But this relation is not in the DocRED&apos;s label space. In this case, the architect relation shall be excluded from DocRED. However, annotators from Huang et al. (2022) uses creator to describe this kind of relation, which is not precise. Document: Sir David Alan Chipperfield ( born 18 December 1953 ) is an English architect . He established David Chipperfield Architects in 1985</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His major works include the River and Rowing Museum in Henley -on -Thames</title>
		<meeting><address><addrLine>Oxfordshire; Iowa</addrLine></address></meeting>
		<imprint>
			<publisher>Moines Public Library</publisher>
			<date type="published" when="1989">2021. 1989? 1998. 2002? 2006</date>
			<biblScope unit="page" from="14612" to="14620" />
		</imprint>
	</monogr>
	<note>Proceedings of AAAI. the Museum of Modern Literature in Marbach , Germany ; the Des</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m">the Neues Museum</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997 ? 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wrong Triples: (Museo Jumex, David Alan Chipperfield, creator), (River and Rowing Museum, David Alan Chipperfield, creator), (Museum of Modern Literature, David Alan Chipperfield, creator), (Saint Louis Art Museum, David Alan Chipperfield, creator), (Hepworth Wakefield gallery, David Alan Chipperfield, creator), (Neues Museum, David Alan Chipperfield, creator) Example 2 Misunderstanding of Definition: series vs. part of Error Cause: Annotator&apos;s Misunderstanding of Wikidata Relation Definition Document: Chapman Square is the debut studio album released by four piece British band Lawson . The album was released on 19 October 2012 via Polydor Records . The album includes their three top ten singles &quot; When She Was Mine</title>
		<author>
			<persName><forename type="first">The</forename><surname>Hepworth Wakefield ; Berlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Shanghai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The album was mainly produced by John Shanks with Duck Blackwell</title>
		<title level="s">the Saint Louis Art Museum</title>
		<editor>
			<persName><forename type="first">Paddy</forename><surname>Dalton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ki</forename><surname>Fitzgerald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carl</forename><surname>Falk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rami</forename><surname>Yacoub</surname></persName>
		</editor>
		<meeting><address><addrLine>Missouri</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003? 2011. 2005? 2013. 2009? 2013</date>
		</imprint>
		<respStmt>
			<orgName>Museo Jumex in Mexico City</orgName>
		</respStmt>
	</monogr>
	<note>The album was re -released in the autumn of 2013 as Chapman Square Chapter II , with the lead single from the re -release being &quot; Brokenhearted. which features American rapper B.o . B. As of July 2016 , the album has sold 169,812 copies . Wrong Triples: (Taking Over Me, Chapman Square Chapter II, series), ((Brokenhearted, (Chapman Square Chapter II, series), ((When She Was Mine, (Chapman Square, series), (Standing in the Dark, Chapman Square, series</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ia?i ) , was a Moldavian Romanian writer , literary critic and publicist . Russo is credited with having discovered one of the most elaborate forms of the Romanian national folk ballad Miori?a . He was also a contributor to the Ia?i periodical Zimbrul , in which he published one of his best -known works , Studie Moldovan?</title>
	</analytic>
	<monogr>
		<title level="m">Moldovan Studies &quot; )</title>
		<imprint>
			<date type="published" when="1859-02-05">February 5 , 1859</date>
			<biblScope unit="page" from="1851" to="1852" />
		</imprint>
	</monogr>
	<note>He also wrote Ia?ii ?i locuitorii lui ?n 1840 &quot; Ia?i and its inhabitants in 1840 &quot; -a glimpse into Moldavian society during the Organic Statute administration , and two travel accounts ( better described as folklore studies ) , Piatra Teiului and St?nca Corbului. Russo is also notable for his Amintiri ( &quot; Recollections &quot; ) , a memoir</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Example 4 Error Type 3: Slippery slope Error Cause: Improper reasoning based on punctuations for judgement of date of birth/death. Document: South Wigston High School was founded in 1938 and is a school serving the local community of South Wigston . Today the school is an 11 ??? 16 yrs Academy . The main feeder primary schools are Glen Hills , Fairfield and Parkland . The school also attracts students from many areas of the city of Leicester and the county of Leicestershire . The school is oversubscribed and is growing year on year . South Wigston is known for its wide range of extra -curricular opportunities and for being a school that is inclusive and at the heart of the community . The school has extensive grounds and a purpose build sports centre opened by Gary Lineker</title>
		<author>
			<persName><forename type="first">Wrong</forename><surname>Triples</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ia?i</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ia?i</forename><surname>Moldavian ; Moldavian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rugby Player for Leicester Tigers and England ; and Brett Deacon</title>
		<title level="s">Notable alumni include Sue Townsend</title>
		<imprint>
			<publisher>Rugby Player for Leicester Tigers and England</publisher>
			<date type="published" when="1946">1946 -1950. 1991 -1995. 1992 -1996. 2016</date>
		</imprint>
	</monogr>
	<note>located in the administrative territorial entity. author ; Louis Deacon. The Secret Life of Sue Townsend Aged 68. Much of the documentary was filmed at the school and current students participated</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">date of birth), (Brett Deacon, 1996, date of death), (Brett Deacon, 1992, date of birth), (Sue Townsend, 1946, date of birth), (Sue Townsend, 1950, date of death), (Louis Deacon, 1995, date of death) Table 9: Examples for the common error types by</title>
		<editor>Huang et al.</editor>
		<imprint>
			<date type="published" when="1991">1991. 2022</date>
		</imprint>
	</monogr>
	<note>Wrong Triples: (Louis Deacon. We use blue to color the entities and green to color the relations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
