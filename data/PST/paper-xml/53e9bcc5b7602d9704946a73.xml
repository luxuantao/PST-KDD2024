<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Discriminative Latent Model of Object Classes and Attributes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
							<email>mori@cs.sfu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Discriminative Latent Model of Object Classes and Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7319BAA6B6BB1E5F0B209164F0B566D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a discriminatively trained model for joint modelling of object class labels (e.g. "person", "dog", "chair", etc.) and their visual attributes (e.g. "has head", "furry", "metal", etc.). We treat attributes of an object as latent variables in our model and capture the correlations among attributes using an undirected graphical model built from training data. The advantage of our model is that it allows us to infer object class labels using the information of both the test image itself and its (latent) attributes. Our model unifies object class prediction and attribute prediction in a principled framework. It is also flexible enough to deal with different performance measurements. Our experimental results provide quantitative evidence that attributes can improve object naming.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>What can we say about an object when presented with an image containing it, such as images shown in Fig. <ref type="figure" target="#fig_0">1</ref>? First of all, we can represent the objects by their categories, or names ("bird" "apple" "chair", etc). We can also describe those objects in terms of certain properties or attributes, e.g. "has feather" for (a), "red" for (b), "made of wood" for (c) in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>In the computer vision literature, most work in object recognition focuses on the categorization task, also known as object naming, e.g. "Does this image window contain a person?" or "Is this an image of a dog (versus cat, chair, table, ...)?". Some recent work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> proposes to shift the goal of recognition from naming to describing, i.e. instead of naming the object, try to infer the properties or attributes of objects. Attributes can be parts (e.g. "has ear"), shape (e.g. "is round"), materials (e.g. "made of metal"), color (e.g. "is red"), etc. This attribute-centric approach to object recognition provides many new abilities compared with the traditional naming task, e.g. when faced with an object of a new category, we can still make certain statements (e.g. "red" " furry" "has ear") about it even though we cannot name it.</p><p>The concept of attributes can be traced back (at least) to the early work on intrinsic images <ref type="bibr" target="#b0">[1]</ref>, in which an image is considered as the product of characteristics (in particular, shading and reflectance) of a scene. Conceptually, we can consider shading and reflectance as examples of semantically meaningful properties (or attributes) of an image. Recently there has been a surge of interest in the computer vision community on learning visual attributes. Ferrari and Zisserman <ref type="bibr" target="#b8">[9]</ref> propose a generative model for learning simple color and texture attributes from loose annotations. Farhadi et al. <ref type="bibr" target="#b6">[7]</ref> learn a a richer set of attributes including parts, shape, materials, etc. Vaquero et al. <ref type="bibr" target="#b17">[18]</ref> introduce a video-based visual surveillance system which allows one to search based on people's fine-grained parts and attributes, e.g. an example could be "show me people with bald head wearing red shirt in the video".</p><p>The attribute-centric approach certainly has great scientific value and practical applications. Some attributes (e.g. "red") can indeed be recognized without considering object names, and it is possible for people to infer attributes of objects they have never seen before. But object naming is clearly still important and useful. Consider the image in Fig. <ref type="figure" target="#fig_0">1</ref>(a), we as humans can easily recognize this object has the attribute "eye", even though the "eye" corresponds to a very tiny region in the image. Although it is not entirely clear how humans achieve this amazing ability, it is reasonable to believe that we are not running an "eye" detector in our brain in order to infer this attribute. More likely, we infer the object "has eye" in conjunction with recognizing it as a bird (or at least an animal). The issue becomes more obvious when we want to deal with attributes that are less visually apparent. For example, we as humans can recognize the images in Fig. <ref type="figure" target="#fig_0">1(b,</ref><ref type="figure">c</ref>) have the attributes "being edible" and "being able to sit on", respectively. But those attributes are very difficult to describe in terms of visual appearances of the objects -we infer those attributes most likely because we recognize the objects. In addition, the functions of objects cannot always easily be inferred directly from their visual attributes. Consider the two images in Fig. <ref type="figure" target="#fig_0">1(d,</ref><ref type="figure">e</ref>). They are similar in terms of most of their visual attributes -both are "blue", "made of metal", "3D boxy", etc. But they have completely different functions. Those functions can be easily inferred if we recognize Fig. <ref type="figure" target="#fig_0">1</ref>(d) as a mailbox and Fig. <ref type="figure" target="#fig_0">1</ref>(e) as a trash can. Our ultimate goal is to build recognition systems that jointly learn object classes and attributes in a single framework. In this paper, we take the first steps toward this goal by trying to answer the following question: can attributes help object naming? Although conceptually the answer seems to be positive, there have only been limited cases supporting it in special scenarios. Kumar et al. <ref type="bibr" target="#b10">[11]</ref> show that face verification can benefit from inferring attributes corresponding to visual appearances (gender, race, hair color, etc.) and so-called simile attributes (e.g. a mouth that looks like Barack Obama). Attributes have also been shown to be useful in solving certain non-traditional recognition tasks, e.g. when training and test classes are disjoint <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. However, when it comes to the traditional object naming task, there is little evidence showing the benefit of inferring attributes. The work in <ref type="bibr" target="#b6">[7]</ref> specifically mentions that attribute based representation does not help significantly in the traditional naming task. This is surprising since object classes and attributes are two closely related concepts. Attributes of an object convey a lot of information about the object category, e.g. an object that "has leg" "has head" "furry" should be more likely to be a dog than a car. Similarly, the name of an object also conveys a lot of information about its possible attributes, e.g. a dog tends to "have leg", and is not likely to "have wing". The work on joint learning of visual attributes and object classes by Wang and Forsyth <ref type="bibr" target="#b18">[19]</ref> is the closest to ours. Their work demonstrates that attribute classifiers and object classifiers can improve the performance of each other. However, we would like to point out that the improvement in their work mainly comes from the fact that the training data are weakly labeled, i.e. training data are only labeled with object class labels, but not with exact locations of objects in the image. In this case, an object classifier (say "hat") and an attribute classifier (say "red") can help each other by trying to agree on the same location in an image labeled as "red" and "hat". That work does not answer the question of whether attributes can help object naming without this weakly labeled data assumption, e.g. when an image is represented by a feature vector computed from the whole image, rather than a local patch defined by the location of the object.</p><p>Our training data consist of images with ground-truth object class labels (e.g. "person", "dog", "chair", etc.) and attribute labels (e.g. "has torso", "metal", "red", etc.). During testing, we are given a new image without the ground-truth attribute labels, and our goal is to predict the object class label of the test image. We introduce a discriminative model for jointly modelling object classes and attributes. Our model is trained in the latent SVM framework <ref type="bibr" target="#b7">[8]</ref>. During testing, we treat the attributes as the latent variables and try to infer the class label of a test image.</p><p>The contributions of this paper are three-fold. Firstly and most importantly, we propose a model clearly showing that attributes can help object naming. Our model is also very flexible -it can be easily modified to improve upon many different performance measurements. Secondly, most previous work (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>) assumes attributes are independent of each other. This is clearly not true. An object that "has ear" is more likely to "has head", and less likely to be "made of metal". An important question is how to model the correlations among attributes. We introduce the attribute relation graph, an undirected graphical model built from training data, to capture these correlations. Thirdly, our model can be broadly applied to address a whole class of problems which we call recognition with auxiliary labels. Those problems are characterized as classification tasks with certain additional information provided on training data. Many problems in computer vision can be addressed in this framework. For example, in pedestrian detection, auxiliary labels can be the body part locations. In web image classification, auxiliary labels can be the textual information surrounding an image. There has been work that tries to build recognition systems that make use of those auxiliary labels, e.g. <ref type="bibr" target="#b16">[17]</ref> for pedestrian detection and <ref type="bibr" target="#b19">[20]</ref> for object image classification. However, those work typically use a simple two-stage classification process by first building a system to predict the auxiliary labels, then learning a second system taking into account those auxiliary labels. Conceptually, it is much more appealing to integrate these two stages in a unified framework and learn them jointly, which is exactly what we do in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Formulation</head><p>A training example is represented as a tuple (x, h, y). Here x is the image itself. The object class label of the image is represented by y ∈ Y, where Y is a finite label alphabet. The attributes of the image x are denoted by a K-dimensional</p><formula xml:id="formula_0">vector h = (h 1 , h 2 , ..., h K ), where h k ∈ H k (k = 1, 2, ..., K) indicates the k-th</formula><p>attribute of the image. We use H k to indicate the set of possible configurations of the k-th attribute. For example, if the k-th attribute is "2D boxy", we will have H k = {0, 1}, where h k = 1 means this object is "2D boxy", while h k = 0 means it is not. If the k-the attribute is "leg", h k = 1 means this object "has leg", while h k = 0 means it does not. The datasets used in this paper only contain binary-valued attributes, i.e. H k = {0, 1} (k = 1, 2, ..., K). For ease of presentation, we will simply write H instead of H k from now on when there are no confusions. But we emphasize that our proposed method is not limited to binary-valued attributes and can be generalized to multi-valued or continuousvalued attributes.</p><p>We assume there are certain dependencies between some attribute pairs (h j , h k ). For example, h j and h k might correspond to "head" and "ear", respectively. Then their values are highly correlated, since an object that "have head" tends to "have ear" as well. We use an undirected graph G = (V, E), which we call the attribute relation graph, to represent these dependency relations between attribute pairs. A vertex j ∈ V corresponds to the j-th attribute, and an edge (j, k) ∈ E indicates that attributes h j and h k have a dependency. We only consider dependencies of pairs of attributes in this paper, but it is also possible to define higher-order dependencies involving more than two attributes. We will describe how to obtain the graph G from training data in Sec. 5.</p><p>Given a set of</p><formula xml:id="formula_1">N training examples {(x (n) , h (n) , y (n) )} N n=1</formula><p>, our goal is to learn a model that can be used to assign the class label y to an unseen test image x. Note that during testing, we do not know the ground-truth attributes h of the test image x. Otherwise the problem will become a standard classification problem and can be solved using any off-the-shelf classification method.</p><p>We are interested in learning a discriminative function f w : X × Y → R over an x image and its class label y, where w are the parameters of this function. During testing, we can use f w to predict the class label y * of the input x as y * = arg max y∈Y f w (x, y). Inspired by the latent SVM <ref type="bibr" target="#b7">[8]</ref> (also called the max-margin hidden conditional random field <ref type="bibr" target="#b20">[21]</ref>), we assume f w (x, y) takes the following form: f w (x, y) = max h w Φ(x, h, y), where Φ(x, h, y) is a feature vector depending on the image x, its attributes h and its class label y. We define w Φ(x, h, y) as follows:</p><formula xml:id="formula_2">w Φ(x, h, y) = w y φ(x) + j∈V w hj ϕ(x) + j∈V w y,hj ω(x) + (j,k)∈E w j,k ψ(h j , h k ) + j∈V v y,hj<label>(1)</label></formula><p>The model parameters w are simply the concatenation of the parameters in all the factors, i.e. w = {w hj ; w y,hj ; w j,k ; w y ; v y,hj } y∈Y,hj∈H,j∈V,(j,k)∈E . The details of the potential functions in Eq. ( <ref type="formula" target="#formula_2">1</ref>) are described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object class model w y φ(x):</head><p>This potential function represents a standard linear model for object recognition without considering attributes. Here φ(x) ∈ R d represents the feature vector extracted from the image x, the parameter w y represents a template for object class y. If we ignore other potential functions in Eq. ( <ref type="formula" target="#formula_2">1</ref>) and only consider the object class model, the parameters {w y } y∈Y can be obtained by training a standard multi-class linear SVM.</p><p>In our current implementation, rather than keeping φ(x) as a high dimensional vector of image features, we simply represent φ(x) as the score of a pre-trained multi-class linear SVM. In other words, we first ignore the attributes in the training data and train a multi-class SVM from {(x (n) , y (n) )} N n=1 . Then we use φ(x; y) to denote the SVM score of assigning x to class y. Note that we explicitly put y in the notation of φ(•) to emphasize that the value depends on y. We use φ(x; y) as the feature vector. In this case, w y is a scalar used to re-weight the SVM score corresponding to class y. This significantly speeds up the learning algorithm with our model. Similar tricks have been used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global attribute model w hj ϕ(x):</head><p>This potential function is a standard linear model trained to predict the label (1 or 0) of the j-th attribute for the image x, without considering its object class or other attributes. The parameter w hj is a template for predicting the j-th attribute to have label h j . If we only consider this potential function, the parameters {w hj } hj∈H can be obtained via a standard binary linear SVM trained from {(x (n) , h</p><formula xml:id="formula_3">(n) j )} N n=1 .</formula><p>Similarly, instead of keeping ϕ(x) as a high dimensional vector of image features, we simply represent it using a scalar ϕ(x; j, h j ), which is the score of predicting the j-th attribute of x to be h j by the pre-trained binary SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-specific attribute model w y,hj ω(x):</head><p>In addition to the global attribute model, we also define a class-specific attribute model for each object class y ∈ Y.</p><p>Here w y,hj is a template for the j-th attribute to take the label h j if the object class is y. If we only consider this potential function, w y,hj (h j ∈ {0, 1}) for a fixed y can be obtained by learning a binary linear SVM from training examples of object class y. Similarly, we represent ω(x) as a scalar ω(x; y, j, h j ), which is the score of predicting the j-th attribute to be h j by an SVM pre-trained from examples of class y.</p><p>The motivations for this potential function are two-fold. First, as pointed out by Farhadi et al. <ref type="bibr" target="#b6">[7]</ref>, learning an attribute classifier across object categories is difficult. For example, it is difficult to learn a classifier to predict the attribute "wheel" on a dataset containing cars, buses, trains. The learning algorithm might end up learning "metallic" since most of the examples of "wheels" are surrounded by "metallic" surfaces. Farhadi et al. <ref type="bibr" target="#b6">[7]</ref> propose to address this issue by learning a "wheel" classifier within a category and do feature selection. More specifically, they learn a "wheel" classifier from a single object category (e.g. cars). The "wheel" classifier learned in this fashion is less likely to be confused by "metallic", since both positive and negative examples (i.e. cars with or without "wheel") in this case have "metallic" attributes. Then they can select features that are useful for differentiating "wheel" from "non-wheel" based on the classifier trained within the car category. The disadvantage of the feature selection approach in <ref type="bibr" target="#b6">[7]</ref> is that it is disconnected from the model learning and requires careful manual tuning. Our class-specific attribute model achieves a goal similar to the feature selection strategy in <ref type="bibr" target="#b6">[7]</ref>, but in a more principled manner since the feature selection is implicitly achieved via the model parameters returned by the learning algorithm.</p><p>Second, the same attribute might appear differently across multiple object classes. For example, consider the attribute "leg". Many object classes (e.g. people, cats) can "have leg" . But the "legs" of people and "legs" of cats can be very different in terms of their visual appearances. If we learn a "leg" attribute classifier by considering examples from both people and cat categories, the learning algorithm might have a hard time figuring out what "legs" look like due to the appearance variations. By separately learning a "leg" classifier for each object category, the learning becomes easier since the positive examples of "legs" within each category are similar to each other. This allows the learning algorithm to use certain visual properties (e.g. furry-like) to learn the "leg" attribute for cats, while use other visual properties (e.g. clothing-like) to learn the "leg" attribute for people.</p><p>One might think that the class-specific attribute model eliminates the need for the global attribute model. If this is the case, the learning algorithm will set w hj to be zero. However, in our experiment, both w hj and w y,hj have non-zero entries, indicating these two models are complementary rather than redundant.</p><p>Attribute-attribute interaction w j,k ψ(h j , h k ): This potential function represents the dependencies between the j-th and the k-th attributes. Here ψ(h j , h k ) is a sparse binary vector of length |H| × |H| (i.e. 4 in our case, since |H| = 2) with a 1 in one of its entries, indicating which of the four possible configurations {(1, 1), (1, 0), (0, 1), (0, 0)} is taken by (h j , h k ), e.g. ψ(1, 0) = [0, 1, 0, 0] . The parameter w j,k is a 4-dimensional vector representing the weights of all those configurations. For example, if the j-th and the k-th attributes correspond to "ear" and "eye". The entries of w j,k that correspond to (1,1) and (0,0) will probably tend to have large values, since "ear" and "eye" tend to appear together in any object.</p><p>Object-attribute interaction v y,hj : This is a scalar indicating how likely the object class being y and the j-th attribute being h j . For example, let y correspond to the object class "people" and the j-th attribute is "torso", then v y,1 will probably have a large value since most "people" have "torso" (i.e. h j = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Objective</head><p>If the ground-truth attribute labels are available during both training and testing, we can simply consider them as part of the input data and solve a standard classification problem. But things become tricky when we want to take into account the attribute information on the training data, but do not want to "overly trust" this information since we will not have it during testing. In this section, we introduce two possible choices of learning approaches and discuss why we choose a particular one of them.</p><p>Recall that an image-label pair (x, y) is scored by the function of the form f w (x, y) = max h w Φ(x, h, y). Given the model parameter w, we need to solve the following inference problem during testing:</p><formula xml:id="formula_4">h * = arg max h w Φ(x, h, y) ∀y ∈ Y<label>(2)</label></formula><p>In our current implementation, we assume h forms a tree-structured model. In this case, the inference problem in Eq. ( <ref type="formula" target="#formula_4">2</ref>) can be efficiently solved via dynamic programming or linear program relaxation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Learning with latent attributes: Given a set of N training examples S = {(x (n) , h (n) , y (n) )} N n=1 , we would like to train the model parameter w that tends to produce the correct label for an image x. If the attributes h are unobserved during training and are treated as latent variables, a natural way to learn the model parameters is to use the latent SVM <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref> formulation as follows: min</p><formula xml:id="formula_5">w,ξ β||w|| 2 + N n=1 ξ (n) s.t. max h w Φ(x (n) , h, y (n) ) -max h w Φ(x (n) , h, y) ≥ Δ(y, y (n) ) -ξ (n) , ∀n, ∀y (3)</formula><p>where β is the trade-off parameter controlling the amount of regularization, and ξ (n) is the slack variable for the n-th training example to handle the case of soft margin, Δ(y, y (n) ) is a loss function indicating the cost of misclassifying y (n) as y. In standard multi-class classification problems, we typically use the 0-1 loss Δ 0/1 defined as:</p><formula xml:id="formula_6">Δ 0/1 (y, y (n) ) = 1 if y = y (n) 0 otherwise<label>(4)</label></formula><p>Learning with observed attributes: Now since we do observe the groundtruth attributes h (n) on the training data, one might think a better choice would be to fix those values for y (n) rather than maximizing over them, as follows: min</p><formula xml:id="formula_7">w,ξ β||w|| 2 + N n=1 ξ (n) s.t.w Φ(x (n) , h (n) , y (n) ) -max h w Φ(x (n) , h, y) ≥ Δ(y, y (n) ) -ξ (n) , ∀n, ∀y<label>(5)</label></formula><p>The two formulations Eq. ( <ref type="formula">3</ref>) and Eq. ( <ref type="formula" target="#formula_7">5</ref>) are related as follows. First, let us define h (n) as h (n) = arg max h w Φ(x (n) , h, y (n) ). Then it is easy to show that Eq. ( <ref type="formula">3</ref>) is a non-convex optimization, while Eq. ( <ref type="formula" target="#formula_7">5</ref>) is convex. In particular, Eq. ( <ref type="formula" target="#formula_7">5</ref>) provides a convex upper-bound on Eq. ( <ref type="formula">3</ref>). The bound is tight if h (n)  and h (n) are identical for ∀n.</p><p>Discussion: Even though Eq. ( <ref type="formula" target="#formula_7">5</ref>) provides a surrogate of optimizing Eq. ( <ref type="formula">3</ref>) as its upper bound, our initial attempt of using the formulation in Eq. ( <ref type="formula" target="#formula_7">5</ref>) suggests that it does not work as well as that in Eq. ( <ref type="formula">3</ref>). We believe the reason is the optimization problem in Eq. ( <ref type="formula" target="#formula_7">5</ref>) assumes that we will have access to the groundtruth attributes during testing. So the objective being optimized in Eq. ( <ref type="formula" target="#formula_7">5</ref>) does not truthfully mimic the situation at run-time. This will not be an issue if the bound provided by Eq. ( <ref type="formula" target="#formula_7">5</ref>) is tight. Unfortunately, having a tight bound means we need to set the parameters w to almost perfectly predict h given (x (n) , y (n) ), which is obviously difficult. This might be surprising given the fact that the formulation in Eq. ( <ref type="formula">3</ref>) seems to ignore some information (i.e. ground-truth attribute labels) during training. At first glance, this argument seems to be reasonable, since Eq. ( <ref type="formula">3</ref>) does not require the ground-truth attributes h (n) at all. But we would like to argue that this is not the case. The information provided by the ground-truth attributes on training data has been implicitly injected into the feature vectors ϕ(x) and ω(x) defined in the global attribute model and class-specific attribute model (see the descriptions in Sec. 2), since ϕ(x) and ω(x) are vectors of SVM scores. Those scores are obtained from SVM classifiers trained using the ground-truth attribute labels. So implicitly, Eq. ( <ref type="formula">3</ref>) already makes use of the information of the groundtruth attributes from the training data. In addition, Eq. ( <ref type="formula">3</ref>) effectively models the uncertainty caused by the fact that we do not know the attributes during testing and it is difficult to correctly predict them. So in summary, we choose the learning with latent attributes (i.e. non-convex version) formulated in Eq. (3) as our learning objective. But we would like to emphasize that the convex version in Eq. ( <ref type="formula" target="#formula_7">5</ref>) is also a reasonable learning objective. In fact, it has been successfully applied in other applications <ref type="bibr" target="#b2">[3]</ref>. We leave the further theoretical and empirical studies of these two different formulations as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Non-convex Cutting Plane Training</head><p>The optimization problem in Eq. ( <ref type="formula">3</ref>) can be solved in many different ways. In our implementation, we adopt a non-convex cutting plane method proposed in <ref type="bibr" target="#b3">[4]</ref> due to its ease of use. First, it is easy to shown that Eq. ( <ref type="formula">3</ref>) is equivalent to min w L(w) = β||w|| 2 + N n=1 R n (w) where R n (w) is a hinge loss function defined as:</p><formula xml:id="formula_8">R n (w) = max y Δ(y, y (n) ) + max h w Φ(x (n) , h, y) -max h w Φ(x (n) , h, y (n) )(6)</formula><p>The non-convex cutting plane method in <ref type="bibr" target="#b3">[4]</ref> aims to iteratively build an increasingly accurate piecewise quadratic approximation of L(w) based on its sub-gradient ∂ w L(w). The key issue here is how to compute the sub-gradient ∂ w L(w). Let us define:</p><formula xml:id="formula_9">h (n) y = arg max h w Φ(x (n) , h, y) ∀n, ∀y ∈ Y y * (n) = arg max y Δ(y, y (n) ) + w Φ(x (n) , h (n) y , y) (7)</formula><p>As mentioned in Sec. 2, the inference problem in Eq. ( <ref type="formula">7</ref>) can be efficiently solved if the attribute relation graph forms a tree. It is easy to show a sub-gradient ∂ w L(w) can be calculated as follows:</p><formula xml:id="formula_10">∂ w L(w) = 2β • w + N n=1 Φ(x (n) , h (n) y * (n) , y * (n) ) - N n=1 Φ(x (n) , h (n) y (n) , y (n) ) (8)</formula><p>Given the sub-gradient ∂ w L(w) computed according to Eq. ( <ref type="formula">8</ref>), we can minimize L(w) using the method in <ref type="bibr" target="#b3">[4]</ref>. In order to extend the algorithm to handle more general scenarios involving multi-valued or continuous-valued attributes, we can simply modify the maximization over h in Eq. (6,7) accordingly. For example, arg max h will be replaced by some continuous optimization in the case of continuous attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Attribute Relation Graph</head><p>We now describe how to build the attribute relation graph G = {V, E}. In order to keep the inference problem in Eq. ( <ref type="formula" target="#formula_4">2</ref>) tractable, we will assume G is a treestructured graph. Our approach is inspired by the Chow-Liu algorithm <ref type="bibr" target="#b1">[2]</ref> for learning Bayesian network structures. A vertex j ∈ V corresponds to the j-th attribute. An edge (j, k) ∈ E means the j-th and the k-th attributes have dependencies. In practice, the dependencies between certain attribute pairs might be weaker than others, i.e. the value of one attribute does not provide much information about the value of the other one. We can build a graph that only contains edges corresponding to those strong dependencies. The graph G could be built manually by human experts. Instead, we adopt an automatic process to build G by examining the co-occurrence statistics of attributes in the training data. First, we measure the amount of dependency between the j-th and the k-th attributes using the normalized mutual information defined as NormMI(j, k) = MI(j,k) min{H(j),H(k)} , where MI(j, k) is the mutual information between the j-th and the k-th attributes, and H(j) is the entropy of the j-th attribute. Both MI(j, k) and H(j) can be easily calculated using the empirical distributions p(h j ), p(h k ) and p(h j , h k ) estimated from the training data.</p><p>A large NormMI(j, k) means a strong interaction between the j-th and the k-th attributes. We assign a weight NormMI(j, k) to the connection (j, k), then run a maximum spanning tree algorithm to find the edges E to be included in the attribute relation graph G. Similar ideas have been used in <ref type="bibr" target="#b12">[13]</ref> to find correlations between video annotations. The attribute relation graph with 64 attributes built from our training data is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Other Loss Functions</head><p>This paper mainly deals with multi-class classification problems, where the performance of an algorithm is typically measured by its overall accuracy. It turns out we can modify the learning approach in Sec. 3 to directly optimize other performance measurements. In this section, we show how to adapt the learning objective so it optimizes a more sensible measurement for problems involving highly skewed class distributions. First we need a new interpretation of Eq. ( <ref type="formula">3</ref>). From Eq. ( <ref type="formula">3</ref>), it is easy to show ξ (n) ≥ Δ(y * (n), y (n) ), where y * (n) = arg max y f w (x n , y) is the predicted class label of x by the model f w . So ξ (n) can be interpreted as an upper bound of the loss incurred on x (n) by the model. The cumulative loss on the whole training data is then upper bounded by N n=1 ξ (n) . In the case of 0-1 loss, the cumulative loss is exactly the number of training examples incorrectly classified by the model, which is directly related to the overall training error. So we can interpret Eq. (3) as minimizing (an upper bound of) the overall training error, with a regularization term β||w|| 2 .</p><p>If the distribution of the classes is highly skewed, say 90% of the data are of a particular class, the overall accuracy is not an appropriate metric for measuring the performance of an algorithm. A better performance measure is the mean perclass accuracy defined as follows. Let n pq (p, q ∈ Y) be the number of examples in class p being classified as class q. Define m p = q n pq , i.e. m p is the number of examples with class p. Then the mean per-class accuracy is calculated as 1/|Y| × |Y| p=1 n pp /m p . We can define the following new loss function that properly adjust the loss according to the distribution of the classes on the training data:</p><formula xml:id="formula_11">Δ new (y, y (n) ) = 1 mp if y = y (n) and y (n) = p 0 otherwise<label>(9)</label></formula><p>It is easy to verify that N n=1 Δ new (y * (n), y (n) ) directly corresponds to the mean per-class accuracy on the training data. The optimization in Eq. ( <ref type="formula">3</ref>) with Δ new will try to directly maximize the mean per-class accuracy, instead of the overall accuracy. This learning algorithm with Δ new is very similar to that with Δ 0/1 . All we need to do is use Δ new in Eq. ( <ref type="formula">3</ref>).</p><p>Our learning approach can also be extended for detection tasks <ref type="bibr" target="#b7">[8]</ref>. In that case, we can adapt our algorithm to directly optimize other metrics more appropriate for detections (e.g. F-measure, area under ROC curve, or the 50% overlapping criterion in Pascal VOC challenge <ref type="bibr" target="#b4">[5]</ref>) using the technique in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>. We omit the details due to space constraints. The flexibility of optimizing different performance measurements is an important advantage of the max-margin learning method compared with other alternatives, e.g. the hidden conditional random fields <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We test our algorithm on two datasets (called a-Pascal and a-Yahoo) introduced in <ref type="bibr" target="#b6">[7]</ref>. The first dataset (a-Pascal) contains 6340 training images and 6355 test images collected from Pascal VOC 2008 challenge. Each image is assigned one of the 20 object class labels: people, bird, cat, cow, dog, horse, sheep, aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, and TV/monitor. Each image also has 64 binary attribute labels, e.g. "2D boxy", "has hair","shiny", etc. The second dataset (a-Yahoo) is collected for 12 object categories from Yahoo images. Each image in a-Yahoo is described by the same set of 64 attributes. But the object class labels in a-Yahoo are different from those in a-Pascal. Object categories in a-Yahoo are: wolf, zebra, goat, donkey, monkey, statue of people, centaur, bag, building, jet ski, carriage, and mug.</p><p>We follow the experiment setup in <ref type="bibr" target="#b6">[7]</ref> as close as possible. However, there is one caveat. These two datasets are collected to study the problem of attribute prediction, not object class prediction. Farhadi et al. <ref type="bibr" target="#b6">[7]</ref> use the training images in a-Pascal to learn their model, and test on both the test images in a-Pascal and images in a-Yahoo. We are interested in the problem of object class prediction, so we cannot use the model trained on a-Pascal to predict the class labels for images in a-Yahoo, since they have different object categories. Instead, we randomly split a-Yahoo dataset into equal training/testing sets, so we can train a model on a-Yahoo training set and test on a-Yahoo test set.</p><p>We use the training images of a-Pascal to build the attribute relation graph using the method in Sec. 5. The graph is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We use the exact same graph in the experiments on the a-Yahoo dataset. In order to do a fair comparison with <ref type="bibr" target="#b6">[7]</ref>, we use exactly the same image features (called base feature in <ref type="bibr" target="#b6">[7]</ref>) in their work. Each image is represented as a 9751-dimensional feature vector extracted from information on color, texture, visual words, and edges. Note that since the image features are extracted from the whole image, we have essentially eliminated the weakly labeled data assumption in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> (left) shows the confusion matrix of our model trained with Δ 0/1 on the a-Pascal dataset. Table <ref type="table" target="#tab_1">1</ref> summarizes our results compared with other baseline methods. Since this dataset is heavily biased toward "people" category, we report both overall and mean per class accuracies. Here we show the results of our approach with Δ 0/1 and Δ new . The baseline algorithm is to train an SVM classifier based on the base features. To make a fair comparison, we also report results of SVM with Δ 0/1 and Δ new . We also list the result of the baseline algorithm taken from <ref type="bibr" target="#b6">[7]</ref> and the best reported result in <ref type="bibr" target="#b6">[7]</ref>. The best reported result in <ref type="bibr" target="#b6">[7]</ref> is obtained by performing sophisticated feature selection and extracting more semantic attributes. We can see that both of our models outperform the baseline algorithms. In particular, the mean per class accuracies of our models are significantly better. It is also interesting to notice that models (both our approach and SVMs) trained with Δ new achieve lower overall accuracies than Δ 0/1 , but higher mean per class accuracies. This is exactly what we would expect, since the former optimizes an objective directly tied to the mean per class accuracy, while the latter optimizes one directly tied to the overall accuracy.  The results on a-Yahoo are summarized in Table <ref type="table" target="#tab_2">2</ref>. Here we compare with baseline SVM classifiers using the base features. Farhadi et al. <ref type="bibr" target="#b6">[7]</ref> did not perform object category prediction on this dataset, so we cannot compare with them. On this dataset, the performances of using Δ 0/1 and Δ new are relatively similar. We believe it is because this dataset is not heavily biased toward any particular class. So optimizing the overall accuracy is not very different from optimizing the mean per-class accuracy. But the results still show the benefits of attributes for object classification. Figure <ref type="figure" target="#fig_2">3</ref>(right) shows the confusion matrix of our approach trained with Δ 0/1 on this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented a discriminatively trained latent model for joint modelling of object classes and their visual attributes. Different from previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, our model encapsulates the correlations among different attributes via the attribute relation graph built from training data and directly optimize the classification accuracy. Our model is also flexible enough to be easily modified according to different performance measurements. Our experimental results clearly demonstrate that object naming can benefit from inferring attributes of objects. Our work also provides a rather general way of solving many other classification tasks involving auxiliary labels. We have successfully applied a similar technique to recogize human actions from still images by considering the human poses as auxiliary labels <ref type="bibr" target="#b21">[22]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Why cannot we forget about object naming and only work on inferring attributes? Look at the image in (a), it is very hard to infer the attribute "has eye" since "eye" is a very tiny region. But we as humans can recognize it "has eyes" most likely because we recognize it is a bird. Other attributes are difficult to infer from visual information alone, e.g. "edible" for (b) and "sit on" for (c). Meanwhile, objects with similar visual attributes, e.g. (d) and (e), can have different functions, which can be easily inferred if we can name the objects.</figDesc><graphic coords="2,57.42,417.45,61.10,57.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the attribute relation graph learned from the training data from the a-Pascal dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Confusion matrices of the classification result of our approach with Δ 0/1 on the a-Pascal (left) and a-Yahoo (right) datasets. Horizontal rows are ground truths, and vertical columns are predictions. Each row is normalized to sum to 1. The mean per class accuracy is calculated by averaging the main diagonal of this matrix. Dark cells correspond to high values.</figDesc><graphic coords="12,245.48,367.60,122.36,86.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results on the a-Pascal dataset. We report both overall and mean per class accuracies, due to the fact that this dataset is heavily biased toward "people" category</figDesc><table><row><cell>method</cell><cell cols="2">overall mean per-class</cell></row><row><cell cols="2">Our approach with Δ 0/1 62.16</cell><cell>46.25</cell></row><row><cell cols="2">Our approach with Δnew 59.15</cell><cell>50.84</cell></row><row><cell>SVM with Δ 0/1</cell><cell>58.77</cell><cell>38.52</cell></row><row><cell>SVM with Δnew</cell><cell>53.74</cell><cell>44.04</cell></row><row><cell cols="2">[7] (base features+SVM) 58.5</cell><cell>34.3</cell></row><row><cell>[7] (best result)</cell><cell>59.4</cell><cell>37.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on the a-Yahoo dataset. Similarly, we report both overall and mean per class accuracies</figDesc><table><row><cell>method</cell><cell cols="2">overall mean per-class</cell></row><row><cell cols="2">Our approach with Δ 0/1 78.67</cell><cell>71.45</cell></row><row><cell cols="2">Our approach with Δnew 79.88</cell><cell>73.31</cell></row><row><cell>SVM with Δ 0/1</cell><cell>74.43</cell><cell>65.96</cell></row><row><cell>SVM with Δnew</cell><cell>74.51</cell><cell>66.74</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering intrinsic scene characteristics from images</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Systems</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distributions with dependence trees</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative models for static humanobject interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Structured Models in Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large margin training for hidden markov models with partially observed states</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A support vector method for multivariate performance measures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Correlative multi-label video annotation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Optimizing complex loss functions in structured prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured prediction, dual extragradient and Bregman projections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1627" to="1653" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Configuration estimates improve pedestrian finding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attribute-based people search in surveillance environments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Vaquero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>IEEE Workshop on Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint learning of visual attributes, object classes and visual saliency</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building text features for object image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Max-margin hidden conditional random fields for human action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
