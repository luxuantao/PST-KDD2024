<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforced Negative Sampling for Recommendation with Exposure Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingtao</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
							<email>liyong07@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforced Negative Sampling for Recommendation with Exposure Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In implicit feedback-based recommender systems, user exposure data, which record whether or not a recommended item has been interacted by a user, provide an important clue on selecting negative training samples. In this work, we improve the negative sampler by integrating the exposure data. We propose to generate high-quality negative instances by adversarial training to favour the difficult instances, and by optimizing additional objective to favour the real negatives in exposure data. However, this idea is non-trivial to implement since the distribution of exposure data is latent and the item space is discrete. To this end, we design a novel RNS method (short for Reinforced Negative Sampler) that generates exposure-alike negative instances through feature matching technique instead of directly choosing from exposure data. Optimized under the reinforcement learning framework, RNS is able to integrate user preference signals in exposure data and hard negatives. Extensive experiments on two real-world datasets demonstrate the effectiveness and rationality of our RNS method. Our implementation is available at: https://github. com/dingjingtao/ReinforceNS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prevalence of implicit feedback has boosted the research and development of implicit feedback-based recommender systems <ref type="bibr" target="#b0">[Bayer et al., 2017;</ref><ref type="bibr" target="#b14">Yang et al., 2018]</ref>. The key challenge in learning from implicit feedback lies in the natural scarcity of negative signal, known as one-class problem <ref type="bibr" target="#b12">[Pan et al., 2008]</ref>. To address this issue, negative sampling has been widely adopted in previous works <ref type="bibr">[Rendle et al., 2009]</ref>, where the common approach is to uniformly sample negative instances from the missing data (i.e., the unobserved interactions) <ref type="bibr" target="#b8">[Jiang et al., 2018;</ref><ref type="bibr" target="#b7">He et al., 2018;</ref><ref type="bibr" target="#b12">Lin et al., 2019]</ref>. This process, with no doubt, plays a critical role in training recommender models from implicit feedback.</p><p>Given the importance of designing a quality negative sampler for implicit recommender models, two types of methods have been proposed in previous works. The first is heuristicbased methods, including dynamic negative sampling (DNS) that oversamples the hard negative instances during the training process <ref type="bibr" target="#b14">[Zhang et al., 2013;</ref><ref type="bibr">Rendle and Freudenthaler, 2014]</ref>, and frequency-based sampling that subsamples frequent instances <ref type="bibr" target="#b2">[Caselles-Dupré et al., 2018]</ref>. The other type is auxiliary information-based methods, which focus on choosing more reliable negative instances by leveraging the auxiliary data such as clicked but non-purchased items in Ecommerce websites <ref type="bibr" target="#b4">[Ding et al., 2018a]</ref>.</p><p>In real-world scenarios, platforms can easily collect whether the recommended (i.e., exposed) item has been interacted by a user. These records, also referred to as the exposure data, contain rich information about the negative preference of users. However, due to the inaccessibility of such data by the third parties, previous academic research exploits the interaction data only to build the negative sampler. As a result, the reliability of the generated negative instances is questionable. Nevertheless, it is non-trivial to integrate the exposure data into the negative sampler design because of the following challenges:</p><p>• Incompleteness of negative preference in exposure data.</p><p>The exposed items are typically selected by a recommendation engine. Besides the exposed but non-interacted items, other non-exposed items can also be the negative preference for a user. If the sampler simply generates negative instances according to the exposure data, it will cause selection bias, making the model under-trained and resulting in suboptimal performance <ref type="bibr" target="#b10">[Lian et al., 2017;</ref><ref type="bibr" target="#b4">Ding et al., 2018a</ref>]. • Difficulty of optimizing the negative sampler. Due to the discrete sampling process on item IDs, the objective function is non-differentiable. As such, it cannot be optimized with traditional gradient-based techniques that can only deal with continuous functions.</p><p>In this work, we design a novel embedding-based sampler model named Reinforced Negative Sampler (RNS) that learns to generate informative and effective negative samples. Specifically, the sampler collaborates with another embedding-based recommender model, supplying negative instances for the recommender to do pairwise learning. The sampler has two goals -generating hard and real negative instances. The hard goal is achieved through adversarial training between the recommender and the sampler. Simultaneously, the sampler is also rewarded to generate negative instances that overlap with the non-interacted instances in the exposure data. Corresponding to aforementioned two challenges, here the "reinforced" has two-fold meaning. First, the generation of real negative instances is reinforced by a feature matching technique, which forces the empirical distribution of the generated and the exposed negative instances to have matched moments in the latent feature space. Second, we consider a reinforcement learning setting in the sampler so as to receive gradient information from the recommender and optimize the non-differentiable exposure-based objective.</p><p>We summarize the contributions of this paper as follows. 1. We are the first to consider generating exposure-alike negative instances for implicit recommendation. The proposed RNS model is general in optimizing any recommender models, with the potential of large impact. 2. We propose two specific designs to generate high-quality negative instances, including the adversarial training for hard negatives and the feature matching for generating exposure-alike negatives that are more reliable. 3. We conduct extensive experiments on two real-world datasets to demonstrate the effectiveness of RNS. More ablation studies verify the efficacy of the two designs and the utility of feature matching in leveraging exposure data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We start by introducing some basic notations. For a specific user u, C u denotes the set of items that are interacted by u, while E u refers to those non-interacted items within u's exposure history. We represent matrices, vectors, and scalars as bold capital letters (e.g., X), bold lower-case letters (e.g., x), and normal lower-case letters (e.g., x), respectively. We use symbols σ and to denote the sigmoid function and elementwise production, respectively. Our proposed recommender-sampler framework is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. The sampler (S) calculates a probability distribution over a set of candidate negative instances, then samples one of them as the output. Next the recommender (R) is optimized to learn the pairwise ranking relation between a ground truth instance and a generated negative one. After receiving the multiple reward signals, S is encouraged to generate both hard (ω 1 uj ) and real (ω 2 uj /ω 3 uj ) negative instances. During training process, R can benefit more from the better quality negative instances and thus perform better on predicating user preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Recommender Model (R)</head><p>To learn recommender models from implicit feedback, Rendle et al. <ref type="bibr">[Rendle et al., 2009]</ref> proposed the Bayesian Personalized Ranking (BPR) method, which assumes that a positive instance should be predicted with a much higher score over the negative one. Based on BPR, the training objective of R can be formulated as minimizing the following loss function:</p><formula xml:id="formula_0">L R = (u,i)∈C − ln σ(r ui (Θ)−r uj (Θ)), where j ∼ ΨS (j|u). (1)</formula><p>In (1), for each user u, the predicted preference score on items is denoted as ru• (Θ), where Θ refers to the model parameters. The negative instance j is generated by the sampler (S) according to a conditional distribution ΨS (•|u), while the positive instance i is randomly chosen from ground truth set C u . Minimizing L R is equivalent to maximizing the margin between rui and ruj , which encourages R to learn the pairwise ranking relation of user preference between i and j.</p><p>To calculate ru• (Θ), we adopt the Generalized Matrix Factorization (GMF) <ref type="bibr">[He et al., 2017]</ref> model that allows different dimensions of the embedding space to have different weights. Specifically, it first uses an element-wise product to obtain an interacted feature vector, and then project the feature vector to an output score with a weight vector as follows, rui (Θ) = h r T f ui = h r T (p r u q r i ), where h r ∈ R K×1 denotes the learnable weight vector and f ui denotes the interacted feature vector. The K-dimensional user embedding and item embedding are represented as p r u and q r i , respectively. Based on GMF, the model parameters Θ is {p r u , q r i , h r }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Sampler Model (S)</head><p>We first introduce two specific design of our proposed reinforced negative sampler (RNS), corresponding to generating hard and real negative samples. Then we train RNS through policy gradient in reinforcement learning (RL) setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Sampler</head><p>Intuitively, S can generate adversarial negative instances that have high scores ru• , which are hard for R to rank correctly. Therefore, the objective of this adversarial sampler is formulated as maximizing the expectation of j-related part in L R :</p><formula xml:id="formula_1">L AS = (u,i)∈C E j∼ ΨS (j|u) −σ(−r uj (Θ)) denoted as ω 1 uj .<label>(2)</label></formula><p>Note that we have left out the logarithm to control its values within [−1, 0]. By this means, the generated negative instances are given higher prediction scores by R, being more close to those of positive instances, which provides larger gradients and more information for R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exposure-matching Sampler</head><p>As both users' interaction and non-interaction are explicitly recorded in the exposure data, it is reasonable to generate real negative samples based on the exposed but not interacted instances. Therefore, our second design of RNS is introducing an exposure-matching sampler that learns a probability distribution to match negative signal in exposure data. Given u's exposed itemset E u , a direct design is to maximize the overlap between the set of generated instances and E u . Specifically, we consider the following objective:</p><formula xml:id="formula_2">L 0 ES = (u,i)∈C E j∼ ΨS (j|u) 1 Eu (u, j) ,</formula><p>where</p><formula xml:id="formula_3">1 Eu (u, j) = 1 , j ∈ E u 0 , else ,<label>(3)</label></formula><p>where the binary indicator function 1 Eu serves as a guiding signal, encouraging S to generate a set of negative instances that has a big overlap with exposed itemset E u . However, for the set of generated negative instances and the set of exposed instances, L 0 ES measures their distance by the size of overlapping set, i.e., a simple scalar, which does not provide information in latent feature level and thus be suboptimal. Consequently, the sampler trained with this objective may tend to choose exactly the same instances from the exposure data, which will harm the performance as not all of the exposed instances are beneficial.</p><p>Therefore, we further measure the similarity in the latent feature space. As we adopt GMF to calculate preference scores in R, for a user-item pair denoted as (u, i), the interacted feature vector f ui is the corresponding representation in the latent feature space. Consider a subset of generated negative instances G s , for each (u, j) ∈ G s , we sample a (u, k) from E u and group all these sampled instances into a subset denoted as E s . Then, to measure the distance between G s and E s , we consider the maximum mean discrepancy (MMD) between the empirical distribution of feature vectors in these two subsets <ref type="bibr" target="#b10">[Li et al., 2015;</ref><ref type="bibr">Zhang et al., 2017]</ref>. Concisely, MMD measures the mean squared difference between two sets of samples over a universal reproducing kernel Hilbert space. In our case, the MMD for two empirical distributions of f in G s and E s is given by</p><formula xml:id="formula_4">J M M D 2 = 1 L 2 (u,j)∈Gs (u ,j )∈Gs k(f G uj (Θ), f G u j (Θ)) − 2 L 2 (u,j)∈Gs (v,k)∈Es k(f G uj (Θ), f E vk (Θ)) + 1 L 2 (v,k)∈Es (v ,k )∈Es k(f E vk (Θ), f E v k (Θ)),<label>(4)</label></formula><p>where L is the size of two sets and k(•,</p><formula xml:id="formula_5">•) : R K × R K → R is the kernel function.</formula><p>Here we use a universal Gaussian kernel, i.e.,, k(x, x ) = exp(−||x − x || 2 /2τ ) with bandwidth τ , and minimize MMD to match all moments of two distributions.</p><p>From the above considerations, we design the exposurematching sampler with a weighted sum of overlap-based objective and MMD-based objective. Mathematically, it can be formulated as maximizing</p><formula xml:id="formula_6">L ES = (u,i)∈C E j∼ ΨS (1−β) 1 Eu (u, j) denoted as ω 2 uj +β M (u, j) denoted as ω 3 uj , (5)</formula><p>where β controls the importance of MMD-based objective M (u, j) that aims to match the feature distribution. As the MMD is calculated between two sets, i.e., G s and E s , for each u and u's generated negative instance j, M (u, j) only refers to those corresponding terms in (4), which is given by</p><formula xml:id="formula_7">M (u, j) = 2 L 2 (v,k)∈Es k(f G uj , f E vk ) − (u ,j )∈Gs k(f G uj , f G u j ) . (6)</formula><p>Training with Reinforcement Learning Finally, combining the above two specifically designed sampler together, we obtain the following objective for the negative sampler (S):</p><formula xml:id="formula_8">L S = L AS + αL ES = (u,i)∈C E j∼ ΨS (j|u) ω 1 uj + α(1 − β)ω 2 uj + αβω 3 uj denoted as ωuj ,<label>(7)</label></formula><p>where {ω 1 uj , ω 2 uj , ω 3 uj } refer to adversarial objective, overlapbased objective and MMD-based objective, respectively (See (2) and ( <ref type="formula">5</ref>)). By maximizing L S , S is encouraged to generate both hard and real negative samples with exposure data.</p><p>However, unlike the optimization of R that can be achieved by the stochastic gradient descent (SGD), training S has following two problems. First, it involves a discrete sampling step, which makes simple differentiation infeasible. Second, the exposure-aware indicator function 1 Eu is nondifferentiable. Therefore, for S with model parameters Φ, we use the policy gradient based RL <ref type="bibr" target="#b13">[Sutton et al., 2000]</ref> to derive its gradient:</p><formula xml:id="formula_9">∇ Φ L S = ∇ Φ (u,i)∈C E j∼ ΨS (j|u) ω uj = (u,i)∈C E j∼ ΨS (j|u) ω uj ∇ Φ log ΨS (j|u) (u,i)∈C 1 T jt∼ ΨS (j|u),t≤T ω ujt ∇ Φ log ΨS (j t |u) ,<label>(8)</label></formula><p>where we approximate the expectation with sampling in the last step. With the RL terminology, the agent, i.e., the sampler S, follows a policy ΨS (j|u) and takes an action as generating a negative instance j for a certain user u. Then, for each action (u, j), the environment, i.e., the recommender R, will return a reward ω uj to S that guides the direction of optimization. In this way, S is iteratively optimized towards maximizing the returned reward, i.e., generating the negative instances that are both hard and real.</p><p>To produce the probability distribution for sampling negative instances, i.e., ΨS (j|u), S first calculates the scores for a set of negative candidate instances and then obtain their corresponding softmax probability. Similar to R, S also uses GMF to calculate the score with model parameters Φ = {p s u , q s i , h s }. Mathematically, ΨS (j|u) is modeled as</p><formula xml:id="formula_10">ΨS (j|u) = exp ŝuj (Φ) j ∈Nu exp ŝuj (Φ) ,<label>(9)</label></formula><p>where ŝu• (Φ) denotes the score and N u denotes u's candidate set for the generated negative instances. Intuitively, N u should contain all the instances that are not interacted by u, i.e., C u . However, as the hard negative instances are very likely to be those unobserved positive instances, we generate N u by uniformly preselecting N s instance from C u . The overall training process is summarized in Algorithm 1. Both R and S require pre-training, which is achieved by training the GMF model with BPR objective. The learning process is carried out in mini-batch mode, where R and S alternatively update their parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion</head><p>Our proposed recommender-sampler framework follow the general design of Generative Adversarial Networks <ref type="bibr" target="#b6">[Goodfellow et al., 2014]</ref>, which contains two parts, the generator and the discriminator. In recommendation fields, the previous GAN-based models, such as <ref type="bibr">IRGAN [Wang et al., 2017]</ref>, CF-GAN <ref type="bibr" target="#b2">[Chae et al., 2018]</ref> and AdvIR [Park and Chang, 2019], focuses on training a better generator to deceive the discriminator. In this sense, their generator is optimized to generate the positive instance and thus can predict user preference. However, in our work, the sampler generates the negative instances to train a better recommender as prediction model. The most related work is KBGAN <ref type="bibr" target="#b1">[Cai and Wang, 2018</ref>] that generates hard negative instances to train better knowledge graph embeddings. However, our RNS model not only has an adversarial sampler, but also tries to generate real negative instances with exposure data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets and Preprocessing. We perform experiments on two real-world datasets with both interactions and exposure:</p><p>Beibei<ref type="foot" target="#foot_0">1</ref> is one of the largest Chinese E-commerce websites. We sample a subset of data that contains item clicks and exposure within the time period from 2017/06 to 2017/07.</p><p>Zhihu<ref type="foot" target="#foot_1">2</ref> is the largest question-and-answer website in China, where users click articles of interest to read. Here we use a public benchmark released in CCIR-2018 Challenge<ref type="foot" target="#foot_2">3</ref> .</p><p>In the raw data of Beibei and Zhihu, each user's records are grouped into different sessions, during which the user is recommended with a fixed number of items and only clicks some of them. Therefore, we consider a session-based data preprocessing with three steps. First, we filter out the repetitive clicks after the earliest one, as we aim to recommend novel items. Second, we only retain those unclicked items in the exposure data. Third, we filter out users and items with less than 4 (Beibei) and 6 (Zhihu) sessions to overcome the problem of high sparsity of raw datasets. Evaluation Methodology. As the exposure and clicks are grouped into sessions, we adopt an evaluation protocol similar to leave-one-out <ref type="bibr">[Rendle et al., 2009]</ref>, where the click interactions in the latest session of each user are held out for testing. For hyper-parameters tuning we further hold out the latest session from each user's training data as the validation set. Table <ref type="table" target="#tab_0">1</ref> summarizes the statistics of experiment datasets. For the metrics, we employ Area Under the Curve (AUC) and Normalized Discounted Cumulative Gain (NDCG) on the ranking of a list of testing items for a user. We fix the list length as L and newly add some unclicked items into a user's list if it is less than L, and it is set as 40 and 160 for Beibei and Zhihu, respectively, which are the same as those in raw data. In this way, both AUC and NDCG equal to 1 when all the clicked items are ranked higher than other unclicked items. Finally we report the average score of all users.</p><p>Baselines. We compare our proposed RNS method with three groups of the baselines.</p><p>First we consider two common baselines:</p><p>-ItemPop. This method ranks items base on their popularity, as judged by the number of click interactions.</p><p>-BPR-GMF <ref type="bibr">[Rendle et al., 2009]</ref>. BPR optimizes the MF-based model with a pairwise ranking loss to learn from implicit feedback.</p><p>For methods related to the adversarial sampler, we choose:</p><p>-BPR-DNS <ref type="bibr" target="#b14">[Zhang et al., 2013]</ref>. Dynamic Negative Sampling (DNS) selects the item with the highest prediction score among X randomly sampled negatives.</p><p>-KBGAN <ref type="bibr" target="#b1">[Cai and Wang, 2018]</ref>. With the generator serving as an adversarial sampler, KBGAN can be considered as a soft version of DNS. Here we combine it with BPR objective.</p><p>- <ref type="bibr">IRGAN [Wang et al., 2017]</ref>. This method is a GANbased IR model that aims to obtain a better generator through adversarial training, which is used to predict user preference.</p><p>Finally, we consider two exposure-enhanced samplers:</p><p>-BPR-EN. This method selects negatives only from exposure data. We use it to investigate the impact of selecting negatives from an incomplete candidate set.</p><p>-EBPR. Similar to <ref type="bibr" target="#b4">[Ding et al., 2018a]</ref>, we consider to weight those exposed but unclicked items differently (compared with other unexposed items) when choosing negatives.</p><p>Parameter settings. For above baselines, we use GMF as scoring model and explore hyper-parameters similarly as the original paper. The mini-batch size and embedding size for all methods are set as 1024 and 32, respectively. We search L 2 regularizer and learning rate in [10 −6 ,10 −5 ,10 −4 ,10 −3 ,10 −2 ] and [0.0001, 0.0005, 0.001, 0.05, 0.1], respectively, and use Adam optimizer for learning. In addition, the size of negative candidate set, i.e., N s , is set as 100 and 30 in Beibei and Zhihu, respectively, which is optimal among <ref type="bibr">[10, 20, ..., 150]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison</head><p>Table <ref type="table" target="#tab_1">2</ref> displays the recommendation performance w.r.t. AUC and NDCG on two datasets, where we perform paired t-test between RNS and each of baselines over 10-round results. We can observe that our proposed RNS achieves the best performance compared to those using the adversarial sampler. For Beibei, it improves the AUC and NDCG by 0.60% and 1.20%, while the improvement is much larger for Zhihu, i.e., 1.97% and 1.91%. Compared with these baselines that simply choose hard negative instances based on the model's own inference, RNS use previous exposure information as the guiding signal to find more reliable negative instances, which explains the above performance improvement. Besides, we observe that BPR-DNS with a rather straightforward design consistently outperforms KBGAN and IRGAN, which may be due to the difficulty of learning an accurate distribution within a large item space (10 4 ∼ 10 5 ). • RNS better leverages negative preference signal in exposure data. Corresponding to our aforementioned challenge, we observe the significant performance degradation with a naive exposure-enhanced sampler.</p><p>For BPR-EN that, as a common practice in most companies, selects negative instances only from the exposure, RNS outperforms it by 17.55% and 23.74% in AUC and NDCG for Beibei and similarly for Zhihu. As for EBPR, the relative improvement of RNS is 3.02% and 4.24% in AUC and NDCG for Beibei and 1.45% and 1.47% in AUC and NDCG for Zhihu. With our designed feature matching scheme, RNS can generate exposure-alike negatives from a much larger space that are not limited to exposure data, avoiding the incompleteness of negative preference in exposure data. This explains the observed outperformance of RNS. To summarize, these comparisons verify that our proposed RNS model can effectively generate both hard and real negative instances to train a better implicit recommender model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hard Negative v.s. Real Negative</head><p>In RNS, we combine the adversarial sampler (AS) and exposure-matching sampler (ES) together so as to improve the quality of generated negative instances. An intuitive question is whether the designed two parts can really help? To answer it, we conduct experiments on two degenerative methods of RNS, in which only the adversarial objective L AS and the exposure-matching objective L ES are considered in sampler, respectively. We adopt the same evaluation method with above experiments, and the performance comparison is shown in Table <ref type="table" target="#tab_2">3</ref>, along with the value of weighting parameters for L ES and MMD-based objective, i.e., α and β. By comparing with BPR-GMF, we observe that both AS part and ES part play an essential role in RNS. Comparatively, generating negative instances that matches the exposure data (RNS-ES) is more helpful than generating hard negative instances through adversarial learning (RNS-AS). As these two samplers capture different signals, our experiments demonstrate that unifying them can achieve further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Impact of Feature Matching Scheme</head><p>Here we investigate whether the designed feature matching scheme helps generating exposure-alike instances, by conducting experiments on RNS-ES that only retain the exposure-matching objective. Figure <ref type="figure" target="#fig_2">2</ref> shows the recommendation performance with respect to different β. We can clearly observe that increasing β from 0.0 to 0.9 can effectively improve the NDCG. More specifically, it increases from 0.3986 to 0.4060 (+1.86%) and 0.6555 to 0.6605 (+0.76%) on Beibei and Zhihu, respectively. As β denotes the weight of MMD-based objective for the feature matching, a peak with large value (0.8∼0.9) highlights the advantage of our proposed feature matching scheme.</p><p>To further illustrate its advantage, we randomly select two subsets of generated and exposed instances on Beibei, 1000 each, and visualize their interacted feature vectors before (β = 0.0) and after (β = 0.9) feature matching in Figure <ref type="figure">3</ref>(a) and (b), respectively, via t-SNE. With red points as the exposed instances, blue points as the generated instances and green points as those overlapped ones, we observe a much more similar distribution in Figure <ref type="figure">3(b</ref>) . On the one hand, the generated negatives (blue) are much closer to exposed ones (red) when β = 0.9, while they are densely gathered in a small area near (0, 0) when β = 0.0. On the other hand, the number of overlapped instances (green) decreases significantly with β increasing from 0.0 to 0.9. To measure the similarity between two sets of generated and exposed instances, we calculate two metrics of overlap and MMD and illustrate them in Figure <ref type="figure">3(c) and (d)</ref>, where the smaller value of both overlap and MMD can be observed with β = 0.9. This finding is interesting and insightful, implying that the proposed feature matching scheme encourages the sampler to focus on generating the negative instances that are similar to exposed instances in the feature space, rather than choosing exposed instances directly. By this means, although the overlap between generated negatives and exposure data is small, they are more similar in the distribution perspective (small MMD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Generating Adversarial Samples for Recommendation. A typical approach is the dynamic negative sampling strategy that generates hard negative instances to construct informative item pairs during the training process <ref type="bibr" target="#b14">[Zhang et al., 2013;</ref><ref type="bibr" target="#b15">Zhang et al., 2019a]</ref>. Recently, GAN-based approach has also been adopted in training better recommender models with adversarial instances <ref type="bibr">[Wang et al., 2017;</ref><ref type="bibr" target="#b2">Chae et al., 2018;</ref><ref type="bibr">Park and Chang, 2019</ref>]. As we discussed in methodology section, our RNS model differs from them by generating both hard and real negative instances with exposure data. Recommendation with Exposure Data. There exists no previous work on training a better negative sampler with user exposure. Due to lack of data, most works use probabilistic approach to model user exposure as a latent variable and infer its value from interaction data <ref type="bibr" target="#b11">[Liang et al., 2016]</ref> or social relationship <ref type="bibr" target="#b3">[Chen et al., 2019]</ref>. Previous works using exposure data in recommendation are not related to user preference learning. <ref type="bibr" target="#b9">Lee et al. [2014]</ref> propose a re-ranking approach based on items' historical impressions (i.e., exposure). <ref type="bibr" target="#b17">Zhao et al. [2018]</ref> investigate reasons behind user inaction. In contrast, our RNS improves the performance by leveraging the negative preference information existed in exposure data, which cannot be simply integrated like other auxiliary information in multiple feedback recommendation <ref type="bibr" target="#b5">[Ding et al., 2018b;</ref><ref type="bibr" target="#b6">Gao et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We study the problem of learning to sample negative instances for recommendation from exposure data, rather than manually designing sampling heuristics. To generate both hard and real negatives, we propose a RNS model that combines adversarial training and feature matching together, which is trained with RL method. With these designs, RNS can not only achieve higher accuracy, but also be applied to any recommender systems with negative sampling. In the future, we plan to learn more general negative samplers for social-aware or context-rich recommender systems <ref type="bibr" target="#b12">[Lin et al., 2019;</ref><ref type="bibr" target="#b16">Zhang et al., 2019b]</ref>, and other related fields such as network embedding and NLP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed recommender-sampler framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>The RNS algorithm. Data : Interaction data C = {(u, i)}, exposure data E; Input : Pre-trained recommender R 0 with parameters Θ, Pre-trained sampler S 0 with parameters Φ; Output: The final recommender R for prediction; 1 while Stopping criteria is not met do 2 Sample a mini-batch of data C s from C 3 for (u, i) ∈ C s do 4 Uniformly sample N s negative instances as N u ; 5 Obtain their sampling probability { Ψ} by (9); 6 S generate a negative instance j ∈ N u with { Ψ}; 7 G R ← G R + ∇ Θ L R (u, i, j); // R's gradients 8 G s .add((u, j)); // the set of generated instances 9 Sample one exposed instance j from E u , E s .add((u, j )); // used for calculating MMD 10 end 11 for (u, j) ∈ G s do 12 Calculate reward ω uj by (2)-(7); 13 G S ← G S + ω uj ∇ Φ log Ψ(j|u); // S's gradients 14 end 15 Θ ← Θ + λ R G R , Φ ← Φ + λ S G S ; // update R, S 16 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of weight β on RNS-ES's performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: 2d t-SNE visualizations of the feature vectors and statistics of the similarity (overlapMMD), before and after feature matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets.</figDesc><table><row><cell>Dataset User# Item# Train#</cell><cell>Val.# Test# Exposure#</cell></row><row><cell cols="2">Beibei 66,450 59,290 1,617,541 73,906 73,208 29,694,415</cell></row><row><cell cols="2">Zhihu 16,015 45,782 2,433,969 410,736 440,029 6,711,820</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between all the methods, significant test is based on AUC.</figDesc><table><row><cell cols="2">Datasets</cell><cell>Beibei</cell><cell></cell><cell>Zhihu</cell><cell></cell></row><row><cell cols="6">Group Methods AUC NDCG p-value AUC NDCG p-value</cell></row><row><cell>Com.</cell><cell cols="5">ItemPop 0.6694 0.3668 1e-44 0.6500 0.6203 8e-47 BPR-GMF 0.7065 0.3950 1e-22 0.6903 0.6443 2e-26</cell></row><row><cell></cell><cell cols="5">BPR-DNS 0.7125 0.4013 3e-24 0.6939 0.6499 7e-35</cell></row><row><cell>AS</cell><cell cols="5">KBGAN 0.7109 0.3995 6e-25 0.6934 0.6486 1e-33</cell></row><row><cell></cell><cell cols="5">IRGAN 0.7091 0.3963 5e-26 0.6686 0.6272 6e-21</cell></row><row><cell>ES</cell><cell cols="5">BPR-EN 0.6098 0.3282 9e-38 0.6196 0.5697 3e-40 EBPR 0.6958 0.3896 1e-28 0.6975 0.6527 2e-19</cell></row><row><cell>AS+ES</cell><cell>RNS</cell><cell>0.7168 0.4061</cell><cell>-</cell><cell>0.7076 0.6623</cell><cell>-</cell></row><row><cell cols="6">From above results, we have the following observations:</cell></row><row><cell cols="6">• RNS significantly improves the recommendation per-</cell></row><row><cell cols="6">formance by training with the high quality negative</cell></row><row><cell cols="6">instances. Compared with ItemPop and BPR-GMF, our</cell></row><row><cell cols="6">proposed RNS outperforms the best of them by 1.46%</cell></row><row><cell cols="6">and 2.81% in AUC and NDCG for Beibei and by 2.51%</cell></row><row><cell cols="6">and 2.79% in AUC and NDCG for Zhihu. It demon-</cell></row><row><cell cols="6">strates that generating better quality negative instances is</cell></row><row><cell cols="6">vital for learning user preference among different items.</cell></row><row><cell cols="6">• RNS further improves the generation of hard neg-</cell></row><row><cell cols="6">ative instances by integrating exposure information.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Impact of AS and ES in RNS.    </figDesc><table><row><cell>Datesets</cell><cell></cell><cell>Beibei</cell><cell>Zhihu</cell></row><row><cell cols="2">Methods α</cell><cell>β AUC NDCG α</cell><cell>β AUC NDCG</cell></row><row><cell cols="2">BPR-GMF -</cell><cell>-0.7065 0.3950 -</cell><cell>-0.6903 0.6443</cell></row><row><cell cols="4">RNS-AS 0.00 -0.7106 0.3985 0.00 -0.7002 0.6570</cell></row><row><cell cols="2">RNS-ES -</cell><cell cols="2">0.30 0.7160 0.4055 -0.90 0.7066 0.6605</cell></row><row><cell>RNS</cell><cell cols="3">2.00 0.20 0.7168 0.4061 2.50 0.75 0.7076 0.6623</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.beibei.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.zhihu.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://biendata.com/competition/CCIR2018/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by the National Nature Science Foundation of China under 61861136003, 61621091 and 61673237, Beijing National Research Center for Information Science and Technology under 20031887521, and research fund of Tsinghua University -Tencent Joint Laboratory for Internet Innovation Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A generic coordinate descent framework for learning from implicit feedback</title>
		<author>
			<persName><surname>Bayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kbgan: Adversarial learning for knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">;</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cfgan: A generic collaborative filtering framework based on generative adversarial networks</title>
		<author>
			<persName><surname>Caselles-Dupré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Samwalker: Social recommendation with informative sampling strategy</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An improved sampler for bayesian personalized ranking by leveraging view data</title>
		<author>
			<persName><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018a. 2018</date>
			<publisher>WWW Companion</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving implicit recommender systems with view data</title>
		<author>
			<persName><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018b. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural multi-task recommendation from multi-behavior data</title>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative filtering</title>
				<imprint>
			<publisher>Xiangnan He</publisher>
			<date type="published" when="2014">2019. 2019. 2014. 2014. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xiangnan He, Zhankui He, Xiaoyu Du, and Tat-Seng Chua. Adversarial personalized ranking for recommendation</title>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recommendation in heterogeneous information networks based on generalized random walk model and bayesian personalized ranking</title>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling impression discounting in large-scale recommender systems</title>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xing Xie, and Guangzhong Sun. Practical lessons for job recommendations in the cold-start scenario</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys Challenge</title>
				<imprint>
			<date type="published" when="2015">2015. 2015. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modeling user exposure in recommendation</title>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial sampling and training for semi-supervised information retrieval</title>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback</title>
				<editor>
			<persName><forename type="first">Dae</forename><surname>Hoon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Park</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2019. 2019. 2008. 2008. 2019. 2019. 2014. 2009</date>
		</imprint>
	</monogr>
	<note>UAI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Irgan: A minimax game for unifying generative and discriminative information retrieval models</title>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2000">2000. 2000. 2017. Jun. 2017</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unbiased offline recommender evaluation for missing-not-atrandom implicit feedback</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2013">2018. 2018. 2013. 2013. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nscaching: Simple and efficient negative sampling for knowledge graph embedding</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2019">2019a. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Next: a neural network framework for next poi recommendation</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<date type="published" when="2019">2019b. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Interpreting user inaction in recommender systems</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<editor>RecSys</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
