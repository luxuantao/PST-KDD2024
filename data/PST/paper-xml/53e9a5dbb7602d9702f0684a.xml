<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An inexact primal-dual path following algorithm for convex quadratic SDP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-01-20">20 January 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kim-Chuan</forename><surname>Toh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">K.-C</forename><surname>Toh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">K.-C</forename><surname>Toh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Singapore-MIT Alliance</orgName>
								<address>
									<addrLine>4 Engineering Drive 3</addrLine>
									<postCode>117576</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An inexact primal-dual path following algorithm for convex quadratic SDP</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-01-20">20 January 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">A89093048832C4E9DD3E7A14069103DD</idno>
					<idno type="DOI">10.1007/s10107-006-0088-y</idno>
					<note type="submission">Received: 15 February 2006 / Accepted: 27 October 2006 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semidefinite programming</term>
					<term>Semidefinite least squares</term>
					<term>Interior point method</term>
					<term>Inexact search direction</term>
					<term>Krylov iterative method Mathematics Subject Classification (2000) 90C22</term>
					<term>90C25</term>
					<term>90C51</term>
					<term>65F10</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose primal-dual path-following Mehrotra-type predictorcorrector methods for solving convex quadratic semidefinite programming (QSDP) problems of the form: min</p><p>and A is a linear map from S n to R m . At each interior-point iteration, the search direction is computed from a dense symmetric indefinite linear system (called the augmented equation) of dimension m + n(n + 1)/2. Such linear systems are typically very large and can only be solved by iterative methods. We propose three classes of preconditioners for the augmented equation, and show that the corresponding preconditioned matrices have favorable asymptotic eigenvalue distributions for fast convergence under suitable nondegeneracy assumptions. Numerical experiments on a variety of QSDPs with n up to 1600 are performed and the computational results show that our methods are efficient and robust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Let S n be the space of n × n symmetric matrices endowed with the standard trace inner product denoted by "•". We consider the following convex quadratic semidefinite program (QSDP):</p><formula xml:id="formula_0">(QSDP) min X X • Q(X)/2 + C • X : A(X) = b, X 0 ,<label>(1)</label></formula><p>where Q : S n → S n is a given self-adjoint positive semidefinite linear operator, A : S n → I R m is a linear map, and b ∈ I R m . The notation X 0 means that X is positive semidefinite. Let A T be the adjoint of A. The Lagrangian dual problem of ( <ref type="formula" target="#formula_0">1</ref>) is given by</p><formula xml:id="formula_1">max X,y,S -X • Q(X)/2 + b T y : A T (y) -Q(X) + S = C, S 0 . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>We use the following notation and terminology. For an integer n, we let n = n(n + 1)/2. Given U ∈ I R q×l , V ∈ I R q×n , the symmetrized Kronecker product U V is the linear map from I R n×l to S q defined by U V(M) = (VMU T + UM T V T )/2. For U ∈ I R p×l and V ∈ I R q×n , the Kronecker product U ⊗ V is the liner map from I R n×l to I R q×p defined by U ⊗ V(M) = VMU T ; see <ref type="bibr">[15, p. 254</ref>]. The Hadamard product, U • V, of two matrices with the same dimensions is defined by</p><formula xml:id="formula_3">(U • V) ij = U ij V ij .</formula><p>The set of symmetric positive semidefinite (definite) matrices is denoted by S n + (S n ++ ). We use • 2 to denote the vector 2-norm or matrix 2-norm, and • F to denote the Frobenius norm. We denote the identity matrix or operator of dimension d by I d . The vector of ones is denoted by e. We let I R n×n + be the set of non-negative matrices. The notation x = Θ(ν) means that there exist constants c 1 , c 2 &gt; 0 independent of ν such that c 1 ν ≤ x ≤ c 2 ν.</p><p>Let p be the rank of Q. By considering the Cholesky factorization of Q, it is readily shown that (1) can be reformulated as a standard semidefinitequadratic-linear program (SQLP) by introducing an additional p linear constraints and p + 1 variables. Unfortunately, the computational cost and memory space needed by a standard primal-dual interior-point method to solve the reformulated problem grow at least like Θ((m + p) 3 ) and Θ((m + p) 2 ), respectively. Thus unless m + p is small, it not viable to solve <ref type="bibr" target="#b0">(1)</ref> by reformulating it into a standard SQLP.</p><p>The problem (1) can also be reformulated as a semidefinite linear complementarity problem (SDLCP) <ref type="bibr" target="#b16">[17]</ref>. However, the computational cost at each interior-point iteration for the SDLCP problem is the same as that for <ref type="bibr" target="#b0">(1)</ref> because both need to solve linear systems with coefficient matrices having the same dimensions and numerical properties. In <ref type="bibr" target="#b16">[17]</ref>, polynomial iteration complexities of some theoretical path-following and potential reduction methods were established. But as far as we know, there is little research on the efficient numerical computation of the solution of <ref type="bibr" target="#b0">(1)</ref> or the SDLCP problem derived from it. In <ref type="bibr" target="#b22">[23]</ref>, a theoretical primal-dual potential reduction algorithm was proposed for (1) and <ref type="bibr" target="#b1">(2)</ref>. At each iteration, the search direction is computed from a dense augmented system of dimension n + m. The authors suggested using the conjugate gradient method to compute an approximate direction. However, as the focus of <ref type="bibr" target="#b22">[23]</ref> was on establishing polynomial iteration complexity, the crucial issue of preconditioning for the conjugate gradient method was not discussed. There was also no numerical implementation to test the proposed method.</p><p>A prime example of QSDP is the nearest correlation matrix (NCM) problem, where given a data matrix K ∈ S n and a self-adjoint linear operator L on S n , we want to solve min X L(X -K) 2  F /2 : diag(X) = e, X 0 .</p><p>(</p><formula xml:id="formula_4">)<label>3</label></formula><p>The QSDP resulting from (3) has Q = L 2 and C = -L 2 (K). Previous research on QSDP were mainly on the NCM problem with the special choice L = U 1/2 U 1/2 (hence Q = U U) for a given U ∈ S n + . One of the earliest works on such a special case for (3) was by Higham <ref type="bibr" target="#b13">[14]</ref> who proposed a modified alternating projection solution method. The paper <ref type="bibr" target="#b13">[14]</ref> also briefly considered the problem (3) with L(X) = •X for a given ∈ S n ∩I R n×n + (correspondingly, Q(X) = U • X, with U = • ). However, no practical solution method was proposed in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Subsequent works on (3) for the special case where L = I (hence Q = I), but extendable to the case L = U 1/2 U 1/2 , include Anjos et al. <ref type="bibr" target="#b3">[4]</ref>, Malick <ref type="bibr" target="#b20">[21]</ref>, and Sun et al. <ref type="bibr" target="#b24">[25]</ref>. The latter two papers used a Lagrangian dual approach that relied critically on the assumption that Q = U U to derive an analytical formula for the projection of I R n×n onto S n + with respect to the norm U 1/2 (•)U 1/2 F . The recent paper <ref type="bibr" target="#b28">[29]</ref> for (1) also focused on the special case Q = U U. It proposed efficient methods for computing the search direction at each interior-point iteration for <ref type="bibr" target="#b0">(1)</ref> by applying an iterative solver with two suitably designed preconditioners to the m × m Schur complement equation. All the previous techniques for solving <ref type="bibr" target="#b0">(1)</ref> with Q = U U, however, do not extend to the case of a general positive semidefinite self-adjoint linear operator Q. The last statement holds true even if Q is a diagonal operator where Q(X) = U • X for some U ∈ S n ∩ I R n×n + . The problem (1) also arises from the nearest Euclidean distance matrix (EDM) problem <ref type="bibr" target="#b0">[1]</ref>:</p><formula xml:id="formula_5">min H • (B -L E (X)) 2 F /2 : (L E (X)) ij = B ij ∀ (ij) ∈ E, X 0 ,<label>(4)</label></formula><p>where B ∈ S n is a given dissimilarity matrix, H is a given weight matrix which typically has the same sparsity pattern as B, E is a given set of indices, and L E (X) = diag(X V )e T + e diag(X V ) T -2X V with X V = VXV T . Here V ∈ I R n× (n-1) is a given matrix with orthonormal columns and V T e = 0. Note that the operator Q associated with (4) is typically not positive definite but positive semidefinite. In <ref type="bibr" target="#b0">[1]</ref>, the EDM problem (4) was solved by a primal-dual interior-point method for which the search direction at each iteration was computed from a linear system of dimension n 2 by a direct method. However, the direct approach in <ref type="bibr" target="#b0">[1]</ref> is computationally viable only for small problems, say with n less than a hundred. We refer the reader to <ref type="bibr" target="#b0">[1]</ref> wherein some other applications of QSDP are cited. More recently, QSDP has also been formulated in local compliance estimation in deformable object modelling <ref type="bibr" target="#b17">[18]</ref>. While the search direction at each interior-point iteration for (1) can be computed from the m × m Schur complement equation when Q = U U <ref type="bibr" target="#b28">[29]</ref>, the corresponding direction for a general QSDP for which Q does not have this special form (even if Q = U V where U = V or Q is a diagonal operator) must be computed from a dense ill-conditioned augmented equation of dimension n + m. This is because reducing the augmented equation to the Schur complement equation is not viable due to the excessive memory (Θ(n 4 ) bytes) and computational cost (Θ(n 6 ) flops) required. Unfortunately, the augmented equation is generally very large, even for a moderate n. Thus it is impossible to solve the augmented equation by a direct method on a personal computer when n is say more than a hundred. The only other alternative is to use a preconditioned Krylov subspace iterative method. But because the augmented and Schur complement equations are quite different in structure, the preconditioning techniques described in <ref type="bibr" target="#b28">[29]</ref> are not applicable to the augmented equation.</p><p>Our inexact primal-dual path-following method for (1) and ( <ref type="formula" target="#formula_1">2</ref>) follows the same framework as the primal-dual path-following method with Mehrotra's predictor-corrector appeared in <ref type="bibr" target="#b28">[29]</ref>. The key difference is in the computation of search directions. Here, the search direction at each iteration is computed from a dense augmented equation of dimension m + n. We apply a preconditioned symmetric quasi-minimal residual (PSQMR) method <ref type="bibr" target="#b10">[11]</ref> to solve the augmented equation.</p><p>In this paper, we are primarily interested in QSDPs for which following assumption holds: Assumption A1. The linear map A is sparse or structured in the sense that if A k denotes A T acting on the kth unit vector of I R m , then the matrices A 1 , . . . , A m are either sparse or low-rank. We assume that m is a moderate number, say less than 5000, so that a matrix of the form AU VA T and its Cholesky factorization can be computed at a moderate cost. The dimension of the primal matrix X is restricted to the range of say less than 2000 so that its full eigenvalue decomposition can be computed at a moderate cost.</p><p>Our contributions in this paper are as follows. We first analyze the asymptotic spectral property of the augmented matrix arising at each interior-point iteration for (1). Then we design three classes of preconditioners for the augmented matrix. Under suitable conditions including nondegeneracy of the optimal solution, the preconditioned matrices are shown to have favorable asymptotic spectral distributions to accelerate the convergence of the PSQMR method used to solve the augmented equation. But note that for one of the classes of preconditioners, no nondegeneracy assumption is needed. We also addressed numerous implementation issues to make our inexact interior-point method for (1) practical.</p><p>Preconditioning for the augmented equations arising from interior-point methods for sparse linearly constrained convex quadratic programming (LC-CQP) problems of the form</p><formula xml:id="formula_6">min x T Qx/2 + c T x : Ax = b, x ∈ I R n + (5)</formula><p>has been studied in <ref type="bibr" target="#b6">[7]</ref>. The preconditioners constructed in <ref type="bibr" target="#b6">[7]</ref> are based on those proposed in <ref type="bibr" target="#b15">[16]</ref>, with the (1, 1) block -X -1 S -Q of the augmented matrix being approximated by -X -1 S-diag(Q). Note that X and S are positive definite diagonal matrices for LCCQP. A reader who is familiar with SDP would realize that the augmented equations in LCCQP and QSDP are different in a fundamental way. For LCCQP, the augmented matrices are sparse if A and Q are sparse, but those in QSDP are typically dense even if A and Q are sparse. Thus the construction of preconditioners for the augmented matrices in QSDP is considerably more difficult and the analysis and computation involved are more complex.</p><p>The problem (1) can be viewed as a generalization of ( <ref type="formula">5</ref>) just as a linear SDP is a generalization of a linear program. We note that when the variable X in ( <ref type="formula" target="#formula_0">1</ref>) is restricted to a diagonal matrix of the form X = diag(x), then (1) reduces to <ref type="bibr" target="#b4">(5)</ref>. Based on this observation, many of the results derived for (1) in this paper can be modified to suit <ref type="bibr" target="#b4">(5)</ref>. However, to keep the paper coherent, we will not separately state the corresponding results for <ref type="bibr" target="#b4">(5)</ref>.</p><p>The paper is organized as follows. In the next section, we derive the augmented equation from which the search direction at each interior-point iteration for (1) is computed. In Sect. 3, the asymptotic spectrum and conditioning of the augmented matrix are analyzed. This motivates the construction of preconditioners for the augmented matrix in Sect. 4. Three classes of preconditioners are constructed, and the asymptotic spectra of the associated preconditioned matrices are analyzed. In Sect. 5, we discuss the construction of symmetrized Kronecker product approximations for self-adjoint linear operators. Section 6 presents numerical experiments to test the performance of our inexact interiorpoint methods that employ iterative solvers with appropriate preconditioners to solve the augmented equation at each iteration. In the last section we give the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Additional notation and terminology</head><p>For a linear map T : (X , •) → (Y, •), where X = I R k×l or S l , and Y = I R p×q or S q , we define T = max{ T (M) F : M F ≤ 1}. The adjoint of T is denoted by T T . The null space is denoted by N (T ). The matrix representation of T with respect to the canonical orthonormal bases of X and Y is denoted by mat(T ). We will typically identify T with mat(T ) and a phrase such as "the matrix T " means the matrix representation of T . Note that T = mat(T ) 2 . Given a self-adjoint linear operator V defined on a finite dimensional inner product space, we let λ j (V) be the jth eigenvalue (sorted in ascending order) and eig(V) be the set of eigenvalues. The largest and smallest eigenvalues in magnitudes are denoted by λ max (V) and λ min (V), respectively. The condition number is denoted by κ(V) := |λ max (V)|/|λ min (V)|. Note that V = |λ max (V)|, and if V is invertible, then V -1 = 1/|λ min (V)|. For two matrices P and Q, [P; Q] denotes the matrix obtained by appending Q to the last row of P. For self-adjoint linear operators S, T :</p><formula xml:id="formula_7">(X , •) → (X , •), the notation S T means that M • S(M) ≤ M • T (M) for all M ∈ X .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Computation of search direction</head><p>Our interior-point method for (1) is a primal-dual path-following method with Mehrotra's predictor-corrector. It is based on the perturbed KKT conditions associated with (1) and ( <ref type="formula" target="#formula_1">2</ref>), which are given by</p><formula xml:id="formula_8">-Q(X) + A T (y) + S = C, A(X) = b, XS = νI, X, S 0,<label>(6)</label></formula><p>where ν &gt; 0 is a parameter that is to be driven to zero explicitly. Let ρ ≥ 0 be a given constant. We may observe that by adding -ρA T A(X) = -ρA T b to the first condition in <ref type="bibr" target="#b5">(6)</ref>, we get an equivalent condition:</p><formula xml:id="formula_9">-Q ρ (X) + A T (y) + S = C ρ ,<label>( 7 )</label></formula><p>where</p><formula xml:id="formula_10">Q ρ := Q + ρA T A and C ρ := C -ρA T b.</formula><p>Thus we can replace the first condition in ( <ref type="formula" target="#formula_8">6</ref>) by <ref type="bibr" target="#b6">(7)</ref>. The motivation for using <ref type="bibr" target="#b6">(7)</ref> is given in Remark 1.</p><p>The framework of our interior-point algorithm for (1) is described in Algorithm IP-QSDP in <ref type="bibr" target="#b28">[29]</ref>. At a given iterate (X, y, S) with X, S 0 (positive definite), the search direction (∆X, ∆y, S) at the current iteration is the solution of the following symmetrized Newton system:</p><formula xml:id="formula_11">-Q ρ (∆X) + A T (∆y) + S = R d = C ρ -S -A T y + Q ρ (X) A(∆X) = R p = b -A(X) F S ∆X + F X S = R c = σ µI -H K (XS),<label>(8)</label></formula><p>where F X and F S are linear operators on S n that depend on the symmetrization scheme H K (•) chosen; for more details, see for example <ref type="bibr" target="#b26">[27]</ref>. Here µ = X • S/n, and σ ∈ (0, 1) is the centering parameter. By eliminating S, we get the following augmented equation of dimension n + m:</p><formula xml:id="formula_12">B ρ ∆X ∆y = R a R p , where B ρ = -K ρ A T A 0 ,<label>( 9 )</label></formula><p>with</p><formula xml:id="formula_13">K ρ = F -1 X F S + Q ρ and R a = R d -F -1 X R c .</formula><p>In this paper, we will consider only the Nesterov-Todd (NT) symmetrization scheme <ref type="bibr" target="#b26">[27]</ref> for which</p><formula xml:id="formula_14">F -1 X F S = W -1 W -1 , where W ∈ S n</formula><p>++ is the unique matrix satisfying WSW = X. But note that the preconditioning strategies we are describing later are also applicable to the purely primal (purely dual) scheme for which</p><formula xml:id="formula_15">F -1 X F S = νX -1 X -1 (S Sν).</formula><p>By further eliminating ∆X from (9), we get the Schur complement equation of dimension m below:</p><formula xml:id="formula_16">M ρ ∆y = R p + AK -1 ρ R a , where M ρ = AK -1 ρ A T . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>When Q has the special form U U and ρ = 0, the Schur complement matrix M ρ can be computed at a cost of at most 4mn 3 + m 2 n 2 floating point operations (flops); see <ref type="bibr" target="#b28">[29]</ref>. This is done by exploiting the fact that K -1 ρ has an analytical expression and K -1 ρ (V) can be computed with Θ(n 3 ) flops for any given V ∈ S n . However, for a general Q, K -1 ρ does not have an analytical expression and the computation of M ρ requires the inversion of K ρ (with dimension n) that costs Θ(n 6 ) flops. Such a cost is prohibitively expensive when n is larger than a hundred. Thus for a general Q, computing the search direction via <ref type="bibr" target="#b9">(10)</ref> is not a viable approach. The practical alternative is to use the augmented equation <ref type="bibr" target="#b8">(9)</ref>.</p><p>However, the linear system <ref type="bibr" target="#b8">(9)</ref>, with dimension n + m, is a large system even for a moderate n. In addition, it is typically dense since F -1 X F S is typically so. Thus unless n + m is of moderate size, it is impossible to solve (9) by a direct method, and iterative methods are the only alternatives. In this paper, we use the PSQMR method as the iterative solver for <ref type="bibr" target="#b8">(9)</ref>. The unpreconditioned PSQMR method is mathematically equivalent to the well known minimal residual (MINRES) method <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. It has the same work and storage requirements as MINRES and usually converges in about the same number of iterations. But the PSQMR method has the advantage that it allows the use symmetric indefinite preconditioners whereas MINRES allows only symmetric positive definite preconditioners.</p><p>As iterative methods do not solve a linear system exactly (modulo rounding errors), we need the following result to gauge the quality of the computed direction.</p><p>Lemma 1 Suppose the residual in computed direction (∆X, ∆y) for (9) is given by (η 1 , η 2 ), and that S is computed exactly from the first equation in <ref type="bibr" target="#b7">(8)</ref> based on the computed (∆X, ∆y). Then the residual of the direction (∆X, ∆y, S) with respect to <ref type="bibr" target="#b7">(8)</ref> is given by (0 , η 2 , -F X (η 1 )).</p><p>Proof We omit the proof since it is straightforward.</p><p>In the numerical experiments in Sect. 6, we deem a direction (∆X, ∆y) computed by an iterative solver from <ref type="bibr" target="#b8">(9)</ref> to be sufficiently accurate if</p><formula xml:id="formula_18">max{ η 2 2 , F X (η 1 ) F } ≤ 0.01 max{ R d F , R p 2 , R c F }.<label>(11)</label></formula><p>Observe that the above criterion required the symmetrized Newton system (8) be solved with a relative accuracy of 0.01. While we do not investigate the question of whether such a criterion is sufficient for establishing a globally polynomial convergent interior-point algorithm, we note that it is motivated by a similar criterion used in the globally polynomial convergent inexact interiorpoint algorithms for SDP in <ref type="bibr" target="#b31">[32]</ref>. We end this section by stating some basic facts concerning a 2 × 2 block matrix of the form G = [-U, V T ; V, W]. The inversion of such a matrix is at the heart of the computation of search directions. When the (1,1) block U and the associated Schur complement matrix Y := W + VU -1 V T are invertible, the inverse of G can be computed from the following analytical expression whose proof can be found for example in <ref type="bibr">[26, p. 389</ref>]:</p><formula xml:id="formula_19">G -1 = -U -1 + U -1 V T Y -1 VU -1 U -1 V T Y -1 Y -1 VU -1 Y -1 . (<label>12</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">Lemma 2 Suppose U ∈ S p ++ , W ∈ S q + , and V ∈ I R q×p has full row rank. Let G = [-U, V T ; V, W] and Y = W + VU -1 V T . The following results hold: max{ U , V , W } ≤ G ≤ 2 max{ U , V , W }, Y -1 ≤ G -1 ≤ 2 max{ U -1 , Y -1 }.</formula><p>Proof We shall prove only the second inequality since the first follows from the inequality, G 2 ≤ U 2 + 2 V 2 + W 2 , proven in <ref type="bibr" target="#b8">[9]</ref>. It is readily shown that the (1,1) block of G -1 can be expressed as</p><formula xml:id="formula_22">U -1/2 ( -I)U -1/2 , where = U -1/2 V T Y -1 VU -1/2</formula><p>. By noting that 0 I, the required result can be proven easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conditioning of the augmented matrix B k ρ</head><p>We made the following assumptions on (1) and ( <ref type="formula" target="#formula_1">2</ref>). Assumption A2. The problems (1) and ( <ref type="formula" target="#formula_1">2</ref>) are strictly feasible and that A is surjective. Note that the last condition implies that m ≤ n.</p><p>Assumption A2 stated the necessary and sufficient conditions for the existence and uniqueness of solutions (X ν , y ν , S ν ) of the central path equations <ref type="bibr" target="#b5">(6)</ref>. Also, these solutions converge to some optimal solution (X * , y * , S * ) as ν ↓ 0; see <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b19">[20]</ref>. We further assume that (X * , y * , S * ) satisfies the following assumption.</p><p>Assumption A3. Strict complementarity holds for (X * , y * , S * ) in the sense defined in <ref type="bibr" target="#b1">[2]</ref>. Thus the ranks of X * and S * sum to n.</p><p>Suppose {ν k } is a monotonically decreasing sequence with lim k→∞ ν k = 0. Let the coefficient matrices in ( <ref type="formula" target="#formula_12">9</ref>) and <ref type="bibr" target="#b9">(10)</ref> corresponding to (X ν , y ν , S ν ) be B ν ρ and M ν ρ , respectively. For simplicity of notation, we write</p><formula xml:id="formula_23">B k ρ , M k ρ , X k , S k , etc., for B ν k ρ , M ν k ρ , X ν k , S ν k ,</formula><p>and so on. Since X k and S k commute, there is an orthogonal matrix P k that simultaneously diagonalizes X k and S k so that</p><formula xml:id="formula_24">X k = P k Λ k (P k ) T , S k = P k k (P k ) T ,</formula><p>where the eigenvalue matrices</p><formula xml:id="formula_25">Λ k = diag(λ k 1 , . . . , λ k n ), k = diag(σ k 1 , . . . , σ k n ) satisfy λ k i σ k i = ν k</formula><p>, and the eigenvalues are ordered such that</p><formula xml:id="formula_26">λ k 1 ≥ • • • ≥ λ k n &gt; 0, 0 &lt; σ k 1 ≤ • • • ≤ σ k n .</formula><p>Let P * be a limit point of the set {P k }. We refine the sequence if necessary so that {P k } converges to P * . Then P * is an orthogonal matrix that simultaneously diagonalizes X * and S * with</p><formula xml:id="formula_27">X * = P * Λ * (P * ) T , S * = P * * (P * ) T , (<label>13</label></formula><formula xml:id="formula_28">)</formula><p>where</p><formula xml:id="formula_29">Λ * = diag(λ * 1 , . . . , λ * n ), * = diag(σ * 1 , . . . , σ * n ) satisfy λ * i σ * i = 0, and λ * 1 ≥ . . . λ * r &gt; λ * r+1 = • • • = λ * n = 0, 0 = σ * 1 = • • • = σ * n-s &lt; σ * n-s+1 ≤ . . . σ * n .</formula><p>Here r and s are the ranks of X * and S * , respectively. We are assuming that (X * , y * , S * ) satisfies the strict complementarity condition, i.e., r + s = n. To analyze the spectrum of B k ρ , we will identify the space S n with the space S r × R r×s × S s as follows. For an element X ∈ S n , consider the partition X = [X 1 , X 2 ; X T 2 , X 3 ], where X 1 ∈ S r , X 2 ∈ R r×s , and</p><formula xml:id="formula_30">X 3 ∈ S s . Then X is identified with the element [X 1 ; X 2 ; X 3 ] in S r × R r×s × S s . The notation [X 1 ; X 2 ; X 3 ] means that the objects X 1 , X 2 , X 3 are placed in a column format. The space S r × R r×s × S s is endowed with the inner product [X 1 ; X 2 ; X 3 ] • [Y 1 ; Y 2 ; Y 3 ] = X 1 • Y 1 + 2X 2 • Y 2 + X 3 • Y 3 so that the identi- fication of S n with S r × R r×s × S s is an isometry. Note the factor of 2 in front of the inner product X 2 • Y 2 . Thus the space R r×s is I R r×s but with the inner product for X 2 , Y 2 ∈ R r×s given by 2X 2 • Y 2 .</formula><p>Let P * 1 and P * 2 be the submatrices denoting the first r and the last n-r columns of P * , respectively. Let P * = P * P * . Based on the identification of S n with S r × R r×s × S s , P * can be partitioned as follows. For X = [X 1 , X 2 ; X T 2 , X 3 ], we have</p><formula xml:id="formula_31">P * (X) = P * 1 (X 1 ) + P * 2 (X 2 ) + P * 3 (X 3 ), (<label>14</label></formula><formula xml:id="formula_32">)</formula><p>where</p><formula xml:id="formula_33">P * 1 = P * 1 P * 1 : S r → S n , P * 2 = 2P * 1 P * 2 :</formula><p>R r×s → S n , and</p><formula xml:id="formula_34">P * 3 = P * 2 P * 2 : S s → S n . We may write P * = [P * 1 , P * 2 , P * 3 ] so that P * (X) = [P * 1 , P * 2 , P * 3 ][X 1 ; X 2 ; X 3 ]. Note that (P * 2 ) T = (P * 2 ) T ⊗ (P * 1 ) T .</formula><p>Similarly, we let</p><formula xml:id="formula_35">A * := AP * = [ A * 1 , A * 2 , A * 3 ] with A * j = AP * j for j = 1, 2, 3, and let Q * ρ = (P * ) T Q ρ P * with ( Q * ρ ) ij = (P * i ) T Q ρ P * j for i, j = 1, 2, 3. We define Q * and ( Q * ) ij similarly.</formula><p>Based on the partitions of P * and A * , we can define the degeneracy of the optimal solution (X * , y * , S * ) as follows.</p><p>Definition 1 Suppose the optimal solution (X * , y * , S * ) satisfies the strict complementarity condition.</p><p>(a) The solution X * is said to be primal nondegenerate <ref type="bibr" target="#b1">[2]</ref> if the linear map</p><formula xml:id="formula_36">[ A * 1 , A * 2 ] : S r × R r×s → I R m defined by [ A * 1 , A * 2 ][U; V] = A(P * 1 U(P * 1 ) T ) + A(P * 1 V(P * 2 ) T + P * 2 V T (P * 1 ) T ) is surjective (this is an equivalent definition: see Theorem 6 in [2]). (b)</formula><p>The solution S * is said to be dual nondegenerate <ref type="bibr" target="#b1">[2]</ref> if the linear map</p><formula xml:id="formula_37">A * 1 : S r → I R m defined by A * 1 (U) = A(P * 1 U(P * 1 ) T ) is injective (this is again an equivalent definition: see Theorem 9 in [2]).</formula><p>The importance of nondegeneracy of the optimal solution will become clear when we analyze the asymptotic spectra of {B k ρ } and those of the preconditioned matrices later.</p><p>Note that for the QSDP arising from (3), the optimal solution (X * , y * , S * ) is always primal nondegenerate; see <ref type="bibr" target="#b28">[29]</ref>.</p><p>On the central path {(X ν , y ν , S ν ) : ν &gt; 0}, and under Assumptions A2 and A3, the eigenvalue decomposition of (W k ) -1 , where W k is the NT scaling matrix corresponding to (X k , y k , S k ), must have the following form:</p><formula xml:id="formula_38">(W k ) -1 = P k D k (P k ) T = P k 1 D k 1 (P k 1 ) T + P k 2 D k 2 (P k 2 ) T , (<label>15</label></formula><formula xml:id="formula_39">)</formula><p>where</p><formula xml:id="formula_40">D k 1 = diag(d k 1 ) ∈ I R r×r , P k 1 ∈ I R n×r correspond to the small eigenvalues of order Θ( √ ν k ), and D k 2 = diag(d k 2 ) ∈ I R s×s , P k 2 ∈ I R n×s correspond to the large eigenvalues of the order Θ(1/ √ ν k ). Recall that the notation γ = Θ( √ ν k ) means that there are constants c 1 , c 2 &gt; 0 such that c 1 √ ν k ≤ γ ≤ c 2 √ ν k for all k.</formula><p>It is easily shown that the following eigenvalue decomposition holds <ref type="bibr" target="#b26">[27]</ref>:</p><formula xml:id="formula_41">(W k ) -1 (W k ) -1 = (P k P k )(D k D k )(P k P k ) T = P k D k (P k ) T , (<label>16</label></formula><formula xml:id="formula_42">)</formula><p>where</p><formula xml:id="formula_43">P k = P k P k and D k = D k D k . Let D k 1 = D k 1 D k 1 , D k 2 = D k 2 ⊗ D k 1 and D k 3 = D k 2 D k 2 .</formula><p>Then the diagonal entries of the matrix representations of D k 1 , D k 2 , and D k 3 consist of r, rs, and s eigenvalues of <ref type="bibr" target="#b0">(1)</ref>, and Θ(1/ν k ), respectively. We assume that there are constants τ , τ &gt; 0 such that τ I D k 2 τ I for all k. Without loss of generality, we may choose τ large enough and τ small enough so that 0</p><formula xml:id="formula_44">(W k ) -1 (W k ) -1 of the orders Θ(ν k ), Θ</formula><formula xml:id="formula_45">D k 1 τ I and D k 3 τ I for all k. Note that we have D k = diag(D k 1 , D k 2 , D k 3 ). Consider the partition P k = [P k 1 , P k 2 , P k 3 ] with P k 1 = P k 1 P k 1 , P k 2 = 2P k 1 P k 2 and P k 3 = P k 2 P k 2 corresponding to D k 1 , D k 2 , and D k 3 , respectively. Let A k = AP k , with the partition A k = [ A k 1 , A k 2 , A k 3 ] = [AP k 1 , AP k 2 , AP k 3 ] conforming to that of diag(D k 1 , D k 2 , D k 3 ). Suppose Q k = (P k ) T QP k . Then Q k ρ := (P k ) T Q ρ P k = Q k + ρ( A k ) T A k . Let ( Q k ρ ) ij = (P k i ) T Q ρ P k j for i, j = 1, 2, 3. We define Q k ij simi- larly. Note that since P k → P * as k ↑ ∞, we have A k → A * and Q k ρ → Q * ρ as k ↑ ∞.</formula><p>Based on the eigenvalue decomposition <ref type="bibr" target="#b15">(16)</ref>, it is readily shown that the matrix B k ρ in (9) has the following decomposition:</p><formula xml:id="formula_46">B k ρ = P k 0 0 I m B k ρ (P k ) T 0 0 I m , where B k ρ = -D k D k -Q k ρ ( A k ) T A k 0 .</formula><p>For later usage, note that we have the following partition:</p><formula xml:id="formula_47">D k D k + Q k ρ = ⎡ ⎢ ⎢ ⎣ D k 1 + ( Q k ρ ) 11 ( Q k ρ ) 12 ( Q k ρ ) 13 ( Q k ρ ) 21 D k 2 + ( Q k ρ ) 22 ( Q k ρ ) 23 ( Q k ρ ) 31 ( Q k ρ ) 32 D k 3 + ( Q k ρ ) 33 ⎤ ⎥ ⎥ ⎦ . (<label>17</label></formula><formula xml:id="formula_48">)</formula><p>We note that it is possible to have the case where r = n and s = 0. In that case, D k 2 is a null matrix for k sufficiently large, and correspondingly</p><formula xml:id="formula_49">D k 2 , D k 3 and P k 2 , P k 3 are null maps. Similarly, ( Q k ρ ) ij , i, j = 1, 2, 3, are null maps except for ( Q k ρ ) 11 .</formula><p>As we shall see from the following lemma, the norm B k ρ behaves differently for r = n and r &lt; n. Lemma 3 Let r and s be the ranks of X * and S * , respectively.</p><p>(a) Suppose r = n and s = 0.</p><formula xml:id="formula_50">Then lim sup k→∞ B k ρ ≤ 2 max{ A , Q ρ }. (b) Suppose r &lt; n and s = n -r &gt; 0. Then B k ρ ≥ max{ Q ρ , D k 3 } = Θ(1/ν k ). Hence B k ρ → ∞ as k ↑ ∞.</formula><p>Proof (a) In this case, D k = Θ(ν k ), and the required result follows from Lemma 2. (b) By Lemma 2 and the fact that D k 3 = Θ(1/ν k ), the required result follows.</p><p>We state the following lemma before analyzing the conditioning of B k ρ . Lemma 4 Suppose that Assumptions A2 and A3 hold for the optimal solution (X * , y * , S * ), and ( Q * ρ ) 11 0. (a) Then we have</p><formula xml:id="formula_51">Υ * ρ := ⎡ ⎣ ( Q * ρ ) 11 ( Q * ρ ) 12 ( Q * ρ ) 21 τ I + ( Q * ρ ) 22 ⎤ ⎦ 0. (<label>18</label></formula><formula xml:id="formula_52">) (b) Let M * ρ = [ A * 1 , A * 2 ](Υ * ρ ) -1 [ A * 1 , A * 2 ] T . (<label>19</label></formula><formula xml:id="formula_53">)</formula><p>Suppose further that (X * , y * , S * ) is primal nondegenerate. Then M * ρ 0.</p><p>Proof (a) Since the sub-matrix of maps, <ref type="bibr" target="#b11">12</ref> τ I. By Theorem 7.7.6 in <ref type="bibr" target="#b14">[15]</ref>, Υ * ρ in ( <ref type="formula" target="#formula_51">18</ref>) is positive definite. (b) Under the assumption that X * is primal nondegenerate, the linear map <ref type="bibr" target="#b10">11</ref> to be positive definite compared to Q * 11 , this explains why we used <ref type="bibr" target="#b6">(7)</ref> instead of the first condition in <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_54">[( Q * ρ ) 11 , ( Q * ρ ) 12 ; ( Q * ρ ) 21 , ( Q * ρ ) 22 ], of Q ρ is positive semidefinite, we have τ I + ( Q * ρ ) 22 -( Q * ρ ) 21 ( Q * ρ ) -1 11 ( Q * ρ )</formula><formula xml:id="formula_55">[ A * 1 , A * 2 ] is surjective. From here, it is easy to show that M * ρ 0. Remark 1 (a) If Q * 11 is positive definite on N ( √ ρ A * 1 ) = N (ρ( A * 1 ) T A * 1 ), then ( Q * ρ ) 11 0. This result follows from the fact that if U, V ∈ S p + are such that U is positive definite on N (V), then U + V 0. Since it is easier for ( Q * ρ )</formula><formula xml:id="formula_56">(b) If either ρ &gt; 0 and (X * , y * , S * ) is dual nondegenerate, or Q * 11 is positive definite, then ( Q * ρ ) 11 0. Note that if Q 0, then Q * 11 0 since eig( Q * 11 ) ⊂ eig( Q * ) = eig(Q). Let Υ k ρ = D k 1 + ( Q k ρ ) 11 ( Q k ρ ) 12 ( Q k ρ ) 21 D k 2 + ( Q k ρ ) 22 , Υ k ρ = ( Q k ρ ) 11 ( Q k ρ ) 12 ( Q k ρ ) 21 τ I + ( Q k ρ ) 22 .<label>(20)</label></formula><p>Note that Υ k ρ is positive definite, and</p><formula xml:id="formula_57">Υ k ρ Υ k ρ τ I + Υ k ρ . (<label>21</label></formula><formula xml:id="formula_58">)</formula><p>Recall that M k ρ denotes the Schur complement matrix in <ref type="bibr" target="#b9">(10)</ref> corresponding to (X k , y k , S k ). We have the following three theorems concerning</p><formula xml:id="formula_59">{ M k ρ }, { (M k ρ ) -1 }, and { (B k ρ ) -1 }.</formula><p>Theorem 1 Suppose that Assumptions A2 and A3 hold for the optimal solution (X * , y * , S * ), and ( Q * ρ ) 11 0. Then the following results hold.</p><p>(a) There exists c &gt; 0 such that</p><formula xml:id="formula_60">lim sup k→∞ M k ρ ≤ c M * ρ . (b) Suppose further that (X * , y * , S * ) is primal nondegenerate. Then there exists c &gt; 0 such that lim sup k→∞ (M k ρ ) -1 ≤ c (M * ρ ) -1 . Proof Let J k ρ = Υ k ρ -( Q k ρ ) 13 ; ( Q k ρ ) 23 D k 3 + ( Q k ρ ) 33 -1 ( Q k ρ ) 31 ( Q k ρ ) 32 . (<label>22</label></formula><formula xml:id="formula_61">)</formula><p>Note that the second term on the right-hand side converges to 0 as k ↑ ∞, and Υ k ρ satisfies <ref type="bibr" target="#b20">(21)</ref>. Since Υ k ρ in <ref type="bibr" target="#b20">(21)</ref> converges to Υ * ρ in <ref type="bibr" target="#b17">(18)</ref> as k ↑ ∞, and Υ * ρ 0 by Lemma 4(a), thus there exist c 1 ,</p><formula xml:id="formula_62">c 2 &gt; 0 such that c -1 1 Υ * ρ J k ρ c -1 2 Υ * ρ for sufficiently large k. This implies that c 1 (Υ * ρ ) -1 (J k ρ ) -1 c 2 (Υ * ρ ) -1 for all k sufficiently large. (a) Observe that M k ρ = A k (D k D k + Q k ρ ) -1 ( A k ) T .</formula><p>By applying the formula in <ref type="bibr" target="#b11">(12)</ref> to the matrix in <ref type="bibr" target="#b16">(17)</ref>, we have</p><formula xml:id="formula_63">(D k D k + Q k ρ ) -1 = diag (J k ρ ) -1 , 0 + O(ν k (J k ρ ) -1 ).</formula><p>Thus, for k sufficiently large,</p><formula xml:id="formula_64">M k ρ = [ A k 1 , A k 2 ](J k ρ ) -1 [ A k 1 , A k 2 ] T + O(ν k (J k ρ ) -1 A 2 ) c 1 [ A k 1 , A k 2 ](Υ * ρ ) -1 [ A k 1 , A k 2 ] T + O(ν k (Υ * ρ ) -1 A 2 ). (<label>23</label></formula><formula xml:id="formula_65">)</formula><p>Since the first term on the right-hand side converges to M * ρ in <ref type="bibr" target="#b18">(19)</ref>, the required result follows.</p><p>(b) From ( <ref type="formula" target="#formula_64">23</ref>), we have that for k sufficiently large,</p><formula xml:id="formula_66">M k ρ c 2 [ A k 1 , A k 2 ](Υ * ρ ) -1 [ A k 1 , A k 2 ] T + O(ν k (Υ * ρ ) -1 A 2 ).</formula><p>Since the first term on the right-hand side converges to M * ρ , and M * ρ 0 by Lemma 4(b), thus lim inf k→∞ λ min (M k ρ ) ≥ c 2 λ min (M * ρ ) &gt; 0. From here, the required result follows.</p><p>Theorem 2 Under the assumptions stated in Theorem 1(b), there exists c &gt; 0 such that</p><formula xml:id="formula_67">lim sup k→∞ (B k ρ ) -1 ≤ c max{ (Υ * ρ ) -1 , (M * ρ ) -1 }. (<label>24</label></formula><formula xml:id="formula_68">)</formula><p>Proof By Lemma 2, we get</p><formula xml:id="formula_69">(B k ρ ) -1 ≤ 2 max{ (K k ρ ) -1 , (M k ρ ) -1 }. Now (P k ) T (K k ρ ) -1 P k = (D k D k + Q k ρ ) -1 = diag (J k ρ ) -1 , 0 + O(ν k (J k ρ ) -1 ),</formula><p>where J k ρ is defined in <ref type="bibr" target="#b21">(22)</ref>, and there exists c 1 &gt; 0 such that for k sufficiently large,</p><formula xml:id="formula_70">c 1 (Υ * ρ ) -1 (J k ρ ) -1 . Thus (K k ρ ) -1 = (J k ρ ) -1 (1 + O(ν k )) ≤ c 1 (Υ * ρ ) -1 (1 + O(ν k )), implying that lim sup k→∞ (K k ρ ) -1 ≤ c 1 (Υ * ρ ) -1 . By Theorem 1(b), we have lim sup k→∞ (M k ρ ) -1 ≤ c 2 (M * ρ ) -1</formula><p>for some constant c 2 &gt; 0. From here, the required result in <ref type="bibr" target="#b23">(24)</ref> follows.</p><p>Theorem 3 Suppose that Assumptions A2 and A3 hold for the optimal solution (X * , y * , S * ) and</p><formula xml:id="formula_71">Q ρ 0. If S * = 0, then lim sup k→∞ M k ρ ≤ AQ -1 ρ A T (25) lim sup k→∞ (M k ρ ) -1 ≤ (AQ -1 ρ A T ) -1 (26) lim sup k→∞ (B k ρ ) -1 ≤ 2 max{ Q -1 ρ , (AQ -1 ρ A T ) -1 }. (<label>27</label></formula><formula xml:id="formula_72">)</formula><p>Proof Under the assumption of strict complementarity, S * = 0 implies that X * has full rank. In this case,</p><formula xml:id="formula_73">D k = Θ(ν k ). Now, M k ρ = A k (D k + Q k ρ ) -1 ( A k ) T = AQ -1 ρ A T + O(ν k )</formula><p>, thus the result in <ref type="bibr" target="#b24">(25)</ref> follows. Since AQ -1 ρ A T is nonsingular, the inequality in <ref type="bibr" target="#b25">(26)</ref> follows readily. The result in <ref type="bibr" target="#b26">(27)</ref> can be proven similarly.</p><p>Example 1 To illustrate the asymptotic result in Theorem 2 and later results on the spectra of preconditioned matrices, we consider an example of (1) arising from the NCM problem (3) with n = 30. The matrix K in (3) is generated in a similar way as Higham did in <ref type="bibr" target="#b13">[14]</ref> from the Matlab segment: x=10. ∧ [-4 : 4/(n -1) : 0]; B=gallery('randcorr',n*x/sum(x)). We take K =B+1e-4*E, where E is a random symmetric matrix with E F = 1. The linear operator Q associated with ( <ref type="formula" target="#formula_4">3</ref>) is chosen to be the positive definite operator Q(X) = U • X, where U is randomly generated as follows: tmp=rand(n); U=(tmp+tmp')/2.</p><p>Note that for all the numerical examples in this paper, we take ρ = 0. The spectra of B k ρ and M k ρ corresponding to the iterate (X k , y k , S k ) with complementarity gap X k • S k /n = 1.9 × 10 -10 are contained in the intervals [-1.62, -0.01] ∪ [0.62, 0.98] and [1.00, 20.25], respectively. For this example, X * has full rank and S * = 0. It is clear that B k ρ , (B k ρ ) -1 , M k ρ , and (M k ρ ) -1 remain bounded as k ↑ ∞. This observation is consistent with the asymptotic results in Lemma 3 and Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples 2 and 3</head><p>The last example is not very interesting since B k ρ remains well conditioned as k ↑ ∞. We now consider Examples 2 and 3, which are similar to Example 1 but with K taken to be B + E and B + 1e2 * E, respectively. Observe that we make the perturbation to the random correlation matrix B larger than 1e-4*E considered in <ref type="bibr" target="#b13">[14]</ref> so as to generate QSDPs with augmented matrices B k ρ that become increasing ill-conditioned as k increases. Note that the optimal solution of a typical random instance of Example 2 is primal nondegenerate but dual degenerate, whereas that of Example 3 is both primal and dual nondegenerate.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the spectra of B k ρ corresponding to the final iterates for 10 random instances of Examples 2 and 3. From the spectra shown in plots, it is clear that B k ρ will tend to ∞ as k ↑ ∞. On the other hand, (B k ρ ) -1 remains bounded as k ↑ ∞. This observation is consistent with Theorem 2. Note that for all the problem instances, the complementarity gaps of the final iterates are less than 10 -8 and they are primal and dual feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preconditioners for the augmented matrix B k ρ</head><p>The convergence rate of a Krylov iterative method such as MINRES applied to the matrix B k ρ depends in a complicated manner on the eigenvalue distribution of the matrix; see <ref type="bibr" target="#b30">[31]</ref>. Even though there is no accurate estimate of the convergence rate based on the condition number κ(B k ρ ) like the case of a symmetric positive definite matrix, the condition number is generally still a reasonable measure of the convergence rate-the smaller the condition number is, the faster the convergence. Our purpose now is to design preconditioners for B k ρ so that the condition numbers of the preconditioned matrices are bounded independent of ν k .</p><p>We have shown in the previous section that {κ(B k ρ )} ∞ k=1 is unbounded except when the optimal solution is primal nondegenerate, and S * = 0 and rank</p><formula xml:id="formula_74">(X * ) = n. But note that even if {κ(B k ρ )} ∞ k=1 is bounded, κ(B k ρ ) can still be large. Thus preconditioning for B k ρ is generally needed. Let H k 1 : S r → S r , H k 2 :</formula><p>R r×s → R r×s , and H k 3 : S s → S s be given self-adjoint linear operators such that H k j is a positive definite approximation of D k j +( Q k ρ ) jj , j = 1, 2, 3. We will discuss specific choices of H k 1 , H k 2 , H k 3 later when needed. For later usage, we define the linear operator E k : S n → S n :</p><formula xml:id="formula_75">E k (U) = (H k 1 ) -1 (U 1 ) (H k 2 ) -1 (U 2 ) ((H k 2 ) -1 (U 2 )) T (H k 3 ) -1 (U 3 )</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Part I</head><p>In this subsection, we assume that H k 1 , H k 2 , H k 3 satisfy the conditions below for all k:</p><formula xml:id="formula_76">σ 1 I H k 1 σ 1 I, σ 2 I H k 2 σ 2 I, D k 3 H k 3 D k 3 + σ 3 I, (<label>28</label></formula><formula xml:id="formula_77">)</formula><p>for some positive constants σ 1 , σ 1 , σ 2 , σ 2 , and σ 3 . Let</p><formula xml:id="formula_78">N k ρ = diag(H k 1 , H k 2 ) -1/2 Υ k ρ diag(H k 1 , H k 2 ) -1/2<label>(29)</label></formula><formula xml:id="formula_79">θ k = min{1, λ min (N k ρ )}, θ k = max{1, λ max (N k ρ )}, (<label>30</label></formula><formula xml:id="formula_80">)</formula><p>where Υ k ρ is defined in <ref type="bibr" target="#b19">(20)</ref>. We have the following results for {θ k } and {θ k }.</p><p>Lemma 5 Suppose that Assumptions A2 and A3 hold for the optimal solution (X * , y * , S * ), and</p><formula xml:id="formula_81">( Q * ρ ) 11 0. Then lim inf k→∞ θ k ≥ min 1, min(σ -1 1 , σ -1 2 )λ min (Υ * ρ ) &gt; 0,<label>(31)</label></formula><formula xml:id="formula_82">lim sup k→∞ θ k ≤ max 1, max(σ -1 1 , σ -1 2 )λ max τ I + Υ * ρ &lt; ∞.<label>(32)</label></formula><p>Consequently, there exist constants c 1 , c 2 &gt; 0 such that for k sufficiently large,</p><formula xml:id="formula_83">c 1 &lt; θ k ≤ θ k &lt; c 2 .</formula><p>Proof By noting that σ j I H k j σ j I, j = 1, 2, and τ I D k 2 τ I for all k,</p><formula xml:id="formula_84">we have N k ρ min(σ -1 1 , σ -1 2 ) Υ k ρ ,</formula><p>where Υ k ρ is defined in <ref type="bibr" target="#b19">(20)</ref>, and it converges to the matrix Υ * ρ in <ref type="bibr" target="#b17">(18)</ref> as k ↑ ∞. Since Υ * ρ 0 by Lemma 4(a), <ref type="bibr" target="#b30">(31)</ref> follows. On the other hand, we have</p><formula xml:id="formula_85">N k ρ max(σ -1 1 , σ -1 2 )(τ I + Υ k ρ ).</formula><p>From here, the inequality in <ref type="bibr" target="#b31">(32)</ref> follows easily.</p><p>The first preconditioner we consider is the following block diagonal matrix whose partition conforms to that of B k ρ :</p><formula xml:id="formula_86">k ρ,± = ±P k diag(H k 1 , H k 2 , H k 3 )(P k ) T 0 0 M k ρ , (<label>33</label></formula><formula xml:id="formula_87">)</formula><p>where M k ρ is a symmetric positive definite approximation of the matrix M k ρ in <ref type="bibr" target="#b9">(10)</ref>. An example would be M k ρ = I m . We assume that M k ρ satisfies the following condition: αI M k ρ αI for all k for some constants α, α &gt; 0. The motivation for assuming H k 3 D k 3 in (28) for the (1,1) block of k ρ,± is to obliterate the effect of large eigenvalues of D k on the conditioning of B k ρ . Note that for k ρ,± , the assumption that m is a moderate number in Assumption A1 is not needed. This is in contrast to the preconditioners we are considering in the next two parts. Given [X; y] ∈ S n × I R m , ( k ρ,± ) -1 [X; y] can be evaluated efficiently through the steps given below:</p><formula xml:id="formula_88">Compute X = (P k ) T XP k Compute ( k ρ,± ) -1 [X; y] = [±P k (E k ( X))(P k ) T ; ( M k ρ ) -1 y].</formula><p>The above computation requires four n × n matrix-matrix multiplications that cost 4n 3 flops; see <ref type="bibr" target="#b21">[22]</ref>.</p><p>Block diagonal preconditioners for sparse 2 × 2 block symmetric indefinite systems have been studied extensively. We shall not give the literature review here but refer the reader to the recent survey paper <ref type="bibr" target="#b4">[5]</ref>. As far as we know, this paper is the first attempt to apply a block diagonal preconditioner to a dense 2 × 2 block symmetric indefinite system.</p><p>It is well known that the convergence rate of a preconditioned Krylov iterative method such as PSQMR is determined primarily by the spectral distribution and condition number of the preconditioned matrix. Thus it is of great interest to estimate its spectrum. We will first establish an asymptotic result for the spectrum of</p><formula xml:id="formula_89">( k ρ,+ ) -1 B k ρ .</formula><p>Theorem 4 (a) Suppose that Assumptions A2 and A3 hold. Let r = rank(X * ) and s = n-r. Then the preconditioned matrix</p><formula xml:id="formula_90">( k ρ,+ ) -1 B k ρ has s eigenvalues equal to -1 + O( √ ν k max{ Q ρ , A }), the remaining eigenvalues are O( √ ν k max { Q ρ , A })</formula><p>perturbations of those of the following matrix:</p><formula xml:id="formula_91">G k ρ,+ := -N k ρ (L k ) T L k 0 , where L k = ( M k ρ ) -1/2 [ A k 1 , A k 2 ]diag(H k 1 , H k 2 ) -1/2</formula><p>and N k ρ is defined as in <ref type="bibr" target="#b28">(29)</ref>.</p><formula xml:id="formula_92">Let M k ρ = [ A k 1 , A k 2 ](Υ k ρ ) -1 [ A k 1 , A k 2 ]</formula><p>T , and</p><formula xml:id="formula_93">σ k j = 1 + 4λ j (( M k ρ ) -1 M k ρ ) 1/2 , j = 1, . . . , m. Let σ k max = max j {σ k j } and σ k min = min j {σ k j }.</formula><p>The spectrum of G k ρ,+ is contained in the union of the following intervals:</p><formula xml:id="formula_94">- θ k 2 (σ k max + 1), - θ k 2 (σ k min + 1) , -θ k , -θ k , θ k 2 (σ k min -1), θ k 2 (σ k max -1) .</formula><p>(b) Under the assumptions stated in Theorem 1(b), there exist constants c 1 , c 2 , c 3 , c 4 such that for k sufficiently large,</p><formula xml:id="formula_95">0 &lt; c 1 &lt; θ k ≤ θ k &lt; c 2 , and 1 &lt; c 3 &lt; σ k min ≤ σ k max &lt; c 4 . As a result, eig(G k ρ,+ ) ⊂ [-c 2 (c 4 + 1), -c 1 (c 3 + 1)] ∪ [-c 2 , -c 1 ] ∪ [c 1 (c 3 -1), c 2 (c 4 -1)].</formula><p>Proof For simplicity, we drop the superscript k in this proof. Note that the spectrum of -1 ρ,+ B ρ is the same as that of</p><formula xml:id="formula_96">-1/2 ρ,+ B ρ -1/2 ρ,+ . (a) It is readily shown that up to a perturbation of O( √ ν max{ Q ρ , A }), -1/2 ρ,+ B ρ -1/2</formula><p>ρ,+ is orthogonally similar to diag(G ρ,+ , -I s). Thus the eigenvalues of</p><formula xml:id="formula_97">-1/2 ρ,+ B ρ -1/2 ρ,+ are O( √ ν max{ Q ρ , A }) perturbations of those of diag</formula><p>(G ρ,+ , -I s), whose eigenvalues are either -1 or those of G ρ,+ . Now, we can write G ρ,+ = VUV T , where</p><formula xml:id="formula_98">V = diag diag(H 1 , H 2 ) -1/2 Υ 1/2</formula><p>ρ , I m , and</p><formula xml:id="formula_99">U = -I r+rs Υ -1/2 ρ [ A 1 , A 2 ] T M -1/2 ρ M -1/2 ρ [ A 1 , A 2 ]Υ -1/2 ρ 0 .</formula><p>By Corollary 4.5.11 in <ref type="bibr" target="#b14">[15]</ref>, we have λ j (G ρ,+ ) = θ j λ j (U) for j = 1, . . . , r + rs + m, where θ j satisfies the inequalities: θ ≤ θ j ≤ θ. It is easy to show that U has r + rsm eigenvalues equal to -1, and the remaining 2m eigenvalues are given by -1 ± σ j /2, j = 1, . . . , m. From here, we can show that eig(G ρ,+ ) is contained in the required intervals.</p><p>(b) The existence of c 1 , c 2 &gt; 0 is guaranteed by Lemma 5. The existence of positive constants c 3 and c 4 will follow if can prove the following results:</p><formula xml:id="formula_100">lim inf k→∞ λ min (( M k ρ ) -1 M k ρ ) ≥ α λ min [ A * 1 , A * 2 ] τ I + Υ * ρ -1 [ A * 1 , A * 2 ] T &gt; 0 (34) lim sup k→∞ λ max (( M k ρ ) -1 M k ρ ) ≤ α λ max [ A * 1 , A * 2 ](Υ * ρ ) -1 [ A * 1 , A * 2 ] T &lt; ∞. (<label>35</label></formula><formula xml:id="formula_101">)</formula><p>Noting that αI M k Next, we shall establish an asymptotic result for the spectrum of ( k ρ,-) -1 B k ρ . Theorem 5 (a) Suppose that Assumptions A2 and A3 hold. Let r = rank(X * ) and s = n-r. Then the preconditioned matrix</p><formula xml:id="formula_102">( k ρ,-) -1 B k ρ has s eigenvalues equal to 1+O( √ ν k max{ Q ρ , A }), the remaining eigenvalues are O( √ ν k max{ Q ρ , A }) perturbations of those of G k ρ,-= N k ρ -(L k ) T L k 0 ,</formula><p>where N k ρ and L k are defined as in Theorem 4(a). The real eigenvalues of G k ρ,-are contained in the interval [0, θ k ], whereas the complex eigenvalues (with non-zero imaginary parts) are contained in the region</p><formula xml:id="formula_103">x + iy : θ k /2 ≤ x ≤ θ k /2, |y| ≤ L k .</formula><p>(b) Under the assumptions stated in Theorem 1(b), there exist positive constants c 1 , c 2 , c 3 such that for k sufficiently large, the real eigenvalues of G k ρ,-are contained in the interval [0, 2c 2 ], whereas the complex eigenvalues are contained in the region: {x + iy :</p><formula xml:id="formula_104">c 1 ≤ x ≤ c 2 , |y| ≤ c 3 }. Proof (a) We drop the superscript k in this proof. It is readily shown that up to a perturbation of O( √ ν max{ Q ρ , A }), -1 ρ,-B ρ is similar to diag(G ρ,-, I s). Thus the eigenvalues of -1 ρ,-B ρ are O( √ ν max{ Q ρ , A })</formula><p>perturbations of those of diag(G ρ,-, I s), whose eigenvalues are either 1 or those of G ρ,-. The remaining result follows from Proposition 2.11 in <ref type="bibr" target="#b5">[6]</ref> and the definitions of θ and θ in <ref type="bibr" target="#b29">(30)</ref>.</p><p>(b) The existence of c 1 , c 2 &gt; 0 is guaranteed by Lemma 5. Since</p><formula xml:id="formula_105">L ≤ α -1/2 max{σ -1/2 1 , σ -1/2 2</formula><p>} A =: c 3 , the required result is proved.</p><p>Remark 2 (a) Unlike the matrix G k ρ,+ in Theorem 4, we are not able to show that the real eigenvalues of G k ρ,-in Theorem 5 are bounded away from zero even with the assumptions stated in Theorem 1(b). However, even though the asymptotic result we are able to prove for eig(( k ρ,-) -1 B k ρ ) is weaker than that for eig(( k ρ,+ ) -1 B k ρ ), numerical results in <ref type="bibr" target="#b23">[24]</ref> showed that a preconditioner that is analogous to k ρ,-is typically more effective than the counterpart of k ρ,+ . For the numerical experiments in Sect. 6, our empirical experience (which we do not report in this paper) confirmed that k ρ,-is indeed more effective than</p><formula xml:id="formula_106">k ρ,+ . (b) It is not difficult to show that M k ρ = ((M k 0 ) -1 + ρI) -1 . Thus when ρ is large, M k ρ = ρ -1 I is a good choice to approximate M k ρ .</formula><p>Finally, we illustrate the asymptotic result in Theorem 4 using the problems in Examples 2 and 3. In this case, we take</p><formula xml:id="formula_107">H k 1 = D k 1 + Q ρ I r, H k 2 = D k 2 + Q ρ I rs , H k 3 = D k 3 , M k ρ = I m . (<label>36</label></formula><formula xml:id="formula_108">)</formula><p>Observe that we have approximated Q ρ by Q ρ I n, and hence</p><formula xml:id="formula_109">( Q k ρ ) 11 = (P k 1 ) T Q ρ (P k 1 ) ≈ Q ρ I r, ( Q k ρ ) 22 = (P k 2 ) T Q ρ (P k 2 ) ≈ Q ρ I rs .</formula><p>From the plots in Fig. <ref type="figure" target="#fig_1">2</ref>, it is clear that for k large, eig((</p><formula xml:id="formula_110">k ρ,+ ) -1 B k ρ ) is contained in an interval of the form [-c 1 , -c 2 ] ∪ [c 3 , c 4 ] with c 1 , c 2 , c 3 , c 4 &gt; 0 independent of k.</formula><p>This observation is consistent with the result in Theorem 4(b) by noting that the optimal solutions for all the QSDPs in both examples are primal nondegenerate and ( Q * ρ ) 11 0 (since Q 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Part II</head><p>Our construction of the next preconditioner start with the observation that a matrix of the form given below can be inverted at a moderate cost under Assumption A1 when m is not too large (say, less than 5000): Fig. <ref type="figure" target="#fig_1">2</ref> Same as Fig. <ref type="figure" target="#fig_0">1</ref>, but for the spectra of</p><formula xml:id="formula_111">V k ρ = -V k ρ V k ρ A T A 0 ,<label>(37)</label></formula><formula xml:id="formula_112">( k ρ,+ ) -1 B k ρ where V k ρ V k ρ is a symmetric positive definite approximation of (W k ) -1 (W k ) -1 + Q ρ . Observe that the (1,1) block of V k</formula><p>ρ can be inverted easily and the corresponding Schur complement matrix S k</p><formula xml:id="formula_113">V := A(V k ρ ) -1 (V k ρ ) -1</formula><p>A T can also be computed at a moderate cost under Assumption A1. Thus given</p><formula xml:id="formula_114">[X; y] ∈ S n × I R m , (V k ρ ) -1 [X;</formula><p>y] can be evaluated efficiently through the steps given below:</p><formula xml:id="formula_115">Compute X = (V k ρ ) -1 X(V k ρ ) -1 ; u = A( X); v = (S k V ) -1 (u + y); Compute (V k ρ ) -1 [X; y] = [(V k ρ ) -1 (A T v -X)(V k ρ ) -1 ; v].</formula><p>Assuming that (V k ρ ) -1 is pre-computed, the above computation requires four n × n matrix-matrix multiplications that cost 4n 3 flops.</p><p>We construct the matrix V k ρ in (37) as follows. First we find a positive semidefinite symmetrized Kronecker product approximation, say ∆ ρ ∆ ρ , of Q ρ ; see Sect. 5. Then consider the decomposition of the sum:</p><formula xml:id="formula_116">( W k ) -1 ( W k ) -1 + ∆ ρ ∆ ρ = (P k P k )( D k D k + k ρ k ρ )(P k P k ) T , with k ρ = (P k ) T ∆ ρ P k , and D k = D k + γ I,</formula><p>where γ ≥ 0 is a given constant. (Recall that D k is the diagonal matrix of eigenvalues of (W k ) -1 . The motivation for adding γ I to D k is to make D k more positive definite when ν k tends to zero.) For the linear operator</p><formula xml:id="formula_117">D k D k + k ρ k ρ in the decomposition, find a positive definite symmetrized Kronecker product approximation V k ρ V k ρ . Finally, take V k ρ = P k V k ρ (P k ) T .</formula><p>Observe that the construction of V k ρ does not depend on the clear separation of the eigenvalues of (W k ) -1 into two distinct clusters as we had relied upon in the construction of k ρ,± . Thus when the complementarity gap ν k is relatively large and the eigenvalues of (W k ) -1 do not separate into two distinct clusters, V k ρ is a suitable preconditioner to use. The last statement is especially applicable during the initial phase of the interior-point iteration.</p><p>We do not have asymptotic results for the spectrum of <ref type="formula" target="#formula_111">37</ref>) is an arbitrary positive definite matrix. However, for the special case Theorem 6 Let Z k j , j = 1, 2, 3, be defined as in <ref type="bibr">Lemma 7</ref> for</p><formula xml:id="formula_118">(V k ρ ) -1 B k ρ if the matrix V k ρ in (</formula><formula xml:id="formula_119">N ( A k (D k ) -1/2 ). Let Z k = [Z k 1 ; Z k 2 ; Z k 3 ] and G k ρ = (Z k ) T (D k ) -1/2 (D k + Q k ρ )(D k ) -1/2 Z k .</formula><p>We have the following results:</p><p>(a) The preconditioned matrix (Ω k ) -1 B k ρ has 2m eigenvalues equal to 1, and the remaining nm eigenvalues are those of G k ρ . (b) Suppose the optimal solution (X * , y * , S * ) is dual nondegenerate. Then for k sufficiently large,</p><formula xml:id="formula_120">eig(G k ρ ) ⊂ 1 + 0, Q ρ ( A k 1 ) † 2 A 2 + 1 max(τ -1 , Θ(ν k )) .</formula><p>Proof (a) The result follows from Lemma 6. (b) For the rest of the proof, we assume that k is sufficiently large. We can write</p><formula xml:id="formula_121">G k ρ = I + T k ρ , where T k ρ := (Z k ) T (D k ) -1/2 Q k ρ (D k ) -1/2 Z k . Since T k ρ 0, it is clear that G k ρ I. Now con- sider matrix T k ρ . Since 0 Q k ρ Q ρ I, we have T k ρ Q ρ 3 j=1 (Z k j ) T (D k j ) -1 Z k j . Hence T k ρ Q ρ ( ( A k 1 ) † 2 A 2 + 1) (Z k 2 ) T (D k 2 ) -1 Z k 2 + (Z k 3 ) T (D k 3 ) -1 Z k 3 . Now τ I D k 2 , Θ(1/ν k )I D k 3 ⇒ (Z k j ) T (D k j ) -1 Z k j max(τ -1 , Θ(ν k ))(Z k j ) T Z k j for j = 2, 3. Together with the fact that 3 j=1 (Z k j ) T Z k j = I, we have T k ρ Q ρ ( ( A k 1 ) † 2 A 2 + 1) max(τ -1 , Θ(ν k )) I.</formula><p>From here, the required result follows readily.</p><p>We consider again the problems in Examples 2 and 3 to illustrate the asymptotic result in Theorem 6(b). The spectra of (Ω k ) -1 B k ρ for 10 random instances of Examples 2 and 3 are plotted in Figure <ref type="figure" target="#fig_3">3</ref>. From the right-hand side plot, it is clear that for k large, eig((Ω k ) -1 B k ρ ) for the instances in Example 3 are each contained in an interval of the form [c 1 , c 2 ] with c 1 , c 2 &gt; 0 independent of k. This observation is consistent with the result in Theorem 6(b) by noting that the optimal solutions for the instances in Example 3 are dual nondegenerate. On the other hand, the optimal solutions for the instances in Example 2 are dual degenerate, and the spectra of (Ω k ) -1 B k ρ shown in the left-hand side plot of Fig. <ref type="figure" target="#fig_3">3</ref> are not contained in a finite interval as k ↑ ∞.</p><p>Remark 3 An alternative to V k ρ in (37) during the initial phase of the interiorpoint iteration is the following preconditioner:</p><formula xml:id="formula_122">-diag((W k ) -1 ) diag((W k ) -1 ) -diag(Q ρ ) A T A 0 . (<label>40</label></formula><formula xml:id="formula_123">)</formula><p>The above is analogous to the preconditioner proposed in <ref type="bibr" target="#b8">[9]</ref> for the LCCQP problem <ref type="bibr" target="#b4">(5)</ref>. Unfortunately, it is not competitive at all compared to V k ρ for the  <ref type="figure" target="#fig_0">1</ref> but for the spectra of (Ω k ) -1 B k ρ test problems we are considering in Sect. 6. An important distinction between the preconditioner in (40) and its counterpart for LCCQP is that for the latter, the contribution by the interior-point iterate is fully incorporated into the (1,1) block of the preconditioner, but for the former, only a diagonal approximation of (W k ) -1 (W k ) -1 is incorporated. Such a diagonal approximation appears to be too weak to make (40) an effective preconditioner for B k ρ , even when the complementarity gap ν k is relatively large. One may argue that the preconditioner (40) for (QSDP) is actually not the correct analog of the preconditioner proposed in <ref type="bibr" target="#b6">[7]</ref> for LCCQP, and the correct version should have its (1,1) block equal to</p><formula xml:id="formula_124">-P k (D k + diag( Q k ρ ))(P k ) T . However, it is expensive to compute diag( Q k ρ ) since at least Θ(n 4</formula><p>) flops is needed in general. As such, the latter preconditioner is usually not a practical choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Part III</head><p>The next preconditioner we propose for B k ρ is constructed by suitably modifying D k in the decomposition:</p><formula xml:id="formula_125">(W k ) -1 (W k ) -1 = P k diag(D k 1 , D k 2 , D k 3 )(P k ) T .</formula><p>Specifically, the preconditioner is given by</p><formula xml:id="formula_126">k ρ = -P k k ρ (P k ) T A T A 0 ,</formula><p>where</p><formula xml:id="formula_127">k ρ = diag(H k 1 , H k 2 , H k 3 ). (<label>41</label></formula><formula xml:id="formula_128">)</formula><p>For the above preconditioner, we typically want to choose the linear operators H k 1 , H k 2 , and H k 3 to have the following forms:</p><formula xml:id="formula_129">H k 1 = H k 1 H k 1 , H k 2 = H k 2 ⊗ H k 2 , H k 3 = H k 3 H k 3 ,<label>(42)</label></formula><p>where </p><formula xml:id="formula_130">H k 1 , H k 2 ∈ S r</formula><formula xml:id="formula_131">(D k + Q k ρ ) k ρ β(D k + Q k ρ ) for all k for some constants β, β &gt; 0. Then eig(G k ρ ) ⊂ β -1 , β -1 .</formula><p>Proof (a) The first result follows from Lemma 6. (b) The matrix G k ρ can be written as</p><formula xml:id="formula_132">G k ρ = (Z k ) T diag(N k ρ , I s)Z k + O( √ ν k Q ρ ),</formula><p>where N k ρ is defined in <ref type="bibr" target="#b28">(29)</ref>. Thus, using the definitions of θ k and θ k in <ref type="bibr" target="#b29">(30)</ref>,</p><formula xml:id="formula_133">we have eig(G k ρ ) ⊂ [θ k , θ k ] + O( √ ν k Q ρ )</formula><p>. By Lemma 5, the required result follows.</p><p>(c) It is easy to see that</p><formula xml:id="formula_134">G k ρ β -1 (Z k ) T Z k = β -1 I. On the other hand, G k ρ β -1 I + (Z k ) T ( k ρ ) -1/2 Q k ρ ( k ρ ) -1/2 Z k β -1 I + Q ρ (Y k ) T Y k ,</formula><p>where (b) As an example, we show how Theorem 7(b) can be modified to suit the LCCQP problem <ref type="bibr" target="#b4">(5)</ref>. Note that for a LCCQP problem, D k 2 does not exist,</p><formula xml:id="formula_135">Y k := [Y k 1 ; Y k 2 ; Y k 3 ] = ( k ρ ) -1/2 Z k . Now I = (Z k ) T Z k = (Y k ) T k ρ Y k = (Y k 1 ) T H k 1 Y k 1 + (Y k 2 ) T H k 2 Y k 2 + (Y k 3 ) T H k 3 Y k 3 min{σ 1 , σ 2 , Θ(1/ν k )} (Y k ) T Y k , hence G k ρ β -1 I + Q ρ max{σ -1 1 , σ -1 2 , Θ(ν k )}I. From here, (45) follows. (d) Since β -1 I ( k ρ ) -1/2 (D k + Q k ρ )( k ρ ) -1/2 β -1 I,</formula><formula xml:id="formula_136">D k = diag(D k 1 , D k 3 ) with diag(D k 1 ) = Θ(ν k ), diag(D k 3 ) = Θ(1/ν k )</formula><p>, and P k is a permutation matrix. A k is actually A but with its columns permuted according to the partition in D k . Similarly, Q k ρ is Q ρ := Q + ρA T A but with its rows and columns permuted. Assuming that H k 1 and H k 3 satisfy the conditions in <ref type="bibr" target="#b27">(28)</ref> and</p><formula xml:id="formula_137">( Q ρ ) 11 0, then there exist c 1 , c 2 &gt; 0 such that eig( k ρ ) -1 B k ρ ) ⊂ {1} ∪ [c 1 , c 2 ]</formula><p>for k large. We note that such a result is established under a weaker condition than the one required in Corollary 4.5 of <ref type="bibr" target="#b6">[7]</ref>, which assumed Q to be positive definite.</p><p>We consider again the problems in Examples 2 and 3 to illustrate the asymptotic result in Theorem 7(b). We take H k 1 , H k 2 , and H k 3 as in (36). The spectra of ( k ρ ) -1 B k ρ for 10 instances of Examples 2 and 3 are plotted in Fig. <ref type="figure">4</ref>. From the plots, it is clear that for each problem instances, eig(( The construction of preconditioners in the last section relied to some extent on our ability to find a suitable symmetrized Kronecker product (SKP) approximation of Q ρ or submatrices of Q k ρ . However, very little is known about such an approximation problem. Our objective in this section is to provide a strategy for constructing an SKP approximation.</p><formula xml:id="formula_138">k ρ ) -1 B k ρ ) is contained in a finite interval [c 1 , c 2 ] with c 1 , c 2 &gt; 0 independent of k.</formula><p>Contrary to the SKP approximation problem, the problem of finding the nearest (in Frobenius norm) Kronecker product (NKP) approximation of a linear operator defined on I R n×n has been studied in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">Sect. 6]</ref>. (We refer the reader to the references therein for earlier work on the subject.) It is shown in <ref type="bibr" target="#b29">[30]</ref> that the NKP approximation can be derived from the best rank-one approximation of a permuted version of the matrix representation of the linear operator with respect to the canonical basis of I R n×n . The special case of finding the NKP approximation of a sum of Kronecker products is studied in <ref type="bibr" target="#b18">[19]</ref>, wherein the solution is obtained by solving a small non-convex optimization problem. It is shown recently in <ref type="bibr" target="#b28">[29]</ref> that the latter solution can actually be found analytically by solving a small eigenvalue problem.</p><p>By applying our knowledge on the NKP problem, a natural route for constructing a suitable SKP approximation of a linear operator T defined on S n is as follows. We first extend the linear operator to T : R n×n → R n×n defined by T (U) = T ((U + U T )/2). Suppose U ⊗ V is the NKP approximation of T . Then U V can be used as an SKP approximation of T . Moreover, the error in the SKP approximation is no larger than that for the NKP approximation in the following sense. Suppose the matrix representations of T and T in the canonical orthonormal bases of S n and I R n×n are denoted by mat(T ) and mat( T ), respectively. We define mat(U ⊗ V) and mat(U V) similarly. Then mat(T -U V) = T mat( T -U ⊗ V) , where ∈ I R n 2 × n has orthonormal columns; see <ref type="bibr" target="#b26">[27]</ref>. As a result, mat</p><formula xml:id="formula_139">(T -U V) F ≤ mat( T -U ⊗ V) F .</formula><p>Note that in the special case where Q is a diagonal operator defined by Q(X) = U • X, where U ∈ S n ∩ I R n×n + is given, a positive semidefinite SKP approximation for Q can be constructed readily as follows. Consider the best We implemented the algorithms in Matlab (version 7.0) and the experiments were conducted on a 3.0GHz Pentium 4 PC with 4GB of RAM. We stopped the algorithms when the accuracy measure φ defined by</p><formula xml:id="formula_140">φ = max X • S 1 + |pobj| + |dobj| , R p 2 1 + b 2 , R d F 1 + C F (<label>46</label></formula><formula xml:id="formula_141">)</formula><p>is less than 10 -7 , or when the algorithms did not improve both the duality gap and infeasibilities. In (46), "pobj" and "dobj" denote the primal and dual objective values, respectively. The stopping criterion used to solve ( <ref type="formula" target="#formula_12">9</ref>) is described in <ref type="bibr" target="#b10">(11)</ref>. We also set the maximum number of PSQMR steps allowed to solve each augmented system to 1000. Moreover, we stopped the interior-point iteration when PSQMR hit the maximum number of steps allowed. The last condition indicated that the linear system was becoming very ill-conditioned and there were little to be gained in continuing the outer iteration unless the maximum number of PSQMR steps allowed was increased. The initial iterate for all the algorithms was taken to be X 0 = (n/ √ 2)I, y 0 = 0, S 0 = √ nI. The performance results of our algorithms on E1-E8 are given in Table <ref type="table" target="#tab_1">1</ref>. The columns corresponding to "it" give the number of interior-point iterations taken, whereas the columns "sq" give the average number of PSQMR steps taken to solve each of the two linear systems (9) during the computation of predictor and corrector directions at each interior-point iteration. The time reported is in the format of "hour:minute:second". Note that we did not run Algorithm A0 or A1 for some of the larger problems. Those entries with an " * " mean that the algorithms were terminated because the PSQMR solver hit the maximum number of steps allowed. Table <ref type="table" target="#tab_1">1</ref> contains a variety of information that we extract and summarize below.</p><p>1. The number of interior-point iterations required by our proposed interiorpoint method grows very modestly with the problem dimension n. In all the test problems, the number of iterations required is less than 20. 2. Algorithm A0 took more than 35 minutes to solve E1-100, . . . , E8-100</p><p>(not shown in Table <ref type="table" target="#tab_1">1</ref>). This indicates that solving (9) via a direct solver is extremely expensive. For the problems E1 -100, . . . , E8 -100, it is at least 100 times more expensive than using an iterative solver with an appropriate preconditioner, say k ρ . 3. Based on the stopping criterion <ref type="bibr" target="#b10">(11)</ref> for the PSQMR method used to solve (9), Algorithms A2, A3 and A4 took about the same number of interiorpoint iterations to converge compared to Algorithm A0 that uses a direct method. This indicates that the inexact search directions are computed to sufficient accuracy, and thus the residual errors do not degrade the outer iterations. 4. Algorithm A1, which employs no preconditioner, is not able to solve most of the test problems to the required accuracy in φ, defined in (46). All An entry with an " * " means that the algorithm was terminated before the accuracy of φ ≤ 10 -7 was achieved. For these problems, φ ranges from the level of 10 -6 for E1 to the level of 10 -3 for E8.</p><p>our test problems have ill-conditioned augmented matrices B k ρ when the complementarity gaps X k • S k /n are small. 5. Algorithm A3, which employs V k ρ in (37) as the preconditioner, performed fairly well on the problems in E1, E3, E5, and E7, but less so for those in E2, E4, E6 and E8. Note that the optimal solutions for all the problems are dual degenerate. This violates one of the condition in Theorem 6(b), and so there is no theoretical basis to expect V k ρ to be an effective preconditioner. 6. The preconditioners k ρ,-and k ρ are very effective for B k ρ , as attested by the good performance of Algorithms A2 and A4. We note that for the QSDPs arising from (3), the optimal solutions are always primal nondegenerate, and so the good performance of Algorithm A2 that involves k ρ,-is consistent with Theorem 5(b). 7. Comparing Algorithms A2 and A4, we see that the preconditioner k ρ is more effective (in terms of the number of PSQMR steps) than k ρ,-for the test problems in E1, E3, E5, and E7. For the problems E1, E3, E5, E7, with n = 1600, the average numbers of PSQMR steps required to solve <ref type="bibr" target="#b8">(9)</ref> for Algorithm A4 is between 37% to 75% of the corresponding numbers for Algorithm A2. However, because evaluating ( k ρ ) -1 [X; y] requires two times the number of n × n matrix-matrix multiplications for ( k ρ,-) -1 [X; y], the savings in computation time are not as impressive as the reductions in the number of PSQMR steps. Notice that k ρ,-is a more effective preconditioner than k ρ for the problems in E2, E4, E6, and E8. For these test problems, the choices H k 1 , H k 2 and H k 3 in (36) used for k ρ,-are more effective than the choices in (42) constructed from Procedure SKPA for k ρ . 8. We should emphasize that the dimension of the system in ( <ref type="formula" target="#formula_12">9</ref>) is m + n(n + 1)/2, and it is dense. Thus for n = 1600, the dimension is more than 1.28 millions. It is quite surprising that an iterative solver with appropriate preconditioning can solve such a large and ill-conditioned linear system in less than a hundred iterative steps. Because the preconditioning steps can be evaluated at moderate costs, the savings in the iterative steps enabled the test problems, E1-1600, . . . ,E8-1600, to be solved in a few hours on a desktop computer.</p><p>To further evaluate the performance of our algorithms, we consider the following classes of QSDPs arising from the EDM problem (4): EDM1. QSDPs arising from the EDM problem (4) with the dissimilarity matrix B ∈ S n generated as follows. First we generate n random points, x 1 , . . . , x n , in the unit cube centered at the origin in I R 3 . Then we set B ij = x ix j if the distance is less than a certain cut-off distance R; otherwise, set B ij = 0. The non-negative weight matrix H is chosen to be the 0-1 matrix having the same sparsity pattern as B. The set of indices where the distances are fixed is given by E = {(1, j) : B 1j = 0, j = 1, . . . , n}. Note that the operators Q for these QSDPs are positive semidefinite, but not positive definite. We generated 4 test problems with n = 100, 200, 400, 800, and their corresponding dissimilarity matrices B have 16.6%, 8.6%, 4.5%, and 2.4% nonzero elements, respectively. Note that in the actual QSDP test problems, we added the term -0.01I • X to the objective function in (4) so as to induce a low-rank primal optimal solution. EDM2. Same as EDM1 but the points are chosen to be the coordinates of the atoms in the following protein molecules, 1PTQ, 1HOE, 1LFB, 1PHT, 1POA, 1AX8, taken from the Protein Data Bank <ref type="bibr" target="#b7">[8]</ref>. These An entry with an " * " means that the algorithm was terminated before the accuracy of φ ≤ 10 -7 was achieved. For these problems, φ is at the level of 10 The performance results of our algorithms on EDM1 and EDM2 are given in Table <ref type="table" target="#tab_2">2</ref>. Basically, the observations we have in Items 1 to 4 for Table <ref type="table" target="#tab_1">1</ref> are applicable also to Table <ref type="table" target="#tab_2">2</ref>. For the QSDPs in EDM1 and EDM2, Algorithm A4 is the most efficient, followed by A3, and then A2. It appears that for a QSDP where Q is not positive definite, the preconditioner k ρ is a better choice over k ρ,-.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed an inexact primal-dual path-following Mehrotra-type predictorcorrector method for solving convex quadratic SDP problems. We suggested computing the search direction at each iteration from the augmented equation by an iterative solver with preconditioners properly designed to overcome the ill-conditioning of the augmented matrix. The preconditioners are shown to have favorable asymptotic spectral distributions for the preconditioned systems to achieve fast convergence under suitable nondegeneracy assumptions. For one of the classes of preconditioners, no nondegeneracy assumption is needed. Numerical experiments on a variety of convex QSDPs with matrices of dimension up to 1600 showed that our inexact interior-point method is quite efficient and robust. For the test problems considered in this paper, our inexact interior-point method can solve each QSDPs at a cost that is at most Θ(mn 3 ) + Θ(m 2 n 2 ) + Θ(m 3 ). However, the computational complexity at each iteration of our interior-point method is inherently higher than first-order methods such as the nonlinear-programming based method of Burer, Monteiro, and Zhang <ref type="bibr" target="#b9">[10]</ref>, so it is of interest to ask whether such methods can be extended to QSDPs. It is also of interest to investigate whether variants of augmented Lagrangian dual methods can be applied to QSDPs and whether such methods are competitive to our inexact interior-point methods. The performance of our method here may serve as the benchmark for evaluating other (possibly more efficient) algorithms in the future.</p><p>While extensive testing on large QSDPs coming from practical problems remain to be done, we have provided the essential computational framework for such exploration to be possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig.1The spectra of B k ρ corresponding to the final iterates for 10 random instances of Examples 2 and 3. The solid dots correspond to negative eigenvalues, and the diamonds correspond to positive eigenvalues. The x-coordinates are the indices of the problem instances, while the y-coordinates are the magnitudes of the eigenvalues in logarithmic scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ρ αI and τ I D k 2 τ</head><label>2</label><figDesc>I for all k, the first inequality in (34) is straightforward to prove. Under the assumptions of Theorem 1(b), [ A * 1 , A * 2 ] is surjective and Υ * ρ 0, from here, the second inequality in (34) follows. We can prove (35) similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Same as Fig. 1 but for the spectra of (Ω k ) -1 B k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and (Z k ) T Z k = I, thus β -1 I G k ρ β -1 I, and the required result follows. Remark 4 (a) Observe that unlike Theorems 4(b) and 6(b), Theorem 7(b) does not assume primal or dual nondegeneracy for the optimal solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 5 A</head><label>45</label><figDesc>Fig. 4 Same as Fig. 1 but for the spectra of ( k ρ ) -1 B k ρ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>++ , and H k 2 , H k 3 ∈ S s ++ . The motivation for considering the operators in (42) will become apparent when we derive the Schur complement matrix associated with k ρ in the ensuing paragraph. Note that if (X</figDesc><table><row><cell>(d) Suppose β</cell></row><row><cell>is primal degenerate (i.e., [ A  *  1 , A  *  2 ] is not surjective), then k ρ may be nearly</cell></row></table><note><p>* , y * , S * )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Performance of the Algorithms A1-A4 on the problem sets E1-E8</figDesc><table><row><cell></cell><cell>A1 (I)</cell><cell></cell><cell></cell><cell cols="2">A 2 ( V ρ , ρ,-)</cell><cell></cell><cell>A 3 ( V ρ )</cell><cell></cell><cell></cell><cell>A 4 ( V ρ , ρ )</cell></row><row><cell>n</cell><cell>it time</cell><cell>sq</cell><cell cols="2">it time</cell><cell>sq</cell><cell cols="2">it time</cell><cell>sq</cell><cell cols="2">it time</cell><cell>sq</cell></row><row><cell cols="2">E1 100 11* 0:15</cell><cell cols="2">167 11</cell><cell cols="2">0:05 23</cell><cell>12</cell><cell>0:08</cell><cell cols="2">28 12</cell><cell>0:06 15</cell></row><row><cell cols="2">400 13* 8:16</cell><cell cols="2">167 13</cell><cell cols="2">3:11 24</cell><cell>13</cell><cell>2:50</cell><cell cols="2">20 13</cell><cell>2:39 13</cell></row><row><cell cols="4">800 13* 36:40 122 13</cell><cell cols="2">18:48 22</cell><cell>13</cell><cell>14:23</cell><cell cols="2">15 13</cell><cell>13:29 10</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>14</cell><cell cols="2">2:53:43 28</cell><cell>14</cell><cell>1:58:00</cell><cell cols="2">17 13</cell><cell>1:38:13 11</cell></row><row><cell>E2 100</cell><cell cols="2">7* 0:11 207</cell><cell>9</cell><cell cols="2">0:03 11</cell><cell>9</cell><cell>0:08</cell><cell>43</cell><cell>9</cell><cell>0:03</cell><cell>8</cell></row><row><cell>400</cell><cell cols="3">8* 6:39 221 11</cell><cell cols="2">1:44 14</cell><cell>11</cell><cell>7:58</cell><cell cols="2">78 11</cell><cell>3:24 22</cell></row><row><cell>800</cell><cell cols="3">8* 33:11 185 11</cell><cell cols="2">12:03 15</cell><cell>12</cell><cell cols="3">1:12:12 105 11</cell><cell>30:03 30</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>12</cell><cell cols="2">1:43:36 18</cell><cell>12</cell><cell>7:05:16</cell><cell cols="2">90 12</cell><cell>4:42:59 38</cell></row><row><cell>E3 100</cell><cell cols="2">8* 0:13 206</cell><cell>9</cell><cell cols="2">0:05 27</cell><cell>9</cell><cell>0:05</cell><cell cols="2">24 10</cell><cell>0:06 19</cell></row><row><cell>400</cell><cell cols="3">9* 6:21 186 10</cell><cell cols="2">3:27 36</cell><cell>10</cell><cell>2:05</cell><cell cols="2">19 10</cell><cell>2:34 18</cell></row><row><cell cols="4">800 10* 40:19 180 11</cell><cell cols="2">28:55 44</cell><cell>11</cell><cell>14:03</cell><cell cols="2">18 11</cell><cell>21:38 21</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>11</cell><cell cols="2">3:06:59 41</cell><cell>11</cell><cell>1:22:35</cell><cell cols="2">14 11</cell><cell>2:21:15 20</cell></row><row><cell>E4 100</cell><cell cols="3">7* 0:17 309 10</cell><cell cols="2">0:04 15</cell><cell>10</cell><cell>0:13</cell><cell>64</cell><cell>9</cell><cell>0:03 11</cell></row><row><cell>400</cell><cell cols="3">7* 7:21 283 10</cell><cell cols="2">2:01 19</cell><cell>10</cell><cell>7:28</cell><cell cols="2">80 10</cell><cell>2:47 19</cell></row><row><cell>800</cell><cell cols="3">7* 36:58 239 11</cell><cell cols="2">15:11 21</cell><cell>10</cell><cell>42:59</cell><cell cols="2">73 11</cell><cell>42:50 45</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>11</cell><cell cols="2">1:56:01 23</cell><cell>11</cell><cell cols="3">7:18:52 102 11</cell><cell>4:09:31 37</cell></row><row><cell>E5 100</cell><cell cols="3">9* 0:14 195 11</cell><cell cols="2">0:07 30</cell><cell>11</cell><cell>0:12</cell><cell cols="2">49 11</cell><cell>0:06 18</cell></row><row><cell cols="4">400 11* 7:02 167 12</cell><cell cols="2">3:16 27</cell><cell>13</cell><cell>4:40</cell><cell cols="2">35 13</cell><cell>3:53 20</cell></row><row><cell cols="4">800 12* 53:52 201 13</cell><cell cols="2">24:52 30</cell><cell>13</cell><cell>22:58</cell><cell cols="2">26 13</cell><cell>19:11 15</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>13</cell><cell cols="2">2:53:12 31</cell><cell>14</cell><cell>3:05:14</cell><cell cols="2">30 13</cell><cell>2:01:09 13</cell></row><row><cell>E6 100</cell><cell cols="3">7* 0:14 247 10</cell><cell cols="2">0:04 15</cell><cell>10</cell><cell>0:13</cell><cell cols="2">61 10</cell><cell>0:04 10</cell></row><row><cell>400</cell><cell cols="3">8* 7:35 253 11</cell><cell cols="2">1:54 16</cell><cell>11</cell><cell>8:23</cell><cell cols="2">81 11</cell><cell>4:26 28</cell></row><row><cell>800</cell><cell cols="3">8* 35:05 196 11</cell><cell cols="2">17:01 24</cell><cell>12</cell><cell cols="3">1:15:02 108 12</cell><cell>43:58 40</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>12</cell><cell cols="2">1:54:35 20</cell><cell>12</cell><cell>7:11:38</cell><cell cols="2">91 12</cell><cell>4:59:57 40</cell></row><row><cell>E7 100</cell><cell cols="2">7* 0:18 326</cell><cell>9</cell><cell cols="2">0:05 28</cell><cell>9</cell><cell>0:10</cell><cell>53</cell><cell>9</cell><cell>0:05 17</cell></row><row><cell>400</cell><cell cols="3">8* 7:30 251 10</cell><cell cols="2">3:31 37</cell><cell>10</cell><cell>3:37</cell><cell cols="2">36 10</cell><cell>2:47 19</cell></row><row><cell>800</cell><cell cols="3">9* 50:48 256 11</cell><cell cols="2">29:49 46</cell><cell>11</cell><cell>25:43</cell><cell cols="2">37 11</cell><cell>22:37 22</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>11</cell><cell cols="2">3:29:34 47</cell><cell>11</cell><cell>2:07:16</cell><cell cols="2">25 11</cell><cell>2:21:26 20</cell></row><row><cell>E8 100</cell><cell cols="3">6* 0:12 251 10</cell><cell cols="2">0:04 19</cell><cell>10</cell><cell>0:14</cell><cell cols="2">70 10</cell><cell>0:04 12</cell></row><row><cell>400</cell><cell cols="3">7* 8:00 308 10</cell><cell cols="2">2:08 20</cell><cell>10</cell><cell>7:07</cell><cell cols="2">75 10</cell><cell>4:01 29</cell></row><row><cell>800</cell><cell cols="3">7* 40:26 262 10</cell><cell cols="2">19:30 31</cell><cell>11</cell><cell cols="3">1:08:58 109 11</cell><cell>43:15 45</cell></row><row><cell>1600</cell><cell></cell><cell></cell><cell>11</cell><cell cols="2">2:23:34 30</cell><cell>10</cell><cell>3:53:29</cell><cell cols="2">57 11</cell><cell>5:10:25 46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Performance of the Algorithms A1-A4 on the problem sets EDM1 and EDM2</figDesc><table><row><cell>n</cell><cell>A1 (I)</cell><cell></cell><cell></cell><cell cols="2">A 2 ( V ρ , ρ,-)</cell><cell></cell><cell>A 3 ( V ρ )</cell><cell></cell><cell></cell><cell>A 4 ( V ρ , ρ )</cell><cell></cell></row><row><cell></cell><cell>it time</cell><cell>sq</cell><cell>it</cell><cell>time</cell><cell>sq</cell><cell>it</cell><cell>time</cell><cell>sq</cell><cell>it</cell><cell>time</cell><cell>sq</cell></row><row><cell>99</cell><cell>7* 0:23</cell><cell>331</cell><cell>16</cell><cell>0:22</cell><cell>63</cell><cell>16</cell><cell>0:17</cell><cell>44</cell><cell>16</cell><cell>0:15</cell><cell>32</cell></row><row><cell>199</cell><cell>8* 2:50</cell><cell>374</cell><cell>18</cell><cell>1:29</cell><cell>41</cell><cell>18</cell><cell>1:46</cell><cell>48</cell><cell>18</cell><cell>1:16</cell><cell>27</cell></row><row><cell>399</cell><cell>6* 8:40</cell><cell>278</cell><cell>19</cell><cell>9:46</cell><cell>46</cell><cell>19</cell><cell>9:45</cell><cell>44</cell><cell>19</cell><cell>6:56</cell><cell>23</cell></row><row><cell>799</cell><cell>7* 1:09:35</cell><cell>349</cell><cell>22</cell><cell>1:03:54</cell><cell>42</cell><cell>21</cell><cell>1:05:10</cell><cell>44</cell><cell>21</cell><cell>53:53</cell><cell>26</cell></row><row><cell>401</cell><cell>8* 17:37</cell><cell>421</cell><cell>21</cell><cell>41:14</cell><cell>188</cell><cell>21</cell><cell>11:13</cell><cell>46</cell><cell>21</cell><cell>10:50</cell><cell>34</cell></row><row><cell>557</cell><cell>8* 37:26</cell><cell>400</cell><cell>20</cell><cell>47:14</cell><cell>95</cell><cell>20</cell><cell>26:31</cell><cell>49</cell><cell>20</cell><cell>21:03</cell><cell>29</cell></row><row><cell>640</cell><cell>7* 32:00</cell><cell>281</cell><cell>22</cell><cell>1:11:21</cell><cell>93</cell><cell>22</cell><cell>47:34</cell><cell>58</cell><cell>22</cell><cell>32:44</cell><cell>30</cell></row><row><cell>813</cell><cell>8* 1:12:51</cell><cell>306</cell><cell>21</cell><cell>5:06:31</cell><cell>225</cell><cell>22</cell><cell>2:40:06</cell><cell>106</cell><cell>22</cell><cell>1:38:35</cell><cell>46</cell></row><row><cell>913</cell><cell></cell><cell></cell><cell>22</cell><cell>4:38:10</cell><cell>141</cell><cell>22</cell><cell>1:59:40</cell><cell>56</cell><cell>22</cell><cell>1:23:49</cell><cell>28</cell></row><row><cell>1002</cell><cell></cell><cell></cell><cell>23</cell><cell>3:41:58</cell><cell>82</cell><cell>23</cell><cell>3:07:46</cell><cell>67</cell><cell>23</cell><cell>1:53:04</cell><cell>28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>-1 6 test problems have dimension n=401, 557, 640, 813, 913, 1002, with 17.4%, 13.1%, 11.0%, 8.3%, 10.9%, 7.4% nonzeros in B, respectively.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research supported in part by Academic Research Grant R146-000-076-112.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where V k ρ = Ω k , with</p><p>we will prove an asymptotic result for the spectrum of (Ω k ) -1 B k ρ in Theorem 6. The motivation for considering the special case Ω k is as follows. Let</p><p>it is rather natural to consider using Ω k as a preconditioner for B k ρ . However, note that when the complementarity gap ν k is small, Ω k may have small eigenvalues of the order Θ(ν k ) (which would happen if (X * , y * , S * ) is primal degenerate) and these small eigenvalues may blow up the norm of (Ω k ) -1 Q k ρ . Thus, from the equation</p><p>it is not clear whether Ω k would be a good preconditioner. We will next show that when the optimal solution (X * , y * , S * ) is dual nondegenerate, Ω k can in fact be an effective preconditioner for B k ρ . Before we proceed further, we state two lemmas that will be used in the analysis of the spectrum of the preconditioned matrix.</p><p>. Suppose U is a symmetric positive definite approximation of U, and we consider G = [-U, V T ; V, 0] as a preconditioner for G. Then G -1 G has 2m eigenvalues equal to 1, and the remaining pm eigenvalues are those of the matrix Z T U -1/2 U U -1/2 Z, where Z ∈ I R p×(p-m) is a matrix whose columns form an orthonormal basis of N (V U -1/2 ).</p><p>Proof See Theorem 2 in <ref type="bibr" target="#b27">[28]</ref>.</p><p>where</p><p>By noting that</p><p>, the equation in (39) easily leads to the required result. </p><p>y] can be evaluated efficiently through the steps given below:</p><p>The above computation requires eight n × n matrix-matrix multiplications that cost 8n 3 flops.</p><p>The Schur complement matrix associated with k ρ is given by</p><p>and H k 3 in (42), we have:</p><p>where</p><p>T , and</p><p>In this case, the computation of S k is similar to that of the Schur complement matrix in a linear SDP employing the NT direction. In particular, any sparsity structure in A can be exploited efficiently in the computation; see <ref type="bibr" target="#b11">[12]</ref>. The formula in (44) for S k holds only for the special choices of H k 1 , H k 2 and H k 3 in (42). Without using the special choices, S k must be computed from (43) for which the computation is akin to that of the Schur complement matrix of a linear SDP employing the AHO direction <ref type="bibr" target="#b2">[3]</ref>. It is well known that the latter computation cannot exploit sparsity in A as effectively as the computation using (44). This explains the rationale for the special choices of H k 1 , H k 2 and H k 3 in (42).</p><p>(a) Then the preconditioned matrix</p><p>rank-one approximation uu T of U such that Uuu T F = min, then diag(u) diag(u) is a positive semidefinite SKP approximation of Q. The fact that the preceding SKP is positive semidefinite is a consequence of the Perron-Frobenius Theorem, which ensures that u can be chosen in I R n + when the entries of U are non-negative; see <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Theorem 8.3.1]</ref>.</p><p>For the preconditioner k ρ in (41), a procedure for constructing an approximation of the form <ref type="formula">42</ref>) is as follows. Procedure SKPA:</p><p>1 by using the method described in <ref type="bibr" target="#b18">[19]</ref>. It is shown in <ref type="bibr" target="#b28">[29]</ref> that H k 1 can be chosen to be positive semidefinite. (iii) Take</p><p>The same procedure can similarly be applied to the construction of the approximations: <ref type="bibr" target="#b21">22</ref> , and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Numerical experiments</head><p>Before we present the details of the numerical experiments, we should emphasize that our purpose here is not conduct extensive numerical testing but to demonstrate that the algorithms we have proposed for solving (1) are correct and they can potentially be efficient and robust. These algorithms may be adopted as prototypes from which more sophisticated and tailor-made algorithms can be designed for solving QSDPs coming from real-world applications.</p><p>To evaluate the performance of our interior-point algorithms for solving (1) and the effectiveness of the preconditioners constructed in Sect. 4 for solving <ref type="bibr" target="#b8">(9)</ref>, we consider the following classes of test problems: E1. QSDPs arising from the NCM problem (3) with Q(X) = U • X. We generate the matrix K in (3) as in Example 2. The matrix U is generated randomly as follows: tmp = rand(n); U = (tmp+tmp')/2. We generated 4 test problems with n = 100, 400, 800, 1600. For these problems, the condition numbers of Q range from 7.67 × 10 2 to 4.37 × 10 3 . The norms Q are approximately 1 for all problems. E2. Same as E1 but K ∈ S n is generated as in Example 3. E3. Same as E1 but U is given by tmp = rand(n);U = 5*(tmp+tmp').</p><p>In this case, the quadratic form X • Q(X) has more weight in the objective function of (1). The norms Q are approximately 10 for all problems. E4. Same as E2 but U is given by tmp = rand(n); U=5*(tmp+tmp'). E5. Same as E1 but with additional linear constraints added. Let J n be a randomly chosen subset of {(i, j) : 1 ≤ i &lt; j, j = 1, . . . , n}. We set X ij = 0 for all (i, j) ∈ J n . The purpose here is to study the effect of having a more complicated linear map A(X) compared to the diagonal map diag(X) in <ref type="bibr" target="#b2">(3)</ref>. Again, we generated 4 test problems with n = 100, 400, 800, 1600. The number of additional linear constraints added are 120, 397, 772, 1566, respectively. E6-E8. Same as E2-E4 but with additional linear constraints similar to those in E5 added.</p><p>We use 5 variants of Algorithm IP-QSDP described in <ref type="bibr" target="#b28">[29]</ref> to solve each test problem: A0. Algorithm IP-QSDP that uses a direct solver to solve <ref type="bibr" target="#b8">(9)</ref>. A1. Algorithm IP-QSDP that uses PSQMR to solve <ref type="bibr" target="#b8">(9)</ref> with no preconditioning. A2. Algorithm IP-QSDP that employs PSQMR to solve <ref type="bibr" target="#b8">(9)</ref> with the preconditioner chosen as follows: it is taken to be V k ρ in (37) if κ(W k ) ≤ 10 3 ; otherwise, it is taken to be k ρ,-in (33). As mentioned in Remark 2, k ρ,is empirically observed to be more effective than k ρ,+ . A3. Algorithm IP-QSDP that employs PSQMR to solve (9) using the preconditioner V k ρ in (37). Recall that V k ρ is a variant of the preconditioner Ω k in (38). Even though we do not have any asymptotic result for eig((V k ρ ) -1 B k ρ ), we have found that V k ρ is typically much more effective than Ω k . A4. Algorithm IP-QSDP that employs PSQMR to solve <ref type="bibr" target="#b8">(9)</ref> with the preconditioner chosen as follows: it is taken to be V k ρ in (37) if κ(W k ) ≤ 10 3 ; otherwise, it is taken to be k ρ in (41).</p><p>When k ρ,-is chosen as the preconditioner, the eigenvalues of (W k ) -1 is partitioned according to the threshold value of 1. The linear operators H k 1 , H k 2 , H k 3 , and M k ρ in k ρ,-are chosen as in (36). When k ρ is chosen as the preconditioner, the eigenvalues of (W k ) -1 is also partitioned according to the threshold value of 1. The linear operators H k 1 , H k 2 , and H k 3 in k ρ are chosen to have the forms in (42) and they are constructed from Procedure SKPA described in Sect. 5 with ∆ ρ = Q ρ I n . Note that such a choice of ∆ ρ is particularly attractive because the resulting constituent matrices</p><p>ρ is chosen as the preconditioner, the operator Q(X) = U • X is approximated by ∆ ∆, with ∆ = diag(u), where u ∈ I R n + is the vector such that Uuu T F = min.</p><p>For the test problems in E1-E8, the main computational cost at each PSQMR step lies in n × n matrix-matrix multiplications, and they are summarized as follows. The cost per PSQMR step for A1 is 2n 3 flops; the corresponding cost for A2 or A3 is 6n 3 flops; and for A4, it is 6n 3 and 10n 3 flops when the preconditioner is V k ρ and k ρ , respectively. For these problems, evaluating A(X), A T y, Q(X) cost at most O(n 2 ) flops, and these terms are ignored.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Solving Euclidean distance matrix completion problems via semidefinite programming</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Alfakih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Complementarity and nondegeneracy in semidefinite programming</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><forename type="middle">A</forename><surname>Haeberly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="111" to="128" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Primal-dual interior-point methods for semidefinite programming: convergence results, stability and numerical results</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><forename type="middle">A</forename><surname>Haeberly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="746" to="768" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semidefinite programming approach for the nearest correlation matrix problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Anjos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Takouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Report</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Combinatorics and Optimization, University of Waterloo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Numerical solution of saddle point problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numer</title>
		<imprint>
			<biblScope unit="page" from="1" to="137" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the eigenvalues of a class of saddle point matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Simoncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="173" to="196" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preconditioning indefinite systems in interior point methods for optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bergamaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gondzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="149" to="171" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucl. Acids Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Norm inequalities for partitioned operators and an application</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kittaneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Ann</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="page" from="719" to="726" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Solving a class of semidefinite programs via nonlinear programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D C</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="97" to="122" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new Krylov-subspace method for symmetric indefinite linear systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nachtigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th IMACS World Congress on Computational and Applied Mathematics</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Ames</surname></persName>
		</editor>
		<meeting>the 14th IMACS World Congress on Computational and Applied Mathematics<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting sparsity in primal-dual interior-point method for semidefinite programming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fujisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="235" to="253" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Limiting behaviour of the central path in semidefinite optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Halicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Klerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Roos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing the nearest correlation matrix-a problem from finance</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="329" to="343" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constraint preconditioning for indefinite linear systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wathen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1300" to="1317" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interior-point methods for the monotone linear complementarity problem in symmetric matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shindoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="86" to="125" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local compliance estimation via positive semidefinite constrained least sqaures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krislock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Varah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1007" to="1011" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Kronecker product approximate preconditioner for SANs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="723" to="752" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superlinear convergence of a symmetric primal-dual path following algorithm for semidefinite programming</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="59" to="81" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A dual approach to semidefinite least-squares problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="272" to="284" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Implementation of primal-dual methods for semidefinite programming based on Monteiro and Tsuchiya Newton directions and their variants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D C</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Zanjácomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Methods Softw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="91" to="140" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A predictor-corrector algorithm for QSDP combining Dikin-type and Newton centering steps</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient diagonal preconditioner for finite element solution of Biot&apos;s consolidation equations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Phoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Methods in Eng</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="377" to="400" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A quadratically convergent Newton method for computing the nearest correlation matrix</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="360" to="385" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Iterative Methods for Sparse Linear Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>PWS Publishing Company</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the Nesterov-Todd direction in semidefinite programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Tütüncü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="769" to="796" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Block preconditioners for symmetric indefinite linear systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Phoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Methods Eng</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1361" to="1381" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inexact primal-dual path-following algorithms for a special class of convex quadratic SDP and related problems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Tütüncü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific J. Optim</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The ubiquitous Kronecker product</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The convergence of iterative solution methods for symmetric and indefinite linear systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wathen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Silvester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical Analysis</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Griffiths</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison Wesley Longman, Harlow</publisher>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="page" from="230" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Polynomiality of an inexact infeasible interior point algorithm for semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="261" to="282" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
