<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training via Denoising for Molecular Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-31">31 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sheheryar</forename><surname>Zaidi</surname></persName>
							<email>szaidi@stats.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Schaarschmidt</surname></persName>
							<email>mschaarschmidt@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">Battaglia</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">Godwin</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neural</forename><surname>Graph</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Network</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training via Denoising for Molecular Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-31">31 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.00133v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique that utilizes large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Inspired by recent advances in noise regularization, our pre-training objective is based on denoising. Relying on the well-known link between denoising autoencoders and score-matching, we also show that the objective corresponds to learning a molecular force field -arising from approximating the physical state distribution with a mixture of Gaussians -directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -dataset sizes, model size and architecture, and the choice of upstream and downstream datasets -on pre-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of the best performing neural networks in vision and natural language processing (NLP) relies on pre-training the models on large datasets to learn meaningful features for downstream tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>. For molecular property prediction from 3D structures (a point cloud of atomic nuclei in R 3 ), the problem of how to similarly learn such representations remains open. For example, none of the best models on the widely used QM9 benchmark use any form of pre-training <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b22">23]</ref>, in stark contrast with vision and NLP. Effective methods for pre-training could have a significant impact on fields such as drug discovery and material science.</p><p>In this work, we focus on the problem of how large datasets of 3D molecular structures can be utilized to improve performance on downstream molecular property prediction tasks that also rely on 3D structures as input. We address the question: how can one exploit large datasets like PCQM4Mv2 3 , that contain over 3 million structures, to improve performance on datasets such as DES15K that are orders of magnitude smaller? Our answer is a form of self-supervised pre-training that generates useful representations for downstream prediction tasks. Inspired by recent advances in noise regularization for graph neural networks (GNNs) <ref type="bibr" target="#b22">[23]</ref>, our pre-training objective is based on denoising in the space of structures (and is hence self-supervised).</p><p>Unlike existing pre-training methods, which largely focus on 2D graphs, our approach targets the setting where the downstream task involves 3D point clouds defining the molecular structure. Relying on the well-known connection between denoising and score-matching <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b24">25]</ref>, we also show that the denoising objective is equivalent to (approximate) learning of a particular force field, shedding light on how it aids representation learning.</p><p>The contributions of our work are summarized as follows:</p><p>• We investigate a simple and effective method for pre-training via denoising in the space of 3D structures with the aim of improving downstream molecular property prediction from such 3D structures. Our denoising objective is shown to be related to learning a specific force field. • Our experiments demonstrate that pre-training via denoising significantly improves performance on multiple challenging datasets that vary in size, nature of task, and molecular composition. This establishes that denoising over structures successfully transfers to molecular property prediction, setting, in particular, a new state-of-the-art on 10 out of 12 targets in the widely used QM9 dataset. Figure <ref type="figure" target="#fig_0">1</ref> illustrates performance on one of the targets in QM9. • We make improvements to a common GNN architecture, in particular showing how to apply Tailored Activation Transformation (TAT) <ref type="bibr" target="#b79">[80]</ref> to Graph Network Simulators (GNS) <ref type="bibr" target="#b49">[50]</ref>, which is complementary to pre-training and further boosts performance. • We analyze the benefits of pre-training by gaining insights into the effects of dataset size, model size and architecture, and the relationship between the upstream and downstream datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pre-training of GNNs. Various recent works have formulated methods for pre-training using graph data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b30">31]</ref>, rather than 3D point clouds of atom nuclei as in this paper. Approaches based on contrastive methods rely on learning representations by contrasting different views of the input graph <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b36">37]</ref>, or bootstrapping <ref type="bibr" target="#b64">[65]</ref>. Autoregressive or reconstruction-based approaches, such as ours, learn representations by requiring the model to predict aspects of the input graph <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Most methods in the current literature are not designed to handle 3D structural information, focusing instead on 2D graphs. The closest work to ours is GraphMVP <ref type="bibr" target="#b36">[37]</ref>, where 3D structure is treated as one view of a 2D molecule for the purpose of upstream contrastive learning. Their work focuses on downstream tasks that only involve 2D information, while our aim is to improve downstream models for molecular property prediction from 3D structures.</p><p>Denoising, representation learning and score-matching. Noise has long been known to improve generalization in machine learning <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b6">7]</ref>. Denoising autoencoders have been used to effectively learn representations by mapping corrupted inputs to original inputs <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>. Specific to GNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b9">10]</ref>, randomizing input graph features has been shown to improve performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51]</ref>. Applications to physical simulation also involve corrupting the state with Gaussian noise <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45]</ref>. Our work builds on Noisy Nodes <ref type="bibr" target="#b22">[23]</ref>, which incorporates denoising as an auxiliary task to improve performance, indicating the effectiveness of denoising for molecular property prediction (cf. Section 3.2.2).</p><p>Denoising is also closely connected to score-matching <ref type="bibr" target="#b69">[70]</ref>, which has become popular for generative modelling <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b56">57]</ref>. We also rely on this connection to show that denoising structures corresponds to learning a force field.</p><p>Equivariant neural networks for 3D molecular property prediction. Recently, the dominant approach for improving models for molecular property prediction from 3D structures has been through the design of architectures that incorporate roto-translational inductive biases into the model, such that the outputs are invariant to translating and rotating the input atomic positions. A simple way to achieve this is to use roto-translation invariant features as inputs, such as inter-atomic distances <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b67">68]</ref>, angles <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b37">38]</ref>, or coordinates with respect to the principal axes of inertia <ref type="bibr" target="#b22">[23]</ref>. There is also broad literature on equivariant neural networks, whose intermediate activations transform accordingly with roto-translations of inputs thereby naturally preserving inter-atomic distance and orientation information. Such models can be broadly categorized into those that are specifically designed for molecular property prediction <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41]</ref> and general-purpose architectures <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref>. Our pre-training technique is architecture-agnostic, and we show that it can be applied to enhance performance in both a GNN-based architecture <ref type="bibr" target="#b49">[50]</ref> and a Transformer-based one <ref type="bibr" target="#b65">[66]</ref>. We conjecture that similar improvements will hold for other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setup</head><p>Molecular property prediction consists of predicting scalar quantities given the structure of one or more molecules as input. Specifically, each example in the dataset is a labelled set specified as follows: we are provided with a set of atoms S = {(a 1 , p 1 ), . . . , (a |S| , p |S| )}, where a i ∈ {1, . . . , 118} and p i ∈ R 3 are the atomic number and 3D position respectively of atom i in the molecule, alongside a label y ∈ R. Some molecular datasets (e.g. Open Catalyst 2020 <ref type="bibr" target="#b12">[13]</ref>) provide additional per-atom features such as tags for atoms fixed during reactions, but here we focus on widely shared features.</p><p>We assume that the model, which takes S as input, is any architecture consisting of a backbone, which first processes S to build a latent representation of it, followed by a vertex-level or graph-level "decoder", that returns per-vertex predictions or a single prediction for the whole input respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training via Denoising</head><p>Given a dataset of molecular structures at equilibrium, we pre-train the network by denoising the structures, which operates as follows. Let D structures = {S 1 , . . . , S n } denote the upstream dataset of structures, and let GNN θ denote a graph neural network with parameters θ which takes S ∈ D structures as input and returns per-vertex predictions GNN θ (S) = (ˆ 1 , . . . , ˆ |S| ). The precise parameterization of the models we consider in this work is described in Section 3.3 and Appendix A.</p><p>Starting with an input molecule S ∈ D structures , we perturb it by adding i.i.d. Gaussian noise to its atomic positions p i . That is, we create a noisy version of the molecule:</p><formula xml:id="formula_0">S = {(a 1 , p1 ), . . . , (a |S| , p|S| )}, where pi = p i + σ i and i ∼ N (0, I 3 ),<label>(1)</label></formula><p>The noise scale σ is a tuneable hyperparameter (an interpretation of which is given in Section 3.2.1). We train the model by treating it as a denoising autoencoder and minimizing the following loss with respect to θ:</p><formula xml:id="formula_1">E p( S,S) GNN θ ( S) − ( 1 , . . . , |S| ) 2 .</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>The distribution p( S, S) corresponds to sampling a structure S from D structures and adding noise to it according to Equation <ref type="bibr" target="#b0">(1)</ref>. Note that the model predicts the noise, not the original coordinates. Next, we motivate using denoising as our pre-training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Denoising as Learning a Force Field</head><p>Datasets in quantum chemistry are typically generated by minimizing expensive-to-compute interatomic forces with methods such as density functional theory (DFT) <ref type="bibr" target="#b42">[43]</ref>. We speculate that learning this force field would give rise to useful representations for downstream tasks, since molecular properties vary with forces and energy. Therefore, a reasonable pre-training objective would be one that involves learning the force field. Unfortunately, this force field is either unknown or expensive to evaluate, and hence it cannot be used directly for pre-training. An alternative is to approximate the data-generating force field with one that can be cheaply evaluated and use it to learn good representations -an approach we outline in this section. Using the well-known link between denoising autoencoders and score-matching <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref>, we can show that the denoising objective in Equation ( <ref type="formula" target="#formula_2">2</ref>) is equivalent to learning a particular force field directly from equilibrium structures with some desirable properties. For clarity, in this subsection we condition on and suppress the atom types and molecule size in our notation, specifying a molecular structure by its coordinates x ∈ R 3N (with N as the size of the molecule).</p><p>From the perspective of statistical physics, a structure x can be treated as a random quantity sampled from the distribution p physical (x) ∝ exp(−E(x)), where E(x) is the energy associated with x.</p><p>According to p physical , low energy structures have a high probability of occurring. Moreover, the per-atom forces are given by ∇ x log p physical (x) = −∇ x E(x), which is referred to as the force field.</p><p>Our goal is to learn this force field. However both the energy function E and distribution p physical are unknown, and we only have access to a set of equilibrium structures x 1 , . . . , x n that locally minimize the energy E. Since x 1 , . . . , x n are local maxima of the distribution p physical , our main approximation is to replace p physical with a mixture of Gaussians centered at the data:</p><formula xml:id="formula_3">p physical (x) ≈ q σ (x) := 1 n n i=1 q σ (x | x i ),</formula><p>where we define q σ (x</p><formula xml:id="formula_4">| x i ) = N (x; x i , σ 2 I 3N</formula><p>). This approximation captures the facts that p physical will have local maxima at the equilibrium structures and vary smoothly with x, and is computationally convenient. Learning the force field corresponding to q σ (x) now yields a score-matching objective:</p><formula xml:id="formula_5">E qσ(x) GNN θ (x) − ∇ x log q σ (x) 2 .<label>(3)</label></formula><p>As shown by Vincent <ref type="bibr" target="#b69">[70]</ref>, and recently applied to generative modelling <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b75">76]</ref>, this objective is equivalent to the denoising objective. Specifically, defining q 0 (x) = 1 n n i=1 δ(x = x i ) to be the empirical distribution and q σ (x, x) = q σ (x | x)q 0 (x), the objective in Equation ( <ref type="formula" target="#formula_5">3</ref>) is equivalent to:</p><formula xml:id="formula_6">E qσ(x,x) GNN θ (x) − ∇ x log q σ (x | x) 2 = E qσ(x,x) GNN θ (x) − x − x σ 2 2 . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>We notice that the RHS corresponds to the earlier denoising loss in Equation (2) (up to a constant factor of 1/σ applied to GNN θ that can be absorbed into the network). To summarize, denoising equilibrium structures corresponds to learning the force field that arises from approximating the distribution p physical with a mixture of Gaussians. Note that we can interpret the noise scale σ as being related to the sharpness of p physical or E around the local maxima x i . We also remark that the equivalence between Equation (3) and the LHS of Equation (4) does not require q σ (x | x i ) to be a Gaussian distribution <ref type="bibr" target="#b69">[70]</ref>, and other choices will lead to different denoising objectives, which we leave as future work. See Appendix E for technical caveats and details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Noisy Nodes: Denoising as an Auxiliary Loss</head><p>Recently, Godwin et al. <ref type="bibr" target="#b22">[23]</ref> also applied denoising as an auxiliary loss to molecular property prediction, achieving significant improvements on a variety of molecular datasets. In particular, their approach, called Noisy Nodes, consisted of augmenting the usual optimization objective for predicting y with an auxiliary denoising loss. They suggested two explanations for why Noisy Nodes improves performance. First, the presence of a vertex-level loss discourages oversmoothing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b16">17]</ref> of vertex/edge features after multiple message-passing layers -a common problem plaguing GNNs -because successful denoising requires diversity amongst vertex features in order to match the diversity in the noise targets i . Second, they argued that denoising can aid representation learning by encouraging the network to learn aspects of the input training distribution.</p><p>The empirical success of Noisy Nodes indicates that denoising can indeed result in meaningful representations. Since Noisy Nodes incorporates denoising only as an auxiliary task, the representation learning benefits of denoising are limited to the downstream training dataset on which it is used as an auxiliary task. Our approach is to apply denoising as a pre-training objective for training on another large (unlabelled) dataset of molecular structures to learn higher-quality representations, which results in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GNS and GNS-TAT</head><p>The main two models we consider in this work are Graph Net Simulator (GNS) <ref type="bibr" target="#b49">[50]</ref>, which is a type of GNN, and a better-performing variant we contribute called GNS-TAT. GNS-TAT makes use of a recently published network transformation method called Tailored Activation Transforms (TAT) <ref type="bibr" target="#b79">[80]</ref>, which has been shown to prevent certain degenerate behaviors at initialization in deep MLPs/convnets that are reminiscent of oversmoothing in GNNs (and are also associated with training difficulties). While GNS is not by default compatible with the assumptions of TAT, it can be made so by adopting a special GNN initialization scheme we call "Edge-Delta", which initializes to zero the weights that carry "messages" from vertices to edges. See Appendix A for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The goal of our experimental evaluation in this section is to answer the following questions. First, does pre-training a neural network via denoising improve performance on the downstream task compared to training from a random initialization? Second, how does the benefit of pre-training depend on the relationship between the upstream and downstream datasets? Our evaluation involves four realistic and challenging molecular datasets, which vary in size, compound compositions (organic or inorganic) and labelling methodology (DFT-or CCSD(T)-generated), as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Training Setup</head><p>PCQM4Mv2. The main dataset we use for pre-training is PCQM4Mv2 <ref type="bibr" target="#b41">[42]</ref> (license: CC BY 4.0), which contains 3,378,606 organic molecules, specified by their 3D structures at equilibrium (atom types and coordinates) calculated using DFT. An earlier version of this dataset without any 3D structures, called PCQM4M, was used for supervised pre-training <ref type="bibr" target="#b77">[78]</ref>, but to our knowledge, this is the first time the 3D structures from v2 have been used and in a self-supervised manner. Molecules in PCQM4Mv2 have around 30 atoms on average and vary in terms of their composition, with the dataset containing 22 unique elements in total. The molecules in PCQM4Mv2 only contain one label, unlike e.g. QM9, which contains 12 labels per molecule, however we do not use these labels as denoising only requires the structures. The large scale and diversity of PCQM4Mv2 hence makes it well-suited for pre-training via denoising.</p><p>QM9. Widely used as a molecular property prediction benchmark <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b22">23]</ref>, QM9 is a dataset <ref type="bibr" target="#b45">[46]</ref> (license: CCBY 4.0) with approximately 130,000 small organic molecules containing up to nine heavy C, N, O, F atoms, specified by their structures. Each molecule has 12 OC20. Open Catalyst 2020 (OC20, license: CC Attribution 4.0) <ref type="bibr" target="#b12">[13]</ref> is a recent large benchmark containing trajectories of interacting surfaces and adsorbates that are relevant to catalyst discovery and optimization. This dataset contains three tasks: predicting the relaxed state energy from the initial structure (IS2RE), predicting the relaxed structure from the initial structure (IS2RS) and predicting the energy and forces given the structure at any point in the trajectory (S2EF). For IS2RE and IS2RS, there are 460,000 training examples, where each data point is a trajectory of a surface-adsorbate molecule pair starting with a high-energy initial structure that is relaxed towards a low-energy, equilibrium structure. For S2EF, there are 113 million examples of (non-equilibrium) structures with their associated energies and per-atom forces. We explore different combinations of upstream and downstream tasks as described in Section 4.3.</p><p>DES15K. DES15K <ref type="bibr" target="#b17">[18]</ref> (license: CC0 1.0) is a small dataset containing around 15,000 interacting molecule pairs, specifically dimer geometries with non-covalent molecular interactions. Each pair is labelled with the associated interaction energy computed using the coupled-cluster method with single, double, and perturbative triple excitations (CCSD(T)) <ref type="bibr" target="#b2">[3]</ref>, which is widely regarded as the gold-standard method in electronic structure theory. Note that CCSD(T) is usually more expensive and accurate than DFT, which is used for all aforementioned datasets. Pre-trained models are fine-tuned to predict the interaction energies in DES15K.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> (right) shows what percentage of elements appearing in each of QM9, OC20 and DES15K also appear in PCQM4Mv2. Whereas QM9 is fully covered by PCQM4Mv2, we observe that DES15K contains some elements not contained in PCQM4Mv2 and less than &lt; 30% of elements in OC20 are contained in PCQM4Mv2. This is owing to the fact that surface molecules in OC20 are inorganic lattices, none of which appear in PCQM4Mv2. This suggests that we can expect least transfer from PCQM4Mv2 to OC20. We also compare PCQM4Mv2 and QM9 in terms of the molecular compositions, i.e. the number of atoms of each element, that appear in each. Due to presence of isomers, both datasets contain multiple molecules with the same composition. For each molecular composition in QM9, Figure <ref type="figure" target="#fig_1">2</ref> (left) shows its frequency in both QM9 and PCQM4Mv2. We observe that most molecular compositions in QM9 also appear in PCQM4Mv2. We also remark that since pre-training is self-supervised using only unlabelled structures, test set contamination is not possible -in fact, PCQM4Mv2 does not have most of the labels in QM9.</p><p>Training setup. GNS/GNS-TAT were implemented in JAX <ref type="bibr" target="#b7">[8]</ref> using Haiku and Jraph for models, and automap for distributed training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b53">54]</ref>. Unless specified otherwise, training was performed on TPU v3 devices <ref type="bibr" target="#b29">[30]</ref>, and models were evaluated using early stopping on V100 GPUs. All experiments were averaged over 3 seeds. Detailed hyperparameter and hardware settings can be found in Appendix C. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on QM9</head><p>We evaluate two variants of our model on QM9 in Table <ref type="table" target="#tab_0">1</ref>, GNS-TAT with Noisy Nodes trained from a random initialization versus pre-trained parameters. Pre-training is done on PCQM4Mv2 via denoising. For best performance on QM9, we found that using atom type masking and prediction during pre-training additionally helped <ref type="bibr" target="#b26">[27]</ref>. We fine-tune a separate model for each of the 12 targets, as usually done on QM9, using a single pre-trained model. This is repeated for three seeds (including pre-training). Following customary practice, hyperparameters, including the noise scale for denoising during pre-training and fine-tuning, are tuned on the HOMO target and then kept fixed for all other targets. We first observe that GNS-TAT with Noisy Nodes performs competitively with other models and significantly improves upon GNS with Noisy Nodes, revealing the benefit of the TAT modifications. Utilizing pre-training then further improves performance across all targets, achieving a new state-of-the-art compared to prior work for 10 out of 12 targets. Interestingly, for the electronic spatial extent target R 2 , we found GNS-TAT to perform worse than other models, which may be due to the optimal noise scale being different from that of other targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on OC20</head><p>Next, we consider the Open Catalyst 2020 benchmark focusing on the downstream task of predicting the relaxed energy from the initial structure (IS2RE). We compared GNS with Noisy Nodes trained from scratch versus using pre-trained parameters. We experimented with two options for pre-training:</p><p>(1) pre-training via denoising on PCQM4Mv2, and (2) pre-training via denoising on OC20 itself. For the latter, we follow Godwin et al.'s [2022] approach of letting the denoising target be the relaxed (equilibrium) structure, while the perturbed input is a random interpolation between the initial and relaxed structures with added Gaussian noise -this loosely corresponds to the IS2RS task with additional noise. As shown in Figure <ref type="figure" target="#fig_2">3</ref> (left), pre-training on PCQM4Mv2 offers no benefit for validation performance on IS2RE, however pre-training on OC20 leads to considerably faster convergence but the same final performance. The lack of transfer from PCQM4Mv2 to OC20 is likely due to the difference in nature of the two datasets and the small element overlap as discussed in Section 4.1 and Figure <ref type="figure" target="#fig_1">2</ref> (right). On the other hand, faster convergence from using parameters pre-trained on OC20 suggests that denoising learned meaningful features. Unsurprisingly, the final performance is unchanged since the upstream and downstream datasets are the same in this case, so pre-training with denoising is identical to the auxiliary task of applying Noisy Nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on DES15K</head><p>In our experiments so far, all downstream tasks were based on DFT-generated datasets. While DFT calculations are more expensive than using neural networks, they are relatively cheap compared to higher quality methods such as CCSD(T) <ref type="bibr" target="#b2">[3]</ref>. <ref type="foot" target="#foot_0">4</ref> In this section, we evaluate how useful pre-training on DFT-generated structures from PCQM4Mv2 is when fine-tuning on the recent DES15K dataset which contains higher quality CCSD(T)-generated interaction energies. Moreover, unlike QM9, inputs from DES15K are systems of two interacting molecules and the dataset contains only around 15,000 examples, rendering it more challenging. We compare the test performance on DES15K achieved by GNS-TAT with Noisy Nodes when trained from scratch versus using pre-trained parameters from PCQM4Mv2. As shown in Figure <ref type="figure" target="#fig_2">3</ref> (right), using Noisy Nodes significantly improves performance compared to training from scratch, with a further improvement resulting from using pre-training. This shows that pre-training on structures obtained through relatively cheap methods such as DFT can even be beneficial when fine-tuning on relatively expensive downstream datasets, where the number of training examples may be limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we analyze how different factors affect pre-training. We consider the impact of changing the architecture, varying the dataset sizes, varying the model size and fixing the features to only fine-tune the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pre-training a Different Architecture</head><p>To explore whether pre-training is beneficial beyond GNS/GNS-TAT, we applied pre-training via denoising to the TorchMD-NET architecture <ref type="bibr" target="#b65">[66]</ref>. TorchMD-NET is a transformer-based architecture whose layers maintain per-atom scalar features x i ∈ R F and vector features v i ∈ R 3×F , where F is the feature dimension, that are updated in each layer using a self-attention mechanism. We implemented denoising by using gated equivariant blocks <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b55">56]</ref> applied to the processed scalar and vector features. The resulting vector features are then used as the noise prediction. 3 ± 0.9 13.5 ± 0.9</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we evaluate the effect of adding Noisy Nodes and pre-training to the architecture on the HOMO and LUMO targets in QM9. Pre-training yields a similar boost in performance, allowing the model to achieve state-of-the-art results. We did however observe that pre-training on this architecture was sometimes unstable, with one of the three seeds diverging -we will explore the cause of this. We also note that the results shown for TorchMD-NET are from our runs using Thölke and De Fabritiis's <ref type="bibr">[2022]</ref> open-source code, which led to slightly worse results than their published ones (our pre-training results still outperform their published results). Our code for experiments on TorchMD-NET will be open-sourced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Varying Dataset Sizes</head><p>We also investigate how downstream test performance on the HOMO target in QM9 varies as a function of the number of upstream and downstream training examples. First, we compare the performance of GNS-TAT with Noisy Nodes either trained from scratch or using pre-trained parameters for different numbers of training examples from QM9; we also include the performance of just GNS-TAT. As shown in Figure <ref type="figure" target="#fig_3">4</ref> (left), pre-training improves the downstream performance for all dataset sizes. The difference in test MAE also grows as the downstream training data reduces. Second, we assess the effect of varying the amount of pre-training data while fixing the downstream dataset size for both GNS and GNS-TAT as shown in Figure <ref type="figure" target="#fig_3">4</ref> (middle). For both models, we find that downstream performance generally improves as upstream data increases, with saturating performance for GNS-TAT. More upstream data can yield better quality representations, the result of which is shown in the downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Varying Model Size</head><p>We study the benefit of pre-training as models are scaled up on large downstream datasets. Recall that the S2EF dataset in OC20 contains around 130 million DFT evaluations for catalytic systems, providing three orders of magnitude more training data than QM9. We compare the performance of four GNS models with sizes ranging from 10 million to 1.2 billion parameters scaled up by increasing the hidden layer sizes in the MLPs. Each is pre-trained via denoising using the trajectories provided for the IS2RE/IS2RS tasks as described in Section 4.3. We also compare this to a 130 million parameter variant of GNS trained from scratch. As shown in Figure <ref type="figure" target="#fig_3">4</ref> (right), the pre-trained models continue to benefit from larger model sizes.We also observe that pre-training is beneficial, as the model trained from scratch underperforms in comparison: the 130 million parameters model trained from scratch is outperformed by a pre-trained model of less than half the size (53 million parameters).</p><p>Sriram et al. <ref type="bibr" target="#b62">[63]</ref> also recently studied a custom training scheme to scale up atomic simulations by distributing training for DimeNet++ <ref type="bibr" target="#b31">[32]</ref> and GemNet <ref type="bibr" target="#b33">[34]</ref>. In contrast to their approach, scaling up GNS variants in our experiments did not require additional engineering as we rely on automap <ref type="bibr" target="#b53">[54]</ref> to automatically partition the model, optimizer state and inputs across available hardware. Automap is a novel JAX API and data-driven compiler developed at DeepMind to simplify large model training by allowing users to flexibly combine manual (e.g. data parallelism) and automated (e.g. model parallelism) forms of parallelism. In our experiments on billion parameter GNNs, we found automap to typically combine parameter sharding with input sharding, mirroring expert strategies. We leave more detailed analysis of our largest models to future work.  Finally, we perform an experiment to assess how useful the features learned by pre-training are if they are not fine-tuned for the downstream task but kept fixed instead. Specifically, on the HOMO target in QM9, we freeze the backbone of the model and fine-tune only the decoder (cf. Appendix A). To evaluate this, we compare it to using random parameters from initialization for the model's backbone, which allows us to isolate how useful the pre-trained features are. As described in Appendix A, the decoder is a simple module involving no message-passing. Figure <ref type="figure" target="#fig_5">5</ref> shows that only training the decoder while keeping the pre-trained parameters fixed results in test MAE of 40 meV, which is worse than fine-tuning the entire model but substantially better than the performance of &gt;100 meV in test MAE resulting from training the decoder when the remaining parameters are randomly initialized. This suggests that the features learned by denoising are more discriminative for downstream prediction than random features. We note that training only the decoder is also substantially faster than training the entire network -one batch on a single V100 GPU takes 15ms, which is 50× faster than one batch using 16 TPUs for the full network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Freezing Pre-trained Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations &amp; Future Work</head><p>We have shown that pre-training can significantly improve performance for various tasks. One additional advantage of pre-trained models is that they can be shared in the community, allowing practitioners to fine-tune models on their datasets. However, unlike vision and NLP, the architectures used in molecular property prediction currently vary widely and the community has not yet settled on a "standard" architecture, making pre-trained weights less reusable. We also observed that the success of pre-training inevitably depends on the relationship between the upstream and downstream datasets. In the context of molecular property prediction, understanding what aspects of the upstream data distribution must match the downstream data distribution for transfer is an important direction for future work. Lastly, as described in Section 5.1, we found that the TorchMD-NET architecture was occasionally unstable during pre-training, indicating that certain architectures might be better conditioned for denoising than others -we highlight this as future work. More generally, pre-training models on large datasets incurs a computational cost. However, our results show that pre-training for molecular property prediction does not require the same scale as large NLP and vision models. We discuss considerations on the use of compute and broader impact in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our work investigated pre-training neural networks by denoising in the space of 3D molecular structures. We showed that denoising is equivalent to learning a particular force field, motivating its ability to learn useful representations and shedding light on successful applications of denoising in other works <ref type="bibr" target="#b22">[23]</ref>. This technique enabled us to utilize existing large datasets of 3D structures for improving performance on various downstream molecular property prediction tasks from 3D structures. More broadly, this bridges the gap between the utility of pre-training in vision/NLP and molecular property prediction from structures. We hope that this approach will be particularly impactful for applications of deep learning to scientific problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Architectural Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Standard GNS</head><p>As our base model architecture we chose a Graph Net Simulator (GNS) <ref type="bibr" target="#b49">[50]</ref>, which consists of an ENCODER which constructs a graph representation from the input S, a PROCESSOR of repeated message passing blocks that update the latent graph representation, and a DECODER which produces predictions. Our implementation follows <ref type="bibr">Godwin et al.'s [2022]</ref> modifications to enable molecular and graph-level property predictions which has been shown to achieve strong results across different molecular prediction tasks without relying on problem-specific features.</p><p>In the ENCODER, we represent the set of atoms S = {(a 1 , p 1 ), (a 2 , p 2 ), . . . , (a |S| , p |S| )} as a directed graph G = (V, E) where V = {v 1 , v 2 , . . . , v |S| } and E = {e i,j } i,j are the sets of "featurized" vertices and edges, respectively. Edges e i,j ∈ E are constructed whenever the distance between the i-th and j-th atoms is less than the connectivity radius R cut , in which case we connect v i and v j with a directed edge e i,j from i to j that is a featurization of the displacement vector p j − p i . Meanwhile, for the i-th atom, v i is given by a learnable vector embedding of the atomic number a i .</p><p>The PROCESSOR consists of L message-passing steps that produce intermediate graphs G 1 , . . . , G L (with the same connectivity structure as the initial one). Each of these steps computes the sum of a shortcut connection from the previous graph, and the application of an Interaction Network <ref type="bibr" target="#b3">[4]</ref>. Interaction Networks first update each edge feature by applying an "edge update function" to a combination of the existing feature and the features of the two connected vertices. They then update each vertex feature by applying a "vertex update function" to a combination of the existing feature and the (new) edge features of incoming edges. In GNS, edge update functions are 3 hidden layer fullyconnected MLPs, using a "shifted softplus" (ssp(x) = log(0.5e x + 0.5)) activation function, applied to the concatenation of the relevant edge and vertex features, followed by a layer normalization layer. Vertex update functions are similar, but are applied to the concatenation of the relevant vertex feature and sum over relevant edge features.</p><p>In our implementation of GNS we applied the same PROCESSOR in sequence three times (with shared parameters), with the output of each being decoded to produce a prediction and corresponding loss value. The loss for the whole model is then given by the average of these. (Test-time predictions are meanwhile computed using only the output of the final PROCESSOR.)</p><p>The DECODER is responsible for computing graph-level and vertex-level predictions from the output of each PROCESSOR. Vertex-level predictions, such as noise as described in Section 3.2, are decoded using an MLP applied to each vertex feature. Graph-level predictions (e.g. energies) are produced by applying an MLP to each vertex feature, aggregating the result over vertices (via a sum), and then applying another MLP to the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 GNS with Tailored Activation Transformation (GNS-TAT)</head><p>Tailored Activation Transformation (TAT) <ref type="bibr" target="#b79">[80]</ref> is a method for initializing and transforming neural networks to make them easier to train, and is based on a similar method called Deep Kernel Shaping (DKS) <ref type="bibr" target="#b39">[40]</ref>. TAT controls the propagation of "q values", which are initialization-time approximations to dimension-normalized squared norms of the network's layer-wise activation vectors, and "c values", which are cosine similarities between such vectors (for different inputs). In other words, q values approximate z(x) 2 / dim(z(x)), where z(x) denotes a layer's output as a function of the network's input x, and c values approximate z(x) z(x )/( z(x) z(x ) ), where x is another possible network input. In standard deep networks, c values will converge to a constant value c ∞ ∈ [0, 1], so that "geometric information" is lost, which leads to training difficulties <ref type="bibr" target="#b39">[40]</ref>. DKS/TAT prevents this convergence through a combination of careful weight initialization, and transformations to the network's activation functions and sum/average layers.</p><p>Oversmoothing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b16">17]</ref> is a phenomenon observed in GNN architectures where vertex/edge features all converge to approximately the same value with depth, and is associated with training difficulties. It is reminiscent of how, when c ∞ = 1, feature vectors will converge with depth to a constant input-independent vector in standard deep networks. It therefore seems plausible that applying TAT to GNNs may help with the oversmoothing problem and thus improve training performance.   Unfortunately, the GNS architecture violates two key assumptions of TAT. Firstly, the sums over edge features (performed in the vertex update functions) violate the assumption that all sum operations must be between the outputs of linear layers with independently sampled initial weights. Secondly, GNS networks have multiple inputs for which information needs to be independently preserved and propagated to the output, while DKS/TAT assumes a single input (or multiple inputs whose representations evolve independently in the network).</p><p>To address these issues we introduce a new initialization scheme called "Edge-Delta", which initializes to zero the weights that multiply incoming vertex features in the edge update functions (and treats these weights as absent for the purpose of computing the initial weight variance). This approach is inspired by the use of the "Delta initialization" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b73">74]</ref> for convolutional networks in DKS/TAT, which initializes filter weights of the non-central locations to zero, thus allowing geometric information, in the form of c values, to propagate independently for each location in the feature map. When using the Edge-Delta initialization, edge features propagate independently of each other (and of vertex features), through what is essentially a standard deep residual network (with edge update functions acting as the residual branches), which we will refer to as the "edge network".</p><p>Given the use of Edge-Delta we can then apply TAT to GNS as follows <ref type="foot" target="#foot_1">5</ref> . First, we replace GNS's activation functions with TAT's transformed Leaky-ReLU activation functions (or "Tailored ReLUs"), which we compute with TAT's η parameter set to 0.8, and its "subnetwork maximizing function" defined on the edge network <ref type="foot" target="#foot_2">6</ref> . We also replace each sum involving shortcut connections with weighted sums, whose weights are 0.9 and √ 1 − 0.9 2 for the shortcut and non-shortcut branches respectively. We retain the use of layer normalization layers in the edge/vertex update functions, but move them to before the first fully-connected layer, as this seems to give the best performance. As required by TAT, we use a standard Gaussian fan-in initialization for the weights, and a zero initialization for the biases, with Edge-Delta used only for the first linear layer of the edge update functions. Finally, we replace the sum used to aggregate vertex features in the DECODER with an average. See Figures <ref type="figure" target="#fig_8">6  and 7</ref> for an illustration of these changes.</p><p>We experimented with an analogous "Vertex-Delta" initialization, which initializes to zero weights in the vertex update functions that multiply summed edge features, but found that Edge-Delta gave the best results. This might be because the edge features, which encode distances between vertices (and are best preserved with the Edge-Delta approach), are generally much more informative than the vertex features in molecular property prediction tasks. We also ran informal ablation studies, and found that each of our changes to the original GNS model contributed to improved results, with the use of Edge-Delta and weighted shortcut sums being especially important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Broader Impact</head><p>Who may benefit from this work? Molecular property prediction works towards a range of applications in materials design, chemistry, and drug discovery. Wider use of pre-trained models may accelerate progress in a similar manner to how pre-trained language or image models have enabled practitioners to avoid training on large datasets from scratch. Pre-training via denoising is simple to implement and can be immediately adopted to improve performance on a wide range of molecular property prediction tasks. As research converges on more standardized architectures, we expect shared pre-trained weights will become more common across the community.</p><p>Potential negative impact and ethical considerations. Pre-training models on large structure datasets incurs additional computational cost when compared to training a potentially smaller model with less capacity from scratch. Environmental mitigation should be taken into account when pretraining large models <ref type="bibr" target="#b43">[44]</ref>. However, the computational cost of pre-training can and should be offset by sharing pre-trained embeddings when possible. Moreover, in our ablations of upstream dataset sizes for GNS-TAT, we observed that training on a subset of PCQM4Mv2 was sufficient for strong downstream performance. In future work, we plan to investigate how smaller subsets with sufficient diversity can be used to minimize computational requirements, e.g. by requiring fewer gradient steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Setup and Compute Resources</head><p>Below, we list details on our experiment setup and hardware resources used.</p><p>GNS &amp; GNS-TAT. GNS-TAT training for QM9, PCQM4Mv2 and DES15K was done on a cluster of 16 TPU v3 devices and evaluation on a single V100 device. GNS training for OC20 was done on 8 TPU v4 devices, with the exception of the 1.2 billion parameters variant of the model, which was trained on 64 TPU v4 devices. Pre-training on PCQM4Mv2 was executed for 3 • 10 5 gradient updates (approximately 1.5 days of training). Fine-tuning experiments were run until convergence for QM9 (10 6 gradient updates taking approximately 2 days) and DES15K (10 5 gradient updates taking approximately 4 hours) and stopped after 5 • 10 5 gradient updates on OC20 (2.5 days) to minimize hardware use (the larger models keep benefiting from additional gradient updates).</p><p>TorchMD-NET. We implemented denoising for TorchMD-NET on top of Thölke and De Fabritiis's <ref type="bibr">[2022]</ref> open-source code. <ref type="foot" target="#foot_3">7</ref> Models were trained on QM9 using data parallelism over two NVIDIA RTX 2080Ti GPUs. Pre-training on PCQM4Mv2 was done using three GPUs to accommodate the larger molecules while keeping the batch size approximately the same as QM9. All hyperparameters except the learning rate schedule were kept fixed at the defaults. Instead of the default learning rate schedule consisting of 10K warm-up steps followed by PyTorch's ReduceLROnPlateau schedule, we used 50K warm-up steps followed by cosine annealing leading to a total of 300K steps. The longer warm-up improved learning stability and reduced (although it did not completely prevent) spikes in the training loss. We still observed that TorchMD-NET sometimes diverged during pre-training on PCQM4Mv2, the cause of which we are currently exploring.</p><p>Hyperparameter optimization. We note that effective pre-training via denoising requires sweeping noise values, as well as loss co-efficients for denoising and atom type recovery. For GNS/GNS-TAT, we relied on the hyperparameters published by Godwin et al. <ref type="bibr" target="#b22">[23]</ref> but determined new noise values for pre-training and fine-tuning by tuning over a grid of approximately 5 values each on PCQM4Mv2 and QM9 (for the HOMO target). We used the same values for DES15K without modification. We also ran a similar number of experiments to determine cosine cycle parameters for learning rates. where the Gaussian distributions q σ (x | x i ) are defined on V as:</p><formula xml:id="formula_8">q σ (x | x i ) = 1 (2πσ) (3N −3)/2 exp − 1 2σ 2 x − x i 2 .</formula><p>For convenience, we have expressed structures as vectors in the ambient space R 3N , however they are restricted to lie in the smaller space V . Note that the normalizing constant accounts for the fact that V is (3N − 3)-dimensional. As before, we define q 0 (x) = 1 n n i=1 δ(x = x i ) to be the empirical distribution and q σ (x, x) = q σ (x | x)q 0 (x). The score-matching objective is given by:</p><formula xml:id="formula_9">J 1 (θ) = E qσ(x) GNN θ (x) − ∇ x log q σ (x) 2 ,<label>(5)</label></formula><p>where the expectation is now over V . As shown by Vincent <ref type="bibr" target="#b69">[70]</ref>, minimizing the objective above is equivalent to the minimizing the following objective:</p><formula xml:id="formula_10">J 2 (θ) = E qσ(x,x) GNN θ (x) − ∇ x log q σ (x | x) 2 . (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>This is recognized as a denoising objective, because ∇ x log q σ (x | x) = (x − x)/σ 2 . A practical implication of this analysis is that the noise (x − x)/σ 2 ∈ V should be mean-centered, which is intuitive since it is impossible to predict a translational component in the noise.</p><p>We include a proof of the equivalence between Equations ( <ref type="formula" target="#formula_9">5</ref>) and ( <ref type="formula" target="#formula_10">6</ref>) for completeness: Proposition 1 (Vincent <ref type="bibr" target="#b69">[70]</ref>). The minimization objectives J 1 (θ) and J 2 (θ) are equivalent.</p><p>Proof. We first observe:</p><formula xml:id="formula_12">J 1 (θ) = E qσ(x) GNN θ (x) 2 − 2E qσ(x) [ GNN θ (x), ∇ x log q σ (x) ] + C 1 J 2 (θ) = E qσ(x) GNN θ (x) 2 − 2E qσ(x,x) [ GNN θ (x), ∇ x log q σ (x | x) ] + C 2 ,</formula><p>where C 1 , C 2 are constants independent of θ. Therefore, it suffices to show that the middle terms on the RHS are equal. Since expectations over q σ (x) and q σ (x, x) are restricted to V ⊆ R 3N , we apply a change of basis to write them as integrals against the (3N −3)-dimensional Lebesgue measure. Pick an orthonormal basis {v 1 , . . . , v 3N −3 } ⊆ R 3N for V and let P V = [v 1 , . . . , v 3N −3 ] ∈ R 3N ×3(N −1) be the projection matrix, so z = P V x expresses a mean-centered structure x in terms of the coordinates of the chosen basis for V . Noting that P V has orthonormal columns and that it yields a bijection between V and R 3N −3 , we calculate:</p><formula xml:id="formula_13">E qσ(x) [ GNN θ (x), ∇ x log q σ (x) ] = R 3N −3</formula><p>q σ (P V z) GNN θ (P V z), ∇ log q σ (P V z) dz = R 3N −3 q σ (P V z) GNN θ (P V z), ∇q σ (P V z)</p><formula xml:id="formula_14">q σ (P V z) dz = R 3N −3 GNN θ (P V z), ∇q σ (P V z) dz = R 3N −3 GNN θ (P V z), 1 n n i=1 ∇q σ (P V z | x i ) dz = R 3N −3 GNN θ (P V z), 1 n n i=1 q σ (P V z | x i )∇ log q σ (P V z | x i ) dz = R 3N −3 1 n n i=1</formula><p>q σ (P V z | x i ) GNN θ (P V z), ∇ log q σ (P V z | x i ) dz = E qσ(x,x) [ GNN θ (x), ∇ x log q σ (x | x) ]</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Schematic of pre-training via denoising. Right: A comparison of GNS-TAT pretrained on PCQM4Mv2 with various baselines on the HOMO target in QM9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Left: Frequency of compositions of molecules appearing in QM9 overlayed with the corresponding frequency in PCQM4Mv2. Each bar represents one molecular composition (e.g. one carbon atom, two oxygen atoms). Right: Percentage of elements appearing in QM9, DES15K, OC20 that also appear in PCQM4Mv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Validation performance curves on the OC20 IS2RE task (ood_both split). Right: Test performance curves for predicting interaction energies of dimer geometries in the DES15K dataset. "PT" and "NN" stand for pre-training and Noisy Nodes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Impact of varying the downstream dataset size for the HOMO target in QM9 with GNS-TAT. Middle: Impact of varying the upstream dataset size for the HOMO target in QM9. Right: Validation performance curves on the OC20 S2EF task (ood_both split) for different model sizes. "PT" and "NN" stand for pre-training and Noisy Nodes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>training w/ random features Decoder training w/ PT features Full model fine-tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Training only the decoder results in significantly better performance when using pre-trained features rather than random ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Diagram showing the edge update for a single step t of the PROCESSOR. Left: Edge update for GNS. Right: Edge update for GNS-TAT (with modifications shown in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Diagram showing the vertex update for a single step t of the PROCESSOR. Left: Vertex update for GNS. Right: Vertex update for GNS-TAT (with modifications shown in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on QM9 comparing the performance of GNS-TAT + Noisy Nodes (NN) with and without pre-training on PCQM4Mv2 with other baselines.</figDesc><table><row><cell cols="9">Target Unit SchNet E(n)-GNN DimeNet++ SphereNet PaiNN TorchMD-NET</cell><cell>GNS + NN</cell><cell>GNS-TAT + NN</cell><cell>Pre-trained GNS-TAT + NN</cell></row><row><cell>µ</cell><cell>D</cell><cell></cell><cell>0.033</cell><cell>0.029</cell><cell>0.030</cell><cell>0.027</cell><cell>0.012</cell><cell>0.011</cell><cell cols="2">0.025 0.021</cell><cell>0.016</cell></row><row><cell>α</cell><cell>a 0</cell><cell>3</cell><cell>0.235</cell><cell>0.071</cell><cell>0.043</cell><cell>0.047</cell><cell>0.045</cell><cell>0.059</cell><cell cols="2">0.052 0.047</cell><cell>0.040</cell></row><row><cell cols="4">HOMO meV 41.0</cell><cell>29.0</cell><cell>24.6</cell><cell>23.6</cell><cell>27.6</cell><cell>20.3</cell><cell>20.4</cell><cell>17.3</cell><cell>14.9</cell></row><row><cell cols="4">LUMO meV 34.0</cell><cell>25.0</cell><cell>19.5</cell><cell>18.9</cell><cell>20.4</cell><cell>18.6</cell><cell>17.5</cell><cell>17.1</cell><cell>14.7</cell></row><row><cell>∆</cell><cell cols="3">meV 63.0</cell><cell>48.0</cell><cell>32.6</cell><cell>32.3</cell><cell>45.7</cell><cell>36.1</cell><cell>28.6</cell><cell>25.7</cell><cell>22.0</cell></row><row><cell cols="2">R 2 a 0</cell><cell>2</cell><cell>0.07</cell><cell>0.11</cell><cell>0.33</cell><cell>0.29</cell><cell>0.07</cell><cell>0.033</cell><cell>0.70</cell><cell>0.65</cell><cell>0.44</cell></row><row><cell cols="4">ZPVE meV 1.700</cell><cell>1.550</cell><cell>1.210</cell><cell>1.120</cell><cell>1.280</cell><cell>1.840</cell><cell cols="2">1.160 1.080</cell><cell>1.018</cell></row><row><cell>U 0</cell><cell cols="3">meV 14.00</cell><cell>11.00</cell><cell>6.32</cell><cell>6.26</cell><cell>5.85</cell><cell>6.15</cell><cell>7.30</cell><cell>6.39</cell><cell>5.76</cell></row><row><cell>U</cell><cell cols="3">meV 19.00</cell><cell>12.00</cell><cell>6.28</cell><cell>7.33</cell><cell>5.83</cell><cell>6.38</cell><cell>7.57</cell><cell>6.39</cell><cell>5.76</cell></row><row><cell>H</cell><cell cols="3">meV 14.00</cell><cell>12.00</cell><cell>6.53</cell><cell>6.40</cell><cell>5.98</cell><cell>6.16</cell><cell>7.43</cell><cell>6.42</cell><cell>5.79</cell></row><row><cell>G</cell><cell cols="3">meV 14.00</cell><cell>12.00</cell><cell>7.56</cell><cell>8.00</cell><cell>7.35</cell><cell>7.62</cell><cell>8.30</cell><cell>7.41</cell><cell>6.90</cell></row><row><cell>c v</cell><cell cols="3">cal mol K 0.033</cell><cell>0.031</cell><cell>0.023</cell><cell>0.022</cell><cell>0.024</cell><cell>0.026</cell><cell cols="2">0.025 0.022</cell><cell>0.020</cell></row></table><note>different labels corresponding to different molecular properties, such as highest occupied molecular orbital (HOMO) energy and internal energy, which we use for fine-tuning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of TorchMD-NET with Noisy Nodes and pre-training on PCQM4Mv2.</figDesc><table><row><cell>Method</cell><cell>HOMO</cell><cell>LUMO</cell></row><row><cell cols="3">TorchMD-NET 22.0 ± 0.6 18.7 ± 0.4</cell></row><row><cell cols="3">+ Noisy Nodes 18.1 ± 0.1 15.6 ± 0.1</cell></row><row><cell>+ Pre-training</cell><cell>16.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>GNS-TAT hyperparameters for fine-tuning on QM9 and DES15K.</figDesc><table><row><cell>Parameter</cell><cell>Value or description</cell></row><row><cell>Gradient steps</cell><cell>10 6 QM9 / 10 5 DES15K</cell></row><row><cell>Optimizer</cell><cell>Adam with warm up and 1-cycle cosine decay schedule</cell></row><row><cell>β 1</cell><cell>0.9</cell></row><row><cell>β 2</cell><cell>0.95</cell></row><row><cell>Warm up steps</cell><cell>10 4</cell></row><row><cell>Warm up start learning rate</cell><cell>10 −5</cell></row><row><cell>Warm up max learning rate</cell><cell>10 −4</cell></row><row><cell>Cosine min learning rate</cell><cell>3 • 10 −7</cell></row><row><cell>Cosine cycle length</cell><cell>10 6 QM9 / 10 5 DES15K</cell></row><row><cell>Loss type</cell><cell>Mean squared error</cell></row><row><cell>Batch size</cell><cell>Dynamic to max edge/vertex/graph count</cell></row><row><cell>Max vertices in batch</cell><cell>256</cell></row><row><cell>Max edges in batch</cell><cell>3072</cell></row><row><cell>Max graphs in batch</cell><cell>8</cell></row><row><cell>Distance featurization</cell><cell>Bessel first kind (r min = 0, σ = 1.0)</cell></row><row><cell>Max edges per vertex</cell><cell>20</cell></row><row><cell>MLP number of layers</cell><cell>3</cell></row><row><cell>MLP hidden sizes</cell><cell>1024</cell></row><row><cell>Activation</cell><cell>Tailored ReLU (with negative slope chosen using TAT)</cell></row><row><cell>message passing layers</cell><cell>10</cell></row><row><cell>Block iterations</cell><cell>3</cell></row><row><cell cols="2">Vertex/edge latent vector sizes 512</cell></row><row><cell>Decoder aggregation</cell><cell>Mean</cell></row><row><cell>Position noise</cell><cell>Gaussian (µ = 0, σ = 0.05)</cell></row><row><cell>Parameter update</cell><cell>Exponentially moving average (EMA) smoothing</cell></row><row><cell>EMA decay</cell><cell>0.9999</cell></row><row><cell>Position loss coefficient</cell><cell>0.01</cell></row><row><cell>Atom type mask probability</cell><cell>0.0</cell></row><row><cell>Atom type loss coefficient</cell><cell>0.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">DFT methods tend to scale as O(n 2 ) or O(n 3 ), whereas CCSD(T) scales as O(n 7 ) where n is the size of the molecule.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">Note that Edge-Delta initialization is compatible with TAT, since for the purposes of q/c value propagation, zero-initialized connections in the network can be treated as absent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2">For the purposes of computing the subnetwork maximizing function we ignore the rest of the network and just consider the edge network. While the layer normalization layer (which we move before the MLP) technically depends on the vertex features, this dependency can be ignored as long as the q values of these features is 1 (which will be true given the complete set of changes we make to the GNS architecture).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3">Available on GitHub at: https://github.com/torchmd/torchmd-net.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Petar Veličković and Dan Belov for their helpful comments on the draft. We are grateful to DeepMind's model partitioning team for their support in using automap on large GNNs. Finally, we would also like to express our gratitude to Aron Cohen, Charlie Chen, David Pfau, Jovana Mitrovic, Nenad Tomasev and Victor Bapst for insightful discussions related to this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Hyperparameters</head><p>We report the main hyperparameters used for GNS and GNS-TAT below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Denoising as Learning a Force Field</head><p>We specify a molecular structure as x = (x (1) , . . . , x (N ) ) ∈ R 3N , where x (i) ∈ R 3 is the coordinate of atom i. Let E(x) denote the total (potential) energy of x, such that −∇ x E(x) are the forces on the atoms. As discussed in Section 3.2.1, learning the force field, i.e. the mapping x → −∇ x E(x), is a reasonable pre-training objective. Furthermore, learning the force field can be viewed as scorematching if we define the distribution p physical (x) ∝ exp(−E(x)) and observe that the score of p physical is the force field:</p><p>However, a technical caveat is that p physical is an improper probability density, because it cannot be normalized due to the translation invariance of E. Writing the translation of a structure as x + t := (x (1) + t, . . . , x (N ) + t) where t ∈ R 3 is a constant vector, we have E(x + t) = E(x). This implies that the normalizing constant R 3N p physical (x) dx diverges to infinity. To remedy this, we can restrict ourselves to the (3N − 3)-dimensional subspace</p><p>consisting of the mean-centered structures, over which p physical can be defined as a normalizable distribution.</p><p>Proceeding similarly as Section 3.2.1, let x 1 , . . . , x n ∈ V be a set of mean-centered equilibrium structures. For any x ∈ V , we now approximate p physical (x) ≈ q σ (x) := 1 n n i=1 q σ (x | x i ),</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lennox</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Duo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coupled-cluster theory in quantum chemistry</title>
		<author>
			<persName><forename type="first">Rodney</forename><forename type="middle">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Musiał</surname></persName>
		</author>
		<idno type="DOI">10.1103/RevModPhys.79.291</idno>
		<ptr target="https://link.aps.org/doi/10.1103/RevModPhys.79.291" />
	</analytic>
	<monogr>
		<title level="j">Rev. Mod. Phys</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="291" to="352" />
			<date type="published" when="2007-02">Feb 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>ArXiv, abs/1612.00222</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Çaglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelsey</forename><forename type="middle">R</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<idno>ArXiv, abs/1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Se(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mailoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kozinsky</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.03164</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to tikhonov regularization</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Geometric and physical quantities improve e (3) equivariant message passing</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Hesselink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elise</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02905</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A note on over-smoothing for graph neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2006.13318</idno>
		<ptr target="https://arxiv.org/abs/2006.13318" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open catalyst 2020 (oc20) dataset and community challenges</title>
		<author>
			<persName><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aini</forename><surname>Palizhati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwoong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ulissi</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscatal.0c04525</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Catalysis</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1909.03211</idno>
		<ptr target="http://arxiv.org/abs/1909.03211" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks with node transition probability-based message passing and dropnode regularization</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Huu Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName><surname>Deligiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page">114711</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantum chemical benchmark databases of gold-standard dimer interaction energies</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Donchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Taube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Decolvenaere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><surname>Hargus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Mcgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><forename type="middle">A</forename><surname>Gregersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je-Luen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Palmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bergdorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">L</forename><surname>Klepeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3165" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Se(3)-transformers: 3d rototranslation equivariant attention networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.10503</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Jraph: A library for graph neural networks in jax</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/jraph" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple GNN regularisation for 3d molecular property prediction and beyond</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=1wVvweK3oIb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Haiku: Sonnet for JAX</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/dm-haiku" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>arxiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Equivariant diffusion for molecule generation in 3d</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.17003" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Strategies for pre-training graph neural networks. arXiv: Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GPT-GNN: generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2006.15437</idno>
		<ptr target="https://arxiv.org/abs/2006.15437" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lietransformer: Equivariant self-attention for lie groups</title>
		<author>
			<persName><forename type="first">Charline</forename><forename type="middle">Le</forename><surname>Michael J Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheheryar</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjik</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4533" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayana</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.04760" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Bo Tian, Horia Toma, Erick Tuttle,</pubPlace>
		</imprint>
	</monogr>
	<note>datacenter performance analysis of a tensor processing unit. CoRR, abs/1704.04760</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv, abs/1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fast and uncertainty-aware directional message passing for non-equilibrium molecules</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankari</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">T</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>CoRR, abs/2011.14115</idno>
		<ptr target="https://arxiv.org/abs/2011.14115" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.03123</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gemnet: Universal directional graph neural networks for molecules</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.08903" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hy</forename><surname>Truong Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno>CoRR, abs/1801.02144</idno>
		<ptr target="http://arxiv.org/abs/1801.02144" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">N-gram graph: Simple unsupervised representation for graphs, with applications to molecules</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">F</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pre-training molecular graph representation with 3d geometry</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/2110.07728</idno>
		<ptr target="https://arxiv.org/abs/2110.07728" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Spherical message passing for 3d graph networks</title>
		<author>
			<persName><forename type="first">Yaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bora</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.05013</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/2103.00111</idno>
		<ptr target="https://arxiv.org/abs/2103.00111" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rapid training of deep neural networks without skip connections or normalization layers using deep kernel shaping</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.01765" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Relevance of rotationally equivariant convolutions for predicting molecular properties</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><forename type="middle">E</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><surname>Noé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08461</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pubchemqc project: A large-scale first-principles electronic structure database for data-driven chemistry</title>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomomi</forename><surname>Shimazaki</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.7b00083</idno>
		<idno type="PMID">28481528</idno>
		<ptr target="https://doi.org/10.1021/acs.jcim.7b00083" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1300" to="1308" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Density-Functional Theory of Atoms and Molecules</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">G</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Weitao</surname></persName>
		</author>
		<ptr target="http://www.amazon.com/Density-Functional-Molecules-International-Monographs-Chemistry/dp/0195092767/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1279096906&amp;sr=1-1" />
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Carbon emissions and large neural network training</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/2104.10350</idno>
		<ptr target="https://arxiv.org/abs/2104.10350" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning mesh-based simulation with graph networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.03409</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The truly deep graph convolutional networks for node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/1907.10903</idno>
		<ptr target="http://arxiv.org/abs/1907.10903" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Self-supervised graph transformer on large-scale molecular data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.02835" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>ArXiv, abs/1806.01242</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/sanchez-gonzalez20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">E(n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Automap: Towards ergonomic automated parallelism for ML models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schaarschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Alexander Rink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<idno>CoRR, abs/2112.02958</idno>
		<ptr target="https://arxiv.org/abs/2112.02958" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huziel</forename><surname>Enoc Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gastegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9377" to="9388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adeesh</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ulissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09575</idno>
		<title level="m">Rotation invariant graph neural networks using spin convolutions</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Creating artificial neural networks that generalize</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sietsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Generative Modeling by Estimating Gradients of the Data Distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
				<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ISBN 9781713829546</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards training billion parameter graph neural networks for atomic simulations</title>
		<author>
			<persName><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=0jP2n0YFmKG" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Bootstrapped representation learning on graphs</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<idno>CoRR, abs/2102.06514</idno>
		<ptr target="https://arxiv.org/abs/2102.06514" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Petar Velickovic, and Michal Valko</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Thölke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabritiis</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02541</idno>
		<title level="m">Torchmd-net: Equivariant transformers for neural network based molecular potentials</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno>CoRR, abs/1802.08219</idno>
		<ptr target="http://arxiv.org/abs/1802.08219" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Physnet: A neural network for predicting energies, forces, dipole moments, and partial charges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><surname>Meuwly</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jctc.9b00181</idno>
		<ptr target="http://dx.doi.org/10.1021/acs.jctc.9b00181" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<idno type="ISSN">1549-9626</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rklz9iAcKQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00142</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks</title>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5393" to="5402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>CoRR, abs/2102.10757</idno>
		<ptr target="https://arxiv.org/abs/2102.10757" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Geodiff: A geometric diffusion model for molecular conformation generation</title>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PzcvxEMzvQC" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13663</idno>
		<title level="m">Revisiting&quot; over-smoothing&quot; in deep gcns</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Do transformers really perform bad for graph representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep learning without shortcuts: Shaping the kernel with tailored rectifiers</title>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=U0k7XNTiFEq" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.12223</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Effective training strategies for deep graph neural networks</title>
		<author>
			<persName><forename type="first">Kuangqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno>CoRR, abs/2006.07107</idno>
		<ptr target="https://arxiv.org/abs/2006.07107" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
