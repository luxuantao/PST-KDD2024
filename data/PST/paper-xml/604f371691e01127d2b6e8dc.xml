<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-12">12 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sian-Yao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Cheng Kung University</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei-Ta</forename><surname>Chu</surname></persName>
							<email>wtchu@gs.ncku.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Cheng Kung University</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-12">12 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.07289v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In one-shot NAS, sub-networks need to be searched from the supernet to meet different hardware constraints. However, the search cost is high and N times of searches are needed for N different constraints. In this work, we propose a novel search strategy called architecture generator to search sub-networks by generating them, so that the search process can be much more efficient and flexible. With the trained architecture generator, given target hardware constraints as the input, N good architectures can be generated for N constraints by just one forward pass without re-searching and supernet retraining. Moreover, we propose a novel single-path supernet, called unified supernet, to further improve search efficiency and reduce GPU memory consumption of the architecture generator. With the architecture generator and the unified supernet, we propose a flexible and efficient one-shot NAS framework, called Searching by Generating NAS (SGNAS). With the pre-trained supernt, the search time of SGNAS for N different hardware constraints is only 5 GPU hours, which is 4N times faster than previous SOTA single-path methods.</p><p>After training from scratch, the top1-accuracy of SGNAS on ImageNet is 77.1%, which is comparable with the SO-TAs. The code is available at: https://github.com/ eric8607242/SGNAS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is time-consuming and difficult to manually design neural architectures under specific hardware constraints. Neural architecture search (NAS) <ref type="bibr" target="#b29">[30]</ref>[2] <ref type="bibr" target="#b0">[1]</ref> aiming at automatically searching the best neural architecture is thus highly demanded. However, how to efficiently and flexibly determine the architectures conforming to various constraints is still very challenging <ref type="bibr" target="#b3">[4]</ref>.</p><p>The earliest NAS methods were developed based on reinforcement learning (RL) <ref type="bibr" target="#b32">[33]</ref> <ref type="bibr" target="#b0">[1]</ref> or the evolution algorithm <ref type="bibr" target="#b29">[30]</ref>. However, extremely expensive computation is needed.</p><p>Figure <ref type="figure">1</ref>. Overview of SGNAS. Given the target hardware constraint as the input, the architecture generator can generate architecture parameters instantly within the inference time of one forward pass. With the generated parameters, the specific architectures can be sampled from the unified supernet.</p><p>For example, 2,000 GPU days are needed by an RL method <ref type="bibr" target="#b0">[1]</ref>, and 3,150 GPU days are needed by the evolution algorithm <ref type="bibr" target="#b29">[30]</ref>.</p><p>To improve searching efficiency, one-shot NAS methods <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b25">[26]</ref>[5] <ref type="bibr" target="#b36">[37]</ref> were proposed to encode the entire search space into an over-parameterized neural network, called a supernet. Once the supernet is trained, all sub-networks in the supernet can be evaluated by inheriting the weights of the supernet without additional training. One-shot NAS methods can be divided into two categories: differentiable NAS (DNAS) and single-path NAS.</p><p>In addition to optimizing the supernet, DNAS <ref type="bibr" target="#b15">[16]</ref>[26] <ref type="bibr" target="#b36">[37]</ref>[38] <ref type="bibr" target="#b27">[28]</ref> utilizes additional differentiable parameters, called architecture parameters, to indicate the architecture distribution in the search space. Because DNAS couples architecture parameters optimization with supernet optimization, for N different hardware constraints, the supernet and the architecture parameters should be trained jointly for N times to find N different best architectures. This makes DNAS methods inflexible.</p><p>In contrast, single-path methods <ref type="bibr" target="#b17">[18]</ref>[10] <ref type="bibr" target="#b8">[9]</ref>[40] decouple supernet training from architecture searching. For supernet training, only a single path consisting of one block in each layer is activated and is optimized in one iteration. The main idea is to simulate discrete neural architectures in the search space and save GPU memory consumption. Once the supernet is trained, different search strategies, like the evolution algorithm <ref type="bibr" target="#b39">[40]</ref> <ref type="bibr" target="#b17">[18]</ref>, can be used to search the architecture under different constraints without retraining the supernet. Single-path methods are thus more flexible than DNAS. However, re-executing the search strategy N times for N different constraints is costly and not flexible enough.</p><p>On top of one-shot NAS, we especially investigate efficiency and flexibility. For efficiency, we mean that, when the supernet is available, the time required to search the best architecture for a specific hardware constraint. For flexibility, we mean that, when N different hardware constraints are to be met, how much total time required to search for N best architectures. As a comparison instance, GreedyNAS <ref type="bibr" target="#b39">[40]</ref> takes almost 24 GPU hours to search for the best neural architecture under a specific constraint. Totally 24N GPU hours are required for N different constraints.</p><p>In this work, we focus on improving efficiency and flexibility of the search strategy of the single-path method. The main idea is searching the best architecture by generating it. First, we decouple supernet training from architecture searching and train the supernet as a single-path method. After obtaining the supernet, we propose to build an architecture generator to generate the best architecture directly. Given a hardware constraint as input, the architecture generator can generate the architecture parameter within the inference time of one forward pass. This method is extremely efficient and flexible. The total search time for various hardware constraints of the architecture generator is only 5 GPU hours. Moreover, we do not need to re-execute search strategies or re-train the supernet once the architecture generator is trained. When N different constraints are to be met, the search strategy only needs to be conducted once, which is more flexible than N searches required in previous single-path methods <ref type="bibr" target="#b9">[10]</ref>[40] <ref type="bibr" target="#b8">[9]</ref>.</p><p>The aforementioned idea is on top of a trained supernet. However, we notice that searching on a single-path supernet still requires a lot of GPU memory and time because of the huge number of supernet parameters and complex supernet structure. Previous single-path NAS methods <ref type="bibr" target="#b17">[18]</ref>[40] <ref type="bibr" target="#b8">[9]</ref> determine a block for each layer, and there may be different candidate blocks with various configurations. For example, GreedyNAS <ref type="bibr" target="#b39">[40]</ref> has 13 types of candidate blocks for each layer, and thus size of the search space is 13 L , where L denotes the total number of layers in the supernet. Inspired by the fine-grained supernet in AtomNAS <ref type="bibr" target="#b27">[28]</ref>, we propose a novel single-path supernet called unified supernet to reduce GPU memory consumption. In the unified supernet, we only construct a block called unified block in each layer. There are multiple sub-blocks in the unified block, and each sub-block can be implemented by different operations. By combining sub-blocks, all configurations can be described in a block. In this way, the number of parameters of the unified supernet is much fewer than previous single-path methods.</p><p>The contributions of this paper are summarized as follows. With the architecture generator and the unified supernet, we propose Searching by Generating NAS (SGNAS), which is a flexible and efficient one-shot NAS framework. We illustrate the process of SGNAS in Fig. <ref type="figure">1</ref>. Given various hardware constraints as the input, the architecture generator can generate the best architecture for different hardware constraints instantly in one forward pass. After training the best architecture from scratch, the evaluation results show that SGNAS achieves 77.1% top-1 accuracy on the Ima-geNet dataset <ref type="bibr" target="#b13">[14]</ref> at around 370M FLOPs, which is comparable with the state-of-the-arts of the single-path methods. Meanwhile, SGNAS outperforms SOTA single-path NAS in terms of efficiency and flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, one-shot NAS <ref type="bibr" target="#b25">[26]</ref>[37] <ref type="bibr" target="#b17">[18]</ref> has received much attention because of reduced search cost brought by supernet. To futher reduce the search cost, a number of methods have been proposed, which can be roughly divided into two types: efficient NAS and flexible NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Efficient NAS</head><p>For efficiency, many methods were proposed to improve the supernet training strategy or redesign the supernet architecture. Stamoulis et al. <ref type="bibr" target="#b31">[32]</ref> proposed a single-path supernet to encode architectures with shared convolutional kernel parameters, which reduce search cost of differential NAS. To reduce the huge cost when training on largescale datasets, training supernet and searching on proxy datasets like CIFAR10 or part of ImageNet was proposed in <ref type="bibr" target="#b36">[37]</ref> <ref type="bibr" target="#b35">[36]</ref>[21] <ref type="bibr" target="#b25">[26]</ref>[39] <ref type="bibr" target="#b25">[26]</ref>. PC-DARTS <ref type="bibr" target="#b37">[38]</ref> only sampled a small part of the supernet for training in each iteration to reduce computation cost. DA-NAS <ref type="bibr" target="#b11">[12]</ref> designed a dataadaptive pruning strategy for efficient architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Flexible NAS</head><p>For flexibility, in OFA <ref type="bibr" target="#b3">[4]</ref> a single full network is carefully trained. Sub-networks inherit weights from the network and can be directly deployed without training from scratch. An accuracy predictor is trained after training supernet to guide the process for searching a specialized subnetwork. FBNetV3 <ref type="bibr" target="#b12">[13]</ref> trained a predictor on a proxy dataset. The accuracy predictor estimates performance of a candidate sub-network. However, it is still time-consuming to train an accuracy predictor.</p><p>In this work, we focus on improving the search strategy in terms of both efficiency and flexibility. Note that our search strategy can be incorporated with the methods mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Searching by Generating NAS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Given a supernet A represented by weights w, to find an architecture that achieves the best performance while meeting a specific hardware constraint, we need to find the best sub-network a * from A which achieves the minimum validation loss L val (a, w). Sampling a from A is a nondifferentiable process. To optimize a by the gradient descent algorithm, DNAS <ref type="bibr" target="#b36">[37]</ref>[5] <ref type="bibr" target="#b25">[26]</ref> relaxes the problem as finding a set of continuous architecture parameters α, and computes the weighted sum of outputs of candidate blocks by the Gumbel softmax function <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_0">x l+1 = i m i l • b i l (x l ),<label>(1)</label></formula><formula xml:id="formula_1">m i l = exp(α i l + g i l /τ ) K k=1 exp(α k l + g k l /τ ) ,<label>(2)</label></formula><p>where x l is the input tensor of the lth layer, b i l is the ith block of the lth layer, and thus b i l (x l ) denotes the output of the ith block. The term α i l is the weight of the ith block in the lth layer. The term g i l is a random variable sampled from the Gumbel distribution Gumbel(0, 1) and τ is the temperature parameter. The value m i l is the weight for the output b i l (x l ). After relaxation, DNAS can be formulated as a bi-level optimization:</p><formula xml:id="formula_2">α * = min α L val (w * , α)<label>(3)</label></formula><formula xml:id="formula_3">s.t. w * = argmin w L train (w, α)<label>(4)</label></formula><p>where L train (w, α) is the training loss.</p><p>Because of the bi-level optimization of w and α, the best architecture α * sampled from the supernet is only suitable to a specific hardware constraint. With this searching process, for N different hardware constraints, the supernet and architecture parameters should be retrained for N times. This makes DNAS less flexible.</p><p>In contrast, single-path methods <ref type="bibr" target="#b17">[18]</ref>[10] <ref type="bibr" target="#b8">[9]</ref>[8] decouple supernet training from architecture searching. For supernet training, only a single path consisting of one block in each layer is activated and is optimized in one iteration to simulate discrete neural architecture in the search space. We can formulate the process as:</p><formula xml:id="formula_4">w * = argmin w E a∼Γ(A) (L train (w(a)))<label>(5)</label></formula><p>where w(a) denotes the subset of w corresponding to the sampled architecture a, and Γ(A) is a prior distribution of a ∈ A. The best weights w * to be determined are the ones yielding the minimum expected training loss. After training, the supernet is treated as a performance estimator to all architectures in the search space. With the pretrained supernet weights w * , we can search the best architecture a * :</p><formula xml:id="formula_5">a * = argmin a∈A L val (w * (a)).<label>(6)</label></formula><p>Single-path methods are more flexible than DNAS, because supernet training and architecture search are decoupled.</p><p>Once the supernet is trained, for N different constraints, only architecture search should be conducted for N times.</p><p>In this work, we propose to decouple supernet training from architecture searching and train supernet as in singlepath NAS (Eq. ( <ref type="formula" target="#formula_4">5</ref>)). After supernet training, we search the best architecture by the gradient descent algorithm as in DNAS (Eq. ( <ref type="formula" target="#formula_2">3</ref>)). Instead of training architecture parameters for one specific hardware constraint, we propose a novel search strategy called architecture generator to largely increase flexibility and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Essential Idea</head><p>Given the target hardware constraint C, the architecture generator can generate the best architecture parameters for the hardware constraint C. The process of the architecture generator can be described as α = G(C) such that Cost(α) &lt; C. With the architecture generator G, the objective function of the architecture searching in Eq. ( <ref type="formula" target="#formula_2">3</ref>) can be reformulated as :</p><formula xml:id="formula_6">G * = min G L val (w * , α).<label>(7)</label></formula><p>To make G * generate the best architecture parameters for different hardware constraints accurately, we propose the hardware constraint loss L C as:</p><formula xml:id="formula_7">L C (α, C) = (Cost(α) − C) 2 ,<label>(8)</label></formula><p>where the cost yielded by the generated architecture Cost(α) is estimated by:</p><formula xml:id="formula_8">Cost(α) = l i m i l • Cost(b i l ).<label>(9)</label></formula><p>The term Cost(b i l ) is the constant cost of the ith block in the l layer and m i l is the weight of different blocks described in Eq. ( <ref type="formula" target="#formula_1">2</ref>). The cost Cost(α) is differentiable with respect to m i l and α, similarly in <ref type="bibr">[37][21]</ref>. Note that Eq. ( <ref type="formula" target="#formula_8">9</ref>) is also highly correlated with latency, as mentioned in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b20">[21]</ref>. By combining the hardware constraint loss L C and the cross entropy loss L val defined in Eq. ( <ref type="formula" target="#formula_6">7</ref>), the overall loss of the architecture generator L G is:</p><formula xml:id="formula_9">L G = L val (w * , α) + λL C (α, C). (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where λ is a hyper-parameter to trade-off the validation loss and hardware constraint loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Accurate Generation with Random Prior</head><p>In practice, we found that the architecture generator easily overfits to a specific hardware constraint. The reason is that it is too difficult to generate complex and high-dimensional architecture parameters based on a given simple integer hardware constraint C.</p><p>To address this issue, a prior is given as input to stabilize the architecture generator. We randomly sample a neural architecture from the search space, and encode the neural architecture into a one-hot vector to be the prior knowledge of architecture parameters. We name it as a random prior B = B 1 , ..., B L . Formally, B l = one hot(a l ), l = 1, ..., L, where a l is the lth layer of the neural architecture a randomly sampled from A, and L is the total number of layers in the supernet. With the random prior, the architecture generator is to learn the residual from the random prior to the best architecture parameters, making training architecture generator more stable and accurately (blue line in Fig. <ref type="figure" target="#fig_8">7(a)</ref>), and the process of the architecture generator can be reformulated as α = G(C, B) such that Cost(α) &lt; C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">The Generator Training Algorithm</head><p>We illustrate the algorithm of architecture generator training in Algorithm 1. In each iteration, given the target constraint and the random prior, the architecture generator can generate the architecture parameters α (as illustrated in Fig. <ref type="figure">1</ref>). With α, the corresponding cost C α can be calculated by Eqn. <ref type="bibr" target="#b8">(9)</ref>. We can predict ŷ based on the pretrained supernet N with α. The total loss is given by Eqn. <ref type="bibr" target="#b9">(10)</ref>. No matter what constraint is given, the architecture generator generates architecture parameters to get the best prediction results. Therefore, training the generator is equivalent to searching the best architectures for various constraints in the proposed SGNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">The Architecture of the Generator</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the architecture of the generator. We set the channel size of all convolutional layers as 32 and set the stride as 1, making sure the output shape same as the shape of the random prior. Please refer to supplementary materials for detailed configurations of the architecture generator and random prior representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unified Supernet</head><p>Previous single-path NAS <ref type="bibr" target="#b9">[10]</ref>[9] <ref type="bibr" target="#b17">[18]</ref>[40] adopts the MobilenetV2 inverted bottleneck <ref type="bibr" target="#b30">[31]</ref> as the basic building block. Given the input tensor X, the corresponding output Y out is obtained by  Update G from gradients 10: end for where P cj ,ci denotes the pointwise convolution with the input channel size c i and output channel size c j , D K×K denotes the depthwise convolution with K × K kernel size. Eq. ( <ref type="formula" target="#formula_11">11</ref>) represents that X of c 1 channels is first expanded to a tensor of c 2 channels, which can be described as c 2 = e×c 1 , and then a depthwise convolution is conducted.</p><formula xml:id="formula_11">Y out = P c3,c2 (D K×K (P c2,c1 (X))),<label>(11)</label></formula><p>The term e denotes the expansion rate of the inverted bottleneck. After that, the tensor of c 2 channels is embedded into the output tensor of c 3 channels. Because one basic building block can only represent one configuration with one kernel size and one expansion rate, previous single-path NAS <ref type="bibr" target="#b4">[5]</ref>[9][10] needs to construct blocks of various configurations in each layer, which leads an exponential increase in parameter numbers and complexity of the supernet.</p><p>In this work, we propose a novel single-path supernet called unified supernet to improve efficiency and flexibility of the architecture generator. The only type of block, i.e., unified block, is constructed in each layer. The unified block is built with only the maximum expansion rate e max , i.e., c 2 = e max × c 1 .</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the idea of a unified block. To make the unified block represent all possible configurations, we replace the depthwise convolution D by e max sub-blocks, and each sub-block can be implemented by different operations or skip connection. The output tensor of the first pointwise convolution Y 1 is equally split into e max parts,   </p><formula xml:id="formula_12">d 1 (Y 1,1 ) • d 2 (Y 1,2 ) • • • • • d emax (Y 1,emax ),<label>(12)</label></formula><p>where • denotes the channel concatenation function.</p><p>With the sub-blocks implemented by different operations, we can simulate blocks with various expansion rates, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. The unified supernet thus can significantly reduce the parameters and GPU memory consumption. It is interesting that the MixConv in MixNet <ref type="bibr" target="#b34">[35]</ref> is a special case of our search space if different sub-blocks are implemented by different kernel sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Large Variability of BNs Statistics</head><p>As in <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b40">[41]</ref>, and <ref type="bibr" target="#b2">[3]</ref>, we suffer from the problem of unstable running statistics of batch normalization (BN). In the unified supernet, because one unified block would represent different expansion rates, the BN scales change more dramatically during training. To address the problem, BN recalibration <ref type="bibr" target="#b27">[28]</ref>[41][3] is used to recalculate the running statistics of BNs by forwarding thousands of training data after training. On the other hand, shadow batch normalization (SBN) <ref type="bibr" target="#b7">[8]</ref> or switchable batch normalization <ref type="bibr" target="#b41">[42]</ref> are used to stabilize BN. In this work, we utilize SBN to address the large variability issue, as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. In our setting, there are five different expansion rates, i.e., 2, 3, 4, 5, and 6. We thus take five BNs after the second pointwise convolution block to capture the BN statistics for different expansion rates. With SBN, we can capture different statistics and make supernet training more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Architecture Redundancy</head><p>Denote two sub-blocks as b 1 and b 2 . In the unified supernet, for example, the case of b 1 using 3×3 kernel size and b 2 using skip connection is distinct from the case of b 1 using skip connnection and b 2 using 3 × 3 kernel size. However these two cases actually correspond to the same sub-network, and thus the architecture redundancy problem arises. This redundancy makes the unified supernet more complex and hard to train. To address this issue, we force that skip connection can only be used in sub-blocks with higher index. For example, if we want to train a unified block with expansion rate 3, only the last three sub-blocks can be skip connection. We call this strategy forced sampling (FS). Please refer to supplementary materials for details of architecture redundancy and forced sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Experimental Settings</head><p>We adopt the macro structure of supernet (e.g., channel size of each layer and layer number) same as <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b4">[5]</ref>, but utilize the proposed unified blocks to reduce GPU memory consumption and the number of parameters. Each subblock in the unified blocks can be realized based on convolutional kernel sizes 3, 5, or 7, or the skip connection. We set the minimum and maximum expansion rates as 2 and 6, respectively. The size of our search space is 80 19 . Please refer to supplementary materials for more details.</p><p>For experiments on the ImageNet dataset <ref type="bibr" target="#b13">[14]</ref>, we train the unified supernet for 50 epochs using batch size 256 and adopt the stochastic gradient descent optimizer. The learning rate is decayed with the cosine annealing strategy <ref type="bibr" target="#b26">[27]</ref> from the initial value 0.045. After supernet training, the architecture generator is trained for 50 epochs by the Adam optimizer with the learning rate 0.001. After searching/generating the best architecture under hardware constraints, we adopt the RMSProp optimizer with 0.9 momentum <ref type="bibr" target="#b32">[33]</ref> to train the searched architecture from scratch. Learning rate is increased from 0 to 0.16 in the first 5 epochs with batch size 256, and then decays 0.03 every 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparison with Baselines</head><p>Li and Talwalkar <ref type="bibr" target="#b22">[23]</ref> presented that a random search approach usually achieves satisfactory performance. To make comparison, we randomly select 1,000 candidate architectures with FLOPs under 320 millions (320M) from the unified supernet and pick the architecture yielding the highest top-1 accuracy, as mentioned in <ref type="bibr" target="#b22">[23]</ref>. Besides, we also search the network with FLOPs under 320M by the evolution algorithm <ref type="bibr" target="#b17">[18]</ref> as another baseline. Table <ref type="table" target="#tab_1">1</ref> shows the comparison results. As can be seen, with around 320M FLOPs, the proposed SGNAS achieves the highest top-1 accuracy. Both baselines take around 34 GPU hours to complete the search. For N different hardware constraints, the search strategy should be re-executed for N times, and the search time of each of two baselines is 34N GPU hours totally. In contrast, SGNAS only takes 5 GPU hours totally for N different hardware constraints, which is much more efficient and flexible than the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison with SOTAs</head><p>This section is dedicated to compare with various SOTA one-shot NAS methods that utilize the augmented techniques (e.g., Swish activation function <ref type="bibr" target="#b28">[29]</ref> and Squeezeand-Excitation <ref type="bibr" target="#b19">[20]</ref>). We directly modify the searched architecture by replacing all ReLU activation with H-Swish <ref type="bibr" target="#b18">[19]</ref> activation and equip it with the squeeze-and-excitation module as in AtomNAS <ref type="bibr" target="#b27">[28]</ref>.</p><p>For comparison, similar to the settings in ScarletNAS <ref type="bibr" target="#b8">[9]</ref> and GreedyNAS <ref type="bibr" target="#b39">[40]</ref>, we search architectures under 275M, 320M, and 365M FLOPs, and denote the searched architecture as SGNAS-C, SGNAS-B, and SGNAS-A, respectively. The comparison results are shown in Table <ref type="table" target="#tab_2">2</ref>. The column "Train time" denotes that the time needed to train the supernet, and the column "Search time" denotes that the time needed to search the best architecture based on the pre-trained supernet. Because DNAS couples architecture searching with supernet optimization, we list the time needed for the entire pipeline in the "Search time" column.</p><p>As can be seen, our SGNAS is competitive with SOTAs in terms of top-1 accuracy under different FLOPs. For example, SGNAS-A achieves 77.1% top-1 accuracy, which outperforms ScarletNAS <ref type="bibr" target="#b8">[9]</ref> by 0.2%, outperforms MixNet-M <ref type="bibr" target="#b34">[35]</ref> by 0.1%, outperforms MixPath-A [8] by 0.2%, and is comparable with GreedyNAS-A <ref type="bibr" target="#b39">[40]</ref>. More importantly, SGNAS achieves much higher search efficiency. With the architecture generator and the unified supernet, even for N different architectures under N different hardware constraints, totally only 5 GPU hours are needed for SGNAS on a Tesla V100 GPU. However, Fair-NAS <ref type="bibr" target="#b9">[10]</ref>, GreedyNAS <ref type="bibr" target="#b39">[40]</ref>, and ScarletNAS <ref type="bibr" target="#b8">[9]</ref> need 48N , 24N , and 48N GPU hours, respectively, because of the cost of re-executing search. Supernet retraining is needed for FBNetV2 <ref type="bibr" target="#b35">[36]</ref> and AtomNAS <ref type="bibr" target="#b27">[28]</ref>, which makes search very inefficient.</p><p>Note that after finding the best architecture, training from scratch is required in most methods in Table <ref type="table" target="#tab_2">2</ref> (including SGNAS, except for AtomNAS <ref type="bibr" target="#b27">[28]</ref>). However, training a supernet that can be directly deployed to many constraints (like AtomNAS) needs expensive computation. Even with the time for training from scratch, SGNAS is still more efficient and flexible than AtomNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on NAS-Bench-201</head><p>To demonstrate efficiency and robustness of SGNAS more fairly, we evaluate it based on a NAS benchmark dataset called NAS-Bench-201 <ref type="bibr" target="#b16">[17]</ref>. NAS-Bench-201 includes 15,625 architectures in total. It provides full information of the 15,625 architectures (e.g., top-1 accuracy and FLOPs) on CIFAR-10, CIFAR-100, and ImageNet-16-120 datasets <ref type="bibr" target="#b6">[7]</ref>, respectively.</p><p>Based on the search space defined by NAS-Bench-201, we follow SETN <ref type="bibr" target="#b14">[15]</ref> to train the supernet by uniform sampling. After that, the architecture generator is applied to search architectures on the supernet. We search based on the CIFAR-10 dataset and look up the ground-truth performance of the searched architectures on CIFAR-10, CIFAR-100, and ImageNet-16-120 datasets, respectively. This process is run for three times, and the average performance is calculated as in Table <ref type="table" target="#tab_3">3</ref>. We see that the architectures searched by SGNAS outperform previous methods on both CIFAR-10 and ImageNet16-120. It is worth noting that,   with the supernet training strategy same as SETN <ref type="bibr" target="#b14">[15]</ref>, our result greatly surpasses SETN <ref type="bibr" target="#b14">[15]</ref> on all three datasets. Moreover, the required search time of SGNAS is only 2.5 GPU hours even for N different hardware constraints. We show the 15,625 architectures in NAS-Bench-201 on each dataset as gray dots in Fig. <ref type="figure" target="#fig_5">5</ref>, and draw the architectures searched by the architecture generator under different FLOPs as blue rectangles. After searching once, the architecture generator can generate all blue rectangles directly without re-searching. Moreover, various generated architectures approach the best among all choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance on Object Detection</head><p>To verify the transferability of SGNAS on object detection, we adopt the RetinaNet <ref type="bibr" target="#b23">[24]</ref> implemented in MMDetection <ref type="bibr" target="#b5">[6]</ref> to do object detection, but replace its backbone by the network searched by SGNAS. The models are trained and evaluated on the MS COCO dataset <ref type="bibr" target="#b24">[25]</ref> (train2017 and val2017, respectively) for 12 epochs with batch size 16 <ref type="bibr">[10][9]</ref>. We use the SGD optimizer with 0.9 momentum and 0.0001 weight decay. The initial learning rate is 0.01, and is multiplied by 0.1 at epochs 8 and 11. Table <ref type="table" target="#tab_4">4</ref> shows that SGNAS has better transferability than the baselines, especially in terms of mAP. The relationship between the number of supernet parameters and the number of candidate operations in each layer. In Fig. <ref type="figure" target="#fig_6">6</ref>(a), we randomly sample 360 architectures from the search space and illustrate the corresponding top-1 validation accuracies as blue dots. Moreover, we draw the architectures searched by SGNAS under different hardware constraint as red rectangles. As can be seen, the architectures searched by SGNAS are almost always the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Unified Supernet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Efficiency of Unified Supernet</head><p>To show efficiency of the proposed unified supernet, we report the relationship between the total number of parameters in the (unified) supernet and the number of candidate operations per layer in Fig. <ref type="figure" target="#fig_6">6(b)</ref>. For fair comparison, we calculate the number of parameters of different supernets all based on 19 layers. As can be seen, the number of possible operations of the unified supernet in each layer is 7 times larger than GreedyNAS <ref type="bibr" target="#b39">[40]</ref> and ScarletNAS <ref type="bibr" target="#b8">[9]</ref>, but the number of parameters needed to represent this unified supernet is only 1/6 times of them. The number of possible operations is 13 times larger than FairNAS <ref type="bibr" target="#b9">[10]</ref> and Proxy-lessNAS <ref type="bibr" target="#b4">[5]</ref>, but the number of supernet parameters for the unified supernet is only half of them. To compare under the same size of search space, we estimate the number of required supernet parameters in previous single-path methods <ref type="bibr" target="#b9">[10]</ref> Table <ref type="table" target="#tab_5">5</ref> shows the comparison in terms of GPU memory consumption and search time of the architecture generator when it works based on the unified supernet or based on the previous single-path supernet <ref type="bibr">[10][5]</ref>. Based on the unified supernet, the GPU memory consumption reduces to 12 GB and the search time is only 0.45 times of that based on the previous supernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Training Stabilization</head><p>Although the unified supernet largely reduces supernet parameters, the large search space makes the supernet hard to train. To study the effect of force sampling (FS) and shadow batch normalization (SBN) <ref type="bibr" target="#b7">[8]</ref> on supernet training, we train the supernet based on different settings, including baseline, with FS, with SBN, and with both FS and SBN. After training the supernet, we randomly sample 360 architectures from the search space and show the corresponding top-1 accuracies in Fig. <ref type="figure" target="#fig_6">6(a)</ref>. Without FS and SBN, because of large variability and complex architecture, the baseline supernet is hard to train. After utilizing SBN, variability can be well characterized, and the performance becomes more stable. After applying FS, complexity of the supernet is greatly reduced by reducing architecture redundancy. Performance is largely boosted when redundancy is reduced. With both FS and SBN, the unified supernet can more consistently represent architectures with better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Study of Random Priors</head><p>To enable the generator to generate architectures under various hardware constraints accurately, random prior is given as the input of the generator. In Fig. <ref type="figure" target="#fig_8">7</ref>(a), we show the correlation between the target FLOPs and the FLOPs of the generated architectures. With the random prior, the generator can generate architectures much more accurately. With the random prior, the Kendall tau correlation between the target FLOPs and the generated is 1, while the Pearson correlation is 0.99, which are significantly positive. We randomly sample four sub-networks A, B, C, and D from the unified supernet as four priors to train the architecture generator. Inherited from the weights of the unified supernet, the top-1 validation accuracy of these four subnetworks are 58.02%, 63.36%, 66.48%, and 68.48%, respectively. Fig. <ref type="figure" target="#fig_8">7</ref>(b) shows that, no matter starting from good priors or bad priors, the corresponding trained architecture generators are able to generate the architecture yielding the best performance. This shows that random priors are not to improve the top-1 accuracy, but to give reasonable priors to make the architecture generator generate good architectures under the target constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>To improve efficiency and flexibility of finding best subnetworks from the supernet under various hardware constraints, we propose the idea of architecture generator that searches the best architecture by generating it. This approach is very efficient and flexible for that only one forward pass is needed to generate good architectures for various constraints, comparing to previous one-shot NAS methods. To ease GPU memory consumption and boost searching, we propose the idea of unified supernet which consists of a stack of unified blocks. We show that the proposed one-shot framework, called SGNAS (searching by generating NAS), is extremely efficient and flexible by comparing with state-of-the-art methods. We further comprehensively investigate the impact of architecture generator and unified supernet from multiple perspectives. Please refer to supplementary materials for the limitation of SGNAS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Limitation</head><p>Careful hyperparameter tuning: In SGNAS, the overall loss function of the architecture generator is defined in Eq. <ref type="bibr" target="#b9">(10)</ref>. However, in our experiment, carefully tuning the hyperparameter λ for different datasets is required to get trade off between hardware constraints and performance.</p><p>Architecture of the architecture generator: In SG-NAS, we manually design architecture of the architecture generator. But we definitely believe that there is a better architecture for the generator. It is worth further study in the future. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 : 8 :</head><label>28</label><figDesc>Get a data batch X and y from D val 3: Randomly sample C target from [C L , C H ] 4: α = G(B, C target ) 5: C α = Cost(α) 6: ŷ = N (X, α) 7: L total = L val (y, ŷ) + λL C (C target , C α ) Calculate gradients from L total 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Structure of the architecture generator. Given the input target hardware constraint, the expansion layer expands the input to a tensor with shape same as the random prior.</figDesc><graphic url="image-2.png" coords="4,308.86,256.48,236.25,78.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of a unified block. Given the input, the first pointwise convolution expands the input channel size to emax times. The channel split layer splits the tensor into emax parts and feeds to each sub-block, respectively.</figDesc><graphic url="image-4.png" coords="5,50.11,200.95,236.25,58.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Different expansion rates can be simulated by sub-blocks with different operations. For example, the expansion rate 6 can be simulated if no skip connection is implemented, and the expansion rate 2 can be simulated if four skip connections are implemented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Y 1 , 1 ,</head><label>11</label><figDesc>Y 1,2 , ..., Y 1,emax . With the sub-blocks d i and split tensors Y 1,i , we can reformulate D K×K in Eq. (11) as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Search results of SGNAS on the CIFAR10, CIFAR100, and ImageNet16-120 datasets. (a) Result on CIFAR-10; (b) Result on CIFAR100; (c) Result on ImageNet16-120.</figDesc><graphic url="image-5.png" coords="7,52.60,183.84,75.60,72.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (a) Top-1 validation accuracy of randomly sampled architectures (blue dots) and the architectures searched by SGNAS (red rectangles). Performance of other variants is also shown. (b)The relationship between the number of supernet parameters and the number of candidate operations in each layer.</figDesc><graphic url="image-9.png" coords="7,436.69,198.55,118.12,93.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc><ref type="bibr" target="#b39">[40]</ref>[9]<ref type="bibr" target="#b4">[5]</ref> when the number of possible operations in each layer increases to 25, 50, 60, and 80, and show them by green squares. Fig.6(b)shows that the required parameters are significantly boosted when the number of possible operations increases, while the unified supernet avoids this intractability. Under the same size of search space, the number of needed parameters to represent the unified supernet is only 1/35 times of estimated supernets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) The relationship between target FLOPs and the FLOPs of generated architecture. (b) Performance of the architectures randomly sampled from the unified supernet (blue dots) and those generated by the architecture generator trained based on four random priors.</figDesc><graphic url="image-10.png" coords="8,311.35,78.99,115.75,84.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visualization of the architectures searched by SGNAS (SGNAS-A, SGNAS-B, and SGNAS-C). "MBE1" denotes the mobile inverted bottleneck convolution layers with expansion rate 1. "KX" denotes depthwise convolution with the kernel size X. The gray blocks are predefined blocks before searching.</figDesc><graphic url="image-13.png" coords="12,50.11,71.99,495.00,495.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Visualization of the architectures search by SGNAS under different hardware constraints.</figDesc><graphic url="image-14.png" coords="13,50.11,155.24,494.99,448.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison with baselines.</figDesc><table><row><cell>Search Strategy</cell><cell>Search Time (GPU hrs)</cell><cell>FLOPs (M)</cell><cell>Top-1 (%)</cell></row><row><cell>Random search</cell><cell>34N</cell><cell>322</cell><cell>74.63</cell></row><row><cell>Evolution algorithm</cell><cell>34N</cell><cell>318</cell><cell>74.67</cell></row><row><cell>SGNAS</cell><cell>5</cell><cell>324</cell><cell>74.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the SOTAs for different hardware constraints. † : training with AutoAugment<ref type="bibr" target="#b10">[11]</ref>. ‡ : searching on a proxy dataset. The unit of search time and train time is GPU hours.</figDesc><table><row><cell>Method</cell><cell>FLOPs (M)</cell><cell>Top-1 (%)</cell><cell>Train time</cell><cell>Search time</cell></row><row><cell>MobileNetV2 [31]</cell><cell>300</cell><cell>72.0</cell><cell>-</cell><cell>-</cell></row><row><cell>EfficientNet B0 [34]</cell><cell>390</cell><cell>76.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MixNet-M [35]</cell><cell>360</cell><cell>77.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MixPath-A [8]</cell><cell>349</cell><cell>76.9</cell><cell>240</cell><cell>-</cell></row><row><cell>AtomNAS-C [28]</cell><cell>363</cell><cell>77.6</cell><cell>0</cell><cell>816N</cell></row><row><cell>PC-DARTS [38]</cell><cell>597</cell><cell>75.8</cell><cell>0</cell><cell>91N</cell></row><row><cell>ScarletNAS-A [9]</cell><cell>365</cell><cell>76.9</cell><cell>240</cell><cell>48N</cell></row><row><cell>GreedyNAS-A [40]</cell><cell>366</cell><cell>77.1</cell><cell>168</cell><cell>∼ 24N</cell></row><row><cell>SGNAS-A (Ours)</cell><cell>373</cell><cell>77.1</cell><cell>280</cell><cell>5</cell></row><row><cell>FBNetV2-L1 [36]</cell><cell>325</cell><cell>77.2</cell><cell>0</cell><cell>600N  ‡</cell></row><row><cell>Proxyless-R [5]</cell><cell>320</cell><cell>74.6</cell><cell>0</cell><cell>200N</cell></row><row><cell>FairNAS-C [10]</cell><cell>325</cell><cell>76.7  †</cell><cell>240</cell><cell>48N</cell></row><row><cell>ScarletNAS-B [9]</cell><cell>329</cell><cell>76.3</cell><cell>240</cell><cell>48N</cell></row><row><cell>SPOS [18]</cell><cell>326</cell><cell>74.5</cell><cell>288</cell><cell>∼ 24N</cell></row><row><cell>GreedyNAS-B [40]</cell><cell>324</cell><cell>76.8</cell><cell>168</cell><cell>∼ 24N</cell></row><row><cell>SGNAS-B (Ours)</cell><cell>326</cell><cell>76.8</cell><cell>280</cell><cell>5</cell></row><row><cell>MobileNetV3-L [19]</cell><cell>219</cell><cell>75.2</cell><cell>-</cell><cell>-</cell></row><row><cell>ScarletNAS-C [9]</cell><cell>280</cell><cell>75.6</cell><cell>240</cell><cell>48N</cell></row><row><cell>GreedyNAS-C [40]</cell><cell>284</cell><cell>76.2</cell><cell>168</cell><cell>∼ 24N</cell></row><row><cell>SGNAS-C (Ours)</cell><cell>281</cell><cell>76.2</cell><cell>280</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on different datasets in the NAS-Bench-201 benchmark.</figDesc><table><row><cell>Method</cell><cell>Search Time (GPU hrs)</cell><cell>Val</cell><cell cols="2">CIFAR-10</cell><cell>Test</cell><cell>Val</cell><cell cols="2">CIFAR-100 Test</cell><cell>ImageNet-16-120 Val Test</cell></row><row><cell>optimal</cell><cell>N/A</cell><cell cols="2">91.61</cell><cell cols="2">94.37</cell><cell cols="2">73.49</cell><cell>73.51</cell><cell>46.77</cell><cell>47.31</cell></row><row><cell>RSPS [23]</cell><cell>2.6</cell><cell cols="4">84.16±1.69 87.66±1.69</cell><cell cols="3">59.00±4.60 58.33±4.34</cell><cell>31.56±3.28 31.14±3.88</cell></row><row><cell>DARTS [26]</cell><cell>2.2N</cell><cell cols="4">39.77±0.00 54.30±0.00</cell><cell cols="3">15.03±0.00 15.61±0.00</cell><cell>16.43±0.00 16.32±0.00</cell></row><row><cell>SETN [15]</cell><cell>7.9N</cell><cell cols="4">82.25±5.17 86.19±4.63</cell><cell cols="3">56.86±7.59 56.87±7.77</cell><cell>32.54±3.63 31.90±4.07</cell></row><row><cell>GDAS [16]</cell><cell>6.6N</cell><cell cols="4">90.00±0.21 93.51±0.13</cell><cell cols="3">71.14±0.27 70.61±0.26</cell><cell>41.70±1.26 41.84±0.90</cell></row><row><cell>SGNAS (Ours)</cell><cell>2.5</cell><cell cols="4">90.18±0.31 93.53±0.12</cell><cell cols="2">70.28±1.2</cell><cell>70.31±1.09</cell><cell>44.65±2.32 44.98±2.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison on the COCO object detection.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs(M) Top-1 (%)</cell><cell>mAP (%)</cell></row><row><cell>MobileNetV2  *  [31]</cell><cell>300</cell><cell>72.0</cell><cell>28.3</cell></row><row><cell>MixNet-M  *  [35]</cell><cell>360</cell><cell>77.0</cell><cell>31.3</cell></row><row><cell>FairNAS-A  *  [10]</cell><cell>392</cell><cell>77.5</cell><cell>32.4</cell></row><row><cell>Scarlet-A  *  [9]</cell><cell>365</cell><cell>76.9</cell><cell>31.4</cell></row><row><cell>MobileNetV2  †</cell><cell>300</cell><cell>72.0</cell><cell>29.4</cell></row><row><cell>SGNAS-A (Ours)</cell><cell>373</cell><cell>77.1</cell><cell>33.9</cell></row></table><note>† : Our implementation result. * reported in<ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison in terms of of GPU memory consumption and search time of the architecture generator.</figDesc><table><row><cell>Unified</cell><cell>Search</cell><cell>Batch</cell><cell>Search Time</cell><cell>Memory Cost</cell></row><row><cell>Supernet</cell><cell>Space</cell><cell>Size</cell><cell>(GPU hrs)</cell><cell>(GPU)</cell></row><row><cell>×</cell><cell>6 19</cell><cell>32 128</cell><cell>11</cell><cell>10.5GB 40 GB</cell></row><row><cell></cell><cell>80 19</cell><cell>32 128</cell><cell>5</cell><cell>9GB 28GB</cell></row><row><cell cols="3">5. Ablation Studies</cell><cell></cell><cell></cell></row><row><cell cols="3">5.1. Analysis of SGNAS</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was funded in part by Qualcomm through a Taiwan University Research Collaboration Project and in part by the Ministry of Science and Technology, Taiwan, under grants 108-2221-E-006-227-MY3, 107-2923-E-006-009-MY3, and 109-2218-E-002-015.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Details of Experimental Settings</head><p>A. <ref type="bibr">1. Dataset</ref> We perform all experiments based on the ImageNet dataset <ref type="bibr" target="#b13">[14]</ref>. Same as the settings in <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b39">[40]</ref>[9], we randomly sample 50,000 images (50 images for each class) from the training set as our validation set, and the rest is kept as the training set. The original validation set is taken as the test set to measure the final performance of each model. The resolution of input images is 224×224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Supernet Training</head><p>We train the unified supernet for 50 epochs using batch size 256 and adopt the stochastic gradient descent optimizer with a momentum of 0.9 and weight decay of 4×10 −5 . The learning rate is decayed based on the cosine annealing strategy from initial value 0.045. We train the unified supernet with strict fairness <ref type="bibr" target="#b9">[10]</ref> so that each operation in all subblocks and each expansion rate are trained fairly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Generator Training</head><p>After supernet training, the architecture generator is trained for 50 epochs using batch size 128 by the Adam optimizer with the learning rate 0.001, momentum (0.5, 0.999), and weight decay 0. The temperature τ of Gumbel Softmax <ref type="bibr" target="#b21">[22]</ref> in Eq. ( <ref type="formula">2</ref>) is initially set to 5.0 and annealed by a factor of 0.95 for each epoch. The trade-off parameter λ in Eq. ( <ref type="formula">11</ref>) is set to 0.0003 in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Search Space</head><p>The marco-architecture of our unified supernet is shown in Table <ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Details of Architecture Generator</head><p>A random prior is encoded into a one-hot format and then is reshaped into the shape of architecture parameters to be generated. The output of the architecture generator is a parameter map with size LayerSize × OperationN umber. Motivated by generative adversarial networks, we reshape a random prior so that its shape is the same as the output map. We then feed it to the architecture generator, where we can apply 2D convolution with stride 1 for processing. Without carefully tuning convolution parameters, we can ensure that shape of the output map fits different search spaces. This design is to make the generator easily adapt to different settings. We have experimented with various structures for the architecture generator (e.g., fully connected) and found that convolutional layers yield reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Details of Architecture Redundancy</head><p>We illustrate architecture redundancy in the left of Fig. <ref type="figure">8</ref> and forced sampling (FS) in the right of Fig. <ref type="figure">8</ref>. In the four  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization of Searched Architectures</head><p>We visualize SGNAS-A, SGNAS-B, and SGNAS-C in Fig. <ref type="figure">9</ref>. Besides, we also visualize the architectures searched by SGNAS under different hardware constraints in Fig. <ref type="figure">10</ref>. It is interesting that even if the target hardware constraint is low (e.g., 280M), the expansion rate simulated by subblocks is still high in some layers (e.g., layer 1, layer 7, and layer 19).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AOWS: Adaptive and optimal network width search with latency constraints</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2011">2019. 1, 3, 4, 5, 6, 8, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the CI-FAR datasets</title>
		<author>
			<persName><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1707.08819</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mixpath: A unified approach for one-shot neural architecture search</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05887</idno>
		<imprint>
			<date type="published" when="2008">2020. 3, 5, 6, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scarlet-nas: Bridging the gap between scalability and fairness in neural architecture search</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06022</idno>
		<imprint>
			<date type="published" when="2007">2019. 1, 2, 3, 4, 6, 7</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2007">2019. 1, 2, 3, 4, 5, 6, 7</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Da-nas: Data adapted pruning for efficient neural architecture search</title>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fbnetv3: Joint architecture-recipe search using neural acquisition function</title>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One-shot neural architecture search via self-evaluated template network</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nas-bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/CVF International Conference on Computer Vision</title>
				<meeting>IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tf-nas: Rethinking three search freedoms of latency-constrained differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2007">2019. 1, 2, 3, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AtomNAS: Finegrained end-to-end neural architecture search</title>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Single-path nas: Designing hardwareefficient convnets in less than 4 hours</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.02877</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mixed depthwise convolutional kernels</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mixconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
				<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fb-netv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pc-darts: Partial channel connections for memory-efficient architecture search</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cars: Continuous evolution for efficient neural architecture search</title>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Greedynas: Towards fast oneshot nas with greedy supernet</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 4, 6</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
