<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum Pre-training for End-to-End Speech Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
							<email>cywang@mail.nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
							<email>wu.yu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
							<email>yangzl@nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Curriculum Pre-training for End-to-End Speech Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end speech translation poses a heavy burden on the encoder, because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech-to-Text translation (ST) is essential to breaking the language barrier for communication. It aims to translate a segment of source language speech to the target language text. To perform this task, prior works either employ a cascaded method where an automatic speech recognition (ASR) model and a machine translation (MT) model are chained together or an end-to-end approach where a single model converts the source language audio sequence to the target language text sequence directly <ref type="bibr" target="#b8">(Berard et al., 2016)</ref>.</p><p>Due to the alleviation of error propagation and lower latency, the end-to-end ST model has been a hot topic in recent years. However, large paired data of source audios and target sentences is required to train such a model, which is not easy to satisfy for most language pairs. To address this issue, previous works resort to pre-training technique * Works are done during internship at Microsoft   <ref type="bibr" target="#b7">(Berard et al., 2018;</ref><ref type="bibr" target="#b4">Bansal et al., 2019)</ref>, where they leverage the available ASR and MT data to pre-train an ASR model and an MT model respectively, and then initialize the ST model with the ASR encoder and the MT decoder. This strategy can bring faster convergence and better results.</p><p>The end-to-end ST encoder has three inherent roles: transcribe the speech, extract the syntactic and semantic knowledge of the source sentence and then map it to a semantic space, based on which the decoder can generate the correct target sentence. This poses a heavy burden to the encoder, which can be alleviated by pre-training. However, we argue that the current pre-training method restricts the power of pre-trained representations. The encoder pre-trained on the ASR task mainly focuses on transcription, which learns the alignment between the acoustic feature with phonemes or words, and has no ability to capture linguistic knowledge or understand the semantics, which is essential for translation.</p><p>In order to teach the model to understand the arXiv:2004.10093v1 [cs.CL] 21 Apr 2020</p><p>sentence and incorporate the required knowledge, extra courses should be taken before learning translation. Motivated by this, we propose a curriculum pre-training method for end-to-end ST. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, we first teach the model transcription through ASR task. After that, we design two tasks, named frame-based masked language model (FMLM) task and frame-based bilingual lexicon translation (FBLT) task, to enable the encoder to understand the meaning of a sentence and map words in different languages. Finally, we fine-tune the model on ST data to obtain the translation ability.</p><p>For the FMLM task, we mask several segments of the input speech feature, each of which corresponds to a complete word. Then we let the encoder predict the masked word. This task aims to force the encoder to recognize the content of the utterance and understand the inner meaning of the sentence. In FBLT, for each speech segment that aligns with a complete word, whether or not it is masked, we ask the encoder to predict the corresponding target word. In this task, we give the model more explicit and strong cross-lingual training signals. Thus, the encoder has the ability to perform simple word translation and the burden on the ST decoder is greatly reduced. Besides, we adopt a hierarchical manner where different layers are guided to perform different tasks (first 8 layers for ASR and FMLM pre-training, and another 4 layers for FBLT pre-training). This is mainly because the three pre-training tasks have different requirements for language understanding and different output spaces. The hierarchical pre-training method can make the division of labor more clear and separate the incorporation of source semantic knowledge and cross-lingual alignments.</p><p>We conduct experiments on the LibriSpeech En-Fr and IWSLT18 En-De speech translation tasks, demonstrating the effectiveness of our pre-training method. The contributions of our paper are as follows: (1) We propose a novel curriculum pretraining method with three courses: transcription, understanding and mapping, in order to force the encoder to have the ability to generate necessary features for the decoder. (2) We propose two new tasks to learn linguistic features, FMLM and FBLT, which explicitly teach the encoder to do source language understanding and target language meaning mapping. (3) Experiments show that both the proposed courses are helpful for speech translation, and our proposed curriculum pre-training leads to significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speech Translation</head><p>Early work on speech translation used a cascade of an ASR model and an MT model <ref type="bibr" target="#b28">(Ney, 1999;</ref><ref type="bibr" target="#b25">Matusov et al., 2005;</ref><ref type="bibr" target="#b24">Mathias and Byrne, 2006)</ref>, which makes the MT model access to ASR errors. Recent successes of end-to-end models in the MT field <ref type="bibr" target="#b2">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b23">Luong et al., 2015;</ref><ref type="bibr" target="#b34">Vaswani et al., 2017)</ref> and the ASR fields <ref type="bibr" target="#b9">(Chan et al., 2016;</ref><ref type="bibr" target="#b10">Chiu et al., 2018)</ref> inspired the research on end-to-end speech-to-text translation system, which avoids error propagation and high latency issues.</p><p>In this research line, <ref type="bibr" target="#b8">Berard et al. (2016)</ref> give the first proof of the potential for an end-to-end ST model. After that, pre-training, multitask learning, attention-passing and knowledge distillation have been applied to improve the ST performance <ref type="bibr" target="#b0">(Anastasopoulos et al., 2016;</ref><ref type="bibr" target="#b12">Duong et al., 2016;</ref><ref type="bibr" target="#b7">Berard et al., 2018;</ref><ref type="bibr" target="#b38">Weiss et al., 2017;</ref><ref type="bibr" target="#b3">Bansal et al., 2018</ref><ref type="bibr" target="#b4">Bansal et al., , 2019;;</ref><ref type="bibr" target="#b33">Sperber et al., 2019;</ref><ref type="bibr" target="#b22">Liu et al., 2019;</ref><ref type="bibr" target="#b16">Jia et al., 2019)</ref>. However, none of them attempt to guide the encoder to learn linguistic knowledge explicitly. Recently, <ref type="bibr" target="#b36">Wang et al. (2019b)</ref> propose to stack an ASR encoder and an MT encoder as a new ST encoder, which incorporates acoustic and linguistic knowledge respectively. However, the gap between these two encoders is hard to bridge by simply concatenating the encoders. <ref type="bibr" target="#b18">Kano et al. (2017)</ref> propose structured-based curriculum learning for English-Japanese speech translation, where they use a new decoder to replace the ASR decoder and to learn the output from the MT decoder (fast track) or encoder (slow track). They formalize learning strategies from easier networks to more difficult network structures. In contrast, we focus on the curriculum learning in pre-training and increase the difficulty of pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Curriculum Learning</head><p>Curriculum learning is a learning paradigm that starts from simple patterns and gradually increases to more complex patterns. This idea is inspired by the human learning process and is first applied in the context of machine learning by <ref type="bibr" target="#b6">Bengio et al. (2009)</ref>. The study shows that this training approach results in better generalization and speeds up the convergence. Its effectiveness has been verified in multiple tasks, including shape recognition <ref type="bibr" target="#b6">(Bengio et al., 2009)</ref>, object classification <ref type="bibr" target="#b13">(Gong et al., 2016)</ref>, question answering <ref type="bibr" target="#b14">(Graves et al., 2017)</ref>, etc. However, most studies focus on how to control the difficulty of the training samples and organize the order of the learning data in the context of single-task learning.</p><p>Our method differs from previous works in two ways: (1) We leverage the idea of curriculum learning for pre-training. (2) We do not train the model on the ST task directly with more and more difficult training examples or use more and more difficult structures. Instead, we design a series of tasks with increased difficulty to teach the encoder to incorporate diverse knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overview of our training process is shown in Figure <ref type="figure" target="#fig_2">2</ref>. It can be divided into three steps: First, we train the model towards the ASR objective L ASR to learn transcription. We note this as the elementary course. Next, we design two advanced courses (tasks) to teach the model understanding a sentence and mapping words in two languages, named Frame-based Masked Language Model (FMLM) task and Frame-based Bilingual Lexicon Translation (FBLT) task. In the FMLM task, we mask some speech segments and ask the encoder to predict the masked words. In the FBLT task, we ask the encoder to predict the target word for each speech segment which corresponds to a complete source word. In this stage, the encoder is updated by L ADV . We adopt a hierarchical training manner where N encoder blocks are used to perform ASR and FMLM tasks, since they both require outputs in source word space, and N e blocks are used in FBLT task. After the two-phases pretraining, the encoder is finally combined with a new decoder or a pre-trained MT decoder to perform the ST task towards L ST .</p><p>Problem Formulation The speech translation corpus usually contains speech-transcriptiontranslation triples, denoted as S = {(x, y s , y t )}. Specially, x = (x 1 , • • • , x Tx ) is a sequence of acoustic features which are extracted from the speech signals.</p><formula xml:id="formula_0">y s = (y s 1 , • • • , y s Ts ) and y t = (y t 1 , • • • , y t Tt )</formula><p>represent the corresponding transcription in source language and the translation in target language respectively. To pre-train the encoder, an extra ASR dataset A = {(x, y s )} can be leveraged . Finally, the data for encoder pre-training is denoted as</p><formula xml:id="formula_1">{(x, y s )|(x, y s ) ∈ A ∨ (x, y s , y t ) ∈ S}</formula><p>After the encoder is pre-trained, we fine-tune the model using only S, to enable it generate y t from x directly. The model is updated using cross-entropy loss L ST = − log P (y t |x).</p><p>Model Architecture In this work, we adopt the architecture of Transformer as in <ref type="bibr" target="#b19">(Karita et al., 2019)</ref>. The encoder is a stack of two 3×3 2D CNN layers with stride 2 and N e Transformer encoder blocks. The CNN layers result in downsampling by a factor of 4. The decoder is a stack of N d Transformer decoder blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Elementary Course: Transcription</head><p>In the elementary course, we train an end-to-end ASR model, which has the similar architecture as the ST model. The ASR encoder consists of N blocks, and these blocks are used to initialize the bottom N blocks of the ST encoder. For the ASR task, we follow <ref type="bibr" target="#b19">Karita et al. (2019)</ref>, to employ a multi-task learning strategy, that is, both the E2E decoder and a CTC module predict the source sentence. Offline experiments indicate that the CTC objective is crucial for attentional encoder-decoder based ASR models. The final objective combines the CTC loss L ctc and the cross-entropy loss L CE :</p><formula xml:id="formula_2">LASR = αLCT C + (1 − α)LCE = −α log Pctc(y s |x) − (1 − α) log Ps2s(y s |x)<label>(1)</label></formula><p>In this work, we set α to 0.3. The CTC loss works on the encoder output and it pushes the encoder to learn frame-wise alignment between speech with words. The cross-entropy loss works on both the encoder and the ASR decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Advanced Courses: Understanding and Word Mapping</head><p>With the ability of transcription, we further propose two new tasks for the advanced courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Frame-based Masked Language Model</head><p>The design of the Frame-based Masked Language Model task is inspired by the Masked Language Model (MLM) objective of BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, and semantic mask for ASR task <ref type="bibr" target="#b35">(Wang et al., 2019a)</ref>. This task enables the encoder to understand the inner meaning of a segment of speech. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, we first perform forcealignment between the speech and the transcript sentence to determine where in time particular words occur in the speech segment. For each word y s i , we obtain its corresponding start position s i and the end position e i in the sequence x according to force alignment results. At each training iteration, we randomly sample some percentage of the words in the y s and denote the selected word set as ỹs . Next, for each selected token y s j in ỹs , we mask the corresponding speech piece [x s j : x e j ]. The masked utterance is denoted as x and used as input to the encoder:</p><formula xml:id="formula_3">h = Enc( x)<label>(2)</label></formula><p>After that, for a masked piece [x s j : x e j ], we average the corresponding output hidden states</p><formula xml:id="formula_4">[h s j 4 : h e j 4</formula><p>]<ref type="foot" target="#foot_0">1</ref> , and compute the distribution probability over source words as shown in follows:</p><formula xml:id="formula_5">hj = mean([h s j 4 : h e j 4 ]) (3) p(y s j | x) = softmax( hj • W ) (4)</formula><p>In practice, the sentence is represented in BPE tokens and W ∈ R d model ×|Vs| , where |V s | is the size of source vocabulary. In this way, a speech piece can be aligned with one or more tokens. We compute KL-Divergence loss as:</p><formula xml:id="formula_6">L F M LM = − y s j ∈ ỹs q(y s j )log p(y s j | x) q(y s j )</formula><p>(5) q(y s i ) ∈ R |Vs| is a distribution over all BPE tokens in source vocabulary V s and defined as:</p><formula xml:id="formula_7">q(y s j ) (pos) = 1/n j , V s [pos] ∈ y s j 0, otherwise.<label>(6)</label></formula><p>where pos represents the dimension index and n j is the total number of BPE tokens contained in word y s j .</p><p>In this work, we use a mask ratio of 15% following BERT and the masked speech piece is filled with the mean value of the whole utterance following <ref type="bibr" target="#b31">Park et al. (2019)</ref>. Because FMLM focuses on the understanding of source language, we computes its loss at the N -th layer of encoder (same with ASR loss), in the hope that the bottom N layers are only concerned with source language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Frame-based Bilingual Lexicon Translation</head><p>Aside from predicting masked source words, we go further to leverage cross-lingual information. Specifically, for each segment of speech features [x s i : x e i ] which aligned with a source word y s i , we assume we can obtain its target counterpart ỹt i . Similar to FMLM, we average the output hidden states from position s i 4 to e i 4 , and then compute the distribution probability over target vocabulary. The alignment between speech segments and target words is a many-to-many correspondence, so there are cases where ỹt i contains nothing or contains multiple foreign words. For the former case, we set the loss to zero, and for the latter case, we also compute KL-Divergence loss as:</p><formula xml:id="formula_8">L F BLT = − ỹt i q(ỹ t i )log p(ỹ t i | x) q(ỹ t i )<label>(7)</label></formula><p>The definition of q(ỹ t i ) is the length normalized distribution over all tokens appear in ỹt i . Note that the loss is computed on every speech segments, whether or not it is masked.</p><p>The only question remaining is how to obtain ỹt i for each speech segment. Since there are two types of data for pre-training, (x, y s , y t ) ∈ S and (x, y s ) ∈ A, we use two methods to get the alignment:</p><p>∀(x, y s , y t ) ∈ S, we simply run Moses<ref type="foot" target="#foot_1">2</ref> scripts to establish word alignments. It begins from running of GIZA++<ref type="foot" target="#foot_2">3</ref> to get source-to-target and targetto-source alignments, and then runs a heuristic grow-diag-final algorithm to get the final results, which means ∀y s i ∈ y s , we choose one word from its translation sentence as the corresponding word ∃ỹ t i ∈ y t s.t. ỹt i ∼ y s . Through the above alignment process, we can calculate a bilingual lexical translation table T with {(y s , y t )|(x, y s , y t ) ∈ S}, which estimates the translation probability between a source word w s i and a target word w t j , denoted as T = (w s i , w t j , p(w s i , w t j )). After that, ∀(x, y s ) ∈ A, we compute a ỹt i for each y s i in y s according to ỹt i = argmax w s j p(y s i , w s j ). We compute the L F BLT at the top layer of the encoder, indicating that the top N e − N layers are duty on bilingual word mapping. The final training objective in the advanced course combines FMLM and FBLT losses For references pre-processing, we tokenize and lowercase all the text with the Moses scripts. For pre-training tasks, the vocabulary is generated using sentencepiece <ref type="bibr" target="#b21">(Kudo and Richardson, 2018)</ref> with a fixed size of 5k tokens for all languages, and the punctuation is removed. For ST task, we normalize the punctuation using Moses and use the character-level vocabulary due to its better performance <ref type="bibr" target="#b7">(Berard et al., 2018)</ref>. Since there is no human-annotated segmentation provided in the IWSLT tst2013, we use two methods to segment the audios: 1) Following ESPnet, we segment each audio with the LIUM SpkDiarization tool <ref type="bibr" target="#b27">(Meignier and Merlin, 2010)</ref>. For evaluation, the hypotheses and references are aligned using the MWER method with RWTH toolkit <ref type="bibr" target="#b5">(Bender et al., 2004)</ref>. 2) We perform sentence-level forcealignment between audio and transcription using aeneas<ref type="foot" target="#foot_4">5</ref> tool and segment the audio according to alignment results.</p><formula xml:id="formula_9">L ADV = L F M LM + L F BLT<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Experiments are conducted in two settings: base setting and expanded setting. In base setting, only the corpus described in Section 4.1 is used for each task. In the expanded setting, additional ASR and/or MT data can be used. All results are reported on case-insensitive BLEU with the multibleu.perl script unless noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">End-to-End ST Baselines</head><p>We mainly compare our method with the conventional encoder pre-training method which uses only the ASR task to pre-train the encoder. Besides, we also compare with the results of the other works in the literature by copying their numbers. IWSLT: Since previous works use different segmentation methods and BLEU-score scripts, it is unfair to copy their numbers. In our work, we choose the ESPnet results as base setting baseline, the multilingual model and TCEN-LSTM model as expanded baselines. <ref type="bibr" target="#b15">Inaguma et al. (2019)</ref> use the same multilingual model as described in Lib-riSpeech baselines. And <ref type="bibr" target="#b36">Wang et al. (2019b)</ref> use an additional 272h TEDLIUM2 <ref type="bibr" target="#b32">(Rousseau et al., 2014)</ref> ASR corpus and 41M parallel data from WMT18 and WIT3<ref type="foot" target="#foot_6">7</ref> . All of them use ESPnet code, LIUM segmentaion method and multi-bleu.perl script. We follow <ref type="bibr" target="#b36">Wang et al. (2019b)</ref> to use another 272h ASR data for encoder pre-training and a subset of WMT18<ref type="foot" target="#foot_7">8</ref> for decoder pre-training. We use the same processing method for MT data, resulting in 4M parallel sentences in total. We also reimplement the CL-fast track of <ref type="bibr" target="#b18">Kano et al. (2017)</ref> using our model architecture and data as another baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Cacased Baselines</head><p>For LibriSpeech ST task, we use results of <ref type="bibr" target="#b7">Berard et al. (2018)</ref>, <ref type="bibr" target="#b15">Inaguma et al. (2019)</ref> and <ref type="bibr" target="#b22">Liu et al. (2019)</ref> as base cascaded baselines. The first two use LSTM models for ASR and MT. While the last work trains Transformer ASR and MT models. We build an expanded cascaded system with the pretrained Transformer ASR model and a LSTM MT model with the default setting in ESPnet recipe. For IWSLT ST task, we use <ref type="bibr" target="#b15">Inaguma et al. (2019)</ref> as base cascaded baseline, which is based on LSTM architecture. And we implement a Transformerbased baseline using our pre-trained ASR and MT models in the expanded setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>All our models are implemented based on ESPnet. We set the model dimension d model to 256, the head number H to 4, the feed forward layer size d f f to 2048. For LibriSpeech expanded setting, d model = 512 and H = 8. For all the ST models, we set the number of encoder blocks N e = 12 and the number of decoder blocks N d = 6. Unless noted, we use N = 8 encoder blocks to perform the ASR and the FMLM pre-training tasks. For MT model used in IWSLT expanded setting, we use the Transformer architecture in <ref type="bibr" target="#b34">Vaswani et al. (2017)</ref> with N e = 6, N d = 6, H = 4, d model = 256.</p><p>We train the model with 4 Tesla P40 GPUs and batch size is set to 64 per GPU. The pre-training takes 50 and 20 epochs for each phase and the final ST task takes another 50 epochs (a total of 120 epochs). We use the Adam optimizer with warmup steps 25000 in each phase. The learning rate decays proportionally to the inverse square root of the step number after 25000 steps. We save checkpoints every epoch and average the last 5 checkpoints as the final model. To avoid overfitting, SpecAugment strategy <ref type="bibr" target="#b31">(Park et al., 2019)</ref> is used in ASR pre-training with frequency masking (F = 30, mF = 2) and time masking (T = 40, mT=2). The decoding process uses a beam size of 10 and a length penalty of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Comparison with End-to-End Baselines</head><p>LibriSpeech En-Fr: The results on LibriSpeech En-Fr test set are listed in Table <ref type="table" target="#tab_1">1</ref>. In base setting, our method improves the "Transformer+ASR Method Enc pre-train Dec pre-train BLEU MT <ref type="bibr" target="#b7">(Berard et al., 2018)</ref>* --19.3 MT <ref type="bibr" target="#b15">(Inaguma et al., 2019)</ref> --18.3 base setting LSTM ST <ref type="bibr" target="#b7">(Berard et al., 2018)</ref>* 12.9 +pre-train+multitask <ref type="bibr">(Berard et al., 2018)* 13.4 LSTM ST+pre-train (ESPnet)</ref> 16.68 Transformer+pre-train <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> 14.30 +knowledge distillation <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> 17.02 TCEN-LSTM <ref type="bibr" target="#b36">(Wang et al., 2019b)</ref> 17.05 Transformer+ASR pre-train 15.97 Transformer+curriculum pre-train 17.66 expanded setting LSTM+pre-train+SpecAugment <ref type="bibr" target="#b1">(Bahar et al., 2019)</ref> (236h) 17.0 Multilingual ST+pre-train <ref type="bibr">(Inaguma et al.,</ref>  pre-train" baseline by 1.7 BLEU and beats all the previous works, even though we do not pre-train the decoder. It indicates that through a well-designed learning process, the encoder has a strong potential to incorporate large amount of knowledge. Our method beats a knowledge distillation baseline, where an MT model is utilized to teach the ST model. The reason, we believe, is that our method gives the model more training signals and makes it easier to learn. We also outperform a TCEN baseline which includes two encoders. Compared to them, our method is more flexible and incorporates all information into a single encoder, which avoids the representation gap between the two encoders.</p><p>As the ASR data size increases, the model performs better. In the expanded setting, we find the FBLT task performs poorly compared with the base setting. This is because the target word prediction task is dictionary-supervised in expanded setting rather than reference-supervised as in base setting. However, our method still outperforms the simple pre-training method by a large margin. Besides, it is surprising to find that the end-to-end ST model is approaching the performance of an MT model, which is the upper bound of the ST model since it accepts golden source sentence without any ASR errors. This further verifies the effectiveness of our method.</p><p>IWSLT En-De: The results on IWSLT tst2013 are listed in Table <ref type="table" target="#tab_2">2</ref>, showing a similar trend as in LibriSpeech dataset. We find that the segmentation methods have a big influence on the final results. In the base setting, our method can improve the ASR pre-training baseline by 0.9 to 2.2 BLEU scores, depending on the segmentation methods. In the expanded setting, we find when combined with decoder pre-train, the performance is further improved and beats other expanded baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Comparison with Cascaded Baselines</head><p>Table <ref type="table" target="#tab_4">3</ref> shows comparison with cascaded ST systems. For the base setting of two tasks, our end-toend model can achieve comparable or better results with cascaded methods. This shows the end-toend model has powerful learning capabilities and combines the functions of two models. In the Lib-riSpeech expanded setting, when more ASR data is available, we also obtain a competitive performance. This indicates our method can make a good use of ASR corpus and learn valuable linguistic knowledge other than simple acoustic information. However, when additional MT data is used, there is still a gap between the end-to-end method and the cascaded method. How to utilize bilingual parallel sentences to improve the E2E ST model is worth further studying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis and Discussion</head><p>Ablation Study To better understand the contribution of each component, we perform an ablation study on LibriSpeech expanded setting. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. On the one hand, we show that both of our proposed pre-training tasks are beneficial: In "-FMLM task" and "-FBLT task"<ref type="foot" target="#foot_8">9</ref> , we perform single-task pre-training for advanced course. The performance drops when we remove  <ref type="bibr" target="#b7">(Berard et al., 2018)</ref> 14.6 LSTM ASR+ MT <ref type="bibr" target="#b15">(Inaguma et al., 2019)</ref> 15.8 Transformer ASR + MT <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> 17  either one of them. On the other hand, we show the two-phases pre-training paradigm is necessary:</p><p>The "-phase 2" experiment degenerates to the simple ASR pre-training baseline. In "-phase 1" setting, we find that without the ASR pre-training, the training accuracy on FMLM task and FBLT task drops a lot, which further affects the ST performance. This means the ASR task is necessary for both the advanced courses and ST. In "Multi3" setting, we pre-train the model on ASR, FMLM and FBLT tasks in one phase. In this setting, we observe multi-task learning also decrease individual task performances (ASR, FMLM and FBLT) compared to curriculum learning. One reasonable expanation is that it is hard to train on the FMLM and FBLT tasks which takes masked input from randomly initialized parameters, which also leads to performance degradation on the ST task. influence of different choices. We keep N e = 12 unchanged and always use the top layer to perform the FBLT task. Then we alter the hyperparameter N . We find if N = 6, the model finds it difficult to converge during ST training. That may be because the distance between the decoder and the bottom 6 encoder layers is too far so that the valuable source linguistic knowledge can not be well utilized. Moreover, the model performs undesirable if the choice is 10 or 12, which results in 16.47 and 16.14 BLEU score respectively, since the number of blocks for FBLT task is not enough. The model achieves the best performance when we choose N = 8. Thus, we use this strategy in our main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter</head><p>Unlabeled Speech Data In this work, we also explore how to utilize the unlabeled speech data in pre-training, but only get negative results. We conduct exploratory experiments on the LibriSpeech ST task. Assume that the (x, y s ) from 100h ST corpus as labeled pre-training data and (x) from 960h LibriSpeech ASR corpus as unlabeled data.</p><p>Following <ref type="bibr" target="#b17">Jiang et al. (2019)</ref>, we design an unsupervised pre-training task for elementary course, in which we randomly mask 15% of fbank features and let the bottom 4 encoder layers predict the masked part. We compute the L1 loss between the prediction and groundtruth filterbanks. However, we find that this method is not helpful for the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper investigates the end-to-end method for ST. We propose a curriculum pre-training method, consisting of an elementary course with an ASR loss, and two advanced courses with a frame-based masked language model loss and a bilingual lexicon translation loss, in order to teach the model syntactic and semantic knowledge in the pre-training stage. Empirical studies have demonstrated that our model significantly outperforms baselines. In the future, we will explore how to leverage unlabeled speech data and large bilingual text data to further improve the performance. Besides, we expect the idea of curriculum pre-training can be adopted on other NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) previous encoder pre-training (b) curriculum encoder pre-training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between previous encoder pre-training method with our curriculum pre-training method.</figDesc><graphic url="image-2.png" coords="1,319.80,291.63,190.48,89.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proposed curriculum pre-training process. L F M LM only predicts the mask word, while L F BLT predicts all words in the target language.</figDesc><graphic url="image-3.png" coords="3,83.34,62.81,430.85,187.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LibriSpeech:</head><label></label><figDesc>In the context of base setting,<ref type="bibr" target="#b7">Berard et al. (2018)</ref> and ESPnet have reported results on a LSTM-based ST model with pre-training and/or multi-task learning strategy. Liu et al. (2019) use a Transformer ST model and knowledge distillation method. Wang et al. (2019b) stack an ASR encoder and an MT encoder for final ST task, named as TCEN. Regarding the expanded setting, Bahar et al. (2019) apply the SpecAugment on ST task. They use the total 236h of speech for ASR pre-training. Inaguma et al. (2019) combine three ST datasets of 472h training data 6 to train a multilingual ST model. In our work, we use the LibriSpeech ASR corpus as additional pre-training data, including 960h of speech. As the dev and test set of LibriSpeech ST task are extracted from the 960h corpus, we exclude all training utterances with the same speaker that appear in dev or test sets .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison on LibriSpeech En-Fr test set. The size of ASR data for base setting is 100h unless labeled. Since inputs of the MT models are ground-truth text, the results of MT models can be seen as the upper-bound of ST models. *: Unknown BLEU score script.</figDesc><table><row><cell>2019)</cell><cell>(472h)</cell><cell>17.6</cell></row><row><cell>Transformer+ASR pre-train</cell><cell>(960h)</cell><cell>16.90</cell></row><row><cell>Transformer+curriculum pre-train</cell><cell>(960h)</cell><cell>18.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ST results on IWSLT En-De tst2013 set.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">Enc pre-train Dec pre-train segment method (speech data) (text data) LIUM aeneas</cell></row><row><cell>base setting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ESPnet</cell><cell></cell><cell></cell><cell></cell><cell>12.50</cell><cell>-</cell></row><row><cell>+enc pre-train</cell><cell></cell><cell></cell><cell></cell><cell>13.12</cell><cell>-</cell></row><row><cell>+enc dec pre-train</cell><cell></cell><cell></cell><cell></cell><cell>13.54</cell><cell>-</cell></row><row><cell>Transformer+ASR pre-train</cell><cell></cell><cell></cell><cell></cell><cell>15.35</cell><cell>17.10</cell></row><row><cell>Transformer+curriculum pre-train</cell><cell></cell><cell></cell><cell></cell><cell>16.27</cell><cell>19.29</cell></row><row><cell>expanded setting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Multilingual ST+pre-train(Inaguma et al., 2019)</cell><cell>(472h)</cell><cell></cell><cell>14.6</cell><cell>-</cell></row><row><cell>TCEN-LSTM (Wang et al., 2019b)</cell><cell></cell><cell>(479h)</cell><cell>(40M)</cell><cell>17.65</cell><cell>-</cell></row><row><cell cols="2">CL-fast(Kano et al., 2017)(re-implemented)</cell><cell>(479h)</cell><cell></cell><cell>14.33</cell><cell>16.23</cell></row><row><cell cols="2">Transformer+curriculum pre-train+dec pre-train</cell><cell>(479h)</cell><cell>(4M)</cell><cell>18.15</cell><cell>20.35</cell></row><row><cell>Method</cell><cell>BLEU</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LibriSpeech base setting</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM ASR+ MT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with cascaded ST. *:we find the LSTM model outperforms Transformer model in our setting since the training data is scarce.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>N During pre-training, which layer conducts ASR pre-training and FMLM loss is an important hyper-parameter. We conduct experiments on LibriSpeech base setting to explore the Ablation study on LibriSpeech expanded setting. '-' indicates removing the task or phase from our method.</figDesc><table><row><cell>Method</cell><cell>BLEU</cell></row><row><cell>Our method</cell><cell>18.01</cell></row><row><cell>-FMLM task</cell><cell>17.62</cell></row><row><cell>-FBLT task</cell><cell>17.65</cell></row><row><cell>-phase 2</cell><cell>16.90</cell></row><row><cell>-phase 1</cell><cell>14.26</cell></row><row><cell>Multi3</cell><cell>14.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>final ST task, which results in 16.85 BLEU score, lower than our base setting model (without extra data pre-training). It is still an open question about how to use unlabeled speech data.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The position indexs are divided by 4 due to downsampling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://www.statmt.org/moses</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/moses-smt/giza-pp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/espnet/espnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://www.readbeyond.it/aeneas</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">LibriSpeech En-Fr, IWSLT En-De and Fisher-CallHome Es-En</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://wit3.fbk.eu/mt.php?release= 2017-01-trnted</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">Europarl v7, Common Crawl, News Comentary v13 and Rapid corpus of EU press releases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">we use 12-layer encoder for ASR and FMLM pre-training for a fair comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant No.U1636116 and the Ministry of education of Humanities and Social Science project under grant 16YJC790123.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An unsupervised probability model for speech-to-translation alignment of low-resource languages</title>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1133</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2016</title>
				<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1255" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On using specaugment for endto-end speech translation</title>
		<author>
			<persName><forename type="first">Parnia</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno>CoRR, abs/1911.08876</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lowresource speech-to-text translation</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1326</idno>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India</title>
				<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2018-02-06">2018. 2-6 September 2018</date>
			<biblScope unit="page" from="1298" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pretraining on high-resource speech recognition improves low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alignment templates: the RWTH SMT system</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT 2004</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2009</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">End-toend automatic speech translation of audiobooks</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Can</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461690</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="6224" to="6228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Listen and translate: A proof of concept for end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Berard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Servan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<idno>CoRR, abs/1612.01744</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2016.7472621</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2016</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8462105</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An attentional model for speech translation without transcription</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1109</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016</title>
				<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="949" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal curriculum learning for semi-supervised image classification</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2016.2563981</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3249" to="3260" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual end-to-end speech translation</title>
		<author>
			<persName><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU46091.2019.9003832</idno>
	</analytic>
	<monogr>
		<title level="m">ASRU 2019</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="570" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Leveraging weakly supervised data to improve end-to-end speech-to-text translation</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8683343</idno>
		<idno>ICASSP 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="7180" to="7184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving transformer-based speech recognition using unsupervised pre-training</title>
		<author>
			<persName><forename type="first">Dongwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wubo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ne</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangang</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1910.09932</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured-based curriculum learning for endto-end english-japanese speech translation</title>
		<author>
			<persName><forename type="first">Takatomo</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2017</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2630" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparative study on transformer vs RNN in speech applications</title>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takenori</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">Enrique</forename><surname>Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASRU46091.2019.9003750</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019</title>
				<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-12-14">2019. December 14-18, 2019</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Ali Can Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><surname>Kraif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2018: System Demonstrations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-to-end speech translation with knowledge distillation</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">2019. 2019. 1904.08075</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1166</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical phrase-based speech translation</title>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2006.1660082</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2006</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the integration of speech recognition and statistical machine translation</title>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2005</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3177" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Montreal forced aligner: Trainable textspeech alignment using kaldi</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Socolof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Mihuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Sonderegger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="498" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lium spkdiarization: an open source toolkit for diarization</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Meignier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teva</forename><surname>Merlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CMU SPUD Workshot</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech translation: coupling of recognition and translation</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.1999.758176</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP &apos;99</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The iwslt 2018 evaluation campaign</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronaldo</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT</title>
				<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2015.7178964</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1904.08779</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2014</title>
				<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3935" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention-passing models for robust and data-efficient end-to-end speech translation</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="313" to="325" />
			<date type="published" when="2019-01">Jan Niehues, and Alex Waibel. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic mask for transformer based end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoli</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1912.03010</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bridging the gap between pretraining and fine-tuning for end-to-end speech translation</title>
		<author>
			<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1909.07575</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Enrique Yalta Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1456</idno>
	</analytic>
	<monogr>
		<title level="m">Adithya Renduchintala, and Tsubasa Ochiai</title>
				<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Sequence-tosequence models can directly translate foreign speech</title>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>ISCA</publisher>
			<biblScope unit="page" from="2625" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
