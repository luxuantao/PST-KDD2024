<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evidence Accumulation Clustering based on the K-Means Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ana</forename><surname>Fred</surname></persName>
							<email>afred@lx.it.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Instituto de Telecomunicações</orgName>
								<orgName type="institution">Instituto Superior Técnico</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
							<email>jain@cse.msu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evidence Accumulation Clustering based on the K-Means Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">008D6DE6604430F89045C0034F3E48FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The idea of evidence accumulation for the combination of multiple clusterings was recently proposed <ref type="bibr" target="#b6">[7]</ref>. Taking the K-means as the basic algorithm for the decomposition of data into a large number, k, of compact clusters, evidence on pattern association is accumulated, by a voting mechanism, over multiple clusterings obtained by random initializations of the K-means algorithm. This produces a mapping of the clusterings into a new similarity measure between patterns. The final data partition is obtained by applying the single-link method over this similarity matrix. In this paper we further explore and extend this idea, by proposing: (a) the combination of multiple K-means clusterings using variable k; (b) using cluster lifetime as the criterion for extracting the final clusters; and (c) the adaptation of this approach to string patterns. This leads to a more robust clustering technique, with fewer design parameters than the previous approach and potential applications in a wider range of problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering algorithms can be categorized into hierarchical methods and partitional methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. A partitional structure organizes patterns into a small number of clusters. The K-means is one of the simplest clustering algorithms in this class: it is computationally efficient and does not require the specification of many parameters. Hierarchical methods propose a nesting of clusterings, providing additional information about data structure, represented graphically as a dendrogram. A particular partition is obtained by cutting the dendrogram at some level. The single link algorithm is one of the most popular methods in this class <ref type="bibr" target="#b11">[12]</ref>.</p><p>A large number of clustering algorithms exist <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Examples of different classes of algorithms are model-based techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>, non-parametric density estimation based methods <ref type="bibr" target="#b20">[21]</ref>, central clustering <ref type="bibr" target="#b1">[2]</ref>, square-error clustering <ref type="bibr" target="#b18">[19]</ref>, and graph theoretical based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref> methods. Each handles differently the issues related to cluster validity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8]</ref>, number of clusters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>, and structure imposed on the data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16</ref>]; yet, no single algorithm can adequately handle all sorts of cluster shapes and structures.</p><p>Inspired by the work in sensor fusion and classifier combination techniques in pattern recognition <ref type="bibr" target="#b13">[14]</ref>, Fred <ref type="bibr" target="#b6">[7]</ref> proposed a combination of clusterings in order to devise a consistent data partition. It follows a split and merge strategy. First, the data is split into a large number of small clusters, using the K-means algorithm; with fixed k, different clusterings are produced by an arbitrary initialization of cluster centers. The clustering results are combined using a voting mechanism, leading to a new similarity matrix between patterns. The final clusters are obtained by applying the single-link (SL) method on this matrix, thus merging small clusters produced in the first stage of the method.</p><p>In this paper we further analyze the above method and propose three main refinements/extensions: the use of cluster lifetime as a criterion for the identification of the final data partition from the dendrogram produced by the SL method, instead of fixed level thresholding; the combination of clusterings with different values of k in a reasonably large range; adaptation of this approach to process string patterns. These modifications improve the previous strategy in terms of robustness and simplicity of the method, with fewer parameters to be defined.</p><p>Section 2 discusses the method in <ref type="bibr" target="#b6">[7]</ref>. Refinements and extensions of the method are proposed in section 3. The performance of the new method is illustrated through a set of experimental results given in section 4, followed by the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evidence Accumulation Clustering</head><p>The idea of evidence accumulation clustering is to combine the results of multiple clusterings into a single data partition, by viewing each clustering result as an independent evidence of data organization. Fred <ref type="bibr" target="#b6">[7]</ref> used the K-means algorithm as the basic algorithm for decomposing the data into a large number, k, of compact clusters; evidence on pattern association is accumulated, by a voting mechanism, over N clusterings obtained by random initializations of the K-means algorithm. This produces a mapping of the clusterings into a new similarity measure between patterns, summarized in the matrix co assoc, where co assoc(i, j) indicates the fraction of times the pattern pair (i, j) is assigned to the same cluster among N clusterings. The final data partition is obtained by applying the single-link method over this similarity matrix, using a fixed threshold, t.</p><p>The method has two design parameters: k, the number of clusters for the K-means algorithm; and t, the threshold on the dendrogram produced by the SL method. We discuss these parameters using the half-rings data set example, depicted in figure <ref type="figure" target="#fig_4">1(a)</ref>. This data set is composed of 400 two-dimensional patterns (upper cluster -100 patterns; lower cluster -300 patterns). Due to the particular cluster shapes, the K-means algorithm by itself is unable to identify the two natural clusters (see figure <ref type="figure" target="#fig_4">1(b)</ref>). The uneven data sparseness of the two clusters also prevents the SL method to produce the correct data partition, as shown by the associated dendrogram (figure <ref type="figure" target="#fig_4">1(c)</ref>).    The K-means algorithm can be seen as performing a decomposition of the data into a mixture of Gaussians. k is the critical parameter in this decomposition: low values of k are not enough to capture the complexity of the data, while large values may produce an over-fragmentation of the data (in the limit, each pattern forming a cluster). By using the method in <ref type="bibr" target="#b4">[5]</ref> the data set is decomposed into 10 gaussian components. This should be a lower bound on the value of k to be used with the K-means, as this algorithm imposes spherical shaped clusters, and therefore a higher number of components may be needed for evidence accumulation. This is in agreement with the dendrograms in figures 1(d)-1(f). As shown in figure <ref type="figure" target="#fig_4">1(d)</ref>, although the two-cluster structure starts to emerge in the dendrogram for k = 5, the two natural clusters cannot yet be identified. A clear cluster separation is present in the dendrogram for k = 15 (fig. <ref type="figure" target="#fig_4">1(e)</ref>). As k increases, similarity values between pattern pairs decrease, and links in the dendrograms progressively form at higher levels, causing the two natural clusters to be less clearly defined (see fig. <ref type="figure" target="#fig_2">1</ref>  </p><formula xml:id="formula_0">2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 * 5 7 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 * 2 * 5 20 2 * 10 1 1 1 1 1 2 2 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evidence Accumulation Clustering with Varying k and Dynamic Threshold</head><p>As noted in the previous section, cluster lifetime is a better criterion for identifying the natural clusters than a fixed threshold, as the dendrogram scales up with increasing values of k. On the other hand, in order to determine an adequate value or range for k, one should use some a priori information (for instance, by applying a mixture decomposition method for determining the number of components in the mixture). Otherwise, several values of k should be tested, the final number of clusters being the most stable solution found. The evidence of a clear cluster separability on the dendrograms associated with a large range for k (see figures 1(e), 1(f)) suggests a combination of K-means clusterings with variable k. Our hypothesis is that the combined evidence will reinforce the intrinsic data structure, diluting the effect produced by low values of k (while combined with other values, low k values contribute to a scaling up of similarity measures -lower values on the dendrograms); high values of k produce random, high granularity data partitions, so they should also not be disruptive of the structure imposed by more adequate k values, scaling down the similarity values. We therefore propose a combination of multiple K-means clusterings with varying k, the final data partition being obtained as the cluster configuration with the highest lifetime in the dendrogram produced by the SL method over the similarity matrix, co assoc. The proposed evidence accumulation clustering method is summarized below: Update the co-association matrix: for each pattern pair, (i, j), in the same cluster in P , set co assoc(i, j) = co assoc(i, j) + 1 N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Detect consistent clusters in the co-association matrix using the SL technique:</head><p>compute the SL dendrogram and identify the final clusters as the ones with the highest lifetime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Vector Representations: Artificial Data Sets</head><p>The proposed evidence accumulation clustering method was applied to the halfrings data set, used as the illustrative example in section 2. Several ranges for k were tested in order to evaluate the robustness of the method. Dendrograms for some of these tests are plotted in figure <ref type="figure" target="#fig_7">2</ref>. The number of clusterings used were N = 600; experiments with N = 200 and lower values led to similar results, since the method converges for values of N around 50 (see figure <ref type="figure" target="#fig_7">2(d)</ref>). Table <ref type="table">4</ref> summarizes the experiments and the number of clusters (NC) obtained. As shown, all ranges for k, except the ones completely below the minimum number of mixture components, 10, (first two columns), lead to the correct identification of the natural clusters, demonstrating the robustness of the method.   Table <ref type="table">4</ref>. Evidence accumulation clustering with varying k for the half-rings data set.</p><p>The spiral data set (fig. <ref type="figure" target="#fig_8">3(a)</ref>) is another example of complex shaped clusters. Using the method of <ref type="bibr" target="#b6">[7]</ref>, the two natural clusters are identified for values of k in the interval <ref type="bibr">[25; 70]</ref> for t = 0.5 or t = 0.6. In all the tests with the proposed method, the true clusters were identified for all the intervals considered (values of k &gt; 90 were not tested as the number of training patterns is only 200), except for ranges totally in the interval [2; 20], as this is lower than the minimum number of components required to decompose the data (the method in <ref type="bibr" target="#b4">[5]</ref> identifies 24 gaussian components). We also performed tests on uni-modal random data (gaussian and uniform distributions) in order to assess if the proposed clustering technique imposes some structure on data. In all the tests performed (an example of uniform data set is illustrated in figure <ref type="figure" target="#fig_8">3(b)</ref>), a single cluster was identified, no matter what interval for k was considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">String Patterns: Clustering of Contour Images</head><p>We have applied the proposed technique to the classification of string descriptions of contour images of 2D shapes. The data set consists of 126 images from three types of tools (42 patterns per class); sample images are shown in figures 4(a) to 4(c). Each image was segmented to separate the object from the background and the object boundary was sampled at 50 equally spaced points; object shapes were encoded using an 8-directional differential chain code <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. In order to apply the cluster combination technique, similarity between all pattern pairs was calculated using the Levensthein distance normalized by the length of the editing path <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. The K-means algorithm was adapted in order to handle string patterns: cluster centroids are selected as the training pattern with the minimum average distance to the remaining patterns within a cluster; therefore, the algorithm simply needs a similarity/dissimilarity matrix between pattern pairs as input.  As shown in figure <ref type="figure" target="#fig_10">4</ref>(d), a direct application of the SL method to the string patterns using the normalized string edit distance does not produce a correct partitioning of the data. With the proposed method, a good separation of the three clusters is obtained, for instance with k ∈ [2; 30] and N=200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have proposed a novel algorithm for evidence accumulation clustering. The method introduced in <ref type="bibr" target="#b6">[7]</ref> was extended/modified by: (1) using cluster lifetime as a criterion for determining the final number of clusters; (2) proposing the formation of clustering ensembles by using the K-means algorithm with random initialization and arbitrary k values within a large interval. Furthermore, the adaptation of the K-means algorithm by using cluster median patterns, and thus simply requiring as input a similarity or dissimilarity matrix, extended the potential use of this technique to a wider range of applications, namely those based on string descriptions. The new method enhances the previous approach in terms of robustness and simplicity of evaluations, with fewer parameters being defined. The ability of the clustering method to correctly identify well separated clusters with complex shapes has been demonstrated on a set of artificial and real data, using both vector and string descriptions of patterns. Moreover, tests on unimodal/uniform data showed that the method does not impose any structure on data, a single cluster being identified for this data. Further tests are needed for touching clusters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Half-rings shaped clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>K-means clustering, K = 2. (c) Single-link method. Thresholding this graph splits the upper ring cluster into several small clusters. (d) Evidence accumulation clustering, k = 5. (e) Evidence accumulation clustering, k = 15. (f) Evidence accumulation clustering, k = 80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Half-rings data set. Vertical axis on dendrograms (d) to (f) corresponds to distances, d(i, j), with d(i, j) = 1co assoc(i, j).</figDesc><graphic coords="3,313.65,418.21,160.77,83.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figures 1 (</head><label>1</label><figDesc>Figures1(d)-1(f) plot the dendrograms produced by the evidence accumulation algorithm after 200 runs (N = 200) of the K-means algorithm, for different values of k. The K-means algorithm can be seen as performing a decomposition of the data into a mixture of Gaussians. k is the critical parameter in this decomposition: low values of k are not enough to capture the complexity of the data, while large values may produce an over-fragmentation of the data (in the limit, each pattern forming a cluster). By using the method in<ref type="bibr" target="#b4">[5]</ref> the data set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table 1 .</head><label>1</label><figDesc>Number of clusters identified as a function of k and t for the half-rings data set (N = 200). The 2 * notation indicates that, although the correct number of clusters is identified, this does not correspond to the correct data partition. The rightmost column indicates the final number of clusters according to the largest lifetime criterion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) k ∈ [2; 20]. (b) k ∈ [60; 90]. (c) k ∈ [2; 80].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Convergence curve (k ∈ [2; 80]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Combining 600 K-means clusterings, with varying k for data in fig 1(a). Dendrograms (a) to (c) illustrate the wide range of k values with a clear cluster separation, showing the robustness of the combination technique. (d)-Convergence curve of the final number of clusterings as a function of N , the number of clusterings, for k ∈ [2; 80].</figDesc><graphic coords="6,149.04,264.59,162.27,83.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Artificial data sets. (a)-Spiral data set (100 samples per class). (b)-2-D projection of 300 patterns uniformly distributed in a 5-dimensional hypercube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Hardware tools data set.</figDesc><graphic coords="7,295.10,556.44,185.17,53.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(f) for k = 80). The same conclusions can be drawn by analyzing table 1, showing the number of clusters identified for different values of k and of t. The lifetime of a cluster in the dendrogram for a given k (distance gap between two successive merges) can be evaluated on the corresponding line in this table. As shown, using a fixed threshold, the range of k values for which the true number of clusters is identified is limited and depends on t. Using the longest lifetime (clusters persisting for the largest range of t) as the criterion for identifying the final number of clusters, leads to the values on the rightmost column of table 1, with the identification of the true clusters for a larger k range. k\t .05 .10 .15 .20 .25 .30 .35 .40 .45 .50 .55 .60 .65 .70 .75 .80 .85 .90 .95 NC</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the Portuguese Foundation for Science and Technology (FCT), Portuguese Ministry of Science and Technology, and FEDER, under grant POSI/33143/SRI/2000, and ONR grant no. N00014-01-1-0266.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cluster validity profiles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dubes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="83" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning without overfitting: Empirical risk approximation as an induction principle for reliable clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Pattern Recognition</title>
		<editor>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Pattern Classification. Wiley</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On-line hierarchical clustering. Pattern Recognition Letters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>El-Sonbaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ismail</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1285" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of finite mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="396" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Path based pairwise data clustering with application to texture segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zoller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2134</biblScope>
			<biblScope unit="page" from="235" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding consistent clusters in data partitions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<editor>
			<persName><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2096</biblScope>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clustering under a hypothesis of smooth dissimilarity increments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leitão</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Int&apos;l Conference on Pattern Recognition</title>
		<meeting>of the 15th Int&apos;l Conference on Pattern Recognition<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="190" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hidden markov models vs syntactic modeling in object recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP&apos;97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic validation approach for clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Har-Even</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Brailovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1189" to="1196" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<title level="m">Fundamentals of Digital Image Processing</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data clustering: A review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On finding the number of clusters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detection and separation of ring-shaped clusters using fuzzy clusters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="855" to="861" />
			<date type="published" when="1994-08">August 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computation of normalized edit distance and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mixture Models: Inference and Application to Clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Basford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Marcel Dekker</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Concept learning and feature selection based on square-error clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On cluster validity for the fuzzy c-means model</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="370" to="379" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fiding regions of interest for content-extraction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Frederix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IS&amp;T/SPIE Conference on Storage and Retrieval for Image and Video Databases VII</title>
		<meeting>of IS&amp;T/SPIE Conference on Storage and Retrieval for Image and Video Databases VII<address><addrLine>San Jose</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-01">January 1999</date>
			<biblScope unit="volume">SPIE</biblScope>
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning string-edit distance</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ristad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="531" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian approaches to gaussian mixture modelling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Husmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rezek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1998-11">November 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Principal curve clustering with noise</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
		<ptr target="http://www.stat.washington.edu/raftery" />
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MDL-based selection of the number of components in mixture models for pattern recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tenmoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Pattern Recognition</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Adnan</forename><surname>Amin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dov</forename><surname>Dori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pavel</forename><surname>Pudil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Herbert</forename><surname>Freeman</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1451</biblScope>
			<biblScope unit="page" from="831" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph-theoretical methods for detecting and describing gestalt structures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers, C</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="86" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
