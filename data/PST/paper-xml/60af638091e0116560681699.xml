<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Adapters for Cross-lingual Low-resource Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenxin</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
							<email>zhuhan@hccl.ioa.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
							<email>jindong.wang@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Renjun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Takahiro</forename><surname>Shinozaki</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">H</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Acoustics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Adapters for Cross-lingual Low-resource Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speech recognition</term>
					<term>cross-lingual adaptation</term>
					<term>meta-learning</term>
					<term>parameter-efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-lingual speech adaptation aims to solve the problem of leveraging multiple rich-resource languages to build models for a low-resource target language. Since the low-resource language has limited training data, speech recognition models can easily overfit. In this paper, we propose to use adapters to investigate the performance of multiple adapters for parameterefficient cross-lingual speech adaptation. Based on our previous MetaAdapter that implicitly leverages adapters, we propose a novel algorithms called SimAdapter for explicitly learning knowledge from adapters. Our algorithm leverages adapters which can be easily integrated into the Transformer structure. MetaAdapter leverages meta-learning to transfer the general knowledge from training data to the test language. SimAdapter aims to learn the similarities between the source and target languages during fine-tuning using the adapters. We conduct extensive experiments on five-low-resource languages in Common Voice dataset. Results demonstrate that our MetaAdapter and SimAdapter methods can reduce WER by 2.98% and 2.55% with only 2.5% and 15.5% of trainable parameters compared to the strong full-model fine-tuning baseline. Moreover, we also show that these two novel algorithms can be integrated for better performance with up to 3.55% relative WER reduction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTOMATIC speech recognition (ASR) based on end- to-end (E2E) models has made remarkable progress by training on large-scale data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. We can use a single E2E ASR system for a large number of languages <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> without complicated language-specific processing. Nevertheless, a well-known limitation of E2E ASR methods is that they require considerable amount of training data to achieve superior performances. Therefore, it remains a challenge for E2E ASR models to achieve reasonable recognition performance for most of the low-resource languages among about 7,000 languages in the world.</p><p>Some research has indicated that the performances of lowresource languages benefit by transferring the common knowledge from rich-resource languages in ASR <ref type="bibr" target="#b4">[5]</ref>. For instance, as shown in Fig. <ref type="figure">1</ref>, given Romanian as a low-resource target language, cross-lingual ASR aims to build models by leveraging the available rich-resource languages such as Italian, Fig. <ref type="figure">1</ref>. Illustration of the cross-lingual speech recognition task. Given three rich-resource languages as source languages (Italian, Welsh, and Russian), how to learn the transferable knowledge from them to build cross-lingual ASR models for the target language Romanian? Welsh, and Russian as source languages. Knowledge transfer can be achieved in three avenues: <ref type="bibr" target="#b0">(1)</ref> pre-training on the richresource languages and then fine-tuning on the low-resource tasks <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>; (2) performing multi-task training using all languages <ref type="bibr" target="#b6">[7]</ref>; and (3) learning the general common knowledge and then rapidly adapting to the low-resource languages using meta-learning <ref type="bibr" target="#b7">[8]</ref>. A possible explanation is that different languages intrinsically share some beneficial information like speaker, environment and linguistic information. In this paper, we mainly focus on the fine-tuning methods.</p><p>Due to the limited training data in low-resource languages, direct re-training makes the model easily overfit. These problems make the transfer-based methods inefficient <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Recently, the adapter module was proposed for parameter-efficient fine-tuning in multilingual or cross-lingual settings <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, which can mitigate overfitting. Adapter is an add-on module to the encoder and decoder layers in Transformer that mainly composed of layer normalization and fully-connected layers. During fine-tuning, we can freeze the backbones of the pre-trained models and only train the adapters which has a small number of task-specific parameters. Pfeiffer et al. <ref type="bibr" target="#b11">[12]</ref> studied the fusion of adapters in natural language processing where they linearly combine the outputs of multiple adapters for target classification task adaptation. However, it remains unexplored to investigate the performance of multiple adapters on cross-lingual ASR tasks.</p><p>In our previous work <ref type="bibr" target="#b8">[9]</ref>, we proposed MetaAdapter to learn general and transferable speech representations using model-agnostic meta-learning (MAML) <ref type="bibr" target="#b12">[13]</ref> and achieved promising results on extremely low-resource languages. However, it is unclear whether MetaAdapter can handle the nonextreme cases where there are moderate training data. Moreover, MetaAdapter relies on meta-learning to implicitly learn from source languages, which makes no assumptions on the relationship between source and target languages that may arXiv:2105.11905v1 [cs.CL] 18 May 2021 weaken its interpretability. Therefore, in this paper, we comprehensively investigate the potential of leveraging multiple source adapters in cross-lingual speech recognition. Based on our analysis on MetaAdapter, we propose a novel algorithm: SimAdapter, to learn the similarity between the source and target languages using the attention mechanism. Our key motivation is that different language in the world are sharing similarities based on their similar geological characteristics or evolution <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Therefore, it is feasible to explicitly model such similarities in the ASR models.</p><p>Both of the two algorithms we present in this work are parameter-efficient and thus can prevent the overfitting problem. To our best knowledge, there is no existing research that tries to model the cross-lingual ASR tasks by studying their relationship using meta-learning and transfer learning-based adapters. In addition, the MetaAdapter and SimAdapter are compatible, thus can be integrated for better performance.</p><p>Our contributions can be summarized as follows:</p><p>• We comprehensively analyze our previously proposed MetaAdapter and propose a novel algorithm for crosslingual low-resource ASR: SimAdapter. • Experiments on five low-resource languages demonstrated a relative WER improvement of 2.98% with MetaAdapter and 2.55% with SimAdapter using only 2.5% and 15.5% trainable parameters compared with the strong full-model fine-tuning baseline, respectively. • We show that these two algorithms can be integrated to achieve better performance with up to 3.55% relative improvement. This paper is substantially an extended version of our previously published paper <ref type="bibr" target="#b8">[9]</ref> at ICASSP 2021. Compared to the previous version, we make heavy extensions as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a parallel new algorithm called SimAdapter for crosslingual ASR. <ref type="bibr" target="#b1">(2)</ref> We investigate the difference and integration between the MetaAdapter and SimAdapter algorithms. (3) We conduct extensive experiments on cross-lingual ASR datasets to validate the effectiveness of these algorithms.</p><p>The structure of this paper is as follows. In Section II, we review the related work to multilingual, cross-lingual ASR and adapters. Section III introduces our main ideas. Section IV and Section V introduce the details of MetaAdapter and SimAdapter algorithms, and their integration. Section VI presents experimental design details and Section VII reports our experimental results and analysis. Finally, in Section VIII, we conclude this paper and present some future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multilingual and Cross-lingual Speech Recognition</head><p>Multilingual E2E ASR is getting increasing attention over the years that handles multiple languages with a single model. Watanabe et al. <ref type="bibr" target="#b16">[17]</ref> proposed a language-independent architecture based on hybrid CTC-attention structure <ref type="bibr" target="#b17">[18]</ref> with augmented vocabulary for character-based E2E ASR and joint language identification. Toshniwal et al. <ref type="bibr" target="#b18">[19]</ref> found that multilingual training leads to significant relative improvement of recognition performance and the results can be further boosted by conditioning the model on a language identifier. Some attempts take a step towards realizing language-universal ASR. Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed to replace the characters with the Unicode bytes as the output. Datta et al. <ref type="bibr" target="#b20">[21]</ref> unified different writing systems through a many-to-one transliteration transducer. Recently, large-scale multilingual ASR systems have been investigated <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Pratap et al. <ref type="bibr" target="#b2">[3]</ref> proposed jointly training on 16,000 hours speech data of 51 languages with up to 1 billion parameters. Inspired by <ref type="bibr" target="#b16">[17]</ref>, Hou et al. presented LID-42 <ref type="bibr" target="#b3">[4]</ref>, a large-scale multilingual acoustic Transformer model trained on 11 mixed corpora of 42 languages.</p><p>Cho et al. <ref type="bibr" target="#b22">[23]</ref> validated the effectiveness of cross-lingual transfer learning for improving ASR performance. And this advantage can be further revealed by large-scale pre-training <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>. For example, LID-42 can achieve a relative WER reduction of up to 28.1% on low-resource languages by crosslingual transfer <ref type="bibr" target="#b3">[4]</ref>. Yi et al. <ref type="bibr" target="#b23">[24]</ref> introduced an adversarial learning objective to learn language-agnostic features. They appended a language discriminator after the shared encoder to distinguish which language the bottleneck features belong to. The objective of the discriminator is to correctly identify the language while the adversarial objective of the encoder is to fool the discriminator. The adversarial training process is realized with the use of the gradient reversal layer (GRL) <ref type="bibr" target="#b24">[25]</ref>. Adams et al. <ref type="bibr" target="#b5">[6]</ref> performed experiments to analyze the impacts of language similarity, context-independent phoneme CTC objective and the aforementioned language-adversarial classification objective during multilingual pre-training to encourage language-agnostic features for better cross-lingual adaptation.</p><p>Besides learning the language-agnostic features, the optimization-based meta-learning approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref> that aim to find a proper initialization for rapid adaptation have also been explored for cross-lingual ASR <ref type="bibr" target="#b8">[9]</ref>. Hsu et al. <ref type="bibr" target="#b7">[8]</ref> proposed to apply the model-agnostic meta-learning (MAML) <ref type="bibr" target="#b12">[13]</ref> as the pre-training method and achieved significant improvement over the conventional multilingual pretraining baseline. Xiao et al. <ref type="bibr" target="#b26">[27]</ref> proposed the Adversarial Meta Sampling framework by introducing a policy network (task sampler) to dynamically sample languages based on their task difficulty. The ASR model is trained to minimize the loss while the task sampler learns to choose the most difficult languages that can maximize the loss. As a consequence, the learned initialization has a more balanced distance to all languages and shows good generalization capacity in lowresource speech tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adapters</head><p>Due to the large quantity of parameters contained in the Transformer-based models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>, recent literature proposed the Adapter structure <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> for parameter-efficient adaptation of pre-trained Transformers <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref> on various downstream tasks including language understanding <ref type="bibr" target="#b33">[34]</ref> and neural machine translation (NMT) <ref type="bibr" target="#b32">[33]</ref>, etc. Adapter is a versatile module that can be plugged into the Transformer blocks. The general philosophy for adapter-based fine-tuning is to freeze the parameters θ b of the Transformer backbone and only tune the parameters θ a of the adapter. Compared to finetuning the whole Transformer model, fine-tuning the adapters is significantly efficient with acceptable performance loss <ref type="bibr" target="#b30">[31]</ref>. Therefore, adapters have been adopted as a fine-tuning technique in few-shot domain adaptation for NMT <ref type="bibr" target="#b34">[35]</ref> and unsupervised cross-lingual transfer <ref type="bibr" target="#b35">[36]</ref> or domain adaptation <ref type="bibr" target="#b36">[37]</ref> of large-scale pre-trained language models like BERT <ref type="bibr" target="#b27">[28]</ref> and XLM <ref type="bibr" target="#b37">[38]</ref>. Li et al. <ref type="bibr" target="#b38">[39]</ref> proposed a hypernetwork that could generate parameters of task-specific adapters from task descriptions to enable zero-shot learning <ref type="bibr" target="#b39">[40]</ref>. More recently, Pfeiffer et al. <ref type="bibr" target="#b11">[12]</ref> introduced the AdapterFusion module to fuse adapters trained on different tasks to share the knowledge. The difference between our work and theirs are that we focus on the cross-lingual sequence-to-sequence ASR task while they experiment on text classification tasks based on the pretrained BERT <ref type="bibr" target="#b27">[28]</ref>.</p><p>Some researchers have proposed to apply the Adapters to the E2E ASR tasks. In <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr">Kannan et al.</ref> proposes to use the adapters to handle the data imbalance problem for large-scale multilingual ASR. After obtaining the model trained on the union of data from all languages, they trained the languagedependent adapters on each of the languages again so that the multilingual backbone shares information across languages while the adapters could allow for per-language specialization. Winata et al. <ref type="bibr" target="#b10">[11]</ref> extends this idea by further introducing a common adapter for all languages to learn language-agnostic information in the multilingual data. On the other hand, Hou et al. <ref type="bibr" target="#b8">[9]</ref> investigates the possibility of applying adapters to cross-lingual ASR under the assumption that a largescale pre-trained multilingual model <ref type="bibr" target="#b3">[4]</ref> should have contained adequate general acoustic and linguistic knowledge and could be adapted to any unseen target language with moderate feature adaptation. Furthermore, they proposed to pre-train the adapters with meta-learning to obtain the MetaAdapter that provides a proper initialization for fast adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPLOITING ADAPTERS FOR CROSS-LINGUAL ASR A. Problem Definition</head><p>The goal of cross-lingual speech recognition is to transfer the knowledge from the existing languages to the new language. Formally speaking, given N rich-resource languages {S 1 , S 2 , • • • S N }, cross-lingual ASR aims at adapting the pretrained model to an unseen target low-resource language L T . Each language S i is composed of the speech-text pairs and we typically use X and y to denote them, respectively, i.e., S i = {X j , y j } Ni j=1 , where N i is the total number of training data. Also note that the target language is low-resource compared to the training languages, i.e., N T N i , ∀1 ≤ i ≤ N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview</head><p>In this paper, we comprehensively investigate the potential of the adapters to achieve parameter-efficient cross-lingual speech recognition. On the one hand, the adapters module is the only trainable parameters in the model, which remains parameter-efficient; on the other hand, the adapters module can also help reduce overfitting on the low-resource cross-lingual data.</p><p>To exploit adapters for cross-lingual ASR, it is important to study the relationship between different languages. In this  paper, we comprehensively analyze the MetaAdapter as well as the newly proposed SimAdapter algorithms that learn and exploit the inter-language relationships to improve cross-lingual ASR. Generally speaking, the MetaAdapter is based on the meta-learning algorithm to extract general latent knowledge from existing training tasks and then adapt these knowledge to the target task. On the other hand, the SimAdapter algorithm is to directly explore the similarity between the source and target languages and then exploit such similarity to dynamically fuse the useful knowledge to the target language. Finally, we show that these two algorithms are not independent, but can be integrated for better performance. As shown in Fig. <ref type="figure">2</ref>, our MetaAdapter and SimAdapter can be easily plugged into the Transformer backbone for implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Backbone: Super Multilingual Transformer ASR Model</head><p>The super language-independent 42-lingual ASR model (LID-42) is proposed by Hou et al. in <ref type="bibr" target="#b3">[4]</ref>. LID-42 is based on the big Speech-Transformer <ref type="bibr" target="#b40">[41]</ref> and joint CTC-attention structure <ref type="bibr" target="#b17">[18]</ref>. We elaborate the model details below.</p><p>As model inputs, LID-42 accepts the 83-dimensional acoustic features (filter banks with pitch) computed with 10 ms frame shift and 25 ms frame length. The acoustic features are firstly subsampled by a factor of 4 by 2 convolution layers with kernel size 3 and stride 2. The resulted features have a receptive field of 100 milliseconds for each frame. Then the following encoder layers process the subsampled features by self-attention and feed-forward as illustrated in <ref type="bibr" target="#b32">[33]</ref>. Apart from self-attention and feed-forward, the decoder layers further accept the encoder outputs and perform cross-attention.</p><p>For the CTC-attention hybrid structure, an auxiliary CTC task <ref type="bibr" target="#b41">[42]</ref> is introduced for encoder outputs in order to encourage the monotonic alignment and accelerate the convergence speed <ref type="bibr" target="#b17">[18]</ref>. During training, a weighted sum of the sequenceto-sequence attention loss L ATT and the CTC loss L CTC is employed:</p><formula xml:id="formula_0">L ASR = (1 − λ)L ATT + λL CTC ,<label>(1)</label></formula><p>where λ denotes the weight of the CTC module.</p><p>Similarly, during decoding, the CTC module outputs are used to re-score the beam search results of the Transformer decoder on-the-fly:</p><formula xml:id="formula_1">Ŷ = arg max Y ∈Y (1 − λ) log P ATT (Y |X) + λ log P CTC (Y |X),<label>(2)</label></formula><p>where X are the 83-dimensional acoustic features (filter banks with pitch), Y denotes the set of the decoding hypotheses.</p><p>As model outputs, a shared vocabulary including characters/subwords and language tokens (e.g., &lt;en&gt;, &lt;fr&gt;) of 42 languages is adopted to realize language-independent training and recognizing. Furthermore, a language token is inserted to the beginning of each training label as an auxiliary language identification target. The model is trained to firstly identify the language before recognizing the speech contents. It is worth noting that we focus on monolingual transfer in this work. Therefore, language-specific heads are used and language identification objective is dropped during fine-tuning.</p><p>LID-42 is trained on around 5000-hour labeled speech data mixing 11 corpora covering 42 language and has revealed a strong performance on cross-lingual transfer learning tasks as shown in previous works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adapters</head><p>As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, a commonly-used adapter structure includes layer normalization, a down-projection layer, a nonlinear activation function, and an up-projection layer. There is also a residual connection that allows the adapter to keep the original representation unchanged. Thus, the adapter function is formulated as:</p><formula xml:id="formula_2">a l = Adapter(z l ) = z l + W l u ReLU W l d LN z l ,<label>(3)</label></formula><p>where z l represents the outputs of layer l, LN denotes layer normalization. W u , W d are weight parameters for up projection and down projection. We will introduce these two algorithms and their integration in next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METAADAPTER FOR CROSS-LINGUAL ASR</head><p>In this section, we introduce MetaAdapter in detail. MetaAdapter is inspired by the idea of meta-learning <ref type="bibr" target="#b42">[43]</ref> for fast adaptation to the new target tasks. In our previous work <ref type="bibr" target="#b8">[9]</ref>, we investigated two meta-learning algorithms: Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b12">[13]</ref> and Reptile <ref type="bibr" target="#b25">[26]</ref>. We found that MAML is more robust to the overfitting problem brought by the variance of adaptation data size and pre-training epochs. Therefore, we adopt the MAML as our meta-training algorithm in this work.</p><p>However, it is expensive to perform meta-learning directly on the full Speech-Transformer model since the model has heavy parameters that could easily overfit on the low-resource target data. To resolve this issue, MetaAdapter utilizes the adapter modules to significantly reduce the adaptation parameters while aiming to find a proper initialization for faster adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>The process of MetaAdapter is illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. Given a pre-trained backbone speech-Transformer ASR model, MetaAdapter is composed of two phases: (i) meta-train the MetaAdapter on a bunch of source tasks; (ii) fine-tune the pre-trained adapter on unseen target tasks. To use meta-learning, we view different languages as different tasks. We split the parameters of MetaAdapter into two types: the backbone parameters θ b (i.e., vanilla Transformer) and the parameters of all adapters θ a . Thus, given N different source languages {S 1 , S 2 , • • • , S N }, we pre-train the MetaAdapter module f θa to obtain good initialization parameters θ a that could generalize for fast adaptation given any unseen target language. Meanwhile, parameters of the pretrained backbone θ b are frozen during both the pre-training and the fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Adapter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training MetaAdapter</head><p>In each pre-training episode, two subsets are randomly sampled from each source training language S i , namely metatraining set S tr i and meta-validation set S val i , i.e., S tr i ∩S val i = ∅. One episode is composed of two iterations: an inner iteration and an outer iteration. In the inner iteration, MAML updates Algorithm 1 Learning algorithm of the MetaAdapter Input: Rich-resource languages {S 1 , • • • , S N }, low-resource task L T .</p><p>1: Train language-specific heads on source languages S i . 2: Initialize the MetaAdapter. 3: while meta-learning not done do 4:</p><p>Optimizing the MetaAdapter using Eq. ( <ref type="formula" target="#formula_6">6</ref>). 5: end while 6: Train the target head on target language L T . 7: Fine-tune the MetaAdapter using ASR loss Eq. ( <ref type="formula" target="#formula_0">1</ref>). 8: return Cross-lingual ASR model.</p><p>the adapter parameters θ a by performing one or more gradient descent on S tr i . For notation simplicity, the updated parameter for language S i using the inner gradient descent iteration is:</p><formula xml:id="formula_3">θ a,i = θ a − ∇L S tr i (f θa ) , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where L is the ASR loss function as introduced in section III-C and is the fast adaptation learning rate. In the outer iteration, the adapter parameters are then optimized to improve the performance of f θ a,i with respect to θ a across all the metavalidation sets S val i . The meta-optimization objective of the outer iteration is:</p><formula xml:id="formula_5">L S val i (f θ a,i ) = L S val i f θa− ∇ θa L S tr i (f θa ) .<label>(5)</label></formula><p>We optimize the meta-optimization objective through gradient descent as:</p><formula xml:id="formula_6">θ a = θ a − µ N i=1 ∇ θa L S val i f θ a,i ,<label>(6)</label></formula><p>where µ is the meta step size. After pre-training, the MetaAdapter should obtain a proper initialization for any unseen target language(s). The complete training procedure of the MetaAdapter is presented in Algo. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMADAPTER FOR CROSS-LINGUAL ASR</head><p>Our motivation of SimAdapter is to improve the adapterbased cross-lingual adaptation as well as the model interpretability by explicitly leveraging the knowledge of the source languages stored in the adapter modules. Here, 'Sim' refers to similarity.</p><p>SimAdapter is inspired by existing research on language and speech origins <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, which imply that different languages in the world are are sharing similarities based on their similar geological characteristics or cultural developments. Thus, it is feasible to leverage the knowledge of multilingual adapters for new target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>SimAdapter is a parameter-efficient algorithm that learns the similarities between existing language-specific adapters and the target low-resource language based on the attention mechanism <ref type="bibr" target="#b32">[33]</ref>. Similar to the adapters, SimAdapter can also be easily integrated with existing pre-trained models and adapters. The detailed composition of the SimAdapter is shown in Fig. <ref type="figure">5</ref>. By taking the language-agnostic representations from the backbone model as the query, and the language-specific outputs from multiple adapter as the keys and values, the final output for SimAdapter over attention are computed as (For notation simplicity, we omit the layer index l below):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key</head><formula xml:id="formula_7">SimAdapter(z, a {S1,S2,...,S N } ) = N i=1 Attn(z, a Si )•(a Si W V ) ,<label>(7)</label></formula><p>where SimAdapter(•) and Attn(•) denotes the SimAdapter and attention operations, respectively. Specifically, the attention operation is computed as:</p><formula xml:id="formula_8">Attn(z, a) = Softmax (zW Q )(aW K ) τ , (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where τ is the temperature coefficient, W Q , W K , W V are attention matrices. Note that while W Q , W K are initialized randomly, W V is initialized with a diagonal of ones and the rest of the matrix with small weights (1e − 6) to retain the adapter representations. Furthermore, a regularization term is introduced to avoid drastic feature changes:</p><formula xml:id="formula_10">L reg = i,j ((I V ) i,j − (W V ) i,j ) 2 ,<label>(9)</label></formula><p>where I V is the identity matrix with the same size as W V .</p><p>In our cross-lingual setting, the SimAdapter module is expected to utilize language-specific knowledge from existing language adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion Guide Loss</head><p>Although SimAdapter aims to benefit from the similar knowledge of other languages, we believe that the most crucial information is stored in the adapter of the target language. However, since the weights of source and target adapters are initialized equally, SimAdapter often distracts its attention significantly from the target language during adaptation and generally does not perform well in our experiments. To alleviate this problem, we propose a fusion guide loss to encourage Algorithm 2 Learning algorithm of SimAdapter Input: Rich-resource languages {S 1 , • • • , S N }, low-resource task L T .</p><p>1: Train language-specific heads on the source languages S i and the target language. 2: Train Adapters A t on top of language-specific heads. 3: Initialize SimAdapter layers. 4: while not done do 5:</p><p>Optimizing SimAdapter layers using Eq. ( <ref type="formula" target="#formula_13">12</ref>). 6: end while 7: return Target ASR model. the model to focus on the corresponding adapters for the similarity learning. Specifically, for each language fusion layer f , we average the cross entropy of adapter attention scores among all K time steps and S samples. The layer-wise guide losses are added up as:</p><formula xml:id="formula_11">L f guide = − 1 K × S S s=1 K k=1 log α s f,k ,<label>(10)</label></formula><formula xml:id="formula_12">L guide = f L f guide .<label>(11)</label></formula><p>Note that K represents the number of frames in the encoder and the number of tokens in the decoder side, α s f,k denotes the attention score of target language's Adapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training SimAdapter</head><p>A difference between the previous application of Adapter-Fusion <ref type="bibr" target="#b11">[12]</ref> and our SimAdapter for cross-lingual ASR is that a language-specific language head is required to be trained for the unseen target language. However, training the Adapters together with the language heads may result in the insufficient learning of semantic information in the adapters. Therefore, in this work, we introduced a three-stage training strategy for SimAdapter-based ASR cross-lingual adaptation.</p><p>In the first stage, different from the previous work <ref type="bibr" target="#b8">[9]</ref>, SimAdapter trains the language-specific heads for each source language S i as well as the target language separately. This step aligns the language heads to the same latent semantic space of the backbone model. In the second stage, adapters are trained based on the pre-trained heads to learn the information. In the third stage, SimAdapter leverages the fusion of source languages for better adaptation to the target language. Only the parameters of the SimAdapter are trained in this stage.</p><p>By adding the W V regularization term weighted by η and the fusion guided loss weighted by γ, the final adaptation objective is given by:</p><formula xml:id="formula_13">L = L ASR + ηL reg + γL guide .<label>(12)</label></formula><p>The complete training procedure of SimAdapter is presented in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Integration of MetaAdapter and SimAdapter</head><p>Although MetaAdapter and SimAdapter can both benefit cross-lingual adaptation by leveraging knowledge of source languages, they are designed from different perspectives. MetaAdapter aims to obtain a proper initialization for fast adaptation by learning from the source languages, which can be regarded as a type of latent transfer. On the other hand, SimAdapter explicitly models the similarities between source and target languages with attention mechanism. Therefore, MetaAdapter is good at handling extremely low-resource languages, while with more training data SimAdapter can capture the language similarities more precisely.</p><p>Moreover, note that MetaAdapter and SimAdapter are not independent algorithms. They can be integrated into one algorithm, which we denote as SimAdapter +. We can simply fuse the source adapters with the target adapter learned by the MetaAdapter using SimAdapter. This can be seen as a two-stage knowledge transfer process where we aim to learn general and transferable knowledge from meta-learning in the first stage; then, we perform adaptation using the SimAdapter algorithm for fine-grained knowledge transfer to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set</head><p>We adopt the Common Voice 5.1 <ref type="bibr" target="#b43">[44]</ref> corpus for our experiments. We follow the official data splits for training, validation and testing. For the SimAdapter, we select five rich-resource languages as source languages and five lowresource languages as targets. Note that the source and target languages are all from European and some of them are spoken in geographically close regions to empirically analyze if the language similarities can be revealed by SimAdapter. The detailed data statistics are shown in TABLE <ref type="table">I</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compared Approaches</head><p>We consider the following fine-tuning-based approaches as well as both end-to-end and conventional hybrid models and trained from random initialization for comparison in this work. To evaluate the efficiency of different methods, we also list numbers of trainable parameters in Table <ref type="table" target="#tab_4">II</ref>  full model, respectively, which are significantly parameterefficient.</p><p>• Hybrid DNN/HMM: Standard hybrid DNN/HMM models are trained with lattice-free MMI <ref type="bibr" target="#b44">[45]</ref> criterion using Kaldi <ref type="bibr" target="#b45">[46]</ref>. Specifically, we use 9 layers TDNN <ref type="bibr" target="#b46">[47]</ref> the acoustic model. The acoustic features are 100dimensional i-vector <ref type="bibr" target="#b47">[48]</ref> and 40-dimensional MFCC. 3gram language model is applied for decoding. train the language-specific head on top of it. • Full-FT: We fine-tune the full model on every target language individually, resulting in separate 5 models. • Adapter: We inject and train the vanilla adapters while keeping the backbone model frozen.</p><p>• MetaAdapter: We inject the pre-trained MetaAdapter and train it as the vanilla adapters do. • SimAdapter: We fuse the Adapters of the source languages with the target language to improve the adaptation performance.</p><p>• SimAdapter +: Specifically, we combine the MetaAdapter and the SimAdapter (namely SimAdapter +) to evaluate its performance and verify whether MetaAdapter and SimAdapter are compatible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We implement the E2E methods based on the ESPnet <ref type="bibr" target="#b48">[49]</ref> codebase. The subword-based LID-42 model proposed in <ref type="bibr" target="#b3">[4]</ref> is used as the backbone model for adaptation. The acoustic features are extracted following ESPnet. Numbers of Senten-cePiece <ref type="bibr" target="#b49">[50]</ref> subwords are set to 150 and 100 for high-and low-resource languages, respectively.</p><p>We use Adam <ref type="bibr" target="#b50">[51]</ref> as the optimizer. For the full-model finetuning, we follow the same learning rate scheduling strategy as <ref type="bibr" target="#b32">[33]</ref> and warmup the learning rate to 0.2 in the first 10 epochs. The total number of training epochs is 200 for fullmodel fine-tuning and SimAdapter, and 100 for the other methods. Early stopping with patience 10 is adopted except for the training of source heads and adapters. The source languages heads and adapters are trained using a batch size of 1024 and learning rate 0.028. The target heads and adapters are trained using a batch size of 512 and learning rate 0.02. For the SimAdapter, we used a batch size of 128, learning rate 2e − 5 and regularization loss 0.01. We adopt η = 0.01 for the regularization loss and 1.0 as the guide loss weight γ. The temperature coefficient τ is simply set to 1.0. We train the MetaAdapter for 30 epochs using Adam <ref type="bibr" target="#b50">[51]</ref> with β 1 = 0 in the inner training loop and vanilla stochastic gradient descent (SGD) in the outer loop. The inner adaptation learning rate and initial meta step size µ are 0.028 and 1.0, respectively. The meta step size linearly annealed to 0 over the course of training. The weight of the CTC module λ is set to 0.3 throughout the experiments following ESPnet <ref type="bibr" target="#b48">[49]</ref>. Beam size 10 is employed for joint decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>In this work, we use word error rate (WER) as our evaluation metric. We average the results on 5 languages to evaluate the overall performance of different methods by default. To reflect the performance on target languages according to their imbalanced test data duration (more test data often represents more training data), we also compute the weighted average WERs, which is more friendly to the methods that require relatively more training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-lingual speech recognition</head><p>Table <ref type="table" target="#tab_7">III</ref> shows the main results on cross-lingual ASR. The first three columns show the non-fine-tuning-based baselines. First, it can be found that the hybrid DNN/HMM model outperforms Transformer on 2 out of 4 languages (Romanian (ro), Arabic (ar)), and these 2 languages are with least training data. The results indicate that the overfitting issue occurs to the Transformer model. It could further be inferred that even hybrid DNN/HMM has the overfitting problem on the extremely low-resource Romanian language, since lower WER is obtained with the linear head simply trained on top of the frozen but powerful LID-42 backbone.</p><p>On the other hand, from the fine-tuning-based approaches presented on the right-hand side, we can observe that the adapters successfully avoid the overfitting problem and outperform the full-model fine-tuning method on 3 very lowresource languages (Romanian, Czech, Arabic). It can be also found that the MetaAdapter and SimAdapter approaches can achieve similar and competitive results on the 5 target languages. Furthermore, we notice that both the MetaAdapter and SimAdapter consistently improve the performance over the adapters and narrow the gap with Full-FT on the languages with relatively more training data (Czech and Ukrainian). Meanwhile, the MetaAdapter method performs better on the extremely low-resource languages (ar, ro) and has lower average WER, while SimAdapter shows better results on moderate low-resource languages (br, cs) and obtains lower weighted average WER. Finally, by combining the MetaAdapter with SimAdapter, the SimAdapter + method surpasses all the other approaches and obtains the best average performance on the 5 languages, indicating that the two methods are compatible since they leverage the source information in different ways. Combining the results from    2) Impact of pre-training epochs for MetaAdapter: To validate the meta-training effects for the MetaAdapter, we select checkpoints of 5 pre-trained epochs {10, 15, 20, 25, 30} and fine-tune them following the same setting as explained in Section VI. We present the results in Fig. <ref type="figure" target="#fig_6">8(a)</ref>. It could be found that the resulted WERs get improved with more pretraining epochs, indicating the effectiveness of meta-learning.</p><p>For comparison, we also conduct the same experiment on another adapter pre-trained on source languages using conven-tional multi-objective learning (MOL) method and visualize the average WERs in Fig. <ref type="figure" target="#fig_3">6</ref>. It is clear that with the more pretraining epochs, the MOL-trained adapter tends to overfit on the source data and performs worse on the target languages.</p><p>3) Analyzing the weight of guide loss for SimAdapter: We then analyze the impacts of the weight γ of the proposed guide loss within {0, 0.001, 0.01, 0.1, 0.5, 1.0} for the SimAdapter. As shown in Fig. <ref type="figure" target="#fig_6">8(b)</ref>, increasing γ generally benefits the improvement of average performance, indicating the correctness of our assumption that SimAdapter layers can learn better under the guidance. We can also observe that when with the increasing of γ, SimAdapter achieves comparable results to full-model fine-tuning at the weight of 0.1 and surpasses it consistently at 0.5 and 1.0.</p><p>4) How much information can be shared across adapters: Although SimAdapter improves the WER results, we do not know whether and how much the other languages can contribute due to the existence of the target language's adapter. Therefore, we fuse the adapters without using the adapters from target languages to see whether additional gains can be obtained with only source adapters. TABLE V shows the results. It can be found that even without the target adapter, SimAdapter can still improve the performance for most of the languages except for Romanian, indicating the effectiveness of learning language information from source adapters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Visualization</head><p>To further show the relationship between source and target languages, we visualize the attention maps for each target language. The attention value reflects their similarities. Fig. <ref type="figure" target="#fig_5">7</ref> shows the results of three different types of languages: (1) without target adapter, (2) with target adapter but no guide loss (γ = 0), (3) with target adapter and guide loss, and ( <ref type="formula" target="#formula_3">4</ref>) with target MetaAdapter and guide loss.   We take the Ukrainian (uk) as an example. Firstly, from the figure on the left, we can observe a trend that SimAdapter layers tend to pay more attention to the Russian (ru)'s adapter, which could be because of the linguistic similarity between Ukrainian and Russian. However, after introducing the target adapter, SimAdapter layers obviously turn to focusing more on the target adapter, but there are still diverse attentions across other languages. By introducing the guide loss, the SimAdapter layers is forced to pay more attention to the target adapter and fusing fewer information from other languages.</p><p>We also notice that in the first encoder layer, the attention distribution seems to be uniform across the source languages. By analyzing the outputs, we found that the adapters in the first layer tend to keep the backbone representation unchanged via the residual connection. The same phenomenon can also be observed on the Czech (cs) target language. A possible reason could be that the first layer is to extract general acoustic features which is language-independent. Since we observe a similar trend in the first decoder layer (layer 12) that the attention distributions tend to be more distracted, we thus assume that adapters in the bottom layers in both the encoder and decoder are less important for cross-lingual adaptation, which we conduct experiments in next subsection to analyze the performance of fusing different adapters.</p><p>D. Do all Adapter layers need to be fused?</p><p>By observing the attention maps, we notice that for some layers, the attention seems to focus solely on the target adapter with a 100% attention score. This phenomenon occurs more frequently in the higher decoder layers, i.e., 12th to 17th layers in Fig. <ref type="figure" target="#fig_5">7</ref>. In such cases, the fusion seems not to be necessary. We doubt whether we can achieve comparable performance while fusing adapters in part of the layers only. Therefore, we conduct the ablation experiments by only fusing part of the layers. The results are presented in TABLE VI. Although some languages (e.g., Breton) can retain the performance by only fusing 2 bottom layers, fusing more layers generally lead to better performance.   It could be found that the MetaAdapter module significantly accelerates the training process while the SimAdapter introduces minor additional time cost compared with full-model fine-tuning. The RTFs of Full-FT and MetaAdapter are at the same level. The reason that MetaAdapter has slightly lower RTF could be due to its shorter average prediction lengths. On the other hand, the relative RTF increasing of 22.12% brought by SimAdapter is also acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose to exploit MetaAdapter and SimAdapter for adapter-based cross-lingual speech recognition. The proposed SimAdapter leverages attention mechanism to learn the similarities between the source and target languages during fine-tuning using the adapters. We also show that the two algorithms can be integrated for better performance. Experiments on five low-resource languages from Common Voice dataset demonstrate the superiority of the two algorithms. In the future, we plan to extend these algorithms to other language families and further improve the training and inference speed of our methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of the adapter module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of MetaAdapter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison between MAML and conventional multi-objective learning (MOL) approach for Adapter pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>cy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Attention matrices of five low-resource target languages. A row in the figure denotes a language, whose four settings are: (1) without target adapter, (2) with target adapter but no guide loss (γ = 0), (3) with target adapter and guide loss, and (4) SimAdapter +. Column index indicates the Transformer layer number, where 0th to 11th layers are encoders, 12th to 17th are decoders. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Analysis of (a) pre-training epoch of MetaAdapter and (b) importance of guide loss in SimAdapter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Finally, we compare</head><label></label><figDesc>the average training time of fullmodel fine-tuning, MetaAdapter and SimAdapter methods per iteration as well as their inference real-time factor (RTF) on the 5 target languages. The RTF metric is used to evaluate the decoding time cost by computing the ratio of the model decoding time to the total utterance duration on the test data. The training and decoding are conducted on 1 GeForce RTX 2080 Ti GPU with batch size 64. The results are shown inTABLE VII.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustration of the MetaAdapter and SimAdapter module injected in the Speech-Transformer. Note that the residual connection between the feedforward layer and layer normalization only applies to the SimAdapter.</figDesc><table><row><cell></cell><cell>𝜆 ×</cell><cell cols="2">Final Distribution</cell><cell>× (1 − 𝜆)</cell></row><row><cell></cell><cell>CTC Head</cell><cell></cell></row><row><cell></cell><cell>Layer norm</cell><cell></cell><cell>Adapter-Fusion MetaAdapter or</cell></row><row><cell></cell><cell cols="2">MetaAdapter</cell><cell>Adapters SimAdapter</cell></row><row><cell>N ×</cell><cell>or</cell><cell></cell><cell>× N</cell></row><row><cell></cell><cell cols="2">SimAdapter</cell><cell>Feed forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Layer norm</cell></row><row><cell></cell><cell cols="2">Feed forward</cell></row><row><cell></cell><cell>Layer norm</cell><cell></cell><cell>Cross-attention</cell></row><row><cell></cell><cell cols="2">Self-attention</cell><cell>Layer norm</cell></row><row><cell></cell><cell>Layer norm</cell><cell></cell><cell>Self-attention</cell></row><row><cell></cell><cell>Conv2D</cell><cell></cell><cell>Layer norm</cell></row><row><cell></cell><cell cols="2">Acoustic Features</cell><cell>Previous Predictions</cell></row><row><cell cols="4">ReLU 𝒂 Up Projection Fig. 2. Layer Norm Down Projection</cell></row><row><cell></cell><cell></cell><cell>𝒛</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. It is shown that our MetaAdapter and SimAdapter (and SimAdapter +) only use 2.5% and 15% of the training parameters from the</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF NUMBER OF TRAINABLE PARAMETERS.</figDesc><table><row><cell>Method</cell><cell># Trainable Parameters</cell></row><row><cell>Hybrid DNN/HMM</cell><cell>14,247K</cell></row><row><cell>Full Model</cell><cell>27,235K</cell></row><row><cell>Head</cell><cell>77K</cell></row><row><cell>Head+(Meta-)Adapter</cell><cell>676K</cell></row><row><cell>Head+(Meta-)Adapter+SimAdapter</cell><cell>4,224K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1 • Transformer: We train a randomly-initialized Transformer model from scratch. • Head: We keep the backbone model (LID-42) frozen and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>TABLE II where SimAdapter + only uses 15.5% trainable parameters of the full model, we see that SimAdapter + is both effective and parameter-efficient.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE III WORD</head><label>III</label><figDesc>ERROR RATES (WER) ON THE CROSS-LINGUAL ASR TASKS We compare the impact brought by different adapter-training strategies, i.e., jointly training the adapter with head and the first two stages of the training strategy proposed in Section V-C. The results are presented in Table IV. It is clear that the proposed twostage training strategy can consistently reduce the WERs of both the adapters and the SimAdapter.</figDesc><table><row><cell>Target Language</cell><cell cols="2">Hybrid DNN/HMM Transformer</cell><cell>Head</cell><cell cols="5">Full-FT Adapter SimAdapter MetaAdapter SimAdapter +</cell></row><row><cell>Romanian (ro)</cell><cell>70.14</cell><cell>97.25</cell><cell>63.98</cell><cell>53.90</cell><cell>48.34</cell><cell>47.37</cell><cell>44.59</cell><cell>47.29</cell></row><row><cell>Czech (cs)</cell><cell>63.15</cell><cell>48.87</cell><cell>75.12</cell><cell>34.75</cell><cell>37.93</cell><cell>35.86</cell><cell>37.13</cell><cell>34.72</cell></row><row><cell>Breton (br)</cell><cell>-</cell><cell>97.88</cell><cell>82.80</cell><cell>61.71</cell><cell>58.77</cell><cell>58.19</cell><cell>58.47</cell><cell>59.14</cell></row><row><cell>Arabic (ar)</cell><cell>69.31</cell><cell>75.32</cell><cell>81.70</cell><cell>47.63</cell><cell>47.31</cell><cell>47.23</cell><cell>46.82</cell><cell>46.39</cell></row><row><cell>Ukrainian (uk)</cell><cell>77.76</cell><cell>64.09</cell><cell>82.71</cell><cell>45.62</cell><cell>50.84</cell><cell>48.73</cell><cell>49.36</cell><cell>47.41</cell></row><row><cell>Average</cell><cell>-</cell><cell>76.68</cell><cell>77.26</cell><cell>48.72</cell><cell>48.64</cell><cell>47.48</cell><cell>47.27</cell><cell>46.99</cell></row><row><cell>Weighted Average</cell><cell>-</cell><cell>72.28</cell><cell>77.54</cell><cell>46.72</cell><cell>47.38</cell><cell>46.08</cell><cell>46.12</cell><cell>45.45</cell></row><row><cell>B. Ablation Study</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">1) Impact of different training strategies:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF DIFFERENT ADAPTER TRAINING STRATEGIES.</figDesc><table><row><cell>Target</cell><cell>Joint</cell><cell cols="3">+SimAdapter Two-stage +SimAdapter</cell></row><row><cell>ro</cell><cell>52.92</cell><cell>53.88</cell><cell>48.34</cell><cell>47.37</cell></row><row><cell>cs</cell><cell>39.16</cell><cell>36.79</cell><cell>37.93</cell><cell>35.86</cell></row><row><cell>br</cell><cell>65.10</cell><cell>63.37</cell><cell>58.77</cell><cell>58.19</cell></row><row><cell>ar</cell><cell>50.53</cell><cell>49.31</cell><cell>47.31</cell><cell>47.23</cell></row><row><cell>uk</cell><cell>52.27</cell><cell>48.84</cell><cell>50.84</cell><cell>48.73</cell></row><row><cell>Average</cell><cell>52.00</cell><cell>50.44</cell><cell>48.64</cell><cell>47.48</cell></row><row><cell cols="2">+Weighted 50.35</cell><cell>48.57</cell><cell>47.38</cell><cell>46.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V WER</head><label>V</label><figDesc>RESULTS OF SIMADAPTER WITH OR WITHOUT ADAPTER L T . FUSION GUIDE LOSS IS SET TO 0 FOR SIMADAPTER WITH ADAPTER L T .</figDesc><table><row><cell>Target</cell><cell>Head</cell><cell>w/o Adapter L T</cell><cell>w/ Adapter L T</cell></row><row><cell>ro</cell><cell>63.98</cell><cell>67.83</cell><cell>53.62</cell></row><row><cell>cs</cell><cell>75.12</cell><cell>55.06</cell><cell>36.55</cell></row><row><cell>br</cell><cell>82.80</cell><cell>77.04</cell><cell>60.87</cell></row><row><cell>ar</cell><cell>81.70</cell><cell>64.68</cell><cell>48.47</cell></row><row><cell>uk</cell><cell>82.71</cell><cell>69.09</cell><cell>51.10</cell></row><row><cell>Average</cell><cell>77.26</cell><cell>66.74</cell><cell>50.12</cell></row><row><cell cols="2">+Weighted 77.54</cell><cell>65.33</cell><cell>48.39</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We did not find proper pronunciation dictionary for Breton. Therefore, only results of the other 4 languages are presented.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An overview of end-to-end automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1018</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Massively multilingual asr: 50 languages, 1 model, 1 billion parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4751" to="4755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2020-2164</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2020-2164" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1037" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multilingual techniques for low resource automatic speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chuangsuwanich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech</note>
	<note>Massachusetts Institute of Technology Cambridge United States</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Massively multilingual adversarial speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="96" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multilingual end-to-end speech recognition with a single transformer on low-resource languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05059</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta learning for end-to-end lowresource speech recognition</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-adapter: Efficient cross-lingual adaptation with meta-learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale multilingual speech recognition with a streaming end-to-end model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05330</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adapt-and-adjust: Overcoming the long-tail problem of multilingual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00247</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language origin from an emergentist perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="691" to="716" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The origin of speech</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Macneilage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">14 fossil evidence for the origin of speech sounds</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Frayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicolay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language independent endto-end architecture for joint language identification and speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based endto-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4904" to="4908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5621" to="5625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language-agnostic multilingual modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8239" to="8243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling end-to-end models for large-scale multilingual asr</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14830</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual sequence-tosequence speech recognition: architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial multilingual training for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4899" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial meta sampling for multilingual low-resource speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1538" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning for few-shot nmt adaptation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Neural Generation and Translation</title>
				<meeting>the Fourth Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="43" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation of a pretrained cross-lingual language model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Zero-shot learning by generating task-specific adapters</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00420</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning from task descriptions</title>
		<author>
			<persName><forename type="first">O</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1361" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Meta-learning: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03548</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
				<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4218" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for asr based on lattice-free mmi</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Interspeech</publisher>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
				<imprint>
			<publisher>CONF. IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Frontend factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Espnet: End-toend speech processing toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-E</forename><forename type="middle">Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">66</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
