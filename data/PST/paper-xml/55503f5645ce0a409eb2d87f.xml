<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Graesb√∏ll</forename><forename type="middle">Y</forename><surname>Christensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
							<email>dwang@cse.ohio-state.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering and Center for Cognitive and Brain Sciences</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83C46F3CA5956B2F81BE00BCCB8E042C</idno>
					<idno type="DOI">10.1109/TASLP.2014.2361023</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Speech signal degradation in real environments mainly results from room reverberation and concurrent noise. While human listening is robust in complex auditory scenes, current speech segregation algorithms do not perform well in noisy and reverberant environments. We treat the binaural segregation problem as binary classification, and employ deep neural networks (DNNs) for the classification task. The binaural features of the interaural time difference and interaural level difference are used as the main auditory features for classification. The monaural feature of gammatone frequency cepstral coefficients is also used to improve classification performance, especially when interference and target speech are collocated or very close to one another. We systematically examine DNN generalization to untrained spatial configurations. Evaluations and comparisons show that DNN-based binaural classification produces superior segregation performance in a variety of multisource and reverberant conditions. Index Terms-Binary classification, computational auditory scene analysis (CASA), deep neural networks (DNNs), room reverberation, speech segregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE performance gap between human listeners and speech segregation systems remains large in noisy and reverberant environments despite extensive research in speech segregation. A typical auditory environment contains multiple concurrent sources that change their locations constantly and are reflected by the walls and surfaces in a room environment. The auditory system excels in hearing out the target source from a sound mixture under such adverse conditions. Simulating this perceptual ability, or solving the cocktail party problem <ref type="bibr" target="#b6">[7]</ref>, remains a huge challenge. A solution to the speech segregation problem is essential to an array of applications in hearing prostheses, robust speech recognition, spatial sound reproduction, and mobile communication.</p><p>Inspired by human auditory scene analysis <ref type="bibr" target="#b3">[4]</ref>, computational auditory scene analysis (CASA) <ref type="bibr" target="#b35">[36]</ref> approaches the segregation problem on the basis of perceptual principles. A commonly used computational goal in CASA is the ideal binary mask (IBM) <ref type="bibr" target="#b37">[38]</ref>, which is a two-dimensional matrix of binary labels where 1 indicates that the target signal dominates the corresponding time-frequency (T-F) unit and 0 otherwise. Recent speech perception research shows that IBM segregation produces large improvements of speech intelligibility in noise for normal-hearing listeners <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b33">[34]</ref> and hearing-impaired listeners <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Such improvements persist when room reverberation is present <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>The effectiveness of ideal binary masking implies that the segregation problem may be pursued a binary classification problem, as first formulated by Roman et al. <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> in the binaural domain. The formulation of segregation as supervised classification has recently led to monaural IBM estimation algorithms producing the first demonstrations of speech intelligibility improvements for both normal-hearing <ref type="bibr" target="#b19">[20]</ref> and hearing-impaired listeners <ref type="bibr" target="#b10">[11]</ref>. It should be noted that these monaural classification algorithms have not considered room reverberation, and tested variations from training noises are limited.</p><p>In this study, we address the problem of speech segregation in both noisy and reverberant environments in the binaural setting. A considerable advantage of the classification based approach is that the distinction between monaural and binaural segregation lies only in extracted features, and joint binaural and monaural segregation can be readily addressed by simply concatenating binaural and monaural features. The latter point, we believe, is an important one as such joint analysis is traditionally considered in different stages <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Classification based on both monaural and binaural cues would allow an opportunistic use of available cues in a variety of adverse conditions, characteristic of human listening <ref type="bibr" target="#b7">[8]</ref>. The proposed classification approach to binaural segregation includes monaural cues in the classification, which are expected to be crucial when target and interfering sources are collocated or close to one another. We should point out that this study does not address sound localization.</p><p>As in any classification task, the use of discriminative features is essential for successful classification. Monaural features such as pitch, amplitude modulation spectrogram, mel-frequency cepstral coefficients, and gammatone frequency cepstral coefficients (GFCCs) have been employed in classification-based segregation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Binaural cues contribute to auditory scene analysis <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In particular, the IBM can also be estimated using the binaural cues of interaural time difference (ITD) and interaural level difference (ILD) <ref type="bibr" target="#b27">[28]</ref> assuming that target and interfering sources originate from different spatial directions. Binaural mechanisms are also believed to contribute to sequential grouping in reverberant environments <ref type="bibr" target="#b7">[8]</ref>. However, when the target and interfering sources are collocated or nearby, binaural cues will not be useful. On the other hand, monaural features are not much affected by spatial configuration of sound sources, and can therefore complement binaural segregation. In this paper, we primarily employ ITD and ILD cues for classification <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b23">[24]</ref>, but also use the monaural cue of GFCC <ref type="bibr" target="#b41">[42]</ref> to further enhance binaural segregation. GFCC has been shown to be a good single feature in a recent evaluation <ref type="bibr" target="#b38">[39]</ref>.</p><p>In addition to features, the use of an appropriate classifier is obviously important for T-F unit classification. A variety of classifiers has been explored in classification-based segregation including kernel density estimation <ref type="bibr" target="#b27">[28]</ref> and histograms <ref type="bibr" target="#b12">[13]</ref> in the binaural domain, and Gaussian mixture models (GMM) <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b19">[20]</ref>, support vector machines (SVM) <ref type="bibr" target="#b11">[12]</ref>, multilayer perceptrons (MLP) <ref type="bibr" target="#b18">[19]</ref>, and deep neural networks (DNNs) <ref type="bibr" target="#b39">[40]</ref> in the monaural domain. In this study, we employ DNNs <ref type="bibr" target="#b14">[15]</ref> due to their compelling performance in speech and signal processing, including its recent successful use in monaural classification <ref type="bibr" target="#b39">[40]</ref>, where direct comparisons with SVM and MLP show DNN's superior performance.</p><p>In the following section, we present an overview of our DNN classification-based binaural speech segregation system. Section III describes how to extract binaural and monaural features and perform DNN classification. The evaluation methodology, including a description of comparison methods, is given in Section IV. We present the evaluation results in Section V, including on trained and untrained source locations. Extensive comparison with several related systems is also presented in this section. We conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM OVERVIEW</head><p>The proposed DNN classification-based binaural speech segregation system is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The same two auditory filterbanks are used to decompose the left-ear and right-ear input signals into the T-F domain. The output in each frequency channel is then divided into 20 ms T-F units. A T-F unit corresponds to a certain channel in a filterbank at a certain time frame. This peripheral analysis produces a time-frequency representation of the sound mixture.</p><p>Binaural features are calculated from each pair of corresponding T-F units in the left-ear and right-ear signal. Monaural features are extracted from the left-ear signal. We extract binaural and monaural features of ITD, ILD and GFCC at the T-F unit level. GFCC features are usually derived at the frame level. By treating the signal in each T-F unit as the input, conventional frame-level feature extraction is then carried out to calculate feature values in each T-F unit <ref type="bibr" target="#b38">[39]</ref> (see Section III-C).</p><p>We train DNN to utilize the discriminative power of the entire feature set in a noisy and reverberant environment. As binaural and monaural features vary with frequency <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we train a DNN classifier for each frequency channel. The training labels are provided by the IBM. In testing, the DNN output is interpreted as the posterior probability of a T-F unit dominated by the target and a labeling criterion is used to estimate the IBM. All the T-F units with the target label (unity) comprise the segregated target stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FEATURE EXTRACTION AND CLASSIFICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Auditory periphery</head><p>We use the gammatone filterbank <ref type="bibr" target="#b25">[26]</ref> for auditory peripheral processing as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The bandwidths of the gammatone filterbank are set according to equivalent rectangular bandwidths, and a filter's impulse response is described as <ref type="bibr" target="#b0">(1)</ref> where denotes a filter channel, and we use a total of 64 channels for each ear model. The center frequency of the filter, , varies from 50 Hz to 8000 Hz.</p><p>indicates the bandwidth. The filter order, , is 4. This peripheral analysis is widely used in CASA.</p><p>With the gammatone filterbanks, the input mixture is first decomposed into the time-frequency domain. The response of a filter channel is half-wave rectified followed by a square root operation, to simulate firing activity and saturation effects of the auditory nerve (see <ref type="bibr" target="#b27">[28]</ref>). Finally, the signal in each channel is divided into time frames. Here we use 20-ms frame length with 10-ms frame shift. The resulting T-F representation is called a cochleagram <ref type="bibr" target="#b35">[36]</ref>. With a 16 kHz sampling rate, the signal in the T-F unit in channel and frame , , has 320 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Binaural Feature Extraction</head><p>With the binaural input signals, we extract the two primary binaural features of ITD and ILD. ITD is calculated from the normalized cross-correlation function (CCF) between the two ear signals, denoted as , for left and right ear respectively.</p><p>, indexed by time lag , for a T-F unit pair is described in the following (see <ref type="bibr" target="#b27">[28]</ref>), <ref type="bibr" target="#b1">(2)</ref> In the above equation, varies between ms and 1 ms, and indexes a signal sample in the T-F units. The overbar indicates averaging. For the 16 kHz sampling rate, there are 33 CCF values and we leave out the value at ms resulting in 32 dimensional (32D) CCF features for each pair of T-F units.</p><p>For comparison, we also calculate a single ITD feature for each T-F unit pair. The ITD is estimated as the lag corresponding to the maximum in the cross-correlation function as <ref type="bibr" target="#b27">[28]</ref>,</p><p>(3) ILD corresponds to the energy ratio in dB, and is calculated for each unit pair as <ref type="bibr" target="#b3">(4)</ref> The above feature gives a single ILD value over the 20-ms frame (1D-ILD). We also break the unit feature into two values, each corresponding to a 10-ms duration, for a finer temporal resolution for ILD. We call the resulting two-value feature 2D-ILD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Monaural Feature Extraction</head><p>To obtain monaural GFCC features, the left-ear unit response, , is treated as an ordinary signal and first decomposed by the same 64-channel gammatone filterbank. Then, we decimate fully rectified filter responses to 100 Hz along the time dimension, resulting in an effective frame shift of 10 ms. The magnitude of the decimated filter output is then loudness-compressed by a cubic root operation to , which is a 2D matrix along frequency and time respectively. Finally, discrete cosine transform (DCT) is applied to the compressed signal to yield GFCC <ref type="bibr" target="#b41">[42]</ref>,</p><p>(5) where refers to the number of frequency channels. The energy of speech signals is distributed towards lower frequencies. As suggested in Zhao et al. <ref type="bibr" target="#b41">[42]</ref>, we use 36D-GFCC features (the first 36 components) for each T-F unit in this paper.</p><p>The above binaural and monaural features characterize different properties of the speech signal. For classification, the features are concatenated together to form a long feature vector. Depending on features used, we maximally obtain a 70D feature with 32D-CCF, 2D-ILD and 36D-GFCC for each T-F unit pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DNN Classification</head><p>Each subband DNN classifier consists of an input layer, two hidden layers, and an output layer <ref type="bibr" target="#b39">[40]</ref>. The extracted feature vector within each T-F unit pair is used as the DNN input. The real valued input is suitable for modeling acoustic features.</p><p>DNN training requires appropriate initialization. It is well known that random initialization is usually unsatisfactory. We follow the approach in <ref type="bibr" target="#b39">[40]</ref>, where DNN is pre-trained with restricted Boltzmann machines (RBMs). Boltzmann machines are stochastic generative models that can be used to find more abstract representations in input patterns. RBMs are two-layer Boltzmann machines with connections only between the visible and the hidden layer. Visible units corresponding to the input layer are assumed to be Gaussian random variables with unit variance, so the real valued input is first Gaussian normalized and then fed into the DNN. Each hidden layer contains 200 binary neurons, which are Bernoulli random variables. The output layer has only one neuron with a binary label where 1 indicates that the target speech dominates a T-F unit and 0 otherwise.</p><p>The joint probability of visible and hidden units is given below,</p><p>where and denote the visible and the hidden layer, respectively, and is called the partition function. is an energy function, defined in <ref type="bibr" target="#b6">(7)</ref> for a Gaussian-Bernoulli RBM for training the first hidden layer, and in (8) for a Bernoulli-Bernoulli RBM for training the other layers. In <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>, and are the th and th units of and , and and are the biases for and , respectively. In addition, is the symmetric weight between and . Mini-batch gradient descent with the batch size of 256 is used for training, including a momentum term with the momentum rate set to 0.5. The learning rate for RBM pre-training is set to 0.001 for the first hidden layer, and 0.1 for the other layers. After RBM pre-training, the standard back-propagation algorithm is applied for supervised fine-tuning. Here, the learning rate decreases linearly from 1 to 0.001 in 50 epochs. For more technical discussions and implementation details about DNN training, we refer the interested reader to <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>For both training and evaluation setup, we generate binaural mixtures that simulate pickup of multiple speech sources in a reverberant space. A reverberant signal is generated using binaural impulse responses (BIRs). We use two sets of BIRs to evaluate the proposed system. The ROOMSIM package <ref type="bibr" target="#b5">[6]</ref>, which uses measured head related transfer functions from the KEMAR dummy head in combination with the image method for simulating room acoustics, is used to generate the first BIR set, re-ferred to as BIR Set A. In addition, we use a recorded BIR set, referred to as BIR Set B, which was collected using the head and torso simulator (HATS) in four reverberant rooms (A, B, C and D) at the University of Surrey <ref type="bibr" target="#b16">[17]</ref>. These speech and noise signals are convolved with BIRs to generate individual sources in a room with corresponding reverberation, and summed at each ear to create the binaural mixture input.</p><p>In BIR Set A, the dimension of a simulated room is m m m (length, width, height). The position of the listener is fixed at m m m. Reflection coefficients of the wall surfaces are uniform. The reflection paths of a particular sound source are obtained using the image model for a small rectangular room <ref type="bibr" target="#b0">[1]</ref>. The reverberation times are approximately 0.3s and 0.7s. We also use the anechoic setting as a baseline. All sound sources are presented at the same distance of 1.5 m from the listener (in the available space of each room configuration). We generate BIRs for azimuth angles between 0 and 360 , spaced by 5 . All elevation angles are zero degree. Speech utterances and babble noise are convolved with selected BIRs to generate the mixtures with defined SNRs. These audio signals are originally sampled at 16 kHz. We upsample them to 44.1 kHz to match the sampling rate of the BIRs, and then downsample to 16 kHz for peripheral and subsequent processing.</p><p>In BIR Set B, the reverberant rooms of A, B, C and D have different sizes and reflective characteristics, and their reverberation times are 0.32s, 0.47s, 0.68s, and 0.89s, respectively. In this set, BIRs are measured for azimuths between 90 and 90 , spaced by 5 , at a distance of 1.5 m from the HATS. The sampling rate of the BIRs is 16 kHz, and we apply them to speech and noise signals directly.</p><p>Training utterances come from the training set of the TIMIT corpus <ref type="bibr" target="#b9">[10]</ref>, and the test utterances from the test set. Hence there is no overlap between the training and test utterances. The babble noise from the NOISEX corpus <ref type="bibr" target="#b34">[35]</ref>, about 4 minutes long, is divided into two parts with the first part (106s) used in training and the second part (128s) in testing. Thus there is no overlap in training and test noise segments either. To create a mixture, a noise segment is randomly cut from the training or testing part to match the length of a target utterance. We should note that the motivation of choosing the babble noise as interference is to simplify experimental setup as it is well known that binaural segregation relies on binaural cues, not signal content. As discussed in Section VI, similar results are obtained with interfering speech.</p><p>As described later, our evaluation is conducted in 2-source, 3-source, and 5-source configurations. To isolate location-based segregation from localization, we fix the target source at azimuth 0 , i.e. just in front of the dummy head. More details on training configurations will be given in Section V-A. Regardless of configuration, we generate 500 binaural mixtures to train the DNN classifiers, and use 50 sentences to evaluate the performance of the proposed algorithm in each test condition. Irrespective of test SNRs, training mixtures always have 0 dB SNR. Using a fixed SNR for training, rather than SNR-dependent training, facilitates the potential application of the proposed algorithm. At the same time, it places a higher demand on generalization. The input SNR is measured at the left ear, by treating the reverberant target speech as the target signal in reverberant cases <ref type="bibr" target="#b29">[30]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criterion</head><p>The most straightforward way of measuring classification performance is classification accuracy. In this measure, miss and false-alarm (FA) errors are treated equally. However, as shown in <ref type="bibr" target="#b20">[21]</ref>, FA errors are much more detrimental to speech intelligibility than miss errors. As a result, we use HIT FA as our main evaluation criterion. The HIT rate is the percent of correctly classified target-dominant T-F units in the IBM, and the FA rate is the percent of wrongly classified interference-dominant T-F units. The local SNR criterion (LC) in the IBM definition is set to 0 dB. The HIT FA rate has been shown to be correlated to human intelligibility <ref type="bibr" target="#b19">[20]</ref>.</p><p>In addition to this measure of classification accuracy, we adopt the IBM-modulated SNR metric to account for the underlying signal energy of each T-F unit. The resynthesized speech from the IBM is used as the ground truth since the IBM is the ground truth of DNN classification <ref type="bibr" target="#b15">[16]</ref>: <ref type="bibr" target="#b8">(9)</ref> Here, and denote the signals resynthesized from the IBM and an estimated IBM, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison Systems</head><p>We compare the performance of the proposed method with four representative binaural separation methods. Roman et al.'s method <ref type="bibr" target="#b29">[30]</ref> performs binaural segregation in multi-source reverberant environments. They extract the reverberant target signal from a multisource reverberant mixture by utilizing the location information of the target source. Their system combines target cancellation through adaptive filtering and a binary decision to estimate the IBM.</p><p>Another comparison system is DUET <ref type="bibr" target="#b26">[27]</ref> which is a popular blind source separation method and produces a binary mask. It assumes that the time-frequency representation of speech is sparse, the so-called W-disjoint orthogonality. It can separate an arbitrary number of sources using only two microphones.</p><p>The recent system of Woodruff and Wang <ref type="bibr" target="#b40">[41]</ref> formulates the IBM estimation problem as a search through a multisource state space across time, where each multisource state encodes the number of active sources, and the azimuth and the pitch of each active source. A set of MLPs are trained to assign a T-F unit to one of the active sources in each multisource state. They use a hidden Markov model framework to estimate the most probable path through the multisource state space. This system is particularly relevant as it combines binaural and monaural (pitch) cues.</p><p>A joint localization and segregation approach <ref type="bibr" target="#b22">[23]</ref>, dubbed MESSL, uses spatial clustering for source localization. Given the number of sources the system iteratively modifies GMM models of interaural phase difference and ILD to fit the observed data using an expectation-maximization procedure. Across frequency integration is handled by linking the GMM models in individual frequency bands to a principal ITD. In order to compare with the other systems that all produce binary masks as output, we binarize the MESSL output (with the threshold of 0.5). Note that the binarization does not reduce MESSL's output SNR.</p><p>For Roman et al., Woodruff-Wang, and MESSL systems, we use the implementations provided by their respective authors. The DUET implementation comes from its author's book <ref type="bibr" target="#b26">[27]</ref>. All comparison system parameters are adjusted to get the optimal results. To run DUET and MESSL, we provide them the correct number of sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION AND COMPARISON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DNN Classification Using Binaural Features Only</head><p>We first examine the case without monaural GFCC features. This also facilitates comparison with other binaural segregation algorithms. In all the training and test conditions of this section, the target azimuth is fixed at 0. We use BIR Set A to train and test CCF and ILD features systematically. First, we train and test our system with one interference at the azimuth of 45 (i.e. to the left side), and the test SNR of 0 dB. Fig. <ref type="figure" target="#fig_1">2</ref> shows the classification results for a few and compares three kinds of binaural features. With reverberation increasing, the results of all feature kinds decrease, and the gap between 34D features (CCF+2D-ILD) and other two lower dimensional features becomes greater. The HIT FA rate of the 34D features is 5% (absolute) better than the two-value ITD+1D-ILD features in the anechoic condition, and 25% better at s. In heavily reverberant conditions, strong reflections make the target segregation difficult with only two-dimensional binaural features. CCF features are robust to reverberation. In comparison, 2D-ILD performs slightly better than 1D-ILD.</p><p>We present HIT FA and SNR results for two-source segregation at 5 dB in Table <ref type="table" target="#tab_1">I</ref>. The results are obtained in the anechoic condition with the interference placed at 45 . As in Fig. <ref type="figure" target="#fig_1">2</ref>, 34D features yield the best performance. The 32D-CCF features provide more detailed information about the binaural input than the 1D-ITD feature. 2D-ILD also performs slightly better than 1D-ILD on all evaluation criteria. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the cochleagram results for a TIMIT test utterance mixed with the babble noise at 5 dB. As shown in the figure, 34D features give the best performance (see for instance the energy burst around 2.5s and 857 Hz) and recover nearly all of the target speech energy in this low SNR condition. Because of their superior performance, we will use 34D binaural features, i.e. CCF+2D-ILD, in subsequent evaluations.</p><p>To examine the performance difference between trained and untrained azimuths, we evaluate the system in 2, 3 and 5 sound sources. In the two-source condition, the single interference is located at 45 . In the three-source condition, the two interfering sources are located at the azimuth angles of 45 and 45 . Finally, in the five-source condition, the four interfering sources are located at the azimuths of 45 , 45 , 135 and 135 . These test configurations are the same as in <ref type="bibr" target="#b29">[30]</ref>.</p><p>We train the DNN in two scenarios. In the unmatched training scenario, the interference sources are systematically varied between 0 and 350 , spaced by 10 . More specifically, in 2-source configurations, the single interference is varied systematically. In 3-source configurations, one interference is randomly chosen from the left side and the other interference  randomly chosen from the right side. In 5-source configurations, each of the 4 interfering sources is chosen from a unique quadrant (i.e. the 90 range) of the azimuth space, with the 4 quadrants together covering the entire space. In both 3-and 5-source configurations, all multiples of 10 of the azimuth space have been used during training. In this unmatched training scenario, test (evaluation) results are obtained from untrained interference locations. In the matched training scenario, test interference locations are the same as used in training the DNN. Fig. <ref type="figure" target="#fig_3">4</ref> shows the classification results in both scenarios. As shown in the figure, the performance gap between trained and untrained azimuths is not large. In the two-source condition, the untrained-azimuth results are lower than the trained-azimuth results by 3% in HIT FA. This average HIT FA gap is 4% in the three-source condition, and 2% in five-source condition.</p><p>To more closely compare between trained and untrained azimuths, Fig. <ref type="figure" target="#fig_4">5</ref> shows 2-source segregation results in the anechoic condition by systematically varying training and test azimuths. In Fig. <ref type="figure" target="#fig_4">5(a)</ref>, the interference azimuth used in training varies between 0 and 350 , spaced by 10 . In testing, we place the interference at the azimuths between 0 and 355 in 5 steps. In this way, half of the interference azimuths are used in training whereas the other half are not. As shown in Fig. <ref type="figure" target="#fig_4">5</ref>(a), the HIT FA rates are above 80% for most interference azimuths and close to 90% for some azimuths. When the interference locations are close or opposite to the target sound, at azimuths of 0 , 5 , 175 , 180 , 185 and 355 , the HIT FA rates are down to as low as 30%. This is to be expected as the proposed system operates on the basis of binaural cues only, which have trouble distinguishing an azimuth in the front from its mirror azimuth in the back. Overall, the trained locations yield a little higher HIT FA rates than the nearby untrained locations. At the better ear side (i.e. the side with higher SNR, the right side in this case), for the interference located between 185 and 355 , the performance differences between trained and untrained locations are small. In Fig. <ref type="figure" target="#fig_4">5</ref>(b), we train our system at 4 interference azimuths of 60 , 120 , 240 and 300 , but evaluate interference azimuths at every 5 . As expected, these trained locations produce the four peaks of HIT FA rates, which gradually decrease as the test interference moves away from the trained locations. The performance asymmetry for the untrained azimuths between the left and the right side is due to the fact that the input SNR is measured at the left ear. Comparing the results in Fig. <ref type="figure" target="#fig_4">5</ref>(a) and Fig. <ref type="figure" target="#fig_4">5</ref>(b), it is clear that the more the trained angles cover the azimuth space, the better the trained system performs at untrained angles. The next evaluation tests the system performance by varying the input SNR. In this evaluation we use the babble noise at azimuth between 0 and 350 spaced by 10 to train the DNNs. Then an untrained interference angle at 45 is used to test the system. No reverberation is considered. Note that only the input SNR of 0 dB is used in training. The classification and SNR results are shown in Table <ref type="table" target="#tab_2">II</ref>. The proposed system produces excellent performance in terms of HIT FA and SNR. As the input SNR decreases, the HIT FA rate decreases gradually. With the input SNR of 15 dB, the HIT FA rate of 76.95% is still high; as a reference, this result is higher than the monaural segregation method at 5 dB SNR <ref type="bibr" target="#b19">[20]</ref>. Our informal listening indicates that we can recognize segregated speech at this very low SNR of -15 dB.</p><p>We now compare our classification system and three related systems in Table <ref type="table" target="#tab_3">III</ref>. The Woodruff-Wang method is not included in this comparison because it use both binaural and monaural cues; it will be compared in the next subsection. The test results from our system in the anechoic condition are generated from the untrained interference azimuths. Note that the input SNR of 5 dB is not used in training. The proposed system produces the best results in all test conditions. The MESSL results are better than those of the other two comparison systems, both of which also produce improved SNRs in all test conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Incorporation of Monaural Features</head><p>We first evaluate whether GFCC features enhance classification performance. The first feature set is 34D binaural-only features, and the second feature set includes 36D monaural GFCC features to form 70D joint binaural and monaural features in each T-F unit pair. Fig. <ref type="figure" target="#fig_5">6</ref> compares two-source segregation in the anechoic condition where interference azimuth varies in the training and test between 0 and 350 , spaced by 10 . As shown in the figure, the joint feature set gives the better performance at all interference azimuths. When the interference is close to the target speech, i.e. at 180 , the HIT FA rate of the binaural feature set drops to 31%, and the joint feature set improves the results to 41%, or by 10%. Similar improvement occurs at the interference azimuth of 0 . When the interference is 10 degrees or more away from the target speech, the joint feature set performs slightly better (about one percent).</p><p>With reverberation time s, we evaluate the proposed and comparison systems in 2, 3 and 5 source conditions. This comparison also includes the Woodruff-Wang algorithm <ref type="bibr" target="#b40">[41]</ref>, which is designed for reverberant source segregation and incorporates a monaural pitch cue. The SNR results are given in Table IV. Our system produces the best results in all test conditions, almost 5 dB better than the other systems. The performance of the proposed system is not affected by the number of the interfering sources. All of the comparison systems also produce SNR improvements in all test conditions. Compared to TABLE III, reverberation drops the SNR performance of the comparison systems by about 4 dB.</p><p>Next, we use BIR Set A with of 0.3s to test the generalization of the 70D joint feature set GFCC+CCF+2D-ILD in the reverberant condition. As in Fig. <ref type="figure" target="#fig_4">5</ref>(a), we use the interference azimuths between 0 and 350 spaced by 10 to train   the DNNs. We then place the interference at the azimuths between 0 and 355 in 5 steps to evaluate the trained system. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>(a), the HIT FA rates are above 38% at all interference azimuths and close to 70% for most of the test azimuths. When the interference azimuths are close to the target sound or its mirror angle, at azimuths of 0 , 5 , 175 , 180 , 185 and 355 , the HIT FA rates are down to 40%. Note that, in this reverberant condition the untrained locations yield similar HIT FA rates to the nearby trained locations. The disappearance of the small gap seen in Fig. <ref type="figure" target="#fig_4">5(a</ref>) is due to the use of GFCC features, which are insensitive to azimuth. In Fig. <ref type="figure" target="#fig_6">7</ref>(b), we train the system at 6 azimuths of 0 , 60 , 120 , 180 , 240 and 300 . This way of training produces the four high peaks of HIT FA at the trained azimuths of 60 , 120 , 240 , and 300 . The HIT FA rates decrease as the test interference locations move away from the trained azimuths. Comparing the results in Fig. <ref type="figure" target="#fig_6">7</ref>(a) and Fig. <ref type="figure" target="#fig_6">7</ref>(b), it is clear that with more trained angles, the trained system performs better at untrained angles, similar to Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>We now compare the proposed system with the four comparison systems in the 5-source environment with different levels of reverberation. We use BIR Set A with 0.3 s and 0.7s </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation with Recorded BIRs</head><p>In the following experiments, we use the measured BIR Set B to evaluate our system for 2-source segregation. The babble noise located between 90 and 90 spaced by 10 is used to train the DNNs. We first compare the binaural-only feature set and the joint feature set in the four reverberant rooms. The noise is located at the untrained azimuth of 15 , producing 0 dB mixtures. As shown in Fig. <ref type="figure">9</ref>, the HIT FA rate difference between these two feature sets is, on average, 1.7%. The maximum gap is 2.7% in Room C with s. Fig. <ref type="figure" target="#fig_8">10</ref> illustrates the segregation results for a TIMIT test utterance mixed with babble noise at 0 dB in Room C with s. The joint features recover most of the target speech in this condition, producing a similar cochleagram to that of the target speech.</p><p>We next present more detailed results of the DNN classification system with joint features at the untrained interference angle of 45 in Table <ref type="table" target="#tab_5">V</ref>. As shown in the table, the proposed system produces strong performance in terms of both HIT FA  and SNR. As reverberation increases, the HIT FA rate decreases only gradually. Even in Room D with of 0.89s, the HIT FA is still high. Comparing with the results in Fig. <ref type="figure">9</ref>, we note that the larger azimuth separation in Table V increases the HIT FA rate.</p><p>Table <ref type="table" target="#tab_6">VI</ref> shows SNR comparisons for the test mixtures at 5 dB. The test azimuth of the babble noise is the untrained 15 . Consistent with the results using simulated BIRs, the proposed system gives the best results in all conditions. Woodruff-Wang and MESSL outperform the other two systems in most of the conditions.</p><p>We have also compared the proposed system with the others for 0 dB mixtures with the interference located at 15 or 45 . Similar SNR improvements are obtained as for 5 dB mixtures in Table <ref type="table" target="#tab_6">VI</ref>. With interference farther away from the target speech, the performance increases as concluded in Section V-B, with the only exception of the Roman et al. method that shows little change as this method uses adaptive filtering to segregate speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>The main contributions of this study can be summarized in the following. To our knowledge, this is the first study that introduces deep neural networks to binaural segregation. Consistent with an earlier comparison in the monaural domain <ref type="bibr" target="#b38">[39]</ref>, we find that DNN classifiers outperform MLP classifiers with the same features. Our second contribution lies in the novel use of multi-dimensional CCF and ILD features, as well as the introduction of monaural GFCC features to complement binaural features. Our DNN-based algorithm with joint binaural and monaural features enables us to achieve substantially better results than four representative binaural separation algorithms. Even at very low input SNRs and with strong reverberation, the proposed system yields excellent segregation performance, which decreases only gradually with increased room reverberation.</p><p>The results from our evaluation indicate encouraging generalization to untrained spatial configurations. This is important for supervised learning algorithms. Dependency on trained configurations is a main limitation of the first supervised classification method of Roman et al. <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> for binaural segregation. The key to overcome this limitation is to train with a variety of configurations and the apparent generalization ability of deep neural networks. Training with a variety of configurations also allows the system to perform binaural segregation without sound localization, in contrast to localization-based segregation <ref type="bibr" target="#b35">[36]</ref>.</p><p>Our evaluations have used babble noise as the interfering signal. As mentioned in Section IV-A, this choice was motivated by simplicity and the consideration that binaural segregation is primarily driven by binaural cues, not specific signals presented at different azimuths. Even though it may be unrealistic to have a speech babble from a particular angle, there is no reason to expect that the performance of our system, once trained, will change a lot depending on the content of interference. Indeed, we have conducted a two-source segregation experiment similar to Fig. <ref type="figure" target="#fig_4">5</ref>(a) except that TIMIT utterances, different from the target ones, are presented at the trained azimuth of 20 and the untrained azimuth of 15 . The HIT FA (and SNR) results are very close to those in Fig. <ref type="figure" target="#fig_4">5</ref>(a) at both interference azimuths.</p><p>As in previous studies (e.g. <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b21">[22]</ref>), we fix the target direction to 0 in our evaluations. This choice corresponds to the target signal coming from the look direction, a common assumption made in directional hearing aids <ref type="bibr" target="#b8">[9]</ref>. Our classification framework is, however, not limited to this target direction and other target directions can be similarly trained. For example, we have trained a two-source configuration where the target is placed at 30 and the interference at 0 with 0 dB SNR (similar to Fig. <ref type="figure" target="#fig_4">5(a)</ref>). We then test the trained system at the trained target angle and an untrained target angle at 35 . For both trained and untrained target azimuths, we observe similar HIT FA and SNR results to Fig. <ref type="figure" target="#fig_4">5</ref>(a) with the target at 0 .</p><p>We believe that the classification framework is a very promising direction for future development <ref type="bibr" target="#b11">[12]</ref>. In this framework, for example, it is straightforward to include monaural features to complement binaural features for improved segregation, especially when the target and interfering sources are either collocated or close to one another. We can expect further improvements by including more binaural and monaural features (see e.g. <ref type="bibr" target="#b38">[39]</ref>), as well as concatenating features from neighboring time frames to incorporate temporal dynamics. The seamless integration of binaural and monaural cues in the classification framework provides a natural way for the system to leverage whatever discriminant features that exist in a particular environment to segregate the target signal, a characteristic of human auditory scene analysis <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of the proposed binaural DNN classification system.</figDesc><graphic coords="2,91.02,63.12,408.00,60.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two-source segregation for trained azimuths at 0-dB SNR.</figDesc><graphic coords="4,331.02,64.14,192.00,115.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segregation illustration for a TIMIT utterance mixed with a babble noise at -5 dB. (a) Cochleagram of the mixture. (b) Cochleagram of the target utterance. (c) Cochleagram of separated speech with CCF+2D-ILD features. (d) Cochleagram of separated speech with ITD+2D-ILD features. (e) Cochleagram of separated speech with ITD+1D-ILD features.</figDesc><graphic coords="5,61.98,64.14,468.00,180.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. HIT FA performance at trained and untrained azimuths in anechoic and two reverberant conditions. We train and test with 0-dB mixtures. (a) 2-source segregation. (b) 3-source segregation. (c) 5-source segregation.</figDesc><graphic coords="6,70.02,63.12,451.02,106.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. HIT FA performance for two-source segregation at various interference training azimuths and 0-dB SNR. (a) 36 interference azimuths are used in training. (b) 4 interference azimuths are used in training.</figDesc><graphic coords="6,61.02,211.14,205.98,234.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. HIT FA performance for two-source segregation on the 0-dB test set.</figDesc><graphic coords="7,49.98,64.14,229.98,93.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. HIT FA performance for two-source segregation at various interference training azimuths with joint features in the reverberant condition at 0 dB. (a) 36 interference azimuths are used in training. (b) 6 interference azimuths are used in training.</figDesc><graphic coords="7,329.04,261.12,200.94,180.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. SNR comparisons in the 5-source environment where speech utterances are mixed with the babble noise at 0 dB.</figDesc><graphic coords="8,45.00,64.14,237.16,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Segregation illustration for a TIMIT utterance mixed with babble noise in Room C at 0-dB SNR. (a) Cochleagram of the reverberant mixture. (b) Cochleagram of the reverberant target utterance. (c) Cochleagram of separated speech.</figDesc><graphic coords="9,57.00,64.14,478.98,84.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Binaural Classification for Reverberant Speech Segregation Using Deep Neural Networks Yi Jiang, Student Member, IEEE, DeLiang Wang, Fellow, IEEE, RunSheng Liu, and ZhenMing Feng</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON TWO-SOURCE SEGREGATION AT dB FOR TRAINED AZIMUTHS WITH DIFFERENT KINDS OF BINAURAL FEATURES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II TWO</head><label>II</label><figDesc>-SOURCE BINAURAL SEGREGATION RESULTS WITH RESPECT TO INPUT SNR</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SNR</head><label>III</label><figDesc>(dB) PERFORMANCE COMPARISONS IN MULTISOURCE SEGREGATION WITH NO REVERBERATION AND THE INPUT SNR OF 5 dB</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V TWO</head><label>V</label><figDesc>-SOURCE SEGREGATION RESULTS IN FOUR REVERBERANT ROOMS AT THE INPUT SNR OF 0 dB</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI SNR</head><label>VI</label><figDesc>COMPARISONS IN TWO-SOURCE SEGREGATION USING MEASURED IMPULSE RESPONSES FROM FOUR REVERBERANT ROOMS AT THE INPUT SNR</figDesc><table><row><cell>OF</cell><cell>dB.</cell><cell>(IN S) IN EACH ROOM IS LISTED IN PARENTHESES</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>RunSheng Liu, photograph and biography not provided at the time of publication.ZhenMing Feng, photograph and biography not provided at the time of publication.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank the Ohio Supercomputing Center for providing computing resources. The authors also thank Michael Mandel, Nicoleta Roman, Yuxuan Wang, and John Woodruff for making implementations of their algorithms available to us.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wang was supported in part by the Air Force Office of Scientific Research under Grant FA9550-12-1-0130. This work was performed while the first author was a visiting scholar at the Ohio State University. A preliminary version of this paper was published by Interspeech 2014 <ref type="bibr" target="#b17">[18]</ref>. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Mads</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Determination of the potential benefit of time-frequency gain manipulation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Anzalone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Calandruccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="480" to="492" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Spatial hearing: The psychophysics of human sound localization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blauert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Auditory scene analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Brungart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4007" to="4018" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A MATLAB simulation of &quot;shoebox&quot; room acoustics for use in research and teaching</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Palo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Inf. Syst. J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="48" to="51" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
		<title level="m">On Human Commun</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Listening to speech in the presence of other sounds</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Darwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosoph. Trans. R. Soc. B: Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">1493</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dillon</surname></persName>
		</author>
		<title level="m">Hearing Aids</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Boomerang</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TIMIT Acoustic-Phonetic Continuous Speech Corpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An algorithm to improve speech recognition in noise for hearing-impaired listeners</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3029" to="3038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A classification based approach to speech segregation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3475" to="3483" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask estimation for missing data speech recognition based on statistics of binaural interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Binaural interference in lateralization thresholds for interaural time and level differences</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="319" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monaural speech segregation based on pitch tracking and amplitude modulation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1135" to="1150" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic precedence effect modeling for source separation in reverberant environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hummersone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1867" to="1871" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Binaural deep neural network classification for reverberant speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2400" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A supervised learning approach to monaural segregation of reverberant speech</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm that improves speech intelligibility in noise for normal-hearing listeners</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A channel-selection criterion for suppressing reverberation in cochlear implants</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kokkinakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hazrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3221" to="3232" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1673" to="1682" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model-based expectationmaximization source separation and localization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="382" to="394" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A binaural scene analyzer for joint localization and recognition of speakers in the presence of interfering noise sources and reverberation</title>
		<author>
			<persName><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kohlrausch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2016" to="2030" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Localization by harmonic structure and its application to harmonic sound stream segregation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="653" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SVOS final report, part B: Implementing a gammatone filterbank</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nimmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holdsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MRC Appl. Psychol</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Unit, Rep. 2341</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The DUET blind source separation algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
		<editor>Blind Speech Separation, S. Makino, T. Lee, and H. E. Sawada</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech segregation based on sound localization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2236" to="2252" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A classification-based cocktail-party processor</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1425" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Binaural segregation in multisource reverberant environments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4040" to="4051" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intelligibility of reverberant noisy speech with ideal binary masking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2153" to="2161" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Bayesian classifier for spectrographic mask estimation for missing feature speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A sound segregation algorithm for reverberant conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shamsoddini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Denbigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognition of speech in noise after application of timefrequency masks: Dependency on frequency and threshold parameters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Sinex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2390" to="2396" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Computational Auditory Scene Analysis: Principles, Algorithms, and Applications</title>
		<editor>D. L. Wang and G. J. Brown</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley-IEEE Press</publisher>
			<pubPlace>Hobboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speech intelligibility in background noise with ideal binary timefrequency masking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kjems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2336" to="2347" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On ideal binary mask as the computational goal of auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Separation by Humans and Machines</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Divenyi</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Binaural detection, localization, and segregation in reverberant environments based on joint pitch and azimuth cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="806" to="815" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CASA-based robust speaker identification</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1608" to="1616" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">His research interests include computational auditory scene analysis, speech separation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">B E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">respectively, and the Ph.D. degree in electrical engineering</title>
		<meeting><address><addrLine>Wuhan, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001 and 2004. 2014</date>
		</imprint>
		<respStmt>
			<orgName>Huazhong University of Science and Technology ; Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>S&apos;10) received the. speech enhancement, and machine learning</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
