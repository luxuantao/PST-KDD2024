<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Load Value Prediction using Multiple Predictors and Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rami</forename><surname>Sheikh</surname></persName>
							<email>ralsheik@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Derek</forename><surname>Hower</surname></persName>
							<email>dhower@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm Technologies, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Load Value Prediction using Multiple Predictors and Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA.2019.00057</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Value prediction [1], <ref type="bibr" target="#b1">[2]</ref> has the potential to break through the performance limitations imposed by true data dependencies. Aggressive value predictors can deliver significant performance improvements, but usually require large hardware budgets. While predicting values of all instruction types is possible, prior work has shown that predicting just load values is most effective with a modest hardware budget (e.g., 8KB of prediction state [3], [4]). However, with hardware budget constraints and high prediction accuracy requirements (99%), prior work has struggled to increase the fraction of predicted loads (coverage) beyond the low 30s.</p><p>In this paper, we analyzed four state-of-the-art load value predictors, and found that they complement each other. Based on that finding, we evaluated a new composite predictor that combines all four component predictors. Our results show that the composite predictor, combined with several optimizations we proposed, improve the benefit of load value prediction by 54%-74% depending on the total predictor budget. Moreover, our composite predictor delivers more than twice the coverage of first championship value prediction winner predictor (EVES [4]), and substantially increases the delivered speedup by more than 50%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As microarchitectures continue to extract more Instruction Level Parallelism (ILP) from increasing out-of-order scheduling windows (e.g., 97 instructions in Intel Skylake <ref type="bibr" target="#b4">[5]</ref>), performance is increasingly limited by true data dependencies in a program. Value prediction is a technique to break those true dependencies by allowing consumer instructions to speculatively execute ahead of their producer. Value prediction works because instructions exhibit value locality, meaning that the same static instruction often produces a predictable value <ref type="bibr" target="#b0">[1]</ref>. In the case of load instructions, it is also possible to predict a load memory address, followed by a data cache access, to generate a speculative value that does not necessarily exhibit value locality (e.g., DLVP <ref type="bibr" target="#b2">[3]</ref>). While value predictors can generate speculative results for all instruction types, recent work has shown that load-only predictors are most efficient with a modest hardware budget (e.g., 8KB) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this study, we investigated techniques to increase the effectiveness of load value prediction. First, we thoroughly analyzed and compared four state-of-the-art load value predictors <ref type="foot" target="#foot_0">1</ref> , shown in Table <ref type="table" target="#tab_0">I</ref>, to determine how they complement Last Value Prediction (LVP) <ref type="bibr" target="#b0">[1]</ref> Stride Address Prediction (SAP) <ref type="bibr" target="#b5">[6]</ref> Context aware Context Value Prediction (CVP) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> Context Address Prediction (CAP) <ref type="bibr" target="#b2">[3]</ref> one another. We found that no individual predictor is strictly better than another since they all target loads with different characteristics. Further, we found that a composite predictor that uses all four variants simultaneously outperforms any single predictor in isolation for the same hardware state budget.</p><p>Second, we investigated techniques to increase the effectiveness of a composite load value predictor. We improved a composite predictor by:</p><p>• Adding an Accuracy Monitor (AM) that throttles component predictors when their accuracy drops below a threshold, • Using heterogeneous component predictor table sizes to find the best use of limited resources, • Using a smart training algorithm that avoids redundantly training multiple predictors that are equally effective, and • Dynamically fusing predictor tables to reallocate resources from under-performing predictors to betterperforming predictors.</p><p>After combining all the techniques, we found that a composite predictor can best the performance of a single component predictor of the same size by 54%-74%, depending on the total predictor budget. We found that many of the optimizations are most effective at small to modest predictor sizes, thus allowing designers to trade off design complexity and area with similar performance. Moreover, we show that our composite predictor significantly outperforms the winner of first championship value prediction (EVES <ref type="bibr" target="#b3">[4]</ref>) in terms of coverage (more than doubles) and speedup (increases by more than 50%).</p><p>In the rest of this paper, we present the results of a comprehensive design space exploration of composite load value prediction. We present the details of our methodology and evaluation framework in Section II. Then, we explore the theory and performance of the four state-of-the-art  component predictors in Sections III and IV, including an analysis of how the predictors overlap. Section V contains the design and analysis of a composite predictor, including the four aforementioned optimizations. Finally, we conclude in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY AND EVALUATION FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>We cast a wide net to expose as many load address and value occurrence patterns as possible. We use benchmarks from the following benchmark suites: SPEC2K <ref type="bibr" target="#b8">[9]</ref>, SPEC2K6 <ref type="bibr" target="#b9">[10]</ref>, and EEMBC <ref type="bibr" target="#b10">[11]</ref>. Moreover, we enriched our benchmark pool with other popular applications: Linpack <ref type="bibr" target="#b11">[12]</ref>, media player <ref type="bibr" target="#b12">[13]</ref>, browser benchmark <ref type="bibr" target="#b13">[14]</ref>, and various Javascript benchmarks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Table <ref type="table" target="#tab_2">II</ref> shows a list of our benchmarks. We use 100million instruction simpoints, except for short-running benchmarks (i.e., EEMBC), we simulate the first 100 million instructions, or until the benchmark completes.</p><p>All benchmarks are compiled to the ARM ISA using gcc with -O3 level optimization: SPEC2K and SPEC2K6 are compiled for ARMv8 (aarch64), and the remaining benchmarks are compiled for ARMv7.</p><p>Unless otherwise noted, we present results as the arithmetic average across all workloads (geometric for IPC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Framework</head><p>All results were collected using our internally-developed, cycle-accurate, RTL-validated, industry simulator. The simulator runs ARM ISA binaries (v7 and v8). It is used by our CPU research and development organization. Because the simulator is specific to our proprietary custom ARM CPU design, we do not release it, and there is nothing publicly available that we can cite.</p><p>The parameters of our baseline core are configured as close as possible to those of Intel's Skylake core <ref type="bibr" target="#b4">[5]</ref>. The baseline core uses state-of-art TAGE and ITTAGE branch predictors <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and a memory dependence predictor similar to Alpha 21264 <ref type="bibr" target="#b22">[23]</ref>. We use a fetch-to-execute latency of 13 cycles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical RF 348</head><p>Table <ref type="table" target="#tab_2">III</ref>: Baseline core configuration.</p><p>produced by dynamic instances of the same static instruction exhibit value locality. For example, some instructions generate a constant value for a computation while others might normally produce the same return value indicating successful completion. The result of any instruction type can be predicted, though in this paper we focus only on predicting load values since that is most effective with limited hardware resources <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Value Prediction Approaches</head><p>There are two general approaches to load value prediction, which we discuss briefly below. The goal of both is to have a predicted value ready by the time any consumer of the load enters the instruction queue (aka the scheduler). If the predictors can get a correct value in time, then consumers can execute immediately, making it appear that the load has a zero-cycle load-to-use latency. Predictions are validated when the predicted load executes. If a prediction is found to be incorrect, recovery actions take place. In this work we assume a flush-based recovery microarchitecture, similar to the work of Perais and Seznec <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Because the cost of a misprediction is usually high, it is important for value predictors to deliver very high accuracy (e.g., 99% of predictions correct).</p><p>One approach to load value prediction is to directly predict the value that a static load will produce (e.g., the load at PC X always returns zero). In Figure <ref type="figure" target="#fig_0">1</ref>, we show the changes to support this style of value prediction in a baseline architecture.</p><p>In step ❶, the value predictor is probed as a load is fetched, and if a high confidence prediction is found, the value is forwarded to the Value Prediction Engine (VPE) in step ❷ (we discuss the filter later when combining predictors). <ref type="foot" target="#foot_1">2</ref> VPE provides the mechanism needed to communicate the predicted values from the value-predicted producers to their consumers <ref type="bibr" target="#b2">[3]</ref>. Consumers of the load can use the prediction by reading the stored value out of the VPE rather than waiting on a physical register to be ready. When the load executes, the correct value is read from the data cache and is validated  against the speculative value. The predictor updates in step ❻ and, if a misprediction is detected, the affected instructions are flushed and fetch is redirected to the recovery address.</p><p>Another approach to load value prediction is to use address prediction, combined with a data cache read, to generate a speculative value. Load value prediction through address prediction is similar to data prefetching except that the address is predicted when a load is fetched so that its data can be ready in the pipeline by the time any consumer of the load enters the scheduler. Address predictors are probed when a load is fetched in step ❶, and if a high confidence prediction is found, the address is forwarded to the Predicted Address Queue (PAQ) in step ❷. The PAQ waits for bubbles in the load pipeline, and when it finds one, probes the data cache with a predicted address in step ❸. If the address hits in the data cache, then in step ❹, the value is forwarded to the VPE. As long as the data returns before a consumer of the load reaches rename, the load will appear to have a zero-cycle load-to-use latency. If the predicted address misses in the data cache, we can optionally generate a data prefetch request in step ❺ to accelerate the eventual execution of the predicted load (this feature is disabled in our work). If a predicted value is used, then the value (note, checking the address is insufficient as the value may have changed) of the load must be checked when the load executes, and if the speculative value was incorrect, a misprediction recovery is initiated in step ❻.</p><p>To learn more about the two load value prediction approaches and the recent advances towards practical implementations of value prediction, we encourage the readers to visit prior art papers <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Regarding memory consistency, ARM's relaxed consistency model allows for reordering most memory operations with one exception: dependent loads are not allowed to be reordered <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Value prediction can violate this rule. To avoid violating ARM's memory consistency model, we employ a technique similar to the work of Martin et al. <ref type="bibr" target="#b25">[26]</ref>. Also, address/value prediction is not used with memory ordering instructions, atomic and exclusive memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Value and Address Prediction Strategies</head><p>We studied four different value predictors that we believe cover the current state of the art. We discuss how each predictor works, and then analyze the load types they target, where the predictors overlap, and how efficient each is in terms of hardware storage and power. We group predictors based on whether or not they consider program history (context) when making a prediction since doing so tends to increase accuracy but also decrease storage efficiency.</p><p>While the high-level concept of each predictor is based on published literature, the details (size, confidence, etc.) are based on tuning from our own evaluation (details in Section II). Because the cost of a value misprediction is so high, we tuned each predictor to achieve 99% accuracy (thereby sacrificing coverage). Our evaluation showed that lower accuracy tends to decrease performance gains. Readers may notice that our reported benefit for each design is sometimes significantly lower than previously published results; this is caused by different assumptions about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b26">[27]</ref>).</p><p>In all of the studied predictors, we use forward probabilistic counters to reduce the number of bits needed to track confidence <ref type="bibr" target="#b27">[28]</ref>. Using FPC, a confidence counter is incremented with probability P, where P is found by indexing the FPC vector in Table <ref type="table" target="#tab_0">IV</ref> with the current confidence value. By probabilistically incrementing, FPC lets us represent, for example, a 0..N − 1 counter using fewer than log 2 N bits. To determine the FPC vector size and values, we first ran experiments using scalar confidence counters to determine the smallest confidence level that leads to 99% accuracy, and then construct an FPC vector that gives the same effective confidence using fewer bits.</p><p>1) Context-agnostic Predictors: Last Value Prediction (LVP) A last value predictor exploits the fact that consecutive dynamic instances of a static load will often produce the same value. This commonly occurs, for example, with PCbased loads that read large constants. The pattern can also occur when dynamic instances of a static load produce different addresses, such as when sequencing through an array Table IV: Predictor parameters. We empirically determined confidence thresholds based on a 99% accuracy target. The confidence is listed both as the absolute threshold (i.e., the counter value) and the effective level considering FPC (i.e., the expected number of observations before achieving high confidence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bit fields / entry</head><p>Conf FPC Vector History Length @ 1K entries just initialized with memset. Last value predictors can be viewed as members of the stride value predictors <ref type="bibr" target="#b28">[29]</ref> family, where the stride is zero. In our evaluation, we observed very limited presence of stride loaded values (though did find strided values for other instruction types such as arithmetic instructions), therefore, we excluded stride value predictors from our pool of candidate predictors. LVP uses a PC-indexed, tagged prediction table. Each entry contains a 14-bit tag, 64-bit value, and a 3-bit saturating confidence counter, for a total of 81 bits per entry. LVP is trained when a load executes by hashing the PC bits of a load to access an entry and then updating the entry's tag and value. If the new tag/value match the existing tag/value, then we probabilistically increase the confidence; otherwise, the confidence is reset to zero.</p><p>To make a prediction, the PC of a newly fetched load is hashed to access an entry, and if the tag matches and the confidence is above the threshold, then the stored value will be used as a prediction. We find, like other work before <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, that LVP needs a high confidence to avoid reducing performance through mispredictions, and therefore use a confidence threshold of 7, which, with the FPC vector shown in Table <ref type="table" target="#tab_0">IV</ref>, corresponds to an effective confidence of 64 consecutive observations of a value. Stride Address Prediction (SAP) A stride address predictor identifies static loads that produce strided addresses (possibly with stride = 0), and then probes the data cache to retrieve a predicted value. The stride detection logic is similar to the logic in a stride-based data prefetcher.</p><p>In our implementation of SAP, we maintain a PC-indexed, tagged prediction table. Each entry contains a 14-bit tag, a 49-bit virtual address representing the last known load address for the PC, a 2-bit saturating confidence counter, a 10-bit stride, and a 2-bit load size indicator, for a total of 77 bits per entry.</p><p>To train SAP, when a load executes, it hashes the PC to identify a predictor table entry, writes the delta between the load address and the last known load address into the stride field, and updates the size field to the log base two of the load width. If the tag entry matches and the calculated stride equals the stored stride, then the confidence counter is incremented; otherwise, the confidence counter is reset to zero. Because short-lived strides cause many mispredictions, we found that a modest confidence of 9 consecutive observations was necessary to achieve 99% accuracy.</p><p>After confidence is high, SAP produces a predicted address by adding the last known load address to the stride and sends the address to the PAQ where it will wait for a pipeline bubble and probe the data cache. The returned value is used to speculate while the predicted value is verified. Similar to the enhancements described for the stride value predictor in EVES <ref type="bibr" target="#b3">[4]</ref>, SAP takes into account the number of inflight occurrences of the load instruction, when making a prediction.</p><p>2) Context-aware Predictors: Context Value Prediction (CVP) A context value predictor uses program history along with load PC to generate more accurate predictions. CVP is inspired by branch prediction, which has long observed that branch behavior is correlated with the path history leading to the branch. The seminal work on CVP found that the same holds true for all instruction values <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and subsequent work confirmed the same is true for load instructions in particular <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our implementation of CVP is similar to VTAGE value predictor <ref type="bibr" target="#b6">[7]</ref> except that we excluded the untagged, lastvalue table. The predictor keeps three tables, all of which are indexed using a hash of the PC and a geometric sample of the branch path history. Each table entry stores a 14-bit tag, a 64-bit value, and 3-bit saturating confidence counter, for a total of 81 bits (same of LVP). When a load executes, all three tables are updated in a similar manner as LVP to train the predictor. When predicting a value, CVP uses a value from the table with the longest history whose entry has high confidence. Like LVP, CVP requires high confidence to build high accuracy, so we use forward probabilistic counters and set the confidence threshold to 4, corresponding to 16 consecutive observations. Context Address Prediction (CAP) A context address predictor also uses program history along with load PC to generate more accurate predictions, but uses the data cache as a value store rather than directly generating values from We use the state-of-the-art DLVP predictor as a reference design <ref type="bibr" target="#b2">[3]</ref>. The predictor consists of one tagged table indexed by a hash of PC and load path history. An entry contains a 14-bit tag, a 49-bit virtual address, a 2-bit confidence, and a 2-bit load size, for a total of 67 bits. When a load completes, it updates the table by setting the tag, value, and size. If the new tag, value, and size match the existing entry, the confidence is incremented; otherwise, the confidence is reset to zero. A prediction is made when a fetched load has a tag match and confidence is high. CAP has the lowest confidence threshold of all predictors, corresponding to four consecutive observations of a give path/load PC.</p><p>Please note that, for all of the studied predictors, the total storage can be considerably reduced by employing optimizations similar to the ones described for the enhanced VTAGE implementation in <ref type="bibr" target="#b3">[4]</ref> (e.g., decoupling the value/address arrays and then sharing them among the predictors). We believe that employing such optimizations is outside the scope of this work, and that it will not impact the findings presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPONENT PREDICTOR ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Load Classification</head><p>Each of the four studied load value predictors target different load characteristics. LVP is targeted at static loads that produce the same value. SAP targets static loads that produce a predictable (strided) address. CVP and CAP target dynamic loads that, when taken in context, produce predictable values or addresses, respectively.</p><p>We analyzed all benchmarks to determine the breakdown of loads covered by each of the four predictors. We placed dynamic instances of a load into one of three groups:</p><p>• Pattern-1 (LVP proxy): The PC of the load highly correlates with the load value</p><p>• Pattern-2 (SAP proxy): The PC of the load highly correlates with the load address • Pattern-3 (CVP/CAP proxy): All other loads The classification was performed using infinite resources i.e., we perfectly remember load values/addresses. The patterns are ordered and exclusive, such that any load fitting Pattern-1 will not be considered for Pattern-2 or Pattern-3. We prioritized the patterns based on the preference of their proxy predictors: value before address and context-unaware before context-aware. We prefer load value predictors first because they do not require a data cache access, and are not susceptible to cache misses, cache bandwidth constraints, and the increased power of a cache access. We prefer contextunaware predictors because they are more storage-efficient; a single entry in a context-unaware predictor can cover more dynamic loads than a single entry in a context-aware predictor.</p><p>As Figure <ref type="figure" target="#fig_1">2</ref> shows, the breakdown of loads is almost evenly split between Pattern-1 (LVP), Pattern-2 (SAP), and Pattern-3 (CAP and SAP). This result motivates a composite predictor design. Even though loads from a lower-numbered pattern may also be covered by a higher-number pattern, using the proxy predictor from a higher-number pattern would result in a less efficient design. We explore predictor overlap more in Section V-A while studying composite prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Speedup</head><p>We evaluated the speedup from each component predictor in isolation using the confidence values in Table <ref type="table" target="#tab_0">IV</ref>, which were chosen empirically to hit a 99% accuracy target. Figure <ref type="figure">3</ref> shows the performance improvement of each predictor as we scale the storage budget (by scaling the number of table entries from 64 -4K entries). <ref type="foot" target="#foot_2">3</ref> We observed that all four predictors hit a performance knee around 1K table entries (corresponds to 8-10KB storage), and therefore assume 1K entries as a starting baseline going forward. Even scaling out 0.0% 0.5% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Predictor Overlap</head><p>Even though all four predictors use different strategies to predict load values, there will still be overlap in the predictions they provide. While the overlap may be an indication of inefficient resource usage in a composite predictor, it can at times be beneficial due to effects like training time or aliasing. <ref type="foot" target="#foot_3">4</ref> In this subsection, we explain this subtlety through a simple illustrative example. Consider the loop in Listing 1. The load on line 5 is highly predictable by all four value predictors, though each differ in how long it takes to generate a prediction and for how many iterations through the inner loop the prediction stays valid. SAP will quickly find a stride (equal to the size of an A element) and begin predicting after nine completed loads (i.e., when the effective confidence is reached). SAP has to retrain on each iteration of the outer loop since restarting breaks the stride. CAP will never predict when i &gt;= 16 because at that point the load path history does not change but the address does. For iterations i &lt; 16, CAP will establish entries in the prediction table and then build confidence in these entries over consecutive iterations of the outer loop. When o &gt; 4, CAP will predict all iterations of i &lt; 16. LVP will produce a correct prediction as soon as its high confidence threshold is met (e.g., after encountering 64 instances of the load). LVP Table V: Number of loads from inner loop of Listing 1 that must complete before making a prediction, assuming no predictor aliasing and for various iterations of the outer loop. A dash means that the predictor never makes a prediction, and a zero means that a prediction is made on the first iteration of the inner loop. does not need to retrain on subsequent iterations of the outer loop, and can predict the load on the first iteration of the inner loop once the high confidence threshold is reached (e.g., when o &gt; 0.) CVP will produce a correct prediction when enough iterations execute to fill the branch history register of the smallest CVP table (e.g., 5 iterations) and after enough loads execute to build confidence (e.g., 16 loads with same 5-bit branch history). E.g., CVP will predict iterations with i &gt; 22 because when i &gt; 5 the 5-bit history remains the same, and since the value is fixed, CVP will be confident after 22 iterations (6 iterations + 16 iterations). Also, for iterations of o &gt; 16, CVP will predict all load values, including iterations of i &lt;= 22.</p><p>Note that due to pipelining and/or superscalar effects, the first iteration of the inner loop to see a benefit from value prediction may be larger than the value listed in Table <ref type="table">V</ref>. For example, in SAP, by the time the ninth load completes to build confidence in the entry, some number of iterations greater than nine may have already been fetched and will therefore not be value predicted. In an aggressive machine, where value prediction shows the most benefit, this pipelining effect may be quite large. This highlights why having complementary predictors can often be beneficial. Consider a machine with SAP and CAP; CAP can predict the first 16 iterations while SAP trains (which could take close to 16 iterations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EFFICIENT LOAD VALUE PREDICTION</head><p>In this section, we present the results of a comprehensive design space analysis of a system using multiple load value predictors and filters. We begin by presenting the results of a simple composite predictor that simultaneously uses all four component predictors. Then we refine that design by adding a prediction filter, adjusting component table sizes, improving the training algorithm, and dynamically fusing component resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Composite Predictor</head><p>We first combined all four of the predictors from Section III to create a composite predictor. All four components train in parallel, and we use a prediction from any predictor that is highly confident (using thresholds from Table <ref type="table" target="#tab_0">IV</ref>). Because all predictors are tuned for 99% accuracy, highly confident predictors rarely disagree (results, not shown, confirm that highly-confident predictors disagree less than 0.03% of the time). Therefore, choosing among highly-confident predictors has little impact on performance. However, there is a power implication; when multiple predictors are confident, we prefer the result of a value predictor first since it is most power efficient (e.g., there is no need to speculatively access the data cache), and then chose context-aware over context-agnostic (for accuracy reasons).</p><p>Figure <ref type="figure" target="#fig_4">4</ref> shows which components produce highconfidence predictions across all loads in the benchmark mix. We see that there is significant overlap -66% of loads are predicted by more than one component. We also find that the address predictors (SAP and CAP) pick up most of the loads that can only be predicted by one predictor type, indicating that many more loads have predictable address patterns than predictable value patterns. As we previously established, however, such overlap is not always wasteful. Figure <ref type="figure" target="#fig_5">5</ref> shows the speedup improvement of the composite predictor over the best component predictor as we scale the total number of predictor entries from 256-4K. Except for the smallest configuration, where each component predictor has 64 entries, the composite predictor significantly exceeds the speedup of a single component predictor. Thus, we conclude that a composite predictor is better able to use the available predictor state (though, of course, has more logic complexity than a component predictor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accuracy Monitor</head><p>As previously discussed, the potential gain of a value predictor is often limited by the high cost of a misprediction. All the component predictors include a confidence threshold to throttle predictions from particular loads. In a composite predictor, we can also throttle an entire component predictor when it is producing a high misprediction rate overall. We studied two different throttling mechanisms, which we call Accuracy Monitors (AM).</p><p>We augment the design in Figure <ref type="figure" target="#fig_0">1</ref> by adding one AM per component predictor. At prediction time (Fetch) the AMs are looked up concurrently with the predictors. We squash a component predictor's confident prediction if the associated AM indicates the predictor is unreliable. The exact reliability metric depends on the AM variant, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) M-AM:</head><p>The first AM variant, called M-AM, tracks the misprediction rate for each component predictor within an execution epoch. When the misprediction rate is high at the end of an epoch (&gt; 3 mispredictions-per-kilo-predictions, MPKP), the associated component predictor is silenced in the next epoch. Silenced predictors continue to train.</p><p>The M-AM uses two counters per component predictor: one to track mispredictions and the other to track total predictions. The counters are always reset at an epoch boundary, which in our evaluation is one million instructions.</p><p>2) PC-AM: While M-AM is a heavy hammer that silences a component predictor based on overall accuracy, our second accuracy monitor, PC-AM, attempts to enforce more targeted silencing by tracking accuracy per PC. PC-AM, uses a directmapped, PC-indexed and PC-tagged table. It is indexed by hashing the lower order bits of the PC (e.g., for a 64-entry AM, index is computed as (PC &gt;&gt; 2) ⊕ (PC &gt;&gt; 8) ).</p><p>A PC-AM entry consists of:</p><p>• Tag: A partial tag computed by folding the low order bits of the PC (e.g., for a 10-bit tag, tag is computed as (PC &gt;&gt; 2) ⊕ (PC &gt;&gt; 12) )  • Counters: Per predictor correct and incorrect counters that track the accuracy of the given predictor. We use narrow counters because we only need a rough estimate of accuracy. There are eight counters total (two for each component). If the most significant bit of one of the eight counters gets set, all eight counters are shifted to the right. This action preserves the relative ratio of correctto-incorrect while allowing for using only 8-bit counters. We silence a predictor using PC-AM whenever the accuracy (correct/(correct + incorrect)) for that PC is below 95%.</p><p>The PC-AM is managed slightly differently than M-AM. When a value predicted load mispredicts and triggers recovery actions (a pipeline flush), an entry is allocated in AM, possibly replacing the existing entry. A value predicted load (implies that at least one of the predictors was confident in its prediction) that has an AM entry will increment the corresponding counters for all confident predictors, even though only one of the predictions is actually used. This allows for monitoring the accuracy of the predictors that did not provide the final prediction. For each confident predictor, a correct prediction increments the correct counter, and an incorrect prediction increments the incorrect counter.</p><p>3) AM Effectiveness: Figure <ref type="figure" target="#fig_6">6</ref> shows the speedup gain from three accuracy monitors: M-AM, PC-AM with 64 entries, and PC-AM with infinite entries. All three variants improve the base composite predictor, and PC-AM generally outperforms M-AM. The finite PC-AM design performs nearly as well as the infinite PC-AM, leading us to conclude that a small PC-AM filter is sufficient. Unless otherwise is stated, we assume a 64-entry PC-AM in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Heterogeneous Predictor Tables</head><p>We studied whether or not a composite predictor could benefit from heterogeneous component predictor table sizes. To evaluate the potential, we swept the predictor table sizes independently from 0-1K entries, keeping the same training and selection policies as the baseline. Zero entries means that we left the component predictor out completely.</p><p>Table <ref type="table" target="#tab_7">VI</ref> shows configurations that represent the best performance under different storage budgets. We found several interesting results. First, all winning configurations included all four predictors, indicating that the four complement each other. Second, we found that in two cases (4096 and 1024), the best configuration was actually the homogeneous table allocation. This makes sense in large configurations where none of the predictors are particularly resource starved. Third, we found that heterogeneous configurations were most effective for small predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Smart Training</head><p>As Figure <ref type="figure" target="#fig_4">4</ref> shows, there is significant overlap in the predictions provided by each component in the composite predictor. To mitigate this overlap, we evaluated a training policy that steers loads to a subset, rather than all, of The smart training algorithm works as follows. If no prediction was made, all predictors are trained to minimize the amount of time to get a confident prediction. However, if one or more predictions are made, we only train the predictors that (a) mispredicted, or (b) have the lowest cost according the heuristic listed below. By always training a component that produced an incorrect prediction, we encourage a quick eviction of the associated entry (a trained misprediction resets confidence). Among predictors that produce a correct prediction, we train them in the following order that prefers value over address and context-agnostic over context-aware: LVP, CVP, SAP, CAP. Additionally, whenever SAP produced a correct prediction but was not chosen for training, we invalidate the SAP entry. By skipping training, the SAP stride will be broken, effectively rendering the entry useless anyway. For example, if all four predictors produced correct predictions, we train LVP entry and invalidate SAP entry, meanwhile, CVP and CAP are not trained.</p><p>To validate that smart training worked as intended, we analyzed how many predictors were providing a prediction with and without smart training. Figure <ref type="figure" target="#fig_7">7</ref> shows a significant reduction in the number of times multiple predictions are made. For example, for a 1K-entry composite predictor, the percentage of time multiple predictions are made reduces from 62% to 12%. The Figure <ref type="figure">also</ref> shows the average number of predictors updated at training time; smart training, on average, updates close to one predictor.</p><p>Figure <ref type="figure">8</ref> shows the speedup from smart training. We found that smart training is most effective for small and moderate size predictors. This makes sense, since larger predictors are less sensitive to small changes in effective table size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Table Fusion</head><p>Looking at the component predictors, we see that they have similar storage requirements. To see if it is possible to exploit that fact, we developed a table fusion mechanism that dynamically reallocates tables entries from predictors with low accuracy to predictors with higher accuracy. In order to do so, we assume a design in which all component predictors use the same table width -81 bits in this case -and number of entries (no heterogeneous allocation).</p><p>The table fusion algorithm attempts to separate component predictors into two groups: donors and receivers. Donors are predictors that, over the recent past, have not been very productive (low number of used prediction). Conversely, receivers are predictors that have been useful in the recent past. After classifying the predictors, the fusion algorithm repurposes donor tables as extra storage for receiver predictors. We considered donating partial tables, but found that donating entire predictor tables resulted in the best performance.</p><p>The table fusion algorithm is epoch based (one million instructions in our design). During the execution of an epoch, we track the number of used predictions for each component predictor. At the end of an epoch, we compare the number of used predictions to a threshold (corresponding to 20 predictions per thousand instructions in our evaluation), and increment usefulness counter for any predictor exceeding the threshold. After N epochs (N = 5), we identify donor tables (used predictions lower than threshold in at least one epoch) and receiver tables (all other predictors). After M epochs (M &gt;&gt; N, M = 25), we revert the fusion and repeat.</p><p>If there is at least one donor table, fusion occurs. When there is one donor (and three receivers), the receiver with the highest number of used predictions gets the donor table. When there are two donors and two receivers, each receiver fuses with one of the donors. When there are three donors and one receiver, the receiver fuses with all three donors. Donors are flushed at fusion time, because they hold invalid information.</p><p>When fusion occurs, the donor tables are added as if they were additional cache ways of the now set-associative receiver table. This approach greatly simplifies indexing and data management, though it may incur extra power compared to an approach that tries to maintain a directmapped structure. When tables are unfused, the donor tables are flushed (again) while the receiver tables are maintained since they still contain valid data.</p><p>Figure <ref type="figure">9</ref> shows the performance gains from table fusion. Like the smart training optimization, table fusion is most helpful on small predictors. At 1K entries and above, table fusion results in no speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Combined Benefit</head><p>Figure <ref type="figure" target="#fig_0">10</ref> shows the maximum benefit when combining all of the previously discussed composite predictor optimizations. At all sizes, the composite predictor provides a &gt; 50%  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison Against Championship Value Prediction</head><p>We integrated the winner of the first championship value prediction (CVP-1) <ref type="bibr" target="#b32">[33]</ref>, called EVES predictor <ref type="bibr" target="#b3">[4]</ref>, into our framework. EVES tunes VTAGE and augments it with a stride value predictor. Both predictors employ clever optimizations that improve the predictors' storage efficiency and prediction accuracy. In Figure <ref type="figure" target="#fig_8">11</ref> we compare the speedup and coverage of our proposed composite predictor and EVES predictor.</p><p>In Figure <ref type="figure" target="#fig_9">12a</ref>, we show the per-workload speedup achieved by the best composite predictor (9.6KB budget) vs. EVES predictor (32KB budget). The composite predictor outperforms EVES in 67 out of 85 workloads, while the latter outperforms the composite predictor in 9 workloads only. On average, the composite predictor delivers a substantially higher speedup, 55% higher than EVES. In Figure <ref type="figure" target="#fig_9">12b</ref> we show the per-workload coverage achieved by the two predictors. On average, the composite predictor delivers a substantially higher coverage, 133% higher than EVES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We presented a comprehensive analysis of four state-of-theart load value predictors. Based on that analysis, we proposed and evaluated a composite predictor that uses all four component predictors together. We suggested four optimizations to make a composite predictor more efficient, namely an accuracy monitor that silences poorly-predicting components, a heterogeneous allocation of resources among components, a smart training algorithm, and a table fusion mechanism. We showed that the composite predictor outperforms the best component predictor by 54%-74% depending on predictor size. We showed that the composite predictor delivers more than twice the coverage of first championship value prediction winner predictor (EVES <ref type="bibr" target="#b3">[4]</ref>), and substantially increases the delivered speedup by more than 50%.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pipeline with support for our proposed value prediction scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Load breakdown by pattern</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 4 5 a</head><label>35</label><figDesc>1f o r ( o = 0 ; o &lt; M; o ++) { 2 memset (A, 0 , N * s i z e o f ( * A ) ) ; f o r ( i = 0 ; i &lt; N; i ++) {</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>first 16 iterations of inner loop</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Percent of dynamic loads in the benchmark mix predicted by one, two, three, or four predictors when using 1K-entry tables for each of the predictor types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Speedup from a composite predictor with homogeneous table sizes vs. the best component predictor for a given storage size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Speedup due to throttling from an accuracy monitor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Breakdown of the number of predictions with and without smart training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison between composite and EVES predictors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Per workload comparison of composite (9.6KB) and EVES (32KB) predictors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Four component load value predictors.</figDesc><table><row><cell></cell><cell>Predicts</cell></row><row><cell>Load values</cell><cell>Load addresses</cell></row><row><cell>Context</cell><cell></cell></row><row><cell>agnostic</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Applications used in our evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table III shows the baseline core configuration.</figDesc><table><row><cell>Branch Prediction</cell><cell>BP: state-of-art 32KB TAGE predictor and 32KB ITTAGE predictor</cell></row><row><cell></cell><cell>RAS: 16 entries</cell></row><row><cell>Memory Hierarchy</cell><cell>Block size: 64B (L1), 128B (L2 and L3)</cell></row><row><cell></cell><cell>L1: split, 64KB each, 4-way set-associative, 1-cycle/2-cycle (I/D) access latency</cell></row><row><cell></cell><cell>L2: unified, private, 512KB, 8-way set-associative, 16-cycle access latency</cell></row><row><cell></cell><cell>L3: unified, shared, 8MB, 16-way set-associative, 32-cycle access latency</cell></row><row><cell></cell><cell>Memory: 200-cycle access latency</cell></row><row><cell></cell><cell>Stride-based prefetchers</cell></row><row><cell>TLB</cell><cell>512-entry, 8-way set-associative</cell></row><row><cell cols="2">Fetch through Rename Width 4 instr./cycle</cell></row><row><cell cols="2">Issue through Commit Width 8 instr./cycle (8 execution lanes: 2 support load-store operations, and 6 generic)</cell></row><row><cell>ROB/IQ/LDQ/STQ</cell><cell>224/97/72/56 (modeled after Intel Skylake)</cell></row><row><cell>Fetch-to-Execute Latency</cell><cell>13-cycle</cell></row></table><note>III. VALUE PREDICTIONValue prediction enables speculation past true data dependencies in a program. It is effective because the values</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Address Prediction Address Prediction Value Prediction Engine (VPE) Value Prediction Engine (VPE)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Update Predictor(s), and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Flush on Value Misprediction</cell></row><row><cell cols="2">Fetch (multi-cycle)</cell><cell>Decode (multi-cycle)</cell><cell>Rename</cell><cell>Register File Access</cell><cell>Allocate</cell><cell>Issue</cell><cell></cell><cell>Execute Data</cell><cell>Commit</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">On a hit, return predicted value</cell><cell>Cache</cell><cell>(Optional) On a miss, generate a prefetch</cell></row><row><cell></cell><cell>If hit and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">On pipe</cell></row><row><cell>Fetch Group Address</cell><cell>confident</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Predicted Address Queue (PAQ)</cell><cell cols="2">bubbles</cell><cell>Components needed by any value prediction scheme</cell></row><row><cell>(a proxy for load PC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Components specific to Value</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prediction</cell></row><row><cell>Value Value</cell><cell>If hit and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prediction Prediction</cell><cell>confident</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI :</head><label>VI</label><figDesc>Selected configurations from heterogeneous composite predictor sizing exploration.</figDesc><table><row><cell></cell><cell>Total</cell><cell></cell><cell cols="2">Speedup</cell><cell cols="5">LVP SAP CVP CAP Total</cell><cell></cell><cell cols="3">Speedup/KB Speedup</cell><cell>comments</cell></row><row><cell></cell><cell cols="2">Entries</cell><cell cols="2">vs. No VP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Storage</cell><cell></cell><cell></cell><cell>vs. Homo-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>geneous</cell></row><row><cell></cell><cell>4096</cell><cell></cell><cell cols="2">5.07%</cell><cell cols="6">1024 1024 1024 1024 38.21KB</cell><cell cols="2">0.13%</cell><cell>0.00%</cell><cell>homogeneous was best</cell></row><row><cell></cell><cell>2048</cell><cell></cell><cell cols="2">4.84%</cell><cell>256</cell><cell>1024</cell><cell>512</cell><cell>256</cell><cell cols="2">19.31KB</cell><cell cols="2">0.25%</cell><cell>+19%</cell></row><row><cell></cell><cell>1024</cell><cell></cell><cell cols="2">4.51%</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell>256</cell><cell cols="2">9.56KB</cell><cell cols="2">0.47%</cell><cell>0.00%</cell><cell>homogeneous was best</cell></row><row><cell></cell><cell>512</cell><cell></cell><cell cols="2">3.92%</cell><cell>64</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell cols="2">4.83KB</cell><cell cols="2">0.81%</cell><cell>+33%</cell></row><row><cell></cell><cell>256</cell><cell></cell><cell cols="2">3.20%</cell><cell>32</cell><cell>32</cell><cell>128</cell><cell>64</cell><cell cols="2">3.67KB</cell><cell cols="2">0.87%</cell><cell>+48%</cell><cell>best speedup/KB</cell></row><row><cell>Breakdown of Predicted Loads</cell><cell>0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell><cell cols="2">4.0 Train All</cell><cell cols="2">1.1 Smart Training</cell><cell>4.0 Train All</cell><cell cols="2">1.1 Smart Training</cell><cell>4.0 Train All</cell><cell cols="2">1.1 Smart Training</cell><cell>4.0 Train All</cell><cell>1.1 Smart Training</cell><cell>4.0 Train All</cell><cell>1.2 Smart Training</cell><cell>0 1 2 3 4</cell><cell>Average Number of Predictors Updated</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">256-entry</cell><cell></cell><cell cols="2">512-entry</cell><cell></cell><cell cols="2">1K-entry</cell><cell></cell><cell cols="2">2K-entry</cell><cell>4K-entry</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">One Prediction by LVP</cell><cell></cell><cell cols="3">One Prediction by CVP</cell><cell cols="3">One Prediction by SAP</cell><cell cols="2">One Prediction by CAP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Two Predictions</cell><cell></cell><cell cols="2">Three Predictions</cell><cell></cell><cell cols="2">Four Predictions</cell><cell></cell><cell cols="2">Number of Predictors Updated</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>EVES (8KB) EVES (32KB) EVES (Infinite) Coverage Average Speedup</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>6%</cell><cell></cell><cell></cell><cell></cell><cell cols="5">MAX (Component)</cell><cell>MAX (Composite)</cell></row><row><cell></cell><cell>5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SPEEDUP</cell><cell>2% 3% 4%</cell><cell></cell><cell>54%</cell><cell></cell><cell>74%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65%</cell><cell>57%</cell></row><row><cell></cell><cell>1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell cols="2">4</cell><cell>6</cell><cell></cell><cell></cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">PREDICTOR STORAGE (IN KILOBYTES)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">Figure 10: Best overall composite speedup vs. best overall component speedup.</cell></row><row><cell cols="10">performance boost over a single component predictor. Even</cell></row><row><cell cols="10">though not shown, under comparable budgets, the composite</cell></row><row><cell cols="10">predictor significantly outperforms any of the component</cell></row><row><cell cols="10">predictors, in terms of speedup and coverage, on every</cell></row><row><cell cols="10">individual workload. We would like to reiterate that due</cell></row><row><cell cols="10">to several reasons (e.g., different assumptions about the</cell></row><row><cell cols="10">baseline ISA, microarchitecture, and storage constraints),</cell></row><row><cell cols="10">the speedups reported in this work can be significantly lower</cell></row><row><cell cols="10">than previously published literature. Sheikh reports similar</cell></row><row><cell cols="4">findings in [3], [27].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6%</cell><cell></cell><cell cols="2">Speedup</cell><cell>Coverage</cell><cell></cell><cell></cell><cell></cell><cell>60</cell></row><row><cell></cell><cell>5%</cell><cell></cell><cell>4.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell></row><row><cell></cell><cell>4%</cell><cell>4.1% 42.5</cell><cell>48.0</cell><cell></cell><cell cols="2">3.1%</cell><cell cols="2">3.2%</cell><cell>40</cell></row><row><cell></cell><cell>3%</cell><cell></cell><cell cols="2">2.7%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell></row><row><cell></cell><cell>2%</cell><cell></cell><cell></cell><cell cols="2">19.4</cell><cell>20.6</cell><cell cols="2">23.5</cell><cell>20</cell></row><row><cell></cell><cell>1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>Composite</cell><cell>Composite</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(4.2KB)</cell><cell>(9.6KB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Speedup and Coverage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">160%</cell><cell></cell><cell>Composite (4.2KB)</cell><cell cols="3">Composite (9.6KB)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">120% 140%</cell><cell></cell><cell>119%</cell><cell>148%</cell><cell></cell><cell></cell><cell>106%</cell><cell>133%</cell><cell></cell></row><row><cell cols="2">80% 100%</cell><cell>78%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">60%</cell><cell>52%</cell><cell></cell><cell></cell><cell cols="2">55%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">40%</cell><cell></cell><cell></cell><cell></cell><cell>33%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>% Increase in</cell><cell>% Increase in</cell><cell></cell><cell cols="2">% Increase in</cell><cell cols="2">% Increase in</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Speedup</cell><cell>Coverage</cell><cell></cell><cell>Speedup</cell><cell></cell><cell cols="2">Coverage</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">w.r.t. EVES (8KB)</cell><cell></cell><cell cols="3">w.r.t. EVES (32KB)</cell><cell></cell><cell></cell></row></table><note>(b) Relative Speedup and Coverage</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>earleyboyer eon equake facerec fbital filecycler fma3d gamess gap gbemu gcc2k gcc2k6 gobmk gromacs gzip h264ref hmmer huffde ibench iirflt leslie3d linpack lucas mandreel matrix mcf mesa mp3player mp4dec mp4enc mpeg2dec mpeg2enc mplayer namd nat omnetpp parser pdfjs perlbench perlbmk pktcheck pntrch povray regexp rotate routelookup rspeed scimark sjeng soplex sphinx3 splay sunspider tonto twolf typescript v8 v8shell vortex vpr wrf wupwise xalancbmk zeusmp zlib Average</head><label></label><figDesc></figDesc><table><row><cell>35%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0% 5% 10% 15% 20% 25% 30%</cell><cell cols="3">a2time aifirf apsi astar avmshell</cell><cell>basefp</cell><cell>bezier</cell><cell cols="2">browsermark bzip2k bzip2k6</cell><cell>calculix canrdr</cell><cell cols="2">cjpeg codeload</cell><cell>coremark</cell><cell>crafty dealII</cell><cell cols="13">dither djpeg dromaeo EVES (32KB)</cell><cell>Composite (9.6KB)</cell><cell>3.1%</cell><cell>4.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Speedup</cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.0</cell></row><row><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.6</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>a2time aifirf</cell><cell>apsi</cell><cell>astar avmshell</cell><cell>basefp</cell><cell>bezier</cell><cell>browsermark bzip2k</cell><cell>bzip2k6</cell><cell cols="2">calculix canrdr cjpeg</cell><cell>codeload</cell><cell cols="2">coremark crafty dealII</cell><cell>dither djpeg dromaeo</cell><cell>earleyboyer eon</cell><cell>equake facerec</cell><cell>fbital</cell><cell>filecycler fma3d gamess</cell><cell>gap</cell><cell>gbemu gcc2k</cell><cell>gcc2k6 gobmk</cell><cell>gromacs</cell><cell>gzip</cell><cell>h264ref hmmer huffde</cell><cell>ibench</cell><cell>iirflt leslie3d linpack</cell><cell>lucas</cell><cell>mandreel matrix mcf</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>mesa mp3player mp4dec mp4enc mpeg2dec mpeg2enc mplayer namd nat omnetpp parser pdfjs perlbench perlbmk pktcheck pntrch povray regexp rotate routelookup rspeed scimark sjeng soplex sphinx3 splay sunspider tonto twolf typescript v8 v8shell vortex vpr wrf wupwise xalancbmk zeusmp zlib Average</head><label></label><figDesc></figDesc><table><row><cell>EVES (32KB)</cell><cell>Composite (9.6KB)</cell></row><row><cell cols="2">(b) Coverage</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We analyzed several other predictors, like last address and stride value predictors. These predictors showed limited or no benefit in the presence of the four selected predictors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We could wait and probe the value predictor at rename, but choose to place the predictor access in fetch for parity with the design of an address predictor. The placement in the front-end allows for more slack for prediction while increasing the prediction-to-update latency.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">For CVP, total entries is the sum of the size of each of the three tables.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">In their seminal work, Sheikh et al. proposed a hardware-only fault attack mitigation scheme that leverages the overlap between value predictors to reverse the trust model, trusting the predicted value over the faulted value<ref type="bibr" target="#b29">[30]</ref>,<ref type="bibr" target="#b30">[31]</ref>,<ref type="bibr" target="#b31">[32]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank the reviewers for their valuable feedback. This research was supported by Qualcomm Technologies, Inc. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of Qualcomm Technologies, Inc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Value locality and load value prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="1996-09">Sep. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">EE Department TR 1080, Technion -Israel Institue of Technology</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gabbay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>Speculative execution based on value prediction</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Load value prediction via path-based address prediction: Avoiding mispredictions due to conflicting stores</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Damodaran</surname></persName>
		</author>
		<idno>ser. MICRO-50 &apos;17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 50th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring value prediction with the eves predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Championship Value Prediction</title>
				<meeting><address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03">2018. June 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Technology insight: Intel&apos;s next generation microarchitecture code name skylake</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mandelblat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>Presented at Intel Developer Forum</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speculative execution via address prediction and data prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Supercomputing, ser. ICS &apos;97</title>
				<meeting>the 11th International Conference on Supercomputing, ser. ICS &apos;97</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical data value speculation for future high-end processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2014-02">2014. Feb 2014</date>
		</imprint>
	</monogr>
	<note>IEEE 20th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bebop: A cost effective predictor infrastructure for superscalar value prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2015-02">Feb 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="http://www.spec.org" />
	</analytic>
	<monogr>
		<title level="m">The SPEC CPU 2000 Benchmark Suite</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="http://www.spec.org" />
	</analytic>
	<monogr>
		<title level="m">The SPEC CPU 2006 Benchmark Suite</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A benchmark characterization of the eembc benchmark suite</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Poovey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gal-On</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2009-09">Sept 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Linpack</title>
		<author>
			<persName><forename type="first">Linpack</forename><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://www.netlib.org/benchmark/hpl" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MPlayer</title>
		<author>
			<persName><forename type="first">Media</forename><surname>Player</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benchmark</forename></persName>
		</author>
		<ptr target="http://mplayerhq.hu/design7/dload.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Browser benchmark</title>
		<ptr target="http://web.basemark.com" />
		<imprint/>
	</monogr>
	<note>Browsermark</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sunspider</title>
		<author>
			<persName><forename type="first">Javascript</forename><surname>Sunspider</surname></persName>
		</author>
		<author>
			<persName><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://www.webkit.org/perf/sunspider/sunspider.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">V8</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<author>
			<persName><surname>Benchmarks</surname></persName>
		</author>
		<ptr target="http://code.google.com/apis/v8/benchmarks.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Octane</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Octane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benchmark</forename></persName>
		</author>
		<ptr target="https://developers.google.com/octane" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Javascript Performance Testing</title>
		<author>
			<persName><surname>Dromaeo</surname></persName>
		</author>
		<ptr target="http://dromaeo.com" />
		<imprint/>
	</monogr>
	<note>Dromaeo</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ibench Benchmark</surname></persName>
		</author>
		<ptr target="http://ibench.sourceforge.net" />
		<imprint/>
	</monogr>
	<note>iBench</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Java benchmark for scientific and numerical computing</title>
		<author>
			<persName><surname>Scimark</surname></persName>
		</author>
		<ptr target="http://math.nist.gov/scimark2/" />
	</analytic>
	<monogr>
		<title level="j">SciMark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new case for the tage branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-44</title>
				<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-44</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 64-kbytes ittage indirect branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Championship Branch Prediction, ser. JWAC-2</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The alpha 21264 microprocessor</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="1999-03">Mar 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memory Barriers: a Hardware View for Software Hackers</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Mckenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Technology Center, IBM Beaverton</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reasoning about the arm weakly consistent memory model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishtiaq</surname></persName>
		</author>
		<idno>ser. MSPC &apos;08</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness: Held in Conjunction with the Thirteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;08)</title>
				<meeting>the 2008 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness: Held in Conjunction with the Thirteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS &apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Correctly implementing value prediction in microprocessors that support multithreading or multiprocessing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 34th ACM/IEEE International Symposium on</title>
				<editor>
			<persName><surname>Microarchitecture</surname></persName>
		</editor>
		<meeting>34th ACM/IEEE International Symposium on</meeting>
		<imprint>
			<date type="published" when="2001-12">2001. Dec 2001</date>
		</imprint>
	</monogr>
	<note type="report_type">MICRO-34</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Panel discussion: Speculation: Past, present and future</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>First Championship Value Prediction</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic counter updates for predictor hysteresis and stratification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computer Architecture</title>
				<imprint>
			<date type="published" when="2006-02">2006. Feb 2006</date>
		</imprint>
	</monogr>
	<note>The Twelfth International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A load-instruction unit for pipelined processors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassiliadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Value prediction for security (vpsec): Countering fault attacks in modern microprocessors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vpsec: Countering fault attacks in general purpose microprocessors with value prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Computing Frontiers, ser. CF &apos;18</title>
				<meeting>the 15th ACM International Conference on Computing Frontiers, ser. CF &apos;18</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving performance and mitigating fault attacks using value prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cryptography</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">First championship value prediction</title>
		<ptr target="https://www.microarch.org/cvp1" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
