<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erwin</forename><surname>Quiring</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Arp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Johns</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Konrad</forename><surname>Rieck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning has made remarkable progress in the last years, yet its success has been overshadowed by different attacks that can thwart its correct operation. While a large body of research has studied attacks against learning algorithms, vulnerabilities in the preprocessing for machine learning have received little attention so far. An exception is the recent work of Xiao et al. that proposes attacks against image scaling. In contrast to prior work, these attacks are agnostic to the learning algorithm and thus impact the majority of learning-based approaches in computer vision. The mechanisms underlying the attacks, however, are not understood yet, and hence their root cause remains unknown.</p><p>In this paper, we provide the first in-depth analysis of image-scaling attacks. We theoretically analyze the attacks from the perspective of signal processing and identify their root cause as the interplay of downsampling and convolution. Based on this finding, we investigate three popular imaging libraries for machine learning (OpenCV, TensorFlow, and Pillow) and confirm the presence of this interplay in different scaling algorithms. As a remedy, we develop a novel defense against image-scaling attacks that prevents all possible attack variants. We empirically demonstrate the efficacy of this defense against non-adaptive and adaptive adversaries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning techniques have enabled impressive progress in several areas of computer science, such as in computer vision [e.g., <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> and natural language processing [e.g., <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. This success, however, is increasingly foiled by attacks from adversarial machine learning that exploit weaknesses in learning algorithms and thwart their correct operation. Prominent examples of these attacks are methods for crafting adversarial examples <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>, backdooring neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, and inferring properties from learning models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. While these attacks have gained significant attention in research, they are unfortunately not the only weak spot in machine learning systems.</p><p>Recently, Xiao et al. <ref type="bibr" target="#b34">[35]</ref> have demonstrated that data preprocessing used in machine learning can also suffer from vulnerabilities. In particular, they present a novel type of attack that targets image scaling. The attack enables an adversary to manipulate images, such that they change their appearance when scaled to a specific dimension. As a result, any learning-based system scaling images can be tricked into working on attacker-controlled data. As an example, Figure <ref type="figure" target="#fig_0">1</ref> shows an attack against the scaling operation of the popular TensorFlow library. The manipulated image (left) changes to the output (right) when scaled to a specific dimension.</p><p>Attacks on image scaling pose a threat to the security of machine learning: First, scaling is omnipresent in computer vision, as learning algorithms typically require fixed input dimensions. Second, these attacks are agnostic to the learning model, features, and training data. Third, the attacks can be used for poisoning data during training as well as misleading classifiers during prediction. In contrast to adversarial examples, image-scaling attacks do not depend on a particular model or feature set, as the downscaling can create a perfect image of the target class. As a consequence, there is a need for effective defenses against image-scaling attacks. The underlying mechanisms, however, are not understood so far and the root cause for adversarial scaling is still unknown.</p><p>In this paper, we provide the first comprehensive analysis of image-scaling attacks. To this end, we theoretically analyze the attacks from the perspective of signal processing and identify the root cause of the attacks as the interplay of downsampling and convolution during scaling. That is, depending on the downsampling frequency and the convolution kernel used for smoothing, only very specific pixels are considered for generating the scaled image. This limited processing of the source image allows the adversary to take over control of the scaling process by manipulating only a few pixels. To validate this finding, we investigate three popular imaging libraries for machine learning (OpenCV, TensorFlow, and Pillow) and confirm the presence of this insecure interplay in different scaling algorithms.</p><p>Based on our theoretical analysis, we develop defenses for fending off image-scaling attacks in practice. As a first step, we analyze the robustness of scaling algorithms in the three imaging libraries and identify those algorithms that already provide moderate protection from attacks. In the second step, we devise a new defense that is capable of protecting from all possible attack variants. The defense sanitizes explicitly those pixels of an image that are processed by a scaling algorithm. As a result, the adversary loses control of the scaled content, while the quality of the source image is largely preserved. We demonstrate the efficacy of this strategy in an empirical evaluation, where we prevent attacks from non-adaptive as well as adaptive adversaries.</p><p>Finally, our work provides an interesting insight into research on secure machine learning: While attacks against learning algorithms are still hard to analyze due to the complexity of learning models, the well-defined structure of scaling algorithms enables us to fully analyze scaling attacks and develop effective defenses. As a consequence, we are optimistic that attacks against other forms of data preprocessing can also be prevented, given a thorough root-cause analysis.</p><p>Contributions. In summary, we make the following contributions in this paper:</p><p>• Analysis of image-scaling attacks. We conduct the first in-depth analysis of image-scaling attacks and identify the vulnerability underlying the attacks in theory as well as in practical implementations.</p><p>• Effective Defenses. We develop a theoretical basis for assessing the robustness of scaling algorithms and designing effective defenses. We propose a novel defense that protects from all possible attack variants.</p><p>• Comprehensive Evaluation. We empirically analyze scaling algorithms of popular imaging libraries under attack and demonstrate the effectivity of our defense against adversaries of different strengths.</p><p>The rest of this paper is organized as follows: We review the background of image scaling and attacks in Section 2. Our theoretical analysis is presented in Section 3, and we develop defenses in Section 4. An empirical evaluation of attacks and defenses is given in Section 5. We discuss related work in Section 6, and Section 7 concludes the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Before starting our theoretical analysis, we briefly review the background of image scaling in machine learning and then present image-scaling attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Scaling in Machine Learning</head><p>Image scaling is a standard procedure in computer vision and a common preprocessing step in machine learning <ref type="bibr" target="#b20">[21]</ref>. A scaling algorithm takes a source image S and resizes it to a scaled version D. As many learning algorithms require a fixed-size input, scaling is a mandatory step in most learningbased systems operating on images. For instance, deep neural networks for object recognition, such as VGG19 and Inception V3/V4 expect inputs of 224 × 224 and 299 × 299 pixels, respectively, and can only be applied in practice if images are scaled to these dimensions. Generally, we can differentiate upscaling and downscaling, where the first operation enlarges an image by extrapolation, while the latter reduces it through interpolation. In practice, images are typically larger than the input dimension of learning models and thus image-scaling attacks focus on downscaling. Table <ref type="table" target="#tab_0">1</ref> lists the most common scaling algorithms. Although these algorithms address the same task, they differ in how the content of the source S is weighted and smoothed to form the scaled version D. For example, nearest-neighbor scaling simply copies pixels from a grid of the source to the destination, while bicubic scaling interpolates pixels using a cubic function. We examine these algorithms in more detail in Section 3 when analyzing the root cause of scaling attacks.</p><p>Due to the central role in computer vision, scaling algorithms are an inherent part of several deep learning frameworks. For example, Caffe, PyTorch, and TensorFlow implement all common algorithms, as shown in Table <ref type="table" target="#tab_0">1</ref>. Technically, TensorFlow uses its own implementation called tf.image, whereas Caffe and PyTorch use the imaging libraries OpenCV and Pillow, respectively. Other libraries for deep learning either build on these frameworks or use the imaging libraries directly. For instance, Keras uses Pillow and DeepLearning4j builds on OpenCV. As a consequence, we focus our analysis on these major imaging libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image-Scaling Attacks</head><p>Recently, Xiao et al. <ref type="bibr" target="#b34">[35]</ref> have shown that scaling algorithms are vulnerable to attacks and can be misused to fool machine learning systems. The proposed attack carefully manipulates an image, such that it changes its appearance when scaled to a specific dimension. In particular, the attack generates an image A by slightly perturbing the source image S, such that its scaled version matches a target image T . This process is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, which also serves as a running example throughout this paper. In addition, Table <ref type="table" target="#tab_2">2</ref> provides an overview of our notation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Capabilities and Knowledge</head><p>The . Moreover, Table <ref type="table" target="#tab_0">1</ref> shows that common opensource libraries have a limited number of scaling options, and thus only a few attempts are necessary to discover the correct setup. In some settings, a fixed algorithm can be even enforced by specific image sizes, as we show in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Attack Scope</head><p>As the image is manipulated before any feature extraction, image-scaling attacks can effectively mislead all subsequent steps in a machine-learning pipeline, allowing different attacks during train and test time.  <ref type="bibr" target="#b31">[32]</ref>, both attacks accomplish the same goal. However, image-scaling attacks considerably differ in the threat </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Size Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S m × n</head><p>The source image that is used to create the attack image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T m × n</head><p>The target image that the adversary wants to obtain after scaling. Finally, we note that these attacks are of particular concern in all security-related applications where images are processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Attack Strategy</head><p>There exist a strong and a weak strategy for implementing image-scaling attacks. In the strong strategy, the adversary can choose the source and target image. In the weak version, the adversary can only choose the target, and the calculated attack image is meaningless and easily detectable. We thus focus on the stronger attack strategy in our paper, which is of particular concern in real-world applications.</p><p>Objectives. Formally, image-scaling attacks need to pursue the following two objectives:</p><p>(O1) The downscaling operation on A needs to produce the target image: scale(A) ∼ T .</p><p>(O2) The attack image A needs to be indistinguishable from the source image: A ∼ S.</p><p>The first objective ensures that the target image T is obtained during scaling, while the second objective aims at making the attack hard to detect. We verify objective O1 by checking if the prediction of a neural network corresponds to the target image's class. Note that without the second objective, the attack would be trivial, as the adversary could simply overwrite S with T . In this case, however, the attack would be easily detectable and thus not effective in practice.</p><p>Strong Attack Strategy. The adversary seeks a minimal perturbation ∆ of S, such that the downscaling of ∆ + S = A produces an output similar to T . Both goals can be summarized as the following optimization problem:</p><formula xml:id="formula_0">min( ∆ 2 2 ) s.t. scale(S + ∆) − T ∞ ε .<label>(1)</label></formula><p>Additionally, each pixel value of A needs to remain within the fixed range (e.g., [0, 255] for 8-bit images). This problem can be solved with Quadratic Programming <ref type="bibr" target="#b4">[5]</ref>. When successful, the adversary obtains an image A that looks like the source but matches the target after scaling.</p><p>Horizontal and Vertical Optimization. Common imaging libraries, such as OpenCV or Pillow, implement downscaling by first resizing images horizontally and then vertically. This implementation technique enables approximating the scaling operation from Eq. ( <ref type="formula" target="#formula_0">1</ref>) by a closed-form expression which is based on a simple matrix multiplication:</p><formula xml:id="formula_1">D = scale(S + ∆) = L • (S + ∆) • R<label>(2)</label></formula><p>with L ∈ R m ×m , R ∈ R n×n and D ∈ R m ×n . The matrices L and R contain fixed coefficients that depend on the selected scaling algorithm. Both matrices can be computed in advance and are reusable. We refer to Xiao et al. <ref type="bibr" target="#b34">[35]</ref> for a description how to calculate L and R.</p><p>Based on this matrix multiplication, the attack can also be decomposed into a horizontal and vertical manipulation, which are conducted in reverse order to the scaling, as shown in Figure <ref type="figure" target="#fig_4">3</ref>. The attack proceeds by first computing a resized version of S, that is, S = scale(S) ∈ R m×n . Here, we solve Eq. ( <ref type="formula" target="#formula_0">1</ref>) with S as source image and T as target. Due to the decomposition, we only need the coefficient matrix L and thus arrive at the following optimization problem</p><formula xml:id="formula_2">min( ∆ 2 2 ) s.t. L • S + ∆ − T ∞ ε .<label>(3)</label></formula><p>Next, the horizontal direction is considered. To this end, the adversary calculates the final attack image A with S as source image, but A as target, analogue to Eq. (3). Column-based Optimization. In order to further decrease the computational effort, the optimization can be further decomposed into individual dimensions. We start again with the vertical scaling direction where we resize S ∈ R m×n to D ∈ R m ×n . Instead of considering the whole matrix, we solve the problem from Eq. (3) for each column of S separately:</p><formula xml:id="formula_3">min( ∆ * , j<label>2</label></formula><formula xml:id="formula_4">2 ) s.t. L • S * , j + ∆ * , j − T * , j ∞ ε ,<label>(4)</label></formula><p>where the subscript in X * , j specifies the j-th matrix column of a matrix X. This optimization is repeated for the horizontal direction and finally computed for all color channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attack Analysis</head><p>After introducing the background of image-scaling attacks, we are ready to investigate their inner workings in more detail. Our aim is to find out which vulnerability image-scaling attacks exactly exploit to be successful. We start off by observing that the presented attacks must exploit a vulnerability that is shared by many scaling algorithms. As the implementations of the algorithms differ, this vulnerability needs to be linked to the general concept of scaling. To better grasp this concept, we require a broader perspective on image scaling and thus examine it from the viewpoint of signal processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scaling as Signal Processing</head><p>Images can be viewed as a generic signal, similar to audio and video. While audio is described by a one-dimensional time series, an image represents a discrete and two-dimensional signal. Typically, images are encoded in the spatial domain of pixels. However, any signal can be described by a sum of sinusoids of different frequencies, and hence images can also be represented in the frequency domain [e.g., <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>. Scaling reduces the dimension of an image. As a result, the frequency mixture of the image changes and higher frequencies are lost. This process is closely related to downsampling in signal processing, where a high-frequency signal is transformed to a lower frequency. A major problem of downsampling is that the reduced resolution might not be able to describe all relevant frequencies in the image. According to the Nyquist-Shannon theorem <ref type="bibr" target="#b18">[19]</ref>, it is only feasible to reconstruct a signal s(t) from a discrete number of sampled points, if the sampling rate f T is at least twice as high as the highest frequency f max in the signal:</p><formula xml:id="formula_5">f T ≥ 2 • f max .</formula><p>If the frequency f T is below that threshold, the signal cannot be unambiguously reconstructed. In this case, the sampled points do not provide enough information to distinguish between the original signal and other possible signals. Figure <ref type="figure">4</ref> shows an example of this phenomenon, where it is impossible to decide which one of the two signals s(t) and ŝ(t) is described by the sampled points. Ultimately, the reconstructed signal can differ significantly from the original signal, which is known as the aliasing effect <ref type="bibr" target="#b18">[19]</ref>. As we see in the next sections, image-scaling attacks build on this very effect by cleverly manipulating a signal, such that its downsampled version becomes a new signal. s(t) ŝ(t) Figure <ref type="figure">4</ref>: An example of an undersampled signal s(t). Based on the sampling points, it is not possible to distinguish between s(t) and ŝ(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scaling and Convolution</head><p>It is clear that scaling algorithms do not merely reduce the frequencies in an image. These algorithms carefully interpolate the pixels of the source image before downscaling it in order to mitigate the aliasing effect. This computation can be described as a convolution between the source signal and a kernel function <ref type="bibr" target="#b18">[19]</ref>. For each position in the scaled image, the kernel combines a set of pixels (samples) from the source using a specific weighting. All scaling algorithms given in Table <ref type="table" target="#tab_0">1</ref> can be expressed using this concept.</p><p>Without loss of generality, we focus on the horizontal scaling of a single row in the following, that is, a row s ∈ R n from the source image is scaled to d ∈ R n . We denote by β the respective scaling ratio: β = n/n . The goal of downscaling is to determine the value for each pixel in d from a set of samples from s. This process can be described using a kernel function w as follows</p><formula xml:id="formula_6">(s w)(t) = ∞ ∑ u=−∞ w (t − u) s(u).<label>(5)</label></formula><p>Intuitively, w represents a weighting function that is moved over s as a sliding window. We denote the size of this window as the kernel width σ. Each pixel within this window is multiplied by the respective weight at this position. Figure <ref type="figure" target="#fig_6">5</ref> exemplifies this process for a bilinear kernel with σ = 2. The first pixel in d is the aggregated result from the third and fourth pixel in s, while the second pixel in d is only estimated from the seventh pixel in s. s and w As the downscaling of an image produces a smaller number of pixels, the window of the kernel function needs to be shifted on s by a specific step size, similar to the process of sampling in signal processing. The scaling ratio defines this step size so that each sampling position is given by</p><formula xml:id="formula_7">1 2 x s w 0.5 • (s[3] + s[4]) 1 • s[7]</formula><formula xml:id="formula_8">g(p) = p • β, (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where p is the target pixel in d and g(p) a position in s around which we place the kernel window. Note that the position g(p) is not necessarily discrete and can also fall between two pixels, as shown in Figure <ref type="figure" target="#fig_6">5</ref>. The downscaled output image is then computed as follows: Each scaling algorithm is defined by a particular kernel function. Figure <ref type="figure" target="#fig_7">6</ref> depicts the standard kernels for common scaling algorithms. For instance, nearest-neighbor scaling builds on the following kernel function:</p><formula xml:id="formula_10">d p = (s w)(g(p)) p = 0, 1, . . . , n .<label>(7)</label></formula><formula xml:id="formula_11">w(x) = 1 for − 0.5 x &lt; 0.5, 0 otherwise . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Only the value that is the closest to g(p) is used by this scaling algorithm. In other words, nearest-neighbor scaling simply copies pixels from s on a discrete grid to d. Overall, each kernel differs in the number of pixels that it uses and the respective weighting of the considered pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Root-Cause Analysis</head><p>Based on our insights from signal processing, we can start to investigate the root cause of image-scaling attacks. We observe that not all pixels in the source image equally contribute to its scaled version. Only those pixels close to the center of the kernel receive a high weighting, whereas all remaining pixels play a limited role during scaling. If the step size exceeds the kernel width, some pixels are even ignored and irrelevant for the scaling operation. Figure <ref type="figure" target="#fig_6">5</ref> illustrates this situation: Only three out of nine pixels are considered for computing the scaled output. This imbalanced influence of the source pixels provides a perfect ground for image-scaling attacks. The adversary only needs to modify those pixels with high weights to control the scaling and can leave the rest of the image untouched. This strategy is sufficient for achieving both objectives of the attack: (O1) a modification of pixels with high weights yields scale(A) ∼ T , and (O2) depending on the sparsity of those pixels the attack image A visually matches the source image S.</p><p>From the perspective of signal processing, image-scaling attacks can thus be interpreted as targeted aliasing, where the adversary selectively manipulates those regions of the signal that are sampled during downscaling. These regions create a high-frequency signal in the source image that is not visible in the spatial domain but precisely captures the sampling rate of the downscaling process.</p><p>We can deduce that the success of image-scaling attacks depends on the sparsity of pixels with high weight. If these pixels are dense, the adversary may still achieve objective O1 but will fail to satisfy O2, as the attack becomes visible. Reviewing the general concept of scaling, we identify two factors that determine the sparsity of these pixels: the scaling ratio β and the kernel width σ. For images, we formally bound the ratio r of pixels that are considered during scaling by</p><formula xml:id="formula_13">r ≤ (β h β v ) −1 (σ h σ v ).<label>(9)</label></formula><p>The terms β h , β v as well as σ h and σ v denote the respective scaling ratio and kernel width horizontally and vertically. If the direction is irrelevant, we consider quadratic images for our analysis and use β and σ for both axis. Moreover, note that the right term may exceed one if the windows of the kernels overlap and pixels in the source are considered multiple times.</p><p>Scaling ratio. The larger the ratio β, the fewer pixels are considered during scaling if the kernel width is bounded. In particular, the number of pixels that are discarded growths quadratically with β. An adversary can thus easily control the ratio r by increasing the size of the source image. Figure <ref type="figure">7</ref>(a)-(c) show the influence of the scaling ratio on the attack for a kernel with σ = 1. All images fulfill objective O1, that is, the images are scaled down to the "cat" image. Depending on the scaling ratio, however, their success to objective O2 changes. For a large ratio of β = 4, the attack image looks like the source, and the cat is not visible. For a smaller scaling ratio, the manipulated image becomes a mix of the source and target. For β = 1, the attack obviously fails.</p><p>Kernel width. The smaller the kernel width σ, the fewer pixels are considered during each convolution. While σ is typically not controlled by the adversary, several implementations of scaling algorithms make use of very small constants for this parameter. For example, the nearest-neighbor, bilinear, and bicubic kernels of the TensorFlow framework have a width of 1, 2, and 4, respectively.</p><p>Figure <ref type="figure">7</ref>(d)-(f) depict the influence of the kernel width on the attack for a fixed scaling ratio of β = 4. Again, all images fulfill objective O1 and are scaled down to the "cat" image. For σ = 1, the attack also satisfies objective O2 and is invisible. If two pixels are considered by the kernel, however, the cat becomes visible. For σ = 4, all pixels need to be manipulated and the attack fails.</p><p>Interestingly, our analysis is not limited to the scaling algorithms considered in this work. Any algorithm is vulnerable to image-scaling attacks if the ratio r of pixels with high weight is small enough. Our analysis thus allows developers to check quickly if their algorithms are vulnerable to these attacks. Overall, we are thus the first to provide a general understanding of this attack type in practice. This understanding enables us to compare different scaling algorithms and ultimately develop effective defense strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Defenses</head><p>We continue with the development of defenses that build on our analysis and address the root cause of image-scaling attacks-rather than fixing their symptoms. Our defenses aim to prevent attacks without interfering with the typical workflow of deep learning frameworks. They can thus serve as a plug-in for existing scaling algorithms. Note that the mere detection of attacks is not sufficient here, as the systems would need to cope with rejected inputs. Consequently, we first derive requirements for secure scaling and use these to validate the robustness of existing algorithms (Defense 1). As only a few algorithms realize a secure scaling, we proceed to develop a generic defense that reconstructs the source image and thereby is applicable to any scaling algorithm as preprocessing (Defense 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attacker Model</head><p>For the construction and evaluation of our defenses, we consider two types of adversaries: a non-adaptive adversary who uses existing image-scaling attacks, and an adaptive adversary who is aware of our defense and adapts the attack strategy accordingly. Both adversaries have full knowledge of the scaling algorithm and the target size. In the adaptive scenario, the adversary additionally has full knowledge of the applied defense. Finally, we expect the adversary to freely choose the source and target image so that she can find the best match for conducting attacks in a given setup.</p><p>We note that these assumptions are realistic due to the open-source nature of deep learning frameworks and the use of several well-known learning models in practice, such as VGG19 and Inception V3/V4. With black-box access to the scaling and learning models, an adversary can even deduce the scaling algorithm and target size by sending a series of specially crafted images to the learning system [see 35].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Defense 1: Robust Scaling Algorithms</head><p>Let us start with the conception of an ideal robust scaling algorithm which serves as a prototype for analyzing the properties of existing algorithms.</p><p>An ideal scaling algorithm. In the ideal case, an algorithm investigates each pixel of the source image at least once for downscaling. The robustness of the scaling increases further if the employed convolution kernels overlap, and thus one pixel of the source contributes to multiple pixels of the scaled version. Technically, this requirement can be realized by dynamically adapting the kernel width σ to the scaling ratio β, such that σ ≥ β holds. That is, the larger the ratio between the source and the scaled image, the wider the convolution kernel needs to become to cover all pixels of the image.</p><p>In addition to processing all pixels, an ideal algorithm also needs to weight all pixels equally; otherwise, a kernel with small support would leave pixels untouched if their weights become zero. For example, pixels close to the edge of the convolution window typically receive a very low weighting, as shown in Figure <ref type="figure" target="#fig_7">6</ref>. As a result, the convolution of an ideal algorithm should be uniform and combine all pixels in the current kernel window with equal weight.</p><p>Although both properties-considering all pixels and a uniform convolution-can be technically implemented, they introduce challenges that can limit their practical utility: First, processing all pixels of an image slows down the scaling process. This is not necessarily a problem in applications where large neural networks are trained, and the overhead of scaling is minimal anyway. However, in real-time settings, it might be prohibitive to go over all pixels during scaling. Second, the flattened weighting of the convolution can blur the image content and remove structure necessary for recognizing objects. As a consequence, we identify a trade-off between security and performance in image scaling.</p><p>Existing scaling algorithms. Based on the concept of an ideal algorithm, we examine the source code of the three considered imaging libraries and analyze their scaling algorithms with respect to the processed pixels and the employed convolution kernels. In particular, we inspect the source code of OpenCV version 4.1, Pillow 6.0, and tf.image 1.14 from TensorFlow. Table <ref type="table" target="#tab_3">3</ref> shows the results of this investigation. We observe that several scaling algorithms are implemented with fixed-size convolution kernels. For example, OpenCV and TensorFlow implement nearest-neighbor, bilinear, and bicubic scaling with a kernel width of 1, 2, and 4, respectively. Consequently, these algorithms become vulnerable once the scaling ratio exceeds the kernel width, and pixels of the source image are omitted during scaling.</p><p>Fortunately, however, we also identify one algorithm that is implemented with a dynamic kernel width of β in all frameworks: area scaling. This algorithm scales an image by simply computing the average of all pixels under the kernel window, which corresponds to a uniform convolution, as shown in Figure <ref type="figure" target="#fig_7">6</ref> for β = 4. Moreover, area scaling corresponds to a low-pass filter which mitigates the aliasing effect. As a result, area scaling provides strong protection from image-scaling attacks, and the algorithm is a reasonable defense if the uniform weighting of the convolution does not impact later analysis steps. We demonstrate the robustness of area scaling in our empirical evaluation in Section 5.</p><p>Our analysis provides another interesting finding: Pillow stands out from the other imaging library, as it implements a dynamic kernel width for all algorithms except for nearestneighbor scaling. The dynamic kernel width σ is chosen such that the convolution windows substantially overlap, for example, for bicubic and Lanczos scaling by a factor of 4 and 6, respectively. Although the used convolutions are not uniform for these algorithms, this overlap creates a notable obstruction for the attacker, as dependencies between the overlapping windows need to be compensated. Figure <ref type="figure" target="#fig_9">8</ref> schematically shows the dynamic kernel width of Pillow in comparison to the implementations of OpenCV and TensorFlow.</p><p>Disadvantages. While area scaling and the Pillow library provide a means for robust scaling, they also induce drawbacks. As exemplified in Figure <ref type="figure" target="#fig_10">9</ref>, the algorithms cannot entirely remove all traces from the attacks. Small artifacts can remain, as the manipulated pixels are not cleansed and still contribute to the scaling, though with limited impact. Our evaluation shows that these remnants are not enough to fool the neural network anymore. The predicted class for the scaled images, however, is not always correct due to the noise of the attack remainings. As a remedy, we develop an alternative defense in the next section that reconstructs the source image and thus is applicable to any scaling algorithm. This reconstruction removes attack traces, and thus the classifier predicts the original class again.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Defense 2: Image Reconstruction</head><p>We construct our defense around the main working principle of image-scaling attacks: The attacks operate by manipulating a small set of pixels that controls the scaling process.</p><p>With knowledge of the scaling algorithm, we can precisely identify this set of pixels in the attack image. The naive defense strategy to remove this set effectively blocks any attack, yet it corrupts the scaling, as all relevant pixels are removed. Instead, we first identify all pixels processed by a scaling algorithm and then reconstruct their content using the remaining pixels of the image.</p><p>Reconstructing pixels in images is a well-known problem in image processing, and there exist several methods that provide excellent performance in practice, such as techniques based on wavelets and shearlets [e.g., <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. These involved approaches, however, are difficult to analyze from a security perspective, and their robustness is hard to assess. Hence, we propose two simple reconstruction methods for the considered pixels that possess transparent security properties: a selective median filter and a selective random filter.</p><p>Selective median filter. Given a scaling algorithm and a target size, our filter identifies the set of pixels P in the input image that is processed during scaling. For each of the pixels p ∈ P , it determines a window W p around p, similar to a convolution kernel, and computes the median pixel value for this window. To make the computation robust, we define the size of this window as 2 β h × 2 β v , which ensures that half of the pixels overlap between the different windows and thus hinders existing scaling attacks. Furthermore, we take care of other manipulated points p ∈ P in W p and exclude them from the computation of the median. Figure <ref type="figure" target="#fig_11">10</ref> depicts the basic principle of our selective median filter.  In comparison to other approaches for reconstructing the content of images, this defense builds on the statistical robustness of the median operation. Small groups of pixels with high or low values are compensated by the median. On average, the adversary is required to change about 50% of the pixels in a window to reach a particular target value for the median. Our evaluation demonstrates that non-adaptive as well as adaptive adversaries are not capable of effectively manipulating these median values without introducing strong visible artifacts (see <ref type="bibr">Section 5)</ref>.</p><p>The robustness of the median filter comes at a price: Computing the median for all pixels in each window W p for all</p><formula xml:id="formula_14">p ∈ P yields a run-time complexity of O(|P | • β h • β v ).</formula><p>That is, the run-time growths quadratically with the scaling ratio. While this overhead might be neglectable when working with large neural networks, there also exist applications in which more efficient scaling is necessary. Providing secure and efficient scaling, however, is a challenging task, as the robustness of a scaling algorithm increases with the number of considered pixels.</p><p>Selective random filter. To tackle the problem of efficiency, we also propose a selective random filter that takes a random point from each window instead of the median. This filter is suitable for applications that demand a very efficient runtime performance and might tolerate a loss in visual quality. Appendix B outlines the filter in more detail.</p><p>In summary, we present two defenses that target the core of image-scaling attacks. As exemplified by Figure <ref type="figure" target="#fig_12">11</ref>, both restore the pixels that an adversary changes and prevent the attacks. These defenses can be easily used in front of existing scaling algorithms, such that almost no changes are necessary to the typical workflow of machine learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We continue with an empirical evaluation of our defenses against image-scaling attacks. In Section 5.2 and 5.3, we study the security of robust scaling algorithms (Defense 1). In Section 5.4 and 5.5, we examine our novel defense based on image reconstruction (Defense 2). For each defense, we start the evaluation with a non-adaptive adversary that performs regular image-scaling attacks and then proceed to investigate an adaptive adversary who tries to circumvent our defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>To evaluate the efficacy of our defenses, we consider the objectives O1 and O2 of image-scaling attacks presented in Section 2.2.3. If a defense is capable of impeding one of these objectives, the attack fails. For example, if the control of the adversary over the source is restricted, such that the classification of the scaled version is not changed, the defense has foiled O1. Similarly, if the embedded target image becomes clearly visible, the defense has thwarted O2. Consequently, we design our experiments along with these two objectives.</p><p>Dataset &amp; Setup. We use the ImageNet dataset <ref type="bibr" target="#b24">[25]</ref> with a pre-trained VGG19 model <ref type="bibr" target="#b27">[28]</ref> for our evaluation. This deep neural network is a standard benchmark in computer vision and expects input images of size 224 × 224 × 3. From the dataset, we randomly sample 600 images as an unmodified reference set and 600 source images for conducting attacks. For each source image, we randomly select a target image from the dataset, ensuring that both images have different classes and predictions. As we are interested in investigating different scaling ratios, we sample the images such that we obtain 120 images for each of the following five intervals of ratios: [2, 3), <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr">[5, 7.5</ref>), <ref type="bibr">[7.5, 10)</ref>. Since we have two ratios along the vertical and horizontal direction for each image, we consider the minimum of both for this assignment.</p><p>We implement image-scaling attacks in the strong variant proposed by Xiao et al. <ref type="bibr" target="#b34">[35]</ref>. We make a slight improvement to the original attacks: Instead of using a fixed ε value, we increase its value gradually from 1 up to 50 if the quadratic programming solver cannot find a solution. During our evaluation, we observe that single columns or rows may require a larger ε to find a feasible solution. In this way, we can increase the attack's success rate, if only a single part of an image requires a higher ε value.</p><p>As scaling algorithms, we consider the implementations of nearest-neighbor, bilinear, bicubic, and area scaling from the libraries OpenCV (version 4.1), Pillow (version 6.0), and tf.image (version 1.13) from TensorFlow. We omit the Lanczos algorithm, as it provides comparable results to bicubic scaling in our experiments due to the similar convolution kernel and kernel width (see Figure <ref type="figure" target="#fig_7">6</ref>).</p><p>Evaluation of O1: Predictions using VGG19. To assess objective O1 of the attacks, we check if the deep neural network VGG19 predicts the same class for the scaled image scale(A) and the target image T . As there are typically minor fluctuations in the predicted classes when scaling with different ratios, we apply the commonly used top-5 accuracy. That is, we check if a match exists between the top-5 predictions for the target image T and the scaled image scale(A).</p><p>Evaluation of O2: User Study. To investigate objective O2, we conduct user studies with 36 human subjects. The group consists of female and male participants with different profes-sional background. The participants obtain 3 attack images for each interval of scaling ratio and are asked to visually identify one or more of three classes, where one class corresponds to the source image, one to the embedded target image and the third to an unrelated class. We consider an attack successful, if a participant selects the class of the source image only and does not notice the target image.</p><p>Evaluation of O2: PSNR. As quantitative measurement, we additionally use the Peak Signal to Noise Ratio (PSNR), a common metric in image processing <ref type="bibr" target="#b7">[8]</ref>, to measure the difference between the unmodified source image and the attacked image. Formally, the PSNR for the attack image A and the source image S is defined as</p><formula xml:id="formula_15">PSNR(A, S) = 10 log 10 I 2 max 1 N A − S 2 2 . (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>The denominator represents the mean squared error between both images with N as the total number of pixels, and I max as the maximum of the pixel range. A high PSNR value (larger than 25 dB) indicates a strong match between two images. As a conservative choice, we consider the attack unsuccessful if the PSNR value is below 15 dB. We also experimented with more advanced methods for comparing the quality of images, such as feature matching based on SIFT analysis <ref type="bibr" target="#b15">[16]</ref>. This technique, however, shows the same trends as the simple PSNR measurement, and thus we omit these measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Defense 1: Non-Adaptive Attack</head><p>In our first experiment, we examine the robustness of existing scaling algorithms from OpenCV, TensorFlow, and Pillow against image-scaling attacks. Note that we investigate area scaling in the following Section 5.3, as it is not vulnerable to standard image-scaling attacks.</p><p>Evaluation O1. Figure <ref type="figure" target="#fig_13">12</ref> shows the performance of the attack as the ratio of classifications with the wanted target class after scaling. The attack is successful with respect to O1 for all scaling algorithms from OpenCV, TensorFlow, and Pillow. An exception is Pillow's bilinear scaling where the success rate is 87%, as a feasible solution is not found for all source and target pairs here. Overall, our results confirm that an attacker can successfully manipulate an image such that its scaled version becomes a target image, irrespective of the scaling algorithm or library. This manipulation, however, is not sufficient for a successful attack in practice, as visual traces may clearly indicate the manipulation and undermine the attack. We thus also evaluate O2 in this experiment.</p><p>Evaluation O2. Figure <ref type="figure" target="#fig_14">13</ref> shows the results from our user study investigating the visual perception of the generated attack images. In line with our theoretical analysis, the attack is successful against OpenCV and TensorFlow, once a certain scaling ratio is reached (red bars in Figure <ref type="figure" target="#fig_14">13</ref>). We observe that for ratios exceeding 5, most attack images are not detected by the participants. However, for the implementations of bilinear and bicubic scaling in the Pillow library, the participants always spot the attack and identify the embedded target class in the source image. This result confirms our analysis of the implementations in Section 4.2 and the vital role of the dynamic kernel width used by Pillow. In addition, Figure <ref type="figure" target="#fig_10">19</ref> in Appendix D reports the PSNR values between the attack and source image over the entire dataset. We observe the same trend as in the user study. For OpenCV and TensorFlow, the images become similar to each other with a larger β, reaching PSNR values above 25 dB.</p><p>Summary. We can confirm that image-scaling attacks are effective against several scaling algorithms in popular imaging libraries. The attacks succeed in crafting images that are classified as the target class. However, the visibility of the attacks depends on the scaling ratio and the kernel width. In the case of Pillow, the attack fails for bilinear, bicubic, and Lanczos scaling to hide the manipulations from a human viewer. We thus conclude that these implementations of scaling algorithms can be considered robust against a non-adaptive adversary in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Defense 1: Adaptive Attacks</head><p>In our second experiment, we consider an adaptive adversary that specifically seeks means for undermining robust scaling. To this end, we first attack the implementation of the Pillow library (Section 5.3.1) and then construct attacks against area scaling in general (Section 5.3.2 and 5.3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Attacking the Pillow Library</head><p>Our analysis shows that image-scaling attacks fail to satisfy objective O2 when applied to the Pillow library. The dynamic kernel width forces the attack to aggressively change pixels in the source, such that the target image becomes visible. As a remedy, we propose to limit the number of changed pixels. To build on the successful attacks against OpenCV and TensorFlow, we allow 2 pixels to be freely changed in the optimization from Eq. ( <ref type="formula" target="#formula_4">4</ref>) while using images with β ∈ <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5)</ref>. The goal is to find a modification for these pixels, such that the convolution over the whole kernel yields the target value. To increase the chances to obtain a feasible solution, we additionally allow the remaining pixels to be changed by at most 10. We rerun the experiment from the previous section with this new constraint and report results for 120 image pairs with β ∈ [4, 5) for bilinear and bicubic scaling, respectively.</p><p>Results. The added constraint severely impacts the success rate of the attack. The rate drops to 0% for bilinear scaling and to 0.83% for bicubic scaling. That is, the objective O1 is not reached anymore. In the majority of cases, no feasible solution exists and several columns of the source image are not modified. Only in a single case, the attack is successful for bicubic scaling. However, the attack image shows obvious traces from the target image, clearly revealing the attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Attacking Area Scaling</head><p>Area scaling stands out from the other algorithms as it employs a uniform weighting of pixels and operates on rectangular blocks instead of columns and rows. As a result, the original attack by Xiao et al. <ref type="bibr" target="#b34">[35]</ref> is not applicable to this scaling algorithm. To attack area scaling, we thus propose two novel attack strategies. The first strategy aims at slightly changing all pixels of a block to control its average. That is, we seek a minimal perturbation under the L 1 norm such that the average of the block becomes the targeted value. For a target value t, we solve the following optimization problem:</p><formula xml:id="formula_17">min( ∆ 1 ) s.t. avg( S + ∆) − t ∞ ε , (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where S is the current block, ∆ its perturbation and ε a small threshold. The L 1 norm in Eq. ( <ref type="formula" target="#formula_17">11</ref>) leads to an equally dis- tributed manipulation of the pixels in each block. The results for the L 2 norm are equivalent and thus omitted.</p><p>The second strategy aims at adapting only a few pixels of a block while leaving the rest untouched. In this case, we optimize the L 0 norm, since only the number of changed pixels counts. Our attack works as follows for a current image block: if the target value is larger than the current average, the adversary iteratively sets pixels in the source to I max until the target is reached. If the target is smaller, we iteratively set pixels to 0. Note that the last value generally needs to be adapted, such that the average becomes the target value.</p><p>Results. With respect to objective O1, both the L 1 and L 0 attack are successful in 100% of the images. However, both variants fail reaching objective O2 in all of the cases. A manual inspection of the images reveals that the source is largely overwritten by both attacks and parts of the target become visible in all attack images. Figure <ref type="figure" target="#fig_15">14</ref>(a) provides results on this experiment by showing the distribution of PSNR values over all source-attack image pairs. The average PSNR is 8.6 dB for L 1 and 6.7 dB for L 0 , which corresponds to a very low similarity between the source and the attack image. In addition, Figure <ref type="figure" target="#fig_15">14</ref>(b) depicts the distribution of changed pixels for the L 0 attack. While for the majority around 50% of the pixels are changed, a few images only require to change 28%. Still, this is too much to achieve objective O2. Figure <ref type="figure" target="#fig_1">20</ref> in Appendix D shows the five best images from our evaluation with the smallest number of changed pixels. In all cases, the source image cannot be recognized anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Selective Source Image</head><p>In addition to the two adaptive attacks, we also examine area scaling under a more challenging scenario. In this scenario, the adversary selects the most suitable source image for a fixed target. As a result, the class of the source image is arbitrary and potentially suspicious, yet the attack becomes stronger due to the selected combination of source and target. We implement this strategy as follows: For each target image T , we choose the source image S, for which the scaled version has the smallest average distance to the target image. Fewer changes are thus required to obtain a similar output after scaling. We report results for the 100 best novel source-target pairs in the following.</p><p>As before, both the L 1 and L 0 attack are successful in 100% of all cases regarding objective O1. However, the attack again largely overwrites the source image, such that the target is visible in all cases. The examples from Figure <ref type="figure" target="#fig_1">21</ref> in Appendix D underline that the attack fails to keep the changes minimal, although the source and target are similar to each other. The average PSNR value is 16 dB for L 1 and 12 dB for L 0 . Both are slightly higher than in the non-selective scenario but still far too low compared to successful examples from Section 5.2.</p><p>Summary. We conclude that area scaling is robust against the different adaptive attacks considered in this work, as well as the selection of source images. These attacks are a best effort for assessing the security of area scaling and confirm our theoretical analysis from Section 4.2. In summary, we recommend using area scaling when the uniform weighting of pixels does not impact any following analysis steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Defense 2: Non-Adaptive Attack</head><p>We proceed with evaluating our novel defenses for reconstructing images (Section 4.3). In particular, we combine the selective median and random filter with a vulnerable scaling algorithm and test the robustness of the combination. As attacks, we consider all manipulated images from Section 5.2 that satisfy the objectives O1 and O2 for one scaling algorithm. This includes attacks against nearest-neighbor scaling from all imaging libraries as well as attacks against bilinear and bicubic scaling from OpenCV and TensorFlow.</p><p>Evaluation O1. Our two defenses prevent all attacks. When they are employed, no attack image succeeds in reaching objective O1 for the respective scaling algorithm. The image reconstruction effectively removes the manipulated pixels and thereby prevents a misclassification of the images. Evaluation O2. As the original image content is reconstructed, the visual difference between the source and the reconstructed images are minimal. Figure <ref type="figure" target="#fig_16">15</ref> depicts the distribution of PSNR values between each source and attack image-before and after reconstruction. The quality considerably increases after restoration and reaches high PSNR values above 25 dB. Figure <ref type="figure" target="#fig_1">22</ref> in Appendix D provides some examples before and after reconstruction.</p><p>Reconstruction Accuracy. Table <ref type="table" target="#tab_4">4</ref> depicts the success rate of reconstructing the attack image's original prediction, that is, we obtain the prediction of its actual source image. The median filter recovers the predictions in almost all cases successfully. For the random filter, the success rate is slightly reduced due to the noise from the reconstruction. In addition, we also measure the impact of both filters on benign, unmodified images. The median filter runs with almost no loss of accuracy. The random filter induces a small loss which can be acceptable if a low run-time overhead of this defense is an important criterion in practice.</p><p>Run-time Evaluation. Finally, we evaluate the run-time performance of the two proposed defenses. To this end, we apply the defenses along with different scaling algorithms to 2,000 images and measure the average run-time per image. The test system is an Intel Xeon E5-2699 v3 with 2.4 GHz. Our measurements are shown in Figure <ref type="figure" target="#fig_7">16</ref> on a logarithmic scale in microseconds. Area scaling as well as our defenses introduce a notable overhead and cannot compete with the insecure nearest-neighbor scaling in performance. However, in comparison to a pass through the VGG19 model, our defenses are almost an order of magnitude faster and induce a neglectable overhead for deep learning systems.</p><p>Summary. This experiment shows that the median and random filter provide effective defenses against non-adaptive attacks. In contrast to robust scaling, the defenses prevent the attack and reconstruct the original prediction. Figure <ref type="figure" target="#fig_7">16</ref>: Run-time performance of nearest-neighbor and area scaling as well as our defenses in combination with nearest-neighbor scaling. Additionally, a forward pass of VGG19 is shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Defense 2: Adaptive Attacks</head><p>Finally, it remains to investigate the robustness of the two proposed defenses against an adaptive adversary who is aware of the defenses and adapts her attack accordingly. We thus develop two strategies that aim at misleading the image reconstruction of attack images. Both strategies attempt to manipulate the reconstruction of the pixels p ∈ P , such that they keep their value after applying the median or random filter.</p><p>Median Filter. Our attack strategy for the median filter is as follows: Given a window W p around p ∈ P , we denote by m the current median of W p . Note that p is not part of W p (see Figure <ref type="figure" target="#fig_11">10</ref>). The adversary seeks a manipulation of the pixels in W p , such that m = p. Hence, applying the median filter will not change p and the adversarial modification remains. Without loss of generality, we assume that m &lt; p. In order to increase m, the adversary needs to set more pixels to the value of p. We start with the highest pixel value that is smaller than p and set it to p. We continue with this procedure until the median equals p. In Appendix C, we show that this attack strategy is optimal regarding the L 0 , L 1 , and L 2 norm if the windows W p do not overlap. A smaller number of changes to the image cannot ensure that m = p. These results give a first intuition on the robustness of the median filter. A considerable rewriting is necessary to change the median, even in the overlapping case where an adversary can exploit dependencies across windows.</p><p>In our experiments, we vary the maximum fraction δ of allowed pixel changes per window. This bound allows us to measure the defense's robustness depending on the L 0 norm. Random Filter. For the random filter, our attack strategy increases the probability that the target value in a window W p is selected. To this end, we let the adversary set a fraction δ of all pixels in W p to p. To minimize the number of changes to the image, we replace only those pixels in the window with the smallest absolute distance to p. This strategy is optimal in the sense that manipulation with fewer changes would result in a lower probability for hitting the target value p. Figure <ref type="figure" target="#fig_8">17</ref>: Success rate of the adaptive attacks against defenses with respect to objective O1. Note that O2 is not satisfied (see Figure <ref type="figure" target="#fig_18">18</ref>). and thus omitted. The adaptive attacks need to considerably modify pixels so that the manipulated images are classified as the target class. The median filter is robust until 40% of the pixels in each window can be changed. Against the random filter, a higher number of changed pixels is necessary to increase the probability of being selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>With respect to goal O2, both defenses withstand the adaptive attacks and thus remain secure. Rewriting 20% of the pixels already inserts clear traces of manipulation, as exemplified by Figure <ref type="figure" target="#fig_20">23</ref> in Appendix D. In all cases, the attack image is a mix between source-and target class. In addition, Figure <ref type="figure" target="#fig_18">18</ref> shows the results from our user study for the median filter. The participants identify the attacks in the vast majority of the cases. In a few cases, the participants only recognized the source class. A closer analysis reveals that the distortion in these cases is so strong that the detection of particular classes is difficult. As a result, the participants did not specify the source class.</p><p>Summary. We conclude that the two proposed defenses are robust against the different adaptive attacks. These attacks are both optimal with respect to the number of changes and thus provide strong empirical evidence for the robustness of the defenses. If a vulnerable scaling algorithm needs to be used in a machine-learning system or the reconstruction of the original class is essential, we thus recommend using one of the defenses as a preprocessing step.  We provide a comprehensive analysis of image-scaling attacks and derive defenses for prevention. In a concurrent work <ref type="bibr" target="#b23">[24]</ref>, we study the application for the poisoning scenario. Moreover, we note that image-scaling attacks further bridge the gap between adversarial learning and multimedia security where the latter also considers adversarial signal manipulations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. Finally, image-scaling attacks differ from prior work in two important properties: (a) The attacks affect all further steps of a machine learning system. They are thus agnostic to feature extraction and learning models, giving rise to general adversarial examples and poisoning. (b) Fortunately, we can show that the vulnerability underlying image-scaling attacks can be effectively mitigated by defenses. This rare success of defenses in adversarial machine learning is rooted in the well-defined structure of image scaling that fundamentally differs from the high complexity of deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Image-scaling attacks exploit vulnerabilities in the preprocessing of machine learning with considerable impact on computer vision. In this paper, we provide the first in-depth analysis of these attacks. Based on insights from this analysis, we propose different defenses that address the root cause of the attacks rather than fixing their symptoms.</p><p>For evaluating our defenses, we consider an adaptive adversary who has full knowledge about the implementation of scaling algorithms and our defense strategy. Our empirical results show that image-scaling attacks can be prevented effectively under this threat model. The proposed defenses can be easily combined with existing imaging libraries and require almost no changes to machine learning pipelines. Furthermore, our findings are not limited to the considered scaling algorithms and enable developers to vet their own scaling techniques for similar vulnerabilities.</p><p>Overall, our work provides novel insights into the security of preprocessing in machine learning. We believe that further work is necessary to identify and rule out other vulnerabilities in the different stages of data processing to strengthen the security of learning-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Downgrade Attack to Nearest Scaling</head><p>As part of our analysis, we identified a side effect in the implementation of g(p) (see Eq. ( <ref type="formula" target="#formula_8">6</ref>)) in OpenCV and TensorFlow. An adversary can enforce the usage of nearest scaling by choosing a respective scaling factor although the library is supposed to use bilinear, bicubic or Lanczos scaling. In particular, if the scaling ratio is an uneven integer, β = 2z+1, z ∈ N, OpenCV is effectively using nearest scaling. In TensorFlow, each integer with β ∈ N leads to the same effect. Thus, if the adversary can control the source image size, she can resize her image before to obtain the respective scaling factor. This in turn allows her to perform a more powerful scaling attack by creating attack images with less distortion, as the ratio of considered pixels decreases (see Section 3.3). Note that we do not exploit this issue in our evaluation. We test over a variety of scaling factors to draw general conclusions on scaling attacks. To understand its reason, we need to consider the mapping g(p) and the kernel w. Table <ref type="table" target="#tab_6">5</ref> shows the slightly different implementations of g(p) in OpenCV, TensorFlow and Pillow. For OpenCV, for instance, if β is an uneven integer, g(p) will always be an integer. Thus, only one pixel will be used for the convolution. A closer look on the definition of the kernels in Figure <ref type="figure" target="#fig_7">6</ref> reveals the underlying reason. Each kernel is zero for integer positions. Thus, if g(p) is an integer and the kernel is exactly positioned here, each neighboring pixel obtains a weight of zero. Thus, only the pixel at position g(p) is used. This behavior corresponds to nearest scaling. We observe this effect for bilinear, bicubic and Lanczos scaling in OpenCV and TensorFlow. On the contrary, Pillow makes use of a dynamic kernel width, so that we do not observe this behavior in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Selective Random Filter</head><p>Our random filter is identical to the selective median filter, except for that it takes a random point from each window instead of the median. That is, given a point p ∈ P , we consider a window W p around p of size 2 β h × 2 β v and randomly select a point as a reconstruction of p. Again, we exclude points p ∈ P from this window to limit the attacker's influence.</p><p>Randomly selecting a point for reconstruction obviously comes with problems. First, the reconstruction becomes nondeterministic. Second, the scaled image might suffer from poor quality. Our evaluation, however, shows that the loss due to random sampling is small and might be acceptable for the benefit of a very efficient run-time performance. The filter reconstructs an image with a complexity of O(|P |), which is independent of the scaling ratio. Furthermore, the filter also provides strong protection from attacks. If an image contains |P | relevant points, there exist |P | • 4 β h β v possible combinations for its reconstruction. If we consider a scaling ratio of 5 and a target size of 200 × 200, this already amounts to 4 million different combinations an attacker needs to guess from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Adaptive Attack Against Median Filter</head><p>In the following, we analyze our adaptive attack against the median-based defense. We demonstrate that the attack is optimal regarding the L 0 , L 1 , and L 2 norm if each window W p does not overlap with other windows. An adversary cannot make less changes to control the output of the median filter.</p><p>For a given attack image and window W p , the adversary seeks to manipulate the pixels in W p such that the median m over W p still corresponds to p. In this way, the modifications from the image-scaling attack remain even after applying the median filter. Without loss of generality, we assume that m &lt; p and further unroll W p to a one-dimensional signal. We consider a signal with uneven length k and denote the numerical order by brackets, so that the signal is given by:</p><formula xml:id="formula_19">x (1) , • • • , x ( k 2 ) , m ( k+1 2 ) , x ( k+2 2 ) , • • • , x (l) , • • • , x (k) (12)</formula><p>We denote by x (l) the largest pixel in the sorted signal that is smaller than p. The objective is to change the signal with the fewest possible changes such that m = p.</p><p>We start by observing that we need to change l − k+1 2 + 1 pixels to move the median to p. Less changes do not impact the numerical order sufficiently. We can thus conclude that the minimal L 0 norm for an attack is given by</p><formula xml:id="formula_20">L 0 = l − k+1 2 + 1 .<label>(13)</label></formula><p>Next, we show that setting all pixels between m and x (l) to p successfully moves the median as well as minimizes the L 1 and L 2 norm in addition. First, we observe that if we replace pixels with indices in [1, k/2] by a value smaller than m, the median is not changed. Likewise, replacing pixels larger than x (l) by a value larger than m does not change the median. Two methods remain: <ref type="bibr" target="#b0">(1)</ref> We can replace pixels with indices in [1, (k + 1)/2] by a value larger than m. (2) We can set all pixels with index [(k + 1)/2, l] to p. While both methods can move the median to p, the latter induces less changes regarding the L 1 /L 2 norm, as these values are closer to p. Thus, our adaptive attack uses the optimal strategy for the L 1 /L 2 norm by setting all pixels between m and x (l) to p. Furthermore, we can derive a simple bound for the L 2 norm:</p><formula xml:id="formula_21">(L 2 ) 2 = ∑ ( k+1 2 ) i l x (i) − p 2 L 0 (m − p) 2 . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>Overall, we can exactly compute the number and amount of required changes for a successful attack. Our analysis, however, also shows that the attack always depends on the concrete pair of a source and a target image, and there is no notion of a class boundary. Consequently, we cannot derive a general bound, as achieved with certifiable defenses against adversarial examples. Yet, our empirical results in Section 5.5 demonstrate that the necessary changes are very large if target and source images show realistic content, so that the median m and the target value p are not close to each other.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of an image-scaling attack. Left: a manipulated image showing a cat. The scaling operation produces the right image with a dog.</figDesc><graphic url="image-1.png" coords="1,326.35,606.72,113.44,75.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Principle of image-scaling attacks: An adversary computes A such that it looks like S but downscales to T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>attack is agnostic to the employed learning model and does not require knowledge of the training data or extracted features. Yet, the adversary needs to know two parameters: (a) the used scaling algorithm and (b) the target size m × n of the scaling operation. Xiao et al. describe how an adversary can easily deduce both parameters with black-box access to the machine learning system by sending specifically crafted images [see 35]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A m × n</head><label>n</label><figDesc>The attack image, a slightly perturbed version of S D m × n The output image of the scaling function scale. model: The attacks are model-independent and do not depend on knowledge of the learning model, features, or training data. Furthermore, image-scaling attacks are effective even if neural networks were robust against adversarial examples, as the downscaling can create a perfect image of the target class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Libraries resize an image horizontally first, and then vertically. The attack creates A in reverse order: first the intermediate image A , and then A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scaling with convolution. The triangle illustrates the kernel with its relative weighting. It has a width of 2 and is shifted by a step size of β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of kernel functions using in scaling algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 Figure 7 :</head><label>17</label><figDesc>Figure 7: Influence of the scaling ratio and kernel size (see Figure 2 for the setting of this example); β and σ are the same horizontally and vertically. Plot (a)-(c) show manipulated images under varying ratios. Plot (d)-(f) show manipulated images under varying kernel sizes. The symbols and indicate if the attack is successful.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of bilinear scaling for Pillow, OpenCV and TensorFlow. The latter two fix σ to 2, while Pillow uses a dynamic kernel width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of scaling algorithms: (a) insecure nearest-neighbor scaling, (b) robust area scaling, and (c) robust scaling from Pillow. Note the visible attack traces in (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Image reconstruction using a selective median filter. Around each point p that is considered by the downscaling algorithm (red), we take the median of all values in a window around it (green), except for other candidates that are present in the window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Examples of our defense: (a) insecure nearest-neighbor scaling, (b) robust scaling using a median filter, and (c) a random filter. Note that attack traces are not visible anymore.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Success rate of image-scaling attacks with respect to objective O1: the number of classifications with target class after scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: User study on image-scaling attacks with respect to objective O2. The attack is successful if only the source image S is visible (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Adaptive attack against area scaling: (a) Distribution of PSNR values and (b) the average number of changed pixels by the L 0 -based attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: PSNR distribution before and after attack image reconstruction for median and random filter on OpenCV's scaling algorithms. Results for the other scaling algorithms are similar and thus omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Figure 17  shows the success rate of the adaptive attacks regarding objective O1 for OpenCV and TensorFlow. The results for Pillow's nearest-neighbor scaling are similar 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure18: User study to determine the success rate of the adaptive attack against the median filter with respect to O2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figures 19 to</head><label></label><figDesc>Figures 19 to 23 give further information and examples from our evaluation. In particular, they provide visual examples of successful and failed attacks, thereby highlighting the working principle of image-scaling attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>23</head><label>23</label><figDesc>Figures 19 to 23 give further information and examples from our evaluation. In particular, they provide visual examples of successful and failed attacks, thereby highlighting the working principle of image-scaling attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Figures 19 to 23 give further information and examples from our evaluation. In particular, they provide visual examples of successful and failed attacks, thereby highlighting the working principle of image-scaling attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>FigureFigure 20 :Figure 21 :</head><label>2021</label><figDesc>Figure Success rate of attack regarding objective O2: the similarity between source image and attack image, measured by the PSNR value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 :Figure 23 :</head><label>2223</label><figDesc>Figure 22:  Randomly selected examples before and after restoration with our median filter (first three columns) and random filter (last two columns). Without restoration, the attack is successful, as the downscaling of the attack image produces an unrelated target image (1st and 2nd row). With restoration, the attack fails in all cases with respect to objective O1, as the downscaled output from the restored attack image produces the respective content and not an unrelated image (3rd and 4th row). Moreover, the filtering improves quality, as it removes traces from the attack.</figDesc><graphic url="image-82.png" coords="18,399.52,429.20,71.81,53.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scaling algorithms in deep learning frameworks.</figDesc><table><row><cell>Framework</cell><cell>Caffe</cell><cell cols="2">PyTorch TensorFlow</cell></row><row><cell>Library</cell><cell>OpenCV</cell><cell>Pillow</cell><cell>tf.image</cell></row><row><cell>Library Version</cell><cell>4.1</cell><cell>6.0</cell><cell>1.14</cell></row><row><cell>Nearest Bilinear Bicubic Lanczos Area</cell><cell>• •(*) • • •</cell><cell>•( ‡) •(*) • • •</cell><cell>• •(*) • •</cell></row></table><note>(*) Default algorithm. ( ‡) Default algorithm if Pillow is used directly without PyTorch.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Table of symbols for scaling attacks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Kernel width σ for the scaling algorithms implemented by the imaging libraries OpenCV, tf.image (TensorFlow) and Pillow.</figDesc><table><row><cell>Library</cell><cell cols="2">OpenCV TF</cell><cell>Pillow</cell></row><row><cell>Nearest Bilinear Bicubic Lanczos Area</cell><cell>1 2 4 8 β</cell><cell>1 2 4 -β</cell><cell>1 2 • β 4 • β 6 • β β</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of defense in terms of recovering correct outputs from the attack images, and impact on benign images.</figDesc><table><row><cell>Library</cell><cell>Algorithm</cell><cell cols="2">Median</cell><cell cols="2">Random</cell></row><row><cell></cell><cell></cell><cell cols="4">Attacks Unmod. Attacks Unmod.</cell></row><row><cell></cell><cell>Nearest</cell><cell>99.6%</cell><cell>99.0%</cell><cell>89.3%</cell><cell>89.1%</cell></row><row><cell>OpenCV</cell><cell>Bilinear</cell><cell>100.0%</cell><cell>99.4%</cell><cell>97.7%</cell><cell>98.0%</cell></row><row><cell></cell><cell>Bicubic</cell><cell>100.0%</cell><cell>99.2%</cell><cell>91.4%</cell><cell>93.4%</cell></row><row><cell></cell><cell>Nearest</cell><cell>99.6%</cell><cell>99.0%</cell><cell>88.9%</cell><cell>89.1%</cell></row><row><cell>TF</cell><cell>Bilinear</cell><cell>100.0%</cell><cell>98.9%</cell><cell>97.7%</cell><cell>97.7%</cell></row><row><cell></cell><cell>Bicubic</cell><cell>100.0%</cell><cell>99.4%</cell><cell>91.7%</cell><cell>92.0%</cell></row><row><cell>Pillow</cell><cell>Nearest</cell><cell>100.0%</cell><cell>99.6%</cell><cell>88.1%</cell><cell>90.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Implementation of g(p) in OpenCV, TensorFlow and Pillow The scaling function in TensorFlow can be changed to the definition from OpenCV. However, this option is not exposed in tf.image.resize_images, the high level resizing API.</figDesc><table><row><cell>Library</cell><cell>g(•)</cell></row><row><cell>OpenCV TensorFlow Pillow</cell><cell>g(p) = (p + 0.5) • β − 0.5 g(p) = p • β (*) g(p) = (p + 0.5) • β</cell></row><row><cell>(*)</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank our shepherd Nicolas Papernot, the anonymous reviewers and David Wagner for their suggestions and comments. Furthermore, we acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC 2092 CASA -390781972 and the research grant RI 2469/3-1, by the German Ministry for Education and Research as BIFOLD -Berlin Institute for the Foundations of Learning and Data (ref. 01IS18025A and ref 01IS18037A), and from the state of Lower Saxony under the project Mobilise.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability</head><p>We make our dataset and code publicly available at http://scaling-attacks.net to encourage further research on secure image scaling. Our defenses are also implemented in C++ with Eigen, such that they can be easily employed as plug-ins for TensorFlow.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coping with the enemy: Advances in adversary-aware signal processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support Vector Machines Under Adversarial Label Noise</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Asian Conference on Machine Learning (ACML)</title>
				<meeting>of Asian Conference on Machine Learning (ACML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evasion Attacks against Machine Learning at Test Time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards Evaluating the Robustness of Neural Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>of IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<title level="m">Steganography in Digital Media: Principles, Algorithms, and Applications</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Borisov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Conference on Computer and Communications Security (CCS)</title>
				<meeting>of ACM Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Proccessing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TextBugger: Generating Adversarial Text Against Real-world Applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Network and Distributed System Security Symposium (NDSS)</title>
				<meeting>of Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trojaning Attack on Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Network and Distributed System Security Symposium (NDSS)</title>
				<meeting>of Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Certified Robustness to Adversarial Examples with Differential Privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lécuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>of IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Proccessing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<title level="m">Discrete-Time Signal Processing</title>
				<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical Black-Box Attacks against Machine Learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
			<affiliation>
				<orgName type="collaboration">ASIA CCS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
			<affiliation>
				<orgName type="collaboration">ASIA CCS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
			<affiliation>
				<orgName type="collaboration">ASIA CCS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
			<affiliation>
				<orgName type="collaboration">ASIA CCS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
			<affiliation>
				<orgName type="collaboration">ASIA CCS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
			<affiliation>
				<orgName type="collaboration">ASIA CCS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Asia Conference on Computer Computer and Communications Security</title>
				<meeting>of ACM Asia Conference on Computer Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SoK: Security and Privacy in Machine Learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE European Symposium on Security and Privacy</title>
				<meeting>of IEEE European Symposium on Security and Privacy</meeting>
		<imprint>
			<publisher>Eu-roS&amp;P</publisher>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forgotten Siblings: Unifying Attacks on Machine Learning and Digital Watermarking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Quiring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on Security and Privacy (Eu-roS&amp;P)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Misleading Authorship Attribution of Source Code using Adversarial Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Quiring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security Symposium</title>
				<meeting>of USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Backdooring and Poisoning Neural Networks with Image-Scaling Attacks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Quiring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Security Workshop (DLS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust Wavelet Denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Membership Inference Attacks against Machine Learning Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>of IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Scientist and Engineer&apos;s Guide to Digital Signal Processing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>California Technical Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An efficient method for salt-and-pepper noise removal based on shearlet transform and noise detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEUE -International Journal of Electronics and Communications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Proccessing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stealing Machine Learning Models via Prediction APIs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security Symposium</title>
				<meeting>of USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Security and Privacy (S&amp;P)</title>
				<meeting>of IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security Symposium</title>
				<meeting>of USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
