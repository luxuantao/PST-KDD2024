<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On-Line Adaptive Learning of the Continuous Density Hidden Markov Model Based on Approximate Recursive Bayes Estimate</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qiang</forename><surname>Huo</surname></persName>
							<email>qhuo@itl.atr.co.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ATR Interpreting Telecommunications Research Laborato-ries</orgName>
								<address>
									<postCode>619-02</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ATR Interpreting Telecommunications Research Laborato-ries</orgName>
								<address>
									<postCode>619-02</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Picone</forename><forename type="middle">Q</forename><surname>Joseph</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Communications Research Laboratory</orgName>
								<orgName type="institution">Bell Laboratories</orgName>
								<address>
									<postCode>07974</postCode>
									<settlement>Murray Hill</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Huo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Multimedia Communications Research Laboratory</orgName>
								<orgName type="institution">Bell Laboratories</orgName>
								<address>
									<postCode>07974</postCode>
									<settlement>Murray Hill</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On-Line Adaptive Learning of the Continuous Density Hidden Markov Model Based on Approximate Recursive Bayes Estimate</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F7E56B9A5B3B883C9EB3C204235E11B</idno>
					<note type="submission">received September 28, 1995; revised September 15, 1996.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recursive Bayesian estimation</term>
					<term>incremental maximum likelihood estimation</term>
					<term>hidden Markov model</term>
					<term>EM algorithm</term>
					<term>automatic speech recognition</term>
					<term>speaker adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a framework of quasi-Bayes (QB) learning of the parameters of the continuous density hidden Markov model (CDHMM) with Gaussian mixture state observation densities. The QB formulation is based on the theory of recursive Bayesian inference. The QB algorithm is designed to incrementally update the hyperparameters of the approximate posterior distribution and the CDHMM parameters simultaneously. By further introducing a simple forgetting mechanism to adjust the contribution of previously observed sample utterances, the algorithm is adaptive in nature and capable of performing an online adaptive learning using only the current sample utterance. It can, thus, be used to cope with the time-varying nature of some acoustic and environmental variabilities, including mismatches caused by changing speakers, channels, and transducers. As an example, the QB learning framework is applied to on-line speaker adaptation and its viability is confirmed in a series of comparative experiments using a 26-letter English alphabet vocabulary.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>acoustic mismatch, and this has been a long standing objective of many researchers over the past 20 years. Another way to reduce the possible acoustic mismatch between the training and testing conditions is to adopt the so-called adaptive learning approach. The scenario is like this: starting from a pretrained (e.g., speaker and/or task independent) speech recognition system, for a new user (or a group of users) to use the system for a specific task, a small amount of adaptation data is collected from the user. These data are used to construct a speaker adaptive system for the speaker in the particular environment for that specific application. By doing so, the mismatch between training and testing can generally be reduced. The most fascinating adaptation scheme with a practical value is the so-called on-line (or incremental, sequential) adaptation. This scheme makes the recognition system capable of continuously adapting to the new adaptation data (possibly derived from actual test utterances) without the requirement of storing a large set of previously used training data. It is this kind of approach that this paper focuses on.</p><p>The advantage of a sequential algorithm over a batch algorithm is not necessarily in the final result, but in computational efficiency, reduced storage requirements, and the fact that an outcome may be provided without having to wait for all the data to be processed. Moreover, the parameters of interest are sometimes subject to changes, e.g., they are time varying just like abovementioned acoustic mismatch problem frequently encountered in real speech recognition applications. In such cases, different data segments often correspond to different parameter values. Processing of all the available data jointly is no longer desirable, even if we can afford the computational load of the batch algorithm. To alleviate such problems, a sequential algorithm can be designed to adaptively track the varying parameters.</p><p>Recently, Bayesian adaptive learning of HMM parameters has been proposed and adopted in a number of speech recognition applications. A theoretical framework of Bayesian learning was first proposed by Lee et al. <ref type="bibr" target="#b20">[21]</ref> for estimating the mean and covariance matrix parameters of a continuous density HMM (CDHMM) with a multivariate Gaussian state observation density. It was then extended to handle all the parameters of a CDHMM with Gaussian mixture state observation densities (e.g., <ref type="bibr" target="#b11">[12]</ref>) as well as the parameters of discrete HMM's (DHMM's) and semicontinuous HMM's (SCHMM's, also called tied-mixture HMM's) (e.g., <ref type="bibr" target="#b13">[14]</ref>). It was shown that, for HMM-based speech recognition applications, the MAP framework provides an effective way for combining adaptation data and the prior knowledge, and then creating a set of adaptive HMM's to cope with the new acoustic conditions in the test data. The prior knowledge, which is embodied in a set of seed HMM's as well as in the assumed distributions of the model parameters being adapted, is made use of to mitigate the effect of adaptation data shortage to improve the system robustness. This approach works in a batch adaptation mode using a history of all the adaptation data. It can also be modified to work in a more attractive incremental adaptation mode. A related study was conducted by Matsuoka and Lee <ref type="bibr" target="#b27">[28]</ref> in which they used the segmental MAP algorithm to perform the so-called on-line adaptation. Due to its missing mechanism of updating the hyperparameters of the prior and/or posterior distribution incrementally, all the previously seen adaptation data need to be stored. A full-scale on-line Bayesian adaptation approach should be able to update both the hyperparameters of the prior and/or posterior distributions and the HMM parameters themselves simultaneously upon the presentation of the latest adaptation data. One such approach for adapting the mixture coefficients of SCHMM parameters was recently developed in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b14">[15]</ref>. In this study, we expand the above work and investigate the incremental estimation of all of the CDHMM parameters. The formulation given here can be straightforwardly extended to the DHMM and SCHMM cases.</p><p>A block diagram of the proposed on-line Bayesian adaptive training of HMM's is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Given a new block of input speech, feature extraction (usually spectral analysis) is first performed to derive the feature vector sequences used to characterize the speech input. It is followed by some kind of acoustic normalization to reduce the possible mismatch in the feature vector space. The processed feature vector sequences are then recognized based on the current set of HMM's. After the recognition of the current block of utterances, the HMM's and the posterior distributions of the related speech units are adapted and the updated models are used to recognize future input utterance(s). The adaptation algorithm usually requires some form of supervision in terms of the word (or phone) transcription of the speech utterances. Such a transcription can be provided either by a human transcriber or by the correction made by the user on the recognized output during actual usage. This adaptation scheme is often called supervised adaptation. On the other hand, the supervision information can also be derived directly from the recognition results and this is often referred to as unsupervised adaptation. For real-world applications, the unsupervised mode is usually more realistic and desirable. For the acoustic normalization/equalization module shown in Fig. <ref type="figure" target="#fig_0">1</ref>, many existing techniques can be applied. They include, for example, the popular cepstral mean subtraction algorithm <ref type="bibr" target="#b1">[2]</ref>, different cepstral normalization methods (e.g., CDCN) discussed in <ref type="bibr" target="#b0">[1]</ref>, ML-based feature space stochastic matching methods <ref type="bibr" target="#b6">[7]</ref>, [41], <ref type="bibr" target="#b32">[33]</ref>, signal conditioning techniques <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, etc. Acoustic normalization could even be integrated into the feature extraction stage, e.g., speaker normalization via vocal tract length normalization using frequency warping <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Encouraging results have also been demonstrated in The rest of the paper is organized as follows. A brief introduction of the concept of recursive Bayesian inference for CDHMM is given in Section II. The difficulty of directly applying the recursive scheme is also illustrated. The formulation of approximate quasi-Bayes estimation for incremental CDHMM training is proposed in Section III. Some important implementation issues are discussed in Section IV. In Section V, a series of experimental results along with discussions and analyzes for an incremental speaker adaptation application are reported. Finally, we summarize our findings in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. INCREMENTAL BAYES LEARNING:</head><p>METHOD AND DIFFICULTY Consider an -state CDHMM with parameter vector , where is the initial state probability vector, , is the transition probability matrix, and is the parameter vector composed of mixture parameters for each state . The state observation probability density function (pdf) is assumed to be a mixture of multivariate Gaussian pdf's <ref type="bibr" target="#b0">(1)</ref> where the mixture coefficients 's satisfy the constraint , and is the th normal mixand denoted by <ref type="bibr" target="#b1">(2)</ref> with being the -dimensional mean vector and being the precision (inverse covariance) matrix. Here, " " denotes proportionality and denotes the determinant of the matrix . Note that for notational convenience, it is assumed that the observation pdf's of all the states have the same number of mixture components. Whenever possible, in this paper, we try to use the same notations as in <ref type="bibr" target="#b11">[12]</ref>. Let be independent observation samples which are used to estimate the CDHMM parameters . Our initial knowledge about is assumed to be contained in a known a priori density . A formal Bayesian inference of is based on the following a posteriori density <ref type="bibr" target="#b2">(3)</ref> where denotes an admissible region of the parameter space.</p><p>The classical MAP solution of can be obtained by using the expectation maximization (EM) algorithm <ref type="bibr" target="#b8">[9]</ref> substantiated in <ref type="bibr" target="#b11">[12]</ref>, which is an iterative algorithm working in batch mode to find a local maximum of the posterior pdf . Assume the training/adaptation samples 's are presented successively. Applying the Bayes theorem, we obtain a recursive expression for the a posteriori pdf of , given , as <ref type="bibr" target="#b3">(4)</ref> Starting with the calculation of the posterior pdf from , a repeated use of (4) produces the sequence of densities , and so forth. This provides a basis of making recursive Bayesian inference of parameters <ref type="bibr" target="#b34">[35]</ref>.</p><p>Unfortunately, the implementation of this learning procedure for incremental CDHMM training raises some serious computational difficulties because of the nature of the missingdata problem caused by the underlying hidden processes, i.e., the state mixture component label sequence and the state sequence of the Markov chain for an HMM. It is well known that there exist no reproducing (natural conjugate) densities <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref> for CDHMM. To illustrate this problem more clearly, let us begin with and consider what happens after a training utterance (sample) is observed. For an observation sequence , let be the unobserved state sequence, and be the associated sequence of the unobserved mixture component labels. The posterior pdf of after observing is <ref type="bibr" target="#b4">(5)</ref> where the summations are taken over all possible state and mixture component label sequences. So the exact posterior pdf is a weighted sum of the prior pdf which includes terms. Successive computation of (4) introduces an ever-expanding combination of the previous posterior pdf's and thus quickly leads to the combinatorial explosion of terms. As a result, formal recursive Bayes learning procedures of this kind have been regarded as of purely academic interest. In order to make it more practical, some approximations are needed to alleviate the computational difficulties. The procedure proposed here is to apply the Bayes recursion of (4) incrementally, with one or more observation samples considered at a time. It is followed by a suitable approximation to the resulting posterior pdf so as to obtain recursive estimates of the hyperparameters of the approximate posterior pdf. This is typically accomplished by restricting the approximated pdf to be in the class of conjugate pdf's of the complete-data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROXIMATE SOLUTION: QUASI-BAYES LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. General Formulation</head><p>The Bayesian algorithm for learning about considered in this paper involves the specification of an initial a priori density for , and the subsequent recursive computation of the approximate posterior density. For the general case of CDHMM, in which both the mean and precision parameters are assumed to be random, the initial prior pdf of is assumed to be <ref type="bibr" target="#b11">[12]</ref> (6) where <ref type="bibr" target="#b6">(7)</ref> is the product of a series of Dirichlet pdf (sometimes called multivariate beta pdf), and thus takes the special form of a matrix beta pdf <ref type="bibr" target="#b26">[27]</ref> with sets of positive hyperparameters of . If the Gaussian mixand has a full precision matrix, then is assumed to be a normal-Wishart density of the form <ref type="bibr" target="#b7">[8]</ref> (8) where are the hyperparameters of the prior density such that is a vector of dimension and is a positive definite matrix. Here, tr denotes the trace of a matrix. This class of prior distributions actually constitutes a conjugate family of the complete-data density and is denoted as . The following discussion and formulation will be based on the general assumption of full precision matrix case. However, many practical CDHMM-based speech recognition systems usually adopt the diagonal precision matrices. For completeness, we will also summarize the related formulation in Appendix A.</p><p>We propose in this paper, at each step of the recursive Bayes learning discussed in previous section, to approximate the true posterior distribution by the "closest" tractable distribution within the given class , where denotes the updated hyperparameters after observing the sample . The approximate MAP estimation of CDHMM parameters at this time is then obtained by <ref type="bibr" target="#b8">(9)</ref> The term "closest" here depends, of course, on the particular criterion adopted in making the approximation. From the viewpoint of density approximation, minimizing the Kullback-Leibler directed divergence of the approximate pdf from the exact posterior pdf will give an attractive solution <ref type="bibr" target="#b9">(10)</ref> This procedure has an interesting decision-theoretical justification, as that which minimizes the expected loss when the decision space consists of all available approximations and the utility function is a proper, local scoring rule <ref type="bibr" target="#b4">[5]</ref>. Unfortunately, no explicit closed-form solution exists for this problem and a general optimization procedure is needed to get the hyperparameters estimate. Interested readers are referred to <ref type="bibr" target="#b5">[6]</ref> for an example of such a Bayesian analysis of a simple mixture problem. Instead of direct use of above approximation procedure, we suggest and highlight here a method called quasi-Bayes (QB) learning, which is both conceptually simple and computationally effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quasi-Bayes Learning</head><p>The quasi-Bayes procedure is an approximate solution that is motivated by aiming at achieving computational simplicity while still maintaining the flavor of the formal Bayes procedure. In the context of finite mixture distribution identification, the quasi-Bayes approach was originally proposed by Makov and Smith <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b33">[34]</ref> to conduct recursive Bayes estimation of the mixture coefficients while the mixture components are assumed fixed. In the sense that the approximate posterior distribution with a mean identical to that of the true posterior distribution, the convergence properties were established. We previously adopted this approach to on-line adaptation of the mixture coefficients in the SCHMM case <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In the following, we will expand this method to the CDHMM case.</p><p>At each step of recursive Bayes learning, the proposed quasi-Bayes procedure approximates the resulting posterior distribution , by the "closest" tractable distribution within the given class , under the criterion that both distributions have the same mode. This idea is schematically illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. More specifically, consider at time instant , we have a training utterance , and our prior knowledge about is approximated by . Let denote the associated complete-data and be corresponding missing data with being the unobserved state sequence and being the associated sequence of the unobserved mixture component labels. We get the approximate MAP estimate of by repeating the following steps. where is a forgetting factor and means that there is no forgetting.</p><p>M-step:</p><formula xml:id="formula_0">Choose (12)</formula><p>where is the iteration index and is the total iterations performed.</p><p>If the initial prior knowledge is too strong or after a lot of adaptation data have been incrementally processed, the new adaptation data usually have only a small impact on parameters updating in incremental training. To continuously track the variations of the model parameters corresponding to the new data, some forgetting mechanism is needed to reduce the effect of past observations relative to the new input data. Here we propose an exponential forgetting scheme by using a forgetting coefficient as shown in <ref type="bibr" target="#b10">(11)</ref>. This is analogous to that proposed in [40] and <ref type="bibr" target="#b17">[18]</ref>.</p><p>By choosing the initial prior pdf to be the conjugate family of the complete-data density, it can be similarly verified as in <ref type="bibr" target="#b11">[12]</ref> that with an appropriate normalization factor , belongs to the same distribution family as , thus is denoted as with the hyperparameters detailed as the following:  By repeating the above EM iteration, we can get a series of approximate pdf whose mode is approaching to the mode 1 of the true posterior pdf <ref type="bibr" target="#b30">(31)</ref> Thus, the hyperparameters are obtained at the last (actually th) EM iteration by using ( <ref type="formula">13</ref>)- <ref type="bibr" target="#b18">(19)</ref> to satisfy <ref type="bibr" target="#b31">(32)</ref> and the CDHMM parameters are updated accordingly. 1 Strictly speaking, the EM algorithm can only guarantee the mode of the approximate pdf to approach a local maximum of the above true posterior pdf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>The above forward-backward type procedure can be easily extended to a segmental (or Viterbi) one by replacing ( <ref type="formula">20</ref>)-( <ref type="formula">22</ref>) with (33) (34) <ref type="bibr" target="#b34">(35)</ref> where is the most likely state sequence corresponding to observation sequence , and denotes the Kronecker delta function. In the above quasi-Bayes learning framework, if each time obtaining a training sample, only one EM iteration is performed and no forgetting is activated to update the CDHMM parameters and the associated hyperparameters, then the whole incremental quasi-Bayes learning process becomes the following recursive EM version for approximate MAP estimate originally suggested by Titterington in <ref type="bibr" target="#b36">[37]</ref> as follows.</p><p>E-Step: Compute (36)</p><p>M-Step: Choose to maximize and also update hyperparameters to get . In <ref type="bibr" target="#b35">(36)</ref>, one can initialize using <ref type="bibr" target="#b36">(37)</ref> where is the initial prior density for , with mode and hyperparameters . This also shows that the quasi-Bayes procedure proposed in this paper is truly both computationally efficient and retains the flavor of the formal Bayes solution.</p><p>We have discussed the incremental training procedure to process one utterance at a time. Actually, the quasi-Bayes learning framework is flexible enough to include the batch or block mode learning as a special case. If the application permitted, one can also update the parameters by taking observations in batches, small enough to ensure that the computational requirements of the related Bayesian updating is within reasonable limits, so that the user will not be aware of the long delay. To an extreme, one can update the parameters by using all of the history data and the initial prior distribution. In this case, the QB method will degenerate to be the conventional batch mode MAP estimate.</p><p>The QB framework can also be used to implement an incremental version of the ML estimate of CDHMM. Given all of the training data, one first runs one batch-mode EM (Baum-Welch) iteration, and thus gets an initial prior pdf estimate by using ( <ref type="formula">38</ref>)-(44) in the next section. Starting from this initial prior pdf, one can go through the training data again by using QB framework to incrementally update the related parameters. After the pass of the whole training data (called one epoch), one can refresh the posterior pdf by using (45)-(51) in the next section and then feedback the refreshed pdf to be the initial prior pdf (called prior/posterior feedback). The whole process can be repeated until convergence. In this regard, we wish to draw the reader's attention to the concurrent and independent work of Gotoh et al. <ref type="bibr" target="#b12">[13]</ref>, who have used a similar method as the above quasi-Bayes learning procedure from a different viewpoint of speeding up the convergence of CDHMM training by using the above incremental algorithm instead of standard batch training one. In their work, however, they did not emphasize the underlying approximate nature of the updated posterior distribution to the exact one, thus failed to provide a sound formulation of the forgetting mechanism, albeit their awareness of its importance. We think this insight is important for developing other alternative methods as well as further studying some important issues such as the asymptotic convergence properties and the associated regularity conditions that have yet to be resolved. The existing literature on this topic, together with the ideas presented in this paper, should provide a starting point for such analyses. In fact, based on the general approximation theory discussed in Section III-A, apart from the QB learning method, we have also developed some other inference procedures which can be viewed, in a unified manner, as approximations to the formal recursive Bayesian solution demonstrated in Section II. We will report those results elsewhere. In the following sections, we will show by a series of experiments that the proposed QB algorithm does converge to a reasonable solution in terms of improving speech recognition rate. Before that, in next section, some important implementation issues will be first discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION ISSUES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initial Hyperparameter Estimation</head><p>In previous sections, the initial prior density is assumed to be a member of a preassigned family of prior distributions. In a strict Bayesian approach, the hyperparameter vector of this family of pdf's is also assumed known based on a subjective knowledge about . In reality, it is difficult to possess a complete knowledge of the prior distribution. One solution is to adopt the empirical Bayes (EB) approach <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b25">[26]</ref>  . When enough SI training data are available, the method of moment as discussed in <ref type="bibr" target="#b13">[14]</ref> can be used to estimate the hyperparameters . Otherwise, the following ad hoc method can also be used.</p><p>By viewing the last SI training iteration as MAP estimation with the noninformative prior, we get the estimate of the hyperparameters in the same spirit as in quasi-Bayes learning framework as follows:</p><formula xml:id="formula_1">(38) (39) (40) (41) (42) (43) (44)</formula><p>where is a weighting coefficient to control the importance of the prior knowledge or to balance the contribution between the SI training data and the adaptation data. This parameter can be specified by a user. It can also be determined a posteriori by measuring in some way the similarity between the coming adaptation data and the existing models. This could be a topic for further research. By choosing these estimators for the hyperparameters, we sacrifice the ability of the prior density to accurately model the interspeaker variability but we obtain more robust estimators in case when only insufficient SI training data are available. On the other hand, if a sufficient amount of SI training data is available, the first method (method of moment) can lead to a more accurate hyperparameter estimate by considering the interspeaker statistics. In this way, the importance of the prior knowledge is determined directly by the available SI training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Refreshing Hyperparameters</head><p>In the Bayesian adaptive learning framework, the adaptation effects depend heavily on the suitability of the prior distribution to the new data. The motivation of on-line adaptation (OLA) is to adapt a recognizer continuously to the coming block of new utterances, and then hope this adapted recognizer will do better for the next test utterance than the one without applying OLA. We usually start OLA from a general initial model (e.g., SI model), and then continuously adapt to the new data. As discussed in Section III, if the initial prior knowledge is too strong, or after a lot of adaptation data have been incrementally processed, some forgetting mechanism is needed to help continuously track the variations of the model parameters corresponding to the new data. There are many ways to implement the forgetting mechanism to reduce the mismatch between updated posterior distribution and the coming data, and to track the varying conditions. The exponential forgetting is expected to be helpful for handling the slow changes of acoustic conditions between consecutive utterances by deemphasizing the contribution of the history data. If at a certain time the condition changes abruptly, say, a change of speaker, then the prior (or the updated posterior) distribution may not provide much useful information for this new speaker and thus deteriorate the efficacy of the OLA. In this case, exponential forgetting may be too slow and not able to handle such fast changes. Refreshing the hyperparameters may be more helpful for fast forgetting. The simplest way is to back-off to the general (e.g., SI) initial models, which usually provide a reasonable performance and a robust initial hyperparameters' estimate. If the situation permitted, it will be helpful to maintain multiple sets of prior (updated posterior) distributions and select upon some criterion the best one to refresh. Finally, we can also normalize the updated hyperparameters themselves to deemphasize their contributions to the new adaptation data as follows:</p><formula xml:id="formula_2">(45) (46) (47) (48) (49) (50) (51)</formula><p>where is a weighting coefficient to control the degree of the forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SPEAKER ADAPTATION EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>To examine the viability of the proposed techniques, the incremental quasi-Bayes adaptive learning framework is applied to on-line speaker adaptation. We report on a series of recognition experiments using a vocabulary of the 26-letter English alphabet. Two severely mismatched speech databases were used for evaluating the adaptation algorithm. These two corpora, the OGI ISOLET and the TI46, were recorded at two separate sites with a time gap of ten years. The speech data were digitized at sampling rates of 16 kHz with 16-b quantization and 12.5 kHz with 12-b quantization respectively. The ISOLET corpus was recorded with a Sennheiser HMD 224 close-talking noise-cancelling microphone and the TI46 corpus was recorded with an Electro-Voice RE-16 cardoid dynamic microphone positioned two inches from the speaker's mouth. They have, therefore, very different acoustic characteristics. The speech data in the two corpora are lowpass-filtered at 3.3 kHz and downsampled to 8 kHz so that hopefully, they will become more compatible to each other. For speaker independent training and initial prior density estimation, the OGI ISOLET database was used. It consists of 150 speakers, 75 females and 75 males, each speaking each of the letters twice. For incremental speaker adaptive training and testing, the English alphabet subset of the TI46 isolated word corpus was used. It was produced by 16 speakers, eight females, and eight males. Among them, data from four males were incomplete. Therefore only 12 speakers were used in this study. Each person uttered each of the letters 26 times. Ten of them were collected in the same session. They are collectively denoted as DAT1 in this study. The remaining 16 tokens denoted as DAT2 were collected in eight different sessions in which two tokens of each letter were collected in each session. We divided DAT2 equally into two sets denoted, respectively, as DAT4 and DAT5.</p><p>For all the experiments, each letter in the vocabulary was modeled by a single left-to-right five-state CDHMM with arbitrary state skipping. Each state had four Gaussian mixture components with each component having a diagonal covariance matrix. Each feature vector used in this study consisted of 12 bandpass-liftered LPC-derived cepstral coefficients with a 30-ms frame length and a 10-ms frame shift <ref type="bibr" target="#b19">[20]</ref>. Although there are other alternatives (e.g., <ref type="bibr" target="#b32">[33]</ref>), only utterance-based cepstral mean subtraction (CMS) was applied for acoustic normalization. In all of the experiments, three EM iterations were performed for batch mode MAP training and incremental QB training. The initial hyperparameters were estimated by using the second method discussed in Section IV-A. In the particular experiments here, the weighting coefficient was chosen to be with being the number of SI training tokens corresponding to each HMM. This was equivalent to control the importance of the initial prior knowledge to be comparable with the contribution from a single training token. In recognition, the decision rule determined the recognized letter as the one which attained the highest forward-backward probability.</p><p>In the following subsections, we study the convergence property of the algorithm, the effects of different initial conditions, and the utility of the forgetting mechanism. All of the experiments were performed in a supervised mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence Property</head><p>To examine the convergence property of the proposed algorithm, we started with the SI initial models and performed supervised on-line adaptation by using DAT4 as the adaptation set. After each OLA step, we test the recognizer by using DAT5 as the testing set. We plot in Fig. <ref type="figure" target="#fig_5">3</ref> the OLA performance, averaged over 12 speakers, as a function of the number of adaptation tokens per letter (labeled as "si-ini-ol-dat4"), although OLA is actually performed after each utterance is available. For comparison, the adaptation results by using batch MAP training method are also plotted and labeled as "si-ini-map-dat4." The results showed that both OL and batch MAP adaptation can consistently and continuously improve the recognition performance when more and more adaptation data were available. The small performance difference between the two methods confirms that the proposed quasi-Bayes approximation to the true posterior distribution is viable and efficient. One advantage of the OL implementation over its batch counterpart lies in its computational efficiency and reduced storage requirements. More importantly, by incrementally updating hyperparameters and introducing the forgetting mechanism, the algorithm is truly adaptive in nature and can continuously track the changing conditions. We will provide more experimental evidences in the following subsections. Before that, we also plotted in Fig. <ref type="figure" target="#fig_5">3</ref> the OLA results by using DAT1 as adaptation set and testing on DAT5 to show the session effects between adaptation and testing data. The corresponding performance curve is labeled as "si-ini-ol-dat1." As expected, it is inferior to "si-ini-ol-dat4," because DAT1 and DAT5 were collected in completely different sessions. Whereas for each testing token in DAT5, there correspondingly exists an adaptation token in DAT4 coming from the same session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effects of Initial Conditions</head><p>In speaker adaptation based on Bayesian learning framework, one hopes to use prior distribution of CDHMM parameters to represent the information of the variability of a certain model among different speakers, so SA effects depend heavily on the suitability of the prior distribution to the new speaker. To show effects of OLA under different initial conditions, apart from starting OLA from SI model, other initial conditions are also tried. Specifically here, we first arbitrarily choose two speakers, one female (f4) and one male (m8). Starting from SI models, we perform OLA, respectively, on f4 and m8 by using DAT1 as adaptation data. Then start from these SA models, we perform OLA on other ten speakers by using DAT4 as adaptation data. Once again, DAT5 is used to test the recognizer after each OLA step. In Fig. <ref type="figure" target="#fig_6">4</ref>, we plot the performance comparison averaged over seven female speakers under different initial conditions. We can see that the OLA performance from SA initials of f4 and m8 is inferior to the one from SI initial model. This is partly due to the severer mismatch between the prior distribution (e.g., for m8) and the new adaptation data, and partly because after OLA with DAT1 for f4 and m8, the updated hyperparameters represent too strong prior information in comparison with the contribution of new data from new speakers, especially when new adaptation data is insufficient. The latter is confirmed by the fact that when no new adaptation data is available, the recognition rate with SA initial models of f4 is better than the one with SI model, but the OLA performance starting from f4 initials is  still inferior to the one from SI initials. This also confirms the necessity and importance of some kind of forgetting mechanism for the efficiency of OLA, especially in gender switching conditions. We will show in the next subsection that the introduction of this mechanism does help improve the OLA's efficacy. A similar observation can also be derived from the performance comparison averaged over three male speakers as shown in Fig. <ref type="figure" target="#fig_7">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects of the Forgetting Mechanism</head><p>To examine the effect of the exponential forgetting factor in the case of slowly changing acoustic conditions, we performed for each speaker OLA starting from the SI initials with DAT1 as the adaptation data. Then we activated the exponential forgetting mechanism and continued OLA by using DAT4 as the adaptation data. After each OLA step we tested the recognizer with DAT5. The performance comparison by using different forgetting coefficients (fc) is plotted in Fig. <ref type="figure" target="#fig_8">6</ref>. In the particular experimental setup here, the results showed that even with 18 tokens per letter, the performance has not saturated yet. Although the effect of the forgetting mechanism was  small, we can still see some improvement by activating the forgetting mechanism. The smaller the forgetting coefficient, the faster the forgetting process converged. However, a smaller forgetting coefficient also means a less weight assigned to the latest history data during adaptation. This may sometimes hamper the performance improvement, especially in the cases of insufficient and/or slowly changing adaptation data. The optimal value of forgetting coefficient should be situation dependent and its effect will be more apparent when large amount of adaptation data have been processed. Unfortunately, with the corpus we were using, we did not have enough data to conduct such a simulation.</p><p>To examine the effect of the forgetting mechanism in the case of abrupt switch of conditions (e.g., change of user), as an example, in Fig. <ref type="figure" target="#fig_9">7</ref>, we plot the performance comparison averaged over seven female speakers to show the effects of different forgetting schemes. Starting from the SA initials of f4, when no forgetting mechanism, the OLA performance ("f4ini-fc1.0") is much inferior to the one from the SI initials ("si-ini-fc1.0"). By activating exponential forgetting ("f4-ini-fc0.9"), we can see it helped improve the OLA performance, but it seemed not enough. By further including the mechanism Fig. <ref type="figure">8</ref>. Performance comparison with different forgetting schemes to copewith fast varying conditions (starting from SA initials of f4, averaged over three male speakers). Fig. <ref type="figure">9</ref>. Performance comparison with different forgetting schemes to copewith fast varying conditions (starting from SA initials of m8, averaged over seven female speakers). of refreshing hyperparameters (the weighting coefficient was chosen to be with being the number of SA tokens corresponding to each HMM used for speaker f4 adaptation), we can see that OLA performance ("f4-ini-rf-fc0.9") was improved significantly and quickly approach to the one obtained with the SI initials. Similar results were observed for the cases of starting from the SA initials of m8 as well as the performance comparison averaged over male speakers as shown in Figs. <ref type="figure">8,</ref><ref type="figure">9</ref>, and 10. Note that all of the above experiments were performed in a supervised mode. However, in some applications, the recognition system has to be run in an unsupervised mode. In this case, how to automatically determine when to refresh the priors is an important research topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION AND CONCLUSION</head><p>In this paper, we have presented a theoretical framework of QB learning of CDHMM with Gaussian mixture state observation densities based on a unified view of approximate recursive Bayesian inference. The implied algorithm can be Fig. <ref type="figure" target="#fig_0">10</ref>. Performance comparison with different forgetting schemes to copewith fast varying conditions (starting from SA initials of m8, averaged over 3 male speakers).</p><p>adaptive in nature so that it can be used to perform a fullscale on-line adaptive learning of the CDHMM parameters only using the current available data to continuously track the varying acoustic conditions. To examine the viability of the proposed algorithm, the QB learning framework is applied to an on-line speaker adaptation application using the 26-letter English alphabet vocabulary. In a series of comparative experiments, we studied the convergence property of the algorithm, the effects of different initial conditions, and the utility of the forgetting mechanism. We have found the following.</p><p>• The QB learning algorithm does converge to a reasonable solution in terms of improving recognition rate and has a similar behavior with the batch MAP algorithm in cases when no forgetting mechanism is imposed. • A good initial prior distribution is a key for improving the efficacy of on-line adaptation. • The forgetting mechanism is useful in handling the slow changes of acoustic conditions between consecutive utterances and coping with the abrupt switch of speaking conditions (e.g., change of user). Two methods, exponential forgetting and hyperparameter refreshing, are proposed, and their usefulness has been confirmed. In the experimental study of this paper, OLA is supervised, i.e., the true transcription of the adaptation data was assumed known. In practice, for on-line applications, unsupervised adaptation is often more realistic than the supervised one. The efficiency and effectiveness of unsupervised adaptation depend on the quality of the recognizer being used. If the current recognizer gives poor recognition results, then the supervision information is often wrong. This often results in an adapted model which gives worse performance than that obtained without adaptation. Moreover, OLA only uses the history data once. If it performs wrong adaptation at the very beginning, the system may diverge. In order to make OLA also work well in an unsupervised mode, it is desirable to minimize the effects of wrong supervision. Research along this line of thought is in progress. Another issue is about improving adaptation efficiency using data collected in mismatch acoustic conditions. In the acoustic normalization module in Fig. <ref type="figure" target="#fig_0">1</ref>, we have only used the socalled blind equalization method in this study. Combining other acoustic normalization techniques with the current online adaptation framework is an important research topic. On the other hand, to improve the rate of adaptation when the data amount is insufficient, one may combine on-line Bayes adaptation with other methods such as vector field smoothing (VFS) technique <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, transformation-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and other Bayesian techniques <ref type="bibr" target="#b18">[19]</ref>, where the dependency or correlation between HMM's is exploited to help adjust those HMM parameters without adaptation data. Actually, by combining with the so called extended MAP method <ref type="bibr" target="#b18">[19]</ref>, we have extended the current QB framework to cope with the correlated CDHMM's <ref type="bibr" target="#b15">[16]</ref>. As a final remark, although the experiments discussed in this study are for speaker adaptation, the same formulation can also be used to handle varying channels, environments, and transducer mismatch problems in speech as well as speaker and other pattern recognition problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A FORMULATION FOR DIAGONAL PRECISION MATRIX CASE</head><p>If the Gaussian mixand in (1) has a diagonal precision matrix, then is assumed to be a product of normalgamma densities <ref type="bibr" target="#b7">[8]</ref>   <ref type="bibr">AUDIO PROCESSING (1991</ref><ref type="bibr" target="#b22">-1995)</ref> . He ws a member of the ARPA Spoken Language Processing Coordination Committee between 1991 and 1995. He has also served as a member of the Speech Technical Committee of the IEEE Signal Processing Society (SPS) since 1995. In 1996, he helped promote the newly formed SPS Multimedia Signal Processing Technical Committee (MMSP-TC) and is a member of the MMSP-TC). He is a recipient of the 1994 SPS Senior Award. He currently serves as the Chairman of the SPS Speech Technical Committee.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of on-line Bayesian adaptation of HMM's.</figDesc><graphic coords="2,310.56,59.58,242.16,116.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic illustration of quasi-Bayes procedure: The true posterior distribution is approximated by a simpler distribution under the criterion that both distributions have the same mode.</figDesc><graphic coords="4,335.40,59.58,192.48,106.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>these terms can be computed efficiently by the forwardbackward algorithm<ref type="bibr" target="#b16">[17]</ref>,<ref type="bibr" target="#b28">[29]</ref>. Note that for notational simplicity, we have dropped the related subscripts and/or superscripts which indicate the iteration index and training sample index. The EM reestimation formulas of the CDHMM parameters can thus be derived by taking the mode of and are shown as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>to estimate the initial hyperparameters . Prior density estimation and the choice of density parameters depend on the particular application of interest. In the speaker adaptation (SA) application presented later in this study, the initial prior density represents the initial information of the variability of a certain model among a set of different speakers. Taking the empirical Bayes approach, the speaker-independent (SI) training data set for estimating hyperparameters can be divided into different subsets correspond to different speakers or speaker groups so that each token of the SI training data is associated with a speaker (group) ID. With those clustered training data, one can estimate sets of HMM's with the classical Baum-Welch or segmental -means algorithm. One can also perform an SI training at first by using all of the training data. At the last iteration of SI training, with the help of the speaker (group) ID information associated with each training token, one can accumulate sets of related statistics and thus correspondingly derive sets of HMM's. One then pretends to view as a collection of random observations from the density</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Convergence and performance comparison of on-line and batch adaptation (starting from SI initial models, = 1, averaged over 12 speakers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance comparison under different initial conditions as a function of number of adaptation data ( = 1, averaged over seven female speakers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance comparison under different initial conditions as a function of number of adaptation data ( = 1, averaged over three male speakers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Performance comparison with different forgetting coefficients to cope with slow varying conditions (averaged over 12 speakers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance comparison with different forgetting schemes to copewith fast varying conditions (starting from SA initials of f4, averaged over seven female speakers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>40] E. Weinstein, M. Feder, and A. V. Oppenheim, "Sequential algorithms for parameter estimation based on the Kullback-Leibler information measure," IEEE Trans. Acoust, Speech, Signal Processing, vol.<ref type="bibr" target="#b37">38</ref>, no. 9, pp. 1652-1654. [41] Y.-X. Zhao, "An acoustic-phonetic-based speaker adaptation technique for improving speaker-independent continuous speech recognition," IEEE Trans. Speech Audio Processing, vol. 2, no. 3, pp. 380-394. [42] Y.-X. Zhao, "Self-learning speaker and channel adaptation based on spectral variation source decomposition," Speech Commun., vol. 18, pp. 65-77, 1996. Qiang Huo (M'95) received the B.Eng. degree from University of Science and Technology of China (USTC), China, in 1987, the M.Eng. degree from Zhejiang University, China, in 1989, and the Ph.D. degree from USTC in 1994, all in electrical engineering. From 1986 to 1990, his research work focused on the hardware design and development for realtime digital signal processing, image processing and computer vision, and speech and speaker recognition. From 1991 to 1994, he was with the Department of Computer Science, University of Hong Kong, where he was involved in research on speech recognition. Since April 1995, he has been with ATR Interpreting Telecommunications Research Laboratories, Kyoto, Japan. His current major research interests include adaptive signal modeling and processing, speech recognition, speaker recognition, computational model for spoken dialogue processing, Chinese character recognition, and general pattern recognition theory. Chin-Hui Lee (S'79-M'81-SM'90-F'97) received the B.S. degree from National Taiwan University, Taipei, in 1973, the M.S. degree from Yale University, New Haven, CT, in 1977, and the Ph.D. degree from University of Washington, Seattle, in 1981, all in electrical engineering. In 1981, he joined Verbex Corporation, Bedford, MA, where he was involved in research on connected word recognition. In 1984, he became affiliated with Digital Sound Corporation, Santa Barbara, CA, where he engaged in research on speech coding, speech recognition, and signal processing for the development of the DSC-2000 Voice Server. Since 1986, he has been with AT&amp;T Bell Laboratories, Murray Hill, NJ, where he is now a Distinguished Member of the Technical Staff and the Head of the Dialogue Systems Research Department at Bell Laboratories, Lucent Technologies. His current research interests include signal processing, speech modeling, adaptive and discriminative modeling, speech recognition, speaker recognition, and spoken dialogue processing. His research scope is reflected in a recently edited book Automatic Speech and Speaker Recognition: Advanced Topics (Norwell, MA: Kluwer, 1996). Dr. Lee was he was an Associate Editor for the IEEE TRANSACTIONS ON SIGNAL PROCESSING and TRANSACTIONS ON SPEECH AND</figDesc><table><row><cell>with the form</cell></row><row><cell>(52)</cell></row><row><cell>where the hyperparameters</cell></row><row><cell>. The updating formulas of hyperparameters</cell></row><row><cell>have the same form as (13)-(15) and the</cell></row><row><cell>remaining ones are as follows:</cell></row><row><cell>(53)</cell></row><row><cell>(54)</cell></row><row><cell>(55)</cell></row><row><cell>(59)</cell></row><row><cell>(60)</cell></row></table><note><p>[</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Acoustical and Environmental Robustness in Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An inequality and associated maximization techniques in statistical estimation for probabilistic functions of Markov processes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inequalities</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic function of Markov chains</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximations in statistics from a decision-theoretical viewpoint</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probability and Bayesian Statistics</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Viertl</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of simple mixture problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Giron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Statistics</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Lindley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="1988">1988</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised speaker adaptation by probabilistic spectrum fitting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-89</title>
		<meeting>ICASSP-89</meeting>
		<imprint>
			<biblScope unit="page" from="294" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Degroot</surname></persName>
		</author>
		<title level="m">Optimal Statistical Decisions</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc, Ser. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speaker adaptation using constrained estimation of Gaussian mixtures</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Digalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rtischev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neumeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A parametric approach to vocal tract length normalization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-96</title>
		<meeting>ICASSP-96<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="346" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental MAP estimation of HMM&apos;s for efficient training and improved performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gotoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mashao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-95</title>
		<meeting>ICASSP-95<address><addrLine>Detroit, MI</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="457" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian adaptive learning of the parameters of hidden Markov model for speech recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="334" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On-line adaptation of the SCHMM parameters based on the segmental quasi-Bayes learning for speech recognition</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="141" to="144" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On-line adaptive learning of the correlated continuous density hidden Markov models for speech recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP-96</title>
		<meeting>ICSLP-96<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">Oct. 1996</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation for multivariate mixture observations of Markov chains</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Sondhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="307" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On-line estimation of hidden Markov model parameters based on the Kullback-Leibler information measure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2557" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A posteriori estimation of correlated jointly Gaussian mean vectors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lasry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="530" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acoustic modeling for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wilpon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="165" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study on speaker adaptation of the parameters of continuous density hidden Markov models</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="806" to="814" />
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker normalization using efficient frequency warping procedures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-96</title>
		<meeting>ICASSP-96<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="353" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Leggetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="171" to="185" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation for multivariate observations of Markov sources</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Liporace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A quasi-Bayes unsupervised learning procedure for priors</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">E</forename><surname>Makov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="761" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Maritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lwin</surname></persName>
		</author>
		<title level="m">Empirical Bayes Methods, 2nd ed</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bayesian Decision Problems and Markov Chains</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study of on-line Bayesian adaptation for HMM-based speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUROSPEECH-93</title>
		<meeting>EUROSPEECH-93<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="815" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Signal bias removal by maximum likelihood estimation for robust telephone speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Signal conditioning techniques for robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buhrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="107" to="109" />
			<date type="published" when="1996-04">Apr. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The empirical Bayes approach to statistical decision problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to stochastic matching for robust speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="190" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A quasi-Bayes sequential procedure for mixtures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">E</forename><surname>Makov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A note on the iterative application of Bayes&apos; rule</title>
		<author>
			<persName><forename type="first">J</forename><surname>Spragins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="544" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vector-field-smoothed Bayesian learning for incremental speaker adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-95</title>
		<meeting>ICASSP-95<address><addrLine>Detroit, MI</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recursive parameter estimation using incomplete data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speaker adaptation based on transfer vector field smoothing using maximum a posteriori probability estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tonomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Matsunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-95</title>
		<meeting>ICASSP-95<address><addrLine>Detroit, MI</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="688" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Speaker normalization on conversational telephone speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP-96</title>
		<meeting>ICASSP-96<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="339" to="341" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
