<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Margin Distribution Machine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Teng</forename><surname>Zhang</surname></persName>
							<email>zhangt@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
							<email>zhouzh@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Margin Distribution Machine</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21D9FDF2C080CBFBA68B7117B740BFDE</idno>
					<idno type="DOI">10.1145/2623330.2623710</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.6 [Artificial Intelligence]: Learning</term>
					<term>I.5.2 [Pattern Recognition]: Design Methodology-classifier design and evaluation</term>
					<term>H.2.8 [Database Management]: Database Applications-Data mining Margin distribution</term>
					<term>minimum margin</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first-and second-order statistics, i.e., the margin mean and variance. The LDM is a general learning approach which can be used in any place where SVM can be applied, and its superiority is verified both theoretically and empirically in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Support Vector Machine (SVM) <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">26]</ref> has always been one of the most successful learning algorithms. The basic idea is to identify a classification boundary having a large margin for all the training examples, and the resultant optimization can be accomplished by a quadratic programming (QP) problem. Although SVMs have a long history of literatures, there are still great efforts <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr">8]</ref> on improving SVMs.</p><p>It is well known that SVM can be viewed as a learning approach trying to maximize over training examples the minimum margin, i.e., the smallest distance from the examples to the classification boundary, and the margin theory <ref type="bibr" target="#b26">[26]</ref> provided a good support to the generalization performance of SVM. It is noteworthy that the margin theory not only plays an important role for SVMs, but also has been extended to interpret the good generalization of many other learning approaches, such as AdaBoost <ref type="bibr" target="#b10">[10]</ref>, a major representative of ensemble methods <ref type="bibr" target="#b31">[31]</ref>. Specifically, Schapire et al. <ref type="bibr" target="#b21">[21]</ref> first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting; soon after, Breiman <ref type="bibr" target="#b4">[4]</ref> indicated that the minimum margin is crucial and developed a boosting-style algorithm, Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance. Later, Reyzin et al. <ref type="bibr" target="#b20">[20]</ref> found that although Arc-gv tends to produce larger minimum margin, it suffers from a poor margin distribution; they conjectured that the margin distribution, rather than the minimum margin, is more crucial to the generalization performance. Such a conjecture has been theoretically studied <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b11">11]</ref>, and it was recently proven by Gao and Zhou <ref type="bibr" target="#b11">[11]</ref>. Moreover, it was disclosed that rather than simply considering a single-point margin, both the margin mean and variance are important <ref type="bibr" target="#b11">[11]</ref>. All these theoretical studies, however, focused on boosting-style algorithms, whereas the influence of the margin distribution for SVMs in practice has not been well exploited.</p><p>In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve strong generalization performance by optimizing the margin distribution. Inspired by the recent theoretical result <ref type="bibr" target="#b11">[11]</ref>, we characterize the margin distribution by the first-and second-order statistics, and try to maximize the margin mean and minimize the margin variance simultaneously. For optimization, we propose a dual coordinate descent method for kernel LDM, and propose an averaged stochastic gradient descent (ASGD) method for large scale linear kernel LDM. Comprehensive experiments on twenty regular scale data sets and twelve large scale data sets show the superiority of LDM to SVM and many stateof-the-art methods, verifying that the margin distribution is more crucial for SVM-style learning approaches than minimum margin.</p><p>The rest of this paper is organized as follows. Section 2 introduces some preliminaries. Section 3 presents the LDM. Section 4 reports on our experiments. Section 5 discusses about some related works. Finally, Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>We denote by X ∈ R d the instance space and Y = {+1, -1} the label set. Let D be an unknown (underlying) distribution over X × Y. A training set of size m S = {(x1, y1), (x2, y2), . . . , (xm, ym)}, is drawn identically and independently (i.i.d.) according to the distribution D. Our goal is to learn a function which is used to predict the labels for future unseen instances.</p><p>For SVMs, f is regarded as a linear model, i.e., f (x) = w ⊤ ϕ(x) where w is a linear predictor, ϕ(x) is a feature mapping of x induced by a kernel k, i.e., k(xi, xj) = ϕ(xi) ⊤ ϕ(xj). According to <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">26]</ref>, the margin of instance (xi, yi) is formulated as</p><formula xml:id="formula_0">γi = yiw ⊤ ϕ(xi), ∀i = 1, . . . , m.</formula><p>(</p><p>From <ref type="bibr" target="#b7">[7]</ref>, it is shown that in separable cases where the training examples can be separated with the zero error, SVM with hard-margin (or Hard-margin SVM),</p><formula xml:id="formula_2">min w 1 2 w ⊤ w s.t. yiw ⊤ ϕ(xi) ≥ 1, i = 1, . . . , m,</formula><p>is regarded as the maximization of the minimum margin {min{γi} m i=1 }. In non-separable cases where the training examples cannot be separated with the zero error, SVM with soft-margin (or Soft-margin SVM) is posed,</p><formula xml:id="formula_3">min w,ξ 1 2 w ⊤ w + C m ∑ i=1 ξi s.t. yiw ⊤ ϕ(xi) ≥ 1 -ξi, ξi ≥ 0, i = 1, . . . , m.</formula><p>(</p><p>where ξ = [ξ1, . . . , ξm] ⊤ measure the losses of instances, and C is a trading-off parameter. There exists a constant C such that (2) can be equivalently reformulated as,</p><formula xml:id="formula_5">max w γ0 - C ∑ m i=1 ξi s.t. γi ≥ γ0 -ξi, ξi ≥ 0, i = 1, . . . , m,</formula><p>where γ0 is a relaxed minimum margin, and C is the tradingoff parameter. Note that γ0 indeed characterizes the top-p minimum margin <ref type="bibr" target="#b11">[11]</ref>; hence, SVMs (with both hard-margin and soft-margin) consider only a single-point margin and have not exploited the whole margin distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LDM</head><p>In this section, we first formulate the margin distribution, and then present the optimization algorithms and the theoretical guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>The two most straightforward statistics for characterizing the margin distribution are the first-and second-order statistics, that is, the mean and the variance of the margin. Formally, denote X as the matrix whose i-th column is ϕ(xi), i.e., X = [ϕ(x1) . . . ϕ(xm)], y = [y1, . . . , ym] ⊤ is a column vector, and Y is a m × m diagonal matrix with y1, . . . , ym as the diagonal elements. According to the definition in (1), the margin mean is</p><formula xml:id="formula_6">γ = 1 m m ∑ i=1 yiw ⊤ ϕ(xi) = 1 m (Xy) ⊤ w,<label>(3)</label></formula><p>and the margin variance is</p><formula xml:id="formula_7">γ = 1 m 2 m ∑ i=1 m ∑ j=1 (yiw ⊤ ϕ(xi) -yjw ⊤ ϕ(xj)) 2 = 2 m 2 (mw ⊤ XX ⊤ w -w ⊤ Xyy ⊤ X ⊤ w). (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>Inspired by the recent theoretical result <ref type="bibr" target="#b11">[11]</ref>, LDM attempts to maximize the margin mean and minimize the margin variance simultaneously. We first consider a simpler scenario, i.e., the separable cases where the training examples can be separated with the zero error. In these cases, the maximization of the margin mean and the minimization of the margin variance leads to the following hard-margin LDM,</p><formula xml:id="formula_9">min w 1 2 w ⊤ w + λ1γ -λ2γ s.t. yiw ⊤ ϕ(xi) ≥ 1, i = 1, . . . , m,</formula><p>where λ1 and λ2 are the parameters for trading-off the margin variance, the margin mean and the model complexity. It's evident that the hard-margin LDM subsumes the hardmargin SVM when λ1 and λ2 equal 0.</p><p>For the non-separable cases, similar to soft-margin SVM, the soft-margin LDM leads to min w,ξ</p><formula xml:id="formula_10">1 2 w ⊤ w + λ1γ -λ2γ + C m ∑ i=1 ξi s.t. yiw ⊤ ϕ(xi) ≥ 1 -ξi, ξi ≥ 0, i = 1, . . . , m. (5)</formula><p>Similarly, soft-margin LDM subsumes the soft-margin SVM if λ1 and λ2 both equal 0. Because the soft-margin SVM often performs much better than the hard-margin one, in the following we will focus on soft-margin LDM and if without clarification, LDM is referred to the soft-margin LDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization</head><p>We in this section first present a dual coordinate descent method for kernel LDM, and then present an average stochastic gradient descent (ASGD) method for large scale linear kernel LDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Kernel LDM</head><p>By substituting (3)-( <ref type="formula" target="#formula_7">4</ref>), (5) leads to the following quadratic programming problem, min w,ξ</p><formula xml:id="formula_11">1 2 w ⊤ w + 2λ1 m 2 (mw ⊤ XX ⊤ w -w ⊤ Xyy ⊤ X ⊤ w) -λ2 1 m (Xy) ⊤ w + C m ∑ i=1 ξi s.t. yiw ⊤ ϕ(xi) ≥ 1 -ξi, ξi ≥ 0, i = 1, . . . , m.<label>(6)</label></formula><p>(6) is often intractable due to the high or infinite dimensionality of ϕ(•). Fortunately, inspired by the representer theorem in <ref type="bibr" target="#b22">[22]</ref>, the following theorem states that the optimal solution for (6) can be spanned by {ϕ(xi), 1 ≤ i ≤ m}.</p><p>Theorem 1. The optimal solution w * for problem (6) admits a representation of the form <ref type="bibr" target="#b7">(7)</ref> where α = [α1, . . . , αm] ⊤ are the coefficients.</p><formula xml:id="formula_12">w * = m ∑ i=1 αiϕ(xi) = Xα,</formula><p>Proof. w * can be decomposed into a part that lives in the span of ϕ(xi) and an orthogonal part, i.e.,</p><formula xml:id="formula_13">w = m ∑ i=1 αiϕ(xi) + v = Xα + v for some α = [α1, . . . , αm] ⊤ and v satisfying ϕ(xj) ⊤ v = 0 for all j, i.e., X ⊤ v = 0. Note that X ⊤ w = X ⊤ (Xα + v) = X ⊤ Xα,</formula><p>so the second and the third terms of ( <ref type="formula" target="#formula_11">6</ref>) are independent of v; further note that the constraint is also independent of v, thus the last terms of ( <ref type="formula" target="#formula_11">6</ref>) is also independent of v.</p><p>As for the first term of ( <ref type="formula" target="#formula_11">6</ref>), since X ⊤ v = 0, consequently we get</p><formula xml:id="formula_14">w ⊤ w = (Xα + v) ⊤ (Xα + v) = α ⊤ X ⊤ Xα + v ⊤ v ≥ α ⊤ X ⊤ Xα with equality occurring if and only if v = 0.</formula><p>So, setting v = 0 does not affect the second, the third and the last term while strictly reduces the first term of (6). Hence, w * for problem (6) admits a representation of the form <ref type="bibr" target="#b7">(7)</ref>.</p><p>According to Theorem 1, we have</p><formula xml:id="formula_15">X ⊤ w = X ⊤ Xα = Gα, w ⊤ w = α ⊤ X ⊤ Xα = α ⊤ Gα,</formula><p>where G = X ⊤ X is the kernel matrix. Let G:i denote the i-th column of G, then (6) can be cast as</p><formula xml:id="formula_16">min α,ξ 1 2 α ⊤ Qα + p ⊤ α + C m ∑ i=1 ξi s.t. yiα ⊤ G:i ≥ 1 -ξi, ξi ≥ 0, i = 1, . . . , m,<label>(8)</label></formula><p>where Q = 4λ1(mG ⊤ G -(Gy)(Gy) ⊤ )/m 2 + G and p = -λ2Gy/m. By introducing the lagrange multipliers β = [β1, . . . , βm] ⊤ and η = [η1, . . . , ηm] ⊤ for the first and the second constraints respectively, the Lagrangian of (8) leads to</p><formula xml:id="formula_17">L(α, ξ, β, η) = 1 2 α ⊤ Qα + p ⊤ α + C m ∑ i=1 ξi - m ∑ i=1 βi(yiα ⊤ G:i -1 + ξi) - m ∑ i=1 ηiξi. (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>By setting the partial derivations of {α, ξ} to zero, we have</p><formula xml:id="formula_19">∂L ∂α = Qα + p - m ∑ i=1 βiyiG:i, (<label>10</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">∂L ∂ξi = C -βi -ηi = 0, i = 1, . . . , m. (<label>11</label></formula><formula xml:id="formula_22">) Algorithm 1 Kernel LDM Input: Data set X, λ1, λ2, C Output: α Initialize β = 0, α = λ 2 m Q -1 Gy, A = Q -1 GY , hii = e ⊤ i Y GQ -1 GY ei; while β not converge do for i = 1, . . . m do [∇f (β)]i ← e ⊤ i Y Gα -1; β old i ← βi; βi ← min ( max ( βi -[∇f (β)] i h ii , 0 ) , C ) ; α ← α + (βi -β old i )Aei; end for end while</formula><p>By substituting <ref type="bibr" target="#b10">(10)</ref> and ( <ref type="formula" target="#formula_21">11</ref>) into ( <ref type="formula" target="#formula_17">9</ref>), the dual<ref type="foot" target="#foot_0">1</ref> of ( <ref type="formula" target="#formula_16">8</ref>) can be cast as:</p><formula xml:id="formula_23">min β f (β) = 1 2 β ⊤ Hβ + ( λ2 m He -e ) ⊤ β, s.t. 0 ≤ βi ≤ C, i = 1, . . . , m. (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>where H = Y GQ -1 GY , Q -1 refers to the inverse matrix of Q and e stands for the all-one vector. Due to the simple decoupled box constraint and the convex quadratic objective function, as suggested by <ref type="bibr" target="#b29">[29]</ref>, ( <ref type="formula" target="#formula_23">12</ref>) can be efficiently solved by the dual coordinate descent method. In dual coordinate descent method <ref type="bibr" target="#b13">[13]</ref>, one of the variables is selected to minimize while the other variables are kept as constants at each iteration, and a closed-form solution can be achieved at each iteration. Specifically, to minimize βi by keeping the other β j̸ =i 's as constants, one needs to solve the following subproblem,</p><formula xml:id="formula_25">min t f (β + tei) s.t. 0 ≤ βi + t ≤ C, (<label>13</label></formula><formula xml:id="formula_26">)</formula><p>where ei denotes the vector with 1 in the i-th coordinate and 0's elsewhere. Let H = [hij]i,j=1,...,m, we have</p><formula xml:id="formula_27">f (β + tei) = 1 2 hiit 2 + [∇f (β)]it + f (β),</formula><p>where [∇f (β)]i is the i-th component of the gradient ∇f (β). Note that f (β) is independent of t and thus can be dropped.</p><p>Considering that f (β + tei) is a simple quadratic function of t, and further note the box constraint 0 ≤ αi ≤ C, the minimizer of ( <ref type="formula" target="#formula_25">13</ref>) leads to a closed-form solution,</p><formula xml:id="formula_28">β new i = min ( max ( βi - [∇f (β)]i hii , 0 ) , C</formula><p>) .</p><p>Algorithm 1 summarizes the pseudo-code of kernel LDM. For prediction, according to <ref type="bibr" target="#b10">(10)</ref>, one can obtain the coefficients α from the optimal β * as</p><formula xml:id="formula_29">α = Q -1 (GY β * -p) = Q -1 ( λ2 m GY e + GY β * ) = Q -1 GY ( λ2 m e + β *</formula><p>) .</p><p>Hence for testing instance z, its label can be obtained by sgn</p><formula xml:id="formula_30">( w ⊤ ϕ(z) ) = sgn ( m ∑ i=1 αik(xi, z) ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Large Scale Kernel LDM</head><p>In section 3.2.1, the proposed method can efficiently deal with kernel LDM. However, the inherent computational cost for the kernel matrix in kernel LDM takes O(m 2 ) time, which might be computational prohibitive for large scale problems. To make LDM more useful, in the following, we present a fast linear kernel LDM for large scale problems by adopting the average stochastic gradient descent (ASGD) method <ref type="bibr" target="#b19">[19]</ref>.</p><p>For linear kernel LDM, ( <ref type="formula">5</ref>) can be reformulated as the following form,</p><formula xml:id="formula_31">min w g(w) = 1 2 w ⊤ w + 2λ1 m 2 w ⊤ (mXX ⊤ -Xyy ⊤ X ⊤ )w - λ2 m (Xy) ⊤ w + C m ∑ i=1 max{0, 1 -yiw ⊤ xi}, (<label>14</label></formula><formula xml:id="formula_32">)</formula><p>where</p><formula xml:id="formula_33">X = [x1 . . . xm], y = [y1, . . . , ym] ⊤ is a column vec- tor.</formula><p>For large scale problems, computing the gradient of ( <ref type="formula" target="#formula_31">14</ref>) is expensive because its computation involves all the training examples. Stochastic gradient descent (SGD) works by computing a noisy unbiased estimation of the gradient via sampling a subset of the training examples. Theoretically, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b3">3]</ref>. During the past decade, SGD has been applied to various machine learning problems and achieved promising performances <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b24">24]</ref>.</p><p>The following theorem presents an approach to obtain an unbiased estimation of the gradient ∇g(w). </p><formula xml:id="formula_34">∇g(w,xi, xj) = 4λ1xix ⊤ i w -4λ1yixiyjx ⊤ j w + w -λ2yixi -mC { yixi i ∈ I, 0 otherwise, (<label>15</label></formula><formula xml:id="formula_35">)</formula><p>where</p><formula xml:id="formula_36">I ≡ {i | yiw ⊤ xi &lt; 1} is an unbiased estimation of ∇g(w).</formula><p>Proof. Note that the gradient of g(w) is</p><formula xml:id="formula_37">∇g(w) = Qw + p -C m ∑ i=1 yixi, i ∈ I,</formula><p>where Q = 4λ1(mXX ⊤ -Xy(Xy) ⊤ )/m 2 + I and p = -λ2Xy/m. Further note that</p><formula xml:id="formula_38">Ex i [xix ⊤ i ] = 1 m m ∑ i=1 xix ⊤ i = 1 m XX ⊤ , Ex i [yixi] = 1 m m ∑ i=1 yixi = 1 m Xy. (<label>16</label></formula><formula xml:id="formula_39">)</formula><p>According to the linearity of expectation, the independence </p><formula xml:id="formula_40">= 4λ1Ex i [xix ⊤ i ]w -4λ1Ex i [yixi]Ex j [yjxj] ⊤ w + w -λ2Ex i [yixi] -mCEx i [yixi | i ∈ I] = 4λ1 1 m XX ⊤ w -4λ1 1 m Xy ( 1 m Xy ) ⊤ w + w -λ2 1 m Xy -mC 1 m m ∑ i=1 yixi, i ∈ I = Qw + p -C m ∑ i=1 yixi, i ∈ I = ∇g(w).</formula><p>It is shown that ∇g(w, xi, xj) is a noisy unbiased gradient of g(w).</p><p>With Theorem 2, the stochastic gradient update can be formed as</p><formula xml:id="formula_41">wt+1 = wt -ηt∇g(w, xi, xj), (<label>17</label></formula><formula xml:id="formula_42">)</formula><p>where ηt is a suitably chosen step-size parameter in the t-th iteration.</p><p>In practice, we use averaged stochastic gradient descent (ASGD) which is more robust than SGD <ref type="bibr" target="#b28">[28]</ref>. At each iteration, besides performing the normal stochastic gradient update <ref type="bibr" target="#b17">(17)</ref>, we also compute</p><formula xml:id="formula_43">wt = 1 t -t0 t ∑ i=t 0 +1 wi,</formula><p>where t0 determines when we engage the averaging process. This average can be computed efficiently using a recursive formula: wt+1 = wt + µt(wt+1 -wt), where µt = 1/ max{1, t -t0}.</p><p>Algorithm 2 summarizes the pseudo-code of large scale kernel LDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>In this section, we study the statistical property of LDM. Specifically, we derive a bound on the expectation of error for LDM according to the leave-one-out cross-validation estimate, which is an unbiased estimate of the probability of test error.</p><p>Here we only consider the linear case <ref type="bibr" target="#b14">(14)</ref> for simplicity, however, the results are also applicable to any other feature mapping ϕ. Following the same steps in Section 3.2.1, one can have the dual problem of ( <ref type="formula" target="#formula_31">14</ref>), i.e., min</p><formula xml:id="formula_44">α f (α) = 1 2 α ⊤ Hα + ( λ2 m He -e ) ⊤ α, s.t. 0 ≤ αi ≤ C, i = 1, . . . , m.<label>(18)</label></formula><p>where</p><formula xml:id="formula_45">H = Y X ⊤ Q -1 XY , Q = 4λ 1</formula><p>m 2 (mXX ⊤ -Xy(Xy) ⊤ )+ I, e stands for the all-one vector and Q -1 refers to the inverse matrix of Q. Theorem 3. Let α denote the optimal solution of <ref type="bibr" target="#b18">(18)</ref>, and E[R(α)] be the expectation of the probability of error, then we have</p><formula xml:id="formula_46">E[R(α)] ≤ E[h ∑ i∈I 1 αi + |I2|] m , (<label>19</label></formula><formula xml:id="formula_47">)</formula><p>where</p><formula xml:id="formula_48">I1 ≡ {i | 0 &lt; αi &lt; C}, I2 ≡ {i | αi = C} and h = max{diag{H}}.</formula><p>Proof. Suppose</p><formula xml:id="formula_49">α * = argmin 0≤α≤C f (α), α i = argmin 0≤α≤C,α i =0 f (α), i = 1, . . . , m,<label>(20)</label></formula><p>and the corresponding solution for the linear kernel LDM are w * and w i , respectively. As shown in <ref type="bibr" target="#b17">[17]</ref>,</p><formula xml:id="formula_50">E[R(α)] = E[L((x1, y1), . . . , (xm, ym))] m ,<label>(21)</label></formula><p>where L((x1, y1), . . . , (xm, ym)) is the number of errors in the leave-one-out procedure. Note that if α * i = 0, (xi, yi) will always be classified correctly in the leave-one-out procedure according to the KKT conditions. So for any misclassified example (xi, yi), we only need to consider the following two cases: 1) 0 &lt; α * i &lt; C, according to the definition in (20), we have</p><formula xml:id="formula_51">f (α i ) -min t f (α i + tei) ≤ f (α i ) -f (α * ),<label>(22)</label></formula><formula xml:id="formula_52">f (α i ) -f (α * ) ≤ f (α * -α * i ei) -f (α * ),<label>(23)</label></formula><p>where ei denotes a vector with 1 in the i-th coordinate and 0's elsewhere. We can find that, the left-hand side of ( <ref type="formula" target="#formula_51">22</ref>) is equal to (1 -yix ⊤ i w i )<ref type="foot" target="#foot_1">2</ref> /2hii, and the right-hand side of ( <ref type="formula" target="#formula_52">23</ref>) is equal to α * i 2 hii/2. So by combining ( <ref type="formula" target="#formula_51">22</ref>) and ( <ref type="formula" target="#formula_52">23</ref>), we have</p><formula xml:id="formula_53">(1 -yix ⊤ i w i ) 2 /2hii ≤ α * i 2 hii/2.</formula><p>Further note that yix ⊤ i w i &lt; 0, rearranging the above we can obtain 1 ≤ α * i hii. 2) α * i = C, all these examples will be misclassified in the leave-one-out procedure.</p><p>So we have</p><formula xml:id="formula_54">L((x1, y1), . . . , (xm, ym)) ≤ h ∑ i∈I 1 α * i + |I2|,</formula><p>where</p><formula xml:id="formula_55">I1 ≡ {i | 0 &lt; α * i &lt; C}, I2 ≡ {i | α * i =</formula><p>C} and h = max{hii, i = 1, . . . , m}. Take expectation on both side and with <ref type="bibr" target="#b21">(21)</ref>, we get that (19) holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EMPIRICAL STUDY</head><p>In this section, we empirically evaluate the effectiveness of LDM on a broad range of data sets. We first introduce the experimental settings in Section 4.1, and then compare LDM with SVM and three state-of-the-art approaches 2 in Section 4.2 and Section 4.3. In addition, we also study the cumulative margin distribution produced by LDM and SVM in Section 4.4. The computational cost and parameter influence are presented in Section 4.5 and Section 4.6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We evaluate the effectiveness of our proposed LDMs on twenty regular scale data sets and twelve large scale data sets, including both UCI data sets and real-world data sets like KDD2010 <ref type="foot" target="#foot_2">3</ref> . Table <ref type="table" target="#tab_1">1</ref> summarizes the statistics of these data sets. The data set size is ranged from 106 to more than 8,000,000, and the dimensionality is ranged from 2 to more than 20,000,000, covering a broad range of properties. All features are normalized into the interval [0, 1]. For each data set, half of examples are randomly selected as the training data, and the remaining examples are used as the testing data. For regular scale data sets, both linear and RBF kernels are evaluated. Experiments are repeated for 30 times with random data partitions, and the average accuracies as well as the standard deviations are recorded. For large scale data sets, linear kernel is evaluated. Experiments are repeated for 10 times with random data partitions, and the average accuracies (with standard deviations) are recorded.</p><p>LDMs are compared with standard SVMs which ignore the margin distribution, and three state-of-the-art methods, that is, Margin Distribution Optimization (MDO) <ref type="bibr" target="#b12">[12]</ref>, Maximal Average Margin for Classifiers (MAMC) <ref type="bibr" target="#b18">[18]</ref> and Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) <ref type="bibr" target="#b1">[1]</ref>. For SVM, KM-OMD and LD-M, the regularization parameter C is selected by 5-fold cross validation from <ref type="bibr" target="#b10">[10,</ref><ref type="bibr">50,</ref><ref type="bibr">100]</ref>. For MDO, the parameters are set as the recommended parameters in <ref type="bibr" target="#b12">[12]</ref>. For LDM, the regularization parameters λ1, λ2 are selected by 5-fold cross validation from the set of [2 -8 , . . . , 2 -2 ], the parameters ηt and t0 are set with the same setup in <ref type="bibr" target="#b28">[28]</ref>, and T is fixed to 5. The width of the RBF kernel for SVM, MAMC, KM-OMD and LDM are selected by 5-fold cross validation from the set of [2 -2 δ, . . . , 2 2 δ], where δ is the average distance between instances. All selections are performed on training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Regular Scale Data Sets</head><p>Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_4">3</ref> summarize the results on twenty regular scale data sets. As can be seen, the overall performance of LDM is superior or highly competitive to SVM and other compared methods. Specifically, for linear kernel, LD-M performs significantly better than SVM, MDO, MAMC, KM-OMD on 12, 9, 17 and 10 over 20 data sets, respectively, and achieves the best accuracy on 13 data sets; for RBF kernel, LDM performs significantly better than SVM, MAMC, KM-OMD on 10, 18 and 15 over 20 data sets, respectively, and achieves the best accuracy on 15 data sets. MDO is not compared since it is specified for the linear kernel. In addition, as can be seen, in comparing with standard SVM which does not consider margin distribution, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Large Scale Data Sets</head><p>Table <ref type="table" target="#tab_5">4</ref> summarizes the results on twelve large scale data sets. KM-OMD did not return results on all data sets and MDO did not return results on KDD2010 in 48 hours due to the high computational cost. As can be seen, the overall performance of LDM is superior or highly competitive to SVM and other compared methods. Specifically, LDM performs significantly better than SVM, MDO, MAMC on 6, 7 and 12 over 12 data sets, respectively, and achieves the best accuracy on 8 data sets. In addition, the win/tie/loss counts show that LDM is always better or comparable, never worse than SVM. The curves for other data sets are similar. The point where a curve and the x-axis crosses is the corresponding minimum margin. As can be seen, LDM usually has a little bit smaller minimum margin than SVM, whereas the LD-M curve generally lies on the right side, showing that the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Margin Distributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Time Cost</head><p>We compare the time cost of LDM and SVM on the twelve large scale data sets. All the experiments are performed with MATLAB 2012b on a machine with 8×2.60 GHz CPUs and 16GB main memory. The average CPU time (in seconds) on each data set is shown in Figure <ref type="figure" target="#fig_0">2</ref>. We denote SVM implemented by the LIBLINEAR <ref type="bibr" target="#b9">[9]</ref> package as SVM l and SVM implemented by ASGD<ref type="foot" target="#foot_3">4</ref> as SVM a , respectively. It can be seen that, both SVM a and LDM are much faster than SVM l , owing to the use of ASGD. LDM is just slightly slower than SVM a on three data sets (news20, real-sim and skin) but highly competitive with SVM a on the other nine data sets. Note that both SVM l and SVM a are very fast implementa-  tions of SVMs; this shows that LDM is also computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameter Influence</head><p>LDM has three regularization parameters, i.e., λ1, λ2 and C. In previous empirical studies, they are set according to cross validation. Figure <ref type="figure" target="#fig_4">3</ref> further studies the influence of them on some representative regular scale data sets by fixing other parameters. Specifically, Figure <ref type="figure" target="#fig_4">3(a)</ref> shows the influence of λ1 on the accuracy by varying it from 2 -8 to 2 -2 while fixing λ2 and C as the value suggested by the cross validation described in Section 4.1. Figure <ref type="figure" target="#fig_4">3</ref>(b) and Figure <ref type="figure" target="#fig_4">3</ref>(c) are obtained in the same way. It can be seen that, the performance of LDM is not very sensitive to the setting of the parameters, making LDM even more attractive in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>There are a few studies considered margin distribution in SVM-like algorithms <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b1">1]</ref>. Garg et al. <ref type="bibr" target="#b12">[12]</ref> pro-posed the Margin Distribution Optimization (MDO) algorithm which minimizes the sum of the cost of each instance, where the cost is a function which assigns larger values to instances with smaller margins. MDO can be viewed as a method of optimizing weighted margin combination, where the weights are related to the margins. The objective function optimized by MDO, however, is non-convex, and thus, it may get stuck in local minima. In addition, MDO can only be used for linear kernel. As our experiments in Section 4 disclosed, the performance of MDO is inferior to LDM.</p><p>Pelckmans et al. <ref type="bibr" target="#b18">[18]</ref> proposed the Maximal Average Margin for Classifiers (MAMC) and it can be viewed as a special case of LDM assuming that the margin variance is zero. MAMC has a closed-form solution, however, it will degenerate to a trivial solution when the classes are not with equal sizes. Our experiments in Section 4 showed that LDM is clearly superior to MAMC.</p><p>Aiolli et al. <ref type="bibr" target="#b1">[1]</ref> proposed a Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) from a game theoretical perspective. Similar to MDO, this method directly optimizes a weighted combination of margins over the training data, ignoring the influence of margin variances. Besides, this method considers hard-margin only, which may be another reason why it behaves worse than our method. It is noteworthy that the computational cost prohibits KM-OMD to be applied to large scale data, as shown in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Support vector machines work by maximizing the minimum margin. Recent theoretical results suggested that the margin distribution, rather than a single-point margin such as the minimum margin, is more crucial to the generalization performance. In this paper, we propose the large margin distribution machine (LDM) which tries to optimize the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously. The LDM is a general learning approach which can be used in any place where SVM can be applied. Comprehensive experiments on twenty regular scale data sets and twelve large scale data sets validate the superiority of LDM to SVMs and many state-of-the-art methods. In the future it will be interesting to generalize the idea of LDM to regression and other learning settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 2 .</head><label>2</label><figDesc>If two examples (xi, yi) and (xj, yj) are sampled from training set randomly, then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure1plots the cumulative margin distribution of SVM and LDM on some representative regular scale data sets. The curves for other data sets are similar. The point where a curve and the x-axis crosses is the corresponding minimum margin. As can be seen, LDM usually has a little bit smaller minimum margin than SVM, whereas the LD-M curve generally lies on the right side, showing that the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cumulative frequency (y-axis) with respect to margin (x-axis) of SVM and LDM on some representative regular scale data sets. The more right the curve, the larger the accumulated margin.</figDesc><graphic coords="8,65.75,53.82,478.31,245.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CPU time on the large scale data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parameter influence on some representative regular scale data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Characteristics of experimental data sets.</head><label>1</label><figDesc></figDesc><table><row><cell>Scale</cell><cell>Dataset</cell><cell>#Instance</cell><cell>#Feature</cell><cell>Dataset</cell><cell>#Instance</cell><cell>#Feature</cell></row><row><cell>regular</cell><cell>promoters</cell><cell>106</cell><cell>57</cell><cell>haberman</cell><cell>306</cell><cell>14</cell></row><row><cell></cell><cell>planning</cell><cell>182</cell><cell>12</cell><cell>vehicle</cell><cell>435</cell><cell>16</cell></row><row><cell></cell><cell>colic</cell><cell>188</cell><cell>13</cell><cell>clean1</cell><cell>476</cell><cell>166</cell></row><row><cell></cell><cell>parkinsons</cell><cell>195</cell><cell>22</cell><cell>wdbc</cell><cell>569</cell><cell>14</cell></row><row><cell></cell><cell>colic.ORIG</cell><cell>205</cell><cell>17</cell><cell>isolet</cell><cell>600</cell><cell>51</cell></row><row><cell></cell><cell>sonar</cell><cell>208</cell><cell>60</cell><cell>credit-a</cell><cell>653</cell><cell>15</cell></row><row><cell></cell><cell>vote</cell><cell>232</cell><cell>16</cell><cell>austra</cell><cell>690</cell><cell>15</cell></row><row><cell></cell><cell>house</cell><cell>232</cell><cell>16</cell><cell>australian</cell><cell>690</cell><cell>42</cell></row><row><cell></cell><cell>heart</cell><cell>270</cell><cell>9</cell><cell>fourclass</cell><cell>862</cell><cell>2</cell></row><row><cell></cell><cell>breast</cell><cell>277</cell><cell>9</cell><cell>german</cell><cell>1,000</cell><cell>59</cell></row><row><cell>large</cell><cell>farm-ads</cell><cell>4,143</cell><cell>54,877</cell><cell>ijcnn1</cell><cell>141,691</cell><cell>22</cell></row><row><cell></cell><cell>news20</cell><cell>19,996</cell><cell>1,355,191</cell><cell>skin</cell><cell>245,057</cell><cell>3</cell></row><row><cell></cell><cell>adult-a</cell><cell>32,561</cell><cell>123</cell><cell>covtype</cell><cell>581,012</cell><cell>54</cell></row><row><cell></cell><cell>w8a</cell><cell>49,749</cell><cell>300</cell><cell>rcv1</cell><cell>697,641</cell><cell>47,236</cell></row><row><cell></cell><cell>cod-rna</cell><cell>59,535</cell><cell>8</cell><cell>url</cell><cell>2,396,130</cell><cell>3,231,961</cell></row><row><cell></cell><cell>real-sim</cell><cell>72,309</cell><cell>20,958</cell><cell>kdd2010</cell><cell>8,407,752</cell><cell>20,216,830</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Accuracy (mean±std.) comparison on regular scale data sets. Linear kernels are used. The best accuracy on each data set is bolded</head><label>2</label><figDesc></figDesc><table /><note><p>. •/• indicates the performance is significantly better/worse than SVM (paired t-tests at 95% significance level).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>The win/tie/loss counts are summarized in the last row.</head><label></label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>SVM</cell><cell>MDO</cell><cell>MAMC</cell><cell>KM-OMD</cell><cell>LDM</cell></row><row><cell>promoters</cell><cell>0.723±0.071</cell><cell>0.713±0.067</cell><cell>0.520±0.096•</cell><cell>0.736±0.061</cell><cell>0.721±0.069</cell></row><row><cell>planning-relax</cell><cell>0.683±0.031</cell><cell>0.605±0.185•</cell><cell>0.706±0.034•</cell><cell>0.479±0.050•</cell><cell>0.706±0.034•</cell></row><row><cell>colic</cell><cell>0.814±0.035</cell><cell>0.781±0.154</cell><cell>0.661±0.062•</cell><cell>0.813±0.028</cell><cell>0.832±0.026•</cell></row><row><cell>parkinsons</cell><cell>0.846±0.038</cell><cell>0.732±0.270•</cell><cell>0.764±0.035•</cell><cell>0.814±0.024•</cell><cell>0.865±0.030•</cell></row><row><cell>colic.ORIG</cell><cell>0.618±0.027</cell><cell>0.624±0.040</cell><cell>0.623±0.027</cell><cell>0.635±0.045•</cell><cell>0.619±0.042</cell></row><row><cell>sonar</cell><cell>0.725±0.039</cell><cell>0.734±0.035</cell><cell>0.533±0.045•</cell><cell>0.766±0.033•</cell><cell>0.736±0.036</cell></row><row><cell>vote</cell><cell>0.934±0.022</cell><cell>0.587±0.435•</cell><cell>0.884±0.022•</cell><cell>0.957±0.013•</cell><cell>0.970±0.014•</cell></row><row><cell>house</cell><cell>0.942±0.015</cell><cell>0.943±0.015</cell><cell>0.883±0.029•</cell><cell>0.957±0.020•</cell><cell>0.968±0.011•</cell></row><row><cell>heart</cell><cell>0.799±0.029</cell><cell>0.826±0.026•</cell><cell>0.537±0.057•</cell><cell>0.836±0.026•</cell><cell>0.791±0.030</cell></row><row><cell>breast-cancer</cell><cell>0.717±0.033</cell><cell>0.710±0.031</cell><cell>0.706±0.027</cell><cell>0.696±0.031•</cell><cell>0.725±0.027•</cell></row><row><cell>haberman</cell><cell>0.734±0.030</cell><cell>0.728±0.029</cell><cell>0.738±0.020</cell><cell>0.667±0.040•</cell><cell>0.738±0.020</cell></row><row><cell>vehicle</cell><cell>0.959±0.012</cell><cell>0.956±0.012</cell><cell>0.566±0.160•</cell><cell>0.960±0.010</cell><cell>0.959±0.013</cell></row><row><cell>clean1</cell><cell>0.803±0.035</cell><cell>0.798±0.031</cell><cell>0.561±0.025•</cell><cell>0.821±0.027•</cell><cell>0.814±0.019•</cell></row><row><cell>wdbc</cell><cell>0.963±0.012</cell><cell>0.966±0.010</cell><cell>0.623±0.020•</cell><cell>0.968±0.009•</cell><cell>0.968±0.011•</cell></row><row><cell>isolet</cell><cell>0.995±0.003</cell><cell>0.501±0.503•</cell><cell>0.621±0.207•</cell><cell>0.995±0.003</cell><cell>0.997±0.002•</cell></row><row><cell>credit-a</cell><cell>0.861±0.014</cell><cell>0.862±0.013</cell><cell>0.596±0.063•</cell><cell>0.863±0.013</cell><cell>0.864±0.013•</cell></row><row><cell>austra</cell><cell>0.857±0.013</cell><cell>0.842±0.055</cell><cell>0.567±0.044•</cell><cell>0.858±0.013</cell><cell>0.859±0.015</cell></row><row><cell>australian</cell><cell>0.844±0.019</cell><cell>0.842±0.020</cell><cell>0.576±0.049•</cell><cell>0.858±0.016•</cell><cell>0.866±0.014•</cell></row><row><cell>fourclass</cell><cell>0.724±0.014</cell><cell>0.377±0.238•</cell><cell>0.641±0.020•</cell><cell>0.736±0.014•</cell><cell>0.723±0.014</cell></row><row><cell>german</cell><cell>0.711±0.030</cell><cell>0.737±0.014•</cell><cell>0.697±0.017•</cell><cell>0.729±0.017•</cell><cell>0.738±0.016•</cell></row><row><cell>Ave. accuracy</cell><cell>0.813</cell><cell>0.743</cell><cell>0.650</cell><cell>0.807</cell><cell>0.823</cell></row><row><cell>LDM: w/t/l</cell><cell>12/8/0</cell><cell>9/10/1</cell><cell>17/3/0</cell><cell>10/5/5</cell><cell></cell></row><row><cell cols="3">the win/tie/loss counts show that LDM is always better or</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">comparable, never worse than SVM.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Accuracy (mean±std.) comparison on regular scale data sets. RBF kernels are used. The best accuracy on each data set is bolded. •/• indicates the performance is significantly better/worse than SVM (paired t-tests at 95% significance level). The win/tie/loss counts are summarized in the last row. MDO does not have results since it is specified for the linear kernel.</head><label>3</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>SVM</cell><cell>MDO</cell><cell>MAMC</cell><cell>KM-OMD</cell><cell>LDM</cell></row><row><cell>promoters</cell><cell>0.684±0.100</cell><cell>N/A</cell><cell>0.638±0.121•</cell><cell>0.701±0.085</cell><cell>0.715±0.074•</cell></row><row><cell>planning-relax</cell><cell>0.708±0.035</cell><cell>N/A</cell><cell>0.706±0.034</cell><cell>0.683±0.031•</cell><cell>0.707±0.034</cell></row><row><cell>colic</cell><cell>0.822±0.033</cell><cell>N/A</cell><cell>0.623±0.037•</cell><cell>0.825±0.024</cell><cell>0.841±0.018•</cell></row><row><cell>parkinsons</cell><cell>0.929±0.029</cell><cell>N/A</cell><cell>0.852±0.036•</cell><cell>0.906±0.033•</cell><cell>0.927±0.029</cell></row><row><cell>colic.ORIG</cell><cell>0.638±0.043</cell><cell>N/A</cell><cell>0.623±0.027</cell><cell>0.621±0.039</cell><cell>0.641±0.044</cell></row><row><cell>sonar</cell><cell>0.842±0.034</cell><cell>N/A</cell><cell>0.753±0.052•</cell><cell>0.821±0.051•</cell><cell>0.846±0.032</cell></row><row><cell>vote</cell><cell>0.946±0.016</cell><cell>N/A</cell><cell>0.913±0.019•</cell><cell>0.930±0.029•</cell><cell>0.968±0.013•</cell></row><row><cell>house</cell><cell>0.953±0.020</cell><cell>N/A</cell><cell>0.561±0.139•</cell><cell>0.938±0.022•</cell><cell>0.964±0.013•</cell></row><row><cell>heart</cell><cell>0.808±0.025</cell><cell>N/A</cell><cell>0.540±0.043•</cell><cell>0.805±0.048</cell><cell>0.822±0.029•</cell></row><row><cell>breast-cancer</cell><cell>0.729±0.030</cell><cell>N/A</cell><cell></cell><cell>0.691±0.024•</cell><cell>0.753±0.027•</cell></row><row><cell>haberman</cell><cell>0.727±0.024</cell><cell>N/A</cell><cell>0.742±0.021•</cell><cell>0.676±0.042•</cell><cell>0.731±0.027</cell></row><row><cell>vehicle</cell><cell>0.992±0.007</cell><cell>N/A</cell><cell>0.924±0.025•</cell><cell>0.988±0.008•</cell><cell>0.993±0.006</cell></row><row><cell>clean1</cell><cell>0.890±0.020</cell><cell>N/A</cell><cell>0.561±0.025•</cell><cell>0.772±0.043•</cell><cell>0.891±0.024</cell></row><row><cell>wdbc</cell><cell>0.951±0.011</cell><cell>N/A</cell><cell>0.740±0.042•</cell><cell>0.941±0.040</cell><cell>0.961±0.010•</cell></row><row><cell>isolet</cell><cell>0.998±0.002</cell><cell>N/A</cell><cell>0.994±0.004•</cell><cell>0.995±0.003•</cell><cell>0.998±0.002</cell></row><row><cell>credit-a</cell><cell>0.858±0.014</cell><cell>N/A</cell><cell>0.542±0.032•</cell><cell>0.845±0.029•</cell><cell>0.861±0.013</cell></row><row><cell>austra</cell><cell>0.853±0.013</cell><cell>N/A</cell><cell>0.560±0.018•</cell><cell>0.854±0.017</cell><cell>0.857±0.014•</cell></row><row><cell>australian</cell><cell>0.815±0.014</cell><cell>N/A</cell><cell>0.554±0.015•</cell><cell>0.860±0.014•</cell><cell>0.854±0.016•</cell></row><row><cell>fourclass</cell><cell>0.998±0.003</cell><cell>N/A</cell><cell>0.791±0.014•</cell><cell>0.838±0.014•</cell><cell>0.998±0.003</cell></row><row><cell>german</cell><cell>0.731±0.019</cell><cell>N/A</cell><cell>0.697±0.017•</cell><cell>0.742±0.017•</cell><cell>0.743±0.016•</cell></row><row><cell>Ave. accuracy</cell><cell>0.844</cell><cell>N/A</cell><cell>0.701</cell><cell>0.822</cell><cell>0.854</cell></row><row><cell>LDM: w/t/l</cell><cell>10/10/0</cell><cell>N/A</cell><cell>18/1/1</cell><cell>15/5/0</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : Accuracy (mean±std.) comparison on large scale data sets. Linear kernels are used. The best accuracy on each data set is bolded</head><label>4</label><figDesc></figDesc><table /><note><p>. •/• indicates</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>the performance is significantly better/worse than SVM (paired t-tests at 95% significance level). The win/tie/loss counts are summarized in the last row. KM-OMD and MDO did not return results on some data sets in 48 hours.</head><label></label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>SVM</cell><cell>MDO</cell><cell>MAMC</cell><cell>KM-OMD</cell><cell>LDM</cell></row><row><cell>farm-ads</cell><cell>0.880±0.007</cell><cell>0.880±0.007</cell><cell>0.759±0.038•</cell><cell>N/A</cell><cell>0.890±0.008•</cell></row><row><cell>news20</cell><cell>0.954±0.002</cell><cell>0.948±0.002•</cell><cell>0.772±0.017•</cell><cell>N/A</cell><cell>0.960±0.001•</cell></row><row><cell>adult-a</cell><cell>0.845±0.002</cell><cell>0.788±0.053•</cell><cell>0.759±0.002•</cell><cell>N/A</cell><cell>0.846±0.003•</cell></row><row><cell>w8a</cell><cell>0.983±0.001</cell><cell>0.985±0.001•</cell><cell>0.971±0.001•</cell><cell>N/A</cell><cell>0.983±0.001</cell></row><row><cell>cod-rna</cell><cell>0.899±0.001</cell><cell>0.774±0.203</cell><cell>0.667±0.001•</cell><cell>N/A</cell><cell>0.899±0.001</cell></row><row><cell>real-sim</cell><cell>0.961±0.001</cell><cell>0.955±0.002•</cell><cell>0.744±0.004•</cell><cell>N/A</cell><cell>0.971±0.001•</cell></row><row><cell>ijcnn1</cell><cell>0.921±0.003</cell><cell>0.921±0.002</cell><cell>0.904±0.001•</cell><cell>N/A</cell><cell>0.921±0.002</cell></row><row><cell>skin</cell><cell>0.934±0.001</cell><cell>0.929±0.003•</cell><cell>0.792±0.000•</cell><cell>N/A</cell><cell>0.934±0.001</cell></row><row><cell>covtype</cell><cell>0.762±0.001</cell><cell>0.760±0.003•</cell><cell>0.628±0.002•</cell><cell>N/A</cell><cell>0.763±0.001</cell></row><row><cell>rcv1</cell><cell>0.969±0.000</cell><cell>0.959±0.000•</cell><cell>0.913±0.000•</cell><cell>N/A</cell><cell>0.977±0.000•</cell></row><row><cell>url</cell><cell>0.993±0.006</cell><cell>0.993±0.006</cell><cell>0.670±0.000•</cell><cell>N/A</cell><cell>0.993±0.006</cell></row><row><cell>kdd2010</cell><cell>0.852±0.001</cell><cell>N/A</cell><cell>0.853±0.000•</cell><cell>N/A</cell><cell>0.881±0.001•</cell></row><row><cell>Ave. accuracy</cell><cell>0.913</cell><cell>0.899</cell><cell>0.786</cell><cell>N/A</cell><cell>0.919</cell></row><row><cell>LDM: w/t/l</cell><cell>6/6/0</cell><cell>7/3/1</cell><cell>12/0/0</cell><cell>N/A</cell><cell></cell></row><row><cell cols="3">margin distribution of LDM is generally better than that of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SVM. In other words, for most examples, LDM generally</cell><cell></cell><cell></cell><cell></cell></row><row><cell>produce a larger margin than SVM.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here we omit constants without influence on optimization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>These approaches will be briefly introduced in Section 5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://leon.bottou.org/projects/sgd</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The authors want to thank anonymous reviewers for helpful comments and suggestions. This research was supported by the National Science Foundation of China (61333014) and the National Key Basic Research Program of China (2014CB340501).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A kernel method for the optimization of the margin distribution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Artificial Neural Networks</title>
		<meeting>the 18th International Conference on Artificial Neural Networks<address><addrLine>Prague, Czech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sgd-qn: Careful quasi-newton stochastic gradient descent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Statistics</title>
		<meeting>the 19th International Conference on Computational Statistics<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction games and arcing classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1493" to="1517" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning optimally sparse support vector machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convex formulations of radius-margin based support vector machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd European Conference on Computational Learning Theory</title>
		<meeting>the 2nd European Conference on Computational Learning Theory<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the doubt about margin explanation of boosting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="22" to="44" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Margin distribution and learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="210" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dual coordinate descent method for large-scale linear svm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="408" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local deep kernel learning for efficient non-linear svm prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aggrwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Stochastic approximation and recursive algorithms and applications</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>nd</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Block-coordinate frank-wolfe optimization for structural svms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On estimation of characters obtained in statistical procedure of recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Luntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brailovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technicheskaya Kibernetica</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
	<note>in russian</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A risk minimization principle for a class of parzen estimators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pelckmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How boosting the can also boost classifier complexity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 23rd International Conference on Machine Learning</title>
		<meeting>23rd International Conference on Machine Learning<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="753" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boosting the margin: a new explanation for the effectives of voting methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annuals of Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1651" to="1686" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pegasos: Primal estimated sub-gradient solver for svm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mini-batch primal and dual methods for svms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bijral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtarik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1022" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A refined margin analysis for boosting algorithms via equilibrium margin</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1835" to="1863" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards optimal one pass large scale learning with averaged stochastic gradient descent</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR, abs/1107.2490</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recent advances of large-scale linear classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2584" to="2603" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine learning</title>
		<meeting>the 21st International Conference on Machine learning<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="116" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Ensemble Methods: Foundations and Algorithms</title>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
