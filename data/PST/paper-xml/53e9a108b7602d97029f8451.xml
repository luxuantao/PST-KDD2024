<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">77F6416371C77E1854472547150284C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applying the Genetic Approach to Simulated</head><p>Annealing in Solving Some NP-Hard Problems</p><p>Feng-Tse Lin, Cheng-Yan Kao, and Ching-Chi Hsu</p><p>Abstract-This paper presents a new stochastic a p p r o a c h called the annealing-genetic algorithm f o r solving some wellknown combinatorial optimization problems. This a p p r o a c h incorporates genetic algorithms into simulated annealing to improve t h e performance of simulated annealing. O u r a p p r o a c h has the following features: 1) it can be viewed a s a simulated annealing algorithm with t h e population-based state transition a n d with t h e genetic-operator-based quasi-equilibrium control a n d 2) it can be viewed as a genetic algorithm with t h e Boltzmann-type selection operator. T h e goals of efficiency in o u r algorithm a r e l) t h e g a p between final solution a n d t h e optimal solution should be a r o u n d 3% o r less a n d 2) the computation time should be bounded by a polynomial function of t h e problem size. Empirically, t h e e r r o r r a t e of t h e proposed annealinggenetic algorithm f o r solving the multiconstraint zero-one knapsack problem is less t h a n 1 percent, f o r solving t h e set partitioning problem it is less t h a n 0.1 percent, a n d f o r solving the traveling salesman problem it is a r o u n d 3 percent. I n all the test cases, the annealing-genetic a p p r o a c h obtained much better performance t h a n simulated annealing did. T h e time complexity of t h e proposed algorithm is empirically O(n2) f o r all the t h r e e problems.</p><p>r\ INTRODUCTION ANY combinatorial optimization problems are in-M trinsically NP-hard 141, [ 191 and their deterministic polynomial time algorithms are very unlikely to exist. Heuristic approaches for these NP-hard problems have been the focus of research in this field for quite some time [ 191. However, there are researchers who are trying some different nature-based stochastic approaches for approximately solving these NP-hard problems. Both simulated annealing and genetic algorithms are nature-based stochastic computational techniques; the former is based on thermodynamics and the latter is based on natural evolution. The major advantages of these nature-based algorithms are their broad applicability, flexibility, ease of implementation, and the potential of finding near-optimal solutions [6], <ref type="bibr">[12]</ref>. Kirkpatrick et al. <ref type="bibr">[ l l ]</ref> were the first to propose and demonstrate how to apply the simulation techniques from statistical physics to solve the combinatorial optimization problems. In simulated annealing, a control parameter called temperature is used to control the minimization search, which may occasionally move uphill. The mean and the variance of the cost function are decreasing during the course of the search process. Theoretically, the simulated annealing can be viewed as an algorithm that generates a sequence of Markov chains for a sequence of decreasing temperature values [l], <ref type="bibr">[12]</ref>. At each temperature, the generation process is repeated again and again until the probability distribution of the system states approaches the Boltzmann distribution. If the temperature is decreased slowly enough, the Boltzmann distribution tends to converge to a uniform distribution on the set of globally minimal states. The analysis of simulated annealing based on time-homogeneous and time-inhomogeneous infinite length Markov chains has been carried out independently in the literature <ref type="bibr">[5]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr">[21]</ref>, <ref type="bibr" target="#b21">[23]</ref>. However, in any implementation of the algorithm the Markov chain is of finite length. Therefore, asymptotic convergence can only be approximated. Due to these approximations, the simulated annealing algorithm is no longer guaranteed to find a global minimum with probability 1.</p><p>The performance analysis of an approximation algorithm should be considered with the quality of the final solution obtained as well as the execution time required by the algorithm. Thus, in recent years <ref type="bibr">[12]</ref> many researchers have been engaged in designing an efficient annealing schedule or designing a parallel simulated annealing algorithm for finding a near-optimal solution around a predefined error rate and within a reasonable computation time. Unfortunately, as indicated by Aarts and Korst [ 11, not,,every annealing schedule reported in the literature turns out to be both efficient and certain of quality final results. Therefore, although the simulated annealing is theoretically guaranteed to converge to global optimums with probability 1 if the Markov chains are of infinite length, in practice, efficient annealing schedules are difficult to design.</p><p>Genetic algorithms were pioneered by <ref type="bibr">Holland [8]</ref> and have been proved useful in a variety of search and opti-0018-9472/93$03.00 0 1993 IEEE mization problems over the years. Genetic algorithms are based on the survival-of-the-fitness principle, which tries to retain more genetic information from generation to generation. A genetic algorithm is composed of a reproductive plan that provides an organizational framework for representing the pool of genotypes of a generation. After the successful genotypes are selected from the last generation, the set of genetic operators such as crossover, mutation, and inversion is used in creating the offspring of the next generation. Whenever some individuals exhibit better than average performance, the genetic information of these individuals will be reproduced more often. Genetic algorithms work with a rich database of populations and simultaneously climb many peaks in parallel during the search so that the probability of trapping into a local minimum is reduced significantly. Thus, genetic algorithms are advantageous over simulated annealing for 1) direct manipulation of a coding, 2) search from a population, 3) search using stochastic operators, 4) blindness to auxiliary information, and 5) robustness [2], <ref type="bibr">[6]</ref>.</p><p>From our previous empirical results [ 131, [ 141, there are several observations from the performance analysis of the simulated annealing algorithm.</p><p>1) There is a tradeoff between the quality of the final solution obtained and the execution time required by simulated annealing, and the execution time is sensitive to the decrement ratio of the temperature. 2) It is easily trapped to local minima if the temperature drops too sharply. 3) It is not a trivial task for detecting the equilibrium of the system at each temperature, so that the length of the Markov chain may not be easily controlled. to enhance the potential of finding near-optimal solutions but to reduce the running time of the algorithm. In this paper we propose an annealing-genetic approach incorporating the advantages of genetic algorithms into simulated annealing, so that it is very similar to a combination of steepest descent and Newton's method to improve each other for the constrained optimization problem of the continuous type [ 181. We have devised a very efficient scheme for designing the annealing schedules to solve some wellknown combinatorial optimization problems. The genetic approach to simulated annealing seems to facilitate the exhaustive and parallel treatment of the problem and to increase the probability of finding global minimums.</p><p>In the remainder of this paper we define the major problems of simulated annealing and discuss the characteristics of genetic algorithms in Section 11. The proposed annealing-genetic approach is described in Section 111. The experimental results and the performance analysis of this approach are reported in Section IV and Section V consists of discussions. Finally, the conclusions are given in Section VI. until system equilibrium at Tk Tk+l := Tk* (Y; until system has been frozen print out the current-state as the final state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">SIMULATED ANNEALING AND GENETIC ALGORITHMS</head><p>4) The initial value of temperature affects the total number of iterations required by the annealing. 5) There are still some chances to depart from good solutions if the number of iterations at low temperature regions are not large enough. Subsequently, our work is motivated by these observations and by the idea of adapting the advantages of genetic algorithms to overcome the deficiencies of simulated annealing. However, in contrast to <ref type="bibr">Davis and Ritter's [3]</ref> line of work, we do not optimize the parameters of sim-By the above SA algorithm, a finite-length Markov chain is generated at a certain temperature Tk with a single starting point (state) and a single ending point in a large search space Q. The cost C ( i ) of the ending point i is either lower or higher than the cost of the starting point, which should depend on how much computation time has been spent at that temperature Tk for generating the Markov chain. Thus, the Boltzmann distribution</p><formula xml:id="formula_0">,-CCi)/Ti n-. = 1 e -c ( j ) / T k ' E J S Q</formula><p>where a, is the probability of staying at point i, states that a higher probability of obtaining lower cost of the ending point occurs only after sufficient iterations at temperature Tk. Let a finite state, homogeneous Markov chain be a sequence {Xm, I m = 1, 2, . . } of random variables that take values in Q = { 1 , 2, * * * , n } . It has been proved that if the generated Markov chains are under the conditions of being irreducible, aperiodic, and infinite lengths are allowed <ref type="bibr">[12]</ref>, [21], SA will find a global optimal solution with probability l . Let Q* be the set of indexes of global minimal states in Q. Thus, as Tk tends to zero while k tends to infinity, then</p><formula xml:id="formula_1">lim [ lim Pr ( x , ~ E a*)] = Iim C r, = 1 . T l O , T ~= T m -m T I 0 I € Q *</formula><p>The need of a very long Markov chain at each temperature is the reason that SA may require a long computation time for finding an optimal or a near-optimal solution.</p><p>Next, we study genetic algorithms (GA's) by applying them to solve several typical problems and find some clues for overcoming the above deficiency of SA. Consider the following outline of a GA: another point of view, the mutation operator acts as a local search close to the current point in the search space while the crossover operator causes larger jumps in the search space. Finally, the descendents replace some individuals in the population after the generation step is complete.</p><p>We summarize several key features of GAS as follows:</p><p>1) GA's generate a sequence of populations of candidate solutions to the underlying optimization problem by employing a set of stochastic state transition genetic operators to transform each population of candidate solutions into a descendent population. 2) GA's work from a population instead of a single state. By maintaining a population of well-adapted states, the probability of trapping into a local minimum is greatly reduced.</p><p>3) The crossover operation tries to retain genetic information from generation to generation, assuming that the genetic information always contains important substructures of the quality solutions. The av-1. Initialize the parameters of the genetic algorithm; 2. Randomly generate the old-population; 3. for generation : = 1 to max-generation 4. Clear the newgopulation; 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>7. 8. 9. 10. 11. 12. 13.</p><p>Compute the fitness of each individual in the old-population;</p><p>Copy the highest fitness of individual to the solution-vector;</p><p>while the no-of-individual &lt; population-size do Select two parents from the old-population based on their fitness values: Perform the crossover of the parents to produce two offsprings; Mutate each offspring based on mutation-rate; Place the offspring to new-population; endwhile Replace the old-population by the new-population; 14. endfor 15. Print out the solution-vector as the final solution.</p><p>The above GA simulates an evolutionary process with n individuals (n points) in a large search space Q. From the viewpoint of engineering, GA is an iterative process where each iteration has two steps, the evaluation step and the generation step. In the evaluation step, knowledge of domain is used to determine the fitness, which is a measure of the quality of a candidate. An evaluation function maps candidate solutions into the nonnegative real numbers. The generation step includes a selection operator and modification operators. The selection operator chooses individuals with a probability that corresponds to the relative fitness. Two chosen individuals, called the parents, produce children using the crossover, a genetic operator. The crossover operator exchanges a substring of the codes of the parents at the same randomly determined point or points. However, it does not create any new genetic material in the knowledge base. The mutation operator, on the other hand, randomly changes a bit in the structure introducing a new material into the knowledge base. From ~ ~ erage performance of the next generation is indeed better than the previous. 4) Although the mutation operation has the effect of destroying the structure of a solution, it does not always produce an undesirable solution; there is still a chance for producing a better one. 5) Selecting parents based on their fitness values means that parents with a higher value always have a higher probability of contributing one or more offspring in the next generation. In contrast, parents with a lower fitness value still have a chance to reproduce. Thus, the probability of escaping from local minima of GA's is higher than that of SA.</p><p>6) The genetic operators that emulate biological system behavior possess the property that the conditional dependence of each population is completely determined by its predecessor population. Then, the sequence of populations evolves as a Markov chain. 111. THE PROPOSED ANNEALING-GENETIC APPROACH Since our goal is to look for an approach to design an efficient annealing schedule, a reasonable computation time and a near optimal solution are essential. The goals final solution and the optimal solution should be around 3 % or less and 2) the computation time must be bounded by a polynomial function of the problem size. We start with the following key issues affecting the efficiency of SA from our previous empirical results [ 131, [ 141. These key issues are critical factors to be solved in our work: ature is to be determined in the following manner. For the ceptance probability of detrimental move to be 0.6. From the Metropolis criterion Pr = exp ( -AC/ T ) , where AC = new costold cost, we obtain T = -AC/ln Pr = -AC/ln 0.6 G 2AC. The largest detrimental move in this generation is AC = (the highest costthe lowest cost) /population-size. Thus, the initial temperature value is obtained by dividing the difference between the points of efficiency in our approach are 1) the gap between the sake Of the efficiency Of the we define the ac- <ref type="formula">1</ref>) what is the best range of the initial value of the temof highest cost and the lowest cost P I by half of the popperature ; ulation size, i.e., 2) how to determine the total Iength of the Markov 3) how to detect the equilibrium condition of the sys-the highest cost -the lowest cost chain at each temperature; initial temp = population_size/2 tem at each temperature; minimum;</p><p>Finally, we let the lowest cost of P I be the starting point and the annealing process is then initiated (see Fig. <ref type="figure" target="#fig_1">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Total Length of the Markov Chain</head><p>4, how to keep the system from trapping into a local 5 ) how to know the system has been frozen.</p><p>We tackle these key issues of SA by designing an annealing schedule that has incorporated the advantages of GA's. The important parameters affecting the efficiency of SA are the initial temperature, the total length of the Markov chain at each temperature, the equilibrium condition, and the frozen condition. We use the notions from GA's to initiate and control these parameters. Then, we call this strategy the annealing-genetic (AG) approach for improving SA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Initial Value of Temperature</head><p>The implementation of SA is mostly based on the simulation of condensed matter physics. The initial temperature is set to a sufficiently high value, resulting in a high degree of randomness in the initial stages of the search [9], [12], <ref type="bibr" target="#b21">[23]</ref>. Thus, this adequately hot condition will produce many redundant iterations in the course of SA. Such a strategy is not used in our proposed AG approach. Instead, we use the concepts of GA's. The AG algorithm starts with a randomly generated population Po. Next, the genetic operators are applied to produce a new population PC, by rejecting the higher cost offspring so that the average cost must be less than the random cost Po. Then, the Markov chains are generated from PC, by a small value of temperature until the next generation P I is created. That is, starting with a random point (i.e., individual or state) from PA, the next point is generated from the current point by the move generation strategy. If it is accepted by the Metropolis criterion, the next point not only becomes the current point but also a member of P I ; otherwise, a new starting point from PA is attempted. The process is continued until P , is generated. The initial value of temper-The total length of the Markov chain at temperature Tk is bounded by the population size and the subchains are generated from multiple points of the population. Again, starting with the point of the lowest cost of population Pk, the next point is generated from the current point by the move generation strategy. If it is accepted by the Metropolis criterion, the next point not only becomes the current point but also a point of the next generation P i + otherwise, a new starting point from Pk is attempted. The generated points in P i + I are the candidates for the population of the next generation Pk +</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Quasi-Equilibrium of the System</head><p>After the population Pl'+ I ' has been generated at each temperature Tk, the genetic operators such as crossover, mutation, and inversion are applied to produce a new population Pk + of the next generation. A detailed discussion of these genetic operators is given in Section V. The parents are selected based on their fitness values and their offspring must have their costs less than the average cost of the last generation; otherwise, the parents are reproduced in this generation. The mutation of the offspring is based on the predefined mutation rate, which is always a very small value. From our point of view, the new generation Pk + denotes the quasi-equilibrium of the system at Tk (see Fig. <ref type="figure" target="#fig_2">2</ref>). The main difference between SA and the proposed AG approach is shown in the following figures. Fig. <ref type="figure">3</ref> shows a homogeneous Markov chain with a single starting point and a single ending point by SA at a certain temperature. The cost of the ending point may be lower or higher than the cost of starting point, depending on how much time has been spent at that temperature. The Boltzmann distribution states that at any given temperature the probability of obtaining a lower cost ending point is higher when the iterations are sufficiently large. However, in our approach as shown in Fig. <ref type="figure">4</ref>, there are many starting points and many lower cost ending points that are part of the Markov chains at a given temperature. After the crossover operations, we obtain the new ending points with costs lower than those of old ending points. Thus, at each temperature the Markov chains generated by the proposed AG approach are close to a monotonically decreasing function. (See Fig. <ref type="figure">5</ref> . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. System Frozen</head><p>What is essential to the convergence proof for the system is the fact that the average cost of each generation is less than or equal to that of the last generation. The point of lowest cost in each generation is copied to an independent solution vector as the search continues and is printed at the end of the run. Finally, when 80 percent of the population in a certain generation has the same cost as the solution vector, the frozen condition is signaled. After producing one more generation, the algorithm is terminated.</p><p>The proposed AG approach consists of a two-stage cycle. The first stage is the search by the annealing and the second stage is the evolution by the genetic operators. Fig. <ref type="figure">6</ref> shows the basic concept of the AG approach and the concept is described in the following way. Starting off with a set of arbitrary nodes from the population of cur- rent generation and with a certain temperature, our approach creates a number of different search paths trying to find better solutions simultaneously. A heuristic method with a probability measurement is evaluated to guide these search paths when they climb in the search space. The heuristics includes a move generation mechanism and an acceptance criterion. At each temperature, the nodes visited by these search paths are joined to a set that is called a candidate population. The completion of the annealing stage is reached with the candidate population being created by the search paths. After completing a candidate population, the second stage begins. The genetic operation stage is an evaluation of the evolution function. The evolution function consists of selection and production functions (i.e., crossover, inversion, and mutation). After applying the evolution function to the candidate population, it creates the population of a generation. The next generation is more mature than the last one so that the average cost of the population is decreased. The new structures of the population and the new temperature are then evaluated, and the cycle repeats until the system has frozen. In our approach, the high-performance points become the building blocks for the structures of future generations and are propagated in parallel, generation after generation. The process to generate a sequence of populations from generation to generation is indeed an evolution.</p><p>The proposed genetic approach to simulated annealing is summarized as follows:</p><p>The annealing-genetic (AG) algorithm </p><formula xml:id="formula_2">k : = k + l ;</formula><p>if frozen condition is signaling then set system is frozen; endwhile Perform the local search procedure; Print out the solution-vector as the final solution;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EMPIRICAL RESULTS</head><p>This section describes some computational results with the proposed AG algorithm for three famous NP-hard problems. The first is the multiconstraint zero-one knapsack problem (MCKP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . The MCKP</head><p>The MCKP is a special case of the general zero-one integer programming and arises in the areas of capital budgeting as well as resource allocation [ 191. The MCKP can be used as a subproblem for solving a large integer programming problem. The problem can be stated in the following way. We are given n objects and a knapsack. Object j has weight w , ~ by the ith measure and the knapsack also has m capacities due to m different measures. That is, the capacity of the knapsack by the ith measure is b, and the weight of object j by the ith measure is wlJ. If object j can be placed into the knapsack satisfying all different m measures, then a profit of p,xJ is earned. The objective is to obtain a filling of the knapsack that maximizes the total profit earned. Since the knapsack capacity is limited, we require the total weight of all chosen ob-  2) The time in seconds is the turnaround time. The algorithm is implemented on a 80386-based PC written in Turbo-C.</p><p>3) The annealing schedule for SA is as follows: initial iterations = 2 * n, upper bound of iterations = IO * n ; increment ratio of iterations = 1.2, cooling ratio = 0.85, initial temperature = 10.0 (with sufficiently hot conditions), frozen temperature = 1 .O, and stopping criterion = no better solution obtained. 4) The move generation strategy is stated in Section V.</p><p>the optimal solution of each problem is determined. Therefore, the optimal solution of this set of problems is known and the performance of the algorithm can be easily determined. Table <ref type="table" target="#tab_3">I</ref> shows the performance analysis of AG and SA on this set of 10 randomly generated problems where AG obtains better performance than SA. Although the final solutions obtained by both methods are very close to the optimal solutions, SA always needs more iterations as well as running time, about twice, than that for AG (see Fig. <ref type="figure" target="#fig_5">7</ref>). Furthermore, the solution quality of AG is better than that of SA. Fig. <ref type="figure" target="#fig_6">8</ref> shows the performance analysis of AG where the square root of the total legal moves divided by the product of a problem size n and constraints m is decreased as n grows, and the square root of the turnaround time divided by n * m is gradually stable as n grows. This implies that the algorithm is empirically better than 0 (n2) .</p><p>The second set of experiments consists of well-known test problems from Peterson and Senju-Toyoda [20], [22]. The comparative results of AG and SA are shown in Table <ref type="table">11</ref>. The final solutions of these problems by AG are always less than 0.8 percent error rate of the optimal solution. Fig. <ref type="figure" target="#fig_7">9</ref> shows the comparison of SA and AG in regard to the ratio of the square root of legal moves divided by the problem size where AG has fewer legal moves than SA. The legal moves are the lowest number of iterations used in the algorithm in searching for better solutions. Fig. <ref type="figure" target="#fig_1">10</ref> shows the comparison of SA and AG in terms of the ratio of the square root of the running time divided by the problem size in which AG has a shorter running time than SA. Empirically, the proposed annealing-genetic algorithm is O(n2&gt; for MCKP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Set Partitioning Problem</head><p>The second problem is concerned with the set partitioning problem. The set partitioning problem is formally de- -time,fii(l)-column in Table <ref type="table">1</ref>.</p><p>Legal moves,fi),(i) -column in Table <ref type="table" target="#tab_3">I</ref> .  2) The annealing schedule for SA is as follows: initial iterations = 2 * n , upper bound of iterations = IO * n : increment ratio of iterations = 1.2, cooling ratio = 0.7, initial temperature = max p, -min p , (with sufficiently hot conditions). frozen temperature = I .O. and stopping criterion = no better solution obtained. For the first set of experiments, the problem sizes of n = 10 to n = 120 are randomly generated with the wL, from 0 to 499. Because of the 16-bit integer limitation, the wL, of the larger problems of 130, 140, and 150 are randomly generated from 0 to 299. Table <ref type="table" target="#tab_6">I11</ref> is the performance analysis of AG and SA on this set of 15 randomly generated data. Again, although the solution quality obtained by both methods is about equal and very close to the optimal solutions, SA always need more iterations in addition to as much running time, about twice, than that of AG. Fig. <ref type="figure" target="#fig_9">11</ref> is the performance analysis of AG on this set of randomly generated data. The ratio of the square root of the total moves divided by n is monotonically decreasing as n grows up. The square root of turnaround time divided by n is also gradually stable as n grows. The second set of experiments is concerned with n = 100 and consists of ten randomly generated test data. As shown in Table <ref type="table" target="#tab_8">IV</ref> and Fig. <ref type="figure" target="#fig_10">12</ref>, the ratio of the square root to the running time divided by n and the ratio of the square root of the legal moves divided by n are very stable for these test data, and the gaps between the final and the optimal solutions are less than 0.03 percent. In fact, for seven out of the 10 problems the optimal solutions are obtained. The proposed algorithm is empirically better than 0 (n2) for the set partitioning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Traveling Salesman Problem</head><p>The third problem is the traveling salesman problem. This problem is more difficult to solve than the other two problems because the solution space is much larger. The first set of experiments consist of 9 problems that have <ref type="bibr">16, 25, 36, 49, 64, 81, 100, 121, and</ref>   2) The move generation strategy is stated in Section V .    1) The execution time actually included the time spent in the display of the tours periodically.</p><p>2) The annealing schedule for SA is as follows: iteration ratio = 1.2, initial iterations = 2 * n ( n is the number of cities).</p><p>upper bound of iterations = IO * n; cooling ratio = 0.9, initial temperature = 10.0 (sufficiently hot enough conditions), frozen temperature = 0.05, and stopping criterion = no better solution obtained.</p><p>3) The move generation strategy is stated in Section V 0 1 6 2 5 3 6 4 9 6 4 81 1 0 0 1 2 1 1 4 4 number of cities is 8.2 percent. Table <ref type="table" target="#tab_11">VI</ref> is the performance analysis of AG on the regular-grid TSP's. Again, as shown in Fig. <ref type="figure" target="#fig_1">14</ref>, both the ratios of the square root of the total number of moves and the square root of the turnaround time divided by the number of cities approach to a constant as the number of cities increase. Therefore, the proposed AG approach is also empirically O(n2) for the regular-grid TSP's. The initial cost of each problem is the cost of a random tour; it usually serves as a starting state in the conventional annealing schedule. In our approach, as indicated in Fig. <ref type="figure" target="#fig_1">15</ref>, the starting state of each problem has a lower cost that is generated by the initial stage of the algorithm.</p><p>In other set of experiments, three TSP's taken from the literature [24], 3O-city, 50-city, and 75-city problems are used for performance analysis. The best-known result of the 30-city problem in the literature is 421. However, we have obtained the new optimal solution of 420 by the proposed AG approach. Although the final solutions in the 50-city and 75-city problems are not optimal, their gaps to the best-known solutions are not greater than 3 percent; the total number of moves are reduced significantly and the gaps to the optimal solutions are smaller compared to SA, as indicated in Table VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this section we first discuss the efficiency of the AG algorithm by examining a given simple example of the MCKP. The AG approach has the feature whereby it can be viewed as 1) an SA algorithm with the population-based state transition and with the genetic-operator-based quasiequilibrium control and 2) as a GA with a Boltzmann-type selection operator. The AG approach is a two-stage cycle: the first stage is an annealing stage by SA and the second stage is a genetic operation stage by GA. In each anneal-  ing stage, an iterative generate-and-test procedure is executed to generate many search paths that create a candidate population. In each genetic operation stage, a set of genetic operators is applied to the candidate population to evaluate the cost function of each member and to produce the population of the next generation. Usually, the genetic operations make the next generation more mature than the last generation and manipulate a set of schemata of the population. A schema is a building block describing a subset of points with similarities at certain positions in the bit-string (a representation in the points) [6], [8]. The schemata are also used to guide the searches of the annealing stage in each cycle. They have the power of guid-ing the directions of the search paths toward high-performance regions. Therefore, the concept of schemata is a powerful tool for a genetic approach to simulated annealing. The schema structure restricts the search in a hyperplane and inherits the hyperplane subdivision of the search space defined by the schemata. Because schemata are associated with a characteristic fitness, it follows that the hyperplane subdivision also have a characteristic fitness. The relationship between the whole search space and the high-performance subspace is exactly the relationship we want to establish. Another important feature of the genetic approach is the reliance on global information, i.e., the schemata that are being processed and consequently emerge to dominate the population spread over the entire space. The underlying concept of schemata is that schemata are a degenerate description of the entire space that confines the searches to the high-performance regions. Thus, the search paths will eventually converge to an optimal or near-optimal point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>31</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . The Schemata Theory</head><p>In the following, we discuss why the schemata exhibit high efficiency in search space to guide the searches of the search paths. We give a simple instance of the MCKP with the problem size n = 6 and the constraints m = 10, which is shown in Table <ref type="table" target="#tab_11">VIII</ref>. The optimal solution is a vector X = [0, 1, 1, 0, 0, 11 with cost = 3800. By examining the fitness of any one point, one might expect to obtain information about other points that have a similar structures. Usually, we can associate 3" schemata over the extended alphabet (0, 1, *} with length = n of a point.</p><p>The notion of schemata arises from the observation that in evaluating the fitness of a point we can also derive implicit knowledge about the schemata that describe that point. The accuracy of this extrapolation depends on the specification of the given schema. It is important ta identify the schemata that have above-average fitness in the population, because the important similarities among these highly fit points can help guide a search during the annealing stage by SA. These high-performance hyperplane schemata also play an important role in investigating the structure of large, complex search spaces. However, the high-performance schemata are limited in each hyperplane. Table <ref type="table" target="#tab_15">IX</ref> shows the high-performance schemata in I ) The annealing schedule for SA is as follows: iteration ratio = 1.2, initial iterations = 2 * n (n is the number of cities), upper bound of iterations = 10 * n ; cooling ratio = 0.9. initial temperature = 10.0 (with sufficiently hot enough conditions), frozen temperature = 0.05, and stopping criterion = no better solution obtained.</p><p>2) The move generation strategy is stated in Section V .   Optimal solution = 3800, xi = 1 (i = 2, 3, 6); all others = 0.</p><p>In order to identify the high-performance schemata, it is necessary to study the dynamics of the schema frequencies as the search progress. By examining which schemata have above-average fitness values, we can foresee the population of subsequent generations due to the fact that these schemata are expected to proliferate. Then, the structure of the future populations can be estimated through schema frequency and fitness. Fig. <ref type="figure" target="#fig_14">16</ref> shows the growth of the high-performance schemata that have #(S,) = 1 , 2 , 3 , 4 , 5 , and 6, indicated in Table <ref type="table" target="#tab_15">IX</ref>, as the search progresses in term of generations. At the first generation, only the schemata with #(S,) = 1 and #(S,) = 2 have the frequency of occurrences 12 and 8, respectively. At the second generation, the frequency of schemata with #(S,) = 1 is decreased, the frequency of schemata with #(S,) = 2 is increased, and the schemata with #(S,) = 3 appear.</p><p>While at the third generation the frequency of schemata with #(S,) = 3 become dominant in the population and the high performance with schema #(S,) = 5 appear. Eventually, at the fifth generation only the high-performance schemata, with #(S,) = 5 and with #(S,) = 6, exist in the population.</p><p>As we have stated in the previous paragraph, the concept of schemata restricts the search in a hyperplane. The underlying concept of schemata is that high-performance schemata are a degenerate description of the entire space that confines the search toward high-performance regions so that the search paths will eventually converge to an optimal or near-optimal point. Fig. <ref type="figure" target="#fig_15">17</ref> shows the percentage of reduction in the search space versus the number of specific positions contained in the schema. As #(S,) = 1, the percentage of reduction in the search space is 50 percent. While #(S,) = 6, the percentage of reduction in the search space is 100 percent; this means that the search path is staying at the optimal point X = [0, 1, 1, 0, 0, 13.</p><p>The choice of the population size is essential to the quality of the final solution as well as the total running time required by the AG algorithm. According to the concept of schemata and the operations of genetic operators, the AG algorithm guarantees a better performance generation after generation. The larger population-size version may obtain a better performance population than that of the smaller population-size version, but at the expense of prolonging the execution time. Fig. <ref type="figure" target="#fig_6">18</ref> shows curves of the average cost of the population obtained by the AG algorithm in six generations with the population sizes of 6, 12, 18, and 24, respectively. The high-performance population appears in the larger population-size version. However, all the four versions with different population sizes converge to the optimal solution at the sixth generation. The larger population-size version needs more running time to converge than the smaller population-size ones. From our empirical results [ 151, the population size should depend on the structure of the underlying problem. Usually, the choice of population size is simply a constant factor of the problem size. A Markov chain theoretical analysis of the global convergence behavior and the running time complexity of the AG algorithm is in progress and will be reported in a future paper.</p><p>In fact, the proposed AG approach has been applied to other NP-hard optimization problems, e.g., the task assignment problem in distributed systems [ 161, and the onedimensional bin-packing problem [ 151. However, different problems may have to use different domain-specific knowledge. The representation scheme, the move generation strategy, and the genetic operators are needed to be specified differently to reflect the different structures of the problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Representation Scheme</head><p>The solution representation of the underlying problem can be conceptually viewed as a point; in other words the feasible solutions of the problem are the points in the search space. For the MCKP and the set partitioning problems, the solution representation is quite straightforward and suitable for genetic operations. The solution is represented as a vector X = [x,, * * , x, ] where x, = 1 or x, = 0, 1 I i I n , and n is the problem size. The value of x, is to denote whether an object i is included or not included in the current solution. However, the choice of an appropriate representation of the TSP suitable for genetic operators is a nontrivial task. In our representation, the cities are numbered from 1 to n. Each city i is denoted by its two coordinates (x,, y,). The coordinates of these n cities are placed in an index table where the ith entry is the coordinate of city i. A tour is then represented by a vector X of size n containing the indexes of the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Move Generation Strategy</head><p>There are two general strategies employed in AG as well as in SA for generating the next points of the search path in solving the MCKP and the set partitioning problems. The first is a greedy move and the second is a swapping move. A point in the search space is represented by a vector X( 1 : n ) where n is the problem size. The greedy move means, first, an element X(i) is randomly selected from the current solution (point); second, the selected element is assigned to another value in an attempt to decrease the cost of the current solution and move to another point. For example, change X(i) from 1 to 0. The swapping move means, we randomly choose two elements, say X ( i ) and X ( j ) , which have been assigned to certain values, say 1 and 0, respectively, and then exchange their roles. That is, we exchange X(i) = 1 and X ( j ) = 0 to X(i) = 0 and X( j) = 1. respectively.</p><p>As for solving the TSP, only the swapping move strategy is used both in AG and in SA. The swapping strategies in the TSP algorithm are the random 2-exchange and the locally adjacent swap, which are shown in Fig. <ref type="figure" target="#fig_16">19</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Genetic Operators</head><p>The three genetic operators not only modify the points in the old population to create a new population but also to decrease (or to increase) the average cost of the new population when the underlying problem is a minimization (or a maximization) problem. The crossover operator used in the algorithm is the usual crossover operator in the GA literature [ 8 ] , <ref type="bibr">[lo]</ref>. Crossover takes two selected parents, e.g., two points in the old population, splits the structures at the same two randomly determined crossing sites, and then creates two offspring by swapping the middle portion of the structures. Hence, the function of the crossover is to generate rearrangements of coadapted groups of substructures from high-performance points. Nevertheless, crossover may produce the illegal structure of offspring. However, a penalty is added to the illegal structure. The second operator is inversion. Inversion altars the sequence between two randomly assigned indexes in a single point. This action modifies the structure of a single point, providing a more fertile ground for searching a better point by the greedy move in SA. The final operator is mutation. The intention of using the mutation operator in our approach is to ensure that all points in the search space remain reachable by SA. This can be easily achieved by changing a randomly selected element in a single point. However, mutation acts as a random search for a better point by our approach.</p><p>The genetic operators are performed according to the following steps.</p><p>Step I : The parents are selected from the population based on their fitness values. Then, the crossover operator is applied to produce their offspring. The offspring must have their costs less than the average cost of the old generation, otherwise, the parents continue the following steps.</p><p>Step 2: The inversion operator is applied to the parents for reordering their own sequence to obtain new costs. If the new cost is lower than the old one, the parent is copied to the next generation; otherwise, the parents continue the next operation.</p><p>Step 3: The mutation operator is applied to the parents based on a predefined probability. Finally, the parents are copied to the next generation. This cycle is repeated again and again until the next population of the next generation is generated.</p><p>Due to the fact that the solution representation in TSP is not a binary-string style, the genetic operators may con- struct illegal tours. The penalty approach for infeasible solutions is not suitable for TSP. Therefore, the genetic operators must construct legal tours. Among the genetic operators used in our approach, the crossover is the most difficult operation to implement. Our crossover operation is one of the fundamental parts of the TSP algorithm and is described as follows:</p><p>Step 1:</p><p>Step 2:</p><p>Step 3: After the crossover, the child whose cost is below the average cost of the population is to be copied to the new population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have presented an annealing-genetic algorithm for solving three well-known NP-hard problems. The key features of this approach are the incorporation of the advantages of genetic algorithms into simulated annealing.</p><p>One of the major issues of many annealing schedules is the concept of quasi-equilibrium [l], <ref type="bibr">[12]</ref>. The precise meaning of this concept is one of the key points differentiating one annealing schedule from the others. In our approach, the genetic operators are applied to the population that has been generated at each temperature by simulated annealing to induce maturation. Next, we impose the rule that the offspring must have their costs be less than the average cost of the last generation, otherwise, the parents are reproduced in this generation. This simple rule is intended to guarantee the convergence of the algorithm and also to make a shorter Markov chain that is generated at each temperature to reach the quasi-equilibrium.</p><p>Our results of an empirically O(n2) AG algorithm may have very important practical implications in solving the TSP. As far as we know, the best theoretical result for the regular-grid TSP of SA is O(n4 log n) [ 121. Even for heuristic methods, most of those are at best O(n3) <ref type="bibr">[ 7 ]</ref> , and the famous Lin-Kemighan heuristic is empirically 0 (n2,2) [ 171. In addition, the new best-known solution of 420 in Oliver's 30-city problem is obtained by use of our algorithm.</p><p>In conclusion, we would like to point out that our work has produced the following important results for improving SA and for solving some large scale NP-hard problems:</p><p>A new way for determining an adequate initial temperature value and its relevant sufficient initial state. There are multiple search points in a population instead of a single point; the probability of trapping into local minima is greatly reduced. The crossover of population for generating lower average cost generation denotes the quasi-equilibrium of the Markov chain at each temperature, such that the length of the Markov chain is bounded by the population size. The frozen condition of the system is the convergence of the population that seems more accurate and easier to detect than SA. Although SA is theoretically guaranteed to converge to global optimums with probability 1 if the Markov chains are of infinite length, in practice efficient annealing schedules are difficult to design. The genetic approach to SA, e.g., our proposed annealing-genetic algorithm, seems to facilitate the exhaustive and parallel treatment of the problem and to increase the probability of finding global minimums. Empirical results reveal that our approach gives near-optimal solutions with the gaps to the optimal solutions less than 0.03 percent for the set partitioning problem, less than 1 percent for the MCKP, and around 3 percent for the TSP, all in a reasonable computation time.</p><p>The AG algorithm for these problems is empirically an O(n2) algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received November 15, 1991; revised November 6, 1992. F.-T. Lln was with the Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan. He is now with the Department of Applied Mathematics, Chinese Culture University, Yangminshan, Taipei, Taiwan. C.-Y. Kao and C. C.-Hsu are with the Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan. IEEE Log Number 9207502.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Flg 1</head><label>1</label><figDesc>generation, one can assure a faster convergence ratio of GA's. Determining the initial value of temperature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Markov chains at temperature Tk and its quasi-equilibrium</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5 . The Markov chains generated by AG and SA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1151</head><label></label><figDesc>jects to be at most bi by each different measure i, i = 1, . . . , m. Formally, this problem can be formulated as follows: n maximize z = C pjxj subject to JFl wqxj I b; and xj = 0 or 1 j = 1 n where i = 1, , m and j = l ; * . , n. Without a loss of generality, all pi, wij, and b; are assumed to be nonnegative integers. For the first set of experiments, ten test problems with ten constraints (i.e., m = 10) and a problem size of n = 10, 20, 30, * * , 100 are randomly generated. The wij are uniformly distributed random numbers from 0 to 500 and each p j is equal to each wlJ. By letting xj randomly equal 0 or 1 and defining n bl = c wl,xJ j = I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 ,</head><label>7</label><figDesc>Fig. 7 , Comparison of SA and AG in regard to the square root of the running time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Performance analysis of AG on the ten randomly generated data of MCKP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison of SA and AG in regard to the ratio of the square root of legal moves divided by the problem size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. IO. Comparison of SA and AG in regard to the ratio of the square root of running time divided by the problem size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Performance of AG for each problem size on the set partitioning problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Performance of AG on ten randomly generated data of the same problem size n = 010.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Comparison of SA and AG in regard to the error rate to the optimal solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Performance of AG in regard to the ratio of the square root ratio of moves and time for each problem size on TSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>means the number of specific positions contained in the schema term of #(S,), the number of specific positions contained in the schema, in increasing order of the given example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Frequency histogram of the six schemata as the search progress.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig 18. The average cost of the population obtamed by the AG in different population sizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. The swapping strategy for the TSP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. An example of the crossover operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Fig.20shows an example of the crossover operation. After the crossover, the child whose cost is below the average cost of the population is to be copied to the new population.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Initialize the parameters, i.e., population-size, To, and CY (0 &lt; CY &lt; 1); Apply genetic operators to Ph to create Po; Calculate the fitness and the cost for each point in Po; Calculate the average cost of Po; solution-vector : = currentgoint : = the lowest cost point in Po; Calculate the fitness and the costs for each point in Pk + I ; Calculate the average-cost of Pk + I ; if the bWeSt cost point in Pk + I &lt; solution-vector then update solution-vector;</figDesc><table><row><cell>1.</cell><cell></cell></row><row><cell>2.</cell><cell>Randomly generating P i ;</cell></row><row><cell>3.</cell><cell></cell></row><row><cell>4.</cell><cell></cell></row><row><cell>5 .</cell><cell></cell></row><row><cell>6.</cell><cell></cell></row><row><cell>7.</cell><cell>k : = 0;</cell></row><row><cell>8.</cell><cell>while system is not frozen do</cell></row><row><cell>9.</cell><cell>no-ofgoint := 0;</cell></row><row><cell>10.</cell><cell>while no-ofgoint I population-size do</cell></row><row><cell>11.</cell><cell>Generate nextgoint from current-point by the move generation strategy;</cell></row><row><cell>12.</cell><cell>AC : = cost of next-point -cost of currentjoint;</cell></row><row><cell>13.</cell><cell>Pr := min [ I , exp ( -A c / T k ) ] ;</cell></row><row><cell>13.</cell><cell>if Pr &gt; random[O, 1) then put nextgoint into P l +</cell></row><row><cell>1s. 16.</cell><cell>current-point : = next-point; no-ofgoint : = no-ofgoint + 1</cell></row><row><cell>17.</cell><cell></cell></row><row><cell>18.</cell><cell></cell></row><row><cell>19.</cell><cell></cell></row><row><cell>20.</cell><cell></cell></row><row><cell>21.</cell><cell></cell></row><row><cell>22.</cell><cell></cell></row><row><cell>23.</cell><cell></cell></row><row><cell>24.</cell><cell></cell></row><row><cell>25.</cell><cell></cell></row><row><cell>26.</cell><cell></cell></row><row><cell>27.</cell><cell></cell></row><row><cell>28.</cell><cell></cell></row><row><cell>29.</cell><cell></cell></row><row><cell>30.</cell><cell></cell></row><row><cell>31.</cell><cell></cell></row></table><note><p>; else pick another point from Pk as current-point; endwhile Apply the genetic operators to P;+ I to create P , + I ; if it is the initial stage then determining the initial temperature T I ; TI : = (the highest costthe lowest cost)/(population-size/2); elst! Tk. 1 = Tk* CY; currentgoint : = the lowest cost point in Pk + ;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc>RESULTS OF THE FIRST SET OF 10 RANDOMLY GENERATED DATA ON THE MCKP</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>AG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SA</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cost</cell><cell></cell><cell></cell></row><row><cell>(1) n * m</cell><cell>Optimal</cell><cell>Final</cell><cell>Gap (percent)</cell><cell>(2) Legal Moves</cell><cell>(3) Time (S)</cell><cell>~</cell><cell>Jizi (1)</cell><cell>43 (1)</cell><cell>Final</cell><cell>(percent) Gap</cell><cell>Legal Moves</cell><cell>Time (s)</cell></row><row><cell>10 * 10 20 * 10 30 * 10 40 * 10 50 * 10 60 * 10 70 * 10 80 * 10 90 * 10 100 * 10</cell><cell>1922 3485 3779 392 1 6879 7913 9141 9482 14970 19060</cell><cell>1922 3485 3779 392 1 6879 7913 9140 948 1 14968 19060</cell><cell>0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.00</cell><cell>246 73 1 878 753 1129 1377 1201 1352 1395 1787</cell><cell>2 6 8 9 16 22 22 29 33 49</cell><cell cols="2">0.157 0.135 0.099 0.069 0.067 0.062 0.050 0.046 0.042 0.042</cell><cell>0.014 0.012 0.009 0.008 0.008 0.008 0.007 0.007 0.006 0.007</cell><cell>1922 3485 3779 392 1 6878 791 1 9138 9478 14964 19052</cell><cell>0.00 0.00 0.00 0.00 0.01 0.02 0.03 0.04 0.04 0.04</cell><cell>698 1446 2150 2916 3601 432 1 5042 5767 6501 723 1</cell><cell>3 6 11 19 28 37 49 62 78 91</cell></row><row><cell>1) n</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>= problem size. m = number of constraints.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I1 RESULTS</head><label>I1</label><figDesc>OF THE SECOND SET OF WELL-KNOWN TEST PROBLEMS ON THE MCKP The problem sizes of 60 * 30 are from Senju and Toyoda 1221 and the others are from Peterson [20].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>AG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SA</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cost</cell><cell></cell><cell></cell></row><row><cell>(1) n * m</cell><cell>Optimal</cell><cell>Final</cell><cell>Gap (percent)</cell><cell>(2) Legal Moves</cell><cell>(3) Time (s)</cell><cell cols="2">9 J(li ( 1 ) ( 1 )</cell><cell>Final</cell><cell>Gap (percent)</cell><cell>Legal Moves</cell><cell>Time (s)</cell></row><row><cell>6 * 10 10 * 10 15 * 10 20 * 10 28 * 10 39 * 5 50 * 5 60 * 30 60 * 30</cell><cell>3800 8705 4015 6120 12400 10618 16537 7772 8722</cell><cell>3800 8705 4015 6120 12400 10556 16469 7758 8656</cell><cell>0.00 0.00 0.00 0.00 0.00 0.58 0.41 0.18 0.76</cell><cell>419 690 81 1 1083 1517 1848 3449 6239 3131</cell><cell>3 5 7 10 17 28 56 178 I 92</cell><cell>0.341 0.263 0. I90 0.165 0. I39 0.232 0.235 0.044 0.03 I</cell><cell>0.029 0.022 0.018 0.016 0.015 0.027 0.030 0.007 0.005</cell><cell>3800 8335 4015 6120 12330 10552 16382 7692 8596</cell><cell>0.00 4.25 0.00 0.00 0.56 0.62 0.94 1.03 1.44</cell><cell>214 91 1 1473 2965 4097 8649 17121 38586 18673</cell><cell>2 6 9 16 23 45 84 263 121</cell></row><row><cell>I )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE I11 RESULTS OF THE FIRST SET OF 15 RANDOMLY GENERATED DATA ON THE SET PARTITIONING PROBLEM</head><label>I11</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>~~~~~</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SA</cell><cell></cell><cell></cell></row><row><cell>(1)</cell><cell></cell><cell></cell><cell></cell><cell>Legal (2)</cell><cell>Time (3)</cell><cell>-fi</cell><cell cols="2">3 Final</cell><cell>Gap</cell><cell>Legal</cell><cell>Time</cell></row><row><cell>n</cell><cell>Optimal</cell><cell>Final</cell><cell>Gap (percent)</cell><cell>Moves</cell><cell>(s)</cell><cell>(1)</cell><cell>(1)</cell><cell>Cost</cell><cell>(percent)</cell><cell>Moves</cell><cell>(S)</cell></row><row><cell>10</cell><cell>1202</cell><cell>1202</cell><cell>0.00</cell><cell>124</cell><cell>I</cell><cell>1.114</cell><cell>0.100</cell><cell>1200</cell><cell>0.00</cell><cell>82</cell><cell>0.5</cell></row><row><cell>20</cell><cell>2456</cell><cell>2456</cell><cell>0.00</cell><cell>259</cell><cell>2</cell><cell>0.805</cell><cell>0.071</cell><cell>2456</cell><cell>0.00</cell><cell>171</cell><cell>1</cell></row><row><cell>30</cell><cell>3644</cell><cell>3644</cell><cell>0.00</cell><cell>304</cell><cell>2</cell><cell>0.581</cell><cell>0.047</cell><cell>3643</cell><cell>0.03</cell><cell>497</cell><cell>2</cell></row><row><cell>40</cell><cell>4939</cell><cell>4939</cell><cell>0.00</cell><cell>376</cell><cell>3</cell><cell>0.485</cell><cell>0.043</cell><cell>4939</cell><cell>0.00</cell><cell>743</cell><cell>3</cell></row><row><cell>50</cell><cell>6480</cell><cell>6480</cell><cell>0.00</cell><cell>467</cell><cell>3</cell><cell>0.432</cell><cell>0.035</cell><cell>6480</cell><cell>0.00</cell><cell>836</cell><cell>4</cell></row><row><cell>60</cell><cell>7600</cell><cell>7599</cell><cell>0.01</cell><cell>548</cell><cell>5</cell><cell>0.390</cell><cell>0.037</cell><cell>7599</cell><cell>0.01</cell><cell>1105</cell><cell>5</cell></row><row><cell>70</cell><cell>8907</cell><cell>8906</cell><cell>0.01</cell><cell>598</cell><cell>5</cell><cell>0.349</cell><cell>0.032</cell><cell>8906</cell><cell>0.01</cell><cell>1376</cell><cell>6</cell></row><row><cell>80</cell><cell>10548</cell><cell>10547</cell><cell>0.00</cell><cell>713</cell><cell>7</cell><cell>0.334</cell><cell>0.033</cell><cell>10547</cell><cell>0.00</cell><cell>2070</cell><cell>8</cell></row><row><cell>90</cell><cell>11713</cell><cell>11712</cell><cell>0.00</cell><cell>762</cell><cell>9</cell><cell>0.307</cell><cell>0.033</cell><cell>11712</cell><cell>0.00</cell><cell>3628</cell><cell>14</cell></row><row><cell>100</cell><cell>12897</cell><cell>12897</cell><cell>0.00</cell><cell>880</cell><cell>IO</cell><cell>0.297</cell><cell>0.032</cell><cell>12897</cell><cell>0.00</cell><cell>492 1</cell><cell>19</cell></row><row><cell>110</cell><cell>14166</cell><cell>14166</cell><cell>0.00</cell><cell>947</cell><cell>12</cell><cell>0.280</cell><cell>0.031</cell><cell>14166</cell><cell>0.00</cell><cell>5452</cell><cell>21</cell></row><row><cell>120</cell><cell>15438</cell><cell>15438</cell><cell>0.00</cell><cell>996</cell><cell>14</cell><cell>0.263</cell><cell>0.031</cell><cell>15438</cell><cell>0.00</cell><cell>6776</cell><cell>25</cell></row><row><cell>130</cell><cell>10051</cell><cell>10051</cell><cell>0.00</cell><cell>1070</cell><cell>15</cell><cell>0.252</cell><cell>0.030</cell><cell>1005 1</cell><cell>0.00</cell><cell>7552</cell><cell>29</cell></row><row><cell>140</cell><cell>10766</cell><cell>10766</cell><cell>0.00</cell><cell>1215</cell><cell>18</cell><cell>0.249</cell><cell>0.030</cell><cell>10766</cell><cell>0.00</cell><cell>7552</cell><cell>29</cell></row><row><cell>I50</cell><cell>11597</cell><cell>11596</cell><cell>0.00</cell><cell>1272</cell><cell>19</cell><cell>0.238</cell><cell>0.029</cell><cell>11596</cell><cell>0.00</cell><cell>9613</cell><cell>34</cell></row><row><cell>I ) The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>annealing schedule for SA is as follows: iteration ratio</head><label></label><figDesc></figDesc><table /><note><p>= I .2, initial iterations = n , upper bound of iterations = ( n / IO) * n ; cooling ratio = 0.85, initial temperature = 5.0, frozen temperature = 0.1 (with sufficiently hot conditions), and stopping criterion = no better solution obtained.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV RESULTS OF THE SECOND SET OF RANDOMLY GENERATED DATA ON THE SET PARTITIONING PROBLEM BY AG</head><label>IV</label><figDesc></figDesc><table><row><cell>n = 100</cell><cell>Optimal</cell><cell>Final</cell><cell>Gap (percent)</cell><cell>Moves</cell><cell>(Sec)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2747</cell><cell>2747</cell><cell>0.00</cell><cell>90 1</cell><cell>10</cell><cell>9.01</cell><cell>0. I O</cell><cell>0.300</cell><cell>0.032</cell></row><row><cell></cell><cell>3697</cell><cell>3697</cell><cell>0.00</cell><cell>88 1</cell><cell>I I</cell><cell>8.81</cell><cell>0.11</cell><cell>0.279</cell><cell>0.033</cell></row><row><cell></cell><cell>5297</cell><cell>5297</cell><cell>0.00</cell><cell>892</cell><cell>1 1</cell><cell>8.92</cell><cell>0.11</cell><cell>0.299</cell><cell>0.033</cell></row><row><cell></cell><cell>7897</cell><cell>7897</cell><cell>0.00</cell><cell>892</cell><cell>IO</cell><cell>8.92</cell><cell>0. IO</cell><cell>0.299</cell><cell>0.032</cell></row><row><cell></cell><cell>7877</cell><cell>7877</cell><cell>0.00</cell><cell>899</cell><cell>10</cell><cell>8.99</cell><cell>0. I O</cell><cell>0.299</cell><cell>0.032</cell></row><row><cell></cell><cell>10792</cell><cell>10790</cell><cell>0.02</cell><cell>872</cell><cell>10</cell><cell>8.72</cell><cell>0. I O</cell><cell>0.295</cell><cell>0.032</cell></row><row><cell></cell><cell>11377</cell><cell>11377</cell><cell>0.00</cell><cell>899</cell><cell>9</cell><cell>8.99</cell><cell>0.09</cell><cell>0.299</cell><cell>0.030</cell></row><row><cell></cell><cell>354 1</cell><cell>3540</cell><cell>0.03</cell><cell>893</cell><cell>I O</cell><cell>8.93</cell><cell>0.10</cell><cell>0.299</cell><cell>0.032</cell></row><row><cell></cell><cell>8276</cell><cell>8276</cell><cell>0.00</cell><cell>883</cell><cell>IO</cell><cell>8.83</cell><cell>0. IO</cell><cell>0.297</cell><cell>0.032</cell></row><row><cell></cell><cell>13279</cell><cell>13277</cell><cell>0.02</cell><cell>872</cell><cell>IO</cell><cell>8.72</cell><cell>0. IO</cell><cell>0.295</cell><cell>0.032 -</cell></row><row><cell>tively, with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>the cities being regularly placed in a square Euclidean space. This set of problems has the property that the optimal solutions can be easily determined be- cause of the structures of the cities. The problems men- tioned above are the ideal test cases of the properties of any Euclidean TSP algorithm. The initial population con- sists of randomly generated tours. The population size is proportional to the number of cities. Table V shows the final results obtained by AG and by SA in each problem and the error rate of the gap in percentage to the optimal cost. As shown in Fig. 13, among all the test problems the largest error rate in AG is 3.1 percent, while in SA it</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>8 -</cell></row><row><cell cols="2">h 5 6 -</cell></row><row><cell>0</cell><cell>-</cell></row></table><note><p><p>4</p>c.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE V RESULTS OF THE REGULAR-GRID TSPs BY AG A N D BY SA AG SA</head><label>V</label><figDesc></figDesc><table><row><cell>City</cell><cell>Optimal Cost</cell><cell>Final Cost</cell><cell>Gap (percent)</cell><cell>Time (s)</cell><cell>Final Cost</cell><cell>Gap (percent)</cell><cell>Time (s)</cell></row><row><cell>16</cell><cell>16.00</cell><cell>16.00</cell><cell>0</cell><cell>8</cell><cell>16.00</cell><cell>0</cell><cell>36</cell></row><row><cell>25</cell><cell>25.41</cell><cell>25.4 1</cell><cell>0</cell><cell>19</cell><cell>26.24</cell><cell>3.3</cell><cell>62</cell></row><row><cell>36</cell><cell>26.00</cell><cell>36.00</cell><cell>0</cell><cell>39</cell><cell>37.66</cell><cell>4.6</cell><cell>101</cell></row><row><cell>49</cell><cell>49.41</cell><cell>49.41</cell><cell>0</cell><cell>66</cell><cell>51.07</cell><cell>3.4</cell><cell>I70</cell></row><row><cell>64</cell><cell>64.00</cell><cell>64.83</cell><cell>1.3</cell><cell>128</cell><cell>67.01</cell><cell>4.8</cell><cell>283</cell></row><row><cell>81</cell><cell>81.41</cell><cell>83.90</cell><cell>3.1</cell><cell>212</cell><cell>84.89</cell><cell>4.3</cell><cell>397</cell></row><row><cell>100</cell><cell>100.0</cell><cell>102.54</cell><cell>2.5</cell><cell>394</cell><cell>108.21</cell><cell>8.2</cell><cell>596</cell></row><row><cell>121</cell><cell>121.41</cell><cell>126.38</cell><cell>3.1</cell><cell>568</cell><cell>130.21</cell><cell>7.3</cell><cell>846</cell></row><row><cell>144</cell><cell>144.00</cell><cell>147.31</cell><cell>2.3</cell><cell>792</cell><cell>154.45</cell><cell>7.2</cell><cell>1233</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>ANALYSIS OF AG ON T H E REGULAR-GRID TSP's</figDesc><table><row><cell>City</cell><cell>Initial Cost</cell><cell>Starting Cost</cell><cell>Moves</cell><cell>G / C i t y</cell><cell>Time (a)</cell><cell>&amp;/City</cell></row><row><cell>16</cell><cell>26.06</cell><cell>23.77</cell><cell>976</cell><cell>I .85</cell><cell>8</cell><cell>0.177</cell></row><row><cell>25</cell><cell>65.98</cell><cell>39.42</cell><cell>2275</cell><cell>1.91</cell><cell>19</cell><cell>0. I74</cell></row><row><cell>36</cell><cell>112.67</cell><cell>73.38</cell><cell>4698</cell><cell>I .90</cell><cell>39</cell><cell>0.173</cell></row><row><cell>49</cell><cell>168.78</cell><cell>123.58</cell><cell>8465</cell><cell>1.88</cell><cell>66</cell><cell>0. I66</cell></row><row><cell>64</cell><cell>262.56</cell><cell>177.18</cell><cell>17125</cell><cell>2.04</cell><cell>128</cell><cell>0.177</cell></row><row><cell>81</cell><cell>35 I .44</cell><cell>309.03</cell><cell>3485 I</cell><cell>2.30</cell><cell>212</cell><cell>0.180</cell></row><row><cell>100</cell><cell>507.73</cell><cell>414.84</cell><cell>63926</cell><cell>2.53</cell><cell>3 94</cell><cell>0.198</cell></row><row><cell>121</cell><cell>690.50</cell><cell>533.54</cell><cell>91637</cell><cell>2.50</cell><cell>568</cell><cell>0.197</cell></row><row><cell>I44</cell><cell>905.65</cell><cell>716.05</cell><cell>122344</cell><cell>2.42</cell><cell>792</cell><cell>0.195</cell></row><row><cell></cell><cell></cell><cell cols="2">m e s l c i t y -column in Table VI.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>--"-me</cell><cell cols="2">/city -column in Table VI.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VI1 PERFORMANCE ANALYSIS OF SA A N D AG ON THE THREE TSP'S FROM [24]</head><label>VI1</label><figDesc></figDesc><table><row><cell>~~</cell><cell>~</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SA</cell><cell></cell><cell></cell><cell>AG</cell><cell></cell></row><row><cell>Source</cell><cell>Cities</cell><cell>Best-Known</cell><cell>Final</cell><cell>Gap ( % )</cell><cell>Moves</cell><cell>Final</cell><cell>Gap ( % )</cell><cell>Moves</cell></row><row><cell>Oliver</cell><cell>30</cell><cell>42 I</cell><cell>424</cell><cell>1</cell><cell>24617</cell><cell>420"</cell><cell>0</cell><cell>12620</cell></row><row><cell>Eilon</cell><cell>50</cell><cell>428</cell><cell>44 3</cell><cell>3 7</cell><cell>68512</cell><cell>436</cell><cell>1 9</cell><cell>281 1 I</cell></row><row><cell>Eilon</cell><cell>75</cell><cell>545</cell><cell>5 80</cell><cell>6 4</cell><cell>173250</cell><cell>56 1</cell><cell>3</cell><cell>95506</cell></row><row><cell>~~</cell><cell>~</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* The</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>best-known solution obtained by AG.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VI11 TEST DATA WITH n = 6 AND t?l = 10</head><label>VI11</label><figDesc></figDesc><table><row><cell>1</cell><cell>100</cell><cell>8</cell><cell>8</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>0</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>2</cell><cell>600</cell><cell>12</cell><cell>12</cell><cell>6</cell><cell>10</cell><cell>13</cell><cell>13</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>2</cell></row><row><cell>3</cell><cell>1200</cell><cell>13</cell><cell>13</cell><cell>4</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>0</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>4</cell><cell>2400</cell><cell>64</cell><cell>75</cell><cell>18</cell><cell>32</cell><cell>42</cell><cell>48</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>5</cell><cell>500</cell><cell>22</cell><cell>22</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>6</cell><cell>2000</cell><cell>41</cell><cell>41</cell><cell>4</cell><cell>12</cell><cell>20</cell><cell>20</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>4</cell></row><row><cell></cell><cell>b, =</cell><cell>80</cell><cell>96</cell><cell>20</cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE IX THE HIGH-PERFORMANCE SCHEMA STRUCTURES I N THE EXAMPLE Schemata Cost Value</head><label>IX</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the referees for providing many useful comments and suggestions on the previous draft. The authors also thank the anonymous referee for bringing the paper by Davis and Ritter [3]  to their attention.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feng-Tse Lin was born in Taipei, Taiwan, on September 26, 1951. He received the B.S. degree in computer science from Tamkang University, Tamsui, Taiwan, in 1975, the M.S. degree in computer engineering from National Chiao Tung University, Hsichu, Taiwan, in 1984, and the Ph.D. degree in computer science and information engineering from National Taiwan University, Taipei, Taiwan, in 1991.</p><p>From 1984 to 1987, he was an Assistant Researcher of Computer Technology at Telecommunication Laboratories, Chung Li, Taiwan. Currently, he is an Associate Professor with the Department of Applied Mathematics, Chinese Culture University, Yangminshan, Taipei, Taiwan. His current research interests include genetic algorithms, simulated annealing, and combinatorial optimization problems.</p><p>Dr. Lin is a member of the IEEE Computer Society and the Association for Computing Machinery.</p><p>Cheng-Yan Kao was born in Taipei, Taiwan, on October 3, 1948. He received the B.S. degree in mathematics from National Taiwan University, Taipei, Taiwan, in 1971, and the M.S. degree in computer science in 1976, the M.S. degree in statistics in 1978, and the Ph.D. degree in computer science in 1981, all from the University of Wisconsin-Madison.</p><p>He has previously been with Ford Aerospace, the Unisys Corporation, and was with General Electric from 1980 to 1989 at the Johnson Space Center, NASA. Houston, TX. He has been a professor with the Department of Computer Science and Information Engineering, National Taiwan University since 1989. He has published more than 20 technical papers in various journals and conference records. His research interests include genetic algorithms, simulated annealing, scheduling theory, mathematical pro- gramming, artificial intelligence, and software engineering.</p><p>Dr. Kao is a member of ORSA and the AAAI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ching-Chi</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Simulated Annealing and Boltzmann Machines-A Stochastic Approach to Combinatorial Optimization and Neural Computing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Aarts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Korst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Genetic Algorithms and Simulated Annealing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>Los Altos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Schedule optimization with probabilistic search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Con5 Art@c. Intellig. Applicat., 1987, M. Garey and D. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
		<meeting>3rd IEEE Con5 Art@c. Intellig. Applicat., 1987, M. Garey and D. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>W. H. Freeman &amp; Company</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analy. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms: In Search, Optimization and Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate traveling salesman problem</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Golden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operation Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="694" to="711" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artijcial Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Univ. Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient general cooling schedule for simulated annealing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Con5 Computer-Aided Design</title>
		<meeting>IEEE Int. Con5 Computer-Aided Design<address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using genetic algorithms to solve NP-complete problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Spears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Genetic Algorithms</title>
		<meeting>3rd Int. Conf. Genetic Algorithms</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="124" to="132" />
		</imprint>
		<respStmt>
			<orgName>George Mason University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gellatt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P. J . M. van Laarhoven and E. H. L. Aarts, Simulated Annealing: Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Task assignment problems in distributed computing systems by simulated annealing</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Chinese Instit. Engineers</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="537" to="550" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient annealing schedule for multiconstraint zero-one knapsack problem</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Comput. Symp</title>
		<imprint>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="1990">1990. 1990</date>
			<pubPlace>Hsinchu, Taiwan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A stochastic approach for the one-dimensional bin-packing problem</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 IEEE</title>
		<meeting>1992 IEEE</meeting>
		<imprint/>
	</monogr>
	<note>Syst</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cybern</forename><surname>Man</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">Oct. 18-21, 1992</date>
			<pubPlace>Chicago, Illinois</pubPlace>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using the concepts of genetic algorithms to design the annealing schedule for solving the task scheduling problem</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Symp. Artijic. Intell</title>
		<meeting>5th Int. Symp. Artijic. Intell<address><addrLine>Cancun, Mexico</addrLine></address></meeting>
		<imprint>
			<date>Dec</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An effective heuristic algorithm for the traveling salesman problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Kemighan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operation Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="498" to="516" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Linear and Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational experience with variants of the Balas algorithm applied to the selection of R &amp; D projects</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Peterson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="736" to="750" />
			<date type="published" when="1967">1982. 1967</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>Combinatorial Optimization: Algorithms and Complexity</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The time complexity of maximum matching by simulated annealing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Assoc. Comput. Mach</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An approach to linear programming with 0-1 variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Senju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Toyoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="231" to="236" />
			<date type="published" when="1968">1968. 1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Holland</forename><surname>Dordrecht</surname></persName>
		</author>
		<editor>D. Reidel</editor>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Concepts of scale in simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Inr. Conf. Compur. Design, Port Chester</title>
		<meeting>IEEE Inr. Conf. Compur. Design, Port Chester</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="645" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scheduling problems and traveling salesman: The genetic edge recombination operator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starkweather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fuquay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Con$ Generic Algorirhms</title>
		<meeting>3rd Int. Con$ Generic Algorirhms</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="133" to="140" />
		</imprint>
		<respStmt>
			<orgName>George Mason University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
