<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphSMOTE: Imbalanced Node Classification on Graphs with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-16">16 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianxiang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiang Zhang</orgName>
								<address>
									<settlement>Suhang Wang</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science and Technology</orgName>
								<orgName type="institution">Penn State University State College</orgName>
								<address>
									<country>The USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphSMOTE: Imbalanced Node Classification on Graphs with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-16">16 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3437963.3441720</idno>
					<idno type="arXiv">arXiv:2103.08826v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Node classification is an important research topic in graph learning. Graph neural networks (GNNs) have achieved state-of-the-art performance of node classification. However, existing GNNs address the problem where node samples for different classes are balanced; while for many real-world scenarios, some classes may have much fewer instances than others. Directly training a GNN classifier in this case would under-represent samples from those minority classes and result in sub-optimal performance. Therefore, it is very important to develop GNNs for imbalanced node classification. However, the work on this is rather limited. Hence, we seek to extend previous imbalanced learning techniques for i.i.d data to the imbalanced node classification task to facilitate GNN classifiers. In particular, we choose to adopt synthetic minority oversampling algorithms, as they are found to be the most effective and stable. This task is non-trivial, as previous synthetic minority oversampling algorithms fail to provide relation information for newly synthesized samples, which is vital for learning on graphs. Moreover, node attributes are high-dimensional. Directly over-sampling in the original input domain could generates out-of-domain samples, which may impair the accuracy of the classifier. We propose a novel framework, GraphSMOTE, in which an embedding space is constructed to encode the similarity among the nodes. New samples are synthesize in this space to assure genuineness. In addition, an edge generator is trained simultaneously to model the relation information, and provide it for those new samples. This framework is general and can be easily extended into different variations. The proposed framework is evaluated using three different datasets, and it outperforms all baselines with a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real user Bot (a) Bot detection task</head><p>Real user Bot Generated (b) After over-sampling Figure <ref type="figure">1</ref>: An example of bot detection on a social network, and the idea of over-sampling. Note that the over-sampling is in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed great improvements in learning from graphs with the developments of graph neural networks(GNNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>. One typical task is semi-supervised node classification <ref type="bibr" target="#b38">[39]</ref>, where we have a large graph with a small ratio of nodes labeled. A classifier can be trained on those supervised nodes, and be used to classify other nodes during testing. GNNs have obtained state-ofthe-art performance in this task, and is developing rapidly. For example, GCN <ref type="bibr" target="#b18">[19]</ref> exploits features in the spectral domain efficiently by using a simplified first-order approximation; GraphSage <ref type="bibr" target="#b13">[14]</ref> utilizes features in the spatial domain and is better at adapting to diverse graph topology. Despite all these progresses, existing work mainly focus on the setting that node classes are balanced.</p><p>In many real-world applications, node classes could be imbalanced in graphs, i.e., some classes have significantly fewer samples for training than other classes. For example, for fake account detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b41">42]</ref>, the majority of users in a social network platform are benign users while only a small portion of them are bots. Similarly, topic classification for website pages <ref type="bibr" target="#b36">[37]</ref> could also suffer from this problem, as the materials for some topics are scarce, comparing to those on-trend topics. Thus, we are often faced with imbalanced node classification problem. An example of the imbalanced node classification problem is shown in Figure <ref type="figure">1(a)</ref>. Each blue node refers to a real user, each red node refers to a fake user, and the edges denote the friendship. The task is to predict whether those unlabeled users in dashes are real or fake. The classes are imbalanced in nature, as fake users are often less than 1% of all the users. The semi-supervised setting further magnifies the class imbalanced issue as we are only given limited labeled data, which makes the number of labeled minority samples extremely small.</p><p>The imbalanced node classification brings challenges to existing GNNs because the majority classes could dominate the loss function of GNNs, which makes the trained GNNs over-classify those majority classes and become unable to predict accurately for samples from minority classes. This issue impedes the adoption of GNNs for many real-world applications with imbalanced class distribution such as malicious account detection. Therefore, it is important to develop GNNs for class imbalanced node classification.</p><p>In machine learning domain, traditional class imbalance problem has been extensively studied. Algorithms can be summarized into three groups: data-level approaches, algorithm-level approaches, and hybrid approaches. Data-level approaches seek to make the class distribution more balanced, using over-sampling or downsampling techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>; algorithm-level approaches typically introduce different mis-classification penalties or prior probabilities for different classes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>; and hybrid approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> combine both of them. However, directly applying them to graphs may get sub-optimal results. Relation is the key information needed to be exploited in graph data, and under-representation of minority samples would impair not only their embedding quality, but also the knowledge exchange processes across neighboring nodes. Previous algorithms fail to address that due to their i.i.d assumption, taking each sample as independent.</p><p>Therefore, in this work, we study a novel problem of exploring synthetic minority oversampling for imbalanced node classification with GNNs<ref type="foot" target="#foot_0">1</ref> . The idea is shown in Figure <ref type="figure">1</ref>(b). Previous algorithms are not readily applicable to graphs, due to two-folded reasons. First, it is difficult to generate relation information for synthesized new samples. Mainstream oversampling techniques <ref type="bibr" target="#b25">[26]</ref> use interpolation between target example and its nearest neighbor to generate new training examples. However, interpolation is improper for edges, as they are usually discreet and sparse. Interpolation could break down the topology structure. Second, synthesized new samples could be of low quality. Node attributes are high-dimensional, and directly interpolating on them would easily generate out-ofdomain examples, which are not beneficial for training the classifier.</p><p>Targeting at these two problems, we extend previous over-sampling algorithms to a new framework, GraphSMOTE, in order to cope with graphs. The modifications are mainly at two places. First, we propose to obtain new edges between generated samples and existing samples with an edge predictor. This predictor can learn the genuine distribution of edges, and hence can be used to produce reliable relation information among samples. Second, we propose to perform interpolation at the intermediate embedding space of a GNN network, inspired by <ref type="bibr" target="#b0">[1]</ref>. In this intermediate embedding space, the dimensionality is much lower, and the distribution of samples from the same class would be more dense. As intra-class similarity as well as inter-class differences would have been captured by previous layers, interpolation can be better trusted to generate in-domain samples. Concretely, we propose a new framework in which graph auto-encoding task and node classification task are combined together. These two tasks share the same feature extractor, and oversampling is performed at the output space of that module, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The main contributions are:</p><p>• We propose to study a novel problem, node class imbalance problem for learning on graphs. It has many real-world applications, and this paper is the first work focusing on this task as far as we know.</p><p>• We design a new framework which extends previous oversampling algorithms to work for graph data. It addresses the deficiencies of previous methods, by generating more natural nodes as well as relation information. Besides, it is general and easy to extend. • Experiments are performed on three datasets, and GraphSMOTE outperforms all baselines with a large gap. Extensive analysis of our model's behavior as well as recommended settings are also presented.</p><p>The rest of the paper are organized as follows. In Sec. 2, we review related work. In Sec. 3, we formally define the problem. In Sec. 4, we give the details of GraphSMOTE. In Sec. 5, we conduct experiments to evaluate the effectiveness of GraphSMOTE. In Sec. 6, we conclude with future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review related works, which include graph neural networks and class imbalance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Class Imbalance Problem</head><p>Class imbalance is common in real-world applications, and has long been a classical research direction in the machine learning domain. Plenty of tasks suffer from this problem, like medical diagnosis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref> or fine-grained image classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>. Classes with larger number of instances are usually called as majority classes, and those with fewer instances are usually called as minority classes. The countermeasures against this problem can generally be classified into three groups, i.e., algorithm-level, data-level and hybrid.</p><p>The first group of methods are data-level, seeking to directly adjust class sizes through over-or under-sampling. The vanilla form of over-sampling is replicating existing samples. It reduces this imbalance, but can lead to over-fitting as no extra information is introduced. SMOTE <ref type="bibr" target="#b7">[8]</ref> addresses this problem by generating new samples, performing interpolation between samples in minority classes and their nearest neighbors. SMOTE is the most popular over-sampling approach, and many extensions are proposed on top of it to make the interpolation process more effective. For example, Borderline-SMOTE <ref type="bibr" target="#b14">[15]</ref> limits over-sampling to samples near the borderline of classes, which are believed to be more informative. Safe-Level-SMOTE <ref type="bibr" target="#b6">[7]</ref> computes the safe level for each interpolation direction using majority class neighbors, in order to make the generated new samples safer. Cluster-based Over-sampling <ref type="bibr" target="#b16">[17]</ref> first clusters samples into different groups, than over-samples each group separately, considering that small districts often exist in the input space. Besides, <ref type="bibr" target="#b0">[1]</ref> extends over-sampling to work with CNNs, through interpolation in an embedding space. Under-sampling discards some samples from majority classes, which can also make classes balanced, but at the price of losing some information. To overcome this deficiency, many extensions are proposed to remove only redundant samples, like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. The second group of methods are algorithm-level. Cost sensitive learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b43">44]</ref> generally constructs a cost matrix to assign different mis-classification penalties for different classes. Its effect is similar to vanilla oversampling. <ref type="bibr" target="#b27">[28]</ref> proposes an approximation to F measurement, which can be directly optimized by gradient propagation. Threshold moving <ref type="bibr" target="#b20">[21]</ref> modifies the inference process after the classifier is trained, by introducing a prior probability for each class. Through these approaches, the importance of minority classes can be increased. The last group are hybrid approaches, which combine multiple algorithms from one or both aforementioned categories. <ref type="bibr" target="#b22">[23]</ref> uses a group of classifiers, each one is trained on a subset of majority classes and minority classes. <ref type="bibr" target="#b8">[9]</ref> combines boosting with SMOTE approach, and <ref type="bibr" target="#b15">[16]</ref> combines over-sampling with cost sensitive learning. <ref type="bibr" target="#b32">[33]</ref> introduces three cost-sensitive boosting approaches, which iteratively updates the impact of each class in together with the AdaBoost parameters. Some systematic analysis of them have found that synthetic minority oversampling techniques such as SMOTE are the most popular and effective approaches for addressing class imbalance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>. However, existing work are overwhelmingly dedicated to i.i.d data. They cannot be directly applied to graph structured data because: (i) the synthetic node generation on the raw feature space cannot take the graph information into consideration; and (ii) the generated nodes doesn't have links with the graph, which cannot facilitate the graph based classifier such as GNNs. Hence, in this work, we focus on extending SMOTE into graph domain for GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Network</head><p>In recent years, with the increasing requirements of learning on non-Euclidean space and modeling rich relation information among samples, graph neural networks (GNNs) have received much more attention and are developing rapidly. GNNs generalize convolutional neural networks to graph structured data and have shown great ability in modeling graph structured data. Current GNNs follow a message-passing framework, which is composed of pattern extraction and interaction modeling within each layer <ref type="bibr" target="#b11">[12]</ref>. Generally, existing GNN frameworks can be categorized into two categorizes, i.e., spectral-based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref> and spatial-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Spectral-based GNNs defines the convolution operation in the Fourier domain by computing the eigendecomposition of the graph Laplacian. Early work <ref type="bibr" target="#b4">[5]</ref> in this domain involves extensive computation, and is time-consuming. To accelerate, <ref type="bibr" target="#b34">[35]</ref> adopts Chebyshev Polynomials to approximate spectral kernels, and enforces locality constraints by truncating only top-k terms. GCN <ref type="bibr" target="#b18">[19]</ref> takes a further step by preserving only top-2 terms, and obtains a more simplified form. GCN is one of the most widely-used GNN currently. However, all spectral-based GNNs suffer from the generalization problem, as they are dependent on the Laplacian eigenbasis <ref type="bibr" target="#b42">[43]</ref>. Hence, they are usually applied in the transductive setting, training and testing on the same graph structure. Spatial-based GNNs are more flexible and have stronger in generalization ability. They implement convolutions basing on the neighborhoods of each node. As each node could have different number of neighbors, Duvenaud et al., <ref type="bibr" target="#b9">[10]</ref> uses multiple weight matrices, one for each degree. <ref type="bibr" target="#b1">[2]</ref> proposes a diffusion convolution neural network, and <ref type="bibr" target="#b26">[27]</ref> adopts a fixed number of neighbors for each sample. A more popular model is GraphSage <ref type="bibr" target="#b13">[14]</ref>, which samples and aggregates embedding from local neighbors of each sample. More recently, <ref type="bibr" target="#b37">[38]</ref> extends expressive power of GNNs to that of WL test, and <ref type="bibr" target="#b39">[40]</ref> introduce a new GNN layer that can encode node positions.</p><p>Despite the success of various GNNs, existing work doesn't consider the class imbalance problem, which widely exists in realworld applications and could significantly reduce the performance of GNNs. Thus, we study a novel problem of synthetic minority oversampling on graphs to facilitate the adoption of GNNs for class imbalance node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In this work, we focus on semi-supervised node classification task on graphs, in the transductive setting. As shown in Figure <ref type="figure">1</ref>, we have a large network of entities, with some labeled for training. Both training and testing are performed on this same graph. Each entity belongs to one class, and the distribution of class sizes are imbalanced. This problem has many practical applications. For example, the homophily in social networks which results in the under-representation of minority groups, malicious behavior or fake user accounts on social networks which are outnumbered by normal ones, and linked web pages in knowledge base where materials for some topics are limited.</p><p>Throughout this paper, we use G = {V, A, F} to denote an attributed network, where V = {𝑣 1 , . . . , 𝑣 𝑛 } is a set of 𝑛 nodes. A ∈ R 𝑛×𝑛 is the adjacency matrix of G, and F ∈ R 𝑛×𝑑 denotes the node attribute matrix, where F[ 𝑗, :] ∈ R 1×𝑑 is the node attributes of node j and 𝑑 is the dimension of the node attributes. Y ∈ R 𝑛 is the class information for nodes in G. During training, only a subset of Y, Y 𝐿 , is available, containing the labels for node subset V 𝐿 . There are 𝑚 classes in total, {C 1 , . . . , C 𝑚 }. |C 𝑖 | is the size of 𝑖-th class, referring to the number of samples belong to that class. We use imbalance ratio, ) , to measure the extent of class imbalance. In the imbalanced setting, imbalance ratio of Y 𝐿 is small.</p><formula xml:id="formula_0">𝑚𝑖𝑛 𝑖 ( | C 𝑖 |) 𝑚𝑎𝑥 𝑖 ( | C 𝑖 |</formula><p>Given G whose node class set is imbalanced, and labels for a subset of nodes V 𝐿 , we aim to learn a node classifier 𝑓 that can work well for both majority and minority classes, i.e.,</p><formula xml:id="formula_1">𝑓 (V, A, F) → Y (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we give the details of the proposed framework GraphSMOTE. The main idea of GraphSMOTE is to generate synthetic minority nodes through interpolation in an expressive embedding space acquired by the GNN-based feature extractor, and use an edge generator to predict the links for the synthetic nodes, which forms an augmented balanced graph to facilitate node classification by GNNs. An illustration of the proposed framework is shown in Figure <ref type="figure" target="#fig_0">2</ref>. GraphSMOTE is composed of four components: (i) a GNNbased feature extractor (encoder) which learns node representation that preserves node attributes and graph topology to facilitate the synthetic node generation; (ii) a synthetic node generator which generates synthetic minority nodes in the latent space; (iii) an edge generator which generate links for the synthetic nodes to from an augmented graph with balanced classes; and (iv) a GNN-based classifier which performs node classification based on the augmented graph. Next, we give the details of each component. One way to generate synthetic minority nodes is to directly apply SMOTE on the raw node feature space. However, this will cause several problems: (i) the raw feature space could be sparse and highdimensional, which makes it difficult to find two similar nodes of the same class for interpolation; and (ii) it doesn't consider the graph structure, which can result in sub-optimal synthetic nodes. Thus, instead of directly do synthetic minority over-sampling in the raw feature space, we introduce an feature extractor learn node representations that can simultaneously capture node properties and graph topology. Generally, the node representations should reflect inter-class and intra-class relations of samples. Similar samples should be closer to each other, and dissimilar samples should be more distant. In this way, when performing interpolation on minority node with its nearest neighbor, the obtained embedding would have a higher probability of representing a new sample belonging to the same minority class. In graphs, the similarity of nodes need to consider node attributes, node labels, as well as local graph structures. Hence, we implement it with GNN, and train it on two down-stream tasks, edge prediction and node classification.</p><formula xml:id="formula_2">GN N De co der A F … … Embedding SMOTE A' GNN … … h(v) h(nn(v)) h(v')</formula><p>The feature extractor can be implemented using any kind of GNNs. In this work, we choose GraphSage as the backbone model structure because it is effective in learning from various types of local topology, and generalizes well to new structures. It has been observed that too deep GNNs often lead to sub-optimal performance, as a result of over-smoothing and over-fitting. Therefore, we adopt only one GraphSage block as the feature extractor. Inside this block, the message passing and fusing process can be written as:</p><formula xml:id="formula_3">h 1 𝑣 = 𝜎 (W 1 • 𝐶𝑂𝑁𝐶𝐴𝑇 (F[𝑣, :], F • A[:, 𝑣])),<label>(2)</label></formula><p>F represents input node attribute matrix and F[𝑣, :] represents attribute for node 𝑣. A[:, 𝑣] is the 𝑣-th column in adjacency matrix, and h 1 𝑣 is the obtained embedding for node 𝑣. W 1 is the weight parameter, and 𝜎 refers to the activation function such as ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Synthetic Node Generation</head><p>After obtaining the representation of each node in the embedding space constructed by the feature extractor, now we can perform over-sampling on top of that. We seek to generate the expected representations for new samples from the minority classes. In this work, to perform over-sampling, we adopt the widely used SMOTE algorithm, which augments vanilla over-sampling via changing repetition to interpolation. We choose it due to its popularity, but our framework can also cope with other over-sampling approaches as well. The basic idea of SMOTE is to perform interpolation on samples from the target minority class with their nearest neighbors in the embedding space that belong to the same class. Let h 1 𝑣 be a labeled minority nodes with label as 𝑌 𝑣 . The first step is to finds the closest labeled node of the same class as h 1 𝑣 , i.e.,</p><formula xml:id="formula_4">𝑛𝑛(𝑣) = argmin 𝑢 ∥h 1 𝑢 − h 1 𝑣 ∥, s.t. Y 𝑢 = Y 𝑣<label>(3)</label></formula><p>𝑛𝑛(𝑣) refers to the nearest neighbor of 𝑣 from the same class, measured using Euclidean distance in the embedding space. With the nearest neighbor, we can generate synthetic nodes as</p><formula xml:id="formula_5">h 1 𝑣 ′ = (1 − 𝛿) • h 1 𝑣 + 𝛿 • h 1 𝑛𝑛 (𝑣) ,<label>(4)</label></formula><p>where 𝛿 is a random variable, following uniform distribution in the range [0, 1]. Since h 1 𝑣 and h 1 𝑛𝑛 (𝑣) belong to the same class and are very close to each other, the generated synthetic node h 1 𝑣 ′ should also belong to the same class. In this way, we can obtain labeled synthetic nodes.</p><p>For each minority class, we can apply SMOTE to generate syntetic nodes. We use a hyper-parameter, over-sampling scale, to control the amount of samples to be generated for each class. Through this generation process, we can make the distribution of class size more balanced, and hence make the trained classifier perform better on those initially under-represented classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Edge Generator</head><p>Now we have generated synthetic nodes to balance the class distribution. However, these nodes are isolated from the raw graph G as they don't have links. Thus, we introduce an edge generator to model the existence of edges among nodes. As GNNs need to learn how to extract and propagate features simultaneously, this edge generator can provide relation information for those synthesized samples, and hence facilitate the training of GNN-based classifier. This generator is trained on real nodes and existing edges, and is used to predict neighbor information for those synthetic nodes. These new nodes and edges will be added to the initial adjacency matrix A, and serve as input the the GNN-based classifier.</p><p>In order to maintain model's simplicity and make the analysis easier, we adopt a vanilla design, weighted inner production, to implement this edge generator as:</p><formula xml:id="formula_6">E 𝑣,𝑢 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (𝜎 (h 1 𝑣 • S • h 1 𝑢 )).<label>(5)</label></formula><p>where E 𝑣,𝑢 refers to the predicted relation information between node 𝑣 and 𝑢, and S is the parameter matrix capturing the interaction between nodes. The loss function for training the edge generator is</p><formula xml:id="formula_7">L 𝑒𝑑𝑔𝑒 = ∥E − A∥ 2 𝐹 ,<label>(6)</label></formula><p>where E refers to predicted connections between nodes in V, i.e., no synthetic nodes. Since we learn an edge generator which is good at reconstructing the adjacency matrix using the node representations, it should give good link predictions for synthetic nodes.</p><p>With the edge generator, we attempt two strategies to put the predicted edges for synthetic nodes into the augmented adjacency matrix. In the first strategy, this generator is optimized using only edge reconstruction, and the edges for the synthetic node 𝑣 ′ is generated by setting a threshold 𝜂:</p><formula xml:id="formula_8">Ã[𝑣 ′ , 𝑢] = 1, if E 𝑣 ′ ,𝑢 &gt; 𝜂 0, otherwise. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where Ã is the adjacency matrix after over-sampling, by inserting new nodes and edges into A, and will be sent to the classifier.</p><p>In the second strategy, for synthetic node 𝑣 ′ , we use soft edges instead of binary ones:</p><formula xml:id="formula_10">Ã[𝑣 ′ , 𝑢] = E 𝑣 ′ ,𝑢 ,<label>(8)</label></formula><p>In this case, gradient on Ã can be propagated from the classifier, and hence the generator can be optimized using both edge prediction loss and node classification loss, which will be introduced later.</p><p>Both two strategies are implemented, and their performance are compared in the experiment part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GNN Classifier</head><p>Let H1 be the augmented node representation set by concatenating H 1 (embedding of real nodes) with the embedding of the synthetics nodes, and Ṽ𝐿 be the augmented labeled set by incorporating the synthetic nodes into V 𝐿 . Now we have an augmented graph G = { Ã, H} with labeled node set Ṽ𝐿 . The data size of different classes in G becomes balanced, and an unbiased GNN classifier would be able to be trained on that. Specifically, we adopt another GraphSage block, appended by a linear layer for node classificaiton on G as:</p><formula xml:id="formula_11">h 2 𝑣 = 𝜎 (W 2 • 𝐶𝑂𝑁𝐶𝐴𝑇 (h 1 𝑣 , H1 • Ã[:, 𝑣])),<label>(9)</label></formula><formula xml:id="formula_12">P 𝑣 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (𝜎 (W 𝑐 • 𝐶𝑂𝑁𝐶𝐴𝑇 (h 2 𝑣 , H 2 • Ã[:, 𝑣]))),<label>(10)</label></formula><p>where H 2 represents node representation matrix of the 2nd Graph-Sage block, and W refers to the weight parameters. P 𝑣 is the probability distribution on class labels for node 𝑣. The classifier module is optimized using cross-entropy loss as:</p><formula xml:id="formula_13">L 𝑛𝑜𝑑𝑒 = ∑︁ 𝑢 ∈ Ṽ𝐿 ∑︁ 𝑐 (1(𝑌 𝑢 == 𝑐) • 𝑙𝑜𝑔(P 𝑣 [𝑐]).<label>(11)</label></formula><p>And during testing, the predicted class for node 𝑣, Y ′ 𝑣 will be set as the class with highest probability,</p><formula xml:id="formula_14">Y ′ 𝑣 = argmax 𝑐 P 𝑣,𝑐<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Optimization Objective</head><p>Putting the feature extractor, synthetic node generator, edge generator and GNN classifier together, previous parts together, the final objective function of GraphSMOTE can be written as:</p><formula xml:id="formula_15">min 𝜃,𝜙,𝜑 L 𝑛𝑜𝑑𝑒 + 𝜆 • L 𝑒𝑑𝑔𝑒 ,<label>(13)</label></formula><p>wherein 𝜃, 𝜙, 𝜑 are the parameters for feature extractor, edge generator, and node classifier respectively. As the model's performance is dependent on the quality of embedding space and generated edges, to make training phrase more stable, we also tried pre-training feature extractor and edge generator using L 𝑒𝑑𝑔𝑒 . The design of GraphSMOTE has several advantages: (i) it is easy to implement synthetic minority over-sampling process. Through uniting interpolated node embedding and predicted edges, new samples can be generated; (ii) the feature extractor is optimized using training signal from both node classification task and edge prediction task. Therefore, rich intra-class and inter-class relation information would be encoded in the embedding space, making the interpolation more robust; and (iii) it is a general framework. It can cope with different structure choices for each component, and different regularization terms can be enforced to provide prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Training Algorithm</head><p>The full pipeline of running our framework can be summarized in Algorithm 1. Inside each optimization step, we first obtain node representations using the feature extractor in line 6. Then, from line 7 to line 11, we perform over-sampling in the embedding space to make node classes balanced. After predicting edges for generated new samples in line 12, the following node classifier can be trained on top of that over-sampled graph. The full framework is trained altogether with edge prediction loss and node classification loss, as shown in line 13. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Full Training Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct experiments to evaluate the benefits of GraphSMOTE for the node classification task when classes are imbalanced. Both artificial and genuine imbalanced datasets are used, and different configurations are adopted to test its generalization ability. Particularly, we want to answer the following questions:</p><p>• How effective is GraphSMOTE in imbalanced node classification task? • How different choices of over-sampling scales would affect the performance of GraphSMOTE? • Can GraphSMOTE generalize well to different imbalance ratios, or different base model structures?</p><p>We begin by introducing the experimental settings, including datasets, baselines, and evaluation metrics. We then conduct experiments to answer these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>5.1.1 Datasets. We conduct experiments on two widely used publicly available datasets for node classification, Cora <ref type="bibr" target="#b31">[32]</ref> and Blog-Catalog <ref type="bibr" target="#b33">[34]</ref>, and one fake account detection dataset, Twitter <ref type="bibr" target="#b24">[25]</ref>. The details of these three datasets are given as follows:</p><p>•  <ref type="bibr" target="#b24">[25]</ref> with a dedicated API crawler from Twitter<ref type="foot" target="#foot_2">3</ref> on bot infestation problem. It has 5, 384, 160 users in total. Among them, 63, 167 users are bots. In this work, we split a connected sub-graph from it containing 61, 122 genuine users and 2, 045 robots. Node embedding is obtained through Deepwalk, appended with node degrees. This dataset is used for binary classification, and the imbalance ratio is roughly 1 : 30. We randomly select 25% of total samples for training, 25% for validation, and the remaining 50% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines.</head><p>We compare GraphSMOTE with representative and state-of-the-art approaches for handling imbalanced class distribution problem, which includes:</p><p>• Over-sampling: A classical approach for imbalanced learning problem, by repeating samples from minority classes. We implement it in the raw input space, by duplicating 𝑛 𝑠 minority nodes along their edges. In each training iteration, V is oversampled to contain 𝑛 + 𝑛 𝑠 nodes, and A ∈ R (𝑛+𝑛 𝑠 )×(𝑛+𝑛 𝑠 ) . • Re-weight <ref type="bibr" target="#b40">[41]</ref>: This is a cost-sensitive approach which gives class-specific loss weight. In particular, it assigns higher loss weights to samples from minority so as to alleviate the issue of majority classes dominating the loss function.</p><p>• SMOTE <ref type="bibr" target="#b7">[8]</ref>: Synthetic minority oversampling techniques generate synthetic minority samples by interpolating a minority samples and its nearest neighbors of the same class. For newly generated nodes, its edges are set to be the same as the target node. • Embed-SMOTE <ref type="bibr" target="#b0">[1]</ref>: An extension of SMOTE for deep learning scenario, which perform over-sampling in the intermediate embedding layer instead of the input domain. We set it as the output of last GNN layer, so that there is no need to generate edges.</p><p>Basing on the strategy for training edge generator and setting edges, four implementations of GraphSMOTE are tested:</p><p>• GraphSMOTE 𝑇 : The edge generator is trained using loss from only edge prediction task. The predicted edges are set to binary values with a threshold before sending to GNNbased classifier; • GraphSMOTE 𝑂 : Predicted edges are set as continuous so that gradient can be calculated and propagated from GNNbased classifier. The edge generator is trained along with other components with training signals from both edge generation task and node classification task; • GraphSMOTE 𝑝𝑟𝑒𝑇 : An extension of GraphSMOTE 𝑇 , in which the feature extractor and edge generator are pre-trained on the edge prediction task, before fine-tuning on Equation.13. During fine-tuning, edge generator is optimized using only L 𝑒𝑑𝑔𝑒𝑠 ; • GraphSMOTE 𝑝𝑟𝑒𝑂 : An extension of GraphSMOTE 𝑂 , in which a pre-training process is also conducted before fine-tuning, same as GraphSMOTE 𝑝𝑟𝑒𝑇 .</p><p>In the experiments, all these methods are implemented and tested on the same GNN-based network for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Metrics.</head><p>Following existing works in evaluating imbalanced classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref>, we adopt three criteria: classification accuracy(ACC), mean AUC-ROC score <ref type="bibr" target="#b3">[4]</ref>, and mean Fmeasure. ACC is computed on all testing examples at once, therefore may underweight those under-represented classes. AUC-ROC score illustrates the probability that the corrected class is ranked higher than other classes, and F-measure gives the harmonic mean of precision and recall for each class. Both AUC-ROC score and F-measure are calculated separately for each class and then non-weighted average over them, therefore can better reflect the performance on minority classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Configurations.</head><p>All experiments are conducted on a 64-bit machine with Nvidia GPU (Tesla V100, 1246MHz , 16 GB memory), and ADAM optimization algorithm is used to train all the models. For all methods, the learning rate is initialized to 0.001, with weight decay being 5𝑒 − 4. 𝜆 is set as 1𝑒 − 6, since we did not normalize L 𝑒𝑑𝑔𝑒 and it is much larger than L 𝑛𝑜𝑑𝑒 . On Cora dataset, imbalance_ratio is set to 0.5 and over-sampling scale is set as 2.0 if not specified otherwise. For BlogCatalog and Twitter dataset, imbalance_ratio is not involved, and over-sampling scale is set classwise: 𝑛 𝑚 • | C 𝑖 | for minority class 𝑖, to make the class size balanced. Besides, all models are trained until converging, with the maximum training epoch being 5000.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Imbalanced Classification Performance</head><p>To answer the first question, we compare the imbalanced node classification performance of GraphSMOTE with the baselines on aforementioned three datasets. Each experiment is conducted 3 times to alleviate the randomness. The average results with standard deviation are reported in Table <ref type="table" target="#tab_1">1</ref>. From the table, we can make following observations:</p><p>• All four variants of GraphSMOTE showed significant improvements on imbalanced node classification task, compared to the "Origin" setting, in which no special algorithm is adopted. They also outperform almost all baselines in all datasets, on all evaluation metrics. These results validate the effectiveness of proposed framework. • The improvements brought by GraphSMOTE are much larger than directly applying previous over-sampling algorithms. For example, compared with Over-sampling GraphSMOTE 𝑇 shows an improvement of 0.011, 0.003, 0.021 in AUC-ROC score, and an improvement of 0.016, 0.014, 0.016 in AUC-ROC score compared with Embed-SMOTE. This result validates the advantages of GraphSMOTE over previous algorithms, in constructing an embedding space for interpolation and provide relation information. • Among different variants of GraphSMOTE, pre-trained implementations show much stronger performance than not pre-trained ones. This result implies the importance of a better embedding space in which the similarities among samples are well encoded. To summarize, these results prove the advantages of introducing over-sampling algorithm for imbalanced node classification task. They also validate that GraphSMOTE can generate more realistic samples and the importance of providing relation information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Influence of Over-sampling Scale</head><p>In this subsection, we analyze the performance change of different algorithms w.r.t different over-sampling scales, in the pursuit of answering the second question. To conduct experiments in a constrained setting, we use Cora dataset and fix imbalance ratio as 0.5. Over-sampling scale is varied as {0.2, 0.4, 0.6, 0.8, 1.0, 1.2}. Every experiment is conducted 3 times and the average results are presented in Figure <ref type="figure" target="#fig_3">3</ref>. From the figure, we make the following observations:</p><p>• When over-sampling scale is smaller than 0.8, generating more samples for minority classes, i.e., making the classes more balanced, would help the classifier to achieve better performance, which is as expected because these synthetic nodes not only balance the datasets but also introduce new supervision for training a better GNN classifier. • When the over-sampling scale becomes larger, keeping increasing it may result in opposite effects. It can be observed that the performance remains similar, or degrade a little when changing over-sampling scale from 1.0 to 1.2. This is because when too many synthetic nodes are generated, some of these synthetic nodes contain similar/redundant information which cannot further help learn a better GNN. • Based on these observations, generally setting the oversampling scale set a value that can make the class balanced is a good choice, which is consistent with existing work for synthetic minority oversampling <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Influence of Imbalance Ratio</head><p>In this subsection, we analyze the performance of different algorithms with respect to different imbalance ratios, to evaluate their robustness. Experiment is also conducted in a well-constrained setting on Cora, by fixing over-sampling scale to 1.0, and varying imbalance ratio as {0.1, 0.2, 0.4, 0.6}. Each experiments are conducted 3 times and the average results are shown in Table <ref type="table" target="#tab_2">2</ref>. From the table, we make the following observations:</p><p>• The proposed framework GraphSMOTE generalizes well to different imbalance ratios. It achieves the best performance • The improvement of GraphSMOTE is more significant when the imbalance extent is more extreme. For example, when imbalance ratio is 0.1, GraphSMOTE 𝑝𝑟𝑒𝑂 outperforms Reweight by 0.0326, the gap reduces to 0.0060 when the imbalance ratio become 0.6. This is because when the datasets is not that imbalanced, minority oversampling is not that important, which makes the improvement of proposed algorithm over others not that significant. • Pre-training is important when the imbalance ratio is extreme. When imbalance ratio is 0.1, GraphSMOTE 𝑝𝑟𝑒𝑂 shows an improvement of 0.0268 over GraphSMOTE 𝑝𝑟𝑒𝑂 , and the gap reduces to 0.0055 when the imbalance ratio changes to 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Influence of Base Model</head><p>In this subsection, we test generalization ability of the proposed algorithm by applying it to another widely-used graph neural network: GCN. Comparison between it and baselines is presented in Table <ref type="table">3</ref>. All methods are implemented on the same network. Experiments are performed on Cora, with imbalance ratio set as 0.5 and over-sampling scale as 2.0. Experiments are run three times, with both averaged results and standard deviation reported. From the result, it can be observed that:</p><p>• Generally, GraphSMOTE adapt well to GCN-based model. Four variants of it all work well and achieve the best performance, as shown in Table <ref type="table">3</ref>. • Compared with using GraphSage as base model, a main difference is that pre-training seems to be less necessary in this case. We think it may be caused by the fact that GCN is less powerful than GraphSage in representation ability.</p><p>GraphSage is more flexible and can model more complex relation information, and hence is more difficult to train. Therefore, it can benefit more from obtaining a well-trained embedding space in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Parameter Sensitivity Analysis</head><p>In this part, the hyper-parameter 𝜆 is varied to test GraphSMOTE's sensitivity towards it. To keep simplicity, we adopt GraphSMOTE 𝑇 and GraphSMOTE 𝑝𝑟𝑒𝑇 as base model, and set 𝜆 to be in {1𝑒 −7, 1𝑒 −  • Generally, as 𝜆 increases, the performance first increase then decrease. The performance would drop significantly if 𝜆 is too large. Generally, a smaller 𝜆 between 1𝑒 − 6 and 4𝑒 − 6 works better. The reason could be the difference in scale of two losses. • Pre-training makes GraphSMOTE more stable w.r.t 𝜆.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>Class imbalance problem of nodes in graphs widely exists in realworld tasks, like fake user detection, web page classification, malicious machine detection, etc. This problem can significantly influence classifier's performance on those minority classes, but is left unconsidered in previous works. Thus, in this work, we investigate this imbalanced node classification task. Specifically, we propose a novel framework GraphSMOTE, which extends previous oversampling algorithms for i.i.d data to this graph setting. Concretely, GraphSMOTE constructs an intermediate embedding space with a feature extractor, and train an edge generator and a GNN-based node classifier simultaneously on top of that. Experiments on one artificial dataset and two real-world datasets demonstrated its effectiveness, outperforming all other baselines with a large margin. Ablation studies are performed to understand GraphSMOTE performs under various scenarios. Parameter sensitivity analysis is also conducted to understand the sensitivity of GraphSMOTE on the hyperparameters.</p><p>There are several interesting directions need further investigation. First, besides node classification, other tasks like edge type prediction or node representation learning may also suffer from under-representation of nodes in minority classes. And sometimes, node class might not be provided explicitly. Therefore, we will also extend GraphSMOTE for handling other types of imbalanced learning problems on graphs. Second, in this paper, we mainly conduct experiments on citation network and social media network. There are many other real-world applications which can be treated as imbalanced node classification problems. Therefore, we would like to extend our framework for more application domains such as document analysis in the websites.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 : 3 : 7 : 9 :</head><label>1379</label><figDesc>Input: G = {V, A,F, Y} Output: Predicted node class Y ′ Randomly initialize the feature extractor, edge generator and node classifier; 2: if Require pre-train then Fix other parts, train the feature extractor and edge generator module until convergence, based on loss L 𝑒𝑑𝑔𝑒 ; 4: end if 5: while Not Converged do 6: Input G to feature extractor, obtaining H 1 ; for class c in minority classes do 8: for i in 𝑠𝑖𝑧𝑒 (𝑐) • over-sampling scale do Generate a new sample in class c, Following Equation (the model using L 𝑛𝑜𝑑𝑒 + 𝜆 • L 𝑒𝑑𝑔𝑒 ; 14: end while 15: return Trained feature extractor, edge predictor, and node classifier module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.001 0.914±0.002 0.684±0.003 0.210±0.004 0.586±0.002 0.074±0.002 0.967±0.004 0.577±0.003 0.494±0.001 over-sampling 0.692±0.009 0.918±0.005 0.666±0.008 0.203±0.004 0.599±0.003 0.077±0.001 0.913±0.006 0.601±0.011 0.513±0.003 Re-weight 0.697±0.008 0.928±0.005 0.684±0.004 0.206±0.005 0.587±0.003 0.075±0.003 0.915±0.005 0.603±0.004 0.515±0.002 SMOTE 0.696±0.011 0.920±0.008 0.673±0.003 0.205±0.004 0.595±0.003 0.077±0.001 0.914±0.005 0.604±0.007 0.514±0.002 Embed-SMOTE 0.683±0.007 0.913±0.002 0.673±0.002 0.205±0.003 0.588±0.002 0.076±0.001 0.943±0.004 0.606±0.005 0.514±0.002 GraphSMOTE 𝑇 0.713±0.008 0.929±0.006 0.720±0.002 0.206±0.005 0.602±0.004 0.083±0.003 0.929±0.005 0.622±0.003 0.519±0.001 GraphSMOTE 𝑂 0.709±0.010 0.927±0.011 0.712±0.003 0.215±0.010 0.591±0.012 0.080±0.005 0.905±0.008 0.616±0.006 0.515±0.003 GraphSMOTE 𝑝𝑟𝑒𝑇 0.727±0.003 0.931±0.002 0.726±0.001 0.249±0.002 0.641±0.001 0.126±0.001 0.937±0.003 0.639±0.002 0.531±0.001 GraphSMOTE 𝑝𝑟𝑒𝑂 0.736±0.001 0.934±0.002 0.727±0.001 0.243±0.002 0.641±0.002 0.123±0.001 0.941±0.002 0.636±0.001 0.532±0.001</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Affects of over-sampling scale.</figDesc><graphic url="image-1.png" coords="7,324.59,226.38,226.97,149.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of different algorithm's performance when changed to GCN as base model. ± 0.002 0.907 ± 0.003 0.663 ± 0.001 over-sampling 0.682 ± 0.005 0.907 ± 0.003 0.665 ± 0.003 Re-weight 0.684 ± 0.005 0.913 ± 0.004 0.672 ± 0.002 SMOTE 0.684 ± 0.006 0.910 ± 0.005 0.665 ± 0.003 Embed-SMOTE 0.691 ± 0.002 0.910 ± 0.003 0.667 ± 0.002 GraphSMOTE 𝑇 0.695 ± 0.005 0.920 ± 0.003 0.690 ± 0.002 GraphSMOTE 𝑂 0.693 ± 0.005 0.920 ± 0.005 0.707 ± 0.003 GraphSMOTE 𝑝𝑟𝑒𝑇 0.688 ± 0.001 0.919 ± 0.002 0.682 ± 0.001 GraphSMOTE 𝑝𝑟𝑒𝑂 0.699 ± 0.002 0.914 ± 0.002 0.702 ± 0.001 (a) AUC-ROC Score (b) F Measurement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Affects of hyper-parameter 𝜆.</figDesc><graphic url="image-2.png" coords="8,320.95,254.63,116.01,74.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Cora: Cora is a citation network dataset for transductive learning setting. It contains one single large graph with 2, 708 papers from 7 areas. Each node has a 1433-dim attribution vector, and a total number of 5, 429 citation links exist in that graph. In this dataset, class distributions are relatively balanced, so we use an imitative imbalanced setting: three random classes are selected as minority classes and downsampled. All majority classes have a training set of 20 nodes. For each minority class, the number is 20 × 𝑖𝑚𝑏𝑎𝑙𝑎𝑛𝑐𝑒_𝑟𝑎𝑡𝑖𝑜. We vary 𝑖𝑚𝑏𝑎𝑙𝑎𝑛𝑐𝑒_𝑟𝑎𝑡𝑖𝑜 to analyze the performance of GraphSMOTE under various imbalanced scenarios. • BlogCatalog: This is a social network dataset crawled from BlogCatalog 2 , with 10, 312 bloggers from 38 classes and 333, 983 friendship edges. The dataset doesn't contain node attributes. Following [30], we attribute each node with a 64dim embedding vector obtained from Deepwalk. Classes in this dataset follow a genuine imbalanced distribution, with 14 classes smaller than 100, and 8 classes larger than 500. For this dataset, we use 25% samples of each class for training and 25% for validation, the remaining 50% for testing. • Twitter: This dataset is crawled by</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different approaches for imbalanced node classification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Node classification performance in terms of AUC on Cora under various imbalance ratios. GraphSMOTE 𝑝𝑟𝑒𝑇 0.9167 0.9130 0.9303 0.9317 GraphSMOTE 𝑝𝑟𝑒𝑂 0.9117 0.9116 0.9389 0.9366 across all the settings, which shows the effectiveness of the proposed framework under various scenarios.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Imbalance Ratio</cell><cell></cell></row><row><cell>Methods</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell></row><row><cell>Origin</cell><cell cols="4">0.8681 0.8998 0.9139 0.9146</cell></row><row><cell>over-sampling</cell><cell cols="4">0.8707 0.9039 0.9137 0.9215</cell></row><row><cell>Re-weight</cell><cell cols="4">0.8791 0.8881 0.9257 0.9306</cell></row><row><cell>SMOTE</cell><cell cols="4">0.8742 0.9027 0.9161 0.9237</cell></row><row><cell>Embed-SMOTE</cell><cell cols="4">0.8651 0.8967 0.9188 0.9212</cell></row><row><cell>GraphSMOTE 𝑇</cell><cell cols="4">0.8824 0.9162 0.9262 0.9309</cell></row><row><cell>GraphSMOTE 𝑂</cell><cell cols="4">0.8849 0.9061 0.9216 0.9311</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code available at https://github.com/TianxiangZhao/GraphSmote</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://www.blogcatalog.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://twitter.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>This project was partially supported by NSF projects IIS-1707548, CBET-1638320, IIS-1909702, IIS1955851, and the Global Research Outreach program of Samsung Advanced Institute of Technology under grant #225003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep over-sampling framework for classifying imbalanced data</title>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><forename type="middle">Yuan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="770" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1993">2016. 1993-2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The imbalanced training sample problem: Under or over sampling?</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barandela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">M</forename><surname>Valdovinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvador</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><forename type="middle">J</forename><surname>Ferri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The use of the area under the ROC curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Safe-level-smote: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem</title>
		<author>
			<persName><forename type="first">Chumphol</forename><surname>Bunkhumpornpat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krung</forename><surname>Sinapiromsaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chidchanok</forename><surname>Lursinsap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia conference on knowledge discovery and data mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SMOTE: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Nitesh V Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SMOTEBoost: Improving prediction of the minority class in boosting</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Nitesh V Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on principles of data mining and knowledge discovery</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="107" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on artificial intelligence</title>
				<imprint>
			<publisher>Lawrence Erlbaum Associates Ltd</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="973" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An approach to imbalanced data sets based on changing rule strength</title>
		<author>
			<persName><forename type="first">Jerzy</forename><forename type="middle">W</forename><surname>Grzymala-Busse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">K</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Witold</forename><forename type="middle">J</forename><surname>Grzymala-Busse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinqun</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rough-neural computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Class imbalances versus small disjuncts</title>
		<author>
			<persName><forename type="first">Taeho</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressing the curse of imbalanced training sets: one-sided selection</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Icml</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="179" to="186" />
			<date type="published" when="1997">1997</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural network classification and prior class probabilities</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cost-sensitive learning and the class imbalance problem</title>
		<author>
			<persName><forename type="first">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploratory undersampling for class-imbalance learning</title>
		<author>
			<persName><forename type="first">Xu-Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="539" to="550" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The problem of bias in training data in regression problems in medical decision support</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mac Namee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padraig</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><forename type="middle">I</forename><surname>Corrigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="51" to="70" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Identifying fake accounts on social networks based on graph analysis and classification algorithms. Security and Communication Networks</title>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Mohammadrezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Ebrahim</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir Masoud</forename><surname>Rahmani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Survey of resampling techniques for improving classification performance in unbalanced datasets</title>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>More</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06048</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimizing F-measures by cost-sensitive classification</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Shameem Puthiya Parambath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2123" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<idno>KDD &apos;14</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Handling imbalanced data: A survey</title>
		<author>
			<persName><forename type="first">Neelam</forename><surname>Rout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debahuti</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manas</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mallick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Proceedings on Advances in Soft Computing, Intelligent Systems and Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="431" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galileo Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Costsensitive boosting for classification of imbalanced data</title>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Kc</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ChebNet: Efficient and Stable Constructions of Deep Neural Networks with Rectified Power Units using Chebyshev Approximations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv abs/1911.05467</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The iNaturalist challenge 2017 dataset</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Network Embedding with Completely-imbalanced Labels</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Position-aware Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sampling + reweighting: Boosting the performance of AdaBoost on imbalanced datasets</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2012 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BotGraph: Large Scale Spamming Botnet Detection</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliot</forename><surname>Gillum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="321" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv abs/1812.08434</idno>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Training cost-sensitive neural networks with methods addressing the class imbalance problem</title>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu-Ying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
