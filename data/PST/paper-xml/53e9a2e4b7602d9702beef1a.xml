<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Balanced Scheduling with Compiler Optimizations that Increase Instruction-Level Parallelism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jack</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Susan</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
							<email>eggers@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Balanced Scheduling with Compiler Optimizations that Increase Instruction-Level Parallelism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional list schedulers order instructions based on an optimistic estimate of the load latency imposed by the hardware and therefore cannot respond to variations in memory latency caused by cache hits and misses on non-blocking architectures. In contrast, balanced scheduling schedules instructions based on an estimate of the amount of instruction-level parallelism in the program. By scheduling independent instructions behind loads based on what the program can provide, rather than what the implementation stipulates in the best case (i.e., a cache hit), balanced scheduling can hide variations in memory latencies more effectively.</p><p>Since its success depends on the amount of instruction-level parallelism in the code, balanced scheduling should perform even better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism: loop unrolling, trace scheduling and cache locality analysis. Using code generated for the DEC Alpha by the Multiflow compiler, we simulated a non-blocking processor architecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits from all three optimizations, producing average speedups that range from 1.15 to 1.40, across the optimizations. More importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage over traditional scheduling. Without the optimizations, balanced scheduled code is, on average, 1.05 times faster than that generated by a traditional scheduler; with them, its lead increases to 1.18.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional instruction schedulers order load instructions based on an optimistic assumption that all loads will be cache hits. On most machines, this optimistic estimate is accurate, because the processors block, i.e., stall the pipeline, on cache misses. Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache.</p><p>The traditional blocking processor model has recently been challenged by processors that do not block on loads <ref type="bibr" target="#b4">[ER94]</ref>  <ref type="bibr" target="#b1">[CS95]</ref>. Rather than stalling until a cache miss is satisfied, they use lockup-free caches <ref type="bibr" target="#b13">[Kro81]</ref>  <ref type="bibr" target="#b7">[FJ94]</ref> to continue executing instructions to hide the latency of outstanding memory requests. On these non-blocking architectures, instruction latency is exposed to the compiler and becomes uncertain: not only will the processor see both cache hits and misses, but each level in the memory hierarchy will also introduce a new set of latencies. Instead of handling all loads identically, a non-blocking processor's code scheduler could arrange instructions behind loads more intelligently to produce more efficient code schedules. Unfortunately, a traditional instruction scheduler fails to exploit this opportunity because of its optimistic and architecture-based estimates; its resulting schedules may be therefore intolerant of variations in load latency.</p><formula xml:id="formula_0">[McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94]</formula><p>Balanced scheduling <ref type="bibr" target="#b12">[KE93]</ref> is an algorithm that can generate schedules that adapt more readily to the uncertainties in memory latency. Rather than being determined by a predefined, architecturally-based value, load latency estimates are based on the number of independent instructions that are available to hide the latency of a particular load. Using these estimates as load weights, the balanced scheduler then schedules instructions normally. Previous work has demonstrated that balanced schedules show speedups averaging 8% for several Perfect Club benchmarks for two different cache hit/miss ratios, assuming a workstation-like memory model in which cache misses are normally distributed <ref type="bibr" target="#b12">[KE93]</ref>.</p><p>Since its success depends on the amount of instruction-level parallelism in a program, balanced scheduling should perform better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism. Two of them, loop unrolling and trace scheduling, do so by giving the scheduler a larger window of instructions from which to select. The third, locality analysis, more effectively utilizes the amount of instruction-level parallelism that is currently available.</p><p>Loop unrolling generates more instruction-level parallelism by duplicating loop iterations a number of times equal to the unrolling factor. The technique increases basic block size by eliminating branch overhead instructions for all iterations but the last. Consequently, more instructions are available to hide load latencies and more flexibility is offered to the scheduler. Trace scheduling increases the amount of instruction-level parallelism by permitting instruction scheduling beyond basic blocks <ref type="bibr" target="#b5">[FERN84]</ref> [LFK + 93]. It does this by assigning expected execution frequencies to edges of the control flow graph, and optimizing the schedules along the execution paths with the highest frequencies. Basic blocks along these paths (or traces) are grouped together, and instructions are scheduled, sometimes speculatively, as though the trace were a single basic block. In order to preserve data dependencies and correctness, some code motion is restricted, or may only occur if additional compensation code is added. The third optimization, locality analysis, identifies potential temporal and spatial reuse in array accesses within a loop and transforms the code to exploit it <ref type="bibr" target="#b16">[MLG92]</ref>. When used in conjunction with balanced scheduling, it enables load instructions to be selectively balanced scheduled. If the compiler can determine that a load instruction is a hit, then the optimistic latency estimate is correct, and should be used. This frees more instructions to hide the latencies of other loads (compiler-determined cache misses, as well as loads with no reuse information), which are balanced scheduled.</p><p>The original work on balanced scheduling <ref type="bibr" target="#b12">[KE93]</ref>, which compared it to the traditional approach and without ILP-enhancing optimizations, relied on a stochastic model to simulate cache behavior and network congestion. It included load latencies for a single-level cache hierarchy, but assumed single-cycle execution for all other multi-cycle instructions, and modeled an instruction cache that always hit and an ideal, pipelined floating point unit. By examining the effects of balanced scheduling independent of the particular latencies of other multi-cycle instructions, the simple model allowed us to understand the ramifications of changing just one aspect of code scheduling.</p><p>However, executions on a realistic processor and memory architecture are needed to see how well balanced scheduling will perform in practice. Real machines have longer load latencies (i.e., more cycles), both because of their more complete memory system (a cache hierarchy and a TLB) and their faster processor cycle time, relative to the memory system; they also execute instructions that require multiple, fixed latencies. The former should help balanced scheduling relative to traditional scheduling, as long as there is sufficient instruction-level parallelism in the code, because the balanced technique is more likely to hide the additional load latencies. However, the latter, should dilute its benefits.</p><p>To study the effects of ILP-enhancing optimizations on balanced scheduling and to evaluate the impact of processor and memory features that affect its ability to hide load latencies, we incorporated balanced scheduling and locality analysis into the Multiflow compiler (which already does loop unrolling and trace scheduling). Using code generated for the DEC Alpha and a simulator that models the AXP 21164, we simulated the effects of all three optimizations on both balanced and traditional scheduling, using Perfect Club [BCK+89] and SPEC92 <ref type="bibr" target="#b3">[Dix92]</ref> benchmarks.</p><p>Our results confirm the hypothesis. Balanced scheduling interacts well with all three optimizations, producing average speedups that range from 1.15 to 1.40. More importantly, its performance advantage relative to traditional scheduling increases when combined with the optimizations. Without ILP-enhancing optimizations, balanced scheduled programs execute 1.05 times faster than those scheduled traditionally; with them, its performance advantage rises as high as 1.18.</p><p>The rest of this paper provides more detail and analysis to support these results. Section 2 briefly reviews the balanced scheduling algorithm and contrasts it to traditional instruction scheduling. Section 3 contains a discussion of the three compiler optimizations, in more detail than was presented above, and with examples for both trace scheduling and locality analysis. Section 4 describes the methodology and experimental framework for the studies. In section 5, we present the results from applying all three compiler optimizations and analyze the factors that account for the performance improvements. Finally, in section 6, we conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Balanced Scheduling</head><p>Although balanced scheduling is a new algorithm for calculating load instruction weights, it fits cleanly within the framework of a traditional list scheduler. This section describes the behavior of a traditional scheduler, and then explains how balanced scheduling differs.</p><p>Instruction scheduling seeks to minimize pipeline stalls due to data and control hazards. Given the data and control dependences present in the instruction stream and the estimates for instruction latencies, the scheduler statically determines an instruction order that avoids as many interlocks as possible. The list scheduling approach <ref type="bibr" target="#b11">[HG83]</ref> [GM86] <ref type="bibr" target="#b17">[Sit78]</ref> to this minimization problem relies on a code DAG, instruction latencies, and heuristics for instruction selection. The code DAG represents the instructions to be scheduled and the data dependences between them. Instruction latencies (also known as instruction weights) correspond to the number of cycles required before the results of the instructions become available. In traditional list scheduling, these latencies have architecturally-fixed and optimistic values. (For example, as mentioned above, the weight of load instructions is the time of a cache hit.) Given the code DAG and the instruction weights, the scheduler traverses the DAG and generates an ordering for the instructions by applying the set of heuristics for instruction selection, many of which are based on the instruction weights.</p><p>The traditional list scheduler can be very effective for an architecture with a blocking processor. In this model, the weights accurately reflect the actual latencies that a processor must hide. Variations in actual load latencies (due to either cache misses in the various levels of the memory hierarchy or congestion in the interconnect) are masked from the compiler, since the processor stalls after a number of cycles equal to the optimistic, architecturallydefined latency. This is not the case for non-blocking processors. Because the processor is free to continue instruction execution while a load access is in progress, instruction schedulers have an opportunity to hide load latencies by carefully selecting a number of instructions to place behind loads. Traditional schedulers, with their fixed assumptions about the time to execute loads, are unable to take advantage of this opportunity. The balanced scheduler, however, can produce schedules that tolerate variations in memory latency by making no architectural assumptions about load weights.</p><p>Instead of using fixed (and architecturally-optimistic) latency estimates for all load instructions, the balanced scheduler varies the instruction latency for loads by basing it on the amount of instruction-level parallelism available in the code. We characterize this measurement as load-level parallelism, and we use it for the weight of the load instruction instead of the fixed instruction latency. Working with one basic block at a time, the balanced scheduler calculates the weight for each load separately, as a function of the number of independent instructions that may initiate execution while the load is being serviced and the number of other loads that could hide their latencies with these same instructions. When loads are independent and therefore can be scheduled in sequence without stalling, independent instructions can be used to hide the latencies of them all. For example, in the code DAG of Figure <ref type="figure">1</ref>, loads L0 and L1 are independent of non-load instructions X1 and X2, and the scheduler could issue them in an L0 L1 X1 X2 order. However, when loads appear in series, there is less loadlevel parallelism to hide their latencies. For example, X1 and X2 can be used to hide the latency of either L2 or L3, but not both. The overall effect of the balanced scheduler is to schedule the independent instructions behind loads in a balanced fashion, and proportional to their ability to hide the latency of particular loads.</p><p>The balanced scheduling approach has three potential advantages over traditional schedulers. First, it hides more latency for loads that have more load-level parallelism available to them. Second, it distributes load-level parallelism across all the loads in order to be more tolerant of memory latency variability. Third, since it bases its load weight estimates on the code rather than the architecture, it produces schedules that are independent of the memory system implementation.</p><p>Initial results <ref type="bibr" target="#b12">[KE93]</ref> indicated that balanced scheduling produced speedups averaging 8% for selected programs in the Perfect Club suite, when simulated on a stochastic model of a microprocessor with 1990 processor and memory latencies, a bus interconnect and cache hit rates of 80% and 95%. Code for these studies was generated using gcc <ref type="bibr">[Sta]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Compiler Optimizations</head><p>Balanced scheduling utilizes load-level parallelism to hide the longer load latencies exposed by non-blocking processors. By applying techniques to increase load-level parallelism, the balanced scheduler should be able to generate schedules that are even more tolerant of uncertain memory latency. In this study, we analyzed the effect of three such techniques: loop unrolling, trace scheduling, and locality analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Loop Unrolling</head><p>Loop unrolling increases the size of basic blocks by duplicating iterations a number of times equal to the unrolling factor. It contributes to increased performance in two ways. First, by creating multiple copies of the loop body, it reduces conditional branch and loop indexing overhead from all but the last copy. Second, the consequent increase in the size of the basic block can expose more instruction-level parallelism, thereby providing more opportunities for code scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Trace Scheduling</head><p>Trace scheduling <ref type="bibr" target="#b5">[FERN84]</ref> [LFK+93] enables more aggressive scheduling by permitting code motion across basic block boundaries. Guided by estimated or profiled execution frequencies for each basic block, it creates traces of paths through each procedure. The trace scheduler then picks a trace, in order of decreasing execution frequencies, and schedules the basic blocks in the trace as if they were a single basic block. Code motion takes into account the effects of scheduling instructions across conditional jumps (splits) and merges (joins), following specific rules for when instructions must be copied or cannot be moved. The final schedule effectively combines into a single block what would have been multiple basic blocks if generated by a traditional scheduler.</p><p>Figure <ref type="figure">2</ref> illustrates a simple example of trace scheduling. The trace scheduler identifies basic blocks 1, 2, 4, and 5 as a single trace (trace A), and block 3 forms its own trace (trace B). Assume that trace A is scheduled first and therefore is known as the on-trace path (trace B is the off-trace path). Trace A is viewed by the scheduler as a single basic block: it uses traditional basic block code motion, as long as data dependences are maintained. However, in order to move instructions across splits or joins, compensation code may need to be inserted. For example, if an instruction from block 5 is</p><formula xml:id="formula_1">X1 X2 L1 L0 L2 L3 X0 X3 Figure 1</formula><p>placed in the schedule above the join, then a copy of that instruction must also be placed in the off-trace path, to ensure that the instruction is executed regardless of the path taken. Code motion can also be done speculatively. For example, the scheduler might wish to move an instruction in block 4 above the split in block 2. This operation becomes speculative, because it is not supposed to be executed in the off-trace path. Rather than adding instruction overhead in the off-trace path to undo its effects, speculative motion is restricted to safe operations only, i.e., it is not permitted if the instruction writes memory or sets a variable which is live in the offtrace path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Locality Analysis</head><p>Although memory latency can be uncertain because of cache behavior, this variability is not entirely random. For example, many numeric programs have regular memory access patterns within loops. On different loop iterations, an array reference may access two elements of an array that reside in the same cache line; the first will miss in the cache, but the second will hit because of spatial locality.</p><p>If the compiler can determine cache behavior, it can treat cache hits and misses differently. Cache hits can be scheduled using the traditional scheduling scheme, because their actual load latency matches the optimistic, architecture-defined estimate. Then, because we don't need to hide the latency of hits, more independent instructions are available for loads that will miss. These loads, as well as other loads for which locality analysis cannot determine cache behavior (e.g., those that are not array references within loops), can be scheduled via balanced scheduling.</p><p>We identify spatial and temporal locality by using the locality analysis algorithm that Mowry, Lam and Gupta developed for selective cache prefetching <ref type="bibr" target="#b16">[MLG92]</ref>. We apply their algorithm to determine cache hit and miss behavior for load instructions in inner loops.</p><p>When the indices of array accesses are linear functions of the loop indices, their algorithm can statically determine the relationship between array references with respect to cache behavior, identifying both spatial and temporal reuse within cache blocks.</p><p>Assume that the arrays in our examples (Figures <ref type="figure">3-5</ref>) are laid out in row-major order. An array reference has spatial reuse if, on consecutive iterations of the inner loop, the reference accesses memory locations that map to the same cache line. For example, in the code In order to exploit this locality, however, the scheduler needs to identify the first reference to the cache line as a cache miss and the others as cache hits. This cannot be done in the loop in Figure <ref type="figure">3</ref>, where there is exactly one load instruction corresponding to the A[i][j] reference. However, if the inner loop is unrolled, the load in the first unrolled copy can be marked as a cache miss, and the loads in the other copies as cache hits.</p><p>We align the arrays on cache-line boundaries, and assume the 32 byte cache line size of the Alpha 21164 first-level cache. For arrays with double-word elements, each cache line contains four elements of the array. Because of spatial reuse, references to</p><formula xml:id="formula_2">A[i][0], A[i][4],</formula><p>etc., will be cache misses, while references A</p><formula xml:id="formula_3">[i][1], A[i][2], A[i][3], A[i][5], A[i][6], A[i][7],</formula><p>etc., will hit. When the number of loop iterations is unknown statically, we postcondition the loop to ensure that the first unrolled copy still corresponds to a cache miss. This means that the code to handle extra loop iterations is placed after the code for unrolling the loop, as in Figure <ref type="figure">4</ref>. Note that we cannot simply use another for loop for the extra iterations, because we must be able to mark the load instructions as cache hits or misses.</p><p>For temporal reuse, a code transformation is also applied. An array reference has temporal reuse when the reference accesses the same location in different iterations of the loop. In Figure <ref type="figure">3</ref>, array reference B[i][0] has temporal reuse in the inner loop. The first iteration of the loop will cause a cache miss, while the others will result in hits. By peeling off the first iteration of the loop, we identify the load instruction for B[i][0] in the first iteration as a cache miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head><p>for (i=0</p><formula xml:id="formula_4">; i &lt; n; i++) { C[i][0] = A[i][0] + B[i][0]; for (j = 1; j &lt; n; j++) C[i][j] = A[i][j] + B[i][0] } Figure 3 for (i=0; i &lt; n; i++) for (j=0; j &lt; n; j++) C[i][j] = A[i][j]+B[i][0]; Figure 4 for (i=0; i &lt; n; i++) { for (j=0; j &lt; n-(n%4); j+=4) { C[i][j] = A[i][j] + B[i][0]; C[i][j+1] = A[i][j+1] + B[i][0]; C[i][j+2] = A[i][j+2] + B[i][0]; C[i][j+3] = A[i][j+3] + B[i][0]; } if (j &lt; n) { C[i][j] = A[i][j] + B[i][0]; j++; if (j &lt; n) { C[i][j] = A[i][j] + B[i][0]; j++; if (j &lt; n) { C[i][j] = A[i][j] + B[i][0]; j++; } } } }</formula><p>The rest of the loop is unchanged, and since all successive references to B[i][0] in the loop will now be cache hits, we do not need to apply balanced scheduling to its load. Figure <ref type="figure">5</ref> shows the peeled version of the original loop of Figure <ref type="figure">3</ref>.</p><p>In Figure <ref type="figure">3</ref>, the inner loop actually exhibits both types of reuse, so the two transformations would be applied in conjunction. First, the loop would be peeled, and then the remaining iterations would be unrolled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>To examine the interactions between balanced scheduling and the three compiler optimizations, we ran a set of experiments that measured the effects of individual optimizations on balanced and traditionally scheduled code. We also compared the balanced scheduled code with that produced when combining the optimizations with a traditional list scheduler. The experiments simulate program execution of all code on an architectural model that closely resembles the DEC Alpha 21164. The code was generated using the Multiflow compiler. Section 4.1 describes our workload, section 4.2 the compiler changes and section 4.3 the simulator and the metrics it produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program</head><p>Lang. Description ARC2D Fortran Two-dimensional fluid flow problem solver using Euler equations </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Workload</head><p>Our experiments use benchmarks (Table <ref type="table" target="#tab_0">1</ref>) taken from the Perfect Club [BCK + 89] and SPEC92 <ref type="bibr" target="#b3">[Dix92]</ref> suites (the Multiflow compiler has both Fortran and C front ends). We chose numeric programs because their high percentage of loops make them appropriate testbeds for optimizations targeted for loops. Arrays in the C program are laid out in row-major order; the Fortran benchmarks have a column-major data layout</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Compiler</head><p>By modifying the scheduling module (Phase 3) of the Multiflow compiler, we scheduled instructions by using either a traditional algorithm or balanced scheduling. The list scheduler for the balanced and traditional schedulers compared in this study is topdown, and selects instructions from the ready list in a prioritized order. The priority of an instruction is simply the sum of the instruction's weight and the maximum priority of its successors. Ties are broken by the following heuristics, listed in order of preference. The first heuristic aims to control register pressure by selecting the instruction with the largest difference between consumed and defined registers. The second heuristic selects the instruction that would expose the largest number of successors in the code DAG. The final heuristic selects the instruction that was generated first in the original order of instructions. As another aid in controlling register pressure when applying balanced scheduling, we limited load weights to a maximum of 50.<ref type="foot" target="#foot_0">1</ref> Our version of the Multiflow compiler generates code for DEC Alpha processors.</p><p>For the loop unrolling experiments, we selected two unrolling factors, 4 and 8. We disabled loop unrolling when the unrolled block reached 64 instructions (4) or 128 (8), and did not unroll loops with more than one internal conditional branch. 2</p><p>Using the methodology established in other trace scheduling performance studies <ref type="bibr" target="#b6">[FGL93]</ref>, we first profiled the programs to determine basic block execution frequencies. This information guided the Multiflow compiler in picking traces. Since traces do not cross the back edges of loops, we unrolled them, using the same unrolling factor as in the loop unrolling studies. To gain maximum flexibility of code motion, we also permitted speculative code motion.</p><p>We added a locality analysis module to Multiflow's Phase 2. As in Mowry, Lam and Gupta <ref type="bibr" target="#b16">[MLG92]</ref>, a predicate 3 is created for each array reference that exhibits reuse and is associated with the references's load instruction. Predicates are used to determine both the unrolling factor (which depends on the cache block and array element sizes) 4 and whether loads should be hits or misses. Dependence arcs were added in the code DAG between each miss load and its corresponding hit loads to prevent the latter from floating above the miss during scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Simulation Framework</head><p>The compiled benchmarks were simulated by using an executiondriven simulator that emulates the DEC Alpha 21164 in most respects. The simulator models the 21164's pipelined functional units, 3-level cache hierarchy, first-level lockup-free cache, both the instruction and data TLBs, branch and computed jump prediction and the instruction and memory hierarchy latencies. (The latter are listed in Tables <ref type="table" target="#tab_3">2 and 3</ref>.) Unlike the 21164, we assume single instruction issue, since we would like to understand fully balanced scheduling's ability to exploit load-level parallelism before applying it to multiple-issue processors that need ILP to execute efficiently.</p><p>The simulator produces metrics for execution cycles and number of instructions. Cycle metrics measure total cycles, interlock cycles for both loads and instructions with fixed latencies, and dynamic instruction execution. Instruction counts are obtained for long and short integers, long and short floating point operations, loads, stores, branches, and spill and restore instructions. All met-2 Using the Alpha's conditional move instruction, the Multiflow compiler does predicated execution on simple conditional branches. Our limit on the number of conditional branches only affects the conditions within loops that could not be predicated. Loop unrolling can have limited benefit when applied to loops with internal branches, because in the unrolled loop body, the internal branches limit the expansion of basic block size. Unrolling these loops can be more beneficial, however, when trace scheduling is also applied, because of its ability to schedule beyond basic block boundaries. 3 Predicates contain the loop index, loop depth, stride of access and type of locality. 4 Recall that locality analysis requires some loop unrolling to differentiate between load references that hit or miss. We only unroll those loops that contain arrays that exhibit reuse.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Effect of the Optimizations</head><p>This section details the results of our experiments and provides explanations for our findings. We evaluate each optimization and compare its effects on balanced scheduling relative to the traditional scheduling approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Loop Unrolling</head><p>Loop unrolling with an unrolling factor of 4 improved the performance of all balanced scheduled programs, by an average speedup of 1.19 (Table <ref type="table" target="#tab_4">4</ref>). About half of the benefit was due to a 11% decrease in the number of executed instructions (integer and branch instructions related to branch overhead); a fifth was caused by a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction type Latency</head><p>integer op 1  19% reduction in interlock cycles due to fixed latency instructions; and the rest was a 23% savings in load interlock cycles. The best speedups were seen for programs whose frequently executed loops could be unrolled the full extent of the unrolling factor; when full unrolling did not occur, improvements were smaller. For example, frequently executed loops in doduc, mdljdp2, spice2g6 and swm256 contained multiple conditionals and consequently were not unrolled. For these benchmarks, the change in dynamic instruction count (column 6 in Table <ref type="table" target="#tab_4">4</ref>) was minimal when unrolling by 4, reflecting the paucity of unrollable loops. Unrolling also had limited effects on the ora benchmark, in which most of the execution time is spent in a large, loop-free subroutine. Finally, in the BDNA benchmark, many of the loops were large enough that the iteration instruction limit for unrolling disabled the optimization. <ref type="foot" target="#foot_1">1</ref> Considering only the other eleven programs for which loop unrolling was universally applied, balanced scheduling was able to eliminate on average 33% of interlock cycles, producing average speedups of 1.29.</p><p>Unrolling by 8 brought additional benefit (an average speedup of 1.28 relative to no unrolling), but not as great, and not for all programs. For some programs the more aggressive unrolling could not be completely applied 2 ; in most cases, it increased register pressure, and the independent instructions, now relatively fewer in number, were less able to hide the latency of the additional spill loads. Consequently, for most programs, load interlock cycles and the dynamic instruction count continued to drop, but by less (on average, another 2.8% for load interlock cycles, for a total reduction of 26%, and another 3% for dynamic instruction count, for a total reduction of 14%). The three exceptions were TRFD, doduc and tomcatv, for which overall performance dropped relative to unrolling by 4. In TRFD the increase in spill instructions offset the reduction in branch overhead; in doduc, the more aggressive unrolling increased code size, contributing to a degradation in instruction cache performance; and in tomcatv, interlock cycles increased because of the increase in spill code.</p><p>The more important loop unrolling results, however, were found when comparing balanced and traditional scheduling. The combination of balanced scheduling and loop unrolling produced better average speedups than coupling loop unrolling with traditional list scheduling. Without loop unrolling, balanced scheduled codes executed on average 1.05 times faster than the traditional approach (Table <ref type="table" target="#tab_6">5</ref>); with it, balanced scheduling's advantage rose to 1.12 when unrolling by 4, and to 1.18 when by 8. (In other words, in contrast to balanced scheduling's 1.19 and 1.28 average improvements when unrolling by 4 and 8, traditional scheduling provided speedups of only 1.12 and 1.15.) In all cases (all programs and unrolling optimization levels) dynamic instruction counts between the two scheduling approaches were almost identical. Balanced scheduling's advantage stemmed entirely from its ability to hide load latencies: in particular, the balanced code induced an average 2 There were exceptions, however. For example, the 64 instruction limit on unrolling by 4 prevented swm256 from being fully unrolled; the higher limit with an unrolling factor of 8 allowed more unrolling and therefore provided more benefit.  of 51%, 61%, and 62% fewer load interlock cycles than traditional scheduling when unrolling 0, 4, and 8 times, respectively (again, Table <ref type="table" target="#tab_6">5</ref>). The result was that only 7%, 6% and 6% of total execution cycles were lost to load interlocks, while traditional scheduling's penalty was 15%, 16% and 16%.</p><p>Although balanced scheduling is, on average, a better match than traditional list scheduling with techniques that increase basic block size, such as loop unrolling, the average speedups mask a wide divergence in the programs' ability to benefit from it. For particular benchmarks, traditional scheduling even outperformed balanced scheduling. In these programs, balanced scheduling continued to be more successful at reducing load interlock cycles. However, it did this at the expense of hiding interlock cycles caused by multi-cycle instructions with fixed latencies, such as integer multiply and floating point instructions. The traditional scheduler, on the other hand, gave preference to the fixed latency operations.</p><p>The benchmarks in question exhibited two characteristics that limited the effectiveness of balanced scheduling. First, they had smaller basic blocks, and hence, less load-level parallelism. Second, as a percentage of total cycles, load interlock cycles were small and were outnumbered by non-load interlock cycles. In these cases, when non-load interlock cycles dominated total interlock cycles and insufficient parallelism existed, balanced scheduling sacrificed hiding non-load interlocks to hide the latency of load instructions. When more parallelism was exposed by unrolling loops, the frequency and magnitude of these performance degradations were reduced. Nonetheless, the results argue for either incorporating multi-cycle instructions with fixed latencies into the balanced scheduling algorithm or developing heuristics to statically choose between the two schedulers on a basic block basis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Trace Scheduling</head><p>Trace scheduling alone brought little benefit for this workload (Table <ref type="table" target="#tab_7">6</ref>, column 2). Three factors limited its effectiveness. First, traces do not cross back edges of loops, and therefore trace scheduling does not schedule beyond loop boundaries. Since the bulk of execution time in our applications was spent in loops, there was little opportunity to apply it effectively. Second, because it optimizes the most frequently executed traces at the expense of less frequently executed paths, trace scheduling is most effective when there are dominant paths of execution. DYFESM and doduc, in particular, have few dominant paths, and consequently trace scheduling had either limited effectiveness or even degraded performance. Third, programs such as BDNA already had extremely large basic blocks, and therefore trace scheduling was not needed to increase available parallelism. For these reasons, when evaluating the interaction between trace scheduling and the two code scheduling techniques, we first unrolled the loops.</p><p>Averaged across the entire workload, the speedups of trace scheduling and loop unrolling were 1.19 with an unrolling factor of 4, and 1.26 when unrolling by 8 (again, Table <ref type="table" target="#tab_7">6</ref>). 1 For programs where trace scheduling could accurately pick traces and string together several nonloop basic blocks, the wins were larger, on average 1.29 and 1.43 for unrolling by 4 and 8, respectively. Included in this group were programs for which loop unrolling alone had been less beneficial; by effectively increasing basic block size in both the loops with conditionals and the sequential code, trace scheduling improved their performance. Consequently, the combination of loop unrolling and trace scheduling sometimes brought benefits to balanced scheduling that were greater than the sum of the individual effects.</p><p>As with the loop unrolling experiments, the comparison to traditional scheduling came out in balanced scheduling's favor (Table <ref type="table" target="#tab_8">7</ref>).</p><p>When combined with trace scheduling and an unrolling factor of 4, balanced scheduled programs executed 1.14 times faster than traditional scheduling with the same optimizations. The comparable speedup for trace scheduling and an unrolling factor of 8 was 1.16. As before, the improvements stemmed from balanced scheduling's ability to hide load interlock cycles more effectively: their decrease rose to 65% and 56%, resulting in only 5% of total execution cycles wasted on load interlocks for the two degrees of unrolling. Traditional scheduling with trace scheduling and unrolling by 4 and 8, on the other hand, was still left with 15% of total cycles lost to load interlocks. On an individual basis, however, traditional scheduling was sometimes superior to balanced scheduling, as in some of the loop unrolling experiments, because non-load interlock cycles were hidden more effectively.</p><p>As a final note, we observe that the overall performance gains due to trace scheduling are smaller than those obtained in the original trace scheduling studies. This discrepancy can be attributed to the differing machine models. Trace scheduling was first applied to VLIW machines, where instruction issue bandwidth was not a bottleneck. However, in our single-issue model, where instruction issue bandwidth can be a scarce resource, speculative execution, 1 Because of speculative execution and poor trace selection, the dynamic instruction count for DYFESM more than doubled for both balanced and traditional scheduling on a single-issue machine. On a wider-issue machine (superscalar or VLIW), this effect would be smaller, because of the increased instruction issue bandwidth. If we had excluded DYFESM from the results for balanced scheduling plus trace scheduling and loop unrolling, speedups would have been 1.23 (4) and 1.31 (8). (See also the discussion at the end of this section.)</p><p>with its increased instruction count, can stress it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Locality Analysis</head><p>In contrast to the first two optimizations, which increase the amount of instruction-level parallelism in the code, locality analysis allows the balanced scheduler to exploit the existing parallelism more effectively. Locality analysis should improve performance for two inter-related reasons. First, by relying on more accurate information about memory latency (cache hits and misses), less uncertainty should exist, and the scheduler will presumably generate a better schedule. Specifically, since we do not apply balanced scheduling to load instructions that are cache hits, independent instructions that would normally be used to hide their latency can be applied to loads that will miss. Second, since the spatial locality analysis relies on (limited) loop unrolling, basic block sizes should increase, consequently giving the balanced scheduler more flexibility in using independent instructions to hide load latencies. Taken together, these effects should decrease interlock cycles over applying balanced scheduling alone.</p><p>Program speedups from locality analysis averaged 1.15 (Table <ref type="table" target="#tab_7">6</ref>), with the percentage gain divided almost evenly between decreases in dynamic instruction count (branch overhead and spill code) and non-load interlock cycles 2 . The gains were less than those achieved by the optimizations that increased basic block size (loop unrolling and loop unrolling coupled with trace scheduling), because there were fewer opportunities where locality analysis could be applied. The most striking exception was tomcatv, which has very sequential accesses to large, read-only arrays: speedup for this program was 1.5. In general, however, four factors limited the effectiveness of the locality analysis algorithm. First, in order to exploit locality, the compiler must be able to determine how an array is aligned relative to cache lines. When subsections of arrays are passed as parameters to a subroutine, this is not possible. The second factor is the presence of index expressions that introduce irregularity into the memory reference pattern. When array references contain index expressions that are not exclusively composed of affine functions of loop induction variables, reuse cannot be identified statically. Third, some benchmarks pass multi-dimensional arrays as parameters to subroutines, but the array dimensions are dependent on other subroutine parameters. The cache alignment characteristics of these arrays are therefore unknown at compile time. Fourth, even though a load may be identified as a cache hit, it may actually turn out to be a cache miss because of interference with another load reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">"Putting It All Together"</head><p>Considering these results across optimizations (rather than across programs for a particular optimization, as we have done previously), balanced scheduled code consistently produced speedups over that generated by traditional scheduling. Its performance improvements stemmed from its ability to hide load interlocks more effectively. With only two exceptions, balanced scheduled code produced fewer load interlocks than that of the traditional scheduler for all programs, on all levels of optimization 3 . The differences ranged from two to three times as many interlocks (averaged across all programs for a particular optimization) for the traditional scheduler (see summary results in Table <ref type="table" target="#tab_9">8</ref>).</p><p>2 The bulk of load interlock cycles had been reduced by balanced scheduling. 3 except locality analysis. Since traditional scheduling relies on a single load latency, it has no counterpart in locality analysis.</p><p>As we applied optimizations that increased instruction-level parallelism more aggressively (i.e., no optimization through trace scheduling and loop unrolling with an unrolling factor of 8), balanced scheduling was able to extend its advantage over traditional scheduling by exploiting the additional instruction-level parallelism. The percentage of load interlock cycles in balanced scheduled code dropped, while in traditionally scheduled code it remained constant or rose slightly. The consequence was a consistent rise in speedup of balanced scheduling over the traditional approach.</p><p>Locality analysis contributed additional speedup when applied along with the other optimizations. When used with loop unrolling, speedups of 1.28 and 1.31 were obtained over balanced scheduling alone, for unrolling factors of 4 and 8, respectively (Tables <ref type="table" target="#tab_10">6 and 9</ref>). When trace scheduling was also applied, these speedups reached 1.29 and 1.40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Simulating Real Architectures</head><p>Examining results from the original comparison of balanced and traditional scheduling (without optimizations) <ref type="bibr" target="#b12">[KE93]</ref> illustrates both the limitations of simple architecture models and the benefits of a very optimizing compiler for execution cycle analysis. There were several methodological differences between the two studies. First, the original work relied on a stochastic model, rather than execution-driven simulation, to simulate cache behavior and network congestion. Second, it included first-level cache load latencies, but assumed single-cycle execution for all other multicycle instructions, modeling an instruction cache that always hit and an ideal, pipelined floating point unit, and excluding a cache hierarchy and TLB. This work incorporates all factors; fixed, multicycle instruction latencies, in particular, proved to have a marked effect on code scheduling performance. Finally, the previous study used code generated by gcc, which omits several optimizations, such as predicated execution and array dependence analysis to disambiguate between loads and stores, that benefit code scheduling by exposing more load-level parallelism. These optimizations are included in the Multiflow compiler. For the four programs that both studies have in common, we estimate that balanced scheduling had a 10% advantage over traditional scheduling with the simple model, but only 4% when modeling the 21164. Recall that the 21164 has a longer memory latency, ranging from 2 to 50 cycles, depending on the memory hierarchy level, and multi-cycle floating point and integer multiply instructions. Therefore there are more latencies of several types to hide, some of which (the fixed latency arithmetic operations) balanced scheduling does not yet address. Despite these differences, this study validates the earlier conclusion that balanced scheduling is on average superior to traditional scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Modern processors with lockup-free caches provide new opportunities for better instruction scheduling by exposing the variations in load latency due to the memory hierarchy. Unfortunately, traditional scheduling techniques cannot take advantage of this opportunity, because they assume that load instruction weights are fixed, best-case latency estimates. Balanced scheduling, on the other hand, is able to hide variable latencies by basing load instruction weights on a measure of load-level parallelism in the program, rather than using the fixed and optimistic latency estimates.</p><p>Our studies have shown that the balanced scheduler can increase its advantage over a traditional scheduler when more instruction-level parallelism is available. By applying loop unrolling, trace scheduling, and locality analysis in conjunction with the balanced scheduler, load interlock cycles were reduced to as low as 5% of total cycles when averaged across an entire workload. In contrast, traditionally scheduled code wasted no less than 15% of its execution cycles waiting for load instructions, when the same optimizations were applied. The ability of the balanced scheduler to hide load latency translated into significant performance gains. Average speedups of balanced scheduled code over traditionally scheduled code increased from 1.05 without the ILP-enhancing optimizations to as much as 1.18 when they were used.</p><p>Of all three optimizations, loop unrolling produced the most significant benefits for balanced scheduled code, achieving performance improvements of 1.19 and 1.28 over no unrolling. Relative to traditional scheduling, balanced scheduling increased its edge from from 1.05 (no unrolling) to 1.12 (unrolling by 4) to 1.18 (unrolling by 8).</p><p>While loop unrolling increases parallelism within loop bodies, trace scheduling exposes it in unrolled loops that contain conditionals and in the sequential portions of code. However, since loops dominate execution in our workload, trace scheduling was only beneficial when used in conjunction with loop unrolling. The combination produced balanced schedules that executed 1.19 times faster when unrolling by 4, and 1.26 times faster when unrolling by 8, over balanced scheduling alone. Furthermore, they increased balanced scheduling's lead relative to traditional scheduling to 1.14 (4) and 1.16 (8).</p><p>Locality analysis provided a performance improvement of 1.15 over balanced scheduling alone. The figure reflects both the relatively small amount of array reuse in our workload and the benefits of the limited loop unrolling that is required to implement the algorithm. Combining locality analysis with the other optimizations improved the speedups. The best case, of course, occurred when unrolling by 8, trace scheduling, and locality analysis were all used: here average speedups reached 1.40.</p><p>Although our results demonstrate that the balanced scheduler is more effective than traditional scheduling in taking advantage of the additional instruction-level parallelism generated by the three optimizations for most programs, there are cases where traditional scheduling does better. Because it uses optimistic estimates for load latency, it saves instruction-level parallelism for multi-cycle operations. When (fixed latency) multi-cycle operations dominate loads and there is insufficient instruction-level parallelism to hide instruction latencies, traditionally scheduled code can execute faster.</p><p>Having shown that balanced scheduling is a good match with optimizations that increase instruction-level parallelism, we intend to examine its effects on wider-issue (superscalar) processors that require considerable instruction-level parallelism to perform well. We also plan to investigate new techniques to better handle fixed, non-load interlock cycles within the framework of the balanced scheduling algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The workload.</figDesc><table><row><cell>BDNA</cell><cell>Fortran</cell><cell>Simulation of hydration structure and</cell></row><row><cell></cell><cell></cell><cell>dynamics of nucleic acids</cell></row><row><cell>DYFESM</cell><cell>Fortran</cell><cell>Structural dynamics benchmark to solve dis-</cell></row><row><cell></cell><cell></cell><cell>placements and stresses</cell></row><row><cell>MDG</cell><cell>Fortran</cell><cell>Molecular dynamic simulation of flexible</cell></row><row><cell></cell><cell></cell><cell>water molecules</cell></row><row><cell>QCD2</cell><cell>Fortran</cell><cell>Lattice-gauge QCD simulation</cell></row><row><cell>TRFD</cell><cell>Fortran</cell><cell>Two-electron integral transformation</cell></row><row><cell>alvinn</cell><cell>C</cell><cell>Trains a neural network using back propaga-</cell></row><row><cell></cell><cell></cell><cell>tion</cell></row><row><cell>dnasa7</cell><cell>Fortran</cell><cell>Matrix manipulation routines</cell></row><row><cell>doduc</cell><cell>Fortran</cell><cell>Monte Carlo simulation of the time evolu-</cell></row><row><cell></cell><cell></cell><cell>tion of a nuclear reactor component</cell></row><row><cell>ear</cell><cell>C</cell><cell>Simulates the propagation of sound in the</cell></row><row><cell></cell><cell></cell><cell>human cochlea</cell></row><row><cell>hydro2d</cell><cell>Fortran</cell><cell>Solves hydrodynamical Navier Stokes equa-</cell></row><row><cell></cell><cell></cell><cell>tions to compute galactical jets</cell></row><row><cell>mdljdp2</cell><cell>Fortran</cell><cell>Chemical application program that solves</cell></row><row><cell></cell><cell></cell><cell>equations of motion for atoms</cell></row><row><cell>ora</cell><cell>Fortran</cell><cell>Traces rays through an optical system com-</cell></row><row><cell></cell><cell></cell><cell>posed of spherical and planar surfaces</cell></row><row><cell>spice2g6</cell><cell>Fortran</cell><cell>Circuit simulation package</cell></row><row><cell>su2cor</cell><cell>Fortran</cell><cell>Computes masses of elementary particles in</cell></row><row><cell></cell><cell></cell><cell>the framework of the Quark-Gluon theory</cell></row><row><cell>swm256</cell><cell>Fortran</cell><cell>Solves shallow water equations using finite</cell></row><row><cell></cell><cell></cell><cell>difference equations</cell></row><row><cell>tomcatv</cell><cell>Fortran</cell><cell>Vectorized mesh generation program</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Memory hierarchy parameters. rics are calculated for each optimization, for both scheduling techniques.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Processor latencies.</figDesc><table><row><cell cols="2">integer multiply</cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>load</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>store</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FP op (excluding divide)</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FP div (23 bit fraction)</cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FP div (53 bit fraction)</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>branch</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Total Cycles</cell><cell></cell><cell cols="3">Dynamic Instruction Count</cell><cell cols="3">Load Interlock Cycles</cell></row><row><cell></cell><cell>(Millions)</cell><cell></cell><cell cols="2">Speedup</cell><cell>(Millions)</cell><cell cols="2">Percentage Decrease</cell><cell>(Millions)</cell><cell cols="2">Percentage Decrease</cell></row><row><cell></cell><cell>No Loop</cell><cell></cell><cell></cell><cell></cell><cell>No Loop</cell><cell></cell><cell></cell><cell>No Loop</cell><cell></cell></row><row><cell>Benchmark</cell><cell>Unrolling</cell><cell cols="2">Unroll by 4</cell><cell>Unroll by 8</cell><cell>Unrolling</cell><cell>Unroll by 4</cell><cell>Unroll by 8</cell><cell>Unrolling</cell><cell>Unroll by 4</cell><cell>Unroll by 8</cell></row><row><cell>ARC2D</cell><cell>17844.8</cell><cell></cell><cell>1.26</cell><cell>1.55</cell><cell>7704.9</cell><cell>11.4%</cell><cell>14.7%</cell><cell>32.1</cell><cell>34.3%</cell><cell>20.2%</cell></row><row><cell>BDNA</cell><cell>4316.3</cell><cell></cell><cell>1.03</cell><cell>1.05</cell><cell>3155.1</cell><cell>0.1%</cell><cell>1.3%</cell><cell>51.6</cell><cell>27.9%</cell><cell>21.1%</cell></row><row><cell>DYFESM</cell><cell>1049.5</cell><cell></cell><cell>1.32</cell><cell>1.36</cell><cell>776.4</cell><cell>12.4%</cell><cell>14.6%</cell><cell>19.4</cell><cell>76.8%</cell><cell>72.2%</cell></row><row><cell>MDG</cell><cell>24346.2</cell><cell></cell><cell>1.10</cell><cell>1.16</cell><cell>15344</cell><cell>7.9%</cell><cell>11.7%</cell><cell>184.8</cell><cell>11.0%</cell><cell>-12.1%</cell></row><row><cell>QCD2</cell><cell>1437.2</cell><cell></cell><cell>1.14</cell><cell>1.17</cell><cell>1169.4</cell><cell>16.8%</cell><cell>18.5%</cell><cell>32.1</cell><cell>34.3%</cell><cell>20.2%</cell></row><row><cell>TRFD</cell><cell>3633.1</cell><cell></cell><cell>1.34</cell><cell>1.31</cell><cell>2482.8</cell><cell>17.0%</cell><cell>13.6%</cell><cell>141.2</cell><cell>55.9%</cell><cell>56.1%</cell></row><row><cell>alvinn</cell><cell>8290.8</cell><cell></cell><cell>1.25</cell><cell>1.25</cell><cell>5283.8</cell><cell>36.6%</cell><cell>40.3%</cell><cell>707.8</cell><cell>11.3%</cell><cell>10.2%</cell></row><row><cell>dnasa7</cell><cell>22029.9</cell><cell></cell><cell>1.76</cell><cell>1.85</cell><cell>8532.4</cell><cell>22.5%</cell><cell>25.6%</cell><cell>1682.5</cell><cell>57.6%</cell><cell>61.1%</cell></row><row><cell>doduc</cell><cell>1944.1</cell><cell></cell><cell>1.02</cell><cell>0.99</cell><cell>1164.5</cell><cell>4.1%</cell><cell>4.0%</cell><cell>48.1</cell><cell>0.6%</cell><cell>6.4%</cell></row><row><cell>ear</cell><cell>22289.4</cell><cell></cell><cell>1.13</cell><cell>1.34</cell><cell>15776.9</cell><cell>17.1%</cell><cell>31.2%</cell><cell>142.1</cell><cell>-13.6%</cell><cell>-28.9%</cell></row><row><cell>hydro2d</cell><cell>12782.3</cell><cell></cell><cell>1.52</cell><cell>1.75</cell><cell>7404.7</cell><cell>20.2%</cell><cell>26.5%</cell><cell>511.8</cell><cell>66.6%</cell><cell>62.5%</cell></row><row><cell>mdljdp2</cell><cell>4966.6</cell><cell></cell><cell>1.01</cell><cell>1.01</cell><cell>3490.5</cell><cell>0.4%</cell><cell>0.6%</cell><cell>133</cell><cell>3.0%</cell><cell>2.8%</cell></row><row><cell>ora</cell><cell>8183.9</cell><cell></cell><cell>1.00</cell><cell>1.00</cell><cell>5034.3</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0</cell><cell>----</cell><cell>----</cell></row><row><cell>spice2g6</cell><cell>43659.1</cell><cell></cell><cell>1.01</cell><cell>1.04</cell><cell>24157.2</cell><cell>3.2%</cell><cell>3.9%</cell><cell>12862.5</cell><cell>1.1%</cell><cell>0.3%</cell></row><row><cell>su2cor</cell><cell>8032.5</cell><cell></cell><cell>1.10</cell><cell>1.20</cell><cell>6024.7</cell><cell>4.9%</cell><cell>9.4%</cell><cell>294.3</cell><cell>6.2%</cell><cell>27.4%</cell></row><row><cell>swm256</cell><cell>20470.8</cell><cell></cell><cell>1.00</cell><cell>1.44</cell><cell>13611.1</cell><cell>0.4%</cell><cell>10.9%</cell><cell>3500.0</cell><cell>1.9%</cell><cell>83.0%</cell></row><row><cell>tomcatv</cell><cell>1896.1</cell><cell></cell><cell>1.27</cell><cell>1.26</cell><cell>1232.8</cell><cell>10.2%</cell><cell>11.3%</cell><cell>264.7</cell><cell>21.3%</cell><cell>3.9%</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>1.19</cell><cell>1.28</cell><cell></cell><cell>10.9%</cell><cell>14.0%</cell><cell></cell><cell>23.3%</cell><cell>26.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Balanced</figDesc><table /><note>scheduling: Speedup in total cycles and percentage decrease in dynamic instruction count and load interlock cycles for unrolling factors of 4 and 8, relative to no unrolling.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>BS speedup vs. TS % reduction in load interlock cycles % of total cycles due to load interlock cycles</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>No LU</cell><cell></cell><cell>LU 4</cell><cell></cell><cell>LU 8</cell><cell></cell></row><row><cell>Benchmark</cell><cell>No LU</cell><cell>LU 4</cell><cell>LU 8</cell><cell>No LU</cell><cell>LU 4</cell><cell>LU 8</cell><cell>BS</cell><cell>TS</cell><cell>BS</cell><cell>TS</cell><cell>BS</cell><cell>TS</cell></row><row><cell>ARC2D</cell><cell>1.33</cell><cell>1.52</cell><cell>1.78</cell><cell>53.3%</cell><cell>66.7%</cell><cell>77.4%</cell><cell>18.9%</cell><cell>30.3%</cell><cell>15.4%</cell><cell>30.4%</cell><cell>12.3%</cell><cell>30.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Balanced scheduling (BS) vs. traditional scheduling (TS) for loop unrolling: Total cycles speedup, percentage improvement in load interlock cycles, and load interlock cycles as a percentage of total cycles.</figDesc><table><row><cell>BDNA</cell><cell>1.01</cell><cell>1.01</cell><cell>1.09</cell><cell>75.8%</cell><cell>80.6%</cell><cell>79.1%</cell><cell>1.2%</cell><cell>4.9%</cell><cell>0.9%</cell><cell>4.5%</cell><cell>1.0%</cell><cell>4.4%</cell></row><row><cell>DYFESM</cell><cell>0.90</cell><cell>0.96</cell><cell>0.97</cell><cell>34.5%</cell><cell>83.7%</cell><cell>80.6%</cell><cell>1.8%</cell><cell>3.1%</cell><cell>0.6%</cell><cell>3.6%</cell><cell>0.7%</cell><cell>3.7%</cell></row><row><cell>MDG</cell><cell>0.98</cell><cell>0.99</cell><cell>0.99</cell><cell>64.6%</cell><cell>70.0%</cell><cell>65.9%</cell><cell>0.8%</cell><cell>2.2%</cell><cell>0.7%</cell><cell>2.5%</cell><cell>1.0%</cell><cell>2.9%</cell></row><row><cell>QCD2</cell><cell>0.98</cell><cell>1.01</cell><cell>0.99</cell><cell>6.4%</cell><cell>35.9%</cell><cell>34.5%</cell><cell>2.2%</cell><cell>2.4%</cell><cell>1.7%</cell><cell>2.6%</cell><cell>2.1%</cell><cell>3.2%</cell></row><row><cell>TRFD</cell><cell>0.93</cell><cell>0.96</cell><cell>0.98</cell><cell>51.8%</cell><cell>78.0%</cell><cell>80.8%</cell><cell>3.9%</cell><cell>8.7%</cell><cell>2.3%</cell><cell>10.8%</cell><cell>2.2%</cell><cell>11.8%</cell></row><row><cell>alvinn</cell><cell>0.93</cell><cell>0.99</cell><cell>0.99</cell><cell>14.9%</cell><cell>3.2%</cell><cell>3.7%</cell><cell>8.5%</cell><cell>10.8%</cell><cell>9.4%</cell><cell>9.8%</cell><cell>9.6%</cell><cell>10.0%</cell></row><row><cell>dnasa7</cell><cell>1.18</cell><cell>1.76</cell><cell>1.84</cell><cell>73.0%</cell><cell>87.2%</cell><cell>88.0%</cell><cell>7.6%</cell><cell>23.9%</cell><cell>5.7%</cell><cell>25.2%</cell><cell>5.5%</cell><cell>24.8%</cell></row><row><cell>doduc</cell><cell>1.00</cell><cell>1.00</cell><cell>1.01</cell><cell>44.5%</cell><cell>45.6%</cell><cell>47.4%</cell><cell>2.5%</cell><cell>4.5%</cell><cell>2.5%</cell><cell>4.6%</cell><cell>2.3%</cell><cell>4.3%</cell></row><row><cell>ear</cell><cell>0.93</cell><cell>0.95</cell><cell>0.95</cell><cell>44.2%</cell><cell>39.9%</cell><cell>28.2%</cell><cell>0.6%</cell><cell>1.2%</cell><cell>0.8%</cell><cell>1.4%</cell><cell>1.1%</cell><cell>1.6%</cell></row><row><cell>hydro2d</cell><cell>0.99</cell><cell>1.07</cell><cell>1.11</cell><cell>68.4%</cell><cell>88.7%</cell><cell>87.0%</cell><cell>4.0%</cell><cell>12.8%</cell><cell>2.0%</cell><cell>17.0%</cell><cell>2.6%</cell><cell>18.2%</cell></row><row><cell>mdljdp2</cell><cell>1.03</cell><cell>1.03</cell><cell>1.03</cell><cell>54.0%</cell><cell>55.4%</cell><cell>55.3%</cell><cell>2.7%</cell><cell>5.7%</cell><cell>2.6%</cell><cell>5.7%</cell><cell>2.6%</cell><cell>5.7%</cell></row><row><cell>ora</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>-----</cell><cell>-----</cell><cell>-----</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>0.0%</cell></row><row><cell>spice2g6</cell><cell>1.02</cell><cell>1.01</cell><cell>1.01</cell><cell>7.5%</cell><cell>8.5%</cell><cell>8.0%</cell><cell>29.5%</cell><cell>31.4%</cell><cell>29.5%</cell><cell>32.1%</cell><cell>30.5%</cell><cell>32.7%</cell></row><row><cell>su2cor</cell><cell>1.18</cell><cell>1.22</cell><cell>1.26</cell><cell>87.8%</cell><cell>88.8%</cell><cell>90.4%</cell><cell>3.7%</cell><cell>25.3%</cell><cell>3.8%</cell><cell>27.5%</cell><cell>3.2%</cell><cell>26.5%</cell></row><row><cell>swm256</cell><cell>1.22</cell><cell>1.22</cell><cell>1.63</cell><cell>67.8%</cell><cell>68.4%</cell><cell>94.4%</cell><cell>17.1%</cell><cell>43.5%</cell><cell>16.8%</cell><cell>43.6%</cell><cell>4.2%</cell><cell>45.6%</cell></row><row><cell>tomcatv</cell><cell>1.22</cell><cell>1.36</cell><cell>1.37</cell><cell>72.4%</cell><cell>75.9%</cell><cell>72.7%</cell><cell>14.0%</cell><cell>41.3%</cell><cell>14.0%</cell><cell>42.9%</cell><cell>16.9%</cell><cell>45.3%</cell></row><row><cell>AVERAGE</cell><cell>1.05</cell><cell>1.12</cell><cell>1.18</cell><cell>51.3%</cell><cell>61.0%</cell><cell>62.1%</cell><cell>7.0%</cell><cell>14.8%</cell><cell>6.4%</cell><cell>15.5%</cell><cell>5.8%</cell><cell>16.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Speedups over balanced scheduling alone for combinations of loop unrolling by 4 and 8 (LU 4 and LU 8), trace scheduling (TrS) and locality analysis (LA).</figDesc><table><row><cell>Speedup over BS alone</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Balanced scheduling (BS) vs. traditional scheduling (TS): Total cycles speedup for loop unrolling alone and trace scheduling and loop unrolling</figDesc><table><row><cell></cell><cell cols="4">Speedup of Balanced Scheduling over Traditional Scheduling</cell><cell></cell></row><row><cell></cell><cell></cell><cell>No trace scheduling</cell><cell></cell><cell cols="2">Trace scheduling</cell></row><row><cell>Benchmark</cell><cell>No LU</cell><cell>LU 4</cell><cell>LU 8</cell><cell>LU 4</cell><cell>LU 8</cell></row><row><cell>ARC2D</cell><cell>1.33</cell><cell>1.52</cell><cell>1.78</cell><cell>1.62</cell><cell>1.99</cell></row><row><cell>BDNA</cell><cell>1.01</cell><cell>1.01</cell><cell>1.09</cell><cell>1.09</cell><cell>1.13</cell></row><row><cell>DYFESM</cell><cell>0.90</cell><cell>0.96</cell><cell>0.97</cell><cell>0.85</cell><cell>0.85</cell></row><row><cell>MDG</cell><cell>0.98</cell><cell>0.99</cell><cell>0.99</cell><cell>0.97</cell><cell>0.97</cell></row><row><cell>QCD2</cell><cell>0.98</cell><cell>1.01</cell><cell>0.99</cell><cell>0.96</cell><cell>0.99</cell></row><row><cell>TRFD</cell><cell>0.93</cell><cell>0.96</cell><cell>0.98</cell><cell>1.01</cell><cell>1.01</cell></row><row><cell>alvinn</cell><cell>0.93</cell><cell>0.99</cell><cell>0.99</cell><cell>0.97</cell><cell>0.98</cell></row><row><cell>dnasa7</cell><cell>1.18</cell><cell>1.76</cell><cell>1.84</cell><cell>1.83</cell><cell>2.17</cell></row><row><cell>doduc</cell><cell>1.00</cell><cell>1.00</cell><cell>1.01</cell><cell>1.06</cell><cell>1.02</cell></row><row><cell>ear</cell><cell>0.93</cell><cell>0.95</cell><cell>0.95</cell><cell>0.91</cell><cell>0.92</cell></row><row><cell>hydro2d</cell><cell>0.99</cell><cell>1.07</cell><cell>1.11</cell><cell>1.09</cell><cell>1.06</cell></row><row><cell>mdljdp2</cell><cell>1.03</cell><cell>1.03</cell><cell>1.03</cell><cell>0.91</cell><cell>0.91</cell></row><row><cell>ora</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>spice2g6</cell><cell>1.02</cell><cell>1.01</cell><cell>1.01</cell><cell>1.14</cell><cell>1.06</cell></row><row><cell>su2cor</cell><cell>1.18</cell><cell>1.22</cell><cell>1.26</cell><cell>---</cell><cell>---</cell></row><row><cell>swm256</cell><cell>1.22</cell><cell>1.22</cell><cell>1.63</cell><cell>1.37</cell><cell>1.64</cell></row><row><cell>tomcatv</cell><cell>1.22</cell><cell>1.36</cell><cell>1.37</cell><cell>1.38</cell><cell>1.43</cell></row><row><cell>AVERAGE</cell><cell>1.05</cell><cell>1.12</cell><cell>1.18</cell><cell>1.14</cell><cell>1.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Summary comparison of balanced scheduling and traditional scheduling.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Summary comparison of locality analysis results.</figDesc><table><row><cell>Optimizations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>in addition to balanced scheduling Relative to traditional scheduling with the same optimization: Relative to balanced scheduling with no other optimizations: Load interlock cycles remaining after applying the optimization: (% of total cycles) Program speedup</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Percentage</cell><cell></cell><cell>Percentage</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">decrease in</cell><cell></cell><cell>decrease in</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">load interlock</cell><cell>Program</cell><cell>load interlock</cell><cell>Balanced</cell><cell>Traditional</cell></row><row><cell></cell><cell></cell><cell>cycles</cell><cell>speedup</cell><cell>cycles</cell><cell>scheduling</cell><cell>scheduling</cell></row><row><cell>No optimizations</cell><cell>1.05</cell><cell>51</cell><cell>n.a.</cell><cell>n.a.</cell><cell>7</cell><cell>15</cell></row><row><cell>Loop unrolling by 4</cell><cell>1.12</cell><cell>61</cell><cell>1.19</cell><cell>23</cell><cell>6</cell><cell>16</cell></row><row><cell>Loop unrolling by 8</cell><cell>1.18</cell><cell>62</cell><cell>1.28</cell><cell>26</cell><cell>6</cell><cell>16</cell></row><row><cell>Trace scheduling with loop unrolling by 4</cell><cell>1.14</cell><cell>65</cell><cell>1.19</cell><cell>42</cell><cell>5</cell><cell>15</cell></row><row><cell>Trace scheduling with loop unrolling by 8</cell><cell>1.16</cell><cell>56</cell><cell>1.26</cell><cell>34</cell><cell>5</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Speedup relative to bal-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Speedup relative</cell><cell cols="2">anced scheduling with no</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>to locality analysis</cell><cell cols="2">unrolling and no trace</cell><cell></cell></row><row><cell>Optimizations</cell><cell></cell><cell></cell><cell>alone</cell><cell cols="2">scheduling</cell><cell></cell></row><row><cell>Locality analysis</cell><cell></cell><cell></cell><cell>n.a.</cell><cell>1.15</cell><cell></cell><cell></cell></row><row><cell>Locality analysis with loop unrolling by 4</cell><cell></cell><cell></cell><cell>1.11</cell><cell>1.28</cell><cell></cell><cell></cell></row><row><cell>Locality analysis with loop unrolling by 8</cell><cell></cell><cell></cell><cell>1.14</cell><cell>1.31</cell><cell></cell><cell></cell></row><row><cell cols="2">Locality analysis with trace scheduling and loop unrolling by 4</cell><cell></cell><cell>1.12</cell><cell>1.29</cell><cell></cell><cell></cell></row><row><cell cols="2">Locality analysis with trace scheduling and loop unrolling by 8</cell><cell></cell><cell>1.21</cell><cell>1.40</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In our machine model (described in the next section), the maximum load latency is 50 cycles, and occurs when a load must be satisfied by main memory. Hence, there is no need to hide more than 50 cycles for any load instruction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">These blocks were large enough to exploit load-level parallelism without loop unrolling, corroborated by the comparison of traditional and balanced scheduling (later in this section), in which BDNA had one of the largest relative reductions in interlock cycles.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank John O'Donnell from Equator Technologies, Inc., Michael Adler, Tryggve Fossum and Geoff Lowney from Digital Equipment Corp., Stefan Freudenberger from HP Labs, and Robert Nix from Silicon Graphics Inc. for access to the Multiflow compiler source and technical advice in using and altering it; both were invaluable. We also thank Dean Tullsen for the use of his Alpha simulator. This research was sponsored by NSF PYI Award #MIP-9058-439, ARPA contract #N00014-94-1136, and Digital Equipment Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Perfect Club: Effective performance evaluation of supercomputers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asprey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pointer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clementi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsiung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarzmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orszag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Supercomputer Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5" to="40" />
			<date type="published" when="1989-12">June 1993. December 1989</date>
		</imprint>
	</monogr>
	<note>IEEE Micro</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A 0.6um BiCMOS Processor with Dynamic Execution</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference &apos;95</title>
				<imprint>
			<date type="published" when="1995-02">February 1995</date>
			<biblScope unit="page" from="176" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Organization of the Motorola 88110 superscalar RISC microprocessor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Diefendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<date type="published" when="1992-04">April 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New CPU benchmark suites from SPEC</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Dixit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPCON</title>
				<imprint>
			<date type="published" when="1992-02">February 1992</date>
			<biblScope unit="page" from="305" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An overview of the 21164 Alpha AXP microprocessor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmondson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubinfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Chips VI</title>
				<imprint>
			<date type="published" when="1994-08">August 1994</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel processing: A smart compiler and a dumb machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruttenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN &apos;84 Symposium on Compiler Construction</title>
				<imprint>
			<date type="published" when="1984-06">June 1984</date>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Avoidance and suppression of compensation code in a trace scheduling compiler</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Freudenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Lowney</surname></persName>
		</author>
		<idno>HPL-93- 35</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Hewlett Packard</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Complexity/performance tradeoffs with non-blocking loads</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="1994-04">April 1994</date>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient instruction scheduling for a pipelined architecture</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Muchnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-PLAN &apos;86 Symposium on Compiler Construction, pages 11-16</title>
				<imprint>
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
		<title level="m">UltraSparc Unleashes SPARC Performance. Microprocessor Report</title>
				<imprint>
			<date type="published" when="1994-10-03">October 3, 1994</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">620 Fills Out PowerPC Product Line</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-10-24">October 24. 1994</date>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Postpass code optimization of pipeline constraints</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="422" to="448" />
			<date type="published" when="1983-07">July 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Balanced scheduling: Instruction scheduling when memory latency is uncertain</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kerns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN &apos;93 Conference on Programming Language Design and Implementation</title>
				<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<biblScope unit="page" from="278" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lockup-free instruction fetch/prefetch cache organization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Freudenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Karzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th Annual International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="1981-05">May 1981. 1993</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="51" to="143" />
		</imprint>
	</monogr>
	<note>Ruttenberg. The Multiflow trace scheduling compiler</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Alpha AXP architecture and 21064 processor</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mclellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="36" to="47" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inc. Mips Open RISC Technology R10000 Microprocessor Technical Brief</title>
		<imprint>
			<date type="published" when="1994-10">October 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design and evaluation of a compiler algorithm for prefetching</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<imprint>
			<date type="published" when="1992-10">October 1992</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Instruction Ordering for the Cray-1 Computer</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Sites</surname></persName>
		</author>
		<idno>78-CS-023</idno>
		<imprint>
			<date type="published" when="1978-07">July 1978</date>
			<pubPlace>San Diego</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The GNU Project Optimizing C Compiler</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stallman</surname></persName>
		</author>
		<imprint>
			<publisher>Free Software Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
