<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Guaranteed Smooth Scheduling For Input-Queued Switches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">I</forename><surname>Keslassy</surname></persName>
							<email>keslassy@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<address>
									<addrLine>Bell Laboratories Stanford University 101 Crawfords Corner Road Stanford</addrLine>
									<postCode>94305-9030, 07733</postCode>
									<settlement>Holmdel</settlement>
									<region>CA, NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Murali</forename><surname>Kodialam</surname></persName>
							<email>muralik@bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<address>
									<addrLine>Bell Laboratories Stanford University 101 Crawfords Corner Road Stanford</addrLine>
									<postCode>94305-9030, 07733</postCode>
									<settlement>Holmdel</settlement>
									<region>CA, NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Lakshman</surname></persName>
							<email>lakshman@bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<address>
									<addrLine>Bell Laboratories Stanford University 101 Crawfords Corner Road Stanford</addrLine>
									<postCode>94305-9030, 07733</postCode>
									<settlement>Holmdel</settlement>
									<region>CA, NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitrios</forename><surname>Stiliadis</surname></persName>
							<email>stiliadi@bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<address>
									<addrLine>Bell Laboratories Stanford University 101 Crawfords Corner Road Stanford</addrLine>
									<postCode>94305-9030, 07733</postCode>
									<settlement>Holmdel</settlement>
									<region>CA, NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Guaranteed Smooth Scheduling For Input-Queued Switches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DDA06182D7DE1CE1381ECB9FBE2014D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Input-queued switches are used extensively in the design of high-speed routers. As switch speeds and sizes increase, the design of the switch scheduler becomes a primary challenge, because the time interval for the matching computations needed for determining switch configurations becomes very small. Possible alternatives in scheduler design include increasing the scheduling interval by using envelopes [1], and using a framebased scheduler that guarantees fixed rates between input-output pairs. However, both these alternatives have significant jitter drawbacks: the jitter increases with the envelope size in the first alternative, and previously-known methods do not guarantee tight jitter bounds in the second.</p><p>In this paper, we propose a hybrid approach to switch scheduling. Traffic with tight jitter constraints is first scheduled using a frame-based scheduler that achieves low jitter bounds. Jitter-insensitive traffic is later scheduled using an envelope-based scheduler. The main contribution of this paper is a scheduler design for generating low-jitter schedules. The scheduler uses a rate matrix decomposition designed for low jitter and different from the minimum-bandwidth Birkhoff-Von Neumann (BV) decomposition. In addition to generating low-jitter schedules, this decomposition yields fewer switch configuration matrices (O(n)) than the BV decomposition (O(n 2 )), and so uses far less high-speed switch memory. We develop an efficient algorithm for decomposing the rate matrix and for scheduling the permutation matrices. We prove that our low-jitter algorithm has an O(log n) factor bound on its bandwidth consumption in comparison to the minimum-bandwidth BV decomposition. Experimentally, we find that the bandwidth increase in practice is much lower than the theoretical bound. We also prove several related performance bounds for our scheduler. Finally, we propose a practical bandwidth-guaranteed algorithm, and show how our findings could even be extended to systems with large tuning time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Most high-speed core routers today use input-queueing with a crossbar switch fabric, virtual output queues and fixedsize cells At each time-slot, a scheduler finds a matching between the inputs and the outputs, and configures the crossbar according to this matching. Many heuristic algorithms have been proposed for finding this matching <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, the need for routers with more ports and faster line rates makes these algorithms difficult to scale. For instance, with line rates of 40Gbps (OC768) and 64-byte cells, an algorithm would have to compute a matching every 12.8 ns, while the current highest-capacity commercially available centralized scheduler takes about 50 ns per matching. At least four alternatives have been proposed in the literature in order to decrease the frequency of the matching computation. First, by increasing * This work was done while the author was with Bell Labs. the cell size through the use of envelopes <ref type="bibr" target="#b0">[1]</ref>. Second, by using pipelining <ref type="bibr" target="#b4">[5]</ref>. Third, by using several times the same matching, possibly with slight changes <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this paper, we will be focusing on a fourth alternative: frame-based scheduling. In this approach, at the beginning of each frame period (for instance, every 100 time-slots), the scheduler is assigned a guaranteed rate table <ref type="bibr">(GRT)</ref>, which is a list of rate requirements. Then, it computes a list of matchings called the schedule table, such that during the following frame period, the schedule table will provide any input/output pair with at least as many services as the GRT would guarantee. Finally, the switch fabric transfers the inputqueued cells according to the matchings of the schedule table.</p><p>In order to compute the schedule table, most of the algorithms in the literature are based on the Birkhoff Von-Neumann (BV) decomposition <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. As explained in section 2, this BV algorithm minimizes the bandwidth requirement for the schedule and provides bandwidth guarantees.</p><p>It can be noted that this frame-based scheduling approach applies to several systems other than input-queued routers. First, it is used in SONET all-optical circuit-switches, which switch cells in circuit-based frames by using delay lines <ref type="bibr" target="#b11">[12]</ref>. Second, it applies to the satellite TDMA scheduling problem <ref type="bibr" target="#b12">[13]</ref>. Third, it is of interest in time-slotted wireless FDMA systems where any station can communicate with any other according to a TDM schedule often established by a centralized scheduler. And finally, it is also useful in star-based WDM broadcast-and-select optical systems with tunable transmitters and fixed receivers (or fixed transmitters and tunable receivers), where a centralized scheduler assigns a specific wavelength to a specific station for any time-slot <ref type="bibr" target="#b13">[14]</ref>. Therefore, although this paper will exclusively consider this problem in the context of input-queued router scheduling, most of the findings extend to these other systems.</p><p>For input-queued routers, this frame-based approach has several advantages. First, it solves the scheduling-frequency scalability issue mentioned above, which is one of the main bottlenecks in designing faster core routers. Second, it can be adapted to implement SONET, ATM, DiffServ, MPLS, and most (virtual) circuit-and frame-based schemes, which are predominant in the Internet core with which any Internet core router needs to interact. Third, it is one of the only inputqueued policies that provably guarantee 100% throughput for both a router in isolation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and a network of routers <ref type="bibr" target="#b15">[16]</ref>, and therefore guarantees to the operators that 0-7803-7753-2/03/$17.00 (C) 2003 IEEE <ref type="bibr">IEEE INFOCOM 2003</ref> the lines they expensively laid will be fully utilized. Fourth, the method for computing the frame weights is often flexible. Therefore, this flexibility can be used to provide specific bandwidth guarantees for any high-priority input/output pair, which could correspond to a specific customer. This flexibility can also be used to ensure fairness among flows, capping for instance the bandwidth usage of aggressive users. Finally, this method provides equal guarantees to uniform as well as nonuniform traffic, contrarily to many scheduling heuristics based on uniform traffic. However, frame-based scheduling suffers from an important drawback: it often results in large cell delays and large delay variability (jitter). This is especially true for an algorithm such as BV, which does not take delays into account. The objective of this paper is to define, study and approximate a class of smooth frame-based scheduling, which minimizes the jitter resulting from the schedule table.</p><p>For instance, consider the following simplistic example. Assume that for every frame period of 100 time-slots, we need to schedule the following GRT: GRT = 50 50 0 50 .</p><p>In this GRT, input 1 needs to send 50 cells to output 1 and 50 cells to output 2. Input 2 only needs to send 50 cells to output 2. Call M 1 the matching connecting input 1 to output 1 and input 2 to output 2, and call M 2 the matching connecting input 1 to output 2 and input 2 to output 1.</p><p>A straightforward implementation of BV, as explained later, would lead to a schedule table first implementing 50 times M 1 , then implementing 50 times M 2 , and so on periodically. However, the objective of this paper is to obtain instead a smooth scheduling, which implements M 1 , then M 2 , again M 1 , then M 2 , and so on.</p><p>There are several reasons to desire this smoother scheduling. First, as noted above, this smooth scheduling would reduce the delay variability of the traffic, which is one of the main objections made to frame-based scheduling. Also, not only is this a good property, but guaranteeing a given amount of bandwidth to low-jitter traffic is a requirement of DiffServ for Expedited Forwarding traffic <ref type="bibr" target="#b16">[17]</ref>. Therefore, if a router is to implement DiffServ, it has to find a method for implementing such guarantees, even though the jitter-sensitive traffic may only represent a small part of the total traffic. More generally, a typical implementation in core routers would first schedule traffic with tight jitter constraints using smooth frame-based scheduling, and then schedule the remainder bandwidth for jitter-insensitive traffic using an envelope-based scheduler <ref type="bibr" target="#b0">[1]</ref>.</p><p>Second, this smooth scheduling leads to less burstiness. For instance, in the example above, instead of receiving batches of 50 cells followed by 50 empty slots, output 2 will receive exactly one cell every two time-slots. Therefore, even though the throughput of the router itself is the same, a smoother scheduling may increase the throughput of a network of routers considered as a whole, because the routers down the line will receive a more regular traffic, thus allowing for better multiplexing effects. This is especially true for TCP traffic, for which a bursty scheduling may affect the round-trip time estimation and incur additional losses <ref type="bibr" target="#b17">[18]</ref>.</p><p>Third, a smoother scheduling results in better short-term fairness among flows, from a delay, jitter and bandwidth pointof-view.</p><p>Fourth, suppose that in the example above, the capacity of the second transmitter is only half of the line rate. The transmitter wouldn't then be able to use a BV scheduling, because it would be required to send at twice its capacity during half the time (send 50 packets in 50 slots, while it can only send 25 packets during this time). Therefore, smooth scheduling facilitates different per-port transmission (or reception) speeds. This is useful in heterogenous optical networks, with transmitters (and receivers) having different capacities. It is also useful in a wireless slotted FDMA system, in which the capacity may vary in time and depend on the position of the stations.</p><p>Finally, and perhaps most significantly, a smoother scheduling reduces the amount of buffering needed in the system. First, because a better multiplexing effect reduces the buffering needed in the following routers on the cell path, as explained above. But also because the instantaneous outgoing rates are better suited to the incoming rates. For instance, consider the second input in the example. If the incoming packets always come at the same rate, equivalent to 50% of the line rate, then input 2 will receive and buffer 25 cells during the 50 timeslots in which it is idle. With a smoother traffic, it wouldn't have to buffer more than one cell. While the effects of this second point are negligible in routers, they are crucial for optical, wireless and satellite systems, in which the buffers (or the optical delay-lines) incur significant power consumption, buffer (or delay-line) management complexity, and purchase cost.</p><p>Finally, in addition to finding a smooth scheduling algorithm, our objective is also to find a practical scheduling algorithm. First, because the BV algorithm needs O(n 2 ) permutations <ref type="bibr" target="#b8">[9]</ref>, these are difficult to store on a single chip when n is large (for n = 512, each permutation takes n • log 2 (n) = 4, 608 bits, hence we would need to store up to 150 Mbytes). Second, the BV algorithm complexity is in O(n 4.5 ) <ref type="bibr" target="#b8">[9]</ref>, and thus it is difficult to implement at high speeds.</p><p>The rest of the paper is organized as follows: We first outline the decomposition problem in Section II, provide a heuristic algorithm for solving this problem, and prove efficiency bounds for this algorithm. Then, in Section III, we explain how to use the decomposition algorithm in order to compute a schedule table that achieves low jitter. Section IV is devoted to experimental results. Section V develops a practical algorithm for bandwidth-guaranteed traffic, and finally section VI extends these results to long-tuning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DECOMPOSITION OF THE GUARANTEED RATE TABLE</head><p>We consider an n × n input-output buffered packet switch with virtual output queueing. In each time slot, a central arbiter matches each output to at most one input. We will assume</p><formula xml:id="formula_0">0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</formula><p>that for the portion of the traffic that has bandwidth and jitter requirements, we have some knowledge of the rates required for each input/output port pair. Within the Internet architecture this knowledge is supplied either through a bandwidth broker or through MPLS signaling.</p><p>We are concerned only with the scheduling of the bandwidth guaranteed low jitter traffic that is specified by a (Guaranteed) Rate Matrix. Best effort traffic can be scheduled using any of the matching algorithms available in the literature.</p><p>Let P represent the per-port capacity of the fabric. Let the n × n non-null matrix R = [r ij ] represent the Rate Matrix where r ij represents the rate required from input i to output j. Let M represent the maximum row or column sum of the matrix R. Using standard techniques <ref type="bibr" target="#b8">[9]</ref>, it is easy to augment R into a matrix R = [r ij ] where all the row and column sums of R are M . For the rest of this paper we will assume that all the row sums and column sums of R are M . When M is not given, it is assumed that M = 1 -and in this case, R is called a Doubly Stochastic Matrix. Finally, note that the rate matrix can be scheduled (without jitter constraints) if and only if the aggregate amount of traffic going to (respectively coming from) any port is less than the port capacity, i.e. M ≤ P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Birkhoff Von-Neumann Decomposition</head><p>The basic idea of the Guaranteed Rate Tables is that once a rate matrix is provided, it is decomposed into schedule tables. Each schedule table is either a permutation or a partial permutation matrix (i.e. a 0 -1 matrix whose row sums and column sums are at most one without it being a permutation matrix). The standard approach to creating the schedule tables from the rate matrix relies on the following result due to Birkhoff and Von-Neumann <ref type="bibr" target="#b8">[9]</ref>.</p><p>Theorem 1: Any doubly stochastic matrix can be written as a convex combination of permutation matrices. Therefore the BV decomposition of the rate matrix R is to generate a set of permutation matrices (schedule tables)</p><formula xml:id="formula_1">Y k for k = 1, 2, . . . , K such that R = K k=1 α k Y k .</formula><p>In other words</p><formula xml:id="formula_2">r ij = K i=1 α k Y k ij .</formula><p>We refer to K k=1 α k as the bandwidth requirement of the schedule tables generated by the BV decomposition. After scaling the results of Theorem 1 by a factor of M , we can see that the value of</p><formula xml:id="formula_3">K i=1 α k = M ,</formula><p>where M is the row and column sum of the rate matrix R. Thus, the BV decomposition minimizes the bandwidth requirement. The number of matrices in the BV decomposition is O(n 2 ) and the running time of the algorithm is O(n 4.5 ). More details and references on the decomposition algorithm can be found in <ref type="bibr">Chang et al. ([9]</ref>, <ref type="bibr" target="#b9">[10]</ref>). The permutation (switching) matrices are scheduled across the switch using some Weighted Round Robin (WRR) scheme. Chang et al. <ref type="bibr" target="#b8">[9]</ref> gives a scheduling algorithm and provides bounds on its performance. This method that is based on the BV decomposition is reasonable in the case that there are no jitter constraints. The BV decomposition, however, results in poor jitter performance especially when there is a large number of ports in the switch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Low Jitter Decomposition</head><p>In the BV decomposition of the rate matrix R, a given entry r ij is striped across several permutation matrices. Therefore, independently of the type of algorithm used to schedule the permutation matrices, there is no control on when individual entries in the rate matrix will be scheduled. It is possible to derive bounds on the jitter <ref type="bibr" target="#b8">[9]</ref>, but it is not possible to ensure that the jitter is low. The bounds on the jitter for the traffic between input port i and output port j depends on the number of matrices in the decomposition that r ij is striped across and also on the number of matrices in the decomposition. Since both these factors increase with the number of ports in the switch, the jitter problem becomes severe when the number of ports is large. We formulate an alternate decomposition which we term Low Jitter (LJ) Decomposition. As in the case of the BV decomposition, we decompose the rate matrix into a combination of permutation matrices with the additional restriction that each non-zero entry in the rate matrix appears in only one of the permutation matrices. Let X k , k = 1, 2, . . . K be the set of permutation matrices that form the LJD. Associated with each matrix X k in the decomposition is a rate m k which represents the bandwidth requirement for the switching matrix</p><formula xml:id="formula_4">X k . k m k x k ij ≥ r ij ∀i, j<label>(1)</label></formula><formula xml:id="formula_5">k x k ij = 1 ∀i, j (2) i x k ij ≤ 1 ∀j, k (3) j x k ij ≤ 1 ∀i, k (4) x k ij ∈ {0, 1} ∀i, j, k.<label>(5)</label></formula><p>Constraints (3)(4) and ( <ref type="formula" target="#formula_5">5</ref>) specify that X k is a partial permutation matrix. Constraint (1) specifies that the weighted sum of these permutation matrices should be greater than the rate matrix. Constraint (2) enforces that each entry in the rate matrix belongs to precisely one element in the decomposition. Note, that unlike the BV decomposition which uses only (full) permutation matrices, LJ decomposition splits the rate matrix into partial permutation matrices. Using the scheduling algorithm developed in the next section, we show that the above set of constraints are sufficient to guarantee low jitter. It is possible to construct specific examples where constraint (2) is not necessary to guarantee low jitter. We will comment on this in the last section of the paper. The bandwidth requirement for the schedule is K i=1 m k . Therefore the objective of the LJ decomposition is to solve the following integer programming problem (ILJD):</p><formula xml:id="formula_6">D = min K k=1 m k 0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</formula><p>Subject to the constraints (1)(2)(3)(4)(5) defined above. Since the BV decomposition solves the above problem without constraint (2), and the schedule length of BV is at most M , then D ≥ M . Therefore there will be rate matrices that are schedulable by the BV decomposition but not by the LJ decomposition. This will especially be true if the amount of low jitter guaranteed traffic is a large fraction of the total switch traffic. In the applications that we consider the lowjitter traffic is a relatively small fraction of the total switch traffic. This is a valid assumption within the context of the EF class, that is expected to occupy less than 50% of the total bandwidth. In our experiments we varied the low-jitter traffic load from 10% to 60% and all the rate matrices were schedulable with LJD. As in the case of BV decomposition, the LJ decomposition of the rate matrix is not unique. Unlike the BV decomposition, the integer programming problem ILJD is NP-hard. The proof of the theorem below is a simple variant of a proof due to Rendl <ref type="bibr" target="#b18">[19]</ref>. (In <ref type="bibr" target="#b18">[19]</ref> the number of matrices in the decomposition is restricted to n, which is not the case here.) Theorem 2: The problem ILJD is NP hard. Several flavors of matrix decomposition problems have been studied extensively in the TDMA scheduling literature <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. As in our case, the two most important considerations in TDMA scheduling (especially for satellite scheduling) are to minimize the number of matrices in the decomposition and the total bandwidth needed to support the decomposition. However, the TDMA literature does not deal with low-jitter implementation of the rate matrix decomposition. Since the ILJD problem is NP-hard, our objective is to derive lower bounds on solutions to this problem, and then to use these lower bounds in order to get intuition and motivate a heuristic for solving the problem and implementing a low-jitter decomposition. We first prove a lemma on vector-ordering, and then use this lemma in order to find a lower bound on D. Note that proofs are placed in the appendix for ease of reading.</p><p>Lemma 3: Consider two vectors (v 1 , v 2 , . . . , v p ) and (w 1 , w 2 , . . . , w p ) where v 1 ≤ v 2 ≤ . . . ≤ v p . Then a permutation σ of the set {1, 2, . . . , n} that minimizes</p><formula xml:id="formula_7">p i=1 max{v i , w σ(i) } is a permutation where w σ(1) ≤ w σ(2) ≤ . . . ≤ w σ(p) .</formula><p>Theorem 4: Let R be a rate matrix. Sort each column of the matrix R in descending order to get the matrix R . Compute the maximum g i of each row i of the matrix R . Then n i=1 g i is a lower bound on D. Similarly sort each row of R in descending order to get the matrix R . Compute the maximum h j of each column j of R . Then Consider now a particular matrix X k in the LJ decomposition. The rate m k associated with this matrix is the largest entry in R that is covered by this matrix. Therefore the total amount of bandwidth can be optimized by covering entries of roughly equal size with the same matrix. Of course, the entries should not share the same row or column in order to be in a decomposition. This is the basis of the Greedy Low-Jitter Decomposition (GLJD) which is a heuristic to solve the integer programming problem ILJD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Greedy Low-Jitter Decomposition</head><p>The GLJD algorithm is defined as follows. First, the elements in R are sorted in descending order and put in an ordered list L. Note that each element in L corresponds to some element R ij in the matrix R. Two elements in L are defined to be non-conflicting if these two elements do not belong to the same row or column in R. Then, we fill up each of the decomposition sub-matrices by traversing the list from the top and picking non-conflicting elements greedily. Once an element is picked to be inserted into the decomposition sub-matrix, the element is deleted from L. Once the end of </p><formula xml:id="formula_8">: ∀m ∈ L, if L(m) = r ij then set ρ(m) = i and κ(m) = j. 3: k = 1. 4: While L = ∅ Set m k = 0 and l = 1 Set C[j] = 0 ∀j = 1, 2, . . . , n While (Not end of list L) If (C[ρ[l]] = 0 and C[κ(l)] = 0) X k ρ(l)κ(l) = 1 C[ρ(l)] = C[κ(l)] = 1 If (L(l) &gt; m k ) m k = L(l) Eliminate entry l from list L End If l ← l + 1 End While k ← k + 1</formula><p>End While 5: Output the matrices X k and the corresponding weights m k . the list is reached, then a new decomposition sub-matrix is initiated. This process is repeated until all the elements of L are inserted into some sub-matrix. It is easy to see that the worst case running time of the algorithm is O(n 3 ). The description of the GLJD algorithm is shown in Figure <ref type="figure" target="#fig_1">1</ref>. For the numerical example above, the GLJD algorithm generates the following decomposition. Therefore if the low-jitter traffic load is less than 1  1.48 = 0.68 then the LJ heuristic can guarantee a full capacity to the traffic. This is useful because, in practical situations, we expect the low-jitter traffic to not be a sizable portion of the traffic across the switch. Also, it can be noted that the bandwidth is not necessarily wasted just because the LJ heuristic has a higher value than the BV decomposition. Indeed, any bandwidth not used by the low-jitter traffic class can then carry best-effort traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Guarantees on the GLJD Algorithm</head><p>The primary objective of the GLJD algorithm is to enable traffic scheduling with low-jitter guarantees. However, the following theorems prove that it also provides additional guarantees, with upper bounds on the number of partial permutations needed, the bandwidth required for the scheduling and the competitive ratio with respect to the optimal algorithm.</p><p>For the sake of simplicity, in this section, we will always assume that M = 1, and thus a rate matrix R has to be doubly stochastic with line and column sum 1. The results will then be easy to generalize to any given M . The following theorem provides an upper bound on the number of partial permutations used by GLJD. It derives from a more general result on greedy on-line edge coloring <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Theorem 5: Let K be the number of partial permutation matrices needed in the GLJD algorithm. Then</p><formula xml:id="formula_9">K ≤ 2n -1.</formula><p>We have just found that the GLJD algorithm provides a guarantee on the number of matrices that it will use. We are now interested in the worst-case bandwidth that it will require. More generally, define the length of the schedule provided by an algorithm ALG for a rate matrix R as T ALG (R) = K k=1 m k . Then, define the worst-case bandwidth requirement of algorithm ALG as:</p><formula xml:id="formula_10">BW (ALG) = max R (T ALG (R)),</formula><p>where the maximum is taken over the set of rate matrices R. Then, the next theorem guarantees that BW (GLJD) = O(log n).</p><p>Theorem 6: Let n ≥ 2 and let H represent the harmonic series (</p><formula xml:id="formula_11">H n = n k=1 1/k). Then BW (GLJD) ≤ 2H n -1 ≤ 2 ln(n) + 1.</formula><p>How good is the upper bound provided by this theorem? Theorem 6 states that GLJD guarantees at least 1 2Hn-1 of the bandwidth to low-jitter traffic. Therefore, GLJD guarantees at least 11.8% of the bandwidth to low-jitter traffic when n = 64, and 7.1% when n = 1024.</p><p>However, during simulations, the bandwidth generally guaranteed to low-jitter traffic was around 60% with n = 64, as described later. One may thus wonder if BW (GLJD) really grows as Θ(log n ) and reaches this worst-case bound. The objective of the next theorem is to show that BW (GLJD) indeed grows as Θ(log n ). In addition, this theorem will prove that for any algorithm ALG which satisfies the conditions of the integer programming problem, BW (ALG) grows at least as fast as Θ(log n ). In particular, if ILJD is an optimal such algorithm, i.e. D(R) = T ILJD (R) for any rate matrix R, then the bandwidth-guarantee competitive ratio BW (GLJD) BW (ILJD) is bounded.</p><p>Theorem 7: For any n ≥ 3, there exists at least one rate matrix R n with the following lower bound on the length of its optimal LJ decomposition: Is this bound meaningful? The fact that the bandwidthguarantee competitive ratio of GLJD has a constant upper bound is indeed meaningful. This result implies that the bandwidth guarantee of any other algorithm satisfying the lowjitter decomposition would not better the bandwidth guarantee of GLJD by much as the scheduling complexity increases with n, contrarily to what one would have intuitively thought when considering the Θ(log n) behavior of BW (GLJD). However, note that the bound on the competitive ratio is not tight, and that the following theorems will prove in particular that this ratio is at most 2.</p><formula xml:id="formula_12">D(R n ) ≥ ln(n + 2) 2 ln 2 - 1 2 .</formula><p>We are now interested in the performance of the GLJD algorithm with any given rate matrix R, rather than with its worst-case rate matrix. The following theorems will slightly improve the lower bound on D(R) = T ILJD (R) provided by Theorem 4, and then prove that for any rate matrix R,</p><formula xml:id="formula_13">TGLJD(R) TILJD(R) ≤ 2 -1</formula><p>n . This means that for any matrix, the bandwidth required by GLJD is less than twice the bandwidth required by the optimal low-jitter decomposition. A consequence of this result is that the bandwidth-guarantee competitive ratio is also less than 2 -1 n . Theorem 9: For any given schedule, assume without loss of generality that m 1 ≥ m 2 ≥ ... ≥ m K . Then for any k ∈ {1, . . . , n},</p><formula xml:id="formula_14">m k ≥ max(g k , h k ), and<label>(6)</label></formula><formula xml:id="formula_15">D(R) = T ILJD (R) ≥ n k=1 max(g k , h k ).<label>(7)</label></formula><p>Theorem 10: For any rate matrix R,</p><formula xml:id="formula_16">T GLJD (R) T ILJD (R) ≤ 2 - 1 n .</formula><p>Corollary 11: The bandwidth-guarantee competitive ratio of the GLJD algorithm is upper-bounded by 2 -1 n . Itis interesting to note that it is possible to derive a doublystochastic bipartite version of the worst-case matching in <ref type="bibr" target="#b23">[24]</ref> in order to prove that this bound of 2 is actually tight for large n.</p><p>We now outline the scheduling algorithm that will be used to schedule the matrices generated by the LJ decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SCHEDULING THE LJ DECOMPOSITION</head><p>Since r ij is the desired rate from input port i to output port j, we ideally want the time slots when i is matched to j to be spaced 1 rij apart, i.e., the points 0, 1 rij , 2 rij , 3 rij , . . .. Given that there are multiple connections that share the same bandwidth, this is not possible in general. We settle for a slightly degraded jitter performance. We call a connection Low Jitter if we have exactly one match for this connection in each of the time intervals m rij , m+1 rij , m = 0, 1, 2, . . . . At the end of the LJ decomposition, we have K (perhaps partial) permutation matrices. Let m k be the bandwidth for the switching matrix X k and let D = K k=1 m k . We assume that D &lt; P, where P is the switch speed. We define φ k = m k P , where φ k represents  the fraction of timeslots that should use the schedule table (switching matrix) X k . We assume that K k=1 φ k &lt; 1. We want to schedule matrix k at rate φ k . Since each port pair belongs in exactly one matrix in the decomposition, it is possible to control the jitter for each port pair individually. Let T j k represent the time slot in which the schedule table k is scheduled for the j th time. For low jitter, we want</p><formula xml:id="formula_17">j φ k ≤ T j k ≤ j + 1 φ k .</formula><p>We compute as follows the start time S j k and the finish time F k j for the j th schedule of table k:</p><formula xml:id="formula_18">S j k = j φ k F j k = j + 1 φ k .</formula><p>In the implementation of the scheduling algorithm we define iteratively A k and B k , respectively the current start time and the current finish time for schedule table k, as shown in Figure <ref type="figure" target="#fig_5">2</ref>.</p><p>At the beginning of time slot t, schedule table k is defined to be eligible if S k ≤ t and ineligible otherwise. The scheduling algorithm works as follows: At the beginning of each time slot, among all the eligible schedule tables, the table with the smallest finishing time is picked for scheduling. Let l represent this table. The start time and the finish time for schedule table l is updated. This process is then repeated. If no class is eligible at the beginning of a given time slot, then the inputs are matched to the outputs by some algorithm that optimizes the performance of best effort traffic. Best effort traffic also uses the slots where guaranteed jitter traffic is not available to take its allotted slot or in the case where the schedule table is a partial permutation table. The following theorem is a special case of the general result in <ref type="bibr" target="#b24">[25]</ref>. The proof of the result in our case is straightforward and is proved directly.</p><p>Theorem 12:</p><formula xml:id="formula_19">If K k=1 φ k &lt; 1, then for all classes k, the LJS algorithm results in j φ k ≤ T j k ≤ j + 1 φ k + 1.</formula><p>Therefore by the above theorem, all connections are (almost) low jitter. Note that some connections may miss their allotted range by one time slot. Under the moderate loading 0-7803-7753-2/03/$17.00 (C) 2003 IEEE conditions that we consider, this seems to happen very rarely. In the experimental section, we compare the LJ decomposition with the scheduling algorithm described above with the BV decomposition and the scheduling algorithm outlined in Chang et al. <ref type="bibr" target="#b8">[9]</ref>. The main difference between the scheduling algorithm described above and the one in <ref type="bibr" target="#b8">[9]</ref> is that the scheduling algorithm in <ref type="bibr" target="#b8">[9]</ref> does not use starting times.We ran the BV decomposition algorithm with our scheduling algorithm and the results for BV are not qualitatively different from the ones shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>The experimental results are shown to illustrate the advantages in jitter performance of the LJD. In the simulations, we consider 64 × 64 and 128 × 128 switches in the experiments, and assume that the switches process 100 slots per second. It is also assumed that the guaranteed rate for each input-output pair is independently and uniformly distributed between 0 and some upper bound u. The value of u determines the mean guaranteed rate traffic load in the switch. We adjust the rate to ensure that all row and column sums are the same. If the row and column sums are different for different rows and columns, then the performance of the algorithms are qualitatively similar to the results for the balanced case. We measure the following quantities:</p><p>Jitter Performance: For a given input-output pair (i, j), the LJ decomposition will result in one match in each interval m rij , m+1 rij , for m = 0, 1, 2, . . . In the case of the BV decomposition some intervals will get multiple matches and others will get none. In Figure <ref type="figure">3</ref> we plot the interval number versus the number of matches produced by the different decompositions. Results are shown for a particular inputoutput port combination, but other input-output ports had similar results. The results show a very poor jitter performance for the BV decomposition. There are no matches for almost 20 intervals between 40 and 60, and almost 12 matches in the interval 39. On the contrary, GLJD results in one match per interval as predicted by Theorem 12. For BV, performance was even worse with the 128 × 128 switch, not shown here.</p><p>Speedup Ratio: This represents the ratio of the bandwidth requirement of the GLJD heuristic to the bandwidth requirement of BV. Recall that the bandwidth requirement of BV decomposition is the row sum M of the rate matrix. We show the result for 10 experiments in Figure <ref type="figure">4</ref>. In our experiments we varied the low-jitter traffic load from 10% to 60%, and in all these cases the schedule table obtained by the GLJD was feasible. In other words, the throughput requirement was not higher than the switch capacity. As stated earlier, in practice, we expect the low-jitter traffic load to be less than 50% of the switch traffic.</p><p>Optimality Gap: We compute the ratio of T GLJD to the lower bound D = T ILJD generated by applying Lemma 3. This ratio tracks the performance of GLJD to the optimal solution. The result is shown in Figure <ref type="figure">5</ref>. Note that in all the cases, the heuristic is within 10% of the optimal solution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Matrices in the Decomposition:</head><p>We compare the number of matrices in BV and GLJD. Lower number of matrices results in easier scheduling algorithms. The results are shown for a 64 × 64 and 128 × 128 switch in Table <ref type="table">I</ref> . With n = 64, BV typically generates about 850 matrices versus about 70 matrices for GLJD. Similarly, BV results in about 1600 matrices and GLJD results in about 135 matrices for n = 128. Therefore the size of the schedule table is typically an order of magnitude less for GLJD with respect to BV.</p><p>Total Computation Time: We compare the total computation time to run both the BV and the LJ decompositions in Table <ref type="table">II</ref> . The runs were done on a 1 Ghz Pentium processor. All run times are in seconds of system time. For the BV decomposition we run a fast maximum flow algorithm at each step in order to determine the next switching matrix in the decomposition. Note that although both these computation times could be minimized in hardware implementations, we believe that the LJ decomposition (using GLJD) will still be significantly faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. A PRACTICAL ALGORITHM FOR BANDWIDTH-GUARANTEED TRAFFIC</head><p>Assume that a router designer is to build a frame-based scheduler for bandwidth-guaranteed traffic with little or no jitter constraint. What algorithm should this designer use?</p><p>In a mathematical formulation, the designer needs to consider a rate matrix R with real (floating point) coefficients, and find an algorithm that would guarantee a schedule to R with small speed-up (O(1)), a number of permutations small enough to be stored on a single chip (say O(n)), and a complexity small enough to be implemented at core-router speeds.</p><p>For solving this problem, we have considered many algorithms proposed in the literature. Most of them are based on a common divisor in R and thus require too many permutations, because the common divisor in router implementations might be as small as the floating point precision. Thus, this excludes algorithms related to call routing in Clos networks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, to openshop scheduling <ref type="bibr" target="#b27">[28]</ref>, to heuristic row bottleneck minimization in optical WDMA star networks <ref type="bibr" target="#b28">[29]</ref> and in TDM switching <ref type="bibr" target="#b29">[30]</ref>, and to bipartite graph matching <ref type="bibr" target="#b12">[13]</ref>. Alternative algorithms which do not require a common divisor are often too impractical because they require optimal edge coloring in bipartite graphs, such as BV <ref type="bibr" target="#b8">[9]</ref> and a traffic rate approximation scheme proposed in <ref type="bibr" target="#b30">[31]</ref>.</p><p>Our approach is inspired by this last scheme. However, contrarily to this scheme, we will use maximal matchings <ref type="bibr" target="#b12">[13]</ref> instead of optimal edge coloring. Maximal matchings are faster, but provide less guarantees.</p><p>We thus propose the following iterative maximal framescheduling. First, define an integer d, which will be a common divisor in the coarse matrix, such that d = Θ(n) -for instance, d = n, d = 4n, d = 2 log 2 (n) , and so on.</p><p>Then, given the rates r ij , compute</p><formula xml:id="formula_20">u ij = d • r ij and ij = r ij - uij d .</formula><p>Note that when d is a power of 2 and numbers are represented in a binary form, this computation simply truncates the first digits of r ij . U 1 = U = [u ij ], the coarse approximation matrix, is a nonnegative-integer matrix with row and column sum of at most d. = [ ij ] = R -U d , the remainder matrix, is such that for all {i, j}, 0 ≤ ij ≤ 1 d . Then, apply a maximal matching algorithm to U 1 , yielding (partial) permutation σ 1 , and compute U 2 = U 1 -σ 1 . Apply again a maximal matching to U 2 , yielding σ 2 , and compute U 3 = U 2 -σ 2 . Continue likewise 2d -1 times, until obtaining 2d -1 (partial) permutations {σ 1 , ..., σ 2d-1 }.</p><p>Finally, for 1 ≤ k ≤ n, call π k the permutation that connects any input i to output i+k mod n, and consider the set S = {σ 1 , ..., σ 2d-1 , π 1 , ..., π n }. Then the following theorem proves that a schedule table using each of the permutations in S exactly once in a cycle of d time-slots satisfies the rate matrix requirements.</p><p>Theorem 13: At the end of the algorithm defined above,</p><formula xml:id="formula_21">2d-1 k=1 σ k d ≥ U d , n k=1 π k d ≥ , and therefore, 2d-1 k=1 σ k + n k=1 π k d ≥ R. Assume that d = Θ(n).</formula><p>The resulting decomposition satisfies our objectives. First, it uses at most 2d + n -1 = O(n) permutations in the schedule table, thus these permutations will typically hold on a chip. Second, it needs a finite speed-up of 2+ n-1 d = O(1) in order to guarantee 100% throughput. Finally, it takes a computation time of O(n 3 ) with a centralized scheduler, and O(n) with a distributed scheduling, as shown below. It can also be noted that the jitter guarantee is in O(n) (the length of the schedule), hence this is an alternative for traffic not needing the extremely low jitter implemented with GLJD.</p><p>Also, all the maximal algorithms used in current routers can be extended to frame scheduling, and in particular as well those based on pipelining such as WFA <ref type="bibr" target="#b1">[2]</ref> as those based on a request-grant-accept arbitration type such as iSlip <ref type="bibr" target="#b3">[4]</ref>. This is interesting for legacy issues. The pipelined version of WFA seems especially suited for such an iterative algorithm, since crosspoint (i, j) can start working on iteration k + 1 of the decomposition algorithm at the same time as crosspoints (i, j+ 1) and (i + 1, j) start working on iteration k <ref type="bibr" target="#b1">[2]</ref>. Therefore, for any given crosspoint, there are 2d -1 iterations, with at most one computation and two transmissions of information per iteration. Hence, the distributed algorithm runs in</p><formula xml:id="formula_22">O(d) = O(n) time.</formula><p>Finally, from a practical point of view, the main advantage of such a scheme is that it can easily support incremental rate updates without computing a whole new schedule again. For instance, assume that r 1 ij is updated into r 2 ij . Compute the change in the integer coarse matrix δ = u 2 ij -u 1 ij . If δ = 0, do nothing. If δ &lt; 0, remove δ elements in position (i, j) from the set {σ k }. If δ &gt; 0, add δ elements in position (i, j) to the set {σ k } (possible because the set contains 2d -1 elements <ref type="bibr" target="#b12">[13]</ref>). Hence, with a correct data structure, an update takes O(n) time. Interestingly enough, this feature is related to the ease for establishing and removing calls in a non-blocking Clos network with 2d -1 middle-stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. A LAST WORD ON BURSTY DECOMPOSITION ALGORITHMS</head><p>A surprising side-effect of the greedy smooth scheduling provided in this paper is that it can actually be helpful when looking for the least smooth scheduling possible. For instance, consider an input-queued router using an optical switch fabric. Note that this fabric could be passive (for instance, based on an arrayed waveguide grating with tunable lasers <ref type="bibr" target="#b31">[32]</ref>) as well as active (for instance, based on micro-mirrors <ref type="bibr" target="#b32">[33]</ref>). Similarly, consider a star-based WDM broadcast-and-select optical system <ref type="bibr" target="#b13">[14]</ref>. In those two cases, the tuning time from one channel to another takes a major share of the frame schedule time <ref type="bibr" target="#b30">[31]</ref>. Thus, the main issue is not anymore the bandwidth available, but rather the number of tuning times in a given schedule: the designer will prefer a schedule with very few permutations, and thus few tunings, even if it implies an increased burstiness.</p><p>The primary objective is therefore to minimize as much as possible the number of permutations in a schedule, in order to minimize the number of tuning times, and a secondary objective is to minimize the bandwidth taken by those permutations. Towles and Dally <ref type="bibr" target="#b30">[31]</ref> propose a heuristic algorithm for this problem called MIN. MIN minimizes the number of permutations used, and therefore uses at most n permutations. However, MIN needs a speed-up of at least Θ(log n) in order to schedule any rate matrix. Also, MIN needs to perform several times edge coloring in a bipartite graph, and is therefore difficult to implement in a core router. GLJD could thus be a practical algorithm for approximating MIN. First, GLJD needs at most 2n -1 permutations (Theorem 5), and in simulations the number of permutations is shown to be very close to n. Second, GLJD also needs a speed-up of at most Θ(log n), with simulations results close to a speed-up of 1.5. And finally, GLJD was shown to have a low implementation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper we considered the problem of scheduling guaranteed-bandwidth low-jitter traffic in input-queued switches. We proposed to combine the GLJD algorithm and a Low Jitter Scheduler, and found experimentally that they are both both practical and efficient. We provided several performance bounds on these algorithms, with respect to the number of permutations as well as the schedule time. We proposed a practical algorithm for scheduling the remaining jitter-insensitive bandwidth-guaranteed traffic, and found that it supports incremental updates. Finally, we showed how our findings could even be extended to non-smooth systems with large tuning time. APPENDIX Proof for Lemma 3: Let σ be an optimal permutation. For some k &lt; l let w σ(k) ≥ w σ(l) . Let v l = max{w σ(k) , v l }. By interchanging w σ(k) and w σ(l) note that the objective function value is non increasing. The same result can be shown if</p><formula xml:id="formula_23">w σ(k) = max{w σ(k) , v l }.</formula><p>Proof for Theorem 4: We show the result for the case where the rows are sorted. The proof for the column sorting is identical. Assume that the decomposition has n matrices. It is easy to show that the lower bound is valid if there are more than n matrices in the decomposition. Consider the first row of the matrix R. Assume without loss of generality that the columns of R are permuted so that the first row is sorted in descending order. Each entry in the first row has to belong to a different matrix in the decomposition. Each entry in the second row has to belong to a different matrix in the decomposition. If we ignore the constraint that each column in each decomposition matrix has to have exactly one entry, then the minimum amount of bandwidth that we need for the first two rows, by Lemma 3 is given when the second row is in sorted order. It is a lower bound because we ignore one of the constraints. This argument can be repeated in order to show the result.</p><p>Proof for Theorem 6: Let R be a rate matrix, and assume that r ij is an element of R that still remains to be scheduled after k iterations of the GLJD algorithm. Consider the set of all elements in L that belong to the same row or column as r ij and have at least the same weight. Then, during the first k iterations, the algorithm picked at least k elements from this set (by construction of this greedy maximal algorithm, see also <ref type="bibr" target="#b12">[13]</ref>).Thus, according to the pigeonhole principle, the algorithm picked at least k 2 of these elements either in row i or in column j -without loss of generality, let's assume that it was in row i. Therefore, on row i, there were initially at least 1 + k 2 elements with a rate of at least r ij (counting r ij itself). Hence, since the row sum is 1, r ij ≤ 1 1+ k 2 . We thus know that after k iterations, all remaining elements have a rate of at most 1 ). Using m 1 = max i,j (r ij ) ≤ 1 and K ≤ 2n -1 (Theorem 5), we get</p><formula xml:id="formula_24">K k=1 m k ≤ 1 + K-1 k=1 1 1 + k 2 ≤ 1 + 1 2 + 1 2 + 1 3 + 1 3 + • • • + 1 n + 1 n , i.e. T GLJD (R) ≤ 2H n -1. Using the inequality 1 k ≤ k k-1 (<label>1</label></formula><p>x ) dx = ln(k) -ln(k -1) for k ≥ 2, one gets H k ≤ 1 + ln(k) for k ≥ 2, hence the result of the theorem.</p><p>Note that this proof shows that for if r ij is unscheduled after k ≥ 1 iterations, then r ij is at least the (1 + k 2 ) th element of either its row or column. Thus a finer approximation is m k+1 ≤ max(g 1+ k n k=2 max(g k , h k ) (because g 1 = h 1 ). We can then get the result of the theorem by using the upper bound max(g k , h k ) ≤ 1 k for k ≥ 1. Proof for Theorem 7: For any n ≥ 3, define k such that 2 k+1 -1 ≤ n &lt; 2 k+2 -1. Let R n be the following blocdiagonal doubly-stochastic n × n matrix: </p><formula xml:id="formula_25">Rn =           </formula><formula xml:id="formula_26">           </formula><p>, where for any k ≥ 1, A k is the uniform doubly stochastic 2 k × 2 k matrix of sum 1:</p><formula xml:id="formula_27">A k = 1 2 k ij</formula><p>. R n is an n × n doubly stochastic rate matrix (as in <ref type="bibr" target="#b30">[31]</ref>). Theorem 4 provides the following lower bound on the length of the optimal LJ decomposition of R n : Proof for Theorem 9: Let's prove that m k ≥ g k for a given k ∈ {1, ..., n}. By definition of g k , there is an element r ij in the matrix R such that r ij = g k , and r ij has the k th weight of its column j. In other words, in column j, there are at least k elements with the same weight as r ij . Since two elements from the same column can't be scheduled at the same time, by the pigeonhole principle, at least one of these k elements does not belong to the first k -1 permutations -say it is r i j . r i j thus belongs to some permutation k such that k ≤ k ≤ K. But then m k ≥ m k ≥ r i j ≥ r ij = g k . Similarly, m k ≥ h k .</p><formula xml:id="formula_28">D(R n ) ≥ 1 + 1 2 + 2 4 + • • • + 2 k-1 2 k =</formula><p>Proof for Theorem 10: For k ∈ {1, ..., n}, let f k = max(g k , h k ). Proof for Theorem 13: The inequality 2d-1 k=1 σ k ≥ U is a property of maximal matchings <ref type="bibr" target="#b12">[13]</ref>. By definition, ≤ <ref type="bibr" target="#b0">[1]</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>n j=1 h j is a lower bound on D. Hence D ≥ max( n i=1 g i , n j=1 h j ). The higher of the two lower bounds computed above is a lower bound on D and is tighter than M . Let's illustrate the bounds of this theorem on the 4 × 4 doubly-stochastic rate matrix R introduced in [9]. First we sort each column of R in descending order to obtain matrix R as per Theorem 4. We find the sum of the maximum of each row to get n i=1 g i = 0.60 + 0.38 + 0.23 + 0.05 = 1.26. Now we sort each row to obtain matrix R . R =   0.40 0.38 0.22 0 0.60 0.24 0.11 0.05 0.53 0.33 0.14 0 0.51 0.23 0.22 0.04   We find the sum of the maximum of each column to get n j=1 h j = 0.60 + 0.38 + 0.22 + 0.05 = 1.25. Therefore the lower bound for this problem is 1.26. As a comparison, we solved the integer programming problem (ILJD) using the CPLEX optimization program. The ILJD optimization yields: Therefore the optimal LJ decomposition has a value D = 0.60 + 0.38 + 0.33 + 0.05 = 1.36. We can check 1.26 ≤ D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Description of the Greedy Low Jitter Decomposition Algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>schedule time of 0.60 + 0.38 + 0.23 + 0.22 + 0.05 = 1.48 (as expected, it is greater than the optimal ILJD schedule time of D = 1.36 and the BV schedule time of 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Corollary 8 :</head><label>8</label><figDesc>BW (GLJD) = Θ(log n) and BW (ILJD) = Θ(log n), hence BW (GLJD) BW (ILJD) = Θ(1). 0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>LOW JITTER SCHEDULER 1 :. 3 :</head><label>13</label><figDesc>Lat A k denote the current start time and B k the current finish time for class k. 2: Set A k = 0 and B k = 1 φ k Let t represent the current time slot. 4: Let l = Arg min k:A k ≤t B k . 5: If l = ∅ schedule X l in timeslot t. -Set A l ← B l and B l = B l + 1 φ l . -Set t ← t + 1 and go to Step 3. 6: If l = ∅ then set t ← t + 1 and go to Step 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Description of Low Jitter Scheduling Algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .Fig. 4 .Fig. 5 .</head><label>345</label><figDesc>Fig. 3. Number of Matches per Interval for a 64 × 64 Switch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1+ k 2 .</head><label>2</label><figDesc>This implies that the weight of the partial permutation picked at the next iteration will have the same upper bound (m k+1 ≤ 1 1+ k 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 , h 1+ k 2 ) 2 2n- 2 k=1 max(g 1+ k 2 , h 1+ k 2 ) = g 1 + 2</head><label>2222222</label><figDesc>, where g and h are as defined in Theorem 4. Hence K k=1 m k ≤ max(g 1 , h 1 ) +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>k+1 2 . 2 .</head><label>22</label><figDesc>Since 2 k+2 -2 ≥ n, k ≥ log 2 (n + 2) -2, and thus D(R n ) ≥ log 2 (n+2)-1 Proof for Corollary 8: For n ≥ 3, ln(n+2) 2 ln 2 -1 2 ≤ BW (ILJD) ≤ BW (GLJD) ≤ 2 ln(n) + 1. 0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 n</head><label>1</label><figDesc>First, since g 1 ≥ ... ≥ g n and h1 ≥ ... ≥ h n , clearly f 1 ≥ ... ≥ f n and n • f 1 ≥ n k=1 f k .Second, from the comment in the proof of Theorem 6, we know that T GLJD (R) ≤ f 1 + 2 n k=2 f k . Third, from Theorem 9, we also know that D(R) = T ILJD (R) ≥ n k=1 f k . Hence, combining these three results, we getTGLJD(R) TILJD(R) ≤ 2 -f 1 /( n k=1 f k ) ≤ 2 -1n . Proof for Corollary 11: Consider R such that BW (GLJD) = T GLJD (R). Then from Theorem 10 BW (GLJD) ≤ 2 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>ij = n k=1 π k . Finally, R = U d + . 0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc></figDesc><table><row><cell>A 1</cell><cell>0</cell><cell cols="5">. . . . . . . . . . . . 0</cell></row><row><cell>0</cell><cell>A 2</cell><cell>0</cell><cell cols="4">. . . . . . . . . 0</cell></row><row><cell>. . .</cell><cell>0</cell><cell>. . .</cell><cell>0</cell><cell cols="3">. . . . . . 0</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>0</cell><cell>A k</cell><cell>0</cell><cell cols="2">. . . 0</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>0</cell><cell cols="2">. . . 0</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reduced complexity input buffered switches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stiliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Hot Interconnects VIII</title>
		<meeting>Hot Interconnects VIII<address><addrLine>Palo Alto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Symmetric crossbar arbiters for VLSI communication switches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="27" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High speed switch scheduling for local area networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Owicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Thacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="352" />
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Tiny Tera: A packet switch core</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mekkittikul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ellersick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Hot Interconnects V</title>
		<meeting>Hot Interconnects V</meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delay bounds for approximate maximum weight matching algorithms for input queued switches</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kopikare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Infocom &apos;02</title>
		<meeting>IEEE Infocom &apos;02<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linear complexity algorithms for maximum throughput in radio networks and input queued switches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Infocom &apos;98</title>
		<meeting>IEEE Infocom &apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="533" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards simple, highperformance schedulers for high-aggregate bandwidth switches</title>
		<author>
			<persName><forename type="first">P</forename><surname>Giaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Infocom &apos;02</title>
		<meeting>IEEE Infocom &apos;02<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ATM input-buffered switches with guaranteed rate property</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kesidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ISCC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Athens</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On service guarantees for input-buffered crossbar switches: a capacity decomposition approach by Birkhoff and Von Neumann</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IWQoS</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Birkhoff-Von Neumann inputbuffered crossbar switches</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE INFOCOM &apos;00</title>
		<meeting>IEEE INFOCOM &apos;00<address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scheduling of an input-queued switch to achieve maximal throughput</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Righter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability in the Engineering and Informational Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">327334</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Photonic packet switching: an overview</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="264" />
			<date type="published" when="1999-02">Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scheduling nonuniform traffic in a packetswitching system with small propagation delay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="813" to="823" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A reservation protocol for broadcast WDM networks and stability analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Rouskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="227" />
			<date type="published" when="2000-02">Feb. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced Birkhoff-von Neumann Decomposition Algorithm for Input Queued Switches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ansari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings Communications</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="339" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Local scheduling policies in networks of packet switches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajmone Marsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Giaccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An Expedited Forwarding PHB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">draft-ietf-diffservrfc2598bis-02.txt, Internet Engineering Task Force</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On reducing the degree of self-similarity in network traffic</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sikdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Vastola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalyanaraman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the complexity of decomposing matrices arising in satellite communication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rendl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1985-05">May 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An optimal time slot assignment for an SS/TDMA system with variable number of transponders</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bongiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="1981-05">May 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Traffic assignment in communication satellites</title>
		<author>
			<persName><forename type="first">E</forename><surname>Balas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Landweer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1983-11">November 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimizing the number of switchings in an SS/TDMA system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1985-06">June 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A note on efficient SS/TDMA assignment algorithms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Pomalaza-Raez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="1988-09">September 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The greedy algorithm is optimal for on-line edge coloring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="251" to="253" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An upper bound on delay for the virtual clock service discipline</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Figueria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pasquale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1995-08">August 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new evaluation criterion for Closand Benes-type rearrangeable switching networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gragopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-N</forename><surname>Pavlidou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="126" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new matrix decomposition algorithm for rearrangeable Clos interconnection networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carpinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1572" to="1578" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient algorithms for SS/TDMA scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1367" to="1374" />
			<date type="published" when="1992-08">Aug. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A conflict-free protocol for optical WDMA networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Globecom &apos;91</title>
		<meeting>IEEE Globecom &apos;91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1276" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient time slot assignment algorithms for TDM hierarchical and nonhierarchical switching systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="359" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Guaranteed scheduling for switches with configuration overhead</title>
		<author>
			<persName><forename type="first">B</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Infocom &apos;02</title>
		<meeting>IEEE Infocom &apos;02<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Demonstration of a 1.2 Tb/s optical packet switch fabric based on 40 Gb/s burst-mode clock-data-recovery, fast tunable lasers, and a high-performance NxN AWG</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gripp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOC &apos;01</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Special issue on optical MEMS</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Topics in Quantum Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002-02">Jan.-Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
