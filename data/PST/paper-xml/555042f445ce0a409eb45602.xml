<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kim</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">K</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">are with Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Polytechnic School of Engineering</orgName>
								<orgName type="institution">New York Univer-sity</orgName>
								<address>
									<postCode>11201</postCode>
									<settlement>Brooklyn</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2C896076EFA24623690CAE0930D49176</idno>
					<idno type="DOI">10.1109/TIP.2014.2329776</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Color-Guided Depth Recovery From RGB-D Data Using an Adaptive Autoregressive Model Jingyu Yang, Xinchen Ye, Kun Li, Chunping Hou, and Yao Wang, Fellow, IEEE Abstract-This paper proposes an adaptive color-guided autoregressive (AR) model for high quality depth recovery from low quality measurements captured by depth cameras. We observe and verify that the AR model tightly fits depth maps of generic scenes. The depth recovery task is formulated into a minimization of AR prediction errors subject to measurement consistency. The AR predictor for each pixel is constructed according to both the local correlation in the initial depth map and the nonlocal similarity in the accompanied high quality color image. We analyze the stability of our method from a linear system point of view, and design a parameter adaptation scheme to achieve stable and accurate depth recovery. Quantitative and qualitative evaluation compared with ten state-of-the-art schemes show the effectiveness and superiority of our method. Being able to handle various types of depth degradations, the proposed method is versatile for mainstream depth sensors, time-of-flight camera, and Kinect, as demonstrated by experiments on real systems.</p><p>Index Terms-Depth recovery (upsampling, inpainting, denoising), autoregressive model, RGB-D camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A CQUIRING depth information of real scenes is an essen- tial task for many applications such as 3DTV, augmented reality, and 3D reconstruction. Generally, 3D information of a scene consists of texture information and position information, i.e., depth information in our context. While texture information can be readily captured by popular color cameras, depth information is not so easy to acquire. Until now, there are mainly two categories of methods to obtain depth information: passive methods and active methods.</p><p>In passive methods, depth information is computed from two-view images or multiview images via correspondence matching and triangulation. Being an active area for several decades, the accuracy of stereo matching has been significantly improved. However, there are still some inherent problems for practical application, e.g., the requirement of accurate image rectification and the inefficiency for textureless areas <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>The alternatives to acquire depth information are the active methods in which lights are intentionally projected to the scene and the depth information is measured from the echoed signals. Laser range scanner techniques are the earliest active methods and usually achieve high accuracy <ref type="bibr" target="#b2">[3]</ref>. However, the slice-by-slice scanning of laser scanners makes them rather time-consuming and inapplicable to dynamic scenes. Timeof-flight (ToF) based technique is a recent advance in active depth sensing <ref type="bibr" target="#b3">[4]</ref>. In ToF cameras, depth information is determined by measuring the phase difference between the emitted light and the reflected light. ToF cameras can capture depth information for dynamic scenes in real time, but are noisy and subject to low resolutions, e.g., 176 × 144 and 200 × 200, compared with popular color cameras. Structured-light based sensing technique is another breakthrough to achieve realtime depth capturing for dynamic scenes, and the Microsoft Kinect is a representative commodity device of this kind. In Kinect, an infrared light source projects a dot pattern on the scene and an offset infrared camera receives the pattern and estimates the depth information. The generated depth maps contain considerable holes due to the occlusion caused by the relative displacement of the projector and infrared camera.</p><p>While the new depth capturing techniques are promising, the use of depth cameras is limited by the low quality of produced depth maps, e.g., low resolution, noise, and depth missing in some areas. There have been some previous work on depth recovery for depth cameras. To compensate the undersampling of ToF cameras, an auxiliary color camera is equipped and the resolution of depth maps is enhanced by joint image filtering techniques from a low resolution depth map and a high resolution color image <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Some depth recovery methods for Kinect are adapted from image inpainting techniques <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These methods achieve good quality for smooth regions, but may introduce artifacts, e.g. jagging, blurring, and ringing, around thin structures or sharp discontinuities. Both taking a low quality depth map and a high quality color image as input, depth recovery problems for ToF camera and Kinect are essentially the same, but are treated separately in literature before our preliminary results of this work <ref type="bibr" target="#b9">[10]</ref>.</p><p>This paper proposes an adaptive color-guided AR model to construct a unified depth recovery framework for both ToF and Kinect depth cameras. We first verify the fitness of AR model for depth maps, and then design pixel-wise adaptive AR predictors based on the non-local similarity of both the depth map and the accompanied color image. The depth map is recovered by minimizing AR prediction errors subject to the observation consistency. The stability of the proposed method is analyzed from the linear system point of view. Inspired by the stability analysis, parameters are adaptively set according to the local characteristics of the depth-texture pair to achieve stable and reliable recovery. Experiments demonstrate that our method can handle various depth degradation modes and is applicable to both ToF and Kinect cameras. Without resorting to higher level tools such as segmentation used in <ref type="bibr" target="#b10">[11]</ref>, our proposed method achieves the best quality among several stateof-the-art depth recovery methods.</p><p>The contribution of our work is summarized into the following three aspects:</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are mainly two types of mainstream depth cameras: one is ToF cameras and the other is the structured-light based depth cameras, e.g., Kinect. The recovery of high quality depth information from measurements sensed by these devices is a crucial step for subsequent processing in many computer vision tasks, and many algorithms have been proposed in literature. This section briefly reviews the related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Recovery for ToF Cameras</head><p>As shown in Fig. <ref type="figure" target="#fig_1">1</ref>(a), the depth map captured by ToF camera has a much lower resolution than the color image. Such degradation would impede their practical applications. It seems impossible to recover high quality depth maps from severely undersampled versions due to the loss of salient information around notable discontinuities. However, the depth information and texture information are two descriptions of the same scene from different perspectives, and thus present strong structural correlations <ref type="bibr" target="#b16">[17]</ref>. In particular, as shown in Fig. <ref type="figure" target="#fig_1">1</ref> (a) and (c), discontinuities often simultaneously present at the same locations in a depth map and the corresponding (registered) color image, and homogeneous regions in color image tend to have similar depth. Although the viewpoints between the depth sensor and image sensor are different, as shown in Fig. <ref type="figure" target="#fig_1">1</ref>(a) and (b), the ToF depth map can be aligned with the color image via view warping with camera calibration parameters. Then, the strong structural correlation between the low-resolution depth map and high resolution color image can be conveniently exploited. Therefore, the common wisdom is to couple a color camera with a ToF camera and to recover high quality depth maps with the help of the accompanied color images <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. <ref type="foot" target="#foot_0">1</ref>In an early work, Diebel and Thrun <ref type="bibr" target="#b19">[20]</ref> proposed a two-layer MRF to model the correlation between range measurements and solve the MRF optimization with the conjugate gradient algorithm. This method is able to improve the quality of depth maps, but tends to produce oversmooth results. To reduce oversmoothing, Hannemann et al. <ref type="bibr" target="#b21">[22]</ref> incorporated amplitude values generated by ToF into an MRF model to improve the quality of interpolation. The amplitude can be evaluated as a confidence measurement for the depth values. Lu et al. <ref type="bibr" target="#b22">[23]</ref> further extended this work by designing a data term that fits to the characteristics of depth maps. Huhle et al. <ref type="bibr" target="#b23">[24]</ref> added a third layer to the MRF framework <ref type="bibr" target="#b19">[20]</ref>, where image gradients are encoded as nodes in the graph. Zhu et al. <ref type="bibr" target="#b24">[25]</ref> extended the traditional spatial MRFs to dynamic MRFs so that both the spatial and the temporal relationship can be propagated in local neighbors, improving the accuracy and robustness of depth recovery for dynamic scenes. Aodha et al. <ref type="bibr" target="#b25">[26]</ref> presented an algorithm to synthetically increase the resolution of a solitary depth image using only a generic database of local patches. They match against each low resolution input depth patch, and search database for a list of appropriate high resolution candidate patches. The training of data, matching, and fusion are quite computationally intensive.</p><p>Another category for the recovery of ToF depth maps is to use advanced filters such as bilateral filters and non-local means (NLM) filters <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Garro et al. <ref type="bibr" target="#b27">[28]</ref> used an efficient graph-based segmentation method on color image to interpolate the missing depth information. Joint bilateral filtering <ref type="bibr" target="#b11">[12]</ref> and its variations are readily available tools for depth recovery using high quality auxiliary color images <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Yang et al. <ref type="bibr" target="#b13">[14]</ref> used the joint bilateral filtering method for range images super resolution. Yang et al. <ref type="bibr" target="#b28">[29]</ref> also proposed a hierarchical joint bilateral filtering scheme for depth map upsampling. Chan et al. designed an adaptive multilateral upsampling filter to further address the noise in depth measurements <ref type="bibr" target="#b29">[30]</ref>. Min et al. <ref type="bibr" target="#b30">[31]</ref> proposed a weighted mode filtering method based on a joint histogram of depth video and color video. He et al. <ref type="bibr" target="#b12">[13]</ref> investigated guided filtering to perform as an edge-preserving smoothing operator like the popular bilateral filter. Park et al. <ref type="bibr" target="#b10">[11]</ref> used a non-local term to regularize depth maps and combined with a weighting scheme that involves edge, gradient, and segmentation information extracted from high quality color images, but jaggy artifacts occur in some boundaries. Lu et al. <ref type="bibr" target="#b31">[32]</ref> formulated the filtering process as a local multipoint regression problem, consisting of multipoint estimation within a shape-adaptive local support, and aggregation of a number of multipoint estimates available for each point. It models a zero-order or linear relation between observed low resolution depth patch and color patch. However, the low-order model is not accurate for regions with complex color textures. Liu et al. <ref type="bibr" target="#b32">[33]</ref> used a geodesic distance to compute the filtering coefficients based on the similarity between pixels. The algorithm is accelerated to a low computational complexity by dynamic programming. The geodesic upsampling method provides impressive recovered results for most areas of depth maps, but also introduces some annoying artifacts in regions where the associated color image has rich textures. Ferstl et al. <ref type="bibr" target="#b33">[34]</ref> modeled the smooth term as a second order total generalized variation regularization, and guided the depth upsampling with an anisotropic diffusion tensor calculated from a high-resolution intensity image, providing high-quality upsampling results. It is noted that the moving least squares (MLS) method and its various variants are powerful in 3D surface fitting <ref type="bibr" target="#b34">[35]</ref> and image recovery <ref type="bibr" target="#b35">[36]</ref>. MLS schemes are also expected to have promising performance in depth recovery as evidenced in the comparison results in Section VI.</p><p>Generally, depth recovery schemes based on filtering techniques are competitive to MRF-based methods in recovery accuracy, but have lower asymptotic computational complexities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Recovery for Kinect</head><p>Depth maps provided by Kinect contain numerous structural missing around depth discontinuities due to occlusions between the build-in infrared projector and the infrared sensor. This is clearly shown in a typical depth map captured by Kinect in Fig. <ref type="figure" target="#fig_9">1(d)</ref>. Moreover, shiny objects or transparent surfaces can lead to loss of depth information. These defects correspond to the main degradations of structural depth missing for occlusion regions and random depth missing on the background.</p><p>For low delay and low complexity, many methods focus on the enhancement of Kinect depth within the spatial domain. Abdul Dakkak et al. <ref type="bibr" target="#b36">[37]</ref> proposed an iterative diffusion method that incorporates RGB-D segmentation results to recover missing depth information. Andrew et al. <ref type="bibr" target="#b37">[38]</ref> devised a fast modified two-pass median filter with dynamic window scales, which is effective in filling small holes, but cannot deal with large missing areas. Lai et al. <ref type="bibr" target="#b8">[9]</ref> filled missing depth values by recursively applying a median filter in the construction of the RGB-D object dataset, but blurring occurs for large occlusions. Berdnikov et al. <ref type="bibr" target="#b38">[39]</ref> used the "deepest neighbor" method and the simple spatial interpolation method to handle two different kinds of depth missing. This method achieves real-time processing, but the recovered depth maps are not always consistent with the accompanied color image, particularly around the boundaries between the background and foreground. We also note that there are some work departing from the inpainting approach. For example, Yu et al. <ref type="bibr" target="#b39">[40]</ref> proposed refining noisy depth map in the framework of shapefrom-shading.</p><p>Noting that depth maps along the temporal present strong correlations, another category is to use temporal information besides spatial information. S. Lee et al. <ref type="bibr" target="#b7">[8]</ref> filled out the missing areas by an image inpainting algorithm <ref type="bibr" target="#b40">[41]</ref>, and extended the joint bilateral filter to the joint multilateral filter to improve depth quality and temporal consistency. Matyunin et al. <ref type="bibr" target="#b41">[42]</ref> proposed a depth restoration method via a simple temporal filtering scheme. Camplani and Salgado <ref type="bibr" target="#b42">[43]</ref> reduced the depth noise with a joint-bilateral filter on the spatial domain and repaired the depth value fluctuation on the temporal domain. These methods mentioned above use both the spatial and temporal information for depth recovery, but quality of the filtered depth maps are not satisfactory around depth discontinuities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEGRADATION MODES AND AR MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Degradation Modes of Depth Maps</head><p>Current depth capturing systems are far from perfect. A captured depth map is a degraded version of the underlying groundtruth. Let d and d 0 denote the vector form of the underlying perfect depth map and the captured one, respectively. The observation model for depth capturing is described as where P represents the observation matrix and n is the introduced additive noise.</p><formula xml:id="formula_0">d 0 = P d + n,<label>(1)</label></formula><p>There are mainly four types of degradations: undersampling, random depth missing, structural depth missing, and pollution with additive noise. For the former three ones, the observed depth map d 0 has a smaller number of elements than d and P is a flat matrix to identify valid pixels with depth values. However, P has different structures for different degradation modes. For the degradation of undersampling, P is a sampling matrix; while in structural and random missing, P is constructed from an identity matrix by removing those rows associated with the depth-missing pixels. Depth capturing systems may suffer from different combinations of the degradation modes. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>(a), the depth map captured by ToF camera is undersampled (lower resolution than the accompanied color images), and polluted by noise. After viewpoint registration, the warped depth map contains disoccluded regions around object boundaries, and thus suffers from degradation of structural depth missing. As shown in Fig. <ref type="figure" target="#fig_9">1 (d)</ref> (see also more examples in figures in Section VI-B on experiments), the Kinect depth map contains both random and structural missing degradations. Our method is to recover high quality depth maps from low quality observations, and all the four kinds of degradations are handled in the proposed unified depth recovery framework.</p><p>The observation model (1) considers degradation modes that commonly present in depth sensing technologies. It should be noted that there are other sources of degradation in real depth sensors. Related investigations, see <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b43">[44]</ref>, show that depth cameras generally have some systematic errors for various reasons, e.g., anharmonic LED modulation, integration time offset, pixel offsets, intensity dependent response, and different Lambertian reflectance properties. It is possible to consider these degradations in the observation model. In applications using depth cameras, these complex systematic errors are usually calibrated and compensated as a preprocessing step before subsequence processing <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AR Model of Depth Maps</head><p>The AR model has been applied in many image processing applications, such as detecting and interpolating missing areas in image sequences <ref type="bibr" target="#b45">[46]</ref>, super-resolution, forecasting of spatial-temporal data <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, as well as backward adaptive video coding <ref type="bibr" target="#b48">[49]</ref>. This demonstrates that the simple AR model is versatile for many applications as long as the AR predictors are properly designed.</p><p>As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, Fig. <ref type="figure" target="#fig_11">11</ref>, and Fig. <ref type="figure" target="#fig_13">13</ref>, depth maps for generic 3D scenes contain mainly smooth regions separated by curves. The AR model can well describe such type of 2D signals. The key insight is that a signal can be regenerated by the signal itself. Denote by D a depth map, and D x the depth value at location x. The predicted depth map D by the AR model from the depth map D is expressed as</p><formula xml:id="formula_1">Dx = y∈N (x) a x, y D y ,<label>(2)</label></formula><p>where N (x) is the neighborhood of pixel x and a x, y denotes the AR coefficient for pixel y in the neighborhood N (x). The accuracy of the AR model can be measured by the difference between D and D, e.g., mean absolute difference (MAD) or root mean squared error (RMSE).</p><p>To verify the fitness of the AR model for depth maps, we check the prediction errors between the predicted depth maps and the groundtruth for a set of test depth maps. Four AR predictors are tested: an average filter, a Gaussian filter, a bilateral filter and our proposed filter, all with a 11 × 11 neighborhood. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, all the four filters have good prediction for smooth regions. However, when coming to discontinuities, we can see that the results of the average filter and gaussian filter are apparently of low quality and subject to oversmoothing around the edges. Since the proposed filter adapts the AR model to the nonlocal structures of signals, it almost regenerates the depth map: the average prediction error in MAD is only 0.051/pixel. These results demonstrate that the AR model is quite effective in modeling the depth maps, and thus encourage the application of this model to the recovery of depth maps.</p><p>Depth-color pairs have strong correlation in terms of geometrical structures, and are often acquired and used together <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>(a) and (b), edges in the depth map have their counterparts in color image. This suggests that the locations of edges in depth maps can be inferred from the accompanied color images, and motivates the proposed colorguided AR model for depth recovery from low resolution and incomplete observations. The data term is expressed as</p><formula xml:id="formula_2">E data D, D 0 x∈O D x -D 0 x 2 , (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>and the AR model is incorporated into the depth recovery as the AR term</p><formula xml:id="formula_4">E AR ( D) x ⎛ ⎝ D x - y∈N (x) a x, y D y ⎞ ⎠ 2 , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where the AR coefficient a x, y is defined according to both depth and color information in the following section. The proposed method has a similar form, but is a departure essentially, to the work in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, in addition to color information, segmentation and edge saliency are taken into account in confidence weights. Although such features can be readily incorporated in our recovery model, we found that the elegant AR model can well describe the characteristics of depth maps. Therefore we insist on the low level processing in depth recovery, and retain the simplicity of the model. As shown in Section III-B, the AR model is powerful in describing depth maps only when the AR coefficients are properly designed. However, an accurate AR model is difficult to infer from only the degraded depth map D 0 . Since the depth-color pairs have strong structural correlations, the information loss due to depth degradation can be complemented by the accompanied color image. To achieve high quality depth recovery, we design pixel-wise adaptive AR predictors in Section IV-B using both the initial depth map and the auxiliary color image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Color-Guided AR Model</head><p>As demonstrated by the simulation results in Section III-B, the AR model has very different performance with different AR predictors. The common way in AR-based image processing is to divide images into small units and each unit shares an AR predictor. However, we observe that the AR model cannot provide sufficient adaptivity when each unit contains considerable variations. Therefore, we design pixel-wise adaptive AR predictors: an AR predictor {a x, y }, y ∈ N (x) is constructed for each pixel x by considering both the depth and color information.</p><p>A depth map is reliably recovered with the optimal AR predictors, which can be derived only when the depth map is available. To break this chicken-egg dilemma, we design AR Fig. <ref type="figure">3</ref>.</p><p>Illustrations for contrast between the traditional predictors and our proposed AR predictors: (a) patch-based neighborhood (b) shape-based neighborhood.</p><p>predictors using the available depth map and the accompanied color image. Note that the observed depth map D 0 is not directly applicable due to degradations such as the undersampling or depth missing. Denote by D the rough estimated depth map obtained by bicubic interpolation from D 0 . Represent the accompanied color image with I = {I i , i ∈ C}, where I i is the intensity of the color channel with index i and C is the index set of color channels in a certain color space. We had investigated three color spaces (RGB, YUV, and Lab). All three color spaces yield similar results, and we choose the YUV color space due to its slightly better performance, i.e., C = {Y, U, V } in our implementation. The AR coefficient a x, y consists of two terms:</p><formula xml:id="formula_6">a x, y = 1 S x a D x, y a I x, y ,<label>(6)</label></formula><p>where S x is the normalization factor, a D x, y and a I x, y are the depth term and color term, respectively.</p><p>The depth term a D x, y is defined on the initial estimated depth map D by a range filter:</p><formula xml:id="formula_7">a D x, y = exp - Dx -Dy 2 2σ 2 1 ,<label>(7)</label></formula><p>where σ 1 is the decay rate of the range filter. Qualitatively, a D</p><p>x, y has a large value if Dx is close to Dy . This term is also designed to avoid incorrect depth prediction due to depthcolor inconsistency: pixels of the same depth layers may have very different colors; pixels of similar colors may belong to different depth layers.</p><p>The color term a I x, y is designed to take benefit of the correlations in the depth-color pair. Edges in a depth map cooccur with their counterparts in the accompanied color image. The color term a I</p><p>x, y should be able to prevent the AR model from predicting across depth discontinuities. Based on the nonlocal principle, we propose the following color terms :</p><formula xml:id="formula_8">a I x, y = exp - i∈C ||B x • P i x -P i y || 2 2 2 × 3 × σ 2 2 , (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where σ 2 controls the decay rate of the exponential function, P i x denotes an operator that extracts a w × w patch centered at x in color channel i , "•" represents the element-wise multiplication. The bilateral filter kernel B x is defined in the extracted w × w patch:</p><formula xml:id="formula_10">B x (x, y) = exp - ||x -y|| 2 2 2σ 2 3 exp - i∈C (I i x -I i y ) 2 2 × 3 × σ 2 4 ,<label>(9)</label></formula><p>where σ 3 and σ 4 are parameters of the bilateral kernel to adjust the importance of the spatial distance and intensity difference, respectively.</p><p>The difference between the proposed filter in the color term and the standard NLM filter is that the proposed one uses a bilateral kernel to weight the distance of local patches while the standard one uses a Gaussian kernel. The bilateral kernel B x has a strong response for pixels of similar intensities to x, and hence carries the shape information of local image structures. This extends the NLM filter from patch-based to shape-based in measuring the resemblance of local structures, and has a significant impact on the structure of AR predictors for pixels around edges. As shown in Fig. <ref type="figure">3</ref>, two homogeneous regions are separated by smooth curves and x is close to a curve. To construct the AR predictor for x, the similarity between x and each pixel y in the neighborhood N x is evaluated. Constrained by the patch structure, the standard NLM filter produces large coefficients only for pixels that are parallel to the edge, e.g. y 1 and y 2 , and produces small coefficients for other pixels, such as y 3 , even though they have the same intensity as x. On the contrary, our bilateralweighted NLM filter has a shape-adaptive neighborhood, and increases opportunities to exploit more correlations for pixels around discontinuities. This is illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>. With the shape-adaptive neighborhood, the proposed filter produces an equally large coefficient for y 3 as for y 1 and y 2 . As verified later in Section V, small supports of AR predictors would underdetermine the recovery system and can lead to fail recovery for related pixels; while the proposed AR predictors of larger supports form a more well-determined system, and achieve stable recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. STABILITY ANALYSIS AND PARAMETER ADAPTATION</head><p>The depth recovery system often works under perturbations: 1) observation perturbation: sensed depth measurements may contain some noise; and 2) system perturbation: AR predictors can be affected by highly-textured regions of the color image, which can make the system ill-conditioned. So, it is quite necessary to analyze the behavior of the depth recovery method under perturbations for stable and high-quality recovery.</p><p>In this section, we formulate the quadratic minimization (3) into a quadratic programming and analyze the stability of the recovery model by the conditioning of linear systems (Section V-A). Then we investigate how the parameters affect the system stability, and design a parameter adaptation scheme to achieve stable and high quality depth recovery (Section V-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stability of the Depth Recovery System</head><p>The depth recovery model ( <ref type="formula">3</ref>) are quadratic with respect to D. Therefore, it can be reformulated as an unconstrained quadratic programming and analyzed with the conditioning of linear systems. Let d and d 0 be the vector form of D and D 0 , respectively. Then, the depth recovery model is equivalent to the following minimization with respect to d:</p><formula xml:id="formula_11">min d d 0 -P d 2 2 + λ d -Qd 2 2 , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where P is the observation matrix and Q is the prediction matrix corresponding to AR predictors {a x, y }. In <ref type="bibr" target="#b9">(10)</ref>, the first term is the data term and the second term is the AR term. The unconstrained quadratic programming (10) is convex, and its global minima can be obtained by solving the first-order conditions:</p><formula xml:id="formula_13">P P + λ( I -Q) (I -Q) H d = P d 0 c , (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where H is a squared matrix. Therefore, the stability of the depth recovery model can be analyzed via the conditioning of linear systems. Denote the condition number of H by κ := σ max /σ min , where σ max and σ min are the maximal and minimal singular values of H, respectively. Denote by δc the noise in c, δ H the perturbation in H, and δd the resulting error in d. The sensitivity of the linear system can be obtained by considering the perturbed system: <ref type="bibr" target="#b49">[50]</ref>. The sensitivity of the linear system is described by:</p><formula xml:id="formula_15">(H + δ H) (d + δd) = c+δc</formula><formula xml:id="formula_16">δd d ≤ κ δ H H + δc c , (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>which shows that the relative error of the recovery depth map is proportional to the relative noise in H and c up to a magnification of the condition number κ. When κ is large, a small relative change in either H or c can cause a large change in d, which would severely degrade the performance of depth recovery. Therefore, the depth recovery model should be carefully designed so that the matrix of the resulting linear system has a low condition number. In the linear equations of first-order conditions, the coefficient matrix is the combination of the sampling matrix P, the prediction matrix Q, and their transposes. Note that P P is a highly deficient diagonal matrix, e.g., the rank is only the 1/64 of the full rank for 8 × 8 super-resolution. Therefore, the invertibility and stability of the linear system (11) are determined by the prediction matrix Q governed by five parameters: λ and {σ i } 4 i=1 . The specific influences of the five parameters on the stability as well as the parameter adaptation are detailed in the following section (Section V-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter Adaptation</head><p>To test the influence of the parameters on the stability and the recovery quality, we randomly extract a large number of patches from degraded depth maps, and perform 8 × 8 super-resolution (other upsampling rates also yield the same conclusions). Instead of traversing the whole parameter space, we perform depth recovery by varying each parameter while setting other parameters at fixed reasonable values: λ = 0.01, σ 1 = 2, σ 2 = 9, σ 3 = 5, and σ 4 = 2. The varying range for each parameter is [0.01, 100] which covers the interest fraction of the parameter space. For each test point, we evaluate the condition number of the resulting matrix H and the quality of recovered depth measured in MAD. Results for four test depthtexture pairs are presented in Fig. <ref type="figure" target="#fig_5">5</ref>. We analyze the sensitivity of each parameter as well as its adaptation as follows.</p><p>1) λ: This parameter adjusts the importance of the data term and the AR term. Note that both P P and (I -Q) (I -Q) are rank-deficient matrices. Either a very small or a very large λ will produce H of a large condition number. Fig. <ref type="figure" target="#fig_5">5</ref> shows that λ ∈ [0.01, 100] yield condition numbers lower than 10 5 , which is stable for the recovery system. We observe that the MAD of the recovered depth is monotonically increasing with respect to λ. Therefore, we set λ = 0.01 in our implementation.</p><p>2) σ 1 : In the depth term, the weights for candidates are assigned according to the closeness of their values to the reference Dx . σ 1 controls the tolerance for two different depth values to be considered close enough to assign a significant weight. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the condition number would dramatically increase when σ 1 is small (e.g., below 0.5), and depth around edges cannot be recovered due to the instability of the recovery system. We also observe that too large a σ 1 tends to produce oversmooth results. Therefore, we design an adaptive scheme: σ 1 is assigned to a large value for smooth depth regions to include more depth values for stable and accurate prediction, and is assigned to a small value for regions around depth discontinuities to avoid prediction across depth edges. To this end, σ 1 is determined by the local smoothness:</p><formula xml:id="formula_18">σ 1 (x) = a 1 + b 1 exp -c 1 D [x] , (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where</p><formula xml:id="formula_20">D [x]</formula><p>denotes the gradient magnitude of D at x; a 1 and b 1 determine the lower and upper bounds of σ 1 , and are set at 0.5 and 2.5, respectively; c 1 controls the decay rate of the exponential mapping, and is set at 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) σ 2 :</head><p>The most important role of the color term is to provide clues of depth edges lost due to undersampling based on the assumption of strong structural correlation between color images and the associated depth maps. Being similar to σ 1 in the depth term, σ 2 in the color term is the tolerance for two patches to be considered similar enough to assign a significant weight. The behavior of the stability and recovery performance with respect to σ 2 is quite similar to those with respect to σ 1 . The recovery system become instable when σ 2 is very small, and the depth recovery fails for pixels around depth discontinuities. Also, σ 2 should be large in flat depth regions to have stable prediction and small around depth edges to avoid prediction across discontinuities. Therefore, σ 2 is adapted according to the same strategy as for σ 1 :</p><formula xml:id="formula_21">σ 2 (x) = a 2 + b 2 exp -c 2 D [x] , (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>where a 2 , b 2 , and c 2 are parameters. Note that color images are much more spatially-variant than depth maps. The lower and upper bounds of σ 1 are adapted to local characteristics of the color image with the following piecewise function:</p><formula xml:id="formula_23">⎧ ⎨ ⎩ a 2 = 1.9, b 2 = 3.8, a 2 = 4.8, b 2 = 2.9, a 2 = 6.7, b 2 = 2.9, I [x] &lt; 5, 5 ≤ I [x] &lt; 8, I [x] ≥ 8, (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>where</p><formula xml:id="formula_25">I [x]</formula><p>is the gradient magnitude of the Y-channel image at x. The values of a 2 and b 2 in ( <ref type="formula" target="#formula_23">15</ref>) are obtained by numerically fitting the two parameters and the gradient magnitude to have the best recovery performance.</p><p>4) σ 3 : The two parameters σ 3 and σ 4 of the bilateral kernel in Formula (9) control the shape and the size of the non-local patches. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the conditional number and recovery accuracy with respect to σ 3 are quite stable. For example, the fluctuation of MAD is usually within 0.01 for smooth regions and is within 0.15 for depth regions around edges. Therefore, σ 3 is fixed at 5 in our implementation.</p><p>5) σ 4 : In the bilateral kernel <ref type="bibr" target="#b8">(9)</ref>, σ 4 controls the support of the bilateral kernel. As σ 4 increases, the bilateral kernel B x to define the shape of the patch tends to have equal weights within the w × w window. This will reduce the shapeadaptive patch to a squared patch, and thus the proposed nonlocal kernel degenerates into a conventional non-local mean filter. As shown in Fig. <ref type="figure">3</ref> and Fig. <ref type="figure" target="#fig_4">4</ref>, for pixels around edges, the squared patch produces AR predictors of small supports. This would lead to the instability of the recovery system. As verified by the results shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the conditional number increases rapidly when σ 4 is larger than 5; Accordingly, the recovery performance is stable when σ 4 &lt; 10, but will severely drop beyond this range. When σ 4 has large values, depth values cannot be reliably recovered due to the ill-conditioning of the system. Therefore, we set σ 4 = 3 in our implementation.</p><p>6) Neighborhood Size in AR Predictors: We investigate the influence of neighborhood size N x on the recovery quality and computational complexity. To this end, low resolution depth patches are recovered with various neighborhood size, i.e., 3 × 3, 5 × 5, . . . , 17 × 17. As shown in Fig. <ref type="figure">6</ref>, as the neighborhood size becomes larger, more samples are included into AR prediction, yielding more stable recovery. However, the recovery accuracy does not significantly increase as the 6. Influence neighborhood on (a) recover quality (MAD) and (b) computational complexity in normalized time (relative to the 3 × 3 case normalized to one). neighborhood beyond the size of 11 × 11. Moreover, increasing the support will also increase complexity. The computation is approximately with respect to the neighborhood size. Therefore, the neighborhood size of 11×11 is chosen in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head><p>Our method is first evaluated on Middlebury datasets with various synthetic degradations and compared with several existing methods. Then, our method is applied on two real depth sensing systems to obtain high quality depth maps. All the datasets, results, and recovered depth maps are available in the project website. <ref type="foot" target="#foot_1">2</ref> We direct interested readers to the website for more results on real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on Datasets With Synthetic Degradations</head><p>Six datasets, Art, Book, Moebius, Reindeer, Laundry, and Dolls from the Middlebury's benchmark <ref type="bibr" target="#b50">[51]</ref> are used for evaluation. Three kinds of typical degradations are simulated: undersampling, ToF-like degradation (undersampling with noise), Kinect-like degradation (structural missing along depth discontinuities and random missing in flat regions). Our method is compared with ten state-of-the-art methods (if applicable): Bicubic interpolation, MRF-based method (MRF) <ref type="bibr" target="#b19">[20]</ref>, improved MLS (IMLS) <ref type="bibr" target="#b35">[36]</ref>, joint bilateral filtering on cost volumes (JBFcv) <ref type="bibr" target="#b13">[14]</ref>, guided image filtering (Guided) <ref type="bibr" target="#b12">[13]</ref>, edge-weighted NLM-regularization (Edge) <ref type="bibr" target="#b10">[11]</ref>, patch-based synthesis (PS) <ref type="bibr" target="#b25">[26]</ref>, cross-based local multipoint filtering (CLMF) <ref type="bibr" target="#b31">[32]</ref>, joint geodesic filtering (JGF) <ref type="bibr" target="#b32">[33]</ref>, total generalized variation (TGV) <ref type="bibr" target="#b33">[34]</ref>. Upsampling results (Table <ref type="table">I</ref>) on Art, Book, and Moebius for MRF, JBFcv, Guided, Edge, and TGV are quoted from <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b33">[34]</ref>. The results for MRFs <ref type="bibr" target="#b19">[20]</ref> and JBFcv <ref type="bibr" target="#b13">[14]</ref> on other three RGB-D pairs were not available; For the Patchbased synthesis (PS) method <ref type="bibr" target="#b25">[26]</ref>, the released patch database trained for 4× upsampling is poor for other upsampling rates. Therefore, we only present its results at 4× upsampling. The rest results in Table <ref type="table">I</ref>, and results in Table <ref type="table">II</ref> and Table <ref type="table">III</ref> are generated by the provided codes (if have) or our implementations. We improve the MLS scheme <ref type="bibr" target="#b35">[36]</ref> by extending the Gussian weighting to the cross bilateral weighting <ref type="bibr" target="#b6">[7]</ref> to avoid MLS fitting across depth discontinuities, hence named improved MLS (IMLS). The CLMF method <ref type="bibr" target="#b31">[32]</ref>  has two versions: CLMF0 and CLMF1 for zero-and firstorder polynomial model, respectively. In visual comparisons (Fig. <ref type="figure" target="#fig_6">7</ref>, Fig. <ref type="figure" target="#fig_7">8</ref>, and Fig. <ref type="figure" target="#fig_10">10</ref>), regions highlighted by rectangles are enlarged, and the error maps are shown by subtracting between recovered depth and ground truth, for easy visual inspection.</p><p>1) Undersampling Degradation: Depth recovery results (in MAD) at four upsampling rates for each RGB-D pair are reported in Table <ref type="table">I</ref>. For our method, we present the results with two configurations in Table <ref type="table">I</ref>: 1) Ours_FP uses fixed parameters that are manually tuned to avoid instable recovery and also to obtain the best recovery performance, and 2) Ours_AP adopts the parameter adaptation schemes in Section V-B. As shown in Table <ref type="table">I</ref>, our method nearly obtains the lowest MAD for most cases (especially for high upsampling rates of 8× and 16× upsampling), which demonstrates its effectiveness. The Edge method <ref type="bibr" target="#b10">[11]</ref> provides slightly better results for low upsampling rates on Reindeer, Laundry, and Dolls. We also observe that the PS method <ref type="bibr" target="#b25">[26]</ref> achieves slightly better results for Book and Moebius at 4× upsampling. However, it needs to train patch database at each upsampling rate, and its computational complexity is quite high: it takes about 40 minutes to super-resolve a depth map. Fig. <ref type="figure" target="#fig_6">7</ref> shows 8× upsampled depth maps for Dolls and Art. Upsampled depth maps by three state-of-the-art methods, IMLS <ref type="bibr" target="#b35">[36]</ref>, Edge <ref type="bibr" target="#b10">[11]</ref> and JGF <ref type="bibr" target="#b32">[33]</ref>, are also shown for comparison. The Edge method generates comparable results to ours for Moebius, but introduces some jaggy artifacts along edges. The JGF method provides promising quality for most areas of depth maps, but introduces annoying artifacts in regions where the associated color image has rich textures, e.g., the crayon of Art. The visual comparison show that our method not only achieves low average recovery errors, but also provides visually consistent results.  <ref type="bibr" target="#b35">[36]</ref> (MAD: 0.61; 1.04), (c) Edge <ref type="bibr" target="#b10">[11]</ref> (MAD: 0.56; 1.03), (d) JGF <ref type="bibr" target="#b32">[33]</ref> (MAD: 0.59; 0.78), and (e) our method (MAD: 0.50; 0.64). The first and second MADs for each method are for Dolls and Art, respectively.</p><p>2) ToF-Like Degradation: To simulate the ToF-like depth degradation, we first add Gaussian noise with a variance of 25 to the original datasets, and then downsample the polluted datasets at the four upsampling rates. Quantitative depth recovery results of our method and other eight methods are summarised in Table <ref type="table">II</ref>. Our method obtains the lowest MAD for all cases. The JGF method does not perform as well as in pure upsampling due to the lack of denoising capability. The IMLS, Guided, Edge, PS, and CLMF0 methods provide comparative results thanks to their inherent denoising capabilities. The minimization of total variation in TGV is powerful in suppressing noise, and therefore yields promising results. To compare visual results, Fig. <ref type="figure" target="#fig_7">8</ref> presents depth maps on Book and Reindeer recovered by Bicubic, IMLS, Edge, TGV, and our method. The depth maps recovered by two fitting methods, Bicubic and IMLS, still contain significant redidual noise. The Edge method tends to over-smooth out more useful signal components. The TGV method provides cleaner depth maps, but fails to preserve tiny structures such as the ears of the reindeer. Our method is able to effectively remove noise in upsampling while avoiding contaminating depth content.</p><p>3) Kinect-Like Degradation: To simulate Kinect-like degradation, structural missing is created along depth discontinuities, and random missing is generated in flat areas. Depth maps with Kinect-like degradation are presented in Fig. <ref type="figure" target="#fig_8">9</ref>. Recovery results from Kinect-like depth degradation are reported in Table <ref type="table">III</ref>. Five methods applicable for hole filling are compared: Bicubic, IMLS <ref type="bibr" target="#b35">[36]</ref>, joint bilateral filtering (JBF) <ref type="bibr" target="#b11">[12]</ref>, Guided <ref type="bibr" target="#b12">[13]</ref>, and CLMF0 <ref type="bibr" target="#b31">[32]</ref>. As shown in Table <ref type="table">III</ref>, our method obtains the lowest MAD for all cases, which shows its effectiveness in handling Kinect-like degradation. For visual comparison, depth maps recovered by IMLS, JBF, CLMF0, and our method are presented in Fig. <ref type="figure" target="#fig_10">10</ref>. All methods provide good recovery performance for random missing in flat regions. However, most methods have difficulties in correctly recovering sharp discontinuities within missing areas. Our method is able to recover better geometrical structures as suggested by the error maps.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Real Datasets</head><p>We apply our method on two types of depth sensors to achieve high quality depth recovery from the low quality sensor measurements. To compensate misalignment of different viewpoints, depth maps are warped to the viewpoint of the color camera using intrinsic parameters and extrinsic parameters for both cameras computed by the camera calibration module in the OpenCV library <ref type="bibr" target="#b51">[52]</ref>. We first reject outliers using the associated amplitude images as confidence levels, and then rectify the intensity-dependent error with a pre-measured look-up table, similar to the approach in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Two RGB-D pairs captured by our depth-color rig are shown in Fig. <ref type="figure" target="#fig_11">11(a)</ref>. We compare our method on these datasets with three representative methods: CLMF0 <ref type="bibr" target="#b31">[32]</ref>, JGF <ref type="bibr" target="#b32">[33]</ref>, and IMLS <ref type="bibr" target="#b35">[36]</ref>. As the recovered depth maps shown in Fig. <ref type="figure" target="#fig_11">11 (b</ref>)∼(d), CLMF0 and JGF tend to generate jaggy artifacts due to rich color textures and the discontinuity mismatch between the color images and depth maps, while IMLS brings a little bit over-smoothing around edges although we introduce a cross bilateral weighting for fair comparison. By inspecting the color-depth accordance, our method achieves quite promising recovery quality particularly around depth discontinuities.</p><p>b) ToFMark RGB-D sensing system: We also test on the ToFMark datasets <ref type="bibr" target="#b52">[53]</ref> consisting of three RGB-D pairs, Books, Shark, Devil, with ground-truth depth maps. The depth maps are of size 120 × 160, and the intensity images are of size 610 × 810, suggesting approximately 6.25× upsampling. Table <ref type="table" target="#tab_2">IV</ref> presents quantitative results. The recovery error is measured by MAD in mm. Our method also obtains the lowest recovery error for all the three test cases compared with other seven classic or state-of-the-art methods. In Fig. <ref type="figure" target="#fig_12">12</ref>, it is observed that depth maps recovered by Bicubic, IMLS, and CLMF0 still contain considerable amount of noise, while those recovered by TGV and the proposed method are much more clear. The TGV method in some cases introduces annoying artifacts in regions where the associated intensity image has rich textures, e.g., the bottom of cup and the edges of the book in Books. By closer inspection, TGV is superior in recovering slant planar surfaces that can be well characterized by the total variation minimization, while our method shows advantages in recovery high-order surfaces thanks to the powerful colorguided AR model.</p><p>2) Kinect Depth Maps: Microsoft Kinect is an integrated sensor array for natural user interaction, consisting of a depth FOR ToFMark DATASETS camera and a color camera. The captured depth maps and color images are of size 640 × 480, and registered to the same viewpoint. We suppressed fake-color artifacts in color images by re-demosaicing the color images with an advanced method <ref type="bibr" target="#b53">[54]</ref>. This experiment uses five RGB-D pairs, two of which are captured in our lab while the other three are from the NYU RGB-D dataset <ref type="bibr" target="#b54">[55]</ref>.</p><p>Fig. <ref type="figure" target="#fig_13">13</ref> shows depth recovery results for two RGB-D pairs: one is captured in our lab while the other is from the NYU RGB-D dataset <ref type="bibr" target="#b54">[55]</ref>. It is observed that the depth maps contain lots of holes around depth discontinuities due to occlusions. This corresponds to the case of synthetic datasets with structural missing areas in Section VI-A. Note that methods designed for depth upsampling are not directly applicable to  the recovery of Kinect degradations. Therefore, we compare with the IMLS <ref type="bibr" target="#b35">[36]</ref> and JBF <ref type="bibr" target="#b11">[12]</ref>. JBF produces annoying jaggy artifacts around depth discontinuities, while IMLS tends to smooth out sharp depth edges as in previous experiments. Our method outperforms the two methods, particularly in preserving prominent geometrical structures in depth maps.</p><p>The results in this section demonstrate that the proposed method is versatile for both the two types of mainstream depth cameras, and is applicable in various applications involved depth sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions and Future Work</head><p>1) Taxonomy-Based Discussions: Analogous to the taxonomy for stereo matching <ref type="bibr" target="#b0">[1]</ref>, most depth recovery methods can be classified into two categories: the global methods and local methods. Representatives of global methods include MRF-based methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, the edge-based NLM regularization <ref type="bibr" target="#b10">[11]</ref>, and our method. Most local methods use joint filtering schemes <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>The global methods have very different behaviors from the local ones regarding the interactions between observed pixels and the latent ones. In global methods, there are closed-loop interactions between observed pixels and latent pixels. For example, in solving the MRF energy function, messages iteratively exchange between neighboring pixels. Our AR-based method also has a similar mechanism as all pixels including the observed ones should conform to the autoregression. We call this type of iterations as closed-loop prediction in global methods. On the contrary, in local filtering However, the potential superiority of (closed-loop) global methods comes at the price of higher computational complexity than the (open-loop) methods, as their names imply. Generally, the running times of global methods are at the scale of 10 0 ∼ 10 1 minutes. For example, the MRF optmization in <ref type="bibr" target="#b25">[26]</ref> needs 12.5 minutes for 4× upsampling of a 200 × 200 depth map. The quadratic optimization based method in <ref type="bibr" target="#b10">[11]</ref> takes nearly half minutes for 8× upsampling to size of 1376×1088. There are two time-consuming parts in our methods: nonlocal filtering in the construction of AR predictors, and quadratic optimization as in <ref type="bibr" target="#b10">[11]</ref>. Each has the similar computational complexity. The plain Matlab implementation of our method takes two minutes on average to super-resolve a low-resolution depth map to the resolution of 1376×1088, being independent of upsampling factors. A preliminary GPU version takes 2.8 seconds on average in a desktop (i5 3GHz CPU and 4GB RAM) with an NVIDIA Tesla 2050 GPU card. For the local filtering methods, the running time is at the scale of 10 0 ∼ 10 1 seconds. For example, the guided filter <ref type="bibr" target="#b12">[13]</ref> (C++ implementation) takes about 0.48s to filtering a magepixel color image; and the CLMF method <ref type="bibr" target="#b31">[32]</ref> takes about 0.50 seconds in matching a stereo pair of size of 384 × 288. Our purpose here is not to give a precise comparison, but to grasp the scale of required computation for these two types of methods. Clearly, we observe that the local filtering methods use far less computation than global methods. An interesting point is to develop approximate algorithms of global methods to enjoy both the high accuracy of global methods and low complexity of local filtering methods.</p><p>2) Future Work: Depth recovery for a single frame is relatively well-investigated over the past few years. The remaining challenges includes: 1) accurate recovery of shining and transparent regions; 2) good complexity-quality tradeoff (as discussed above), and 3) temporally-coherent depth video recovery (e.g., the flicking issue). The first two have not been seriously addressed in the literature. Regarding depth video recovery, there have been some work to consider spatialtemporal recovery of depth squences <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper proposes a novel framework to recover depth maps from low quality measurements with various types of degradations. We show that depth maps are well described by the AR model if the AR predictors can adapt to the characteristics of depth maps. Based on this observation, we design pixel-wise adaptive AR predictors using both the depth map and the accompanied color image. The depth map is recovered by minimizing AR prediction errors subject to the observation consistency. We show that the proposed depth recovery method is equivalent to a linear system, and its stability is analyzed by the conditioning of the linear system. To achieve stable and accurate depth recovery, the parameters are adaptively set according to the local structures of the depth maps and the accompanied color images. Experiments demonstrate that our method achieves high quality depth recovery from low quality versions with various degradation. Experiments on two real systems demonstrate that our method is versatile for various depth capturing systems such as ToF cameras and Kinect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>First</head><label></label><figDesc>Attempt at AR Modeling of Depth Maps: We demonstrate that the AR model is able to tightly fit the depth signals if AR coefficients are carefully designed according to the signal characteristics. This accurate depth model brings great success in depth recovery, and also provides a promising tool for other depth-related processing. • A Unified Depth Recovery Framework With an Efficient Color-Guided AR Model: We design high performance AR predictors by fully exploiting characteristics of RGB-D data: non-local correlations, non-stationary nature of depth maps, and structural correlations in RGB-D data. Several depth enhancement problems are unified into an elegant depth recovery framework with a versatile observation model that includes four commonly-existed degradation modes. The global formulation and optimization provide inherent closed-loop interactions between observed pixels and latent pixels. This prominent feature well complements the open-loop nature of filtering-based schemes such as JBF [12], guided filter [13], and their variants [14]-[16]. • Systematic Stability Analysis and Effective Parameter Adaptation: The stability behavior of the proposed method is systematically analyzed by the conditioning of linear systems. The influence of parameters on the stability and recovery quality is investigated. Based on the analysis, we propose an effective parameter adaptation scheme to achieve stable and accurate depth recovery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of RGB-D pairs captured by ToF camera and Kinect: (a) noisy low-resolution depth map from ToF and high-resolution color image from the coupled color camera, (b) the depth samples warped from the ToF view to the color camera view, (c) Kinect color image, and (d) Kinect depth maps in which structural (random) missing is marked by yellow (blue) ellipse.</figDesc><graphic coords="2,436.53,169.10,126.90,98.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Prediction efficiency for four AR predictors: (a) the associated color image, (b) the input depth map, and prediction results of AR predictors constructed by (c) average filter, (d) Gaussian filter, (e) bilateral filter, and (f) our proposed filter. The prediction error (MAD) between the predictions in (c), (d), (e) and (f) against the original depth maps are 3.992, 3.131, 0.129, and 0.051, respectively.</figDesc><graphic coords="4,134.39,58.73,84.14,68.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>IV. COLOR-GUIDED AR MODEL FOR DEPTH RECOVERY A. Depth Recovery Based on AR Model Denote by D 0 the observed depth map and O the set of pixels with observed depth values. Given the observed depth map D 0 , we propose the following depth recovery model based on AR: min D E data D, D 0 + λE AR ( D), (3) where E data ( D, D 0 ) is the data term to make the recovered depth consistent with the observation, E AR ( D) is the AR term to impose AR model on the recovered depth map. The data term and the AR term are weighted by λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustrations for the color term of AR predictors: (a) two pixels with their neighborhoods, (b) and (c) present the enlarged versions (top row) and AR predictors for the two pixels constructed from bilateral filter (2 nd row), standard NLM filter (3 rd row), and the proposed filter (bottom row).</figDesc><graphic coords="6,49.79,58.13,162.26,144.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Condition number κ of the recovery system (left vertical axes in blue) and the recovery quality in MAD (right vertical axes in red) with respect to the parameters: (a) λ, (b) σ 1 , (c) σ 2 , (d) σ 3 , and (e) σ 4 . For σ 1 , σ 2 , and σ 4 that significantly affect the recovery quality, some representative recovered depth maps are presented for comparison and analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual quality comparison for depth upsampling on two Middlebury RGB-D pairs: (a) color image and depth ground depth maps upsampled (8×) by (b) IMLS<ref type="bibr" target="#b35">[36]</ref> (MAD: 0.61; 1.04), (c) Edge<ref type="bibr" target="#b10">[11]</ref> (MAD: 0.56; 1.03), (d) JGF<ref type="bibr" target="#b32">[33]</ref> (MAD: 0.59; 0.78), and (e) our method (MAD: 0.50; 0.64). The first and second MADs for each method are for Dolls and Art, respectively.</figDesc><graphic coords="10,50.27,366.53,170.54,69.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Visual quality comparison for recovered depth maps from ToF-like degradation (8× upsampling with intense Gaussian noise): depth maps are recovered by (a) (MAD: 3.51; 3.82), (b) IMLS [36] (MAD: 2.68; 2.86), (c) Edge [11] (MAD: 1.81; 2.40), (d) TGV [34] (MAD: 1.49; 1.75), and (e) our method (MAD: 1.15; 1.29). The two MADs for each method are for Book (first) and Reindeer (second), respectively.</figDesc><graphic coords="11,48.95,474.77,83.18,66.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Depth maps contaminated by simulated Kinect-like degradations.</figDesc><graphic coords="11,48.95,542.09,83.18,66.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 )</head><label>1</label><figDesc>ToF Depth Maps: We evaluate our method on datasets captured by two ToF-based RGB-D sensing systems: a) one is our depth-color camera rig and b) the other rig is constructed by Ferstl et al. [34]. a) Our RGB-D sensing system: Our depth camera rig is constructed by mounting a high resolution Point Grey Flea2 color camera on a PMD[vision] CamCube3 ToF depth camera. The ToF camera has a resolution of 200 × 200, and the resolution of color camera is set at 640 × 480 to obtain nearly the same field of view as the ToF camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Visual quality comparison for recovered depth maps from Kinect-like degradations: (a) degraded depth maps, depth maps recovered by (b) IMLS (MAD: 0.76; (c) JBF [12] (MAD: 0.76; 0.84), (d) CLMF0 [32] (MAD: 0.74; 1.01), and (e) our method (MAD: 0.69; 0.58). The two MADs for each method are for Dolls (first) and Art (second), respectively. For visual inspection, regions highlighted by rectangles are enlarged, and the error maps are shown by subtracting between recovered depth and ground truth.</figDesc><graphic coords="12,48.71,261.41,102.98,101.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Depth recovery results for our depth-color camera rig: (a) RDB-D pairs, recovered depth maps by (b) CLMF0<ref type="bibr" target="#b31">[32]</ref>, (c) JGF<ref type="bibr" target="#b32">[33]</ref>, (d) IMLS<ref type="bibr" target="#b35">[36]</ref>, and (e) our method. Captured maps are overlaid on the color images to save space.</figDesc><graphic coords="13,48.71,357.17,103.10,93.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visual quality comparison on depth recovery for Books from ToFMark datasets: (a) Bicubic, (b) IMLS<ref type="bibr" target="#b35">[36]</ref>, (c) CLMF0<ref type="bibr" target="#b31">[32]</ref>, (d) TGV<ref type="bibr" target="#b33">[34]</ref>, and (e) our method.</figDesc><graphic coords="13,48.71,450.17,171.62,62.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Depth recovery results for Kinect datasets: (a) RGB-D pairs, recovered depth maps by (b) IMLS<ref type="bibr" target="#b35">[36]</ref>, (c) JBF<ref type="bibr" target="#b11">[12]</ref>, and (d) our method. The RGB-D pair in the top row is in our lab, while the other is from the NYU RGB-D dataset<ref type="bibr" target="#b54">[55]</ref>.</figDesc><graphic coords="14,49.79,155.69,102.50,97.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc></figDesc><table /><note><p>DEPTH UPSAMPLING RESULTS</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We also note that some methods, e.g.,<ref type="bibr" target="#b20">[21]</ref>, use depth maps only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://cs.tju.edu.cn/faculty/likun/projects/depth_recovery/index.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to J. Park for providing recovered depth maps in <ref type="bibr" target="#b10">[11]</ref> for comparison, and thank anonymous reviewers for their comments which help to significantly improve the paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61372084, Grant 61302059, Grant 61228104, and Grant 91320201, in part by the Program for New Century Excellent Talents under Grant NCET-11-0376, in part by the Ph.D. Programs Foundation under Grant 20110032110029 through the Ministry of Education of China, and in part by the Tianjin Research Program of Application Foundation and Advanced Technology under Grant 12JCY-BJC10300 and Grant 13JCQNJC03900. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Chang-Su</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense twoframe stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparative study of energy minimization methods for Markov random fields with smoothness-based priors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1068" to="1080" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagebased 3D laser scanner</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thanusutiyabhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kanongchaiyos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Elect. Eng./Electron</title>
		<meeting>Int. Conf. Elect. Eng./Electron</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="975" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Time-of-flight cameras in computer graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="159" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data-fusion of PMD-based distance-information and high-resolution RGB-images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Signals, Circuits Syst</title>
		<meeting>Int. Symp. Signals, Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2007-07">Jul. 2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TOF imaging in smart room environments towards improved people tracking</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Guomundsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Casas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vis. Pattern Recognit. Workshops, (CVPRW)</title>
		<meeting>IEEE Comput. Vis. Pattern Recognit. Workshops, (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-step joint bilateral depth upsampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Riemens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gangwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barenbrug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berretty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Vis. Commun. Image Process</title>
		<meeting>Vis. Commun. Image ess</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint multilateral filtering for stereo image generation using depth camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Era of Interactive Media</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="373" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multiview RGB-D object dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Robot. Autom</title>
		<meeting>Int. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth recovery using an adaptive color-guided auto-regressive model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>12th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High quality depth map upsampling for 3D-TOF cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1623" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial-depth super resolution for range images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Upsampling range data in dynamic environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1141" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hybrid camera for motion deblurring and depth map super-resolution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-sensor super-resolution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE Workshop Appl. Comput. Vis. (WACV)</title>
		<meeting>6th IEEE Workshop Appl. Comput. Vis. (WACV)</meeting>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">First steps in enhancing 3D vision technique using 2D/3D sensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sluiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Winter Workshop</title>
		<meeting>Comput. Vis. Winter Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="82" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast fusion of range and video sensor data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Linarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jesorsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Microsystems for Automotive Applications</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An application of Markov random fields to range sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-quality scanning using time-of-flight depth superresolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPR)</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Increasing depth lateral resolution based on sensor fusion</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kokai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst. Technol. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="401" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A revisit to MRF-based depth map super-resolution and enhancement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pahwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating 3D time-of-flight camera data and high resolution images for 3DTV applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huhle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DTV Conf</title>
		<meeting>3DTV Conf</meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial-temporal fusion for high accuracy depth maps using dynamic MRFs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="899" to="909" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patch based synthesis for single depth image super-resolution</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fusion of range and color images for denoising and resolution enhancement with a non-local filter</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huhle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schairer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Straßer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1336" to="1345" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new super resolution technique for range data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Garro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cortelazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Associazione Gruppo Telecomunicazioni e Tecnologie dell Informazione</title>
		<meeting>Associazione Gruppo Telecomunicazioni e Tecnologie dell Informazione</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fusion of active and passive sensors for fast 3D capture</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Culbertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apostolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Multimedia Signal Process. (MMSP)</title>
		<meeting>IEEE Int. Workshop Multimedia Signal ess. (MMSP)</meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A noise-aware filter for real-time depth upsampling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Multi-Camera Multi-Modal Sensor Fusion Algorithms Appl</title>
		<meeting>Workshop Multi-Camera Multi-Modal Sensor Fusion Algorithms Appl</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth video enhancement based on joint global mode filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1176" to="1190" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-based local multipoint filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="430" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint geodesic upsampling of depth images</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image guided depth upsampling using anisotropic total generalized variation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust moving leastsquares fitting with sharp features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="544" to="552" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Superresolution and noise filtering using moving least squares</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2239" to="2248" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recovering missing depth information from Microsoft&apos;s Kinect</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dakkak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Husain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Embedded Vis. Alliance</title>
		<meeting>Embedded Vis. Alliance<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE Int. Symp. Mixed Augmented Reality (ISMAR)</title>
		<meeting>10th IEEE Int. Symp. Mixed Augmented Reality (ISMAR)</meeting>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time depth map occlusion filling and scene background restoration for projected-pattern based depth cameras</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Berdnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vatolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IETP Graph. Conf</title>
		<meeting>IETP Graph. Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shading-based shape refinement of RGB-D images</title>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1415" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An image inpainting technique based on the fast marching method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Graph. Tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal filtering for depth maps generated by Kinect depth camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matyunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vatolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Berdnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smirnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DTV Conf</title>
		<meeting>3DTV Conf</meeting>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive spatio-temporal filter for lowcost camera depth maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Emerg. Signal Process. Appl</title>
		<meeting>Int. Conf. Emerg. Signal ess. Appl</meeting>
		<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Theoretical and experimental error analysis of continuous-wave time-of-flight range cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plaue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13602</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reliability fusion of time-of-flight depth and stereo geometry for high quality depth maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1400" to="1414" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image interpolation by adaptive 2-D autoregressive modeling and soft-decision estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="896" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A spatio-temporal auto regressive model for frame rate upconversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1289" to="1301" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The application of auto-regressive time series modelling for the time-frequency analysis of civil engineering structures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woodings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Struct</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="521" to="536" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nested auto-regressive processes for MPEG-encoded video traffic modeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>The Johns Hopkins Univ. Press</publisher>
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Middlebury Datasets</title>
		<ptr target="http://vision.middlebury.edu/stereo/data/" />
		<imprint>
			<date type="published" when="2004">2013. Apr. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Open Source Computer Vision (OpenCV) [Online</title>
		<ptr target="http://opencv.org" />
		<imprint>
			<date type="published" when="2001">2012, Dec. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="http://rvlab.icg.tugraz.at/tofmark/" />
		<title level="m">ToFmark Datasets</title>
		<imprint>
			<date type="published" when="1920">2013. Dec. 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Denoising and interpolation of noisy Bayer data with adaptive cross-color filters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paliy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bilcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2008-01">Jan. 2008</date>
			<biblScope unit="volume">6822</biblScope>
			<biblScope unit="page">68221</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">NYU Datasets</title>
		<ptr target="http://cs.nyu.edu/silberman/datasets/" />
		<imprint>
			<date type="published" when="2013-12-22">2013. Dec. 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient spatio-temporal hole filling strategy for Kinect depth maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2012-02">Feb. 2012</date>
			<biblScope unit="volume">8290</biblScope>
			<biblScope unit="page">82900</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
