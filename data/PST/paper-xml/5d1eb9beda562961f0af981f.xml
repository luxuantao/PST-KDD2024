<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fourier Perspective on Model Robustness in Computer Vision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Yin</surname></persName>
							<email>dongyin@berkeley.edu</email>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Gontijo Lopes</surname></persName>
							<email>iraphael@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
							<email>shlens@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
							<email>cubuk@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
							<email>gilmer@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS UC</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain team Mountain View</orgName>
								<address>
									<postCode>94043</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain team Mountain View</orgName>
								<address>
									<postCode>94043</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain team Mountain View</orgName>
								<address>
									<postCode>94043</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain team Mountain View</orgName>
								<address>
									<postCode>94043</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Fourier Perspective on Model Robustness in Computer Vision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed tradeoffs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment [6], a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C [17] benchmark. * Work done while internship at Google Research, Brain team. † Work done as a member of the Google AI Residency program g.co/airesidency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although many deep learning computer vision models achieve remarkable performance on many standard i.i.d benchmarks, these models lack the robustness of the human vision system when the train and test distributions differ <ref type="bibr" target="#b23">[24]</ref>. For example, it has been observed that commonly occurring image corruptions, such as random noise, contrast change, and blurring, can lead to significant performance degradation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref>. Improving distributional robustness is an important step towards safely deploying models in complex, real-world settings.</p><p>Data augmentation is a natural and sometimes effective approach to learning robust models. Examples of data augmentation include adversarial training <ref type="bibr" target="#b13">[14]</ref>, applying image transformations to the training data, such as flipping, cropping, adding random noise, and even stylized image transformation <ref type="bibr" target="#b10">[11]</ref>.</p><p>However, data augmentation rarely improves robustness across all corruption types. Performance gains on some corruptions may be met with dramatic reduction on others. As an example, in <ref type="bibr" target="#b9">[10]</ref> it was observed that Gaussian data augmentation and adversarial training improve robustness to noise and blurring corruptions on the CIFAR-10-C and ImageNet-C common corruption benchmarks <ref type="bibr" target="#b16">[17]</ref>, while significantly degrading performance on the fog and contrast corruptions. This begs a natural question What is different about the corruptions for which augmentation strategies improve performance vs. those which performance is degraded?</p><p>Understanding these tensions and why they occur is an important first step towards designing robust models. Our operating hypothesis is that the frequency information of these different corruptions offers an explanation of many of these observed trade-offs. Through extensive experiments involving perturbations in the Fourier domain, we demonstrate that these two augmentation procedures bias the model towards utilizing low frequency information in the input. This low frequency bias results in improved robustness to corruptions which are more high frequency in nature while degrading performance on corruptions which are low frequency.</p><p>Our analysis suggests that more diverse data augmentation procedures could be leveraged to mitigate these observed trade-offs, and indeed this appears to be true. In particular we demonstrate that the recently proposed AutoAugment data augmentation policy <ref type="bibr" target="#b5">[6]</ref> achieves state-of-the-art results on the CIFAR-10-C benchmark. In addition, a follow-up work has utilized AutoAugment in a way to achieve state-of-the-art results on ImageNet-C <ref type="bibr" target="#b0">[1]</ref>. Some of our observations could be of interest to research on security. For example, we observe perturbations in the Fourier domain which when applied to images cause model error rates to exceed 90% on ImageNet while preserving the semantics of the image. These qualify as simple, single query <ref type="foot" target="#foot_0">3</ref> black box attacks that satisfy the content preserving threat model <ref type="bibr" target="#b12">[13]</ref>. This observation was also made in concurrent work <ref type="bibr" target="#b25">[26]</ref>.</p><p>Finally, we extend our frequency analysis to obtain a better understanding of worst-case perturbations of the input. In particular adversarial perturbations of a naturally trained model are more highfrequency in nature while adversarial training encourages these perturbations to become more concentrated in the low frequency domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We denote the 2 norm of vectors (and in general, tensors) by • . For a vector x ∈ R d , we denote its entries by x[i], i ∈ {0, . . . , d − 1}, and for a matrix X ∈ R d1×d2 , we denote its entries by X[i, j], i ∈ {0, . . . , d 1 − 1}, j ∈ {0, . . . , d 2 − 1}. We omit the dimension of image channels, and denote them by matrices X ∈ R d1×d2 . We denote by F : R d1×d2 → C d1×d2 the 2D discrete Fourier transform (DFT) and by F −1 the inverse DFT. When we visualize the Fourier spectrum, we always shift the low frequency components to the center of the spectrum.</p><p>We define high pass filtering with bandwidth B as the operation that sets all the frequency components outside of a centered square with width B in the Fourier spectrum with highest frequency in the center to zero, and then applies inverse DFT. The low pass filtering operation is defined similarly with the difference that the centered square is applied to the Fourier spectrum with low frequency shifted to the center.</p><p>We assume that the pixels take values in range [0, 1]. In all of our experiments with data augmentation we always clip the pixel values to [0, 1]. We define Gaussian data augmentation with parameter σ as the following operation: In each iteration, we add i.i.d. Gaussian noise N (0, σ 2 ) to every pixel in all the images in the training batch, where σ is chosen uniformly at random from [0, σ]. For our experiments on CIFAR-10, we use the Wide ResNet-28-10 architecture <ref type="bibr" target="#b26">[27]</ref>, and for our experiment on ImageNet, we use the ResNet-50 architecture <ref type="bibr" target="#b15">[16]</ref>. When we use Gaussin data augmentation, we choose parameter σ = 0.1 for CIFAR-10 and σ = 0.4 for ImageNet. All experiments use flip and crop during training.</p><p>Fourier heat map We will investigate the sensitivity of models to high and low frequency corruptions via a perturbation analysis in the Fourier domain. Let U i,j ∈ R d1×d2 be a real-valued matrix such that U i,j = 1, and F(U i,j ) only has up to two non-zero elements located at (i, j) and the its symmetric coordinate with respect to the image center; we call these matrices the 2D Fourier basis matrices <ref type="bibr" target="#b3">[4]</ref>.</p><p>Given a model and a validation image X, we can generate a perturbed image with Fourier basis noise. More specifically, we can compute X i,j = X + rvU i,j , where r is chosen uniformly at random from {−1, 1}, and v &gt; 0 is the norm of the perturbation. For multi-channel images, we perturb every channel independently. We can then evaluate the models under Fourier basis noise and visualize how the test error changes as a function of (i, j), and we call these results the Fourier heat map of a model. We are also interested in understanding how the outputs of the models' intermediate layers change when we perturb the images using a specific Fourier basis, and these results are relegated to the Appendix.</p><p>3 The robustness problem Figure <ref type="figure" target="#fig_0">1</ref>: Models can achieve high accuracy using information from the input that would be unrecognizable to humans. Shown above are models trained and tested with aggressive high and low pass filtering applied to the inputs. With aggressive low-pass filtering, the model is still above 30% on ImageNet when the images appear to be simple globs of color. In the case of high-pass (HP) filtering, models can achieve above 50% accuracy using features in the input that are nearly invisible to humans. As shown on the right hand side, the high pass filtered images needed be normalized in order to properly visualize the high frequency features (the method that we use to visualize the high pass filtered images is provided in the appendix).</p><p>How is it possible that models achieve such high performance in the standard settings where the training and test data are i.i.d., while performing so poorly in the presence of even subtle distributional shift? There has been substantial prior work towards obtaining a better understanding of the robustness problem. While this problem is far from being completely understood, perhaps the simplest explanation is that models lack robustness to distributional shift simply because there is no reason for them to be robust <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. In naturally occurring data there are many correlations between the input and target that models can utilize to generalize well. However, utilizing such sufficient statistics will lead to dramatic reduction in model performance should these same statistics become corrupted at test time.</p><p>As a simple example of this principle, consider Figure <ref type="figure">8</ref> in <ref type="bibr" target="#b18">[19]</ref>. The authors experimented with training models on a "cheating" variant of MNIST, where the target label is encoded by the location of a single pixel. Models tested on images with this "cheating" pixel removed would perform poorly. This is an unfortunate setting where Occam's razor can fail. The simplest explanation of the data may generalize well in perfect settings where the training and test data are i.i.d., but fail to generalize robustly. Although this example is artificial, it is clear that model brittleness is tied to latching onto non-robust statistics in naturally occurring data. As a more realistic example, consider the recently proposed texture hypothesis <ref type="bibr" target="#b10">[11]</ref>. Models trained on natural image data can obtain high classification performance relying on local statistics that are correlated with texture. However, texture-like information can become easily distorted due to naturally occurring corruptions caused by weather or digital artifacts, leading to poor robustness.</p><p>In the image domain, there is a plethora of correlations between the input and target. Simple statistics such as colors, local textures, shapes, even unintuitive high frequency patterns can all be leveraged in a way to achieve remarkable i.i.d generalization. To demonstrate, we experimented with training and testing of ImageNet models when severe filtering is performed on the input in the frequency domain. While modest filtering has been used for model compression <ref type="bibr" target="#b8">[9]</ref>, we experiment with extreme filtering in order to test the limits of model generalization. The results are shown in Figure <ref type="figure" target="#fig_0">1</ref>. When low-frequency filtering is applied, models can achieve over 30% test accuracy even when the image appears to be simple globs of color. Even more striking, models achieve 50% accuracy in the presence of the severe high frequency filtering, using high frequency features which are nearly invisible to humans. In order to even visualize these high frequency features, we had normalize pixel statistics to have unit variance. Given that these types features are useful for generalization, it is not so surprising that models leverage these non-robust statistics.</p><p>It seems likely that these invisible high frequency features are related to the experiments of <ref type="bibr" target="#b17">[18]</ref>, which show that certain imperceptibly perturbed images contain features which are useful for generalization. We discuss these connections more in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Trade-off and correlation between corruptions: a Fourier perspective</head><p>The previous section demonstrated that both high and low frequency features are useful for classification. A natural hypothesis is that data augmentation may bias the model towards utilizing different kinds of features in classification. What types of features models utilize will ultimately determine the robustness at test time. Here we adopt a Fourier perspective to study the trade-off and correlation between corruptions when we apply several data augmentation methods.  We hypothesize that some of these trade-offs can be explained by the Fourier statistics of different corruptions. Denote a (possibly randomized) corruption function by C : R d1×d2 → R d1×d2 . In Figure <ref type="figure" target="#fig_1">2</ref> we visualize the Fourier statistics of natural images as well as the average delta of the common corruptions. Natural images have higher concentrations in low frequencies, thus when we refer to a "high" or "low" frequency corruption we will always use this term on a relative scale. Gaussian noise is uniformly distributed across the Fourier frequencies and thus has much higher frequency statistics relative to natural images. Many of the blurring corruptions remove or change the high frequency content of images. As a result C(X) − X will have a higher fraction of high frequency energy. For corruptions such as contrast and fog, the energy of the corruption is concentrated more on low frequency components.</p><p>The observed differences in the Fourier statistics suggests an explanation for why the two augmentation methods improve performance in additive noise but not fog and contrast -the two augmentation methods encourage the model to become invariant to high frequency information while relying more on low frequency information. We investigate this hypothesis via several perturbation analyses of the three models in question. First, we test model sensitivity to perturbations along each Fourier basis vector. Results on CIFAR-10 are shown in Figure <ref type="figure" target="#fig_2">3</ref>. The difference between the three models is striking. The naturally trained model is highly sensitive to additive perturbations in all but the lowest frequencies, while Gaussian data augmentation and adversarial training both dramatically improve robustness in the higher frequencies. For the models trained with data augmentation, we see a subtle but distinct lack of robustness at the lowest frequencies (relative to the naturally trained model). Figure <ref type="figure" target="#fig_3">4</ref> shows similar results for three different models on ImageNet. Similar to CIFAR-10, Gaussian data augmentation improves robustness to high frequency perturbations while reducing performance on low frequency perturbations. Again, the naturally trained model is highly sensitive to additive noise in all but the lowest frequencies. On the other hand, Gaussian data augmentation improves robustness in the higher frequencies while sacrificing the robustness to low frequency perturbations. For AutoAugment, we observe that its Fourier heat map has the largest blue/yellow area around the center, indicating that AutoAugment is relatively robust to low to mid frequency corruptions.</p><p>To test this further, we added noise with fixed 2 norm but different frequency bandwidths centered at the origin. We consider two settings, one where the origin is centered at the lowest frequency and one where the origin is centered at the highest frequency. As shown in Figure <ref type="figure">5</ref>, for a low frequency centered bandwidth of size 3, the naturally trained model has less than half the error rate of the other two models. For high frequency bandwidth, the models trained with data augmentation dramatically outperform the naturally trained model.   This is consistent with the hypothesis that the models trained with the noise augmentation are biased towards low frequency information. As a final test, we analyzed the performance of models with a low/high pass filter applied to the input (we call the low/high pass filters the front end of the model).</p><p>Consistent with prior experiments we find that applying a low pass front-end degrades performance on fog and contrast while improving performance on additive noise and blurring. If we instead further bias the model towards high frequency information we observe the opposite effect. Applying a high-pass front end degrades performance on all corruptions (as well as clean test error), but performance degradation is more severe on the high frequency corruptions. These experiments again confirm our hypothesis about the robustness properties of models with a high (or low) frequency bias.</p><p>To better quantify the relationship between frequency and robustness for various models we measure the ratio of energy in the high and low frequency domain. For each corruption C, we apply high pass filtering with bandwidth 27 (denote this operation by H(•)) on the delta of the corruption, i.e., C(X) − X. We use</p><formula xml:id="formula_0">H(C(X)−X) 2 C(X)−X 2</formula><p>as a metric of the fraction of high frequency energy in the corruption. For each corruption, we average this quantity over all the validation images and all 5 severities. We evaluate 6 models on CIFAR-10-C, each trained differently -natural training, Gaussian data augmentation, adversarial training, trained with a low pass filter front end (bandwidth 15), trained with a high pass filter front end (bandwidth 31), and trained with AutoAugment (see a more detailed discussion on AutoAugment in Section 4.3). Results are shown in Figure <ref type="figure" target="#fig_7">6</ref>. Models with a low frequency bias perform better on the high frequency corruptions. The model trained with a high pass filter has a forced high frequency bias. While this model performs relatively poorly on even natural data, it is clear that high frequency corruptions degrade performance more than the low frequency corruptions. Full results, including those on ImageNet, can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Does low frequency data augmentation improve robustness to low frequency corruptions?</head><p>While Figure <ref type="figure" target="#fig_7">6</ref> shows a clear relationship between frequency and robustness gains of several data augmentation strategies, the Fourier perspective is not predictive in all situations of transfer between data augmentation and robustness.</p><p>We experimented with applying additive noise that matches the statistics of the fog corruption in the frequency domain. We define "fog noise" to be the additive noise distribution i,j N (0, σ 2 i,j )U i,j where the σ i,j are chosen to match the typical norm of the fog corruption on basis vector U i,j as  shown in Figure <ref type="figure" target="#fig_1">2</ref>. In particular, the marginal statistics of fog noise are identical to the fog corruption in the Fourier domain. However, data augmentation on fog noise degrades performance on the fog corruption (Table <ref type="table" target="#tab_1">1</ref>). This occurs despite the fact that the resulting model yields improved robustness to perturbations along the low frequency vectors (see the Fourier heat maps in the appendix).</p><p>fog severity 1 2 3 4 5 naturally trained 0.9606 0.9484 0.9395 0.9072 0.7429 fog noise augmentation 0.9090 0.8726 0.8120 0.7175 0.4626 We hypothesize that the story is more complicated for low frequency corruptions because of an asymmetry between high and low frequency information in natural images. Given that natural images are concentrated more in low frequencies, a model can more easily learn to "ignore" high frequency information rather than low frequency information. Indeed as shown in Figure <ref type="figure" target="#fig_0">1</ref>, model performance drops off far more rapidly when low frequency information is removed than high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">More varied data augmentation offers more general robustness</head><p>The trade-offs between low and high frequency corruptions for Gaussian data augmentation and adversarial training lead to the natural question of how to achieve robustness to a more diverse set of corruptions. One intuitive solution is to train on a variety of data augmentation strategies. Towards this end, we investigated the learned augmentation policy AutoAugment <ref type="bibr" target="#b5">[6]</ref>. AutoAugment applies a learned mixture of image transformations during training and achieves the state-of-theart performance on CIFAR-10 and ImageNet. In all of our experiments with AutoAugment, we remove the brightness and constrast sub-policies as they explicitly appear in the common corruption benchmarks. <ref type="foot" target="#foot_1">4</ref> Despite the fact that this policy was tuned specifically for clean test accuracy, we found that it also dramatically improves robustness on CIFAR-10-C. Here, we demonstrate part of the results in Table <ref type="table">2</ref>, and the full results can be found in the appendix. In the third plot in Figure <ref type="figure" target="#fig_7">6</ref>, we also visualize the performance of AutoAugment on CIFAR-10-C. More specifically, on CIFAR-10-C, we compare the robustness of the naturally trained model, Gaussian data augmentation, adversarially trained model, and AutoAugment. We observe that among the four models, AutoAugment achieves the best average corruption test accuracy of 86%. Using the mean corruption error (mCE) metric proposed in <ref type="bibr" target="#b16">[17]</ref> with the naturally trained model being the baseline (see a formal definition of mCE in the appendix), we observe that AutoAugment achieves the best mCE of 64, and in comparison, Gaussian data augmentation and adversarial training achieve mCE of 98 and 108, respectively. In addition, as we can see, AutoAugment improves robustness on all but one of the corruptions, compared to the naturally trained model. Table <ref type="table">2</ref>: Comprison between naturally trained model (natural), Gaussian data augmentation (Gauss), adversarially trained model (adversarial), and AutoAugment (Auto) on CIFAR-10-C. We remove all corruptions that appear in this benchmark from the AutoAugment policy. All numbers are in percentage. The first column shows the average top1 test accuracy on all the corruptions; the second column shows the mCE; the rest of the columns show the average test accuracy over the 5 severities for each corruption. We observe that AutoAugment achieves the best average test accuracy and the best mCE. In most of the blurring and all of the weather corruptions, AutoAugment achieves the best performance among the four models.</p><p>As for the ImageNet-C benchmark, instead of using the compressed ImageNet-C images provided in <ref type="bibr" target="#b16">[17]</ref>, we evaluate the models on corruptions applied in memory, <ref type="foot" target="#foot_2">5</ref> and observe that AutoAugment also achieves the highest average corruption test accuracy. The full results can be found in the appendix. As for the compressed ImageNet-C images, we note that a follow-up work has utilized AutoAugment in a way to achieve state-of-the-art results <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adversarial examples are not strictly a high frequency phenomenon</head><p>Adversarial perturbations remain a popular topic of study in the machine learning community. A common hypothesis is that adversarial perturbations lie primarily in the high frequency domain. In fact, several (unsuccessful) defenses have been proposed motivated specifically by this hypothesis. Under the assumption that compression removes high frequency information, JPEG compression has been proposed several times <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref> as a method for improving robustness to small perturbations.</p><p>Studying the statistics of adversarially generated perturbations is not a well defined problem because these statistics will ultimately depend on how the adversary constructs the perturbation. This difficulty has led to many false claims of methods for detecting adversarial perturbations <ref type="bibr" target="#b4">[5]</ref>. Thus the analysis presented here is to better understand common hypothesis about adversarial perturbations, rather than actually detect all possible perturbations.</p><p>For several models we use PGD to construct adversarial perturbations for every image in the test set.</p><p>We then analyze the delta between the clean and perturbed images and project these deltas into the Fourier domain. By aggregating across the successful attack images, we obtain an understanding of the frequency properties of the constructed adversarial perturbations. The results are shown in Figure <ref type="figure" target="#fig_8">7</ref>.</p><p>For the naturally trained model, the measured adversarial perturbations do indeed show higher concentrations in the high frequency domain (relative to the statistics of natural images). However, for the adversarially trained model this is no longer the case. The deltas for the adversarially trained model resemble that of natural data. Our analysis provides some additional understanding on a number of observations in prior works on adversarial examples. First, while adversarial perturbations for the naturally trained model do indeed show higher concentrations in the high frequency domain, this does not mean that removing high frequency information from the input results in a robust model. Indeed as shown in Figure <ref type="figure" target="#fig_2">3</ref>, the naturally trained model is not worst-case or even average-case robust on any frequency (except perhaps the extreme low frequencies). Thus, we should expect that if we adversarially searched for errors in the low frequency domain, we will find them easily. This explains why JPEG compression, or any other method based on specifically removing high frequency content, should not be expected to be robust to worst-case perturbations.</p><p>Second, the fact that adversarial training biases these perturbations towards the lower frequencies suggests an intriguing connection between adversarial training and the DeepViz <ref type="bibr" target="#b22">[23]</ref> method for feature visualization. In particular, optimizing the input in the low frequency domain is one of the strategies utilized by DeepViz to bias the optimization in the image space towards semantically meaningful directions. Perhaps the reason adversarially trained models have semantically meaningful gradients <ref type="bibr" target="#b24">[25]</ref> is because gradients are biased towards low frequencies in a similar manner as utilized in DeepViz. As a final note, we observe that adding certain Fourier basis vectors with large norm (24 for ImageNet) degrades test accuracy to less than 10% while preserving the semantics of the image. Two examples of the perturbed images are shown in Figure <ref type="figure" target="#fig_8">7</ref>. If additional model queries are allowed, subtler perturbations will suffice -the perturbations used in Figure <ref type="figure" target="#fig_3">4</ref> can drop accuracies to less than 30%. Thus, these Fourier basis corruptions can be considered as content-preserving black box attacks, and could be of interest to research on security. Fourier heat maps with larger perturbations are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>We obtained a better understanding of trade-offs observed in recent robustness work in the image domain. By investigating common corruptions and model performance in the frequency domain we establish connections between frequency of a corruption and model performance under data augmentation. This connection is strongest for high frequency corruptions, where Gaussian data augmentation and adversarial training bias the model towards low frequency information in the input. This results in improved robustness to corruptions with higher concentrations in the high frequency domain at the cost of reduced robustness to low frequency corruptions and clean test error.</p><p>Solving the robustness problem via data augmentation alone feels quite challenging given the tradeoffs we commonly observe. Naively augmenting on different corruptions often will not transfer well to held out corruptions <ref type="bibr" target="#b11">[12]</ref>. However, the impressive robustness of AutoAugment gives us hope that data augmentation done properly can play a crucial role in mitigating the robustness problem.</p><p>Care must be taken though when utilizing data augmentation for robustness to not overfit to the validation set of held out corruptions. The goal is to learn domain invariant features rather than simply become robust to a specific set of corruptions. The fact that AutoAugment was tuned specifically for clean test error, and transfers well even after removing the contrast and brightness parts of the policy (as these corruptions appear in the benchmark) gives us hope that this is a step towards more useful domain invariant features. The robustness problem is certainly far from solved, and our Fourier analysis shows that the AutoAugment model is not strictly more robust than the baseline -there are frequencies for which robustness is degraded rather than improved. Because of this, we anticipate that robustness benchmarks will need to evolve over time as progress is made. These trade-offs are to be expected and researchers should actively search for new blindspots induced by the methods they introduce. As we grow in our understanding of these trade-offs we can design better benchmarks to obtain a more comprehensive perspective on model robustness.</p><p>While data augmentation is perhaps the most effective method we currently have for the robustness problem, it seems unlikely that data augmentation alone will provide a complete solution. Towards that end it will be important to develop orthogonal methods -e.g. architectures with better inductive biases or loss functions which when combined with data augmentation encourage extrapolation rather than interpolation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4. 1</head><label>1</label><figDesc>Gaussian data augmentation and adversarial training bias models towards low frequency information Ford et al. [10] investigated the robustness of three models on CIFAR-10-C: a naturally trained model, a model trained by Gaussian data augmentation, and an adversarially trained model. It was observed that Gaussian data augmentation and adversarial training improve robustness to all noise and many of the blurring corruptions, while degrading robustness to fog and contrast. For example adversarial training degrades performance on the most severe contrast corruption from 85.66% to 55.29%. Similar results were reported on ImageNet-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Fourier spectrum of natural images; we estimate E[|F(X)[i, j]|] by averaging all the CIFAR-10 validation images. Right: Fourier spectrum of the corruptions in CIFAR-10-C at severity 3. For each corruption, we estimate E[|F(C(X) − X)[i, j]|] by averaging over all the validation images. Additive noise has relatively high concentrations in high frequencies while some corruptions such as fog and contrast are concentrated in low frequencies.</figDesc><graphic url="image-2.png" coords="4,133.00,452.71,105.64,105.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model sensitivity to additive noise aligned with different Fourier basis vectors on CIFAR-10. We fix the additive noise to have 2 norm 4 and evaluate three models: a naturally trained model, an adversarially trained model, and a model trained with Gaussian data augmentation. Error rates are averaged over 1000 randomly sampled images from the test set.In the bottom row we show images perturbed with noise along the corresponding Fourier basis vector. The naturally trained model is highly sensitive to additive noise in all but the lowest frequencies. Both adversarial training and Gaussian data augmentation dramatically improve robustness in the higher frequencies while sacrificing the robustness of the naturally trained model in the lowest frequencies (i.e. in both models, blue area in the middle is smaller compared to that of the naturally trained model).</figDesc><graphic url="image-18.png" coords="5,147.60,72.00,316.81,143.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model sensitivity to additive noise aligned with different Fourier basis vectors on ImageNet validation images. We fix the basis vectors to have 2 norm 15.7.Error rates are averaged over the entire ImageNet validation set. We present the 63 × 63 square centered at the lowest frequency in the Fourier domain. Again, the naturally trained model is highly sensitive to additive noise in all but the lowest frequencies. On the other hand, Gaussian data augmentation improves robustness in the higher frequencies while sacrificing the robustness to low frequency perturbations. For AutoAugment, we observe that its Fourier heat map has the largest blue/yellow area around the center, indicating that AutoAugment is relatively robust to low to mid frequency corruptions.</figDesc><graphic url="image-19.png" coords="5,147.60,502.52,316.80,140.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><label></label><figDesc>Figure5: Robustness of models under additive noise with fixed norm and different frequency distribution. For each channel in each CIFAR-10 test image, we sample i.i.d Gaussian noise, apply a low/high pass filter, and normalize the filtered noise to have 2 norm 8, before applying to the image. We vary the bandwidth of the low/high pass filter and generate the two plots. The naturally trained model is more robust to the low frequency noise with bandwidth 3, while Gaussian data augmentation and adversarial training make the model more robust to high frequency noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Relationship between test accuracy and fraction of high frequency energy of the CIFAR-10-C corruptions. Each scatter point in the plot represents the evaluation result of a particular model on a particular corruption type. The x-axis represents the fraction of high frequency energy of the corruption type, and the y-axis represents change in test accuracy compared to a naturally trained model. Overall, Gaussian data augmentation, adversarial training, and adding low pass filter improve robustness to high frequency corruptions, and degrade robustness to low frequency corruptions. Applying a high pass filter front end yields a more significant accuracy drop on high frequency corruptions compared to low frequency corruptions. AutoAugment improves robustness on nearly all corruptions, and achieves the best overall performance. The legend at the bottom shows the slope (k) and residual (r) of each fitted line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (a) and (b): Fourier spectrum of adversarial perturbations. For any image X, we run the PGD attack [22] to generate an adversarial example C(X). We estimate the Fourier spectrum of the adversarial perturbation, i.e., E[|F(C(X) − X)[i, j]|], where the expectation is taken over the perturbed images which are incorrectly classified. (a) naturally trained; (b) adversarially trained. The adversarial perturbations for the naturally trained model are uniformly distributed across frequency components. In comparison, adversarial training biases these perturbations towards the lower frequencies. (c) and (d): Adding Fourier basis vectors with large norm to images is a simple method for generating content-preserving black box adversarial examples.</figDesc><graphic url="image-21.png" coords="9,108.00,72.00,396.00,111.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>5: Robustness of models under additive noise with fixed norm and different frequency distribution. For each channel in each CIFAR-10 test image, we sample i.i.d Gaussian noise, apply a low/high pass filter, and normalize the filtered noise to have 2 norm 8, before applying to the image. We vary the bandwidth of the low/high pass filter and generate the two plots. The naturally trained model is more robust to the low frequency noise with bandwidth 3, while Gaussian data augmentation and adversarial training make the model more robust to high frequency noise.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training with fog noise hurts performance on fog corruption.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">In contrast, methods for generating small adversarial perturbations require 1000's of queries<ref type="bibr" target="#b14">[15]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">Our experiment is based on the open source implementation of AutoAugment at https://github.com/tensorflow/models/tree/master/research/autoaugment.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">The dataset of images with corruptions in memory can be found at https://github.com/tensorflow/ datasets/blob/master/tensorflow_datasets/image/imagenet2012_corrupted.py.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Nicolas Ford and Norman Mu for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AugMix: A simple method to improve robustness and uncertainty under data shift</title>
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>under review</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The effects of JPEG and JPEG2000 compression on attacks using adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Aydemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Temizel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Temizel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why do deep convolutional networks generalize so poorly to small image transformations?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12177</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Fourier transform and its applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>McGraw-Hill</publisher>
			<biblScope unit="volume">31999</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning augmentation policies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shield: Fast, practical defense and vaccination for deep learning using jpeg compression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="196" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 26th International Conference on Computer Communication and Networks (ICCCN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Band-limited training and inference for convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paparrizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1745" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial examples are a natural consequence of test error in noise</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2280" to="2289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNettrained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalisation in humans and deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Temme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7538" to="7550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<title level="m">Motivating the rules of the game for adversarial example research</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07121</idno>
		<title level="m">Simple black-box adversarial attacks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02175</idno>
		<title level="m">Adversarial examples are not bugs, they are features</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Excessive invariance causes adversarial vulnerability</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Measuring the tendency of CNNs to learn surface statistical regularities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11561</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature distillation: DNN-oriented JPEG compression against adversarial examples</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feature visualization. Distill</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<ptr target="https://distill.pub/2017/feature-visualization" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the structural sensitivity of deep convolutional networks to the directions of fourier basis functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
				<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
