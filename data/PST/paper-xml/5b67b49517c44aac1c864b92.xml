<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech Emotion Recognition Based on An Improved Brain Emotion Learning Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-09">May 9, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Guido</forename><forename type="middle">R</forename><surname>Capobianco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen-Tao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiao</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wei-Hua</forename><surname>Cao</surname></persName>
							<email>weihuacao@cug.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Mei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">Hunan University of Arts and Science</orgName>
								<address>
									<postCode>415000</postCode>
									<settlement>Changde</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun-Wei</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speech Emotion Recognition Based on An Improved Brain Emotion Learning Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-09">May 9, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">BFD0B4AAEC65EF8913F12AD1B58A60E5</idno>
					<idno type="DOI">10.1016/j.neucom.2018.05.005</idno>
					<note type="submission">Received date: 21 October 2017 Revised date: 3 April 2018 Accepted date: 3 May 2018 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech</term>
					<term>Emotion Recognition</term>
					<term>Brain-inspired</term>
					<term>Brain Emotion Learning</term>
					<term>Genetic Algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-robot emotional interaction has developed rapidly in recent years, in which speech emotion recognition plays a significant role. In this paper, a speech emotion recognition method based on an improved brain emotional learning (BEL) model is proposed, which is inspired by the emotional processing mechanism of the limbic system in the brain. The reinforcement learning rule of BEL model, however, makes it have poor adaptation and affects its performance. To solve these problems, Genetic Algorithm (GA) is employed to update the weights of BEL model. The proposal is tested on the CASIA Chinese emotion corpus, SAVEE emotion corpus, and FAU Aibo dataset, in which MFCC related features and their 1st order delta coefficients are extracted, and the proposal is tested on INTERSPEECH 2009 standard feature set, and three dimensionality reduction methods of Linear Discriminant Analysis (LDA), Principal Component Analysis (PCA), and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-robot emotional communication and emotional robot have been widely developed and applied into series of areas <ref type="bibr" target="#b0">[1]</ref>. One of the most important abilities of emotional robot is emotion recognition, which mainly includes speech emotion recognition <ref type="bibr" target="#b1">[2]</ref>, facial expression recognition <ref type="bibr" target="#b2">[3]</ref>, and body language emotion recognition <ref type="bibr" target="#b3">[4]</ref>. To make human communicate with robot smoothly and harmoniously, it is necessary to recognize humans' emotional states as accurate as possible for robot. As speech signal is easy to access and easy to understand, it is widely used for emotion recognition in human-robot interaction <ref type="bibr" target="#b4">[5]</ref>.</p><p>In recent years, both speaker-dependent (SD) and speaker-independent (SI) speech emotion recognition have attracted much attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, in which Artificial Neural Network(ANN) <ref type="bibr" target="#b7">[8]</ref>, Support Vector Machine (SVM) <ref type="bibr" target="#b6">[7]</ref>, and Hidden Markov Model (HMM) <ref type="bibr" target="#b8">[9]</ref> are commonly adopted in speech emotion classification. In addition, deep learning is being paid more attention in speech emotion recognition to achieve better performance, for instance, Deep LSTM Recurrent Neural Networks, Deep Convolutional Neural Networks (DCNN), and Deep Brief Network (DBN) have been used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> to solve types of issues of speech emotion recognition. However, these deep neural networks have large computation complexity and their performance significantly depends on the size of samples.</p><p>With the development of brain research, the understanding of brain neural mechanism has been greatly improved. Therefore, brain-inspired artificial intelligence has become a hot topic in artificial intelligence, in which emotion recognition is an important branch. At present, there are many researches on emotion model, including brain emotion learning model (BEL) presented in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T <ref type="bibr" target="#b13">[14]</ref>. It is inspired by a neurobiological mechanism that information response between the amygdala and orbitofrontal cortex in the brain can produce emotion <ref type="bibr" target="#b14">[15]</ref>.</p><p>The BEL model has a simple structure in modeling limbic system of the brain which is regarded as an important part of the emotional process, and it has lower computational complexity compared with other conventional neural networks <ref type="bibr" target="#b15">[16]</ref>. Therefore, it has been used in classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and prediction <ref type="bibr" target="#b18">[19]</ref>. However, the weights of amygdala and orbitofrontal cortex in BEL model are adjusted by the reward signal which is not clearly defined so far. And different reward signal setting methods would make a great difference in training time, convergence, and generalization ability, which may affect the performance of the BEL model.</p><p>To solve the problems mentioned above, a speech emotion recognition method based on an improved BEL model is proposed, in which Genetic Algorithm (GA) is employed to optimize the weights of BEL model (GA-BEL). GA is a global, random and adaptive algorithm with stronger parallel processing capability <ref type="bibr" target="#b19">[20]</ref>. Compared with other optimization algorithms, such as simulated annealing algorithm (SA) and particle swarm optimization (PSO), the GA has a superior global searching ability, and it can quickly search all the solutions in the solution space without falling into the trap of the local optimum solution <ref type="bibr" target="#b20">[21]</ref>. As a result, the proposed GA-BEL integrates the low computational complexity of BEL model and superior global searching ability of GA. In addition, LeDoux's research finds that the limbic system is related to the response to auditory stimulus and human emotion <ref type="bibr" target="#b21">[22]</ref>, therefore, the improved BEL model is employed for speech emotion recognition.</p><p>In this paper, CASIA <ref type="bibr" target="#b22">[23]</ref>, SAVEE <ref type="bibr" target="#b23">[24]</ref> emotion corpus, and FAU Aibo dataset <ref type="bibr" target="#b24">[25]</ref> are used for experiment of speech emotion recognition, in which Low Level Descriptors (LLDs) of MFCC related features and their 1st order delta coefficients are extracted, and the proposal is tested on INTER-SPEECH 2009 standard feature set <ref type="bibr" target="#b25">[26]</ref> as well to verify the robustness. Meanwhile, LDA, PCA, and PCA+LDA are used to reduce the dimension of the feature set, respectively. In addition, two feature sets of row LLDs and row LLDs+1∆ coefficients are compared in terms of the speech emotion recognition accuracy. In this work, the performance of original BEL model and some conventional methods are also presented for comparison to verify the feasibility of the proposal.</p><p>The rest of paper is organized as follows. The overview of anatomical</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula><p>basis and BEL model is introduced in Section 2. The feature extraction and dimensionality reduction method are presented in Section 3. The speech emotion recognition method based on GA-BEL is proposed in Section 4. Experiments on speech emotion recognition and discussion are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of Anatomical Basis and BEL Model</head><p>In past decades, many researches on auditory pathway have been done <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and the understanding of auditory mechanism is becoming clearer gradually. In Watts's research, the auditory pathway is composed of several different levels, which are responsible for different functions, such as speech perception and music perception <ref type="bibr" target="#b27">[28]</ref>. The discussed human auditory pathway for speech perception is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which shows that limbic system plays a significant role in speech perception.  What's more, some researches have suggested that emotional responses to stimulus take place in the limbic system <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>. Fig. <ref type="figure">2</ref> presents the primary structures of the limbic system, and it is composed of four primary structures, i.e, thalamus, sensory cortex, amygdala, and orbitofrontal cortex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Figure <ref type="figure">2</ref>: The anatomy and primary structures of Limbic system <ref type="bibr" target="#b16">[17]</ref>.</p><p>The BEL model is inspired by the emotional circuits in the limbic system <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>. Its framework is shown in Fig. <ref type="figure" target="#fig_1">3</ref>, in which the thalamus and sensory cortex play important roles in input signals processing. And the central structure of the limbic system is the amygdala, which is the critical part of processing and learning the emotion. Meanwhile, amygdala receives the external reward signal, which has an influence on emotion learning in the amygdala. Another essential part is orbitofrontal cortex, which is responsible for adjusting the emotion processing in the amygdala to avoid inappropriate learning connection. In the BEL model, SI is the sensory input; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SI</head><formula xml:id="formula_1">∆v i = α(SI • max(0, REW - j A j )),<label>(1)</label></formula><formula xml:id="formula_2">∆w i = β(SI • j (O j -REW )),<label>(2)</label></formula><p>where v i represents the weight of amygdala and w i is the weight of orbitofrintal cortex. In addition, α and β are learning rates, which usually range from 0 to 1. As shown in Eq.( <ref type="formula" target="#formula_1">1</ref>) and Eq.( <ref type="formula" target="#formula_2">2</ref>), the reward signal is significantly related with weight updating. However, the reward signal is not clearly defined so far, and it is calculated differently in previous researches.</p><p>As BEL model has a simple structure and good performance in learning application, it is widely used and improved for classification, prediction, and control <ref type="bibr" target="#b31">[32]</ref>. To overcome the shortcoming that model can not be generalized to other issues caused by reinforcement learning, ADBEL <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> and NFAD-BEL <ref type="bibr" target="#b34">[35]</ref> are proposed for real-time and online prediction, in which adaptive decayed mechanism is considered in BEL model. What's more, brain emotional learning model combined with fuzzy inference system (BELFIS) is proposed, in which neuro-fuzzy network is integrated into amygdala and orbitofrontal cortex subsystem in BEL to improve the performance of BEL model, and it has been used for speech emotion recognition <ref type="bibr" target="#b28">[29]</ref> and other classification problem <ref type="bibr" target="#b35">[36]</ref>.</p><p>Furthermore, BEL model has been applied in visual pattern recognition based on its physiological mechanism of visual processing in the brain. ACD-BEL is proposed in <ref type="bibr" target="#b36">[37]</ref> for facial detection and facial expression recognition, in which anxious confident decayed and common appraisal-anatomical modeling of emotion is employed. Brain emotional learning based picture classifier (BELPIC) is proposed in <ref type="bibr" target="#b37">[38]</ref> for image classification. In addition, a visual object recognition method of modified BELPIC is proposed inspired by fast processing of emotions in the brain <ref type="bibr" target="#b38">[39]</ref>, which considers the functionality of the amygdala and the orbitofrontal cortex as threshold logic units. Firstly, the initial feature set including the features of 21 statistic functionals of MFCC 0-14, LogMelFreqBand 0-7, and their 1st order delta coefficients are exacted. Secondly, Linear Discriminant Analysis (LDA), Principal Component Analysis (PCA), and PCA+LDA are used to reduce the dimension of feature set and their performance is compared with each other. Finally, speech emotion is classified into different category using the improved BEL model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>Prosodic features and spectral features are the most important characteristics of speech recognition <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. Spectral features are considered to be the manifestation of the correlation between vocal movements and changes of channel shape. Mel Frequency Cepstral Coefficients (MFCC) is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T one of spectral features, which is widely used in speech emotion recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, owning to its advantages of simple calculation, superior ability of distinction and high robustness to noise <ref type="bibr" target="#b42">[43]</ref>.</p><p>MFCC is extracted inspired by the mechanism of human auditory system, i.e., the sensitivity that human ears perceive sound waves of different frequencies is non-linear. Therefore, there is a nonlinear relationship between MFCC and frequency. The extraction process of MFCC is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. To extract MFCC, the speech frame passes through a hamming window at first, then the discrete spectrum is calculated by a discrete Fourier transformation (DFT). And a Mel filter is used to remove the influence of harmonics. Finally, the discrete cosine transformation (DCT) is used to calculate MFCC. In this paper, MFCC-related features including 21 statistic functionals of MFCC 0-14, LogMelFreqBand 0-7, and their 1st order delta coefficient are extracted as the feature set for speech emotion recognition. The features are extracted using openSMILE based on INTERSPEECH2010 <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. The specific description of the features used in this paper are shown in Table <ref type="table">1</ref>.</p><p>Table <ref type="table">1</ref>: Description of initial Feature Set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low Level Descriptors</head><p>MFCC 0-14, LogMelFreqBand 0-7 1st order delta coefficient of MFCC and LogMelFreqBand 21 Statistic Functionals maxPos, minPos, amean, linregc1, linregc2, linregerrA, linregerrQ, stddev, quartile1, quartile2, quartile3, kurtosis, skeweness,iqr1-2, iqr2-3, iqr1-3, percentile1.0, percentile99.0, pctlrange0-1, upleveltime75, upleveltime90</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Dimensionality Reduction</head><p>In general, there is great possibility that the high-dimensionality has influence on classification accuracy and efficiency. Therefore, feature dimension should be reduced to ensure ideal accuracy and shorter computing time. And Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are main dimensionality reduction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>PCA is an unsupervised learning dimensionality reduction method by mapping high-dimensionality feature set to low-dimensionality space, in which feature set is decomposed by the covariance matrix to obtain the principal components and weights of feature set, and then select the representative eigenvectors according to the weight of feature set <ref type="bibr" target="#b46">[47]</ref>. It utilises the variance of data projection to assess the amount of feature representation information. And the larger variance suggests the more information it carries. Therefore, PCA is intended to select first k-dimension features with the largest variance to ensure the data after projection satisfies the variance maximization. The data after projection through this dimensionality reduction method is as close as possible to the original data, however, it does not focus on exploring the internal structural features of the data.</p><p>Different from PCA, LDA is a supervised dimensionality reduction method which measures the information with the difference of labels and categories. Meanwhile, the data dimension after dimensionality reduction by LDA depends on the number of classes, which are more exact than that of PCA. Therefore, LDA can effectively reduce the dimensionality of features while improving the discrimination between different categories of features <ref type="bibr" target="#b46">[47]</ref>.</p><p>LDA aims to select a best projection direction through the Fisher function to obtain the maximum between-class scatter and the minimum within-class scatter after pattern projection. In other words, it can guarantee that there are the minimum within-class distance and the largest between-class distance in the new space after pattern projection, that is, the pattern has the best separability in that space. The procedure of dimensionality reduction through LDA can be concluded as follows.</p><p>Step 1: Standardize the d-dimension initial feature set.</p><p>Step 2: Calculate the mean vector for each category.</p><p>Step 3: Calculate within-class scatter matrix S B and between-class scatter matrix S W .</p><p>Step 4: Calculate the eigenvalues of matrix S -1 W S B and their corresponding eigenvectors.</p><p>Step 5: Select corresponding eigenvectors of first k eigenvalues, and create a d × k dimensional transformation matrix W .</p><p>Step 6: Project the initial feature set into new feature subspace through transformation matrix W .</p><p>In this work, the dimensionality reduction method of PCA+LDA is adopted to project the initial feature set into a new subspace, in which PCA is used to remove the noisy of the initial feature set and LDA is employed to further</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T</formula><p>reduce the dimension and discriminate the features into their corresponding emotional classes. In addition, PCA and LDA are applied for comparison as well, respectively. Fig. <ref type="figure" target="#fig_5">6</ref> shows the projection of the initial training feature set of 6 emotions by PCA+LDA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Speech Emotion Recognition Based on Improved BEL Model</head><p>The description of speech emotion is usually divided into discrete emotion and dimensional emotion. As for discrete emotion, the recognition accuracy is significantly related to emotion classification methods. In this paper, GA-BEL inspired by the process of emotion of the limbic system in the brain is proposed to classify the speech emotion. Compared with deep learning method and some of other conventional emotion classifiers, the proposal is a biologically inspired method with simple structure and has low computational complexity. In addition, the BEL model is closer to the human emotional process and has achieved good performance in learning applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Frameworks of Improved BEL Models</head><p>Original BEL model is based on reinforcement learning rule, which causes the characteristic of model sensitive and poor adaption <ref type="bibr" target="#b17">[18]</ref>. To overcome</p><formula xml:id="formula_4">A C C E P T E D M A N U S C R I P T</formula><p>these shortcomings, Genetic Algorithm (GA) is applied into BEL model to optimize the weights of the amygdala and orbitofrontal cortex. Therefore, the reward signal of the BEL model is removed. The framework of oneoutput of BEL model without the reward signal is shown in Fig. <ref type="figure" target="#fig_6">7</ref>, in which the amygdala receives the input vector S i = [S 1 , S 2 , ..., S n ] from the sensory cortex and A th is from the thalamus which is calculated by In addition, the output of the amygdala E a is related to the output of the sensory cortex and the input signals of the thalamus, and it is controlled by the bias from orbitofrontal cortex, which is calculated by</p><formula xml:id="formula_5">A th = max(S 1 , S 2 , ..., S n ). (<label>3</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">w 1 w 2 w n s 1 s 2 s n . . . Thalamus Sensory Cortex A th f b o . . . Amygdala E o - E v n v n+1 v 2 v 1 . . . E a Orbitofrontal Cortex b a</formula><formula xml:id="formula_8">E a = m i=1 S i v i + A th v m+1 + b a ,<label>(4)</label></formula><p>where v i is the weight of amygdala, b a is the bias of amygdala neuron. And the output of orbitofrontal cortex is determined by the output of the sensory cortex and the bias from amygdala, which is calculated by</p><formula xml:id="formula_9">E o = n i=1 S i w i + b o ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>where w i is the weight of orbitofrontal cortex and b o is the bias of orbitofrontal cortex neuron, which is used to adjust the output of the amygdala. Therefore, the output of the BEL model is calculated by</p><formula xml:id="formula_10">E = E a -E o .<label>(6)</label></formula><p>The BEL model has been used for classification problems because of its low computational complexity. In addition, the feature set is input into the thalamus and the number of neurons is equal to the feature dimension of each sample. For multi-class problems, the amount of outputs of the BEL model is the number of categories of the input of the model. Framework of BEL model used to multi-class problem is shown in Fig. <ref type="figure" target="#fig_7">8</ref>, in which the number of neurons in each subsystem is determined by the input vector, and the number of orbitofrontal cortex and amygdala is equal to the number of class categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_11">M A N U S C R I P T w 1,1 w 2,1 w n,1 S 1 S 2 S n</formula><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thalamus Sensory Cortex</head><formula xml:id="formula_12">A th f b o . . . Amygdala E o1 - Class 1 v n,1 v n+1,1 v 2,1 v 1,1</formula><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E a1</head><p>Orbitofrontal Cortex </p><formula xml:id="formula_13">w 1,m w 2,m w n,m f b o . . . Amygdala E om - v n,m v n+1,m v 2,m<label>v</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GA-BEL model</head><p>In this paper, GA is used to optimize the weights and bias of amygdala and orbitofrontal cortex to replace the reinforcement learning rules. Compared with original BEL model, the target values in GA-BEL model is employed to adjust the fitness value of GA, which is used to estimate the performance of the model. The running process is shown in Fig. <ref type="figure" target="#fig_8">9</ref>. The learning procedure of GA-BEL is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head><p>Step 1: Set the BEL model according to samples and initialize the weights.</p><p>Step 2: Encode the chromosomes and initialize the parameters of GA.</p><p>Step 3: Select the optimal individuals as the population of next generation, and then perform crossover operation and generate new offspring individuals.</p><p>Step 4: Determine whether each individual needs to perform mutation operations according to mutation probability. If needed, select a gene locus for mutation randomly and generate new individuals.</p><p>Step 5: Calculate the fitness value. If the the fitness value meets the condition, then update the weights of BEL model; If not, then return to Step 3.</p><p>Step 6: Calculate the output of BEL model according to the new weights.</p><p>Step 7: Determine whether the output of BEL meet the requirements, if</p><formula xml:id="formula_14">A C C E P T E D M A N U S C R I P T</formula><p>it is satisfied, then stop the running, or return to Step 3. Genetic Algorithm encodes parameters which need to be optimized into chromosomes at first, and then perform selection, crossover, and mutation operations by iterating to exchange the chromosome information in the population, and finally generates the chromosomes that meet the requirements <ref type="bibr" target="#b48">[49]</ref>.</p><p>(1) Chromosome encoding When using GA for parameter optimization, the chromosomes should be encoded, in which the parameters that need to be optimized are encoded into a finite length string. The encoding metods in GA include Binary encoding and real encoding methods. Binary encoding method is simple in operation, but deviation may be generated when translating binary number into a real number. In order to improve the precision of extracted parameters and keep the diversity of the population, the real encoding method is applied and each chromosome is represented as</p><formula xml:id="formula_15">Ch = [w 1 , w 2 , ..., w n , b o , v 1 , v 2 , ..., v n+1 , b a ],<label>(7)</label></formula><p>where b o and b a represent the bias of orbitofrontal cortex and amygdala, respectively. w i is the weight of orbitofrontal cortex and v i is the weight of the amygdala. n is the number of input features, therefore, the number of genes in each chromosome is 2n + 3.</p><p>(2) Fitness function The performance of each individual is accessed by the fitness which is calculated by fitness function that is used to evaluate the ability to reach or approach the optimal solution of chromosomes, and chromosomes with good adaptability are more likely selected to generate offspring individuals. According to the training requirements of BEL model, the smaller training error suggests the better performance of chromosomes. Therefore, this paper selects the average bias between the actual output values of all patterns and the corresponding target values as the fitness function, which is defined as</p><formula xml:id="formula_16">f (Ch k ) = 1 m m k=1 (E k -T k ) 2 ,<label>(8)</label></formula><p>where E k is the response to the k-th input, T k is the target value, and m is the number of pattern-targets. According to Eq. ( <ref type="formula" target="#formula_16">8</ref>), for the trained samples, the minimum fitness value means the minimum total error of the model for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>all training samples. Therefore, the optimal of weights can be selected out through the fitness value.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Selection</head><p>The selection operation aims to select the best individuals from the current group for the next iteration. Traditional selection of GA includes roulette wheel method, the championship method, and the optimum maintaining strategy, etc <ref type="bibr" target="#b48">[49]</ref>. In order to enhance the convergence of GA, the roulette wheel method is employed, which is a selection method based on fitness ratio. The probability for each individual to be selected is calculated by</p><formula xml:id="formula_17">p i = F i N j=1 F j ,<label>(9)</label></formula><p>where F i is the fitness value of individual i, N is the number of individuals in group.</p><p>(4) Crossover Crossover operation is the most significant step in GA, which is used to generate new individuals by exchanging information between the parent individuals. There are several crossover methods in GA, such as signalpoint crossover, two-point crossover, arithmetic crossover, and multi-point crossover <ref type="bibr" target="#b48">[49]</ref>. In this paper, the arithmetic crossover method is adopted to generate new offspring individuals by the linear combination of two father individuals because of the real code strategy, and the rule is <ref type="bibr" target="#b19">[20]</ref> </p><formula xml:id="formula_18">a d+1 i = βa d i + (1 -β)a d j , a d+1 j = βa d j + (1 -β)a d i ,<label>(10)</label></formula><p>where a d+1 i and a d+1 j are offspring individuals after crossover operation, a d i and a d j represent the chromosome i and j of father generation individuals for which occur crossover in the d-th bit, and β is the random number which range from 0 to 1.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Mutation</head><p>The mutation operation aims to keep the diversity of groups and enhance the local search ability <ref type="bibr" target="#b19">[20]</ref>. It needs to select an individual from group and change the one or some gene values of selected individuals, which aims to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>generate preferable individuals. The mutation operation is represented as</p><formula xml:id="formula_19">a ij = a ij + (a ij -a max ) * f (g), r ≥ 0.5 a ij + (a min -a ij ) * f (g), r &lt; 0.5, f (g) = r 2 (1 -g/G max ) 2 , (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>where a ij is the j-th gene of i-th individual, a max is the upper bound of a ij , a min is the lower bound of a ij . And g is the current generation, G max is the maximum iteration, r and r 2 are the random numbers ranging from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Speech emotion recognition</head><p>In this section, the speech emotion is classified through GA-BEL model, and the proposal is tested on Chinese corpus of CASIA <ref type="bibr" target="#b22">[23]</ref>, English corpus of SAVEE <ref type="bibr" target="#b23">[24]</ref>, and FAU Aibo Emotion Corpus <ref type="bibr" target="#b24">[25]</ref>, in which both speakerdependent (SD) and speaker-independent (SI) speech emotion recognition are performed. To verify the suitability and robustness of proposal, the proposal is tested on distribution of INTERSPEECH 2009 standard feature set <ref type="bibr" target="#b25">[26]</ref> as well. To verify the feasibility of GA-BEL model in speech emotion recognition, some of other conventional classification methods such as SVM, ELM, and original BEL model are used for comparison. The experiments are performed in MATLAB R2015b and Python 3.6 on the computer of the 64-bit Windows7 system, and CPU is the dual-core Intel CORE i5, clocked is 3.30 GHz, RAM is 8 GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Speech Emotion Dataset 5.1.1. CASIA Chinese Emotion Corpus</head><p>CASIA Chinese emotion corpus is recorded by the Institute of Automation, Chinese Academy of Sciences <ref type="bibr" target="#b22">[23]</ref>. The speech data of this dataset is recorded by four subjects (i.e., two men and two women) in a clean recording environment (SNR is about 35 dB) including 300 basic emotional short utterances with 16 kHz sampling frequency and 16 bit quantified. It contains Mandarin utterances of six basic emotions (i.e., surprise, happy, sad, angry, fear, and neutral).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Surrey Audio-Visual Expressed Emotion (SAVEE) Database</head><p>Surrey Audio-Visual Expressed Emotion (SAVEE) database consists of recordings from four male actors in seven different emotions, 480 British English utterances in total with seven emotional states (i.e., surprise, happy,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T sad, angry, fear, disgust, and neutral) <ref type="bibr" target="#b23">[24]</ref>. And the sentences are selected from the standard TIMIT corpus and phonetically-balanced for each emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">FAU Aibo Emotion Corpus Database</head><p>The FAU Aibo emotion corpus contains natural elicited emotional speech, which is recorded from 51 German children (30 females/21 males) who interact with the AIBO robot controlled by a human invisible to them. The dataset includes 5 major emotional states, i.e., Angry, Emphatic, Positive, Neutral, and Rest <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Speaker-Dependent Emotion Recognition</head><p>In this section, the proposal is tested using MFCC related features and standard feature set INTERSPEECH 2009, in which emotion recognition based on each individual is performed, respectively. All samples of each individual are used, in which 80% samples of an individual are used for training and the remaining 20% samples are used for testing. Meanwhile, the weights of the GA-BEL model are initialized randomly with a uniform distribution ranging from 0 to 1 while the population groups and the maximal generation are set to be 1000 and 200 by trial and error, respectively. And the crossover probability is set to be 0.8.</p><p>The emotion recognition accuracy of each individual is presented in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref>. As is shown in Table <ref type="table" target="#tab_1">2</ref>, the accuracy of emotion recognition is related to the extracted feature set and dimensionality reduction method. It reveals that the feature set of row LLDs+1∆ coefficients projected by PCA+LDA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T achieves the highest average accuracy of 90.28% and 76.40% on CASIA dataset and SAVEE dataset based on GA-BEL model respectively, while the highest accuracy of 71.05% is achieved by PCA on FAU Aibo dataset. In addition, the dimensionality reduction method of PCA+LDA achieves higher average accuracy than that of PCA and LDA on CASIA dataset and SAVEE dataset. However, the PCA dimensional reduction method obtain higher accuracy on FAU Aibo dataset than that of LDA and PCA+LDA. Table <ref type="table" target="#tab_2">3</ref> shows that the average accuracy of the proposal using standard feature set INTERSPEECH2009 of three datasets, which reveals that the feature set of row LLDs+1∆ coefficients projected by PCA+LDA achieves the highest average accuracy of 88.13% and 71.05% on CASIA dataset and FAU Aibo dataset based on GA-BEL model respectively, while the highest accuracy of 74.98% is achieved by LLDs+1∆ coefficients projected by LDA on SAVEE dataset. In addition, the PCA+LDA has better performance in the distribution of INTERSPEECH 2009 compared with other two dimensional reduction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Speaker-Independent Emotion Recognition</head><p>In this experiment, Speaker-independent (SI) emotion recognition is performed on three datasets using two feature sets. And LOSO (leave-onesubject-out) scheme is set, in which all the data of an individual are used to test while all the data of the remained individuals are used to train, and this set is carried out for all the individuals. In addition, the weights of the GA-BEL model are initialized randomly with a uniform distribution ranging from 0 to 1 while the population groups and the maximal generation are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>set to be 2000 and 300 by trial and error, respectively. And the crossover probability is set to be 0.9. The average accuracy of speaker-independent emotion recognition is shown in Table <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">Table 5</ref>.</p><p>As is shown in Table <ref type="table" target="#tab_3">4</ref> and Table <ref type="table" target="#tab_4">5</ref>, the highest average accuracy of 59.55%, 44.18%, and 38.55% are obtained by using the MFCC related features on FAU Aibo dataset, SAVEE dataset, and CASIA dataset, respectively. In the test on the INTERSPEECH 2009 standard feature set, the highest average accuracy of 64.60%, 38.55%, 39.50% for FAU Aibo dataset, SAVEE dataset, and CASIA dataset, respectively. In addition, it shows that the feature set of INTERSPEECH2009 achieves higher average accuracy in speaker-independent emotion recognition than that of MFCC related features on the FAU Aibo dataset while higher average accuracy is obtained by using MFCC related features on SAVEE dataset and CASIA dataset. However, the speaker-independent emotion recognition accuracy is not very high in total, especially on the SAVEE dataset and CASIA dataset, which requires much superior features or their combination to improve the performance of speaker-independent emotion recognition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison Experiment and Analysis of Performance</head><p>To verify the feasibility of GA-BEL model in speech emotion recognition, its performance on three datasets is compared with original BEL model and other conventional speech emotion recognition methods, such as support vector machine (SVM), extreme learning machine (ELM). In this work, the toolbox named LIBSVM <ref type="bibr" target="#b49">[50]</ref> with RBF kernel is used, and the kernel parameters are set as c=10, γ=0.1. The comparison of different emotion classifiers used for comparison are presented in Table <ref type="table" target="#tab_12">6∼11</ref> .    <ref type="table" target="#tab_6">7</ref> show the results of the original BEL model, SVM, and ELM on the CASIA dataset. It shows that the average accuracy based on the GA-BEL is improved by about 5%∼ 10% for speaker-dependent recognition and 1%∼ 5% for speaker-independent recognition compared with the original BEL model on the CASIA emotion corpus. Although the speaker-dependent average recognition accuracy of GA-BEL is higher than that of the SVM and ELM in total, the lower speaker-independent average recognition accuracy are obtained on average by GA-BEL compared with ELM.    <ref type="table" target="#tab_9">9</ref> show the results of the original BEL model, SVM, and ELM on the SAVEE dataset. It shows that the GA-BEL model shows the higher average accuracy than original BEL model for speaker-dependent emotion recognition. However, it shows that the speaker-independent average accuracy of GA-BEL is lower than that of BEL model on the SAVEE dataset in some instances. In addition, the performance of GA-BEL model is superior on average compared with SVM and ELM as well. However, lower average accuracy is obtained than ELM and SVM in some instance as well. For instance, the average accuracy is lower than SVM and ELM for speakerindependent emotion recognition using the feature set of row LLDs + 1∆ coefficients combined with PCA dimensionality reduction method.    <ref type="table" target="#tab_12">11</ref> show the results of the original BEL model, SVM, and ELM on the FAU Aibo dataset. It shows that the GA-BEL model obtains higher speaker-dependent average accuracy than original BEL model, especially the performance using MFCC related fature set and PCA dimensionality reduction method, which are improved by 10.65% and 9.92%. In addition, compared with the SVM and ELM, the average accuracy of GA-BEL is higher than that of SVM and ELM in most instances, although GA-BEL model obtains lower accuracy than SVM and ELM for the speaker-independent speech emotion recognition on the INTERSEECH 2009 standard feature set.</p><p>According to the comparison above, the GA-BEL obtained higher average accuracy in total owing to the optimization by GA. Compared with the conventional speech emotion classification methods such as SVM and ELM, the GA-BEL achieves higher or approximate performance with them. As a result, the proposed GA-BEL emotion classification is feasible in speech emotion recognition.</p><p>Furthermore, the performance of the proposal is compared with some state-of-art works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b51">52]</ref> as well, which is shown in Table <ref type="table" target="#tab_14">12</ref>. Only a few studies work on speaker-dependent and speaker-independent speech emotion recognition using FAU Aibo dataset, therefore, in this paper, we use CASIA and SAVEE for comparison experiments on speech emotion recognition among our proposal and these previous works. It shows that the proposal achieves higher emotion recognition accuracy in speaker-dependent speech emotion recognition. However, the performance of our proposal is not prominent in terms of speaker-independent speech emotion recognition that is significantly related to the extracted features, which should be paid more attention in the future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>A speech emotion recognition method based on an improved BEL model inspired by the emotional learning rule in the limbic system was proposed. To overcome the absence of adaptability caused by the reinforcement learning rule, Genetic Algorithm was employed to update the weights and biases of the amygdala and orbitofrontal cortex of BEL model. In our work, 21 statistical functionals of MFCC 0-14, LogMelFreqBand 0-7, and their 1st order delta coefficient were extracted, and the proposal was tested on IN-TERSPEECH2009 standard feature set as well. In addition, two kinds of feature set of row LLDs+1∆ coefficients and LLDs were set for comparison. Meanwhile, PCA+LDA, LDA, PCA were used for feature dimensionality reduction and their performance were compared with each other. The speaker-dependent and speaker-independent were performed on the CASIA Chinese emotion corpus, SAVEE emotion corpus, and FAU Aibo dataset based on GA-BEL model. The highest average recognition accuracy of 90.28% , 76.40%, and 71.05% were obtained on CASIA Chinese emotion corpus, SAVEE dataset, and FAU Aibo dataset for speaker-dependent emotion recognition, respectively. As for speaker-independent emotion recognition, the highest average accuracy of 38.55%, 44.18%, 64.60% were obtained on CASIA Chinese emotion corpus, SAVEE dataset, and FAU Aibo dataset, respectively. To verify the feasibility of GA-BEL in speech emotion recognition, its performance was compared with conventional speech emotion recognition methods, and original BEL model. The results show that GA-BEL showed higher average accuracy than BEL in total, and the performance is higher or approximate compared with conventional speech emotion recognition methods as well, which suggests the feasibility of GA-BEL model in </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Human auditory pathway for speech perception discussed by Watts<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Framework of BEL model inspired by the emotion circuits in limbic system<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>jA</head><label></label><figDesc>j is the output of amygdala module; j O j is the output of orbitofrintal cortex module. And REW A C C E P T E D M A N U S C R I P T represents the reward signal, which is used to update the weights of amygdala and orbitofrontal cortex when training the model. The training rules are presented by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 . 4 .Figure 4 :</head><label>344</label><figDesc>Figure 4: Framework of speech emotion recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Extraction process of MFCC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Projection of initial training feature set of 6 emotions by PCA+LDA.</figDesc><graphic coords="11,116.50,221.59,331.84,219.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Framework of one-output of improved BEL model [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Framework of BEL model used to multi-class problem [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Running process of GA-BEL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average recognition accuracy of speaker-dependent emotion recognition based on GA-BEL using MFCC related features.</figDesc><table><row><cell>Initial Feature Set</cell><cell cols="3">Dimensionality FAU Aibo SAVEE CASIA Reduction</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>70.13%</cell><cell>76.40% 90.28%</cell></row><row><cell>Row LLDs+1∆</cell><cell>LDA</cell><cell>60.05%</cell><cell>70.83% 80.00%</cell></row><row><cell></cell><cell>PCA</cell><cell>71.05%</cell><cell>66.65% 80.63%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>68.30%</cell><cell>71.53% 87.65%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell>63.00%</cell><cell>69.48% 86.53%</cell></row><row><cell></cell><cell>PCA</cell><cell>71.05%</cell><cell>71.53% 82.93%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average recognition accuracy of speaker-dependent emotion recognition based on GA-BEL using INTERSPEECH 2009.</figDesc><table><row><cell>Initial Feature Set</cell><cell cols="3">Dimensionality FAU Aibo SAVEE CASIA Reduction</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>71.05%</cell><cell>69.78% 88.13%</cell></row><row><cell>Row LLDs+1∆</cell><cell>LDA</cell><cell>63.10%</cell><cell>74.98% 87.65%</cell></row><row><cell></cell><cell>PCA</cell><cell>70.13%</cell><cell>62.50% 79.70%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>68.88%</cell><cell>68.73% 86.83%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell>54.60%</cell><cell>58.33% 86.10%</cell></row><row><cell></cell><cell>PCA</cell><cell>68.88%</cell><cell>61.45% 77.93%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Average recognition accuracy of speaker-independent emotion recognition based on GA-BEL using MFCC related feature set.</figDesc><table><row><cell>Initial Feature Set</cell><cell cols="3">Dimensionality FAU Aibo SAVEE CASIA Reduction</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>59.55%</cell><cell>41.68% 38.55%</cell></row><row><cell>Row LLDs+1∆</cell><cell>LDA</cell><cell>48.05%</cell><cell>33.75% 30.45%</cell></row><row><cell></cell><cell>PCA</cell><cell>58.43%</cell><cell>39.80% 38.33%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>58.10%</cell><cell>42.08% 34.25%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell>46.88%</cell><cell>27.70% 32.68%</cell></row><row><cell></cell><cell>PCA</cell><cell>58.20%</cell><cell>44.18% 33.35%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Average recognition accuracy of speaker-independent emotion recognition based on GA-BEL using INTERSPEECH 2009.</figDesc><table><row><cell>Initial Feature Set</cell><cell cols="3">Dimensionality FAU Aibo SAVEE CASIA Reduction</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>60.50%</cell><cell>38.55% 37.48%</cell></row><row><cell>Row LLDs+1∆</cell><cell>LDA</cell><cell>56.80%</cell><cell>26.45% 34.85%</cell></row><row><cell></cell><cell>PCA</cell><cell>64.10%</cell><cell>39.60% 35.23%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell>57.25%</cell><cell>35.80% 39.50%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell>54.83%</cell><cell>34.15% 33.63%</cell></row><row><cell></cell><cell>PCA</cell><cell>64.60%</cell><cell>36.68% 33.55%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison results using MFCC related features on CASIA emotion corpus.</figDesc><table><row><cell cols="2">Feature Dimensionality</cell><cell></cell><cell>SD</cell><cell></cell><cell>SI</cell></row><row><cell>Set</cell><cell>Reduction</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell></row><row><cell>Row LLDs</cell><cell>PCA+LDA LDA</cell><cell cols="4">90.28% 84.14% 88.62% 89.87% 38.55% 37.53% 35.04% 38.06% 80.00% 72.25% 81.44% 77.22% 30.45% 29.33% 28.33% 32.39%</cell></row><row><cell>+1∆</cell><cell>PCA</cell><cell cols="4">80.63% 74.73% 73.47% 75.91% 38.33% 37.13% 35.66% 38.34%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell cols="4">87.65% 80.15% 86.05% 89.23% 34.25% 32.45% 33.72% 35.14%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell cols="4">86.53% 82.10% 83.25% 87.22% 32.68% 31.65% 34.06% 34.21%</cell></row><row><cell></cell><cell>PCA</cell><cell cols="4">82.93% 74.30% 84.26% 77.43% 33.35% 32.10% 32.17% 38.17%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison results using INTERSPEECH 2009 on CASIA emotion corpus.</figDesc><table><row><cell cols="2">Feature Dimensionality</cell><cell></cell><cell>SD</cell><cell></cell><cell>SI</cell></row><row><cell>Set</cell><cell>Reduction</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell></row><row><cell>Row LLDs</cell><cell>PCA+LDA LDA</cell><cell cols="4">88.13% 83.05% 85.42% 83.06% 39.50% 38.58% 38.22% 36.39% 87.65% 80.58% 84.65% 89.03% 34.85% 32.88% 30.95% 33.28%</cell></row><row><cell>+1∆</cell><cell>PCA</cell><cell cols="4">79.70% 66.25% 81.25% 71.74% 35.23% 27.33% 35.95% 38.17%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell cols="4">86.83% 80.13% 81.67% 86.11% 35.85% 34.58% 35.22% 33.78%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell cols="4">86.10% 80.55% 82.08% 87.01% 33.63% 27.78% 30.28% 34.67%</cell></row><row><cell></cell><cell>PCA</cell><cell cols="4">77.93% 67.38% 77.99% 68.33% 33.55% 26.35% 34.89% 35.83%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 and</head><label>6</label><figDesc>Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison results using MFCC related features on SAVEE emotion corpus..</figDesc><table><row><cell cols="2">Feature Dimensionality</cell><cell></cell><cell>SD</cell><cell></cell><cell>SI</cell></row><row><cell>Set</cell><cell>Reduction</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell></row><row><cell>Row LLDs</cell><cell>PCA+LDA LDA</cell><cell cols="4">76.40% 69.45% 72.23% 73.61% 41.68% 39.58% 35.42% 41.45% 70.83% 63.88% 65.98% 65.28% 33.75% 36.25% 30.63% 32.50%</cell></row><row><cell>+1∆</cell><cell>PCA</cell><cell cols="4">66.65% 62.50% 70.00% 61.81% 39.80% 43.75% 42.71% 42.59%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell cols="4">71.53% 68.08% 65.28% 63.89% 42.08% 43.30% 35.00% 38.25%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell cols="4">69.48% 62.50% 68.06% 69.48% 27.70% 24.58% 25.96% 27.83%</cell></row><row><cell></cell><cell>PCA</cell><cell cols="4">71.53% 62.53% 68.07% 65.93% 44.18% 39.18% 39.38% 42.71%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison results using INTERSPEECH 2009 on SAVEE emotion corpus.</figDesc><table><row><cell cols="2">Feature Dimensionality</cell><cell></cell><cell>SD</cell><cell></cell><cell>SI</cell></row><row><cell>Set</cell><cell>Reduction</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell></row><row><cell>Row LLDs</cell><cell>PCA+LDA LDA</cell><cell cols="4">69.78% 58.35% 73.96% 67.70% 38.55% 34.58% 33.96% 39.58% 74.98% 68.75% 65.63% 69.78% 26.45% 26.66% 24.17% 26.25%</cell></row><row><cell>+1∆</cell><cell>PCA</cell><cell cols="4">62.50% 43.73% 59.38% 58.34% 39.60% 38.33% 41.46% 41.25%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell cols="4">68.73% 58.35% 64.58% 68.75% 35.80% 38.33% 35.00% 34.51%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell cols="4">58.33% 56.28% 63.54% 56.25% 34.15% 33.75% 27.71% 33.96%</cell></row><row><cell></cell><cell>PCA</cell><cell cols="4">61.45% 54.18% 57.29% 56.25% 36.68% 36.68% 35.21% 35.42%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 and</head><label>8</label><figDesc>Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison results using MFCC related features on FAU Aibo emotion corpus..</figDesc><table><row><cell cols="2">Feature Dimensionality</cell><cell></cell><cell>SD</cell><cell>SI</cell><cell></cell></row><row><cell>Set</cell><cell>Reduction</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell></row><row><cell>Row LLDs</cell><cell>PCA+LDA LDA</cell><cell cols="4">70.13% 64.05% 70.12% 64.63% 59.55% 50.35% 53.30% 58.44% 60.05% 54.28% 58.23% 61.59% 48.05% 45.23% 52.26% 40.83%</cell></row><row><cell>+1∆</cell><cell>PCA</cell><cell cols="4">71.05% 60.40% 68.29% 65.85% 58.43% 57.20% 53.79% 55.24%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell cols="4">68.30% 64.03% 68.60% 66.46% 58.10% 55.23% 58.62% 56.10%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell cols="4">63.00% 53.70% 61.16% 62.29% 46.88% 42.45% 41.87% 43.73%</cell></row><row><cell></cell><cell>PCA</cell><cell cols="4">71.05% 61.13% 68.29% 65.24% 58.20% 50.88% 53.67% 56.91%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparison results using INTERSPEECH 2009 on FAU Aibo emotion corpus.</figDesc><table><row><cell cols="2">Feature Dimensionality</cell><cell></cell><cell>SD</cell><cell></cell><cell>SI</cell></row><row><cell>Set</cell><cell>Reduction</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell><cell>GA-BEL</cell><cell>BEL SVM ELM</cell></row><row><cell>Row LLDs</cell><cell>PCA+LDA LDA</cell><cell cols="4">71.05% 60.38% 67.38% 63.11% 60.50% 55.33% 57.50% 53.12% 63.10% 53.70% 59.63% 62.69% 56.80% 44.30% 53.67% 52.01%</cell></row><row><cell>+1∆</cell><cell>PCA</cell><cell cols="4">70.13% 65.25% 66.50% 67.68% 64.10% 58.18% 65.03% 55.32%</cell></row><row><cell></cell><cell>PCA+LDA</cell><cell cols="4">68.28% 60.98% 68.90% 65.55% 57.25% 54.65% 59.11% 57.70%</cell></row><row><cell>Row LLDs</cell><cell>LDA</cell><cell cols="4">54.60% 51.20% 55.48% 57.31% 54.83% 56.05% 56.91% 56.23%</cell></row><row><cell></cell><cell>PCA</cell><cell cols="4">68.88% 60.38% 64.96% 66.76% 64.60% 59.75% 65.90% 59.79%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 and</head><label>10</label><figDesc>Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Comparison among proposal and some previous works.</figDesc><table><row><cell cols="2">Dataset Reference</cell><cell cols="2">SD average accuracy SI average accuracy</cell></row><row><cell></cell><cell>[41]</cell><cell>89.60%</cell><cell>-</cell></row><row><cell>CASIA</cell><cell>[6]</cell><cell>85.08%</cell><cell>43.50%</cell></row><row><cell></cell><cell>Proposal</cell><cell>90.28%</cell><cell>38.55%</cell></row><row><cell></cell><cell>[53]</cell><cell>48.41%</cell><cell>-</cell></row><row><cell>SAVEE</cell><cell>[6] [52]</cell><cell>75.60% 76.19%</cell><cell>50.00% -</cell></row><row><cell></cell><cell>Proposal</cell><cell>76.40%</cell><cell>44.18%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Mei Ying received her Ph.D. degree in Central South University, China in 2018. Her primary research interests include the development of computational models of emotion, both the cognitive processes involved in appraisal, and the effects of emotions on cognition. Her current research focuses on biological perceptual model and its application in pattern recognition and human-computer interaction.Jun-Wei Mao received the B.E. degree from China Three Gorges University, Yichang, China, in 2016. He is currently a master student in the School of Automation, China University of Geosciences. His main research interests include speech emotion recognition and human-robot interaction system.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>$ This work was supported by the National Natural Science Foundation of China under Grants 61403422 and 61273102, the Hubei Provincial Natural Science Foundation of China under Grant 2018CFB447 and 2015CFA010, the 111 project under Grant B17040, the Wuhan Science and Technology Project under Grant 2017010201010133, and the Fundamental Research Funds for National University, China University of Geosciences (Wuhan).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T speech emotion recognition.</p><p>Brain-inspired method used for speech emotion recognition could be a significant part of Human-Robot Interaction, and the method inspired by brain mechanism will be further developed into the artificial intelligence area. In the future, modelings of other brain emotional mechanisms will be paid attention to and integrated into BEL model. And more attention would be paid on the feature extraction method used for speaker-independent speech emotion recognition. In addition, the proposed brain-inspired models would be applied into the multi-modal emotion communication based human-robots interaction (MEC-HRI) system <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">54]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A multimodal emotional communication based humans-robots interaction system</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 35th Chienese Control Conference</title>
		<meeting>35th Chienese Control Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="6363" to="6368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-corpus speech emotion recognition based on transfer non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An efficient unconstrained facial expression recognition algorithm based on Stack Binarized Auto-encoders and Binarized Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion recognition based on ECG signals for service robots in the intelligent space during daily life</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rattanyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mizukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Adv. Comput Intell Intell Inform (JACIII)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="582" to="591" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech emotion recognition in emotional feedback for Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F G</forename><surname>Razuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sundgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rahmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv Res. Art Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weighted spectral features based on local Humoments for speech emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using fourier parameters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="75" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotion recognition through speech using neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv Res Comput Scie Soft Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="422" to="428" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybrid deep neural network-hidden markov model (DNN-HMM) based speech emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Affective Computing and Intelligent Interaction</title>
		<meeting>IEEE Affective Computing and Intelligent Interaction</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature enhancement by deep LSTM networks for ASR in reverberant multisource environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Speech Lang</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="902" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating deep learning architectures for Speech Emotion Recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Fayek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="60" to="68" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the correlation and transferability of features between automatic speech recognition and speech emotion recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Fayek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavedon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="3618" to="3622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotion recognition from chinese speech for smart affective services using a combination of SVM and DBN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3390/s17071694</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A computational model of emotional learning in the amygdala</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balkenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on the Simulation of Adaptive Behaviour</title>
		<meeting>the 6th International Conference on the Simulation of Adaptive Behaviour</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ledoux</surname></persName>
		</author>
		<title level="m">Emotion and the limbic system concept, Concept. Neurosci</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A winner-take-all approach to emotional neural networks with universal approximation property</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Akbarzadeh-T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Sciences</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">347</biblScope>
			<biblScope unit="page" from="369" to="388" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Brain emotional learning-based pattern recognizer</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akbarzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybernet. Syst</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="402" to="421" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An improved brain-inspired emotional learning algorithm for fast classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3390/a10020070</idno>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wind power forecasting using emotional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>the IEEE International Conference on Systems, Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="311" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Short-term traffic flow forecasting model of optimized BP neural network based on genetic algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Control Conference</title>
		<meeting>the IEEE Control Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8125" to="8129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing weights by genetic algorithm for neural network ensemble</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">3713</biblScope>
			<biblScope unit="page" from="323" to="331" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The emotional brain</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schuster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="582" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design of speech corpus for mandarin text to speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th workshop on Blizzard Challenge 2008 workshop</title>
		<meeting>the 4th workshop on Blizzard Challenge 2008 workshop</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speaker-dependent audio-visual emotion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sanaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J B</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVSP</title>
		<imprint>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic cLassification of Emotion Related User States in Spontaneous Childrens Speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stefan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Erlangen, Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Erlangen-Nuremberg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">GTM-URL contribution to the interspeech 2009 emotion challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Planet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Socoró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>IN-TERSPEECH</publisher>
			<biblScope unit="page" from="316C" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The cortical organization of speech processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hickok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="393" to="402" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reverse-engineering the human auditory pathway</title>
		<author>
			<persName><forename type="first">L</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Congress Conference on Advances in Computational Intelligence</title>
		<meeting>World Congress Conference on Advances in Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech emotion recognition based on a modified brain emotional learning model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Motamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Setayeshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol Inspir Cogn Arc</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Theory of emotion, and its application to understanding the neural basis of emotion-Cognition and Emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Edmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Taylor &amp; Francis</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="190" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emotion and learning -A computational model of the amygdala</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lund University Cognitive Studies</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="611" to="636" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Brain emotional learning based intelligent controller applied to neurofuzzy model of micro-heat exchanger</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="911" to="918" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive brain emotional decayed learning for online prediction of geomagnetic activity indices</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Akbarzadeht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="188" to="196" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised brain emotional learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Akbarzadeht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Joint Conference on Neural Networks</title>
		<meeting>IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neo-Fuzzy integrated adaptive decayed brain emotional learning network for online time series prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S A</forename><surname>Milad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Hawary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACCESS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1037" to="1049" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neo-Fuzzy supported brain emotional learning based pattern recognizer for classification problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACCESS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="6951" to="6968" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Practical emotional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Akbarzadeh-T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Brain-inspired emotional learning for image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Majlesi Journal of Multimedia Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="26" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Neural basis computational model of emotional brain for online visual object recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Setayeshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Taimory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Artif Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="814" to="834" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Review on speech emotion recognition</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Softw</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speech emotion recognition based on feature selection and extreme learning machine decision tree</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page" from="271" to="280" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A new approach of audio emotion recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5858" to="5869" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SVM scheme for speech emotion recognition using MFCC feature</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sharmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Tamil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput Appl</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="34" to="39" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Recognizing realistic emotions and affect in speech:state of the art and lessons leant from the first challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1062" to="1087" />
		</imprint>
	</monogr>
	<note>Speech Commu</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Class-level spectral features for emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bitouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commu</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="613" to="625" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An optimum algorithm in pathological voice quality assessment using wavelet-packet-based features, linear discriminant analysis and support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Arjmandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pooyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed Signal Proces Contr</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stroke</surname></persName>
		</author>
		<title level="m">Pattern Recognition, 2nd edition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Genetic algorithms: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Patnaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Errata: Use of ranks in one-criterion variance analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am Statist Assoc</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="583" to="621" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hybrid BBOPSO and higher order spectral features for emotion andstress recognition from natural speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ngadiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="217" to="232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speech-based emotion recognition: feature selection by self-adaptive multi-criteria genetic algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Language Resources and Evaluation(LREC)</title>
		<meeting>International Conference on Language Resources and Evaluation(LREC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3481" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A facial expression emotion recognition based humans-robots interaction system</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Auto Sin</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="668" to="676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">From 1986 to 2014, he was a Faculty Member of the School of Information Science and Engineering at Central South University as a Full Professor. From 1989 to 1990, he was a Visiting Scholar with the</title>
		<author>
			<orgName type="collaboration">Min Wu received the B.S. and M.S</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His current research interests include robust control and its applications, process control, and intelligent control</title>
		<meeting><address><addrLine>Changsha, China; Tokyo, Japan; Sendai, Japan; Nottingham, U.K. In; Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983">1983. 1986. 1999. 1996 to 1999. 2001 to 2002. 2014</date>
		</imprint>
		<respStmt>
			<orgName>degrees in Engineering from Central South University ; Department of Electrical Engineering, Tohoku University ; Department of Control and Systems Engineering, Tokyo Institute of Technology, Tokyo ; Materials, Manufacturing Engineering and Management, University of Nottingham</orgName>
		</respStmt>
	</monogr>
	<note>respectively, and the Ph.D. degree in Engineering from the Tokyo Institute of Technology. Dr. Wu is a Member of the Chinese Association of Automation. He received the IFAC Control Engineering Practice Prize Paper Award in 1999 (together with M. Nakano and J. She</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">He is a Professor in School of Automation, China University of Geosciences. He was a Visiting Scholar with the</title>
	</analytic>
	<monogr>
		<title level="m">His research interest covers intelligent control and process control</title>
		<meeting><address><addrLine>Changsha, China; Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Wei-Hua Cao received his B.S., M.S., and Ph</publisher>
			<date type="published" when="1983">1983. 1986. 2007. 2007 to 2008</date>
		</imprint>
		<respStmt>
			<orgName>D. degrees in Engineering from Central South University ; Department of Electrical Engineering, Alberta University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. He is also a member of the Institute of Electrical and Electronics Engineers (IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
