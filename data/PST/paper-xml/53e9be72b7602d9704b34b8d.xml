<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scheduling Heterogeneous Multi-Cores through Performance Impact Estimation (PIE)</title>
				<funder>
					<orgName type="full">VSSAD group</orgName>
				</funder>
				<funder ref="#_NBJCeQA">
					<orgName type="full">European Research Council</orgName>
				</funder>
				<funder ref="#_E9yeRkj #_ZbnshV4">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Agency for Innovation by Science and Technology (IWT)</orgName>
				</funder>
				<funder ref="#_7V7gzdM">
					<orgName type="full">FWO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kenzo</forename><surname>Van Craeynest</surname></persName>
							<email>kenzo.vancraeynest@ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
							<email>aamer.jaleel@ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
							<email>lieven.eeckhout@ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Paolo</forename><surname>Narvaez</surname></persName>
							<email>paolo.narvaez@ugent.be</email>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
							<email>joel.emer@intel.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">VSSAD ? MIT</orgName>
								<orgName type="institution">Intel Corporation</orgName>
								<address>
									<settlement>Ghent, Hudson, Cambridge</settlement>
									<region>MA, MA</region>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scheduling Heterogeneous Multi-Cores through Performance Impact Estimation (PIE)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single-ISA heterogeneous multi-core processors are typically composed of small (e.g., in-order) power-efficient cores and big (e.g., out-of-order) high-performance cores. The effectiveness of heterogeneous multi-cores depends on how well a scheduler can map workloads onto the most appropriate core type. In general, small cores can achieve good performance if the workload inherently has high levels of ILP. On the other hand, big cores provide good performance if the workload exhibits high levels of MLP or requires the ILP to be extracted dynamically.</p><p>This paper proposes Performance Impact Estimation (PIE) as a mechanism to predict which workload-to-core mapping is likely to provide the best performance. PIE collects CPI stack, MLP and ILP profile information, and estimates performance if the workload were to run on a different core type. Dynamic PIE adjusts the scheduling at runtime and thereby exploits fine-grained time-varying execution behavior. We show that PIE requires limited hardware support and can improve system performance by an average of 5.5% over recent state-of-the-art scheduling proposals and by 8.7% over a sampling-based scheduling policy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Heterogeneous multi-cores can enable higher performance and reduced energy consumption (within a given power budget) by executing workloads on the most appropriate core type. Recent work illustrates the potential of heterogeneous multi-cores to dramatically improve energyefficiency and power-efficiency <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>. Commercial offerings include CPU and GPU integration, e.g., Intel's Sandy Bridge <ref type="bibr" target="#b11">[12]</ref>, AMD's Fusion <ref type="bibr" target="#b0">[1]</ref>, and NVidia's Tegra <ref type="bibr" target="#b24">[25]</ref>; or CPU plus accelerators, e.g., IBM's * A large part of this work was performed while Kenzo Van Craeynest was an intern at Intel/VSSAD.</p><p>Cell <ref type="bibr" target="#b15">[16]</ref>. Other commercial products integrate different CPU core types on a single chip, e.g., NVidia's Kal-El <ref type="bibr" target="#b25">[26]</ref> which integrates four performance-tuned cores along with one energy-tuned core, and ARM's big.LITTLE chip <ref type="bibr" target="#b9">[10]</ref>, which integrates a high-performance big core with a lowenergy small core on a single chip. The latter two examples are so-called single-ISA heterogeneous multi-cores, which means that the different core types implement the same instruction-set architecture (ISA); single-ISA heterogeneous multi-cores are the main focus of this paper.</p><p>A fundamental problem in the design space of single-ISA heterogeneous multi-core processors is how best to schedule workloads on the most appropriate core type. Making wrong scheduling decisions can lead to suboptimal performance and excess energy/power consumption. To address this scheduling problem, recent proposals use workload memory intensity as an indicator to guide application scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. Such proposals tend to schedule memory-intensive workloads on a small core and compute-intensive workloads on a big core. We show that such an approach causes suboptimal scheduling when memory intensity alone is not a good indicator for workload-tocore mapping.</p><p>In general, small (e.g., in-order) cores provide good performance for compute-intensive workloads whose subsequent instructions in the dynamic instruction stream are mostly independent (i.e., high levels of inherent ILP). On the other hand, big (e.g., out-of-order) cores provide good performance for workloads where the ILP must be extracted dynamically or the workload exhibits a large amount of MLP. Therefore, scheduling decisions on heterogeneous multi-cores can be significantly improved by taking into account how well a small or big core can exploit the ILP and MLP characteristics of a workload. This paper proposes Performance Impact Estimation (PIE) as a mechanism to select the appropriate workloadto-core mapping in a heterogeneous multi-core processor. 978-1-4673-0476-4/12/$31.00(c)2012 IEEE   The key idea of PIE is to estimate the expected performance for each core type for a given workload. In particular, PIE collects CPI stack, MLP and ILP profile information during runtime on any one core type, and estimates performance if the workload were to run on another core type. In essence, PIE estimates how a core type affects exploitable MLP and ILP, and uses the CPI stacks to estimate the impact on overall performance. Dynamic PIE scheduling collects profile information on a per-interval basis (e.g., 2.5 ms) and dynamically adjusts the workload-to-core mapping, thereby exploiting time-varying execution behavior. We show that dynamically collecting profile information requires minimal hardware support: five 10-bit counters and 64 bits of storage.</p><p>We evaluate PIE scheduling using a large number of multi-programmed SPEC CPU2006 workload mixes. For a set of scheduling-sensitive workload mixes on a heterogeneous multi-core consisting of one big (out-of-order) and one small (in-order) core, we report an average performance improvement of 5.5% over recent state-of-the-art scheduling proposals. We also evaluate PIE scheduling and demonstrate its scalability across a range of heterogeneous multicore configurations, including private and shared last-level caches (LLCs). Finally, we show that PIE outperforms a sampling-based scheduling by an average of 8.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Efficient use of single-ISA heterogeneous multi-cores is dependent on the underlying workload scheduling policy. A number of recent proposals use memory intensity as an indicator to guide workload scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. This policy is based on the intuition that compute-intensive workloads benefit more from the high computational capabilities of a big core while memory-intensive workloads execute more energy-efficiently on a small core while waiting for memory.</p><p>To correlate whether memory intensity is a good indicator to guide workload scheduling, Figure <ref type="figure" target="#fig_1">1</ref> compares the slowdown for SPEC CPU2006 workloads on a small core relative to a big core (left y-axis), to the normalized CPI stack <ref type="bibr" target="#b4">[5]</ref> on a big core (right y-axis). The normalized CPI stack indicates whether a workload is memory-intensive or compute-intensive. If the normalized CPI stack is memory dominant, then the workload is memory-intensive (e.g., mcf), else the workload is compute-intensive (e.g., tonto).</p><p>The figure illustrates workloads grouped into three categories on the x-axis: workloads that have reasonable slowdown (&lt;1.75?) on the small core (type-I workloads), workloads that have significant slowdown (&gt;2.25?) on the small core (type-III), and the remaining workloads are labeled as type-II. Making correct scheduling decisions in the presence of type-I and III workloads is most critical: making an incorrect scheduling decision, i.e., executing a type-III workload on a small core instead of a type-I workload, leads to poor overall performance, hence we label type-I and III as scheduling-sensitive workloads.</p><p>The figure shows that while memory intensity alone can provide a good indicator for scheduling some memoryintensive workloads (e.g., mcf) onto a small core, such practice can significantly slowdown other memory-intensive workloads (e.g., soplex).</p><p>Similarly, some computeintensive workloads (e.g., astar.r) observe a significant slowdown on a small core while other compute-intensive workloads (e.g., calculix) have reasonable slowdown when executing on a small core. This behavior illustrates that memory intensity (or compute intensity) alone is not a good indicator to guide application scheduling on heterogeneous multi-cores.</p><p>The performance behavior of workloads on small and  big cores (Figure <ref type="figure" target="#fig_1">1</ref>) can be explained by the design characteristics of each core. Big cores are particularly suitable for workloads that require ILP to be extracted dynamically or have a large amount of MLP. On the other hand, small cores are suitable for workloads that have a large amount of inherent ILP. This implies that performance on different core types can be directly correlated to the amount of MLP and ILP prevalent in the workload. For example, consider a memory-intensive workload that has a large amount of MLP. Executing such a memory-intensive workload on a small core can result in significant slowdown if the small core does not expose the MLP. On the other hand, a compute-intensive workload with large amounts of ILP may have a reasonable slowdown on a small core and need not require the big core.</p><p>To quantify this, Figure <ref type="figure" target="#fig_2">2</ref> illustrates slowdown and the loss in MLP (or ILP) when scheduling a workload on a small core instead of a big core. The workloads are sorted left-to-right based on memory intensity (inferred from the normalized CPI stack). We use MLP ratio to quantify MLP loss and ILP ratio to quantify ILP loss. MLP and ILP ratios are defined as follows:</p><formula xml:id="formula_0">M LP ratio = M LP big /M LP small<label>(1)</label></formula><p>ILP ratio = CP I base big /CP I base small <ref type="bibr" target="#b1">(2)</ref> with M LP defined as the average number of outstanding memory requests if at least one is outstanding <ref type="bibr" target="#b3">[4]</ref>, and CP I base as the base (non-miss) component of the CPI stack. The key observation from Figure <ref type="figure" target="#fig_2">2</ref> is that MLP ratio correlates with slowdown for memory-intensive applications (righthand side of the graph). Similarly, ILP ratio correlates with slowdown for compute-intensive workloads (lefthand side of the graph). In summary, Figures <ref type="figure" target="#fig_1">1</ref> and<ref type="figure" target="#fig_2">2</ref> indicate that memory intensity alone is not a good indicator for scheduling workloads on a heterogeneous multi-core. Instead, scheduling policies on heterogeneous multi-cores must take into account the amount of MLP and ILP that can be exploited by the different core types. Furthermore, the slowdowns (or speedups) when moving between different core types can directly be correlated to the amount of MLP and ILP realized on a target core. This suggests that the performance on a target core type can be estimated by predicting the MLP and ILP on that core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Performance Impact Estimation (PIE)</head><p>A direct approach to determine the best scheduling policy on a heterogeneous multi-core is to apply sampling-based scheduling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. Sampling-based scheduling dynamically samples different workload-to-core mappings at runtime and then selects the best performing mapping. While such an approach can perform well, it introduces performance overhead due to periodically migrating workloads between different core types. Furthermore, these overheads increase with the number of cores (and core types). To address these drawbacks, we propose Performance Impact Estimation (PIE).</p><p>The key idea behind PIE is to estimate (not sample) workload performance on a different core type. PIE accom- plishes this by using CPI stacks. We concentrate on two major components in the CPI stack: the base component and the memory component; the former lumps together all non-memory related components:</p><formula xml:id="formula_1">CP I = CP I base + CP I mem .<label>(3)</label></formula><p>Figure <ref type="figure" target="#fig_2">2</ref> illustrated that MLP and ILP ratios provide good indicators on the performance difference between big and small cores. Therefore, we use MLP, ILP, and CPI stack information to develop our PIE model (see Figure <ref type="figure" target="#fig_3">3</ref>). Specifically, we estimate the performance on a small core while executing on a big core in the following manner:</p><p>CP I small = CP I base small + CP I mem small</p><formula xml:id="formula_2">= CP I base small + CP I mem big ? M LP ratio .<label>(4)</label></formula><p>Similarly, we estimate the performance on a big core while executing on a small core as follows:</p><formula xml:id="formula_3">CP I big = CP I base big + CP I mem big = CP I base big + CP I mem small /M LP ratio .<label>(5)</label></formula><p>In the above formulas, CP I base big refers to the base CPI component on the big core estimated from the execution on the small core; CP I base small is defined similarly. The memory CPI component on the big (small) core is computed by dividing (multiplying) the memory CPI component measured on the small (big) core with the MLP ratio. The remainder of this section details on how we predict the base CPI components as well as the MLP ratio, followed by an evaluation of the PIE model. Section 4 then presents dynamic PIE scheduling, including how we collect the inputs to the PIE model during runtime by introducing performance counters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predicting MLP</head><p>The memory CPI component essentially consists of three contributors: the number of misses, the latency per (isolated) miss, and the number of simultaneously outstanding misses (MLP). In this paper, we assume that the big and small cores have the same cache hierarchy, i.e., the same number of cache levels and the same cache sizes at each level. In other words, we assume that the number of misses and the latency per miss is constant across core types <ref type="foot" target="#foot_0">1</ref> . However, MLP varies across core types as big cores and small cores vary in the amount of MLP that they can exploit. We now describe how we estimate MLP on the big core while running on the small core; and vice versa, we estimate MLP on the small core while running on the big core.</p><p>Combining these MLP estimates with measured MLP numbers on the current core type enables predicting the MLP ratio using Formula 1, which in its turn enables estimating the memory CPI components on the other core type, using Formulas 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Predicting big-core MLP on small core</head><p>Big out-of-order cores implement a reorder buffer, nonblocking caches, MSHRs, etc., which enables issuing independent memory accesses in parallel. The maximum MLP that a big core can exploit is bound by the reorder buffer size, i.e., a necessary condition for independent longlatency load misses to be issued to memory simultaneously is that they reside in the reorder buffer at the same time.</p><p>We therefore estimate the big-core MLP as the average number of memory accesses in the big-core reorder buffer. Quantitatively, we do so by calculating the average number of LLC misses per instruction observed on the small core (M P I small ) multiplied by the big-core reorder buffer size:</p><formula xml:id="formula_4">M LP big = M P I small ? ROB size.<label>(6)</label></formula><p>Note that the above estimate does not make a distinction between independent versus dependent LLC misses; we count all LLC misses. A more accurate estimate would be to count independent LLC misses only, however, in order to simplify the design, we simply count all LLC misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Predicting small-core MLP on big core</head><p>Small in-order cores exploit less MLP than big cores. A stall-on-miss core stalls on a cache miss, and hence, it does not exploit MLP at all -MLP equals one. A stall-on-use core can exploit some level of MLP: independent loads between a long-latency load and its first consumer can be issued to memory simultaneously. MLP for a stall-on-use core thus equals the average number of memory accesses between a long-latency load and its consumer. Hence, we estimate the MLP of a stall-on-use core as the average number of LLC misses per instruction on the big core multiplied by the average dependency distance D between an LLC miss and its consumer. (Dependency distance is defined as the number of dynamically executed instructions between a producer and its consumer.)</p><formula xml:id="formula_5">M LP small = M P I big ? D.<label>(7)</label></formula><p>Again, in order to simplify the design, we approximate D as the dependency distance between any producer (not just an LLC miss) and its consumer. We describe how we measure the dependency distance D in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Predicting ILP</head><p>The second CPI component predicted by the PIE model is the base CPI component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Predicting big-core ILP on small core</head><p>We estimate the base CPI component for the big core as one over the issue width W big of the big core:</p><formula xml:id="formula_6">CP I base big = 1/W big .<label>(8)</label></formula><p>A balanced big (out-of-order) core should be able to dispatch approximately W big instructions per cycle in the absence of miss events. A balanced core design can be achieved by making the reorder buffer and related structures such as issue queues, rename register file, etc., sufficiently large to enable the core to issue instructions at a rate near the designed width <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Predicting small-core ILP on big core</head><p>Estimating the base CPI component for a small (in-order) core while running on a big core is more complicated. For ease of reasoning, we estimate the average IPC and take the reciprocal of the estimated IPC to yield the estimated CPI. We estimate the average base IPC on the small core with width W small as follows:</p><formula xml:id="formula_7">IP C base small = W small i=1 i ? P [IP C = i].<label>(9)</label></formula><p>We use simple probability theory to estimate the probability of executing i instructions in a given cycle. The probability of executing only one instruction in a given cycle equals the probability that an instruction produces a value that is consumed by the next instruction in the dynamic instruction stream (dependency distance of one):</p><formula xml:id="formula_8">P [IP C = 1] = P [D = 1].<label>(10)</label></formula><p>Likewise, the probability of executing two instructions in a a given cycle equals the probability that the second instruction does not depend on the first, and the third depends on either the first or the second:</p><formula xml:id="formula_9">P [IP C = 2] = (1 -P [D = 1]) ? (P [D = 1] + P [D = 2]) .<label>(11)</label></formula><p>This generalizes to three instructions per cycle as well:</p><formula xml:id="formula_10">P [IP C = 3] = (1 -P [D = 1]) ? (1 -P [D = 1] -P [D = 2]) ? (P [D = 1] + P [D = 2] + P [D = 3]) .<label>(12)</label></formula><p>Finally, assuming a 4-wide in-order core, the probability of executing four instructions per cycle equals the probability that none of the instructions depend on a previous instruction in a group of four instructions:</p><formula xml:id="formula_11">P [IP C = 4] = (1 -P [D = 1]) ? (1 -P [D = 1] -P [D = 2]) ? (1 -P [D = 1] -P [D = 2] -P [D = 3]) .<label>(13)</label></formula><p>Note that the above formulas do not take non-unit instruction execution latencies into account. Again, we used this approximation to simplify the design, and we found this approximation to be accurate enough for our purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating the PIE Model</head><p>Figure <ref type="figure">4</ref> evaluates the accuracy of our PIE model. This is done in two ways: we estimate big-core performance while executing the workload on a small core, and vice versa, we estimate small-core performance while executing the workload on a big core. We compare both of these to the actual slowdown. (We will describe the experimental setup in Section 5.) The figure shows that we achieve an average absolute prediction error of 9% and a maximum error of 35% when predicting speedup (predicting big-core performance on the small core). The average absolute prediction error for the slowdown (predicting small-core performance on the big core) equals 13% with a maximum error of 47%. More importantly, PIE accurately predicts the relative performance differences between the big and small cores. This is in line with our goal of using PIE for driving runtime scheduling decisions. As a second step in evaluating our PIE model, we consider a heterogeneous multi-core and use PIE to determine the workload-to-core mapping. We consider all possible two-core multi-programmed workload mixes of SPEC   CPU2006 applications and a two-core system with one big core and one small core and private LLCs. Further, benchmarks are scheduled on a given core and stay there for the remainder of the execution (static scheduling). Figure <ref type="figure">5</ref> reports performance (system throughput or weighted speedup) relative to worst-case scheduling for all workload mixes; we compare PIE scheduling against random and memory-dominance (memdom) scheduling. Memory-dominance scheduling refers to the conventional practice of always scheduling memory-intensive workloads on the small core.</p><p>PIE scheduling chooses the workload-to-core mapping by selecting the schedule that yields the highest (estimated) system throughput across both cores. PIE scheduling outperforms both random and memory-dominance scheduling over the entire range of workload mixes. Figure <ref type="figure" target="#fig_6">6</ref> provides more detailed results for workload mixes with type-I and type-III workloads. PIE outperforms worst-case scheduling by 14.2%, compared to random (8.5%) and memorydominance scheduling (9.2%). Put differently, PIE scheduling achieves 84% of optimal scheduling, compared to 54% for memory-dominance and 50% for random scheduling.</p><p>The PIE model takes into account both ILP and MLP. We also evaluated a version of PIE that only takes MLP into ac- count, i.e., ILP is not accounted for and is assumed to be the same on the big and small cores. We refer to this as MLP-ratio scheduling. Figures <ref type="figure">5</ref> and<ref type="figure" target="#fig_6">6</ref> illustrate the importance of taking both MLP and ILP into account. MLP-ratio scheduling improves worst-case scheduling by 12.7% for type-I and III workloads, compared to 14.2% for PIE. This illustrates that accounting for MLP is more important than ILP in PIE.</p><p>So far, we evaluated PIE for a heterogeneous multi-core with one big and one small core (e.g., ARM's big.LITTLE design <ref type="bibr" target="#b9">[10]</ref>). We now evaluate PIE scheduling for heterogeneous multi-cores with one big core and multiple small cores, as well as several big cores and one small core (e.g., NVidia's Kal-El <ref type="bibr" target="#b25">[26]</ref>); we assume all cores are active all the time. Figure <ref type="figure" target="#fig_8">7</ref> shows that PIE outperforms memorydominance scheduling by a bigger margin even for these heterogeneous multi-core design points than for the onebig, one-small multi-core system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dynamic Scheduling</head><p>So far, PIE scheduling was evaluated in a static setting, i.e., a workload is scheduled on a given core for its entire execu-   tion. There is opportunity to further improve PIE scheduling by dynamically adapting to workload phase behavior.</p><p>To illustrate this, Figure <ref type="figure" target="#fig_9">8</ref> shows big-core and small-core CPI and MLP as a function of time for libquantum from SPEC CPU2006. The key observation here is that, although the average slowdown is high for the small core compared to the big core, the small core achieves comparable performance to the big core for some execution phases. For libquantum, approximately 10% of the instructions can be executed on the small core without significantly affecting overall performance. However, the time-scale granularity is relatively fine-grained (few milliseconds) and much smaller than a typical OS time slice (e.g., 10 ms). This suggests that dynamic hardware scheduling might be beneficial provided that rescheduling (i.e., migration) overhead is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantifying migration overhead</head><p>Dynamic scheduling incurs overhead for migrating workloads between different cores. Not only does migration incur a context switch, it also incurs overhead for warming hardware state, especially the cache hierarchy. A context switch incurs a fixed cost for restoring architecture state. To better understand the overhead due to cache warming, we consider a number of scenarios to gain insight on cache hierarchy designs for low migration overheads at fine-grained dynamic scheduling. Shared LLC. Figure <ref type="figure" target="#fig_10">9</ref> quantifies the performance overhead of migrating a workload every x milliseconds, with x varying from 1 ms to 50 ms. Migration overhead is measured by configuring two identical cores to share a 4MB LLC. Workloads are rescheduled to a different core every x ms. Interestingly, for a 2.5 ms migration frequency, the performance overhead due to migration is small, less than 0.6% for all benchmarks. The (small) performance overhead are due to (private) L1 and L2 cache warmup effects. Private powered-off LLCs. The situation is very different in case of a private LLC that is powered off when migrating a workload. Powering off a private LLC makes sense in case one wants to power down an entire core and its private cache hierarchy in order to conserve power. If the migration frequency is high (e.g., 2.5 ms), Figure <ref type="figure" target="#fig_11">10</ref> reports severe performance overhead for some workloads when the private cache hierarchy is powered off upon migration. The huge performance overheads are because the cache looses its data when powered off, and hence the new core must re-fetch the data from main memory. Private powered-on LLCs. Instead of turning off private LLCs, an alternative is to keep the private LLCs powered on and retain the data in the cache. In doing so, Figure <ref type="figure" target="#fig_12">11</ref> shows that performance overhead from frequent migrations is much smaller and in fact even leads to substantial performance benefits for a significant fraction of the benchmarks. The performance benefit comes from having a larger effective LLC: upon a miss in the new core's private LLC, the data is likely to be found in the old core's private LLC, and hence the data can be obtained more quickly from the old core's LLC through cache coherency than by fetching the data from main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamic PIE Scheduling</head><p>Having described the PIE model, we now describe Dynamic PIE scheduling. PIE scheduling is applicable to any number of cores of any core type. However, to simplify the discussion, we assume one core of each type. We assume as many workloads as there are cores, and that workloads are initially randomly scheduled onto each core. Furthermore, we assume that workload scheduling decisions can be made every x milliseconds.</p><p>To strive towards an optimal schedule, PIE scheduling requires hardware support for collecting CPI stacks on each core, the number of misses, the number of dynamically executed instructions, and finally the inter-instruction dependency distance distribution on the big core. We discuss the necessary hardware support in the next section.</p><p>During every time interval of x milliseconds, for each workload in the system, PIE uses the hardware support to compute CPI stacks, MLP and ILP on the current core type, and also predicts the MLP and ILP for the same workload on the other core type. These predictions are then fed into the PIE model to estimate the performance of each workload on the other core type. For a given performance metric, PIE scheduling uses these estimates to determine whether another scheduling decision would potentially improve overall system performance as compared to the current schedule. If so, workloads are rescheduled to the predicted core type. If not, the workload schedule remains intact and the process is repeated the next time interval.</p><p>Note that PIE scheduling can be done both in hardware and software. If the time interval of scheduling workloads to cores coincides with a time slice, then PIE scheduling can be applied in software, i.e., the hardware would collect the event counts and the software (e.g., OS or hypervisor) would make scheduling decisions. If scheduling decisions would need to be made at smaller time scale granularities, hardware can also make the scheduling decisions, transparent to the software <ref type="bibr" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hardware support</head><p>PIE scheduling requires hardware support for collecting CPI stacks. Collecting CPI stacks on in-order cores is fairly straightforward and is implemented in commercial systems, see for example Intel Atom <ref type="bibr" target="#b10">[11]</ref>. Collecting CPI stacks on out-of-order cores is more complicated because of various overlap effects between miss events, e.g., a longlatency load may hide the latency of another independent long-latency load miss or mispredicted branch, etc. Recent commercial processors such as IBM Power5 <ref type="bibr" target="#b22">[23]</ref> and Intel Sandy Bridge <ref type="bibr" target="#b11">[12]</ref> however provide support for computing memory stall components. PIE scheduling also requires the number of LLC misses and the number of dynamically executed instructions, which can be measured using existing hardware performance counters. In other words, most of the profile information needed by PIE can be readily measured on existing hardware. PIE scheduling requires some profile information that cannot be collected on existing hardware. For example, while running on a big core, PIE requires the ability to measure the inter-instruction dependency distance distribution for estimating small-core MLP and ILP. The PIE model requires the dependency distance distribution for a maximum dependency distance of W small only (where W small is the width of the small core). For a 4-wide core, this involves four plus one counters: four counters for computing the dependency distance distribution up to four instructions, and one counter for computing the average distance.</p><p>The PIE model requires that the average dependency distance D be computed over the dynamic instruction stream. This can be done by requiring a table with as many rows as there are architectural registers. The table keeps track of which instruction last wrote to an architectural register. The delta in dynamic instruction count between a register write and subsequent read then is the dependency distance. Note that the table counters do not need to be wide, because the dependency distance tends to be short <ref type="bibr" target="#b7">[8]</ref>; e.g., four bits per counter can capture 90% of the distances correctly. In summary, the total hardware cost to track the dependency distance distribution is roughly 15 bytes of storage: 4 bits times the number of architectural registers (64 bits for x86-64), plus five 10-bit counters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We use CMP$im <ref type="bibr" target="#b12">[13]</ref> to conduct the simulation experiments in this paper. We configure our simulator to model heterogeneous multi-core processors with big and small cores. The big core is a 4-wide out-of-order processor core; the small core is a 4-wide (stall-on-use) in-order processor core <ref type="foot" target="#foot_1">2</ref> . We assume both cores run at a 2 GHz clock frequency. Further, we assume a cache hierarchy consisting of three levels of cache, separate 32 KB L1 instruction and data caches, a 256 KB L2 cache and a 4 MB last-level L3 cache (LLC). We assume the L1 and L2 caches to be private per core for all the configurations evaluated in this paper. We evaluate both shared and private LLC configurations. We consider the LRU replacement policy in all of the caches unless mentioned otherwise; we also consider a state-ofthe-art RRIP shared cache replacement policy <ref type="bibr" target="#b14">[15]</ref>. Finally, we assume an aggressive stream-based hardware prefetcher; we experimentally evaluated that hardware prefetching improves performance by 47% and 25% on average for the small and big cores, respectively.</p><p>We further assume that the time interval for dynamic scheduling is 2.5 ms; this is small enough to benefit from fine-grained exploitation of time-varying execution behavior while keeping migration overhead small. The overhead for migrating a workload from one core to another (storing and restoring the architecture state) is set to 300 cycles; in addition, we do account for the migration overhead due to cache effects.</p><p>We consider all 26 SPEC CPU2006 programs and all of their reference inputs, leading to 54 benchmarks in total. We select representative simulation points of 500 million instructions each using PinPoints <ref type="bibr" target="#b26">[27]</ref>. When simulating a multi-program workload we stop the simulation when the slowest workload has executed 500 million instructions. Faster running workloads are reiterated from the beginning of the simulation point when they reach the end. We report system throughput (STP) <ref type="bibr" target="#b5">[6]</ref> (also called weighted speedup <ref type="bibr" target="#b30">[31]</ref>) which quantifies system-level performance or aggregate throughput achieved by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>We evaluate dynamic PIE scheduling on private and shared LLCs with LRU, and a shared LLC with RRIP replacement. We compare PIE scheduling to a sampling-based strategy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> that assumes running a workload for one time interval on one core and for the next time interval on the other core. The workload-core schedule that yields the highest performance is then maintained for the next 10 time intervals, after which the sampling phase is reinitiated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Private LLCs</head><p>We first assume that each core has its private LLC. Figure <ref type="figure" target="#fig_13">12</ref> quantifies the relative performance over random scheduling for sampling-based, memory-dominance and PIE scheduling. PIE scheduling clearly outperforms the other scheduling strategies by a significant margin. Across the type-I and III workload mixes, we report an average 5.5% and 8.7% improvement in performance over memory-dominance and sampling-based scheduling, respectively. The improvement over memory-dominance scheduling comes from two sources: PIE is able to more accurately determine the better workload-to-core mapping, and in addition, PIE can exploit fine-grain phase behavior, unlike memorydominance scheduling. PIE also improves upon samplingbased scheduling, because PIE does not incur any overhead from sampling because it (accurately) estimates the performance impact of a workload reschedule, and hence, it can more quickly and better adapt to fine-grain phase behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Shared LLC</head><p>With shared LLCs, Figure <ref type="figure" target="#fig_14">13</ref> shows similar conclusions to private LLCs: PIE outperforms random, sampling-based and memory-dominance scheduling. For the type-I and III workload mixes, we obtain an average 3.7% and 6.4% improvement in performance over memory-dominance and sampling-based scheduling, respectively. The performance improvement is slightly lower for private LLCs though. The reason is that none of the scheduling strategies anticipate conflict behavior in the shared LLC, and, as a result, some of the scheduling decisions may be partly offset by negative conflict behavior in the shared LLC. Further, in the case of sampling-based scheduling, LLC performance changes when switching between core types (as a result of sampling) because the access patterns change, which in turn changes overall performance; in other words, sampling is particularly ineffective in case of a shared LLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">RRIP-managed shared LLC</head><p>So far, we assumed an LRU cache replacement policy. However, it has been shown that LRU is not the most effective shared cache management policy; a state-of-the-art shared cache replacement policy is RRIP <ref type="bibr" target="#b14">[15]</ref> which significantly improves LLC performance by predicting the rereference behavior of cache blocks. The results for PIE scheduling applied to an RRIP-managed LLC are shown in Figure <ref type="figure" target="#fig_15">14</ref>. For the type-I and III workload mixes, PIE scheduling improves performance by 2.4% and 7.8% over memory-dominance and sampling-based scheduling, respectively. An interesting observation to make from Figure <ref type="figure" target="#fig_15">14</ref> is that an intelligent shared cache management policy such as RRIP is able to reduce the performance hit observed for some of the workloads due to scheduling. A large fraction of the workloads observe a significant performance hit under sampling-based scheduling (and a handful workloads under memory-dominance scheduling) for an LRUmanaged shared LLC, see bottom left in Figure <ref type="figure" target="#fig_14">13</ref>; these performance hits are removed through RRIP, see Figure <ref type="figure" target="#fig_15">14</ref>. In other words, a scheduling policy can benefit from an intelligent cache replacement policy: incorrect decisions by the scheduling policy can be alleviated (to some extent) by the cache management policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Heterogeneous multi-cores desings vary from single-ISA cores only varying in clock frequency, to single-ISA cores differing in microarchitecture, to cores with non-identical ISAs. Since we focus on single-ISA heterogeneous multicores, we only discuss this class of heterogeneity.</p><p>Kumar et al. <ref type="bibr" target="#b17">[18]</ref> made the case for heterogeneous single-ISA multi-core processors when running a single application: they demonstrate that scheduling an application across core types based on its time-varying execution behavior can yield substantial energy savings. They evaluate both static and dynamic scheduling policies. In their followon work, Kumar et al. <ref type="bibr" target="#b18">[19]</ref> study scheduling on heterogeneous multi-cores while running multi-program workloads.</p><p>The dynamic scheduling policies explored in these studies use sampling to gauge the most energy-efficient core. Becchi and Crowley <ref type="bibr" target="#b1">[2]</ref> also explore sample-based scheduling. Unfortunately, sample-based scheduling, in contrast to PIE, does not scale well with increasing core count: an infrequent core type (e.g., a big core in a one-big, multiple-small core configuration) quickly becomes a bottleneck. Bias scheduling <ref type="bibr" target="#b16">[17]</ref> is very similar to memorydominance scheduling. It schedules programs that exhibit frequent memory and other resource stalls on the small core, and programs that are dominated by execution cycles (and hence low fraction of stalls) on the big core. Thresholds are used to determine a program's bias towards a big versus small core based on these stall counts.</p><p>HASS <ref type="bibr" target="#b29">[30]</ref> is a static scheduling policy, the key motivation being scalability. Chen and John <ref type="bibr" target="#b2">[3]</ref> leverage offline program profiling. An obvious limitation of static/offline scheduling is that it does not enable exploiting time-varying execution behavior. PIE on the other hand is a dynamic scheduling algorithm that, in addition, is scalable.</p><p>Several studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref> explore scheduling in heterogeneous systems by changing clock frequency across cores; the core microarchitecture does not change though. Such studies do not face the difficulty of having to deal with differences in MLP and ILP across core types. Hence, memory-dominance based scheduling is likely to work well for such architectures.</p><p>Age-based scheduling <ref type="bibr" target="#b19">[20]</ref> predicts the remaining execution time of a thread in a multi-threaded program and schedules the oldest thread on the big core. Li et al. <ref type="bibr" target="#b20">[21]</ref> evaluate the idea of scheduling programs on the big core first, before scheduling programs on the small cores, in order to make sure the big power-hungry core is fully utilized. Chou et al. <ref type="bibr" target="#b3">[4]</ref> explored how microarchitecture techniques affect MLP. They found that out-of-order processors can better exploit MLP compared to in-order processors. We show that MLP and ILP are important criteria to take into account when scheduling on heterogeneous multicores, and we propose the PIE method for doing so.</p><p>Patsilaras et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> study how to best integrate an MLP technique (such as runahead execution <ref type="bibr" target="#b23">[24]</ref>) into an asymmetric multi-core processor, i.e., should one integrate the MLP technique into the small or big core, or both? They found that if the small core runs at a higher frequency and implements an MLP technique, the small core might become more beneficial for exploiting MLP-intensive workloads. Further, they propose a hardware mechanism to dynamically schedule threads to core types based on the amount of MLP in the dynamic instruction stream, which they estimate by counting the number of LLC misses in the last 10K instructions interval. No currently shipping commercial processor employs runahead execution; also, running the small core at a high frequency might not be possible given current power concerns. We therefore take a different approach: we consider a heterogeneous multi-core system as a given -we do not propose changing the architecture nor the frequency of either core type -and we schedule tasks onto the most appropriate core type to improve overall performance while taking both MLP and ILP into account as a criterion for scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>Single-ISA heterogeneous multi-cores are typically composed of small (e.g., in-order) cores and big (e.g., out-oforder) cores. Using different core types on a single die has the potential to improve energy-effiency without sacrificing significant performance. However, the success of heterogeneous multi-cores is directly dependent on how well a scheduling policy maps workloads to the best core type (big or small). Incorrect scheduling decisions can unnecessarily degrade performance and waste energy/power. With this in mind, this paper makes the following contributions:</p><p>? We show that using memory intensity alone as an indictator to guide workload scheduling decisions can lead to suboptimal performance. Instead, scheduling policies must take into account how a core type can exploit the ILP and MLP characteristics of a workload.</p><p>? We propose the Performance Impact Estimation (PIE) model to guide workload scheduling. The PIE model uses CPI stack, ILP and MLP information of a workload on a given core type to estimate the performance on a different core type. We propose PIE models for both small (in-order) and big (out-of-order) cores.</p><p>? Using the PIE model, we propose dynamic PIE scheduling. Dynamic PIE collects CPI stack, ILP and MLP information at run time to guide workload scheduling decisions.</p><p>? We show that the use of shared LLCs can enable high frequency, low-overhead, fine-grained scheduling to exploit time-varying execution behavior. We also show that the use of private LLCs can provide similar capability as long as the caches are not flushed on core migrations.</p><p>We evaluate PIE for a variety of systems with varying core counts and cache configurations. Across a large number of scheduling-sensitive workloads, we show that PIE scheduling is scalable to any core count and outperforms prior work by a significant margin.</p><p>In this paper, we focused on using PIE scheduling to improve the weighted speedup metric for a heterogeneous multi-core system. The evaluations were primarily done for multi-programmed workload mixes. However, PIE scheduling can also be applied to improve multi-threaded workload performance. Furthermore, when multiple workloads contend for the same core type, PIE scheduling can be extended to optimize for fairness. Exploring these extensions is part of our on-going work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Normalized big-core CPI stacks (right axis) and small-core slowdown (left axis). Benchmarks are sorted by their small-versus-big core slowdown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Correlating small-core slowdown to the MLP ratio for memory-intensive workloads (righthand side in the graph) and to the ILP ratio for the compute-intensive workloads (lefthand side in the graph). Workloads are sorted by their normalized memory CPI component (bottom graph).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of the PIE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Evaluating the accuracy of the PIE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparing different scheduling algorithms for type-I and type-III workload mixes assuming a static setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Evaluating PIE for heterogeneous multi-core with one big and three small cores (left graph), and three big cores and one small core (right graph).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Dynamic execution profile of libquantum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Migration overhead for a shared LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Migration overhead for private powered-off LLCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Migration overhead for private powered-on LLCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Relative performance (STP) delta over random scheduling for sampling-based, memory-dominance and PIE scheduling, assuming private LLCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Relative performance (STP) delta over random scheduling for sampling-based, memory-dominance and PIE scheduling, assuming an LRU-managed shared LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Relative performance (STP) delta over random scheduling for sampling-based, memory-dominance and PIE scheduling, assuming an RRIP-managed shared LLC.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If the cache hierarchy is different, then techniques described in<ref type="bibr" target="#b13">[14]</ref> can be used to estimate misses for a different cache size.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We also ran experiments with a 2-wide in-order processor and found the performance for the 2-wide in-order processor to be within 10% of the 4-wide in-order processor, which is a very small compared to the 200%+ performance difference between in-order versus out-of-order processor performance. Hence, we believe that our conclusions hold true irrespective of the width of the in-order processor.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">David Albonesi</rs>, the <rs type="funder">VSSAD group</rs>, <rs type="person">Socrates Demetriades</rs>, <rs type="person">Krishna Rangan</rs>, and the anonymous reviewers for their constructive and insightful feedback. Kenzo Van Craeynest is supported through a doctoral fellowship by the <rs type="funder">Agency for Innovation by Science and Technology (IWT)</rs>. Additional support is provided by the <rs type="funder">FWO</rs> projects G.<rs type="grantNumber">0255.08</rs> and <rs type="grantNumber">G.0179.10</rs>, and the <rs type="funder">European Research Council</rs> under the European Community's <rs type="programName">Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013) / ERC Grant</rs> agreement no. <rs type="grantNumber">259295</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7V7gzdM">
					<idno type="grant-number">0255.08</idno>
				</org>
				<org type="funding" xml:id="_NBJCeQA">
					<idno type="grant-number">G.0179.10</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_E9yeRkj">
					<idno type="grant-number">FP7/2007-2013) / ERC Grant</idno>
				</org>
				<org type="funding" xml:id="_ZbnshV4">
					<idno type="grant-number">259295</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The future is fusion: The industrychanging impact of accelerated computing</title>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<ptr target="http://sites.amd.com/us/Documents/AMDfusionWhitepaper.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic thread assignment on heterogeneous multiprocessor architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Becchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism (JILP)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient program scheduling for heterogeneous multi-core processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Design Automation Conference (DAC)</title>
		<meeting>the 46th Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
			<biblScope unit="page" from="927" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA</title>
		<meeting>ISCA</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding some simple processorperformance limits</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Emma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">System-level performance metrics for multi-program workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2008-06">May/June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A mechanistic performance model for superscalar out-oforder processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Register traffic analysis for streamlining inter-operation communication in fine-grain parallel processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICRO</title>
		<meeting>MICRO</meeting>
		<imprint>
			<date type="published" when="1992-12">Dec. 1992</date>
			<biblScope unit="page" from="236" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scheduling for heterogeneous processors in server systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Computing Frontiers (CF)</title>
		<meeting>the Second Conference on Computing Frontiers (CF)</meeting>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Greenhalgh</surname></persName>
		</author>
		<ptr target="http://www.arm.com/files/downloads/bigLITTLEFinalFinal.pdf" />
		<title level="m">LITTLE processing with ARM Cortex-A15 &amp; Cortex-A7: Improving energy efficiency in high-performance mobile platforms</title>
		<imprint>
			<date type="published" when="2011-09">Sept. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Intel&apos;s tiny Atom</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Halfhill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-04">Apr. 2008</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">2nd generation Intel Core vPro processor family</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="http://www.intel.com/content/dam/doc/white-paper/core-vpro-2nd-generation-core-vpro-processor-family-paper.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CMP$im: A Pin-based on-the-fly multi-core cache simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Workshop on Modeling, Benchmarking and Simulation (MoBS), held in conjunction with ISCA</title>
		<meeting>the Fourth Annual Workshop on Modeling, Benchmarking and Simulation (MoBS), held in conjunction with ISCA</meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CRUISE: Cache Replacement and Utility-aware Scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Najaf-Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="2011-03">March 2011</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (RRIP)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA</title>
		<meeting>ISCA</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to the Cell multiprocessor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hofstee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Maeurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shippy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="589" to="604" />
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bias scheduling in heterogeneous multi-core architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Systems (EuroSys)</title>
		<meeting>the European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2010-04">Apr. 2010</date>
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-ISA heterogeneous multi-core architectures: The potential for processor power reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICRO</title>
		<meeting>MICRO</meeting>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-ISA heterogeneous multi-core architectures for multithreaded workload performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA</title>
		<meeting>ISCA</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Age based scheduling for asymmetric multiprocessors</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Lakshminarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing: the Conference on High Performance Computing Networking, Storage and Analysis</title>
		<meeting>Supercomputing: the Conference on High Performance Computing Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient operating system scheduling for performance-asymmetric multi-core architectures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baumberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing: the Conference on High Performance Computing Networking, Storage and Analysis</title>
		<meeting>Supercomputing: the Conference on High Performance Computing Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Operating system support for overlapping-ISA heterogeneous multi-core architectures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knauerhase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HPCA</title>
		<meeting>HPCA</meeting>
		<imprint>
			<date type="published" when="2010-01">Jan. 2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Performance monitoring on the POWER5 microprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mericas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Evaluation and Benchmarking</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</editor>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="247" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HPCA</title>
		<meeting>HPCA</meeting>
		<imprint>
			<date type="published" when="2003-02">Feb. 2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The benefits of multiple CPU cores in mobile devices</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="http://www.nvidia.com/content/PDF/tegrawhitepapers/Benefits-of-Multi-core-CPUs-in-Mobile-DevicesVer1.2.pdf" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">tegra white papers/ Variable-SMP-A-Multi-Core-CPU-Architecture-for-Low-Power-and-High-Performance-v1.1.pdf</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="http://www.nvidia.com/content/PDF/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Variable SMP -a multi-core CPU architecture for low power and high performance</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pinpointing representative portions of large Intel Itanium programs with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karunanidhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICRO</title>
		<meeting>MICRO</meeting>
		<imprint>
			<date type="published" when="2004-12">Dec. 2004</date>
			<biblScope unit="page" from="81" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design tradeoffs for memory-level parallelism on a asymmetric multicore system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patsilaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Parallel Execution of Sequential Programs on Multi-core Architectures (PESPMA), held in conjunction with ISCA</title>
		<meeting>the Third Workshop on Parallel Execution of Sequential Programs on Multi-core Architectures (PESPMA), held in conjunction with ISCA</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficiently exploiting memory level parallelism on asymmetric coupled cores in the dark silicon era</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patsilaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HASS: A scheduler for heterogeneous multicore systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shelepov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C S</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jeffery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Symbiotic jobscheduling for simultaneous multithreading processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient Interaction between OS and Architecture in Heterogeneous Platforms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable thread scheduling and global power management for heterogeneous many-core architectures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACT</title>
		<meeting>PACT</meeting>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
			<biblScope unit="page" from="29" to="40" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
