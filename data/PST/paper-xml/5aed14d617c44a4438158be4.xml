<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement Learning for UAV Attitude Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">William</forename><surname>Koch</surname></persName>
							<email>wfkoch@bu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Renato</forename><surname>Mancuso</surname></persName>
							<email>rmancuso@bu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>West</surname></persName>
							<email>richwest@bu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Azer</forename><surname>Bestavros</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<addrLine>111 Cummington Mall</addrLine>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcement Learning for UAV Attitude Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F92515E5C6FB4A7551AA97A12145194</idno>
					<idno type="DOI">10.1145/3301273</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Computing methodologies → Reinforcement learning</term>
					<term>Control methods</term>
					<term>Machine learning</term>
					<term>• Computer systems organization → Embedded systems</term>
					<term>Attitude control, UAV, reinforcement learning, quadcopter, autopilot, machine learning, PID, intelligent control, adaptive control</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autopilot systems are typically composed of an "inner loop" providing stability and control, whereas an "outer loop" is responsible for mission-level objectives, such as way-point navigation. Autopilot systems for unmanned aerial vehicles are predominately implemented using Proportional-Integral-Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However, more sophisticated control is required to operate in unpredictable and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL), which has had success in other applications, such as robotics. Yet previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with state-of-the-art RL algorithms-Deep Deterministic Policy Gradient, Trust Region Policy Optimization, and Proximal Policy Optimization. To investigate these unknowns, we first developed an open source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then used our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Over the past decade, there has been an uptrend in the popularity of Unmanned Aerial Vehicles (UAVs). In particular, quadrotors have received significant attention in the research community, where a significant number of seminal results and applications has been proposed and experimented. This recent growth is primarily attributed to the drop in cost of onboard sensors, actuators, and small-scale embedded computing platforms. Despite the significant progress, flight control is still considered an open research topic. On the one hand, flight control inherently implies 22:2 W. <ref type="bibr">Koch et al.</ref> the ability to perform highly time-sensitive sensory data acquisition, processing, and computation of forces to apply to the aircraft actuators. On the other hand, it is desirable that UAV flight controllers are able to tolerate faults, adapt to changes in the payload and/or the environment, and optimize flight trajectory, to name a few.</p><p>Autopilot systems for UAVs are typically composed of an "inner loop" responsible for aircraft stabilization and control, and an "outer loop" to provide mission-level objectives (e.g., way-point navigation). Flight control systems for UAVs are predominately implemented using Proportional-Integral-Derivative (PID) control systems. PIDs have demonstrated exceptional performance in many circumstances, including in the context of drone racing, where precision and agility are key. In stable environments, a PID controller exhibits close to ideal performance. When exposed to unknown dynamics (e.g., wind, variable payloads, voltage sag), however, a PID controller can be far from optimal <ref type="bibr" target="#b29">[29]</ref>. For next-generation flight control systems to be intelligent, a way needs to be devised to incorporate adaptability to mutable dynamics and environment.</p><p>The development of intelligent flight control systems is an active area of research <ref type="bibr" target="#b32">[32]</ref>, specifically through the use of artificial neural networks, which are an attractive option given that they are universal approximators and resistant to noise <ref type="bibr" target="#b30">[30]</ref>.</p><p>Online learning methods (e.g., <ref type="bibr" target="#b14">[14]</ref>) have the advantage of learning the aircraft dynamics in real time. The main limitation with online learning is that the flight control system is only knowledgeable of its past experiences. It follows that its performances are limited when exposed to a new event. Training models offline using supervised learning is problematic, as data is expensive to obtain and derived from inaccurate representations of the underlying aircraft dynamics (e.g., flight data from a similar aircraft using PID control), which can lead to suboptimal control policies <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40]</ref>. To construct high-performance intelligent flight control systems, it is necessary to use a hybrid approach. First, accurate offline models are used to construct a baseline controller, whereas online learning provides fine tuning and real-time adaptation.</p><p>An alternative to supervised learning for creating offline models is known as reinforcement learning (RL). In RL, an agent is given a reward for every action it takes in an environment, with the objective to maximize the rewards over time. Using RL, it is possible to develop optimal control policies for a UAV without making any assumptions about the aircraft dynamics. Recent work has shown RL to be effective for UAV autopilots, providing adequate path tracking <ref type="bibr" target="#b18">[18]</ref>. Nonetheless, previous work on intelligent flight control systems has primarily focused on guidance and navigation.</p><p>Open challenges in RL for attitude control. RL is currently being applied to a wide range of applications, each with its own set of challenges. Attitude control for UAVs is a particularly interesting RL problem for several reasons. We have highlighted three areas we find important: C1: Precision and Accuracy. Many RL tasks can be solved in a variety of ways. For example, to win a game, there may be several sequential moves that will lead to the same outcome.</p><p>In the case of optimal attitude control, there is little tolerance and flexibility as to the sequence of control signals that will achieve the desired attitude (e.g., angular rate) of the aircraft. Even the slightest deviations can lead to instabilities. It remains unclear what level of control accuracy can be achieved when using intelligent control trained with RL for timesensitive attitude control (i.e., the "inner loop"). Therefore, determining the achievable level of accuracy is critical in establishing if RL is suitable for attitude flight control. C2: Robustness and Adaptation: In the context of control, robustness refers to the controller's performance in the presence of uncertainty when control parameters are fixed, whereas adaptiveness refers to the controller's performance to adapt to the uncertainties by adjusting the control parameters <ref type="bibr" target="#b37">[37]</ref>. It is assumed that the neural network trained with</p><p>Reinforcement Learning for UAV Attitude Control 22:3 RL will face uncertainties when transferred to physical hardware due to the gap between the RL environment and the real world. However, it remains unknown in what range of uncertainty the controller can operate safely before adaptation is necessary. Characterizing the controller's robustness will provide valuable insight into the design of the intelligent flight control system architecture. For instance, what will be the necessary adaptation rate, and what sensor data can be collected from the real world to update the RL environment? C3: Reward Engineering: In RL, reward engineering is the process of designing a reward system to provide the agent a signal showing that they are doing the right thing <ref type="bibr" target="#b12">[12]</ref>. In the context of attitude control, the reward must encapsulate the agent's performance in achieving the desired attitude goals. As goals become more complex and demanding (e.g., minimizing energy consumption or stability in presence of damage), identifying which performance metrics are most expressive will be necessary to push the performance of intelligent control systems trained with RL.</p><p>Our contributions. In this article, we study C1 in depth the accuracy and precision of attitude control provided by intelligent flight controllers trained using RL. Although we specifically focus on the creation of controllers for the Iris quadcopter <ref type="bibr" target="#b3">[4]</ref>, the methods developed apply to a wide range of multirotor UAVs and can also be extended to fixed-wing aircraft. We develop a novel training environment called GymFC with the use of a high-fidelity physics simulator for the agent to learn attitude control. GymFC is an OpenAI Environment <ref type="bibr" target="#b11">[11]</ref> providing a common interface for researchers to develop intelligent flight control systems. The simulated environment consists of an Iris quadcopter digital replica or digital twin <ref type="bibr" target="#b16">[16]</ref> with the intention of eventually be used to transfer the trained controller to physical hardware. Controllers are trained using state-of-theart RL algorithms: Deep Deterministic Policy Gradient (DDPG), Trust Region Policy Optimization (TRPO), and Proximal Policy Optimization (PPO). We then compare the performance of our synthesized controllers with that of a PID controller. Our evaluation finds that controllers trained using PPO outperform PID control and are capable of exceptional performance. To summarize, this article makes the following contributions:</p><p>• GymFC, an open source <ref type="bibr" target="#b22">[22]</ref> environment for developing an intelligent attitude flight controller, providing the research community a tool to progress performance. • A learning architecture for attitude control utilizing digital twinning concepts for minimal effort when transferring trained controllers into hardware. • An evaluation for state-of-the-art RL algorithms, such as DDPG, TRPO, and PPO, learning policies for aircraft attitude control. As a first work in this direction, our evaluation also establishes a baseline for future work. • An analysis of intelligent flight control performance developed with RL compared to traditional PID control.</p><p>The remainder of this article is organized as follows. In Section 2, we provide an overview of the quadcopter flight dynamics and RL. Next, in Section 3, we briefly survey existing literature on intelligent flight control. In Section 4, we present our training environment and then use this environment to evaluate RL performance for flight control in Section 5. Finally, Section 6 concludes the article and provides several future research directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Quadcopter Flight Dynamics</head><p>A quadcopter is an aircraft with six degrees of freedom (DOF), three rotational and three translational. With four control inputs (one to each motor), this results in an underactuated system that requires an onboard computer to compute motor signals to provide stable flight. We indicate with ω i , i ∈ 1, . . . , M the rotation speed of each rotor where M = 4 is the total number of motors for a quadcopter. These have a direct impact on the resulting Euler angles ϕ, θ,ψ , for instance, roll, pitch, and yaw, respectively, which provide rotation in D = 3 dimensions. Moreover, they produce a certain amount of upward thrust, indicated with f .</p><p>The aerodynamic effect that each ω i produces depends on the configuration of the motors. The most popular configuration is an "X" configuration, depicted in Figure <ref type="figure" target="#fig_0">1</ref>(a), which has the motors mounted in an "X" formation relative to what is considered the front of the aircraft. This configuration provides more stability compared to a "+" configuration, which in contrast has its motor configuration rotated an additional 45 • along the z-axis. This is due to the differences in torque generated along each axis of rotation in respect to the distance of the motor from the axis. The aerodynamic affect u that each rotor speed ω i has on thrust and Euler angles is given by</p><formula xml:id="formula_0">u f = b (ω 2 1 + ω 2 2 + ω 2 3 + ω 2 4 )<label>(1)</label></formula><formula xml:id="formula_1">u ϕ = b (ω 2 1 + ω 2 2 -ω 2 3 -ω 2 4 )<label>(2)</label></formula><formula xml:id="formula_2">u θ = b (ω 2 1 -ω 2 2 + ω 2 3 -ω 2 4 )<label>(3)</label></formula><formula xml:id="formula_3">u ψ = b (ω 2 1 -ω 2 2 -ω 2 3 + ω 2 4 ).<label>(4)</label></formula><p>where u f , u ϕ , u θ , u ψ is the thrust, roll, pitch, and yaw effect, respectively, whereas b is a thrust factor that captures propeller geometry and frame characteristics. For further details about the mathematical models of quadcopter dynamics, please refer to Bouabdallah et al. <ref type="bibr" target="#b10">[10]</ref>.</p><p>To perform a rotational movement, the velocity of each rotor is manipulated according to the relationship expressed in Equation ( <ref type="formula" target="#formula_3">4</ref>) and as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(b) through (d). For example, to roll right (Figure <ref type="figure" target="#fig_0">1(b)</ref>), more thrust is delivered to motors 3 and 4. Yaw (Figure <ref type="figure" target="#fig_0">1(d)</ref>) is not achieved directly through difference in thrust generated by the rotor as roll and pitch are, but instead through a difference in torque in the rotation speed of rotors spinning in opposite directions. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(d), higher rotation speed for rotors 1 and 4 allow the aircraft to yaw clockwise. Because a net positive torque counterclockwise causes the aircraft to rotate clockwise due to Newton's second law of motion.</p><p>Attitude, in respect to orientation of a quadcopter, can be expressed by its angular velocities of each axis Ω = [Ω ϕ , Ω θ , Ω ψ ]. The objective of attitude control is to compute the required motor signals to achieve some desired attitude Ω * .</p><p>In autopilot systems, attitude control is executed as an inner control loop and is time sensitive. Once the desired attitude is achieved, translational movement (in the X, Y, Z direction) is accomplished by applying thrust proportional to each motor.</p><p>In commercially available quadcopters, the vast, if not all, use PID attitude control. A PID controller is a linear feedback controller expressed mathematically as</p><formula xml:id="formula_4">u (t ) = K p e (t ) + K i t 0 e (τ )dτ + K d de (t ) dt ,<label>(5)</label></formula><p>where K p , K i , K d are configurable constant gains and u (t ) is the control signal. The effect of each term can be thought of as the P term considers the current error, the I term considers the history of errors, and the D term estimates the future error. For attitude control in a quadcopter aircraft, there is PID control for each roll, pitch, and yaw axis. At each cycle in the inner loop, each PID sum is computed for each axis and then these values are translated into the amount of power to deliver to each motor through a process called mixing. Mixing uses a table consisting of constants describing the geometry of the frame to determine how the axis control signals are summed based on the torques that will be generated by the length of each arm (recall differences between "X" and "+" frames). The control signal for each motor y i is loosely defined as</p><formula xml:id="formula_5">y i = f (m (i,ϕ ) u ϕ + m (i,θ ) u θ + m (i,ψ ) u ψ ),<label>(6)</label></formula><p>where m (i,ϕ ) , m (i,θ ) , m (i,ψ ) are the mixer values for motor i and f is the throttle coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement Learning</head><p>In this work, we consider an RL architecture (depicted in Figure <ref type="figure" target="#fig_1">2</ref>) consisting of a neural network flight controller as an agent interacting with an Iris quadcopter <ref type="bibr" target="#b3">[4]</ref> in a high-fidelity physics simulated environment E, more specifically using the Gazebo simulator <ref type="bibr" target="#b23">[23]</ref>.</p><p>At each discrete timestep t, the agent receives an observation x t from the environments consisting of the angular velocity error of each axis e = Ω * -Ω and the angular velocity of each rotor ω i , which are obtained from the quadcopter's inertial measurement unit (IMU) and electronic speed controller (ESC) sensors, respectively. These observations are in the continuous observation spaces x t ∈ R (M +D ) . Once the observation is received, the agent executes an action a t within E. In return, the agent receives a single numerical reward r t indicating the performance of this action. The action is also in a continuous action space a t ∈ R M and corresponds to the four control signals u (t ) sent to each ESC driving the attached motor M. Because the agent is only receiving this sensor data, it is unaware of the physical environment and the aircraft dynamics, and therefore E is only partially observed by the agent. Motivated by Minh et al. <ref type="bibr" target="#b31">[31]</ref>, we consider the state to be a sequence of the past observations and actions</p><formula xml:id="formula_6">s t = x i , a i , . . . , a t -1 , x t .</formula><p>The interaction between the agent and E is formally defined as a Markov decision process (MDP) where the state transitions are defined as the probability of transitioning to state s given the current state and action respectively are s and a, Pr {s t +1 = s |s t = s, a t = a}. The behavior of the agent is defined by its policy π , which is essentially a mapping of what action should be taken for a particular state. The objective of the agent is to maximize the returned reward overtime to develop an optimal policy. We welcome the reader to refer to Sutton and Barto <ref type="bibr" target="#b36">[36]</ref> for further details on RL.</p><p>Until recently, control in a continuous action space was considered difficult for RL. Significant progress has been made combining the power of neural networks with RL. In this work,we elected to use DDPG <ref type="bibr" target="#b27">[27]</ref> and TRPO <ref type="bibr" target="#b33">[33]</ref> due to the recent use of these algorithms for quadcopter navigation control <ref type="bibr" target="#b18">[18]</ref>. DDPG provides improvement to the Deep Q-Network (DQN) <ref type="bibr" target="#b31">[31]</ref> for the continuous action domain. It employs an actor-critic architecture using two neural networks for each actor and critic. It is also a model-free algorithm, meaning that it can learn the policy without having to first generate a model. TRPO is similar to natural gradient policy methods; however, this method guarantees monotonic improvements. We additionally include a third algorithm for our analysis: PPO <ref type="bibr" target="#b34">[34]</ref>. PPO is known to outperform other state-of-the-art methods in challenging environments. PPO is also a policy gradient method and has similarities to TRPO while being easier to implement and tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Aviation has a rich history in flight control dating back to the 1960s. During this time, supersonic aircraft were being developed, which demanded more sophisticated dynamic flight control than what a static linear controller could provide. Gain scheduling <ref type="bibr" target="#b26">[26]</ref> was developed, allowing multiple linear controllers of different configurations to be used in designated operating regions. This, however, was inflexible and insufficient for handling the nonlinear dynamics at high speeds but paved the way for adaptive control. For a period of time, many experimental adaptive controllers were being tested but were unstable. Later advances were made to increase stability with model reference adaptive control (MRAC) <ref type="bibr" target="#b39">[39]</ref> and L 1 <ref type="bibr" target="#b17">[17]</ref>, which provided reference models during adaptation. Additionally numerous other nonlinear control algorithms have been developed and applied to flight control, including feedback linearization <ref type="bibr" target="#b25">[25]</ref>, sliding mode control <ref type="bibr" target="#b25">[25]</ref>, and backstepping <ref type="bibr" target="#b28">[28]</ref>, in an attempt to model the nonlinear dynamics of the aircraft.</p><p>As the cost of small-scale embedded computing platforms dropped, intelligent flight control options became realistic and have been actively researched over the past decade to design flight control solutions that are able to adapt and also learn. The ability to learn is an important distinction that differentiates intelligent control from the control algorithms discussed previously. Intelligent control architectures are capable of planning for future events such as system failure, damage, and emergencies-tasks that would otherwise be difficult or impossible for other control algorithms <ref type="bibr" target="#b24">[24]</ref>.</p><p>As performance demands for UAVs continue to increase, we are beginning to see signs of flight control history repeating itself. The popular high-performance drone racing firmware Betaflight <ref type="bibr">[2]</ref> has recently added a gain scheduler to adjust PID gains depending on throttle and voltage levels. Intelligent PID flight control <ref type="bibr" target="#b15">[15]</ref> methods have been proposed in which PID gains are dynamically updated online, providing adaptive control as the environment changes. However, these solutions still inherit disadvantages associated with PID control, such as integral windup, need for mixing, and, most significantly, they are feedback controllers and therefore inherently reactive. Yet feedforward control (or predictive control) is proactive and allows the controller to output control signals before an error occurs. For feedforward control, a model of the system must exist. Learning-based intelligent control has been proposed to develop models of the aircraft for predictive control using artificial neural networks.</p><p>Notable work by Dierks and Jagannathan <ref type="bibr" target="#b14">[14]</ref> proposes an intelligent flight control system constructed with neural networks to learn the quadcopter dynamics, online, to navigate along a specified path. This method allows the aircraft to adapt in real time to external disturbances and unmodeled dynamics. Matlab simulations demonstrate that their approach outperforms a PID controller in the presence of unknown dynamics, specifically in regard to control effort required to track the desired trajectory. Nonetheless, the proposed approach does require prior knowledge of the aircraft mass and moments of inertia to estimate velocities. Online learning is an essential component to constructing a complete intelligent flight control system. It is fundamental, however, to develop accurate offline models to account for uncertainties encountered during online learning <ref type="bibr" target="#b32">[32]</ref>. To build offline models, previous work has used supervised learning to train intelligent flight control systems using a variety of data sources, such as test trajectories <ref type="bibr" target="#b9">[9]</ref>, and PID step responses <ref type="bibr" target="#b35">[35]</ref>. The limitation of this approach is that training data may not accurately reflect the underlying dynamics. In general, supervised learning on its own is not ideal for interactive problems such as control <ref type="bibr" target="#b36">[36]</ref>.</p><p>RL has similar goals to adaptive control in which a policy improves over time when interacting with its environment. RL has been applied to autonomous helicopters to learn how to track trajectories (guidance), specifically how to hover in place and perform various maneuvers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">21]</ref>. Kim et al. <ref type="bibr" target="#b21">[21]</ref> and Abbeel et al. <ref type="bibr" target="#b5">[6]</ref> demonstrated their trained helicopter's capabilities in helicopter competitions requiring the aircraft to perform advanced aerobatic maneuvers. Performance was compared to trained pilots; nevertheless, it is unknown how their controllers compare to PID control for tracking trajectories. In contrast to their work, we investigate the use and accuracy of using RL for low-level manipulation of the aircraft actuators to maintain a desired attitude, not for guidance control. Furthermore our goal is to compare controllers taught with RL to PID control to determine what applications, if any, would be more appropriate. The first use of RL in quadcopter control was presented by Waslander et al. <ref type="bibr" target="#b38">[38]</ref> for altitude control. The authors developed a model-based RL algorithm to search for an optimal control policy. The controller was rewarded for accurate tracking and damping. Their design provided significant improvements in stabilization in comparison to a linear control system. More recently, Hwangbo et al. <ref type="bibr" target="#b18">[18]</ref> used RL for quadcopter control, particularly for navigation control. They developed a novel deterministic on-policy learning algorithm that outperformed TRPO <ref type="bibr" target="#b33">[33]</ref> and DDPG <ref type="bibr" target="#b27">[27]</ref> in regard to training time. Furthermore, the authors validated their results in the real world, transferring their simulated model to a physical quadcopter. Path tracking turned out to be adequate. Notably, the authors discovered major differences in transferring from simulation to the real world. Known as the reality gap, transferring from simulation to the real world has been researched extensively as being problematic without taking additional steps to increase realism in the simulator <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b30">30]</ref>.</p><p>Most prior work has focused on performance of navigation and guidance. There is limited and insufficient data justifying the accuracy and precision of neural network-based intelligent attitude flight control and none to our knowledge for controllers trained using RL. Furthermore, this work uses physics simulations in contrast to mathematical models of the aircraft and environments used in aforementioned prior work for increased realism. The goal of this work is to provide a platform for training attitude controllers with RL, and to provide performance baselines in regard to attitude controller accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ENVIRONMENT</head><p>In this section, we describe our learning environment, GymFC, for developing intelligent flight control systems using RL. The goal of the proposed environment is to allow the agent to learn attitude control of an aircraft with only the knowledge of the number of actuators. GymFC includes both an episodic task and a continuous task. In an episodic task, the agent is required to learn a policy for responding to individual angular velocity commands. This allows the agents to learn the step response from rest for a given command, allowing its performance to be accurately measured. Episodic tasks, however, are not reflective of realistic flight conditions. For this reason, in a continuous task, pulses with random widths and amplitudes are continuously generated and correspond to angular velocity setpoints. The agent must respond accordingly and track the desired target over time. In Section 5, we evaluate our synthesized controllers via episodic tasks, but we have strong experimental evidence that training via episodic tasks produces controllers that behave correctly in continuous tasks as well (Appendix A).</p><p>GymFC has a multilayer hierarchical architecture composed of three layers: (i) a digital twin layer, (ii) a communication layer, and (iii) an agent-environment interface layer. This design decision was made to clearly establish roles and allow layer implementations to change (e.g., to use a different simulator) without affecting other layers as long as the layer-to-layer interfaces remain intact. A high-level overview of the environment architecture is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. We will now discuss in greater detail each layer with a bottom-up approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Digital Twin Layer</head><p>At the heart of the learning environment is a high-fidelity physics simulator that provides functionality and realism that is hard to achieve with an abstract mathematical model of the aircraft and environment. One of the primary design goals of GymFC is to minimize the effort required to transfer a controller from the learning environment into the final platform. For this reason, the simulated environment exposes identical interfaces to actuators and sensors as they would exist in the physical world. In the ideal case, the agent should not be able to distinguish between interaction with the simulated world (i.e., its digital twin) and its hardware counterpart. In this work, we use the Gazebo simulator <ref type="bibr" target="#b23">[23]</ref> in light of its maturity, flexibility, extensive documentation, and active community.</p><p>In a nutshell, the digital twin layer is defined by (i) the simulated world and (ii) its interfaces to the above communication layer (see Figure <ref type="figure" target="#fig_2">3</ref>). Simulated world. The simulated world is constructed specifically for UAV attitude control in mind. The technique we developed allows attitude control to be accomplished independently of guidance and/or navigation control. This is achieved by fixing the center of mass of the aircraft to a ball joint in the world, allowing it to rotate freely in any direction, which would be impractical, if not impossible, to achieve in the real world due to gimbal lock and friction of such an apparatus. In this work, the aircraft to be controlled in the environment is modeled off of the Iris quadcopter <ref type="bibr" target="#b3">[4]</ref> with a weight of 1.5Kg and 550mm motor-to-motor distance. An illustration of the quadcopter in the environment is displayed in Figure <ref type="figure" target="#fig_3">4</ref>. Note during training that Gazebo runs in headless mode  without this user interface to increase simulation speed. This architecture, however, can be used with any multicopter as long as a digital twin can be constructed. Helicopters and multicopters represent excellent candidates for our setup because they can achieve a full range of rotations along all three axes. This is typically not the case with fixed-wing aircraft. Our design can, however, be expanded to support fixed-wing aircraft by simulating airflow over the control surfaces for attitude control. Gazebo already integrates a set of tools to perform airflow simulation.</p><p>Interface. The digital twin layer provides two command interfaces to the communication layer: simulation reset and motor update. Simulation reset commands are supported by Gazebo's API and are not part of our implementation. Motor updates are provided by a UDP server. We hereby discuss our approach to developing this interface.</p><p>To keep synchronicity between the simulated world and the controller of the digital twin, the pace at which the simulation should progress is directly enforced. This is possible by controlling the simulator step by step. In our initial approach, Gazebo's Google Protobuf <ref type="bibr" target="#b4">[5]</ref> API was used, with a specific message to progress by a single simulation step. By subscribing to status messages (which include the current simulation step), it is possible to determine when a step has completed and to ensure synchronization. However, as we attempted to increase the rate of advertising step messages, we discovered that the rate of status messages is capped at 5Hz. Such a limitation introduces a consistent bottleneck in the simulation/learning pipeline. Furthermore, it was found that Gazebo silently drops messages that it cannot process.</p><p>A set of important modifications were made to increase experiment throughput. The key idea was to allow motor update commands to directly drive the simulation clock. By default, Gazebo comes preinstalled with an ArduPilot ArduCopter <ref type="bibr" target="#b0">[1]</ref> plugin to receive motor updates through a UDP server. These motor updates are in the form of pulse width modulation (PWM) signals.</p><p>At the same time, sensor readings from the IMU on board the aircraft is sent over a second UDP channel. ArduCopter is an open source multicopter firmware, and its plugin was developed to support software in the loop (SITL).</p><p>We derived our Aircraft Plugin from the ArduCopter plugin with the following modifications (as well as those discussed in Section 4.2). On receiving a motor command, the motor forces are updated as normal but then a simulation step is executed. Sensor data is read and then sent back as a response to the client over the same UDP channel. In addition to the IMU sensor data, we also simulate sensor data obtained from the ESC. The ESC provides the angular velocities of each rotor, which are relayed to the client as well. Implementing our Aircraft Plugin with this approach successfully allowed us to work around the limitations of the Google Protobuf API and increased step throughput by more than 200 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Communication Layer</head><p>The communication layer is positioned in between the digital twin and the agent-environment interface. This layer manages the low-level communication channel to the aircraft and simulation control. The primary function of this layer is to export a high-level synchronized API to the higher layers for interacting with the digital twin that uses asynchronous communication protocols. This layer provides the commands pwm_write and reset to the agent-environment interface layer.</p><p>The function call pwm_write takes as input a vector of PWM values for each actuator, corresponding to the control input u (t ). These PWM values correspond to the same values that would be sent to an ESC on a physical UAV. The PWM values are translated to a normalized format expected by the Aircraft Plugin and then packed into a UDP packet for transmission to the Aircraft Plugin UDP server. The communication layer blocks until a response is received from the Aircraft Plugin, forcing synchronized writes for the above layers. The UDP reply is unpacked and returned in response.</p><p>During the learning process, the simulated environment must be reset at the beginning of each learning episode. Ideally, one could use the gz command line utility included with the Gazebo installation, which is lightweight and does not require additional dependencies. Unfortunately, there is a known socket handle leak <ref type="bibr" target="#b2">[3]</ref> that causes Gazebo to crash if the command is issued more than the maximum number of open files allowed by the operating system. Given that we are running thousands episodes during training, this was not an option for us. Instead, we opted to use the Google Protobuffer interface, so we did not have to deploy a patched version of the utility on our test servers. Because resets only occur at the beginning of a training session and are not in the critical processing loop using Google Protobuffers here is acceptable.</p><p>On start of the communication layer, a connection is established with the Google Protobuff API server, and we subscribe to world statistics messages that include the current simulation iteration. To reset the simulator, a world control message is advertised, instructing the simulator to reset the simulation time. The communication layer blocks until it receives a world statistics message indicating that the simulator has been reset and then returns back control to the agent-environment interface layer. Note that the world control message is only resetting the simulation time, not the entire simulator (i.e., models and sensors). This is because we found that in some cases when a world control message was issued to perform a full reset, the sensor data took a few additional iterations for reset. To ensure proper reset to the above layers, this time reset message acts as a signaling mechanism to the Aircraft Plugin. When the plugin detects that a time reset has occurred, it resets the whole simulator and, most importantly, steps the simulator until the sensor values have also reset ensuring above layers that when a new training session starts, reading sensor values accurately reflect the current state and not the previous state from stale values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Environment Interface Layer</head><p>The topmost layer interfacing with the agent is the environment interface layer that implements the OpenAI Gym <ref type="bibr" target="#b11">[11]</ref> environment API. Each OpenAI Gym environment defines an observation space and an action space. These are used to inform the agent of the bounds to expect for environment observations and what are legal bounds for the action input, respectively. As mentioned in Section 2.2, GymFC is in both the continuous observation space and action space domain. The state is of size m × (M + D), where m is the memory size indicating the number of past observations, M = 4 as we consider a four-motors configuration, and D = 3 since each measurement is taken in the three dimensions. Each observation value is in [-∞ : ∞]. The action space is of size M equivalent to the number of control actuators of the aircraft (i.e., four for a quadcopter), where each value is normalized between [-1 : 1] to be compatible with most agents who squash their output using the tanh function.</p><p>GymFC implements two primary OpenAI functions, namely reset and step. The reset function is called at the start of an episode to reset the environment and returns the initial environment state. This is also when the desired target angular velocity Ω * or setpoint is computed. The setpoint is randomly sampled from a uniform distribution between [Ω min , Ω max ]. For the continuous task, this is also set at a random interval of time. Selection of these bounds may refer to the desired operating region of the aircraft. Although it is highly unlikely during normal operation that a quadcopter will be expected to reach the majority of these target angular velocities, the intention of these tasks are to push and stress the performance of the aircraft.</p><p>The step function executes a single simulation step with the specified actions and returns to the agent the new state vector, together with a reward indicating how well the given action was performed. Reward engineering can be challenging. If careful design is not performed, the derived policy may not reflect what was originally intended. Recall from Section 2.2 that the reward is ultimately what shapes the policy. For this work, with the goal of establishing a baseline of accuracy, we develop a reward to reflect the current angular velocity error (i.e., e = Ω * -Ω). In the future, GymFC will be expanded to include additional environments aiding in the development of more complex policies' particularity to showcase the advantages of using RL to adapt and learn. We translate the current error e t at time t into a derived reward r t normalized between [-1, 0] as follows:</p><formula xml:id="formula_7">r t = -clip sum(|Ω * t -Ω t |)/3Ω max , 0, 1 ,<label>(7)</label></formula><p>where the sum function sums the absolute value of the error of each axis, and the clip function clips the result between [0, 1] in cases where there is an overflow in the error. Since the reward is negative, it signifies a penalty, and the agent maximizes the rewards (thus minimizing error) over time to track the target as accurately as possible. Rewards are normalized to provide standardization and stabilization during training <ref type="bibr" target="#b20">[20]</ref>.</p><p>Additionally, we experimented with a variety of other rewards. We found sparse binary rewards<ref type="foot" target="#foot_3">1</ref> to give poor performance. We believe that this is due to complexity of quadcopter control and the limitations of the RL algorithms we tested. In the early stages of learning, the agent explores its environment. However, the event of randomly reaching the target angular velocity within some threshold was rare and thus did not provide the agent with enough information to converge. Conversely, we found that signaling at each timestep was best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In this section, we present our evaluation on the accuracy of studied neural network-based attitude flight controllers trained with RL. Due to space limitations, we present evaluation and results only for episodic tasks, as they are directly comparable to our baseline (PID). Nonetheless, we have obtained strong experimental evidence that agents trained using episodic tasks perform well in continuous tasks (Appendix A). To our knowledge, this is the first RL baseline conducted for quadcopter attitude control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We evaluate the RL algorithms DDPG, TRPO, and PPO using the implementations in the OpenAI Baselines project <ref type="bibr" target="#b13">[13]</ref>. The goal of the OpenAI Baselines project is to establish a reference implementation of RL algorithms, providing baselines for researchers to compare approaches and build on. Every algorithm is run with defaults except for the number of simulations steps, which we increased to 10 million.</p><p>The episodic task parameters were configured to run each episode for a maximum of 1 second of simulated time, allowing enough time for the controller to respond to the command and additional time to identify if a steady state has been reached. The bounds the target angular velocity is sampled from is set to Ω min = -5.24 rad/s, Ω max = 5.24 rad/s (± 300 deg/s). These limits were constructed by examining PID's performance to make sure we expressed physically feasible constraints. The max step size of the Gazebo simulator, which specifies that the duration of each physics update step was set to 1 ms to develop highly accurate simulations. In other words, our physical world "evolved" at 1kHz. Training and evaluations were run on Ubuntu 16.04 with an eight-core i7-7700 CPU and an NVIDIA GeForce GT 730 graphics card.</p><p>For our PID controller, we ported the mixing and SITL implementation from Betaflight <ref type="bibr">[2]</ref> to Python to be compatible with GymFC. The PID controller was first tuned using the classical Ziegler-Nichols method <ref type="bibr" target="#b41">[41]</ref> and then manually adjusted to improve performance of the step response sampled around the midpoint ±Ω max /2. We obtained the following gains for each axis of rotation: K ϕ = [2, 10, 0.005], K θ = [10, 10, 0.005], K ψ = [4, 50, 0.0], where each vector contains the [K p , K i , K d ] (proportional, integrative, derivative) gains, respectively. Next, we measured the distances between the arms of the quadcopter to calculate the mixer values for each motor m i , i ∈ {1, . . . 4}. Each vector m i is of the form To evaluate and compare the accuracy of the different algorithms, we used a set of metrics. First, we define "initial error" as the distance between the rest velocities and the current setpoint. A notion of progress toward the setpoint from rest can then be expressed as the percentage of the initial error that has been "corrected." Correcting 0% of the initial error means that no progress Reinforcement Learning for UAV Attitude Control 22:13 Fig. <ref type="figure">5</ref>. Average normalized rewards shown in magenta received during training of 10,000 episodes (10 million steps) for each RL algorithm and memory m sizes 1, 2, and 3. Plots share a common y and x axis. Additionally, yellow represents the 95% confidence interval and the black line is a 2-degree polynomial added to illustrate the trend of the rewards over time.</p><formula xml:id="formula_8">m i = [m (i,ϕ ) , m (i,θ ) , m (i,ψ ) ]-</formula><p>has been made, whereas 100% indicates that the setpoint has been reached. Each metric value is independently computed for each axis. We hereby list our metrics. Success captures the number of experiments (in percentage) in which the controller eventually settles in a band within 90% and 110% of the initial error (i.e., ±10% from the setpoint). Failure captures the average percentage error relative to the initial error after t = 500ms for those experiments that do not make it in the ±10% error band. The latter metric quantifies the magnitude of unacceptable controller performance. The delay in the measurement (t &gt; 500ms) is to exclude the rise regime. The underlying assumption is that a steady state is reached before 500ms. Rise is the average time in milliseconds it takes the controller to go from 10% to 90% of the initial error. Peak is the max achieved angular velocity represented as a percentage relative to the initial error. Values greater than 100% indicate overshoot, whereas values less than 100% represent undershoot. Error is the average sum of the absolute value error of each episode in radians per second. This provides a generic metric for performance. Our last metric is Stability, which captures how stable the response is halfway through the simulation (i.e., at t &gt; 500ms). Stability is calculated by taking the linear regression of the angular velocities and reporting the slope of the calculated line. Systems that are unstable have a nonzero slope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Each learning agent was trained with an RL algorithm for a total of 10 million simulation steps, equivalent to 10,000 episodes or about 2.7 simulation hours. The agents configuration is defined as the RL algorithm used for training and its memory size m. Training for DDPG took approximately 33 hours, whereas PPO and TRPO took approximately 9 hours and 13 hours, respectively. The average sum of rewards for each episode is normalized between [-1, 0] and displayed in Figure <ref type="figure">5</ref>. This computed average in magenta is from three independently trained agents with the same configuration, whereas the 95% confidence is shown in yellow. Additionally, we have added a 2-degree polynomial in black fit to the data to illustrate the reward trend over time. Training results show clearly that PPO converges consistently compared to TRPO and DDPG, and overall PPO accumulates higher rewards. What is also interesting and counterintuitive is that the larger memory size actually decreases convergence and stability among all trained algorithms. Recall from Section 2 that RL algorithms learn a policy to map states to action. A reason for the decrease in convergence could be attributed to the state space increasing, causing the RL algorithm to take longer to learn the mapping to the optimal action. As part of our future work, we plan to investigate using separate memory sizes for the error and rotor velocity to decrease the state space. Reward gains during training of TRPO and DDPG are quite inconsistent with large confidence intervals.</p><p>Although performance for DDPG m = 1 looks promising, on further investigation into the large confidence interval, we found that this was due to the algorithm completely failing to respond to certain command inputs, thus questioning whether the algorithm has learned the underlying flight dynamics (this is emphasized later in Table <ref type="table" target="#tab_1">2</ref>).</p><p>In the future, we plan to investigate methods to decrease training times by addressing C2 and C3 in Section 1. Specific to C2 to support a large range of aircraft, we will explore whether we can construct a generic neural network taught general flight dynamics (Section 2.1), which will provide a baseline to extend training to create intelligent controllers unique to an aircraft (otherwise known as domain adaptation <ref type="bibr" target="#b8">[8]</ref>). Additionally, considering C3, we will experiment with developing more expressive reward functions to decrease training times.</p><p>Each trained agent was then evaluated on 1,000 never before seen command inputs in an episodic task. Since there are three agents per configuration, each configuration was evaluated over a total of 3,000 episodes. The average performance metrics for Rise, Peak, Error, and Stability for the response to the 3,000 command inputs is reported in Table <ref type="table" target="#tab_0">1</ref>. Results show that the agent trained with PPO outperforms TRPO and DDPG in every measurement. In fact, PPO is the only one that is able to achieve stability (for every m), whereas all other agents have at least one axis where the Stability metric is nonzero.</p><p>Next, the best-performing agent for each algorithm and memory size is compared to the PID controller. The best agent was selected based on the lowest sum of errors of all three axes reported by the Error metric. The Success and Failure metrics are compared in Table <ref type="table" target="#tab_1">2</ref>. Results show that agents trained with PPO would be the only ones good enough for flight, with a success rate close to perfect, and where the roll failure of 0.2% is only off by about 0.1% from the setpoint. However, the Reinforcement Learning for UAV Attitude Control 22:15 best-trained agents for TRPO and DDPG are often significantly far away from the desired angular velocity. For example, TRPO's best agent, 39.2% (60.8% success, see Table <ref type="table" target="#tab_1">2</ref>) of the time, does not reach the desired pitch target with upward of a 20% error from the setpoint.</p><p>Next, we provide our thorough analysis comparing the best agents in Table <ref type="table" target="#tab_2">3</ref>. We have found that RL agents trained with PPO using m = 1 provide performance and accuracy exceeding that of our PID controller in regard to rise time, peak velocities achieved, and total error. What is interesting is that usually a fast rise time could cause overshoot; however, the PPO agent has, on average, a faster rise time and less overshoot. Both PPO and PID reach a stable state measured halfway through the simulation.</p><p>To illustrate the performance of each of the best agents, a random simulation is sampled and the step response for each attitude command is displayed in Figure <ref type="figure" target="#fig_5">6</ref>, along with the target angular velocity to achieve Ω * . All algorithms reach some steady state; however, only PPO and PID  do so within the error band indicated by the dashed red lines. TRPO and DDPG have extreme oscillations in both the roll and yaw axes, which would cause instability during flight. In this particular example, we can observe PID to perform better with a 19% decrease in error compared to PPO, most visibly in yaw control. However, globally speaking, in terms of error, PPO has shown to be a more accurate attitude controller.</p><p>To highlight the performance and accuracy of the PPO agent, we sample another simulation and show the step response and also the PWM control signals generated by each controller in Figure <ref type="figure" target="#fig_6">7</ref>. In this figure, we can see that the PPO agent has exceptional tracking capabilities of the desired attitude. Compared to PID, the PPO controller has a 44% decrease in error. The PPO agent has a 2.25 times faster rise time on the roll axis, is 2.5 times faster on the pitch axis, and is 1.15 times faster on the yaw axis. Furthermore, the PID controller experiences slight overshoot in both the roll and yaw axes, whereas the PPO agent does not. In regard to the control output, the PID controller exerts more power to motor 3 but then motor values eventually level off, whereas the PPO control signal oscillates comparably more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FUTURE WORK AND CONCLUSION</head><p>In this article, we presented our RL training environment, GymFC, for developing intelligent attitude controllers for UAVs and addressed C1: Precision and Accuracy in depth, which identifies if neural networks trained with RL can produce accurate attitude controllers. We placed an emphasis on digital twinning concepts to allow transferability to real hardware. We used GymFC to evaluate the performance of state-of-the-art RL algorithms PPO, TRPO, and DDPG to identify if they are appropriate to synthesize high-precision attitude flight controllers. Our results highlight that (i) RL can train accurate attitude controllers and (ii) that those trained with PPO outperformed a fully tuned PID controller on almost every metric. Although we base our evaluation on results obtained in episodic tasks, we found that trained agents were able to perform exceptionally well also in continuous tasks without retraining (Appendix A). This suggests that training using episodic tasks is sufficient for developing intelligent attitude controllers. The results presented in this work can be considered as a first milestone and a good motivation to further inspect the boundaries of RL for intelligent control. With this premise, we plan to develop our future work along three main avenues. On the one hand, we plan to investigate C2: Robustness and Adaptation and C3: Reward Engineering (Section 1) to harness the true power of RL's ability to adapt and learn in environments with dynamic properties (e.g., wind, variable payload, system damage and failure). On the other hand, we intend to transfer our trained agents onto a real aircraft to evaluate their live performance, including timing and memory analysis of the neural network. This will allow us to define the minimum hardware specifications required to use neural network attitude control. Furthermore, we plan to expand GymFC to support other aircraft, such as fixed -wing aircraft, while continuing to increase the realism of the simulated environment by improving the accuracy of our digital twins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A CONTINUOUS TASK EVALUATION</head><p>In this section, we briefly expand on our findings that show that even if agents are trained through episodic tasks, their performance transfers to continuous tasks without the need for additional training. Figure <ref type="figure">8</ref> shows that an agent trained with Proximal Policy Optimization (PPO) using episodic tasks has exceptional performance when evaluated in a continuous task. Figure <ref type="figure">9</ref> is a closeup of another continuous task sample showing the details of the tracking and corresponding motor output. These results are quite remarkable, as they suggest that training with episodic tasks is sufficient for developing intelligent attitude flight controller systems capable of operating in a continuous environment. In Figure <ref type="figure" target="#fig_8">10</ref>, another continuous task is sampled and the PPO agent is compared to a PID agent. The performance evaluation shows the PPO agent to have a 22% decrease in overall error compared to the PID agent.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Quadcopter rotational movement.</figDesc><graphic coords="4,137.25,83.92,210.64,191.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. RL architecture using the GymFC environment for training intelligent attitude flight controllers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of environment architecture, GymFC. Blue blocks with dashed borders are implementations developed for this work.</figDesc><graphic coords="9,139.94,334.80,206.16,133.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The Iris quadcopter in Gazebo 1m above the ground. The body is transparent to show where the center of mass is linked as a ball joint to the world. Arrows represent the various joints used in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>for instance, roll, pitch, and yaw (see Section 2.1). The final values were m 1 = [-1.0, 0.598, -1.0], m 2 = [-0.927, -0.598, 1.0], m 3 = [1.0, 0.598, 1.0], and last, m 4 = [0.927, -0.598, -1.0]. The mix values and PID sums are then used to compute each motor signal y i according to Equation (6), where f = 1 for no additional throttle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Step response of best-trained RL agents compared to PID. Target angular velocity is Ω * = [2.20, -5.14, -1.81] rad/s, shown by a dashed black line. Error bars ±10% of initial error from Ω * are shown in a dashed red line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Step response and PWM motor signals in microseconds (μs) of the best-trained PPO agent compared to PID. Target angular velocity is Ω * = [2.11, -1.26, 5.00] rad/s, shown by a dashed black line. Error bars ±10% of initial error from Ω * are shown in a dashed red line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Performance of a PPO agent trained with episodic tasks but evaluated using a continuous task for a duration of 60 seconds. The time in seconds at which a new command is issued is randomly sampled from the interval [0.1, 1], and each issued command is maintained for a random duration also sampled from [0.1, 1]. Desired angular velocity is specified by the black line, whereas the red line is the attitude tracked by the agent.</figDesc><graphic coords="18,70.74,112.96,362.68,181.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Response comparison of a PID and PPO agent evaluated in a continuous task environment. The PPO agent, however, is only trained using episodic tasks.</figDesc><graphic coords="19,71.40,83.68,362.32,181.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>RL Performance Evaluation Averages From 3,000 Command Inputs Per Configuration With 95% Confidence</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Success and Failure Results for Considered Algorithms</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>RL Performance Evaluation Compared to PID the Best-Performing Agent</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Transactions on Cyber-Physical Systems, Vol. 3, No. 2, Article 22. Publication date: February 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>BACKGROUNDIn this section, we provide an overview of quadcopter flight dynamics required to understand this work and an introduction to developing flight control systems with RL.ACM Transactions on Cyber-Physical Systems, Vol.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>3, No. 2, Article 22. Publication date: February 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>A reward structured so that r t = 0 if sum( |e t |) &lt; thr eshold; otherwise, r t = -1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank the anonymous reviewers for their comments which helped us improve the quality of this manuscript.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by a grant from the National Science Foundation under awards #1430145, #1414119, and #1718135.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ArduPilot Home Page</title>
		<author>
			<persName><surname>Ardupilot</surname></persName>
		</author>
		<ptr target="http://ardupilot.org/" />
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BetaFlight</title>
		<author>
			<persName><surname>Github</surname></persName>
		</author>
		<ptr target="https://github.com/betaflight/betaflight" />
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">gzserver doesn&apos;t close disconnected sockets</title>
		<ptr target="https://bitbucket.org/osrf/gazebo/issues/2397/gzserver-doesnt-close-disconnected-sockets" />
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
	<note>Open Source Robotics Foundation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Iris QuadCopter</title>
		<author>
			<persName><surname>Copter</surname></persName>
		</author>
		<ptr target="http://www.arducopter.co.uk/iris-quadcopter-uav.html" />
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Protocol Buffers</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/protocol-buffers/" />
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An application of reinforcement learning to aerobatic helicopter flight</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autonomous helicopter control using reinforcement learning policy search methods</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 2001 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>ICRA&apos;01</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1615" to="1620" />
			<pubPlace>Los Alamitos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid parallel neuro-controller for multirotor unmanned aerial vehicle</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bobtsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Guirik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Budko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Budko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 8th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT&apos;16</title>
		<meeting>the 2016 8th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT&apos;16<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design and control of an indoor micro quadrotor</title>
		<author>
			<persName><forename type="first">Samir</forename><surname>Bouabdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierpaolo</forename><surname>Murrieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 2004 IEEE International Conference on Robotics and Automation<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4393" to="4398" />
		</imprint>
	</monogr>
	<note>ICRA&apos;04</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<title level="m">Openai gym</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reinforcement learning and the reward engineering principle</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dewey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 AAAI Spring Symposium Series</title>
		<meeting>the 2014 AAAI Spring Symposium Series</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
	</analytic>
	<monogr>
		<title level="j">OpenAI Baselines. GitHub. Retrieved</title>
		<imprint>
			<date type="published" when="2017-01-20">2017. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Output feedback control of a quadrotor UAV using neural networks</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Dierks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarangapani</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="50" to="66" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An adaptive neuro PID for controlling the altitude of quadcopter robot</title>
		<author>
			<persName><forename type="first">Bahram</forename><surname>Mehdi Fatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Lavi Sefidgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barenji</forename><surname>Vatankhah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 18th International Conference on Methods and Models in Automation and Robotics (MMAR&apos;13</title>
		<meeting>the 2013 18th International Conference on Methods and Models in Automation and Robotics (MMAR&apos;13<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="662" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simulation-based architecture for smart cyber-physical systems</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenz</forename><surname>Belzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Kiermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Till</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Neitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Conference on Autonomic Computing (ICAC&apos;16)</title>
		<meeting>the 2016 IEEE International Conference on Autonomic Computing (ICAC&apos;16)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="374" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">L1 adaptive control for safety-critical systems</title>
		<author>
			<persName><forename type="first">Naira</forename><surname>Hovakimyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Xargay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">M</forename><surname>Gregory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Control Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="54" to="104" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Control of a quadrotor with reinforcement learning</title>
		<author>
			<persName><forename type="first">Jemin</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkyu</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2096" to="2103" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise and the reality gap: The use of simulation in evolutionary robotics</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Jakobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inman</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd European Conference on Advances in Artificial Life</title>
		<meeting>the 3rd European Conference on Advances in Artificial Life</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="704" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep Reinforcement Learning: Pong from Pixels</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://karpathy.github.io/2016/05/31/rl/" />
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Autonomous helicopter flight via reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="799" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">William</forename><surname>Koch</surname></persName>
		</author>
		<ptr target="https://github.com/wil3/gymfc" />
	</analytic>
	<monogr>
		<title level="j">GymFC. GitHub. Retrieved</title>
		<imprint>
			<date type="published" when="2018-01-20">2018. January 20. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Design and use paradigms for Gazebo, an open-source multi-robot simulator</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;04)</title>
		<meeting>the 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;04)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2149" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intelligent control approaches for aircraft applications</title>
		<author>
			<persName><forename type="first">Kalmanje</forename><surname>Krishnakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Gundy-Burlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the JANAFF Interagency Propulsion Committee Meeting</title>
		<meeting>the JANAFF Interagency Propulsion Committee Meeting<address><addrLine>Destin, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feedback linearization vs. adaptive sliding mode control for a quadrotor helicopter</title>
		<author>
			<persName><forename type="first">Daewon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control, Automation and Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Survey of gain-scheduling analysis and design</title>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">J</forename><surname>Leith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Leithead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1001" to="1025" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backstepping control for a quadrotor helicopter</title>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelaziz</forename><surname>Benallegue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3255" to="3260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A reliable system design for nondeterministic adaptive controllers in small UAV autopilots</title>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Ashenayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loyd</forename><forename type="middle">R</forename><surname>Hook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">G</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Hutchins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC&apos;16)</title>
		<meeting>the 2016 IEEE/AIAA 35th Digital Avionics Systems Conference (DASC&apos;16)<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evolving mobile robots in simulated and real environments</title>
		<author>
			<persName><forename type="first">Orazio</forename><surname>Miglino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><forename type="middle">Hautop</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Nolfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="417" to="434" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<title level="m">Playing Atari with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">State-of-the-art intelligent flight control systems in unmanned aerial vehicles</title>
		<author>
			<persName><forename type="first">Fendy</forename><surname>Santoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Garratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenatha</forename><forename type="middle">G</forename><surname>Anavatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="613" to="627" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust neuro-control for a micro quadrotor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kagan</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation</title>
		<meeting>the 12th Annual Conference on Genetic and Evolutionary Computation<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1131" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fundamental limitations and differences of robust and adaptive control</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Feng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 American Control Conference</title>
		<meeting>the 2001 American Control Conference<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4802" to="4807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-agent quadrotor testbed control design: Integral sliding mode vs. reinforcement learning</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lake Waslander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><forename type="middle">J</forename><surname>Tomlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;05</title>
		<meeting>the 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;05<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3712" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Design of Model-Reference Adaptive Control Systems for Aircraft</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Philip</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Yamron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Kezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MA. Reinforcement Learning for UAV Attitude Control</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="1958">1958</date>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MIT Instrumentation Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flight test implementation of a second generation intelligent flight control system</title>
		<author>
			<persName><forename type="first">Peggy</forename><forename type="middle">S</forename><surname>Williams-Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Infotech@Aerospace Conference</title>
		<meeting>the Infotech@Aerospace Conference</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimum settings for automatic controllers</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">B</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ASME</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="759" to="768" />
			<date type="published" when="1942-11">1942. Nov. 1942</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
