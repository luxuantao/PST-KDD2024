<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Named Entity Recognition in Query</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
							<email>guojiafeng@software.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gu</forename><surname>Xu</surname></persName>
							<email>guxu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia Beijing</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
							<email>hangli@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia Beijing</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Named Entity Recognition in Query</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B97E66B8C9E1B14A55540663AA925385</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Query formulation Algorithms</term>
					<term>Experimentation Named Entity Recognition</term>
					<term>Topic Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of Named Entity Recognition in Query (NERQ), which involves detection of the named entity in a given query and classification of the named entity into predefined classes. NERQ is potentially useful in many applications in web search. The paper proposes taking a probabilistic approach to the task using query log data and Latent Dirichlet Allocation. We consider contexts of a named entity (i.e., the remainders of queries after the named entity is removed) as words of a document, and classes of the named entity as topics. The topic model is constructed by a novel and general learning method referred to as WS-LDA (Weakly Supervised Latent Dirichlet Allocation), which employs weakly supervised learning (rather than unsupervised learning) using partially labeled seed entities. Experimental results show that the proposed method based on WS-LDA can accurately perform NERQ, and outperform the baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this paper we address a novel problem in web search, namely Named Entity Recognition in Query (NERQ). In the task given a query we are to detect the named entity within the query and identify the most likely classes of the named entity. Classes of named entities can be, for instance, "Book", "Movie", "Game", and "Music". Given query "harry potter walkthrough", we detect "harry potter" as a named entity and assign "Game" to it as the most likely class, "Movie" and "Book" as less likely classes, and "Music" as unlikely class. This is because the context "walkthrough" strongly indicates that "harry potter" here is more likely to mean the Harry Potter game. (If the query is only "harry potter", then "Book" and "Movie" will be more plausible.)</p><p>NERQ is essentially useful for many applications in web search. According to our analysis, about 71% of search queries contain named entities. Identifying named entities in queries would help us to understand search intents better, and therefore provide better search. For example, in relevance search, we can improve ranking by treating named entity and context separately; in query suggestion, we can generate more relevant suggestions, e.g. "harry potter walkthrough" → "harry potter cheats" (context in the same class) or "halo 3 walkthrough" (entity in the same class).</p><p>As far as we know, there was no previous work on NERQ. Traditionally Named Entity Recognition (NER) is mainly performed on natural language texts <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b8">8]</ref>. Usually a supervised learning approach is exploited and a set of features (e.g., whether "Mr." occurs before the word, or whether the first letter of words is capitalized) is utilized. However, direct application of exiting NER technologies to NERQ would not perform well. This is because queries are usually very short (i.e., 2-3 words on average) and are not necessarily in standard form (e.g., all letters are in lower case), and thus the features are not sufficient for performing accurate NERQ.</p><p>In this paper, we propose a new probabilistic approach to NERQ using query log data. Without loss of generality, a query having one named entity<ref type="foot" target="#foot_0">1</ref> is represented as a triple (e, t, c), where e denotes named entity, t context of e, and c class of e. Note that t can be empty (i.e. no context), e.g. "harry potter". Then the goal of NERQ here becomes to find the triple (e, t, c) for a given query q, which has the largest joint probability Pr(e, t, c). The joint probability is factorized and then estimated by using query log and LDA.</p><p>In the LDA model, contexts of a named entity are represented as words of a document, classes of the named entity are represented as topics of the model. The alignment between model topics and predefined classes needs to be guaranteed. To address this problem, we propose a weakly supervised learning method, referred to as WS-LDA (Weakly Supervised Latent Dirichlet Allocation), which can leverage the weak supervision from humans.</p><p>Our approach is in part inspired by the work <ref type="bibr" target="#b19">[19]</ref>. They proposed a method for acquiring named entities from query log using templates. There are some differences between their work and ours. Our focus is NERQ while theirs is offline query log mining (there is no online prediction in their case). We employ a probabilistic model, while they take a deterministic approach in the sense that they assume that each named entity can only belong to one class.</p><p>Our contribution in this paper lies in the following points. <ref type="bibr" target="#b1">(1)</ref> We have formalized the problem of NERQ. <ref type="bibr" target="#b2">(2)</ref> We have proposed a novel method for conducting NERQ. <ref type="bibr" target="#b3">(3)</ref> We have developed a new topic modeling method with weakly supervised learning, i.e. WS-LDA.</p><p>The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 defines the problem of NERQ and proposes a probabilistic approach to the task. Section 4 describes WS-LDA in details. Experimental results are presented in Section 5. Conclusions are made in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Needless to say, query processing is critically important for web search. Previous work mainly focused on query segmentation, query parsing, query classification, and query log mining. As far as we know, however, there was no work on Named Entity Recognition in Query (NERQ) as defined in this paper.</p><p>Query segmentation separates a query into a number of units <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b25">25]</ref>. However, it does not identify named entities from units and also does not assign class labels to units. Syntactic parsing focuses on identifying linguistic structure of query <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13]</ref>. Query classification falls into two groups:</p><p>(1) classification according to search intent, such as informational, navigational or transactional <ref type="bibr">[7,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b16">16]</ref>; (2) classification according to semantics of query, such as "Shopping" or "Living" <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b1">1]</ref>. In query classification, the whole query is classified and there is no further analysis on the internal structure of query.</p><p>Query log mining is also related to our work, particularly that by Paşca <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23]</ref>. Paşca proposes a method for acquiring named entities in a class from query log. A query is supposed to consist of an instance (named entity) and a template (context). A bootstrapping method is employed to mine instances of a class by utilizing the templates of the class, starting with a small number of seed instances. Their approach is deterministic and it can only work well in the cases in which a named entity belongs to a single class.</p><p>Named Entity Recognition is usually performed on text documents. Early work on NER was based on rules <ref type="bibr" target="#b11">[11]</ref>. Recently machine learning techniques have been applied to NER, including supervised machine learning <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b3">3]</ref>, semisupervised learning <ref type="bibr" target="#b8">[8]</ref> and unsupervised learning <ref type="bibr" target="#b10">[10]</ref>. Features are utilized in these approaches. However, directly applying previous NER approaches to NERQ would not work well, because queries are usually short and not well formed.</p><p>Related work also includes topic modeling. Many topic models have been proposed including PLSI <ref type="bibr" target="#b15">[15]</ref>, LDA <ref type="bibr" target="#b5">[5]</ref>, and their extensions <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b4">4]</ref>. Topic models have been utilized in topic discovery, document classification, citation analysis, and social network analysis. Our work exploits topic modeling in a new application, and is particularly unique in that it trains LDA with a weekly supervised learning method. There are several methods proposed for performing super-vised learning of topic models <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22]</ref>. In WS-LDA, we include weak supervision information as soft constraints in the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR APPROACH TO NERQ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NERQ Problem</head><p>Named Entity Recognition in Query (NERQ) is a task defined as follows. Given a query, we try to detect the named entities within query and categorize the named entities into classes. The classes are from a predefined taxonomy.</p><p>We have conducted a manual analysis on 1,000 unique queries randomly selected from the search log of a commercial web search engine. It indicates that named entities appear very frequently in queries and about 70% of the queries contain named entities. Furthermore, if a named entity occurs in a query, usually only that single named entity occurs and less than 1% of the queries contain two or more named entities. (In this paper, we focus on single-named-entity queries and take the processing of multiple named-entity queries as future work).</p><p>Queries tend to be short (i.e., 2-3 words on average) and not well formed. It makes NERQ a challenging task. In this paper, we propose a probabilistic approach to the problem using query log data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilistic Approach</head><p>A single-named-entity query q can be represented as triples (e, t, c), where e denotes named entity, t denotes the context of e in q, and c denotes the class of e. Note that t is further expressed as α#β, where α and β denote the left and right contexts respectively and # denotes a placeholder for named entity. Either α or β can be empty (e.g. "# walkthrough", "lyrics to #"), or both can be empty (i.e. "#"). For example, for query "harry potter walkthrough" belonging to Game, the associated triple is ("harry potter", "# walkthrough", Game).</p><p>The goal of NERQ is to detect the named entity e in query q, and assign the most likely class label c to e. Therefore, it can be accomplished by finding the triple (e, t, c) * among all possible triples, satisfying: (e, t, c) * = argmax (e,t,c) Pr(q, e, t, c) = argmax (e,t,c) Pr(q|e, t, c) Pr(e, t, c) = argmax (e,t,c)∈G(q) Pr(e, t, c) (</p><p>In Eqn. (1), conditional probability Pr(q|e, t, c) represents how likely query q is generated from triple (e, t, c). Note that given a triple, it will uniquely determine a query. Therefore, for fixed query q and triple (e, t, c), Pr(q|e, t, c) can only be one or zero. That is, there are only two possibilities: either (e, t, c) generates q or (e, t, c) does not generate q. For instance, query "harry potter walkthrough" can be generated by ("harry potter", "# walkthrough", * ), but not ("halo 3", "# walkthrough", * ). We define G(q) as the set containing all possible triples that can generate query q (i.e., Pr(q|e, t, c) equals one). Thus, the triple having largest probability (e, t, c) * must be in G(q). Therefore, to conduct NERQ we only need to calculate the joint probability Pr(e, t, c) for each triple in G(q), which can be further factorized as below:</p><p>Pr(e, t, c) = Pr(e) Pr(c|e) Pr(t|e, c) = Pr(e) Pr(c|e) Pr(t|c)</p><p>In Eqn.</p><p>(2), we assume that Pr(ti|c) = Pr(ti|c, ei), that is, context only depends on class but not specific named entity. This assumption largely reduces the parameter space and thus makes the learning tractable. It is also a reasonable assumption in practice because classes usually share common contexts, e.g., "Music" takes "# lyrics" and "# mp3" as contexts. There are contexts specific to named entities. However, due to data sparseness, one can hardly accurately estimate the probabilities of them.</p><p>The problem then becomes how to estimate Pr(e), Pr(c|e) and Pr(t|c). The number of such probabilities is extremely large, because there are an extremely large number of named entities and contexts. These include variants of named entities like "harry potter 6" and "harry potter and the halfblood prince", and variants of contexts like "# lyrics", "lyrics to #", and even typos "# lyrix".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Topic Model for NERQ</head><p>Suppose there is a training data set available, which contains triples from labeled queries T = {(ei, ti, ci)|i = 1, . . . , N}, where (ei, ti, ci) denotes the "true" triple for query qi and N is the data size. Therefore, the learning problem can be formalized as:</p><formula xml:id="formula_2">max N i=1 Pr(ei, ti, ci)<label>( 3 )</label></formula><p>If each named entity only belongs to one class, we can build the training data T easily (e.g., using the method in <ref type="bibr" target="#b19">[19]</ref>). However, in reality named entities are usually ambiguous, e.g., "harry potter" can belong to classes "Book", "Movie", and "Game". It would be difficult as well as timeconsuming to manually assign class labels to named entities in queries. Therefore, we collect training data T = {(ei, ti)}, and view class label ci as hidden variable. We also know the possible classes of each named entity in training. The learning problem with respect to the new training data T = {(ei, ti)} becomes:</p><formula xml:id="formula_3">max N i=1 Pr(ei, ti)= max N i=1 Pr(ei) c Pr(c|ei)Pr(ti|c) (4)</formula><p>In Eqn. (4), Pr(ei) represents the popularity of named entity ei, Pr(c|ei) represents the likelihood of class c given named entity ei, and Pr(ti|c) represents the likelihood of context ti given class c. The prior probability Pr(ei) can be estimated in different ways, independent of Pr(c|ei) and Pr(ti|c). Suppose it is estimated as Pr(ei), then Eqn. (4) becomes:</p><formula xml:id="formula_4">max N i=1 Pr(ei) N i=1 c Pr(c|ei) Pr(ti|c)<label>( 5 )</label></formula><p>In this way, the learning problem becomes that of learning the probabilities in Eqn. <ref type="bibr" target="#b5">(5)</ref>, which form a topic model. In the topic model, a named entity corresponds to a document, contexts of a named entity correspond to words of the document, classes of a named entity correspond to topics of the model. Without loss of generality, we choose LDA as topic model in this paper. The topic model also has some specialties. The topics (or classes) in the topic model are predefined and the possible topics of each document are given in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>In this section, we explain how to use the topic model and query log to build a NERQ system. The processing consists of two stages, offline training and online prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Offline Training</head><p>The offline training process is a combination of learning algorithm and data mining technique. There are two steps:</p><p>(1) We first select some named entities as seeds, and assign possible classes to each of them. There might be multiple classes for each named entity. Note that the effort in this labeling is limited, since we only need to label a small number of named entities (not queries). Then we scan the query log with the seed named entities and collect all the queries containing them. In this way, we can generate the training data (ei, ti) and learn a topic model with regard to the seed named entities. There is a significant difference between conventional topic modeling and the learning here. First, the hidden topics (or classes) are predefined. Furthermore, the possible topics (classes) of a document (named entity) are given in weak supervision. We propose a method that can conduct weakly supervised learning of topic model, referred to as WS-LDA (Weakly Supervised Latent Dirichlet Allocation). We will introduce the details in the next section. After this step, we obtain the estimated probabilities Pr(c|e) for each seed named entity as well as Pr(t|c) for each class.</p><p>(2) We scan the query log again with the previously learned contexts, collect all the queries containing the contexts, and extract the remainder of these queries as new named entities. (To ensure a high quality extraction, we heuristically make a threshold cut-off in this process). Next, WS-LDA is employed to estimate Pr(c|e) for the newly extracted named entities, with the probabilities Pr(t|c) fixed. The probabilities Pr(e) for newly extracted named entities are also estimated in this process. Specifically, we use the total frequency of queries containing e in the query log to approximate Pr(e). The more frequently named entity e occurs, the larger probability Pr(e) will be.</p><p>In this way, we can estimate all the probabilities we need, that is, Pr(e), Pr(c|e), and Pr(t|c). We create an index for the named entities and the classes, and store the estimated probabilities for efficient online prediction. The detailed algorithm for the offline training process is shown in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Online Prediction</head><p>In online prediction, we try to find the most likely triples in G(q) for a query q. We can generate G(q) by segmenting the query into named entity and context in all possible ways, and labeling segmented named entities with all possible classes. For each triple (e, t, c) in G(q), the joint probability Pr(e, t, c) is then calculated. The triples with highest probabilities are output results for NERQ. The detailed algorithm is shown in Alg. 2. The time complexity of the algorithm is O(kn 2 ), where k denotes number of classes and n denotes number of words in a query. Since both k and n are very small, the prediction can be conducted very efficiently. We skip those queries which do not have named entity and context stored in the index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">WS-LDA</head><p>The learning of topic model in our method for NERQ is a new problem, since the topics in the model are predefined, </p><formula xml:id="formula_5">: initialize IC ← ∅, IE ← ∅ 2: T ← ∅ 3: for all q ∈ Q do 4:</formula><p>for all s ∈ S do 5:</p><p>if (q contains s) then 6:</p><p>t ← RemainderContext(q, s) 7: For readability, we use conventional notations for document processing to describe the topic model. Specifically, contexts become "words", contexts of a named entity form a "document", and classes of named entity correspond to "topics". Suppose that we have named entity "harry potter" with classes "Movie", "Book", and "Game", and find three queries containing "harry potter" in the query log, "harry potter movie", "harry potter walkthrough", and "harry potter review". Then the document with respect to "harry potter" will contain three words, i.e. "# movie", "# walkthrough", and "# review", and the topics of the document will be "Movie", "Book", and "Game". The relationship between query data and document data is summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><formula xml:id="formula_6">T = T ∪</formula><p>Accordingly, we can rewrite the topic model in Eqn. (5) in the following form for better understanding:</p><formula xml:id="formula_7">e {i|e=e i } c Pr(c|ei)Pr(ti|c)<label>( 6 )</label></formula><p>where e denotes a unique named entity in training data. Please note that Pr(ei) is dropped for clarity, and it can be easily integrated into the model. The first product in Eqn. ( <ref type="formula" target="#formula_7">6</ref>) is on all the unique named entities in the training data (document level product), and the second one is on all the contexts of the same named entity (word level product).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Online Prediction Algorithm</head><p>Input: Query q = w1w2 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>We first give the definition of the model in WS-LDA, which is the same as the conventional LDA. Suppose there is a corpus of M documents D = {w1, ..., wM } sharing K topics, and each document is a sequence of N words denoted by w = {w1, ..., wN }. It is assumed that the documents w in the corpus D are generated by the following generative process:  <ref type="figure">,</ref><ref type="figure">β</ref>))dθ <ref type="bibr">(7)</ref> Finally, taking the product of probabilities of documents, we obtain the probability of corpus:</p><formula xml:id="formula_8">p(D|Θ) = M d=1 p(θd|α)( N d n=1 z dn p(z dn |θd )p(w dn |z dn , β))dθd</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weak Supervision</head><p>In NERQ, employing an unsupervised learning method to learn the topic model would not work. This is because the topics (classes) are explicitly predefined in NERQ. In contrast, the topics in a conventional topic model are implicit and are automatically learned. There is no guarantee that the hidden topics learned by unsupervised learning method will be aligned with the predefined topics (classes). Therefore, we need to introduce supervision in the training process of the topic model.</p><p>The supervision is from the manual class labels on each seed named entity. The labels are not exclusive because ambiguity exists in named entities. For example, "harry potter" may have three classes, i.e. "Movie", "Book", and "Game". We only ask human judges to make a judgment on whether a named entity can belong to a class or not. (It would be extremely hard for human judges to decide a probability of a named entity's belonging to a class.) This type of labels is viewed as weak supervision for training. That means, in the terminology of topic modeling, we only assume that a document has high probabilities on labeled topics, but very low probabilities on unlabeled topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Objective Function</head><p>Given document w, the assigned class labels are represented as y = {y1, ..., yK}, where yi takes 1 or 0 when the i-th topic is or is not assigned to the document, and K denotes the number of topics. The weak supervision information will be used as soft constraints in the objective function. WS-LDA tries to maximize the likelihood of data with respect to the model, and at the same time satisfy the soft constraints. The constraints are defined as follows.</p><formula xml:id="formula_9">C(y, Θ) = K i=1 yi zi<label>(8)</label></formula><p>Here we let zi = 1 N N n=1 z i n , where z i n is 1 or 0 when the i-th topic is or is not assigned to the n-th word. That is to say, zi represents the empirical probability of the i-th topic in document w. As we can see, maximizing the soft constraints actually can meet the following two goals at the same time:</p><p>(1) the i-th latent topic is aligned to the i-th predefined class; and (2) the document w is mainly distributed over labeled classes.</p><p>Specifically, the objective function with respect to a document is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(w|y, Θ) = log p(w|Θ) + λC(y, Θ)</head><p>where likelihood function p(w|Θ) and soft constraint function C(y, Θ) are represented as in Eqn. ( <ref type="formula">7</ref>) and ( <ref type="formula" target="#formula_9">8</ref>) respectively, and λ is coefficient. If λ equals 0, WS-LDA learning will degenerate to LDA learning. Finally, substituting Eqn. ( <ref type="formula">7</ref>) and (8) into Eqn. ( <ref type="formula" target="#formula_10">9</ref>) and taking the sum over all documents, we obtain the following total objective function:</p><formula xml:id="formula_11">O(D|Y, Θ) = M d=1 O(wd |yd , Θ) = M d=1 log p(θd|α)( N d n=1 z dn p(z dn |θd )p(w dn |z dn , β))dθd + M d=1 λ K i=1 y di zdi (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Algorithm</head><p>WS-LDA is equivalent to maximizing the objective function in Eqn. <ref type="bibr" target="#b10">(10)</ref>. However, there might be no analytic solution for the problem as in conventional LDA learning. Therefore, we employ a variational method similar to that in <ref type="bibr" target="#b5">[5]</ref> to approximate the posterior distribution of the latent variables. The approximate distribution is characterized by the following variational distribution: q(θ, z|Λ) = q(θ|γ) N n=1 q(zn|φn) where Λ = {γ, φ1:N } are variational parameters. Specifically, γ is Dirichlet parameter and φ1:N are multi-nominal parameters.</p><p>Therefore, the objective function for a single document can be derived as follows.</p><p>O(w|y, Θ) = L(Λ; Θ)+ D(q(θ, z|Λ)||p(θ, z|w, Θ)) <ref type="bibr" target="#b11">(11)</ref> where</p><formula xml:id="formula_12">L(Λ; Θ) = z q(θ, z|Λ) log p(θ, z, w|Θ) q(θ, z|Λ) dθ + z q(θ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>z|Λ)λC(y, Θ)dθ</head><p>Minimizing the KL divergence between the variational posterior probability and the true posterior probability, denoted as D(q(θ, z|Λ)||p(θ, z|w, Θ)), gives a good approximate distribution of p(θ, z|w, Θ). From Eqn. <ref type="bibr" target="#b11">(11)</ref> we can see, this is equivalent to maximizing the lower bound L(Λ; Θ) on the objective function O(w|y, Θ) with respect to Λ which has the form</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(w|y, Θ) ≥ L(Λ; Θ) = Eq[log p(θ|α)]+Eq[log p(z|θ)]+Eq[log p(w|z, β)]</head><p>-Eq[log q(θ)] -Eq[log q(z)] + Eq[λC(y, Θ)]</p><p>Let βiv be p(w v n = 1|z i = 1) for word v. Each of the above terms can be expressed in the following equations ( <ref type="formula">12</ref>)∼( <ref type="formula" target="#formula_15">17</ref>):</p><formula xml:id="formula_13">L(Λ; Θ) = log Γ( K j=1 αj ) -K i=1 log Γ(αi) + K i=1 (αi -1)(Ψ(γi) -Ψ( K j=1 γj)) (12) + N n=1 K i=1 φni(Ψ(γi) -Ψ( K j=1 γj)) (13) + N n=1 K i=1 V v=1 φniw v n log βiv (14) -log Γ( K j=1 γj) + K i=1 log Γ(γi) -K i=1 (γi -1)(Ψ(γi) -Ψ( K j=1 γj )) (<label>15</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">-N n=1 K i=1 φni log φni (16) + λ N N n=1 K i=1 yiφni<label>(17)</label></formula><p>Notice that</p><formula xml:id="formula_16">Eq[zi] = Eq[ 1 N N n=1 z i n ] = 1 N N n=1 Eq[z i n ] = 1 N N n=1</formula><p>φni is used for the derivation of the term <ref type="bibr" target="#b17">(17)</ref>.</p><p>A variational expectation-maximization (EM) algorithm is then employed to estimate the model parameters Θ. E-step:</p><formula xml:id="formula_17">γi = αi + N n=1 φni φni ∝ βiv exp(Ψ(γi) -Ψ( K j=1 γj ) + λ N yi) M-step: βij ∝ M d=1 N d n=1 φ dni w j dn</formula><p>Dirichlet parameter α can be updated in the M-step by using an efficient Newton-Raphson method in which the inverted Hessian can be computed in linear time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prediction</head><p>WS-LDA is also used in prediction. Specifically, we calculate the probability Pr(c|e) for unseen named entities in NERQ. This corresponds to estimating the probability of topic given a new document w with the already estimated model Θ. The estimation is then equivalent to approximating the posterior topic distribution θ of the new document w using the variational inference procedure. Notice this is the same as variational inference in conventional LDA (cf., <ref type="bibr" target="#b5">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head><p>We conducted experiments to verify the effectiveness of NERQ using WS-LDA. In this section, we first introduce the data sets used in experiments. Then we demonstrate the effectiveness of our approach in NERQ. Finally, we compare our method of NEQR using WS-LDA with two baseline methods, the deterministic approach proposed in <ref type="bibr" target="#b19">[19]</ref>, referred to as Determ, and conventional LDA (unsupervised learning), referred to as LDA. Note that although LDA is viewed as a baseline, there was no previous work on using LDA in NERQ. In the experiments λ was set to 1 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Set</head><p>We made use of a real data set consisting of over 6 billion queries, in which the number of unique queries is 930 million. The queries were randomly sampled from the query log of a commercial web search engine.</p><p>Four semantic classes were considered in our experiments, including "Movie", "Game", "Book", and "Music". Based on these classes, 180 named entities were selected from the web sites of Amazon, GameSpot, and Lyrics. Four human annotators labeled the classes of the named entities. If there was a disagreement among the annotators, we took a majority voting. Multiple classes can be assigned to one named entity. The annotated data was further divided into a training set containing 120 named entities and a test set containing 60 named entities.</p><p>The data set has the following characteristics. First, the overlap ratios between classes vary according to class pairs, e.g. the "Movie" and "Game" classes as well as the "Movie" and "Book" classes have higher overlap ratios (≥ 20%). It seems natural because a movie is often adapted from a book with the same title, or a game is often inspired by a movie and named after the movie. Second, the selected classes differ from one another in terms of frequency in query log, e.g. named entities in "Movie" and "Game" classes occur more frequently than in "Book" and "Music" classes.</p><p>Starting from the 120 seed named entities, we trained a WS-LDA model for conducting NERQ. Specifically, we extracted all the possible contexts of seed named entities, and created a WS-LDA model as described in Sections 3 and 4. Finally we obtained 432,304 contexts and indexed about 1.5 million named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NERQ by WS-LDA</head><p>We conducted NERQ on queries from a separate query log, which consists of about 12 million unique queries, and obtained about 0.14 million recognition results. We randomly sampled 400 queries from the recognition results for evaluation. Table <ref type="table" target="#tab_2">2</ref> gives some examples from the data set and Table <ref type="table" target="#tab_3">3</ref> shows the number of queries in the data set grouped by the predicted classes of named entities.</p><p>Each recognition result was then manually labeled as "correct" or "incorrect". A result is viewed as correct if and only if both the detection and classification of the named entity are correct. The performance of NERQ is evaluated in terms of top N accuracy. "Top N accuracy" here is defined in the following way: an algorithm output will be considered "correct" if at least one of top N results is labeled as "correct".</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the accuracy of our NERQ method in terms of top N accuracy. "Overall" stands for the average performance of NERQ over all classes. From Fig. <ref type="figure" target="#fig_0">1</ref> we can see that the overall top 1 accuracy is 81.75% which is reasonably good. When we consider the top 3 results, we can even We further made error analysis on our NERQ results. There were mainly three types of errors. (1) Errors were mainly caused by inaccurate estimation of Pr(e). It seems that the current way of estimating Pr(e) has certain bias, which prefers the segmentation with a shorter named entity. We may reduce such kind of errors by employing a better estimation method. (2) Some contexts were not learned in our approach since they are uncommon. For example, in the query "lyrics for forever by chris brown", "forever by chris brown" was recognized as a "Music" named entity and "lyrics for #" the context. Ideally, "forever" should be recognized as named entity of "Music", and "lyrics for # by chris brown" as context. However, since the context "lyrics for # by chris brown" is quite specific, it was not covered by our learning method. Some of such errors may be eliminated by using more seed named entities. (3) Some queries contained the named entity out of predefined classes. For example, in query "american beauty company", "american beauty" was incorrectly recognized as a movie name. Since "american beauty" was indexed as a movie name and "# company" was as a common context, our NERQ system may occasionally make such kind of errors. We may reduce them when we utilize more classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">WS-LDA v.s. Baselines</head><p>We performed experiments to make comparison between the WS-LDA approach and two baseline methods: Determ and LDA. Note that the main difference of these approaches lies in different assumptions and ways for modeling the relationship between named entity, context, and class.</p><p>Determ learns the contexts of a certain class by simply aggregating all the contexts of named entities belonging to that class. It can perform very well when a named entity only belongs to a single class. In contrast, LDA and WS-LDA take a probabilistic approach and handle the ambiguity of named entities. However, LDA is based on unsupervised learning, and thus cannot ensure the alignment between latent classes and predefined classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Modeling Contexts of Class</head><p>We first compared the learning of contexts of each class between WS-LDA and two baselines. Table <ref type="table" target="#tab_5">4</ref> shows the top ranked contexts of each class according to Pr(t|c) generated by WS-LDA approach and baselines. From the results we can see that the quality of the top ranked contexts generated by Determ is not high. Take the "Movie" class as example, its top ranked contexts are mixed with the contexts of the "Game" class, e.g. "# games" or "free online # games". The reason is that there are many named entities belonging to both "Movie" and "Game" classes. However, Determ ignores the ambiguity and forcibly merges all the contexts of such named entities together. The results indicate that by taking a probabilistic approach, we can improve the quality of the learned contexts. Among the two topic model approaches, WS-LDA achieves better results than LDA, because it can leverage human supervision. Note here manual class alignment is performed to make it a fair comparison with LDA.</p><p>We further looked at the accuracy in ranking named entities by WS-LDA and Determ. In Determ, all the contexts of a class are called the signature of the class. Candidate named entities can then be ranked in each class based on the Jensen-Shannon similarity score <ref type="bibr" target="#b19">[19]</ref> between all the contexts of the named entity and the signature of the class. For comparison, top 250 named entities ranked in each class generated by Determ and WS-LDA were evaluated. Each named entity was manually labeled as "correct" or "incorrect" regarding to class. In total, 2,000 named entities for the four predefined classes were annotated. We used "precision at rank N" <ref type="bibr" target="#b19">[19]</ref> as the measure and obtained the results in Table <ref type="table" target="#tab_6">5</ref>. The results show that WS-LDA can significantly outperform Determ (p-value&lt;0.01). The results demonstrate that WS-LDA can learn the contexts of classes better than Determ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Class Prediction and Convergence Speed</head><p>We next evaluated the accuracies of estimated probabilities Pr(c|e) in LDA and WS-LDA on the test data (i.e., 60 named entities). The overall class likelihood with respect to named entity e is calculated as K i=1 yi Pr(ci|e), where yi takes 1 or 0 when the i-th class is or is not assigned to e. The overall class likelihood measures how consistent the machine predictions are with human labels. Fig. <ref type="figure" target="#fig_4">2(a)</ref> shows the results by LDA and WS-LDA over different runs in testing. The results indicate that WS-LDA significantly outperforms LDA. The average likelihood obtained by LDA is about 34.89, while the average likelihood obtained by WS-LDA is about 53.39. Here to avoid inaccurate manual class alignment in LDA, we enumerate K! possible alignments for LDA in each run and take the highest score as its result. It can be considered as the upper-bound of LDA. As shown in Fig. <ref type="figure" target="#fig_4">2</ref>(a), the performance of LDA is quite unstable. It might be also related to the "local maximum" problem of LDA <ref type="bibr" target="#b5">[5]</ref>. In contrast, WS-LDA model does not seem to suffer from the problem and can constantly produce high   We also found that the convergence speed of training for WS-LDA is much faster than that for LDA. Fig. <ref type="figure" target="#fig_4">2(b)</ref> shows numbers of iterations needed for convergence for the two methods. The average convergence speed of WS-LDA is 3 times faster than that of LDA. It might be due to the regulation from the soft constraint which makes the parameter space much smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Supervision in WS-LDA</head><p>We also tested how the coefficient λ (weight on soft constraints) affects the performance of WS-LDA. We set λ with different values from 0.01 to 100 and ran 10 trials under each setting. Table <ref type="table" target="#tab_4">6</ref> shows the average class likelihood values on the testing set under different values of λ. The results indicate that increasing λ will help WS-LDA to predict class labels more accurately. It demonstrates the necessity of using the supervision in learning and the capability of WS-LDA in utilizing the information. While λ is small, the average class likelihood is unsurprisingly close to that of LDA. Moreover, when λ continues to increase, the convergence speed will decrease and the performance will drop. This is because the supervision is over emphasized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Named Entity Recognition in Query (NERQ) is potentially useful in many applications in web search. We have, for the first time, investigated the problem in this paper, and proposed employing a probabilistic approach to perform the task using query log and a topic model. We have proposed a new weakly supervised learning method for creating the topic model called WS-LDA, in which the topics of a document are assigned. Experimental results indicate that the proposed approach can accurately perform NERQ, and outperforms other baseline methods.</p><p>There are several issues which we plan to address in the future. In this paper, we have verified the effectiveness of our method in experiments in which there are only a small number of classes. We plan to add more classes and conduct the experiments. The proposed method focuses on singlenamed-entity queries. We want to design a more general model to handle more complicated queries.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Offline Training Algorithm Input: Repository of queries Q, set of classes C, set of seed named entities S with their labels Cs, s ∈ S Output: Context set T , class index IC , and named entity index IE Variable: T * = context of named entity * , E = pool of named entities 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Draw topic distribution θ ∼ Dirichlet(α) 2. For each word (a) Draw topic assignment zn ∼ Multinomial(θ) (b) Draw word wn ∼ Multinomial(β zn ), a multinomial distribution conditioned on topic zn Given parameters Θ = {α, β}, we obtain the probability distribution of a document: p(w|Θ) = p(θ|α)( N n=1 zn p(zn|θ)p(wn|zn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NERQ Top N Accuracy on Test Data(%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparisons between WS-LDA and LDA on (a) Overall Class Likelihood on Testing Set, (b) Convergence Speed on Training Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Relationship between Notions</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>t</cell><cell></cell></row><row><cell>8:</cell><cell cols="2">update t's information in Ts</cell></row><row><cell>9:</cell><cell>end if</cell><cell></cell></row><row><cell>10:</cell><cell>end for</cell><cell></cell></row><row><cell cols="2">11: end for</cell><cell></cell></row><row><cell cols="4">12: train topic model WS-LDA over (S, {Ts}s∈S, {Cs}s∈S )</cell></row><row><cell cols="4">13: store learned probabilities {Pr(t|c)}t∈T,c∈C into IC</cell></row><row><cell cols="2">14: E ← ∅</cell><cell></cell></row><row><cell cols="2">15: for all q ∈ Q do</cell><cell></cell></row><row><cell>16:</cell><cell>for all t ∈ T do</cell><cell></cell></row><row><cell>17:</cell><cell cols="2">if (q contains t) then</cell></row><row><cell>18:</cell><cell cols="3">e ← QueryRemainderEntity(q, t)</cell></row><row><cell>19:</cell><cell>E = E ∪ e</cell><cell></cell></row><row><cell>20:</cell><cell cols="2">update t's information in Te</cell></row><row><cell>21:</cell><cell>end if</cell><cell></cell></row><row><cell>22:</cell><cell>end for</cell><cell></cell></row><row><cell cols="2">23: end for</cell><cell></cell></row><row><cell cols="4">24: cut off E to retain high quality named entities</cell></row><row><cell cols="2">25: for all e ∈ E do</cell><cell></cell></row><row><cell>26:</cell><cell>estimate Pr(e) with Te</cell><cell></cell></row><row><cell>27:</cell><cell cols="3">estimate {Pr(c|e)}c∈C with Te and learned WS-LDA</cell></row><row><cell>28:</cell><cell cols="2">store (Pr(e), {Pr(c|e)}c∈C ) in IE</cell></row><row><cell cols="2">29: end for</cell><cell></cell></row><row><cell cols="2">30: return (T, IC , IE )</cell><cell></cell></row><row><cell></cell><cell>Query</cell><cell cols="2">Document Symbol</cell></row><row><cell></cell><cell>Context</cell><cell>Word</cell><cell>wn</cell></row><row><cell></cell><cell cols="2">Named entity Document</cell><cell>w</cell></row><row><cell></cell><cell>Class</cell><cell>Topic</cell><cell>zn</cell></row><row><cell cols="4">and the possible topics of document are given. We propose a</cell></row><row><cell cols="4">new method for learning topic model, WS-LDA (Weakly Su-</cell></row><row><cell cols="4">pervised Latent Dirichlet Allocation). WS-LDA is a general</cell></row><row><cell cols="4">method and can be used in other applications.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. . wn, a set of classes C, context set T , named entity index IE and class index IC</figDesc><table><row><cell cols="2">Output: Top K recognition results R</cell></row><row><cell cols="2">1: initialize R ← ∅</cell></row><row><cell cols="2">2: for i = 1 to n do</cell></row><row><cell>3:</cell><cell>for j = i to n do</cell></row><row><cell>4:</cell><cell>e ← wiwi+1 . . . wj</cell></row><row><cell>5:</cell><cell>t ← w1w2 . . . wi-1#wj+1wj+2 . . . wn</cell></row><row><cell>6:</cell><cell>if (e ∈ IE and t ∈ T ) then</cell></row><row><cell>7:</cell><cell>for all c ∈ C do</cell></row><row><cell>8:</cell><cell>r ← new recognition</cell></row><row><cell>9:</cell><cell>r.triple ← {e, t, c}</cell></row><row><cell>10:</cell><cell>compute Pr(e), Pr(c|e), Pr(t|c) using IE and IC</cell></row><row><cell>11:</cell><cell>r.prob ← Pr(e) Pr(c|e) Pr(t|c)</cell></row><row><cell>12:</cell><cell>R.push(r)</cell></row><row><cell>13:</cell><cell>end for</cell></row><row><cell>14:</cell><cell>end if</cell></row><row><cell>15:</cell><cell>end for</cell></row><row><cell cols="2">16: end for</cell></row><row><cell cols="2">17: sort R by prob</cell></row><row><cell cols="2">18: truncate R to size K</cell></row><row><cell cols="2">19: return R</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Example Queries</head><label>2</label><figDesc></figDesc><table><row><cell>pics of fight club</cell><cell>braveheart quote</cell></row><row><cell>watch gladiator online</cell><cell>american beauty company</cell></row><row><cell>12 angry men characters</cell><cell>mario kart guide</cell></row><row><cell>pc mass effect</cell><cell>crysis mods</cell></row><row><cell>mother teresa images</cell><cell>condemned screenshots</cell></row><row><cell>4 minutes lyric</cell><cell>king kong</cell></row><row><cell>the black swan summary</cell><cell>blackwater novel</cell></row><row><cell>new moon</cell><cell>rehab the song</cell></row><row><cell cols="2">nineteen minutes synopsis umbrella chords</cell></row><row><cell>all summer long video</cell><cell>girlfriend lyrics</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Statistics on Sampled Recognition Results</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">Movie Game Book Music</cell></row><row><cell>Num. of queries</cell><cell>111</cell><cell>108</cell><cell>82</cell><cell>99</cell></row><row><cell cols="5">make the overall accuracy reach 97.5%. Fig. 1 also shows</cell></row><row><cell cols="5">the performances of NERQ in different classes. From the re-</cell></row><row><cell cols="5">sults we can see that our method of NERQ using WS-LDA</cell></row><row><cell>is effective in each class.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 : Average Class Likelihood on Testing Set</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">v.s. Coefficient λ</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Coefficient λ 0.01</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell></row><row><cell>ACL</cell><cell cols="5">20.92 35.92 53.39 53.22 53.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : Comparisons on Learned Contexts of Each Class</head><label>4</label><figDesc></figDesc><table><row><cell>Movie</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Comparisons on Learned Named Entities of Each Class (P@N)</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Movie</cell><cell></cell><cell>Game</cell><cell></cell><cell>Book</cell><cell></cell><cell>Music</cell><cell cols="2">Average-Class</cell></row><row><cell></cell><cell cols="10">Determ WS-LDA Determ WS-LDA Determ WS-LDA Determ WS-LDA Determ WS-LDA</cell></row><row><cell>P@25</cell><cell>0.92</cell><cell>1</cell><cell>0.98</cell><cell>1</cell><cell>0.84</cell><cell>1</cell><cell>0.96</cell><cell>1</cell><cell>0.92</cell><cell>1</cell></row><row><cell>P@50</cell><cell>0.9</cell><cell>1</cell><cell>0.96</cell><cell>1</cell><cell>0.82</cell><cell>1</cell><cell>0.92</cell><cell>1</cell><cell>0.905</cell><cell>1</cell></row><row><cell>P@100</cell><cell>0.85</cell><cell>1</cell><cell>0.93</cell><cell>0.98</cell><cell>0.79</cell><cell>0.98</cell><cell>0.89</cell><cell>1</cell><cell>0.865</cell><cell>0.99</cell></row><row><cell>P@150</cell><cell>0.82</cell><cell>1</cell><cell>0.92</cell><cell>0.953</cell><cell>0.767</cell><cell>0.98</cell><cell>0.833</cell><cell>1</cell><cell>0.835</cell><cell>0.983</cell></row><row><cell>P@250</cell><cell>0.724</cell><cell>0.988</cell><cell>0.896</cell><cell>0.928</cell><cell>0.732</cell><cell>0.968</cell><cell>0.76</cell><cell>0.984</cell><cell>0.778</cell><cell>0.967</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here we only consider queries which contain single named entities.When there are multiple entities appearing in a query, our method can still be applied by viewing the more popular one as "named entity" and the rest as "context".</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic web query classification using labeled and unlabeled training data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Beitzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="581" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning noun phrase query segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nymble: a high-performance learning name-finder</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLC &apos;97</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A maximum entropy approach to named entity recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Borthwick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A taxonomy of web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised models for named entity classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phrase recognition and expansion for short, precision-biased queries based on a query log</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>De Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised named-entity extraction from the web: an experimental study</title>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">University of Sheffield: description of the LaSIE system as used for MUC-6</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MUC6</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study of statistical models for query translation: finding a good unit of translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical query translation models for cross-language information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TALIP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="323" to="359" />
			<date type="published" when="2006">2006</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrating topics and syntax</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic identification of user goals in web search</title>
		<author>
			<persName><forename type="first">U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-label text classification with a mixture model trained by EM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 99 Workshop on Text Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Organizing and searching the world wide web of facts -step two: harnessing the wisdom of the crowds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paşca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of named entities using web search queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paşca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="683" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Query segmentation for web search</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Risvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding user goals in web search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge discovery of multiple-topic document using parametric mixture model with Dirichlet prior</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Acquiring ontological knowledge from query logs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1223" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building bridges for web query classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised query segmentation using generative language models and Wikipedia</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;08</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parametric mixture models for multi-labeled text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
