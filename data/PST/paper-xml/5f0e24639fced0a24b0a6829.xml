<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Lookahead Offset Prefetching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mehran</forename><surname>Shakerinava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Institute for Research in Fundamental Sciences (IPM)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Lookahead Offset Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Offset prefetching has been recently proposed as a lowoverhead yet high-performance approach to eliminate data cache misses or reduce their negative effect. In offset prefetching, whenever a cache block (e.g., A) is requested, the cache block that is distanced by k cache blocks (e.g., A + k) is prefetched, where k is the prefetch offset. This type of data prefetching imposes minimal storage overhead and has been shown quite effective for many important classes of applications.</p><p>In this work, we find that prior proposals for offset prefetching either neglect timeliness or sacrifice miss coverage for timeliness when choosing the prefetch offset. To overcome the deficiencies of prior offset prefetchers, we propose Multi-Lookahead Offset Prefetcher (MLOP), a new mechanism for offset prefetching that considers both miss coverage and timeliness when issuing prefetch requests. MLOP, like prior proposals, evaluates several offsets and allows the qualified offsets to issue prefetch requests; however, unlike them, MLOP considers multiple prefetching lookaheads during the evaluation of prefetch offsets. MLOP uses a lightweight hardware structure, composed of a small storage and a simple logic, to identify the prefetching offsets that could have covered a specific cache miss with various prediction lookaheads. Based on this, MLOP assigns scores to the prefetch offsets and for each lookahead, selects the highest scoring offset for issuing prefetch requests. We evaluate and compare MLOP with various recent state-of-the-art data prefetchers and show that our proposal improves system performance by 30% over a system with no data prefetcher and by 4% over the previous best-performing data prefetcher.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data prefetching has been long proposed and adopted to overcome the performance penalty of long latency cache misses. By predicting the application's future memory accesses and fetching those that are not in the on-chip caches, data prefetchers significantly hide the latency of memory accesses, thereby increasing system performance.</p><p>Traditionally, the ability of data prefetchers at enhancing the performance was the single metric at evaluating data prefetchers. As such, prefetchers have grown in their performance benefits, while other factors such as imposed overheads have been marginalized. However, with the widespread use of multi-and many-core processors, and accordingly, the movement towards lean cores <ref type="bibr" target="#b2">[3]</ref>, computer architects re-design nearly all components, considering low overhead as a major design constraint. One of the components that has recently been targeted for simplification is the data prefetcher. Recent research <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> advocates the use of simple and low-overhead data prefetchers, even if they offer slightly lower performance compared to high-performance but extremely-high-overhead prefetcher designs.</p><p>The research towards lean data prefetchers has culminated in offset prefetching <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Offset prefetching, in fact, is an evolution of stride prefetching, in which, the prefetcher does not try to detect strided streams. Instead, whenever a core requests for a cache block (e.g., A), the offset prefetcher prefetches the cache block that is distanced by k cache lines (e.g., A + k), where k is the prefetch offset. In other words, offset prefetchers do not correlate the accessed address to any specific stream; rather, they treat the addresses individually, and based on the prefetch offset, they issue a prefetch request for every accessed address. Offset prefetchers have been shown to offer significant performance benefits while imposing small storage and logic overheads <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>The initial proposal for offset prefetching, named Sandbox Prefetcher (SP) <ref type="bibr" target="#b15">[16]</ref>, attempts to find offsets that yield accurate prefetch requests. To find such offsets, SP evaluates the prefetching accuracy of several predefined offsets (e.g., -8, -7, . . . , +8) and finally allows offsets whose prefetching accuracy are beyond a certain threshold to issue actual prefetch requests. The later work, named Best-Offset Prefetcher (BOP) <ref type="bibr" target="#b14">[15]</ref> tweaks SP and sets the timeliness as the evaluation metric. BOP is based on the insight that accurate but late prefetch requests do not accelerate the execution of applications as much as timely requests do. Therefore, BOP finds offsets that yield timely prefetch requests in an attempt to have the prefetched blocks ready before the processor actually asks for them.</p><p>In this work, we take another step and propose a novel offset prefetcher. We observe that while the state-of-the-art offset prefetcher is able to generate timely prefetch requests, it loses much opportunity at covering cache misses because of relying on a single best offset and discarding many other appropriate offsets. The state-of-the-art offset prefetcher (BOP) evaluates several offsets and considers the offset that can generate the most timely prefetch requests as the best offset; then, it relies only on this best offset to issue prefetch requests until another offset becomes better, and hence, the new best. In fact, this is a binary classification: the prefetch offsets are considered either as timely offsets or late offsets. After classification, the prefetcher does not allow the socalled late offsets to issue any prefetch requests. However, as we discuss in this paper, there might be many other appropriate offsets that are less timely but are of value in that they can hide a significant fraction of cache miss delays.</p><p>To overcome the deficiencies of prior work, we propose to have a spectrum of timelinesses for various prefetch offsets during their evaluations, rather than binarily classifying them. During the evaluation of various prefetch offsets, we consider multiple lookaheads for every prefetch offset: with which lookahead can an offset cover a specific cache miss? To implement this, we consider several lookaheads for each offset, and assign score values to every offset with every lookahead, individually. Finally, when the time for prefetching comes, we find the best offset for each lookahead and allow it to issue prefetch requests; however, the prefetch requests for smaller lookaheads are prioritized and issued first. By doing so, we ensure that we allow the prefetcher to issue enough prefetch requests (i.e., various prefetch offsets are utilized; high miss coverage) while the timeliness is well considered (i.e., the prefetch requests are ordered). Putting all together, we propose the Multi-Lookahead Offset Prefetcher (MLOP), a novel offset prefetcher, and show that it significantly improves system performance over prior state-of-the-art data prefetchers. Through a detailed evaluation of a set of 57 single-and multi-core workloads, we show that MLOP improves system performance by 30% over a baseline with no data prefetcher and by 4% over prior state-of-the-art data prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposal</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of our proposal. To extract offsets from access patterns, we use an Access Map Table (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMT).</head><p>The AMT keeps track of several recently-accessed addresses, along with a bit-vector for each base address. Each bit in the bit-vector corresponds to a cache block in the neighborhood of the address, indicating whether or not the block has been accessed. For keeping track of recent accesses, this mechanism (i.e., base address plus bit-vector) works better than storing full addresses in terms of storage efficiency, since accesses exhibit significant spatial localities <ref type="foot" target="#foot_0">1</ref> . We size the bit-vectors to embrace 64 bits.</p><p>Like prior proposals <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we consider an evaluation period in which we evaluate several prefetch offsets and choose the qualified ones for issuing prefetch requests later  on <ref type="foot" target="#foot_1">2</ref> <ref type="foot" target="#foot_2">2</ref> . For every offset, we consider multiple levels of score where each level corresponds to a specific lookahead. That is, at the end of the evaluation period, we would have a vector of scores per lookahead level: prefetch offsets at lookahead level one would have their own scores, independent of offset scores at lookahead level two.</p><p>The score of an offset at lookahead level X indicates the number of cases where the offset prefetcher could have prefetched an access, at least X accesses prior to occurrence. For example, the score of offsets at the lookahead level 1 indicates the number of cases where the offset prefetcher could have prefetched any of the futures accesses. As the lookahead level increases (say, at lookahead level 10), we approach the case where the prefetcher has enough time to issue the prefetch request (i.e., the access that we are attempting to prefetch would happen at least 10 accesses in the future); conversely, at lookahead levels close to 1, the prefetcher does not have much time, because the corresponding access would often happen shortly, within a few accesses. We set the number of lookahead levels to 16, efficiently trading off between the imposed overheads (e.g., metadata storage) and performance values.</p><p>To efficiently mitigate the negative effect of all predictable cache misses, we select one best offset from each lookahead level. Then, during the actual prefetching, we allow all selected best offsets to issue prefetch requests. Doing so, we ensure that we choose enough prefetch offsets (i.e., do not suppress many qualified offsets like prior work <ref type="bibr" target="#b14">[15]</ref>), and will cover a significant fraction of cache misses, that are predictable by offset prefetching. To handle the timeliness issue, we try to send the prefetch requests in a way that the application would have sent if there had not been any prefetcher:</p><p>we start from lookahead level 1 (i.e., the accesses that are expected to happen the soonest) and issue the corresponding prefetch requests (using its best offset), then go to the upper level; this process repeats. With this prioritization, we try to hide the latency of all predictable cache misses, as much as possible.</p><p>To update offset scores at lookahead level 1, whenever an access occurs, we find its corresponding bit-vector (using its high-order bits to search the AMT), and then based upon the bit-vector information, we identify the offsets that could have prefetched this access <ref type="foot" target="#foot_3">3</ref> , and accordingly, increase the score of those offsets for the first lookahead level <ref type="foot" target="#foot_4">4</ref> . Finally, we set the bit that corresponds to the currently-accessed block in the AMT.</p><p>To update the score values in lookahead level 2 and above, however, we cannot merely rely on the bit-vector, and we need information about the order of accesses. That is, if we want to evaluate whether an offset could have prefetched an access with a lookahead of 2, we need to know the previous access and exclude it from the evaluation. To enable the evaluation of offsets at lookahead levels higher than 1, up until N , we need to keep the order of the last N -1 accesses within each bit-vector. Therefore, since we consider 16 lookahead levels in our configuration, we hold the last 15 accesses, with the order, within each bit-vector. By keeping track of the last accesses, we can easily exclude some of them from the bit-vector when evaluating offsets in various lookahead levels. For example, when we need to update the offset scores at lookahead level 4, we exclude the bits that correspond to the last three accesses from the bit-vector and then update the score values in the same manner as updating the offset scores at the first lookahead level.</p><p>Through a storage sensitivity analysis, we find that a 256entry (per-core) AMT (?8.47 KB) offers a near-optimal performance improvement. The total storage requirement of the prefetcher, including all metadata structures, is less than 12 KB, well fitting in DPC3's rules<ref type="foot" target="#foot_5">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>We evaluate our proposal in the context of the simulation framework provided with DPC3. We follow the evaluation methodology of the championship and run simulations for all 46 provided single-core traces. Out of 46 provided traces, we exclude two memory-insensitive ones, then create 11 random MIX traces from the other 44 (the MIXes are completely different from each other; no single-core trace repeats in two of the MIXes). For better readability, we only report the simulation results for workloads whose performance is highly affected by the evaluated prefetchers, as well as the average of all simulated workloads. We compare our proposal against prior state-of-the-art data prefetchers: BOP <ref type="bibr" target="#b14">[15]</ref>, ASP <ref type="bibr" target="#b10">[11]</ref>, and SPP <ref type="bibr" target="#b12">[13]</ref>.</p><p>Best-Offset Prefetcher (BOP) <ref type="bibr" target="#b14">[15]</ref> is the state-of-theart offset prefetcher, as well as the winner of DPC2. On each access, BOP tests a single offset to determine whether it would have been able to predict the current access. BOP uses a direct-mapped structure, named Recent Requests (RR) table, to keep track of recently-accessed cache blocks. The size of the RR table is purposely chosen to be small so that old cache blocks are automatically replaced by the information of recently-accessed ones.</p><p>Aggregate Stride Prefetcher (ASP) is a prefetching mechanism proposed in Jain's Ph.D. thesis on "Exploiting Long-Term Behavior for Improved Memory System Performance" <ref type="bibr" target="#b10">[11]</ref>. We include ASP as there are similarities between our mechanism and that of ASP (cf. Section 2). ASP employs a History Buffer to extract the qualified offsets, i.e., the offset that can correctly predict a cache miss, thereby issuing prefetch requests based on that information. Moreover, ASP ignores several recent accesses (e.g., eight), which causes the prefetcher to train on accesses that are (temporally <ref type="bibr" target="#b1">[2]</ref>) further away, and thus, issue timely prefetch requests.</p><p>Signature Path Prefetcher (SPP) <ref type="bibr" target="#b12">[13]</ref> is a recent stateof-the-art prefetcher. SPP works based on the signatures that it creates and associates to various access patterns. The main contribution of SPP is that it adaptively adjusts (increases or decreases) its prefetching degree, trying to issue timely prefetch requests while preventing memory bandwidth pollution. To keep the track of metadata information (e.g., signatures), SPP uses a Signature Table, which directly influences the ability of the prefetcher at keeping the history of accesses and hence predicting future patterns.</p><p>For all prefetching methods, we enlarge metadata tables to the extent that either the performance improvement of the prefetcher plateaus or DPC3's rules are violated. All prefetchers sit in the L1 data cache and are trained by L1-D miss streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Figure <ref type="figure" target="#fig_1">2</ref> and 3 show the miss coverage and performance results, respectively. We report miss coverage results only for single-core programs and performance results for all simulated workloads. Miss coverage is the percentage of cache misses that is covered by the prefetcher. We use Instruction-Per-Clock (IPC) as the performance metric and report the performance improvement of all prefetchers over a system without data prefetcher. MLOP offers the highest miss coverage and performance improvement among the evaluated data prefetchers. On average, MLOP covers 56% of cache misses, which is slightly better than the miss coverage of ASP, the second bestperforming method in this regard <ref type="foot" target="#foot_6">6</ref>  The performance analysis shows that MLOP outperforms the competing prefetching techniques on both single-core and multi-core platforms. On average, MLOP improves performance by 30%, outperforming the second best-performing method (SPP) by 4%.</p><p>Miss coverage and timeliness are the main contributors to MLOP's superior performance improvement. BOP, as discussed in this paper, due to its binary classification, neglects many appropriate prefetch offsets and hence, falls short of covering a significant fraction of cache misses. Moreover, we find that another deficiency of BOP arises from the fact that it updates merely a single offset in each update; whereas, our approach, as well as prior proposals like ASP, use vector operations to efficiently update all offset scores at once. ASP, on the other hand, uses a sequential structure to look up the access maps and shifts the entries to create new access maps. We find that this approach suffers from inaccuracy in that frequent shiftings cause the loss of a lot of useful data. Furthermore, ASP adopts a single global lookahead (eight) for its distance selections and simply multiplies the prefetching offset to increase the prefetching degree, which usually, based upon our observations, results in a high overprediction rate. Corroborating recent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, we find that SPP suffers from the fact that its miss coverage and timeliness is further dependent on the accuracy of its throttling decisions: whenever the throttler makes a wrong prediction, both miss coverage and timeliness of the prefetcher are impaired. MLOP, by considering both miss coverage and timeliness at evaluation and selection of its prefetch offsets, provides the best of both worlds, significantly improving miss coverage and timeliness of prefetching, thereby providing significant performance benefits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The hardware realization of our proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Miss coverage of prefetching techniques. 'Avg SC' stands for the average of all single-core workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Improvement</cell><cell>50% 75% 100%</cell><cell>166%</cell><cell>157%</cell><cell>189%</cell><cell>268%</cell><cell>BOP ASP SPP MLOP</cell></row><row><cell>Performance</cell><cell>0% 25%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Figure 3. Performance comparison of prefetching tech-</cell></row><row><cell cols="7">niques, normalized to a baseline system with no prefetcher.</cell></row><row><cell cols="7">Mix1={mcf_s-472B, gcc_s-1850B, roms_s-1070B, cam4_s-</cell></row><row><cell cols="7">490B}, Mix2={xz_s-2302B, mcf_s-484B, fotonik3d_s-8225B,</cell></row><row><cell cols="7">bwaves_s-1740B}, Mix3={pop2_s-17B, roms_s-1613B,</cell></row><row><cell cols="7">bwaves_s-2609B lbm_s-4268B}, and Mix4={roms_s-1007B,</cell></row><row><cell cols="7">fotonik3d_s-7084B, mcf_s-1554B, xalancbmk_s-165B}. 'Avg</cell></row><row><cell cols="7">SC/MC/All' stands for the average of single-core/multi-</cell></row><row><cell cols="6">core/all workloads.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This mechanism is also employed by pieces of prior work in data prefetching<ref type="bibr" target="#b9">[10]</ref> and even instruction prefetching<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> literature. Address 1 001...1011010</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Address 2 011...0001011 Address n 000...0000010</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The length of the evaluation period is directly tied to the mechanism and components of methods and varies from one method to another. We empirically found that 500 accesses is a suitable evaluation period length for our method.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Every offset, say, k , could have prefetched the currently accessed block, say, A, if the bit corresponding to Ak is set in the bit-vector.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>The operation of increasing the score of appropriate offsets can be done in a single cycle by shifting the bit-vector. Due to space limitation, we do not discuss the implementation of this component, further, and refer the reader to prior work<ref type="bibr" target="#b10">[11]</ref>, where the implementation details of Aggregate Stride Prefetcher (ASP) has been discussed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p><ref type="bibr" target="#b4">5</ref> Note that this storage is chosen to get a near-optimal performance improvement from the prefetcher in the context of DPC3. Our evaluations show that, with storage far below than this level, e.g., 4 KB, MLOP is still able to offer a significant fraction of its optimal performance improvement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Note that the high miss coverage of ASP comes at the cost of huge overpredictions that it produces (?31% overprediction rate more than MLOP; not shown in the results due to space limitations), which causes memory bandwidth pollution, impairing its performance improvement, especially in multi-core substrates where bandwidth is a scarce resource<ref type="bibr" target="#b0">[1</ref></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>, 7, 8].</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Die-Stacked DRAM: Memory, Cache, or Mem-Cache?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08828</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domino Temporal Data Prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Data Delivery for Many-Core Processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurately and Maximally Prefetching Spatial Data Access Patterns with Bingo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third Data Prefetching Championship</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bingo Spatial Data Prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Evaluation of Hardware Data Prefetchers on Server Processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM CSUR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing Writebacks Through In-Cache Displacement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TODAES</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Esmaili-Dokht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04864</idno>
		<title level="m">Scale-Out Processors &amp; Energy Efficiency</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Proactive Instruction Fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Access Map Pattern Matching for Data Cache Prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploiting Long-Term Behavior for Improved Memory System Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Austin, TX, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">B-Fetch: Branch Prediction Directed Prefetching for Chip-Multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kadjo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Path Confidence Based Lookahead Prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<editor>MI-CRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">RDIP: Return-Address-Stack Directed Instruction Prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Best-Offset Hardware Prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sandbox Prefetching: Safe Run-Time Evaluation of Aggressive Prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
