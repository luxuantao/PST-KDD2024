<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Median Filtering Forensics Based on Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiangui</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Z</forename><forename type="middle">Jane</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering Department</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Median Filtering Forensics Based on Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DF4577C6C0C16A14E345CDDEC2EDF00</idno>
					<idno type="DOI">10.1109/LSP.2015.2438008</idno>
					<note type="submission">received March 30, 2015; revised May 22, 2015; accepted May 22, 2015. Date of publication June 01, 2015; date of current version June 04, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks</term>
					<term>deep learning</term>
					<term>hierarchical representations</term>
					<term>median filtering forensics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Median filtering detection has recently drawn much attention in image editing and image anti-forensic techniques. Current image median filtering forensics algorithms mainly extract features manually. To deal with the challenge of detecting median filtering from small-size and compressed image blocks, by taking into account of the properties of median filtering, we propose a median filtering detection method based on convolutional neural networks (CNNs), which can automatically learn and obtain features directly from the image. To our best knowledge, this is the first work of applying CNNs in median filtering image forensics. Unlike conventional CNN models, the first layer of our CNN framework is a filter layer that accepts an image as the input and outputs its median filtering residual (MFR). Then, via alternating convolutional layers and pooling layers to learn hierarchical representations, we obtain multiple features for further classification. We test the proposed method on several experiments. The results show that the proposed method achieves significant performance improvements, especially in the cut-and-paste forgery detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ULTIMEDIA forensics has been an active research area during the last decade. Blind forensics techniques generally utilize statistical fingerprints to verify the authenticity of multimedia data without access to the original source. However such imperceptible fingerprints may be destroyed by various manipulations. Recently more efforts have been made to expose the processing history of digital images, such as filtering <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, re-sampling <ref type="bibr" target="#b5">[6]</ref>, compression <ref type="bibr" target="#b6">[7]</ref>, and contrast enhancement <ref type="bibr" target="#b7">[8]</ref>, since their blind detections are forensically important <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>Widely employed as a popular noise removal and image enhancement tool, median filtering has the properties of nonlinearity and preserving edge information of an image. These characters have been utilized by anti-forensics methods, e.g., removing statistical traces of blocking artifacts left by the JPEG compression <ref type="bibr" target="#b8">[9]</ref>, or destroying linear correlations between adjacent pixels for the purpose of hiding the trace of re-sampling <ref type="bibr" target="#b9">[10]</ref>.</p><p>A number of works have been proposed for median filtering forensics <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Perfect performance was reported when detecting median filtering on uncompressed and large-size images. However, an image is usually saved in a compressed format such as the JPEG format and we could face the scenario that a portion of a filtered image is pasted into the original image. Therefore the detection of median filtering from small blocks of a compressed image is forensically important, and it remains a challenge for median filtering forensics.</p><p>Existing median filtering forensics techniques mainly depend on manually selected features, and the feature extraction and the classification are generally separated and not optimized simultaneously in an iterative way. Furthermore, the performance could degrade due to no exact model of a natural image. To address this concern, in this paper, instead of constructing better artificial features, we plan to automatically learn feature representations jointly with the classification for median filtering forensics.</p><p>Deep learning frameworks are able to learn feature representations and fulfill classification automatically, and also can utilize the classification result to guide the feature extraction via the back propagation algorithm. Motivated by how human brains process information, researchers have explored to train deep multi-layer neural networks, such as Deep Boltzmann Machines <ref type="bibr" target="#b10">[11]</ref>, Deep Auto encoders <ref type="bibr" target="#b11">[12]</ref>, and Convolutional Neural Networks <ref type="bibr" target="#b12">[13]</ref>. These methods have shown impressive performances in artificial intelligence (AI) tasks such as object recognition <ref type="bibr" target="#b12">[13]</ref> and natural language processing <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, the trace in an image left by median filtering may be too weak to be detected by the conventional CNN approach. Therefore, we modify the conventional CNN model by adding a filter layer, and design a specific CNN-based framework for median filtering forensics. Extensive experiments have shown that the proposed method can achieve better detection performance than the state-of-the-art schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE PROPOSED CNN MODEL FOR FORENSICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture of our Modified CNNs</head><p>CNNs automatically learn features and perform the classification. It has deep architectures that consist of multiple levels of non-linear operations. A typical CNN has several types of layers, such as convolutional layers, pooling layers and classification layers. At each convolutional layer, the output feature-map usually combines convolutions with multiple inputs. They can capture local dependencies among neighbor elements. The convolutional outputs from all inputs are then transferred into element-wise non-linearity <ref type="bibr" target="#b14">[15]</ref>. Pooling can reduce the spatial resolution of each feature-map and translates information into more global one. <ref type="bibr" target="#b15">[16]</ref> reported that max pooling can lead to faster convergence and improved generalization while <ref type="bibr" target="#b16">[17]</ref> analyzed the theoretical aspect of feature pooling. Via alternating convolutional layer and pooling layer, the output feature vectors are fed into the classification layer. Finally, the classification layer will output the probability of one sample classified into each class through softmax connection.</p><p>In our preliminary study, we directly employed conventional CNN models <ref type="bibr" target="#b12">[13]</ref> as median filtering forensic models, and they didn't perform well, suggesting that existing CNN models can hardly capture the important statistical forensic properties. In median filtering forensics, since the fingerprint caused by median filtering is heavily affected by image edges and textures, using conventional CNN models directly (i.e., using the raw image pixels as inputs to CNNs) leads to poor performance. Therefore we propose modifying the conventional CNN model by adding a filter layer due to the following intuitive reason: The added filter layer can suppress the interference caused by the presence of image edges and textures, and therefore the trace left by median filtering can be successfully exposed. The proposed framework based on CNNs for median filtering forensics is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Filter Layer: Since in median filtering forensics, using conventional CNN models with the raw image pixels as inputs didn't yield good performances, one additional layer, the filter layer in Fig. <ref type="figure" target="#fig_0">1</ref>, is added to the conventional model. Through this filter layer, the median filtering residual (MFR) of an image is obtained. Then the output MFR is fed into conventional networks. The filter layer is important in the proposed method since it can suppress the interference caused by image edges and textures, as shown in <ref type="bibr" target="#b0">[1]</ref>. With eliminating/suppressing the interference of irrelevant information (e.g., image edges and textures), the trace left by median filtering can be investigated.</p><p>The MFR is defined as follows. Applying the median filtering window on a test image and obtain the output image . The MFR is:</p><p>(1)</p><p>where is chosen to be 3 in our work, is original pixel value at point ( ), is median filter value of and means the MFR, which is the difference between and . Convolutional Layer: A conventional convolutional layer consists of two operations: convolution and non-linearity. The response of a convolutional layer is called feature map. Actually, each feature map is a particular feature representation of the input in a certain area. The convolution operation can be denoted as <ref type="bibr" target="#b1">(2)</ref> where denotes convolution, is the -th output map in layer , (also called weight) is the trainable convolutional kernel connecting the -th output map in layer and the -th output map in layer , is the trainable bias parameter for the -th output map in layer . The convolution operation main includes the theory of receptive field and shared weights. Receptive field means each low level feature will be computed from only a subset of the input, it controls the numbers of pixels in connection. Additionally, the share of parameters reduces the number of free variables, hence increases the generalization performance of the network <ref type="bibr" target="#b17">[18]</ref>. Following the convolution, the non-linearity operation is obtained by applying an element-wise non-linear activation function (sigmoid, tanh, etc.) to each element of feature maps. The Rectified Linear Units (ReLUs) is used in our work because it can lead to fast convergence in the performance of large models trained on large datasets <ref type="bibr" target="#b18">[19]</ref>. Based on equation (2), the operation is expressed as <ref type="bibr" target="#b2">(3)</ref> where (</p><p>) means the pixel index in the feature map, and stands for the input patch centred at location ( ). Pooling Layer: After obtaining feature maps using convolution, we can use all extracted features for classification. However this can be computationally challenging and prone to overfitting. Thus only the mean (or max) value of a particular feature over a region of the image is calculated. The aggregation operation is called pooling. Average pooling and max pooling are two typical pooling methods, which propagate the average and the maximum value within the local region to the next layer respectively. The loss of spatial information is translated to an increasing number of higher level feature representations <ref type="bibr" target="#b15">[16]</ref>.</p><p>Classification Layer: In general, the classification layer consists of a few fully connected layers. When the learned features pass through the first or two fully connected layers, they will be fed to the top layer of the CNNs, where a softmax activation function is used for classification. The back propagation algorithm is used to train the CNN. As described in <ref type="bibr" target="#b12">[13]</ref>, the weights and the bias can be renewed adaptively in the convolutional and fully connected layers following the error propagation procedure. In this way, the classification result can be fed back to guide the feature extraction automatically and the learning mechanism can be established. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameters and Settings of Our Modified CNNs</head><p>The framework of the proposed model is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, where we describe the detailed settings of the architecture, and it is a supplementary explanation to the model in Fig. <ref type="figure" target="#fig_0">1</ref>. The architecture contains nine layers. The first layer is a filter layer, the second to the sixth are convolutional layers, the last three are fully-connected layers. In Fig. <ref type="figure" target="#fig_1">2</ref>, the feature maps and kernels are marked in green and red respectively. Pooling layers will be explained in the next paragraph.</p><p>In this work, we address the challenge of detecting median filtering from a small-sized and compressed image block. We consider two sizes of an input image, i.e. and pixels. For the sake of brevity, we only explain one size choice in detail. Let us take a gray scale image of size as the input to the architecture shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Firstly, the filter layer gets the MFR of an image. Then the first convolutional layer convolves them with 128 kernels of size . The size of the output (C1) is , which means the number of feature maps is 128 and the resolution of feature maps is " ". Then the second convolutional layer takes the output of the first layer (C1) as the input and filters it with 256 kernels of size . The third, fourth, and fifth convolution layers apply convolutions with 384 kernels of size , 384 kernels of size , 256 kernels of size respectively. The Rectified Linear Units (ReLUs) is applied to the output of every convolutional layer. Meanwhile, the first, second, and fifth convolutional layers are followed by an overlapping max pooling operation with window size and step size 2, which operate on each feature map in the corresponding convolutional layer, and lead to the same number of feature maps with the decreasing spatial resolution. Each of the fully-connected layers (F1 and F2 in Fig. <ref type="figure" target="#fig_1">2</ref>) has 5120 neurons. In both fully-connected layers (F1 and F2), a recently-introduced technique, i.e., "dropout" <ref type="bibr" target="#b18">[19]</ref>, is used. The last fully connected layer (F3) has two neurons. Its output is fed to a two-way softmax.</p><p>When the size of an input image is pixels, the only difference of the architecture settings is there is no max pooling layer followed the C1 convolutinal layer. Other settings remain the same as in the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>To evaluate the performance of the proposed model and compare its performance with other methods, we test on a com-posite image database containing 15352 images. These images are from five widely used image databases: the BOSSbase 1.01 <ref type="bibr" target="#b19">[20]</ref>, the UCID database <ref type="bibr" target="#b20">[21]</ref>, the BOSS RAW database <ref type="bibr" target="#b21">[22]</ref>, the Dresden Image Database <ref type="bibr" target="#b22">[23]</ref> and the NRCS Photo Gallery database <ref type="bibr" target="#b23">[24]</ref>. Each of the 4 databases contributes 1338 images, and BOSSbase database contributes 10000 images. All images are converted to gray-scale images before any further processing. Each image from the original composite database belongs to the negative class and its median filtered version belongs to the positive class. The training set contains half number of images in each class, while the other half of images compose the testing set. Detection accuracy ( ) is used to evaluate the performance: <ref type="bibr" target="#b3">(4)</ref> where is the number of correctly predicted samples and is the number of total testing images. We compare the proposed model with existing works: the AR method <ref type="bibr" target="#b0">[1]</ref>, median filter feature (MFF) method <ref type="bibr" target="#b1">[2]</ref> and GLF method <ref type="bibr" target="#b3">[4]</ref>. We perform SVM training and testing for the three conventional methods. For the proposed model, all experiments are conducted on GPU using the programming language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detecting Median Filtering from Small and JPEG Compressed Image Blocks</head><p>We first crop image blocks with size of and from the center of a full-resolution image, and then build a corresponding training set and testing set.</p><p>median filtering (MF3 in short form) and median filtering (MF5 in short form) are considered in our experiments. The detection accuracy results are reported in Table <ref type="table" target="#tab_0">I</ref>. "JPEG_70" denotes that the image without median filtered but JPEG compressed with quality factor (QF) of 70, "</p><p>" denotes that the image with composite operation of median filtering and JPEG compression with QF 70. It is noted that the proposed model performs the best in all cases. Considering that the detection accuracy of the proposed model is about 1%-8% better than that of the state-of-the-art methods in different cases, we believe that the deep learning feature representations are effective.</p><p>It is clear that the filter layer for obtaining MFR is important in the proposed CNN model. The experimental results in Table <ref type="table" target="#tab_1">II</ref> show that, without the filter layer, i.e., the image pixels are used as the input to the layer C1 directly, the model cannot    <ref type="table" target="#tab_1">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cut and Paste Forgery Detection</head><p>The ability to detect median filtering in low-resolution images and image windows is essential for detecting forgeries when a portion of a median filtered image is inserted into a non-median filtered image. An example of cut-and-paste image forgery and the corresponding forensic detection results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Fig. <ref type="figure" target="#fig_2">3(a)</ref> shows the median filtered image from which an object (the boat) was cut. Fig. <ref type="figure" target="#fig_2">3(b)</ref> shows the unaltered image into which the cut object was pasted. Fig. <ref type="figure" target="#fig_2">3(c</ref>) shows the composite image, which was JPEG compressed using QF 90. In order to detect the forgery, the composite image was first segmented into pixel blocks, and then each block was tested for evidence of locally applied median filtering. In this example, each detection method was trained on corresponding training images, i.e. the JPEG 90 compressed images with size of . Blocks corresponding to median filtering detections were boxed and outlined. Fig. <ref type="figure" target="#fig_2">3(d)</ref> shows the result of blockwise detections on the composite image using our proposed CNNs method. Fig. <ref type="figure" target="#fig_2">3</ref>(e) shows the result using the GLF method in <ref type="bibr" target="#b3">[4]</ref>. Fig. <ref type="figure" target="#fig_2">3</ref>(f) shows the result with the AR method in <ref type="bibr" target="#b0">[1]</ref>. In Fig. <ref type="figure" target="#fig_2">3</ref>, the detect blocks of the boat (the true positives) are outlined in green, and other detected blocks outside of the boat (the false alarms) are marked in red. It is clear that the proposed method achieves high detection rate with the lowest false alarm rate. This example illustrates that the proposed CNN-based method outperforms the three state-of-the-art approaches in the cut-andpaste forgery detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we propose a median filtering forensic method based on deep learning. The contributions are outlined as follows: Different from exiting conventional median forensics techniques, the feature extraction and classification steps are unified in a modified CNN-based model with adding a filter layer, and hierarchical feature representations are learned; Using feature representations learned automatically from a deep learning model, we can achieve better detection accuracy results when compared with the state-of-art methods using handcrafted features. We have demonstrated that the proposed CNN-based method can detect median filtering in small and JPEG compressed image blocks and is able to identify cut-and-paste forgeries well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed CNN for median filtering forensic.</figDesc><graphic coords="2,40.98,67.14,249.00,82.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of the proposed method.</figDesc><graphic coords="3,87.00,63.12,415.02,94.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A cut and paste forgery detection example, showing (a) the median filtered image from which an object is cut; (b) the unaltered image into which the cut object is pasted; (c) the composite image which is JPEG compressed using a quality factor of 90. The detected blocks of boat (the true positives) are marked in green, and other detected blocks outside of the boat (the false alarms) are marked in red;.(d) using the proposed model; (e) using the GLF method<ref type="bibr" target="#b3">[4]</ref>; and (f) using the AR method<ref type="bibr" target="#b0">[1]</ref>.</figDesc><graphic coords="4,40.02,350.16,249.96,206.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETECTION</head><label>I</label><figDesc>ACCURACY (%) FOR MEDIAN FILTERING DETECTION AGAINST JPEG COMPRESSION. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DETECTION</head><label>II</label><figDesc>ACCURACY (%) WITH OR WITHOUT MFR</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Science Foundation of China under Grants 61379155, U1135001, and 61332012, the 973 Program under Grant 2011CB302204, and the NSF of Guangdong province under Grant s2013020012788. The associate editor coordinating the review of this manuscript and approving it for publication was Prof.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust median filtering forensics using an autoregressive model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1456" to="1468" />
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind forensics of median filtering in digital images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1335" to="1345" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On detection of median filtering in digital images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Electron. Imaging, Media Forensics, Security II</title>
		<meeting>SPIE, Electron. Imaging, Media Forensics, Security II</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">7541</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blind median filtering detection using statistics in difference domain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inf. Hiding</title>
		<meeting>Inf. Hiding<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Forensic detection of median filtering in digital images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exposing digital forgeries by detecting traces of resampling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="758" to="767" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">JPEG error analysis and its applications to digital image forensics</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forensic estimation and reconstruction of a contrast enhancement mapping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1698" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anti-forensics of digital image compression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1050" to="1065" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hiding traces of resampling in digital images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bohme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="592" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition, in</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Mach. Learn</title>
		<meeting>25th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolutionary design of artificial neural networks with different nodes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf</title>
		<meeting>IEEE Int. Conf</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="670" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Neural Netw</title>
		<meeting>Int. Conf. Artif. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recogn. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recogn. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for image processing: An application in robot vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ghidary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AI 2003: Advances in Artif. Intell</title>
		<meeting>AI 2003: Advances in Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="641" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Break our steganographic system:The ins and outs of organizing boss</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pevny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Hiding</title>
		<imprint>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UCID-An uncompressed color image database</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://exile.felk.cvut.cz/boss/BOSSFinal/index.php?mode=VIEW&amp;tmpl=materials" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dresden image database for benchmarking digital image forensics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bohme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp</title>
		<meeting>ACM Symp</meeting>
		<imprint>
			<date type="published" when="2010-03">Mar. 2010</date>
			<biblScope unit="page" from="22" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Natural resources conservation service photo gallery</title>
		<ptr target="http://photogallery.nrcs.usda.gov" />
		<imprint>
			<publisher>United States Department of Agriculture</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
