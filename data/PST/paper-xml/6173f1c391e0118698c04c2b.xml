<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-30">30 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haozhe</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution" key="instit1">The Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
							<email>jjia@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution" key="instit1">The Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution" key="instit1">The Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yishun</forename><surname>Dou</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">HiSilicon Company</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Duan</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">HiSilicon Company</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingshan</forename><surname>Deng</surname></persName>
							<email>dengqingshan@hisilicon.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">HiSilicon Company</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Imitating Arbitrary Talking Style for Realistic Audio-Driven Talking Face Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-30">30 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475280</idno>
					<idno type="arXiv">arXiv:2111.00203v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>photorealistic talking face</term>
					<term>talking style</term>
					<term>style imitation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People talk with diversified styles. For one piece of speech, different talking styles exhibit significant differences in the facial and head pose movements. For example, the "excited" style usually talks with the mouth wide open, while the "solemn" style is more standardized and seldomly exhibits exaggerated motions. Due to such huge differences between different styles, it is necessary to incorporate the talking style into audio-driven talking face synthesis framework. In this paper, we propose to inject style into the talking face synthesis framework through imitating arbitrary talking style of the particular reference video. Specifically, we systematically investigate talking styles with our collected Ted-HD dataset and construct style codes as several statistics of 3D morphable model (3DMM) parameters. Afterwards, we devise a latent-style-fusion (LSF) model to synthesize stylized talking faces by imitating talking styles from the style codes. We emphasize the following novel characteristics of our framework: (1) It doesn't require any annotation of the style, the talking style is learned in an unsupervised manner from talking videos in the wild. (2) It can imitate arbitrary styles from arbitrary videos, and the style codes can also be interpolated to generate new styles. Extensive experiments demonstrate that the proposed framework has the ability to synthesize more natural and expressive talking styles compared with baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Animation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>/sÉª/ /gËˆ/ /fÉª/ /kÉ™n/ /t/ /nÉª/ Excited Solemn Audio Figure <ref type="figure">1</ref>: Different talking styles have significant differences in facial and head pose movements when pronouncing the word "significant".</p><p>The talking face synthesis is an eagerly anticipated technique for several applications, such as filmmaking, teleconference, virtual/mix reality, and human-computer interaction. One of the key essences behind the talking face synthesis is the stylization of facial and head pose movements. Different from the talking emotion which reflects in the short-term facial motions, the talking style is a crucial factor which affects long-term facial and head pose movements. People usually talk with diversified talking styles such as "excited", "solemn", "communicational", "storytelling", et al. Given one piece of speech, different talking styles exhibit significant differences in the facial and head pose movements. For example, as shown in Figure <ref type="figure">1</ref>, people with the "excited" style usually talk aloud, and thus the facial movement of the mouth wide open occurs frequently. Meanwhile, the "solemn" talking style usually appears in formal occasions, and thus the exaggerated motion seldom occurs. Considering such huge differences between different styles, in order to synthesize diversified and realistic talking faces with respect to one piece of speech, it is necessary to incorporate talking style into the audio-driven talking face synthesis framework. Previous efforts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> have shown the rationality of synthesizing stylized talking faces. Yi et al. <ref type="bibr" target="#b35">[36]</ref> proposed Memory Augmented GAN model to synthesize stylized talking face with the wild training data. Cudeiro et al. <ref type="bibr" target="#b9">[10]</ref> proposed Voice Operated Character Animation (VOCA) model to learn the identity-wise talking style. The VOCA model captures the talking style of each identity by injecting the one-hot identity vector into the audio-motion predicting network. Overall, these methods have the following two disadvantages which constrain the expressiveness of the synthesized talking styles: (1) These methods assume that each identity has only one talking style. However, in the real scenario, the talking style is only relatively stable inside one video clip, one person can talk with significantly different styles among different video clips. (2) These methods require substantial labour to collect adequate synchronized audio-visual data for each identity, which is unavailable for the wild scenarios.</p><p>To address the aforementioned problems, we propose to synthesize stylized talking faces by imitating styles from arbitrary clips of videos. With such motivation, the talking videos with stable and diversified talking styles are in need. Therefore, we collect the Ted-HD dataset which has 834 clips of videos with 60 identities. Each video clip has an average length of 23.5 seconds. The talking styles of the Ted-HD dataset are diversified, and the talking style inside each video clip is stable.</p><p>Based on the constructed dataset, we devise a two stage talking face synthesis framework as shown in Figure <ref type="figure" target="#fig_0">2</ref>. The first stage imitates talking styles from arbitrary videos and synthesizes 3D talking faces according to the driven speech. Afterwards, in the second stage, we render the 3D face model photo-realistically from one static portrait of the speaker. Overall, the key idea of our framework is to construct style codes from video clips in the wild, and then synthesize talking faces by imitating the talking styles from the constructed style codes. Specifically, for the style codes construction, we conduct exhaustive observations on the Ted-HD dataset, which on one hand verify that even one identity has multiple talking styles, on the other hand find that the talking style is closely related to the variance of facial and head pose movements inside each video. With our observations, we define the style codes as several interpretable statistics of 3D morphable model (3DMM) <ref type="bibr" target="#b1">[2]</ref> parameters. Having obtained the style codes of each talking video, we devise a latent-style-fusion (LSF) model to synthesize stylized 3D talking faces by imitating talking styles from the style codes. Detailedly, the LSF model first dropouts <ref type="bibr" target="#b26">[27]</ref> information from the audio stream to prevent the audio from dominating the synthesis process. Further, the LSF model fuses the style codes with the latent audio representation frame-by-frame to synthesize 3D talking face with corresponding talking style. The overall implementation of the LSF model is simple but effective. Our model not only circumvents the annotation for talking styles and avoids collecting substantial training data for each identity, but also enables new talking style generation.</p><p>With our proposed framework, talking faces with diversified styles can thus be synthesized. We perform experiments on the Ted-HD dataset for evaluation. Compared with baseline methods, our framework synthesizes more expressive and diversified talking styles. We conduct extensive user studies to investigate the facial motion naturalness and audio-visual synchronization. With the mean opinion score (MOS) of 20 participants, our framework outperforms baseline methods by 0.67 on average in terms of facial motion naturalness and 0.11 on average in terms of audio-visual synchronization.</p><p>To summarize, our contributions are summarized as three-fold: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Talking face synthesis has received significant attention in previous literatures. Related work in this area can be grouped into two categories: the unimodal talking face synthesis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> and the multimodal talking face synthesis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>For one piece of driven speech, the unimodal talking face synthesis generates unique motion, while the multimodal talking face synthesis generates diversified facial and head pose movements. Most of the prior works focused on the unimodal talking face synthesis. Karras et al. <ref type="bibr" target="#b20">[21]</ref> proposed to synthesize 3D talking face with driven audio and emotion state. Suwajanakorn et al. <ref type="bibr" target="#b27">[28]</ref> synthesized high quality videos of talking Obama through hours of Obama's weekly address footage. Since that Suwajanakorn's method requires hours of data for each identity, several methods <ref type="bibr">[4-6, 24, 29, 37, 39, 41]</ref> proposed to simultaneously reduce the required duration of training data and guarantee the photorealism of the synthesized video. The ATVG framework proposed by Chen et al. <ref type="bibr" target="#b5">[6]</ref> and the DAVS framework proposed by Zhou et al. <ref type="bibr" target="#b38">[39]</ref> synthesize talking face with only one image. Despite that these unimodal talking face synthesis methods can synthesize photo-realistic videos, the lack of style results in the stiffness of the synthesized results.</p><p>To synthesize diversified facial and head pose movements, some recent literatures addressed the multimodal talking face synthesis. Wang et al. <ref type="bibr" target="#b31">[32]</ref> and Eskimez et al. <ref type="bibr" target="#b12">[13]</ref> achieved multimodal synthesis through incorporating emotion condition vector, which enables the generation of diversified facial expressions. However, the synthesized results of these methods still lack in personality because of the overlook of talking style. To address this issue, some methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> proposed to incorporate talking style into the synthesis framework. Yi et al. <ref type="bibr" target="#b35">[36]</ref> proposed Memory-Augmented GAN model to synthesize stylized talking face with wild training data. However, only the head pose is synthesized with multiple styles, while the facial movements still lack personality. Further, Cudeiro et al. <ref type="bibr" target="#b9">[10]</ref> proposed the Voice Operated Character Animation (VOCA) model to learn the identity-wise talking style. The VOCA model injects one-hot identity vector into the audio-motion predicting network, leading to discriminative styles of facial and head pose movements. However, the VOCA model on one hand requires substantial data for each identity, on the other hand forces one identity to have only one talking style, limiting its capacity on synthesizing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>In this paper, we propose a two-stage talking face synthesis framework which synthesizes stylized talking videos with the following three inputs: one static portrait of the speaker, the driven audio and the style reference video. We formalize the first stage of the framework as the 3D talking face synthesis stage and the second stage as the photorealistic render stage. Between two stages, we apply the 3DMM face model <ref type="bibr" target="#b1">[2]</ref> as a crucial bridge. Therefore, before formally defining the two stages, we firstly give a brief introduction of the face model we use.</p><p>We leverage the 3DMM face model to represent each 3D face. With 3DMM, the face shape S is represented as an affine model of facial expression and facial identity:</p><formula xml:id="formula_0">S = S(ğ›¼, ğ›½) = S + B ğ‘–ğ‘‘ ğ›¼ + B ğ‘’ğ‘¥ğ‘ ğ›½,<label>(1)</label></formula><p>where S âˆˆ R ğ‘ Ã—3 is the average face shape; N is the number of the vertexes in the face model; B ğ‘–ğ‘‘ and B ğ‘’ğ‘¥ğ‘ are the PCA bases of identity and expression; ğ›¼ and ğ›½ are the identity parameters and the expression parameters. Following deng et al. <ref type="bibr" target="#b11">[12]</ref>, we adopt the 2009 Basel Face Model <ref type="bibr" target="#b22">[23]</ref> for S, B ğ‘–ğ‘‘ , and use expression bases B ğ‘’ğ‘¥ğ‘ of Guo et al. <ref type="bibr" target="#b13">[14]</ref> built from Facewarehouse <ref type="bibr" target="#b2">[3]</ref>, resulting in</p><formula xml:id="formula_1">ğ›¼ âˆˆ R 80 , ğ›½ âˆˆ R 64 .</formula><p>Afterwards, the 3D face shape is projected on the 2D plane according to the head pose and translation ğ‘ âˆˆ R 7 , where 4 elements represent the pose quaternion and 3 elements represent the translation. Overall, the parameter set (ğ›¼, ğ›½, ğ‘) controls the appearance of each face. In our framework, the facial movements are the time series of parameter ğ›½, which we denote as ğ›½ (ğ‘¡), while the head pose movements are the time series of parameter ğ‘, which we denote as ğ‘ (ğ‘¡). With ğ›½ (ğ‘¡) and ğ‘ (ğ‘¡), we formalize the two stages of our framework as follows.</p><p>3D Talking Face Synthesis Stage. In this stage, given driven audio X ğ‘ , the facial and head pose movements of style reference video ğ›½ ğ‘ ğ‘¡ ğ‘¦ (ğ‘¡), ğ‘ ğ‘ ğ‘¡ ğ‘¦ (ğ‘¡), we aim to generate corresponding facial and head pose movements ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡), ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡).</p><p>Photorealistic Render Stage. In this stage, given the predicted movements ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡), ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and the input portrait X ğ‘ , we aim to generate photorealistic videos Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TALKING STYLE OBSERVATION</head><p>In this section, we systematically investigate how different talking styles reflect in the facial and head pose movements ğ›½ (ğ‘¡), ğ‘ (ğ‘¡). Afterwards, we formally define interpretable style codes for each video based on our observation.</p><p>In order to observe talking styles of each video, we should firstly collect a suitable dataset for observation. The video for the style observation requires the following characteristics: (1) It should be of high resolution, (2) it should contain natural and expressive facial and head pose movements, (3) each clip of video cannot be too short, or the talking style can hardly be observed, (4) the talking style should both be stable inside the clip and be diversified across different clips, <ref type="bibr" target="#b4">(5)</ref> the camera pose and location should be static inside each clip, otherwise the head pose parameters will be influenced by the camera movements. Considering the characteristics  above, current publicly available wild datasets like VoxCeleb2 <ref type="bibr" target="#b6">[7]</ref> and LRS3 <ref type="bibr" target="#b0">[1]</ref> are much too noisy and thus do not meet the requirements. Meanwhile, several in-lab datasets like MEAD <ref type="bibr" target="#b32">[33]</ref> and GRID <ref type="bibr" target="#b8">[9]</ref> do not have natural facial expressions and head poses, which dissatisfy the requirements either.</p><formula xml:id="formula_2">+(#) ,(#) '(â‹…) )(â‹…) '( -(â‹…) -# ) )( -(â‹…) -# ) 0.0 0.</formula><p>To address this issue, we manually collect a wild dataset Ted-HD which is suitable for style observation and further style synthesis. The Ted-HD dataset selects several speech videos from Ted website. Each video in the dataset has one person giving a speech, which focuses on the facial part of each person and is of high resolution. We cut each video into several clips according to the scene change. Totally, Ted-HD dataset contains 834 clips of videos with 60 identities. The average length of each clip is 23.5 seconds, and the total duration of the dataset is 6 hours. The talking style of these videos are diversified across different clips. Even for the same identity, there could be different talking styles.</p><p>Having obtained the dataset, for each video, we reconstruct the facial and head pose movements ğ›½ (ğ‘¡), ğ‘ (ğ‘¡). We conduct exhaustive data observations on the correlation between talking style and ğ›½ (ğ‘¡), ğ‘ (ğ‘¡), with the aim of answering the following questions:</p><p>â€¢ Q1: Whether one identity has multiple talking styles.</p><p>â€¢ Q2: How the talking style is reflected in time series ğ›½ (ğ‘¡), ğ‘ (ğ‘¡).</p><p>To answer the Q1, we verify the talking style diversity of each identity through user study with A/B test. Specifically: we first randomly construct 100 triplets, each triplet contains two talking videos ğ‘£ 1 , ğ‘£ 2 from the same identity and one talking video ğ‘£ 3 from the other identity. Next, we reconstruct ğ›½ (ğ‘¡), ğ‘ (ğ‘¡) of ğ‘£ 1 , ğ‘£ 2 , ğ‘£ 3 , retarget ğ›½ (ğ‘¡), ğ‘ (ğ‘¡) to the same identity and render the retargeted faces to videos. The retargeted videos are signified as ğ‘£ Since that the talking style is not consistent for each identity, formulating style codes is necessary. Therefore, in Q2, we conduct experiments to find how talking style is reflected in time series ğ›½ (ğ‘¡), ğ‘ (ğ‘¡). Specifically, we first randomly select 300 talking videos and annotate the talking style of these videos to three categories: tedious, solemn, and excited. Afterwards, for the motion series ğ›½ (ğ‘¡), ğ‘ (ğ‘¡) of each video, we calculate its derivative series with respect to time ğ‘¡: ( ğœ•ğ‘¡ ) along time, yielding 8 feature vectors (4 for mean, 4 for standard deviation). To observe the relationship between these feature vectors and talking styles, we leverage t-SNE algorithm <ref type="bibr" target="#b30">[31]</ref> to visualize each feature vector and plot points from different style categories with different colors, as shown in Figure <ref type="figure">3</ref>. Figure <ref type="figure">3</ref> demonstrates that the talking style is closely related with ğœ (ğ›½ (ğ‘¡)),</p><formula xml:id="formula_3">ğœ ( ğœ•ğ›½ (ğ‘¡ ) ğœ•ğ‘¡ ), ğœ ( ğœ•ğ‘ (ğ‘¡ ) ğœ•ğ‘¡ ), especially ğœ ( ğœ•ğ‘ (ğ‘¡ )</formula><p>ğœ•ğ‘¡ ). Meanwhile the talking style is less related with ğœ‡ (â€¢), which denotes that the talking style is mostly reflected in the fluctuation of movements, rather than the idle state of movements.</p><p>Based on such observation, we define the style codes as the standard deviation of facial and head pose movements. Formally, given arbitrary video with the reconstructed parameter series ğ›½ (ğ‘¡), ğ‘ (ğ‘¡), the style codes sty are defined as:</p><formula xml:id="formula_4">sty = ğœ (ğ›½ (ğ‘¡)) âŠ• ğœ ( ğœ•ğ›½ (ğ‘¡) ğœ•ğ‘¡ ) âŠ• ğœ ( ğœ•ğ‘ (ğ‘¡) ğœ•ğ‘¡ ),<label>(2)</label></formula><p>where âŠ• signifies the vector concatenation.</p><p>To summarize, we make the following two conclusions: (1) one identity have multiple talking styles, (2) the talking style is closely related to the variance of facial and head pose movements inside each video, following which we define the style codes in Equation <ref type="formula" target="#formula_4">2</ref>. The style codes are utilized to synthesize diversified talking styles, details will be illustrated in the section 5.</p><p>Following the style codes defined in section 4, we propose a twostage talking face synthesis framework to imitate arbitrary talking styles, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. Our framework synthesizes stylized talking videos with the following three inputs: one static portrait of the speaker, the driven audio and the style reference video. In the first stage of our framework, we devise a latent-style-fusion (LSF) model to synthesize stylized 3D talking faces through imitating arbitrary talking styles. Based on the synthesized 3D talking faces, in the second stage, we leverage the deferred neural render <ref type="bibr" target="#b29">[30]</ref> and few-shot neural texture generation model to generate video frames photo-realistically. In the next two subsections, we will introduce the two stages respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Stylized 3D Talking Face Synthesis</head><p>In the first stage of our framework, we propose the latent-stylefusion (LSF) model for talking face synthesis. Overall, the input of the LSF model is the driven audio and the reference talking video for style imitation. The LSF model learns motion related information from audio, and then combines latent audio representation with style information to synthesize 3D talking meshes with target talking style. Details will be illustrated as follows.</p><p>For the driven input audio X ğ‘ of ğ‘‡ seconds, we firstly leverage the DeepSpeech <ref type="bibr" target="#b14">[15]</ref> model to extract speech features. Deepspeech is a deep neural model for automatic speech recognition (ASR). The extracted features from DeepSpeech not only contains rich speech information but also is robust to background noise and generalizes well to different identities. Feeding the input audio X ğ‘ to the DeepSpeech model, yields latent representations X ğ‘‘ âˆˆ R 50ğ‘‡ Ã—ğ· ğ‘ , where ğ· ğ‘ is the dimension of the DeepSpeech features, and 50ğ‘‡ denotes that the DeepSpeech features have 50 frames per second. Afterwards, for the reference talking video, we calculate its style codes sty âˆˆ R ğ· ğ‘  as illustrated in Section 4 for style imitation, where ğ· ğ‘  is the dimension of the style codes.</p><p>Having obtained the X ğ‘‘ and sty, we now elaborate the 3D talking face synthesis process. We devise a latent-style-fusion (LSF) model, which takes X ğ‘‘ âˆˆ R 50ğ‘‡ Ã—ğ· ğ‘ and sty âˆˆ R ğ· ğ‘  as input, outputs facial movements ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) âˆˆ R 25ğ‘‡ Ã—64 and head pose movements ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) âˆˆ R 25ğ‘‡ Ã—7 with 25 frames per second. Based on ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡), we reconstruct the 3D talking meshes with the 3DMM face model <ref type="bibr" target="#b1">[2]</ref>.</p><p>The LSF model leverages a latent fusion mechanism to both synthesize stylized faces and guarantee the synchronization between audio and motion. Specifically, as shown in Figure <ref type="figure" target="#fig_0">2</ref>, the LSF model firstly takes audio X ğ‘‘ as input and encodes X ğ‘‘ with the bottom part of ResNet-50 <ref type="bibr" target="#b15">[16]</ref>, yielding latent audio representation X ğ‘™ . Afterwards, the LSF model fuses the latent audio representation X ğ‘™ and style codes sty to acquire mixed representation for synthesis. During the fusion process, the LSF model first dropouts the latent audio representation X ğ‘™ to obtain X â€² ğ‘™ , while the information from sty remains unchanged. Next, the LSF model concats each frame of X â€² ğ‘™ with style codes sty, leading to the mixed representation. Further, the LSF model leverages the top part of ResNet-50 to predict facial movements ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and head pose movements ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) from the mixed representation. It's noteworthy to stress that the fusion between latent audio representation and style codes enables the synthesis of more expressive talking styles. Meanwhile, the dropout of audio information prevents from discarding style information for synthesis. The overall implementation of the LSF model is simple but effective.</p><p>For the training stage of the LSF model, we adopt the parameter series ğ›½ (ğ‘¡), ğ‘ (ğ‘¡) reconstructed from the 3D face reconstruction algorithm <ref type="bibr" target="#b11">[12]</ref> as ground truth. For each training video, we calculate its style codes sty and randomly clip the the input audio X ğ‘‘ and the ground truth ğ›½ (ğ‘¡), ğ‘ (ğ‘¡) to fixed length. Afterwards, we feed X ğ‘‘ and corresponding sty to the LSF model, yielding the predicted expression parameter series ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡). Based on the predicted ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡), we apply the L 1 loss as follows:</p><formula xml:id="formula_5">L L 1 = ||ğ›½ (ğ‘¡) âˆ’ ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡)|| 1 + ||ğ‘ (ğ‘¡) âˆ’ ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡)|| 1 .</formula><p>(</p><formula xml:id="formula_6">)<label>3</label></formula><p>It is worth emphasizing that the training of the LSF model doesn't require any additional annotation on the speaking identity. Only by training on wild videos with stable talking styles, we obtain expressive style space. During the inference stage, feeding style codes of arbitrary talking videos to LSF model not only yields desired talking styles but also maintains the synchronization between the driven audio and talking faces. Meanwhile, we can interpolate among different talking styles to acquire new talking styles. For the audio representation X ğ‘‘ with arbitrary duration, since that the trained LSF model only digests the audio with fixed length, we apply the slide window strategy to synthesize the corresponding facial movements ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and head pose movements ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡).</p><p>So far, we have obtained the untextured talking 3D faces. In the next subsection, we'll introduce how we render these 3D faces photo-realistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Photorealistic Render</head><p>Conventional deferred neural render <ref type="bibr" target="#b29">[30]</ref> requires substantial training data for each identity. In order to both synthesize photorealistic results and guarantee the few-shot capacity, we devise a few-shot neural texture generation model and combine the generated neural texture with the deferred neural render, which enables synthesizing photorealistic videos with only one source portrait. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, the deferred neural render incorporates the generated neural texture, conducts UV texture sampling on the neural texture, and translates the sampled image to photorealistic domain.</p><formula xml:id="formula_7">Algorithm 1 UV Texture Sampling 1: X ğ‘¢ğ‘£ âˆˆ R 2Ã—ğ» Ã—ğ‘Š 2: Y ğ‘¡ âˆˆ R ğ· ğ‘¡ Ã—ğ» ğ‘¡ Ã—ğ‘Š ğ‘¡ 3: X ğ‘  âˆˆ R ğ· ğ‘¡ Ã—ğ» Ã—ğ‘Š 4: for ğ‘– â† 1 to ğ» do 5: for ğ‘— â† 1 to ğ‘Š do 6: ğ‘¢ = X ğ‘¢ğ‘£ [0, ğ‘–, ğ‘—] 7: ğ‘£ = X ğ‘¢ğ‘£ [1, ğ‘–, ğ‘—] 8:</formula><p>X ğ‘  [:, ğ‘–, ğ‘—] = BilinearSample(Y ğ‘¡ , ğ‘¢, ğ‘£) 9: return X ğ‘  Detailedly, for the input 3D talking faces, we firstly leverage the UVAtlas tool 2 to obtain the UV coordinate of each vertex in the 3D model. Afterwards, we rasterize the 3D face model to 2D image X ğ‘¢ğ‘£ âˆˆ R 2Ã—ğ» Ã—ğ‘Š , of which each pixel represents the UV coordinate. Subsequently, for the input portrait X ğ‘ âˆˆ R 3Ã—ğ» Ã—ğ‘Š and the 3D face model, we extract the RGB texture X ğ‘¡ âˆˆ R 3Ã—ğ» ğ‘¡ Ã—ğ‘Š ğ‘¡ , where ğ» ğ‘¡ ,ğ‘Š ğ‘¡ signifies the height and width of the texture. Based on X ğ‘¡ , we leverage the pix2pix <ref type="bibr" target="#b16">[17]</ref> model to generate neural texture Y ğ‘¡ âˆˆ R ğ· ğ‘¡ Ã—ğ» ğ‘¡ Ã—ğ‘Š ğ‘¡ , where ğ· ğ‘¡ signifies the dimension of the neural texture. With the neural texture, we conduct UV texture sampling on X ğ‘¢ğ‘£ to obtain the sampled image X ğ‘  âˆˆ R ğ· ğ‘¡ Ã—ğ» Ã—ğ‘Š , the details of the sampling algorithm are illustrated in the Algorithm 1. Finally, we translate the sampled image X ğ‘  to photorealistic image through the pix2pixHD <ref type="bibr" target="#b33">[34]</ref> model.</p><p>During the training stage, the few-shot texture generation model and the deferred neural render model are trained simultaneously. Given the rasterized input X ğ‘¢ğ‘£ , we denote the rendered image as Y â€² and the ground truth image as Y. We combine the perceptual loss <ref type="bibr" target="#b19">[20]</ref> and L 1 loss together as L to optimize the neural texture and pix2pixHD model. Formally:</p><formula xml:id="formula_8">L = ||Y âˆ’ Y â€² || 1 + ||ğœ™ (Y) âˆ’ ğœ™ (Y â€² )|| 1 ,<label>(4)</label></formula><p>where ğœ™ (â€¢) is the first few layers of VGGNet <ref type="bibr" target="#b25">[26]</ref> pretrained on the ImageNet <ref type="bibr" target="#b10">[11]</ref>. Due to the limitation of the 3DMM face model, in our render, we only synthesize the facial part of the image, without considering the hair and background rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to demonstrate the effectiveness of our framework. We evaluate our framework on the collected Ted-HD dataset. Our method has acquired better synthesis results both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset</head><p>As illustrated in Section 4, the currently available datasets are either collected in lab which have constrained talking styles or collected in the wild of which the talking styles are unstable and noisy. Therefore for the training and testing of the LSF model, we leverage the Ted-HD dataset described in Section 4. Totally, the Ted-HD dataset has 834 clips of videos, we select 799 clips for training, and hold out the remained 35 for testing. The training set and test set have no overlap on identities. Additionally, for the training of the deferred neural render and the few-shot neural texture generation model, we leverage the Lip Reading in the Wild (LRW) <ref type="bibr" target="#b7">[8]</ref> dataset. ğœ•ğ‘¡ )). The predicted facial movements ğ›½ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) and the head pose movements ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ (ğ‘¡) have 25 frames per second. For the convenience of training, we randomly clip the input to 80 frames, and clip the output to 32 frames. For the implementation of the LSF model, we apply the ResNet-50 as the backbone. We leverage the first 16 layers of ResNet-50 to encode the DeepSpeech features, combine the encoded features with style codes, and leverage the last 34 layers of ResNet-50 to predict the motion series. When optimizing, we adopt the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> to train the LSF model with the initial learning rate of 5 Ã— 10 âˆ’4 . We train for 50000 iterations with a mini-batch size of 128 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation Details</head><p>During the training of the deferred neural render and the fewshot neural texture generation model, the input UV image X ğ‘¢ğ‘£ has a size of 2 Ã— 224 Ã— 224, while the neural texture has a size of 16 Ã— 64 Ã— 64. The dimension ğ· ğ‘¡ of the texture is set to 16, which enables each pixel to contain richer texture information. Meanwhile, the texture size is set to be smaller than the UV image size, which avoids oversampling in the sample process. The output image X ğ‘  is conventional RGB image with size of 3Ã—224Ã—224. When optimizing, we adopt the Adam optimizer to simultaneously train the neural render and the texture generation model. The learning rate is set to 2 Ã— 10 âˆ’4 . We train for 1000000 iterations with mini-batch size of 6 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with VOCA on Style Synthesis</head><p>To the best of our knowledge, the VOCA model <ref type="bibr" target="#b9">[10]</ref> is the only available method that captures diversified talking styles of facial movements. Therefore, in this section, we systematically compare our method with the VOCA model <ref type="bibr" target="#b9">[10]</ref> to demonstrate the effectiveness of the LSF model. Different from our method, the VOCA model learns identity-level talking styles. Specifically, the VOCA model injects a one-hot identity code to the time convolution network, and directly predicts face model vertexes from the DeepSpeech features and the identity code. By adjusting the one-hot identity code, the VOCA model outputs different talking styles. We compare the style space learned from the VOCA model and the style space learned from the LSF model with extensive user studies. Specifically, we randomly select 10 clips of driven audio, each with a duration from 10 to 20 seconds. Afterwards, we randomly sample 5 talking styles from the VOCA style space and 5 talking styles from the style space of our method. With the sampled talking styles and the driven audio, we synthesize corresponding talking faces and retarget the synthesized faces to the same identity. Afterwards, we subsume videos with the same driven audio and synthesis model to the same group. For each group of video, we invite 20 participants to rate (1) the expressiveness of the talking style, (2) the naturalness of facial movements, (3) the audio-visual synchronization between driven audio and talking face. We ask participants to rate the mean opinion score (MOS) <ref type="bibr" target="#b24">[25]</ref> in the range 1-5 (higher MOS score denotes better results). When showing videos to participants, since that the VOCA model only gives untextured 3D faces, we also just give the 3D talking faces synthesized from the LSF model, without utilizing the deferred neural render to generate photorealistic results for fair comparison. Table <ref type="table" target="#tab_3">1</ref> demonstrates the results of user studies. Through the experimental results, we observe that our LSF model has acquired higher talking style expressiveness, which verifies that the style space learned by taking style imitation is more expressive than the identity-level style space. Meanwhile, our LSF model outperforms the VOCA model by 0.08 in terms of the facial movement naturalness and audio-visual synchronization. Such results confirm the effectiveness of style imitation in LSF model further. Additionally, compared with the VOCA model which requires substantial training data for each identity, our method is trained on wild dataset which does not require any annotation on identity or talking style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Study on the Style Space</head><p>In this section, we extensively investigate the style space learned in our method. We conduct two qualitative experiments, which verify that not only the synthesized talking styles are diversified, but new talking styles can also be generated from the interpolation of different talking styles. To verify the diversification of the talking styles, we visualize the distance between lower and upper lip as a function of time. Specifically, we randomly select one piece of driven audio and 10 different talking styles, and then synthesize 10 facial movements corresponding to each talking style and driven audio. Afterwards, we calculate the lip distance as shown in Figure <ref type="figure" target="#fig_5">5</ref>. Through Figure <ref type="figure" target="#fig_5">5</ref> we observe that the lip distance significantly varies among different talking styles, which verifies that the LSF model is able to synthesize diversified talking styles. Meanwhile, different talking styles demonstrate similar trends of fluctuation, the peak and the valley of the distance curve highly overlap, which confirms that the LSF model also guarantees the synchronization between audio and synthesized motions.</p><p>To confirm that the style space in the LSF model is expressive and interpolative, we visualize the interpolation results of different talking styles, as shown in Figure <ref type="figure" target="#fig_4">4</ref>. Detailedly, we select two representative talking styles: excited and solemn, and conduct linear interpolation between the two styles to generate new talking styles. From each row of Figure <ref type="figure" target="#fig_4">4</ref>, we observe that the facial and head pose movements translate smoothly from excited talking style to solemn style. For the excited talking style, the lip motion is exaggerated and the head frequently shakes. Meanwhile, for the solemn talking style, the lip motion and head pose are stable. We provide more synthesis results in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with One-Shot Synthesis</head><p>In this section, we conduct experiments to demonstrate that our method synthesizes more natural and expressive talking faces compared with several baseline methods. Specifically, we compare our method with the following baseline methods: (1) the ATVG framework <ref type="bibr" target="#b5">[6]</ref>, (2) the MakeItTalk framework <ref type="bibr" target="#b40">[41]</ref>, (3) the Wav2Lip framework <ref type="bibr" target="#b23">[24]</ref>. For Wav2Lip framework which requires few seconds of videos as input, we repeat input portrait as videos for fair comparison. Figure <ref type="figure" target="#fig_6">6</ref> shows some synthesis results We observe that our method has more expressive facial movements and head pose movements while also guarantees the synthesized results to be photorealistic.</p><p>Additionally, we conduct both user studies and quantitative evaluations on the Ted-HD dataset to verify the effectiveness of our method. Specifically, for the user studies, we firstly synthesize videos with the randomly selected 20 clips of driven audios and 5 different identities. Afterwards, for each video, we invite 20 participants to rate (1) the naturalness of facial movements, (2) the audio-visual synchronization between driven audio and talking face. The mean opinion score (MOS) is rated in the range 1-5. Besides, we also evaluate the synthesized video quality with the signal-noiseratio (SNR) metric. We do not leverage the PSNR metric since that the ground truth talking videos with arbitrary talking styles are not available. Table <ref type="table" target="#tab_4">2</ref> shows the comparison results. From Table <ref type="table" target="#tab_4">2</ref> we observe that our method has achieved the most expressive facial motion and best video quality. We also notice that the Wav2Lip method achieves unsatisfying motion naturalness since that it cannot resolve the oneshot synthesis scenario. Meanwhile, we observe that the AVS of our method is slightly lower than Wav2Lip, that is because the talking style synthesis in our LSF model mildly sacrifices the performance of audio-visual synchronization, but our method still has better AVS performance compared with ATVG and MakeItTalk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Effectiveness of Latent Style Fusion</head><p>To verify the rationality of the latent style fusion mechanism in LSF model, we conduct the following ablation experiments. For comparison, we remove the latent style fusion mechanism, and directly concatenate DeepSpeech representation with style codes as the input of ResNet-50. Afterwards, we compare the synthesized results with LSF model through similar user studies as Section 6.5 does. The experimental results are shown in the last two rows of Table <ref type="table" target="#tab_4">2</ref>. From the results we observe that the motion naturalness and audio-visual synchronization would degrade significantly once we remove the latent style fusion mechanism, which verifies the effectiveness of the latent style fusion. Meanwhile, the video quality remains constant since that the motion synthesis does not influence the photorealistic render stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose the concept of style imitation for audiodriven talking face synthesis. To imitate arbitrary talking styles, we firstly formulate the style codes of each talking video as several interpretable statistics of 3DMM parameters. Afterwards, we devise a latent-style-fusion (LSF) model to synthesize stylized talking faces according to the style codes and driven audio. The incorporation of style imitation not only circumvents the annotation for talking style during the training phase, but also endows the capacity of arbitrary style synthesis and new talking style generation. Additionally, to synthesize expressive talking styles, we collect Ted-HD dataset with 834 clips of talking videos, which contains stable and diversified talking styles. We conduct extensive experiments on the constructed dataset and obtain expressive synthesis results with our Ted-HD dataset and LSF model. The constructed Ted-HD dataset will be made publicly available in the future. We hope that the proposal of talking style imitation and the construction of Ted-HD dataset pave a new way for audio-driven talking face synthesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall framework of our method contains two stages: the stylized 3D talking face synthesis stage and the photorealistic render stage. The first stage synthesizes stylized 3D talking faces by imitating talking styles from the style codes. The second stage synthesizes photorealistic videos with deferred neural render and neural texture generation model. The stylized 3D talking face synthesis stage and the photorealistic render stage are trained separately in our framework. ğ‘‡ denotes the time dimension of DeepSpeech features. For the calculation of style codes, even style reference videos from one identity would yield diversified style codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 ) 2 )</head><label>32</label><figDesc>transcripts to users, asking the following question: which pair of (ğ‘£ â€² 1 , ğ‘£ â€² 2 ) and (ğ‘£ â€² 1 , ğ‘£ â€² 3 ) has more similar talking styles. Statistics demonstrate that among 100 triplets, (ğ‘£ â€² 1 , ğ‘£ â€² is more similar in 30 triplets, while (ğ‘£ â€² 1 , ğ‘£ â€² is more similar in 70 triplets. With the statistics that 30% of videos inside each identity have dissimilar talking styles, we conclude that one identity has multiple talking styles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ğœ•ğ‘¡ ). Next, we calculate the mean value ğœ‡ (â€¢) and the standard deviation ğœ (â€¢) of (ğ›½ (ğ‘¡), ğ‘ (ğ‘¡), ğœ•ğ›½ (ğ‘¡ ) ğœ•ğ‘¡ , ğœ•ğ‘ (ğ‘¡ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>During the training of the LSF model, the input DeepSpeech audio features have 50 frames per second (FPS), while each frame has the dimension ğ· ğ‘ of 29. The input style codes sty have the dimension ğ· ğ‘  of 135 (64 for ğœ (ğ›½ (ğ‘¡)), 64 for ğœ ( ğœ•ğ›½ (ğ‘¡ ) ğœ•ğ‘¡ ), 7 for ğœ ( ğœ•ğ‘ (ğ‘¡ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The interpolation results between excited talking style and solemn talking style with the same driven audio. The first row has solemn talking style, and the last row has excited talking style, while the middle row has the average style between the first and the last. Among these rows, we observe smooth transition of facial and head pose movements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The distance between the lower lip and the upper lip for the same driven audio conditioned on different talking styles. Different color denotes different talking styles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison with several baseline methods (ATVG, MakeItTalk and Wav2Lip). Our method not only yields expressive facial and head pose movements, but also synthesizes photorealistic videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The mean opinion scores (MOS) of different metrics, higher signifies better. TSE denotes talking style expressiveness, FMN denotes facial movement naturalness, and AVS denotes audio-visual synchronization.</figDesc><table><row><cell></cell><cell>TSE FMN AVS</cell></row><row><cell cols="2">VOCA [10] 3.28 3.13 3.56</cell></row><row><cell>LSF</cell><cell>3.41 3.21 3.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with baseline methods on the Ted-HD dataset, where FMN denotes facial movement naturalness, and AVS denotes audio-visual synchronization. The Pre-Fusion method removes latent style fusion in the LSF model, details are illustrated in Section 6.6</figDesc><table><row><cell></cell><cell cols="2">FMN AVS SNR (dB)</cell></row><row><cell>ATVG [6]</cell><cell>2.71 3.51</cell><cell>2.98</cell></row><row><cell cols="2">MakeItTalk [41] 3.08 3.13</cell><cell>3.01</cell></row><row><cell>Wav2Lip [24]</cell><cell>2.97 4.28</cell><cell>2.78</cell></row><row><cell>Pre-Fusion</cell><cell>2.19 2.25</cell><cell>5.70</cell></row><row><cell>Ours</cell><cell>3.59 3.75</cell><cell>5.76</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/wuhaozhe/style_avatar diversified styles. To resolve this problem, in this work, we propose to imitate talking styles from arbitrary wild talking videos.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/microsoft/UVAtlas</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGMENTS</head><p>This work is supported by the National Key R&amp;D Program of China under Grant No. 2020AAA0108600, the state key program of the National Natural Science Foundation of China (NSFC) (No.61831022), Beijing Academy of Artificial Intelligence No. BAAI2019QN0302.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">LRS3-TED: a large-scale dataset for visual speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1809.00496</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
				<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Talking-head Generation with Rhythmic Head Motion</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">VoxCeleb2: Deep Speaker Recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip Reading in the Wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An audiovisual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3D speaking styles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10101" to="10111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speech Driven Talking Face Generation from a Single Image and an Emotion Condition</title>
		<author>
			<persName><forename type="first">You</forename><surname>Sefik Emre Eskimez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
		<idno>arXiv-2008</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cnn-based real-time dense face reconstruction with inverse-rendered photo-realistic face images</title>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1294" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<title level="m">Deep speech: Scaling up end-to-end speech recognition</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You said that?: Synthesising talking faces from audio</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Amir Jamaludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="1767" to="1779" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-driven emotional video portraits</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14080" to="14089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Yann</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 sixth IEEE international conference on advanced video and signal based surveillance</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Vinay P Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vocabulary for performance and quality of service</title>
	</analytic>
	<monogr>
		<title level="m">ITUT Recommendation</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Yann</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ayush Tewari, Christian Theobalt, and Matthias NieÃŸner</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="716" to="731" />
		</imprint>
	</monogr>
	<note>Neural voice puppetry: Audio-driven facial reenactment</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>ZollhÃ¶fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>NieÃŸner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mead: A large-scale audiovisual dataset for emotional talking-face generation</title>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="700" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation</title>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="670" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Audiodriven talking face video generation with learning-based personalized head pose</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipeng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv-2002</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal inputs driven talking face generation with spatial-temporal dependency</title>
		<author>
			<persName><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Talking Face Generation with Expression-Tailored Generative Adversarial Network</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiming</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1716" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4176" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MakeltTalk: speaker-aware talking-head animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Echevarria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
