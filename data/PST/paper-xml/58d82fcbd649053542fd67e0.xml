<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification</title>
				<funder ref="#_56Ccy5J">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>tingchen@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3018661.3018735</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Heterogeneous Information Networks</term>
					<term>Network Embedding</term>
					<term>Author Identification</term>
					<term>Meta Path</term>
					<term>Task-guided</term>
					<term>Path-augmented</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of author identification under double-blind review setting, which is to identify potential authors given information of an anonymized paper. Different from existing approaches that rely heavily on feature engineering, we propose to use network embedding approach to address the problem, which can automatically represent nodes into lower dimensional feature vectors. However, there are two major limitations in recent studies on network embedding: (1) they are usually general-purpose embedding methods, which are independent of the specific tasks; and</p><p>(2) most of these approaches can only deal with homogeneous networks, where the heterogeneity of the network is ignored. Hence, challenges faced here are two folds: (1) how to embed the network under the guidance of the author identification task, and (2) how to select the best type of information due to the heterogeneity of the network.</p><p>To address the challenges, we propose a task-guided and pathaugmented heterogeneous network embedding model. In our model, nodes are first embedded as vectors in latent feature space. Embeddings are then shared and jointly trained according to task-specific and network-general objectives. We extend the existing unsupervised network embedding to incorporate meta paths in heterogeneous networks, and select paths according to the specific task. The guidance from author identification task for network embedding is provided both explicitly in joint training and implicitly during meta path selection. Our experiments demonstrate that by using pathaugmented network embedding with task guidance, our model can obtain significantly better accuracy at identifying the true authors comparing to existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Heterogeneous networks are ubiquitous. Examples include bibliographic networks <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>, movie recommendation networks <ref type="bibr" target="#b34">[32]</ref> and many online social networks containing information of heterogeneous types <ref type="bibr" target="#b21">[19]</ref>. Different from their homogeneous counterparts, heterogeneous networks contain multiple types of nodes and/or links. For example, in bibliographic networks, node types include paper, author and more; link types include author-writepaper, paper-contain-keyword and so on. Due to the fast emerging of such data, the problem of mining heterogeneous network has gained a lot of attention in the past few years <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b21">19]</ref>.</p><p>In this work, we are interested in the problem of mining heterogeneous bibliographic network <ref type="bibr" target="#b23">[21]</ref>. To be more specific, we consider the problem of author identification under double-blind review setting <ref type="bibr" target="#b13">[11]</ref>, on which many peer review conferences/journals are based. Authors of the paper under double-blind review are not visible to reviewers, i.e. the paper is anonymized, and only content/attributes of the paper (such as title, venue, text information, and references) are visible to reviewers. However, in some cases authors of the paper can still be unveiled by the content and references provided. Affected by the phenomenon, questions exist about whether or not double-blind review process is really effective. In fact, WSDM this year also conducts an experiment trying to answer this question. Here we ponder on this issue by formulating the author identification problem that aims at designing a model to automatically identify potential authors of an anonymized paper. Instead of dealing with full text directly, we treat the information of an anonymized paper as nodes in bibliographic network, such as keyword nodes, venue nodes, and reference nodes. An illustration of the problem can be found in Figure <ref type="figure" target="#fig_0">1</ref>. Other than serving as a study for existing reviewing system, the problem has broader implications for general information retrieval and recommender system, where the model is asked to match queried document with certain target, such as reviewer recommendation <ref type="bibr" target="#b31">[29,</ref><ref type="bibr" target="#b24">22]</ref>.</p><p>To tackle the author identification problem, as well as many other network mining problems, good representations of data are very important, as demonstrated by many previous work <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b9">7]</ref>. Unlike traditional supervised learning, dense vectorized representations <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref> are not directly available in networked data <ref type="bibr" target="#b28">[26]</ref>. Hence, many traditional methods under network settings heavily rely on problem specific feature engineering <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b35">33]</ref>.</p><p>Although feature engineering can incorporate prior knowledge of the problem and network structure, usually it is time-consuming, problem specific (thus not transferable), and the extracted features may be too simple for complicated data sets <ref type="bibr" target="#b5">[3]</ref>. Several network embedding methods <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref> have been proposed to automatically learn feature representations for networked data. A key idea behind network embedding is learning to map nodes into vector space, such that the proximities among nodes can be preserved. Similar nodes (in terms of connectivity, or other properties) are expected to be placed near each other in the vector space.</p><p>Unfortunately, most existing embedding methods produce generalpurpose embeddings that are independent of tasks, and they are usually designed for homogeneous networks <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>. When it comes to author identification problem under the heterogeneous networks, existing embedding methods cannot be applied directly. There are two unique challenges brought by this problem: (1) how to embed the network under the guidance of author identification task, so that embeddings learned are more suitable for this task compared to general network embedding. And (2) how to select the best type of information due to the heterogeneity of the network. As shown in previous work <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b23">21]</ref>, proximity in heterogeneous networks is richer than homogeneous counterparts, the semantic of a connection between two nodes is likely to be dependent on the type of connection they form.</p><p>To address the above mentioned challenges, we propose a taskguided and path-augmented network embedding method. In our model, nodes are first embedded as vectors. Then the embeddings are shared and jointly trained according both task-specific and networkgeneral objectives: (1) the author identification task objective where embeddings are used in a specifically designed model to score possible authors for a given paper, and (2) the general heterogeneous network embedding objective where embeddings are used to predict neighbors of a node. By combing both objectives, the learned network can preserve network structures/proximities, as well as be beneficial to the author identification task. To better utilize the heterogeneous network structure, we extend the existing unsupervised network embedding to incorporate meta paths derived from heterogeneous networks, and select useful paths according to the author identification task. Compared to traditional network embedding <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref>, our method uses the author identification task as an explicit guidance to influence network embedding by joint learning, and also as an implicit guidance to select meta paths, based on which the network embedding is performed. It is worth mentioning that although our model is originally targeted for the author identification problem, it can also be extended to other task-oriented embedding problems in heterogeneous networks.</p><p>The contributions of our work can be summarized as follows.</p><p>? We propose a task-guided and path-augmented heterogeneous network embedding framework, which can be applied to author identification problem under double-blind review setting and many other tasks.</p><p>? We demonstrate the effectiveness of task guidance for network embedding when a specific task is of interest; and also show the usefulness of meta-path selection in heterogeneous network embedding.</p><p>? Our learning algorithm is efficient, parallelizable, and experimental results show that our model can achieve much better results than existing feature based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>In this section, we first introduce the concept of heterogeneous networks and meta paths, and then introduce the embedding representation of nodes. Finally, a formal definition of the author identification problem is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Heterogeneous Networks</head><p>Definition 1 (Heterogeneous Networks) A heterogeneous network <ref type="bibr" target="#b23">[21]</ref> is defined as a network with multiple types of nodes and/or multiple types of links. It can be denoted as G = (V, E), where V is a set of nodes and E is a set of links. A heterogeneous network is also associated with a node type mapping function fv : V ? O, which maps the node to a predefined node type, and a link type mapping function fe : E ? R, which maps the link to a predefined link type. It is worthing noting that a link type automatically defines the node types of its two ends.</p><p>The bibliographic network can be seen as a heterogeneous network <ref type="bibr" target="#b23">[21]</ref>. It is centered by paper, the information of a paper can be represented as its neighboring nodes. The node types N in the network include paper, author, keyword, venue and year, and the set of link types R include author-write-paper, paper-contain-keyword, and so on. The network schema is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Definition 2 (Meta path) A meta path <ref type="bibr" target="#b25">[23]</ref> is a path defined on the network schema TG = (O, L) and is denoted in the form of</p><formula xml:id="formula_0">o1 l 1 -? o2 l 2 -? ? ? ? lm --? om+1</formula><p>, which represents a compositional relations between two given types. For each of the meta path r, we can define an adjacency matrix M (r) , with cardinality equal to the number of nodes, to denote the connectivity of nodes under that meta path. If there are multiple meta paths considered for a given network G, we use a set of adjacency matrices {M (r) } to represent it.</p><p>Examples of meta paths defined in network schema Figure <ref type="figure" target="#fig_1">2</ref> include paper ? keyword ? paper, and paper ? year ? paper. From these two examples, it is easy to see that in a heterogeneous network, even compare two nodes of the same type (e.g. paper), going from different paths can lead to different semantic meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding Representation of Nodes</head><p>The networked data is usually high-dimensional and sparse, as there can be many nodes but the links are usually sparse <ref type="bibr" target="#b3">[1]</ref>. This brings challenges to represent nodes in the network. For example, given two users, it is hard to calculate their similarity or distance directly. To obtain a better data representation, embedding methods are widely adopted <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref>, where nodes in the network are mapped into some common latent feature space. With embedding, we can measure similarity/distance between two nodes directly based on arithmetic operations, like dot product, of their embedding vectors.</p><p>Through the paper, we use a matrix U to represent the embedding table for nodes. The size of the matrix is N ? D, where N is total number of nodes (including all node types, such as authors, keywords, and so on), and D is the number of dimensions. So the feature vector for node n is denoted as un, which is a Ddimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Author Identification Problem</head><p>We formalize the author identification problem using bibliographic networks with network schema shown in Figure <ref type="figure" target="#fig_1">2</ref>. For each paper p, we represent its neighbors in the given network G as Xp = {X</p><formula xml:id="formula_1">(1) p , X<label>(2)</label></formula><formula xml:id="formula_2">p , ? ? ? , X (T ) p }, where X (t)</formula><p>p is a set of neighbor nodes in t-th node type. The node types include keyword, reference, venue, and year in our task. And we use Ap to denote the set of true authors of the paper p.</p><p>Author Identification Problem. Given a set of papers represented as (X, A) where X = {Xp}, A = {Ap}, the goal is to learn a model to rank potential authors for every anonymized paper p based on information in Xp, such that its top ranked authors are in Ap 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED MODEL</head><p>In this section, we introduce the proposed model in details. The model is composed of two major components: (1) author identification based on task-specific embedding, and (2) path-augmented general network embedding. We first introduce them separately and then combine them into a single unified framework, where the meta paths are selected according to the author identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task-Specific Embedding for Author Identification</head><p>In this subsection, we propose a supervised embedding-based model that can rank the potential authors given the information of a paper (such as keywords, references, and venue). Our model first maps each node into latent feature space, and then gradually builds the feature representation for the anonymized paper based on its observed neighbors in the network. Finally the aggregated paper representation is used to score the potential author.</p><p>There are two stages of aggregation to build up the feature representation for a paper p based on node embeddings. In the first stage, it builds a feature vector for each of the t-th node type by averaging node embeddings in X (t) p , which is:</p><formula xml:id="formula_3">V (t) p = n?X (t) p un/|X (t) p |<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">V (t) p</formula><p>is the feature representation of t-th node type (e.g. keyword node type), and un is the n-th node embedding (e.g. keyword node).</p><p>In the second stage, it builds feature vector for the paper p using a weighted combination of feature vectors of different node types:</p><formula xml:id="formula_5">Vp = t wtV (t) p (2)</formula><p>1 Here it is posed as a ranking problem since each paper may have different number of authors and it is unknown beforehand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper embedding</head><p>Author scores Now the anonymized paper p is represented by this feature vector Vp, and can be used to score potential authors (which are also embedding vectors) by taking their dot product. The score between a pair of paper and author is defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node type embedding</head><formula xml:id="formula_6">f (p, a) = u T a Vp = u T a t wtV (t) p = u T a t wt n?X (t) p un/|X (t) p |<label>(3)</label></formula><p>The computational flow is summarized in Figure <ref type="figure" target="#fig_2">3</ref>. Note that the final densely-connected layer has no bias term, and thus its weight matrix can be seen as author node embeddings. The final layer output (green dots) is the score vector for the candidate authors.</p><p>To learn the parameters U and w, we use stochastic gradient descent (SGD) <ref type="bibr" target="#b7">[5]</ref> based on a hinge loss ranking objective. For each triple (p, a, a ), where a is one of the true author for paper p, and a is not the author of paper p, the hinge loss is defined as:</p><formula xml:id="formula_7">max 0, f (p, a ) -f (p, a) + ? (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where ? is a positive number usually referred as margin <ref type="bibr" target="#b6">[4]</ref>. A loss penalty will incur if the score of positive pair f (p, a) is not at least ? larger than the score of f (p, a ).</p><p>To sample a triple (p, a, a ) used in SGD, we randomly select a paper p from Xp and one of its author a from Ap, then sample a negative author from the pre-defined noise distribution a ? P author n (a ), such as discrete distribution based on author degree (with a similar idea of unigram distribution in word2vec <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Path-Augmented General Heterogeneous Network Embedding</head><p>In this subsection, we propose a path-augmented general network embedding model to exploit the rich information in heterogeneous networks.</p><p>Most of existing network embedding techniques <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25]</ref> are based on the idea that, embeddings of nodes can be learned by neighbor prediction, which is to predict the neighborhood given a node, i.e. the linking probability P (j|i) from node i to node j. For existing network embedding methods, the observed neighborhood of a node is usually defined by original network <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b27">25]</ref> or by random walk on the original network <ref type="bibr" target="#b19">[17]</ref>.</p><p>In heterogeneous network, one can easily enrich the semantic of neighbors by considering different types of meta paths <ref type="bibr" target="#b25">[23]</ref>. As shown in <ref type="bibr" target="#b25">[23]</ref>, different meta paths encode different semantic of links. For example, connections between two authors can encode multiple similarities: (1) they are interested in the same topic, or (2) they are associated with the same affiliation. And clearly these two types of connections indicate different semantics. Inspired by the phenomenon, we generalize existing network embedding techniques <ref type="bibr" target="#b27">[25]</ref> to incorporate different meta paths, and propose the path-augmented network embedding.</p><p>In path augmented network embedding, instead of using original adjacency matrices {E (l) } where l is an original link type or onehop meta path (such as author?write?paper), we consider more meta paths (such as author?write?paper?contain?keyword) and use meta path-augmented adjacency matrices {M (r) } for network embedding, where each M (r) indicates network connectivity under a specific meta path r. Here we normalize each M (r) , such that ?r, i,j M (r) i,j = 1, so that the learned embedding will not be dominated by some meta paths with large raw weights. Since there can be infinite many potential meta paths (including original link types), when considered for network embedding, one has to select a limited number of useful meta paths. The selection of meta paths will be discussed in next sub-section, and we assume a collection of meta paths are selected for now.</p><p>To learn embeddings that preserve proximities among nodes induced by meta paths, we follow the neighbor prediction framework, and model the conditional neighbor distribution of nodes. In heterogeneous networks, there can be multiple types of paths starting from a node i, so the neighbor distribution of the node will be conditioned on both the node i and the given path type r, which is defined as follows:</p><formula xml:id="formula_9">P (j|i; r) = exp(u T i uj) j ?DST (r) exp(u T i u j )<label>(5)</label></formula><p>where ui is the embedding of node i, and DST (r) denotes the set of all possible nodes that are in the destination side of path r.</p><p>In real networks, the number of nodes in DST (r) can be very large (e.g. millions of papers), so the evaluation of Eq. 5 can be prohibitively expensive. Inspired by <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>, we apply negative sampling and form the following approximation term:</p><formula xml:id="formula_10">log P (j|i; r) ? log ?(u T i uj + br)+ k l=1 E j ?P r n (j ) [log ?(-u T i u j -br)]<label>(6)</label></formula><p>where j is the negative node sampled from a pre-defined noise distribution P r n (j ) for path r<ref type="foot" target="#foot_0">2</ref> , and a total of k negative nodes are sampled for each positive node i. Furthermore, a bias term br is added to adjust densities of different paths.</p><p>To learn the parameters U and b, we adopt stochastic gradient descent (SGD) with the goal of maximizing the likelihood function.</p><p>The training procedure is given as follows. We first sample a path r uniformly, and then randomly sample a link (i, j) according to their weights in M (r) . The set of negative nodes {j } used in Eq. 6 are also sampled according to some pre-defined P r n (j ), such as "smoothed" node degree distribution under specific edge type <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>. Finally the parameters U, b are updated according to their gradients, such that approximated sample log-likelihood log P (j|i; r) can be maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Combined Model</head><p>The task-specific embedding sub-model and path-augmented general embedding sub-model capture different perspectives of a net-work. The former focuses more on the direct information related to the specific task, while the latter can better explore more global and diverse information in the heterogeneous information network. This motivates us to model them in a single unified framework.</p><p>The two sub-models are combined in two levels as follows.</p><p>? A joint objective is formed by combining both task-specific and network-general objectives, and joint learning is performed. Here the task serves as an explicit guidance for network embedding.</p><p>? The meta paths used in network-general embedding are selected according to the author identification task. Here the task provides an implicit guidance for network embedding as it helps select meta paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Joint Objective -An Explicit Guidance</head><p>The joint objective function is defined as a weighted linear combination of the two sub-models with a regularization term on the embedding, where the embedding vectors are shared in both submodels:</p><formula xml:id="formula_11">L =(1 -?)L task-specif ic + ?L network-general + ?(M) =(1 -?)E (p,a,a ) max 0, f (p, a ) -f (p, a) + ? + ?E (r,i,j) -log P (j|i; r) + ? i ui 2 2 (7)</formula><p>where ? ? [0, 1] is the trade-off factor for task-specific and networkgeneral components. When w = 1, only network-general embedding is used; and when w = 0, only supervised embedding is used. A regularization term is added to avoid over-fitting.</p><p>To optimize the objective in Eq. 7, we utilize Asynchronous Stochastic Gradient Descent (ASGD), where samples are randomly drawn and training is performed in parallel <ref type="bibr" target="#b18">[16]</ref>. The challenge here is that we have two different tasks that learn from two different data sources. To solve this problem, we design a sampling based task scheduler. Basically, for each worker, it first draws a task according to ?, and then draws samples for the selected task and update the parameters according to the samples. In order to reduce the task sampling overhead, the selected task will be trained on a mini-batch of data samples instead of on a single sample.</p><p>The learning algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning Framework</head><p>Input: Training data X, A and path-augmented adjacency matrices {M (r) }. Output: Parameters U, w, b 1: while not converged do 2:</p><p>for each thread do 3:</p><p>Sample one of the two tasks ? Bern(?) 4:</p><p>if the taks is network-general embedding then 5:</p><p>sample a mini-batch of (r, i, j) triplets 6:</p><p>sample negative nodes {j } 7:</p><p>update parameters U, w according to their gradients 8:</p><p>else // the task is author identification 9:</p><p>sample a mini-batch of (p, a, a ) triplets 10: update parameters U, b according to their gradients 11:</p><p>end if 12:</p><p>end for 13: end while Complexity. Firstly, the algorithm can be run in parallel using multiple CPUs thanks to asynchronous SGD. Secondly, the algo-rithm is efficient, as for each iteration of each thread, there are two major components: (1) both edge and negative node sampling only take constant time with alias table <ref type="bibr" target="#b32">[30]</ref>, and (2) gradient update is linear w.r.t. the number of links and number of embedding dimensions. Thirdly, with mini-batch of reasonable size, the overhead in switching tasks is ignorable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Meta Path Selection -An Implicit Guidance</head><p>So far we have assumed that path-augmented adjacency matrices {M (r) } are already provided. Now we discuss how we can select a set of meta paths that can further enhance the performance of the author prediction task.</p><p>The potential meta paths induced from the heterogeneous network G can be infinite, but not every one is relevant and useful for the specific task of interest. So we utilize the author identification task as a guidance to help select the meta paths that can best help the task at hand.</p><p>The path selection problem can be formulated as: given a set of pre-defined candidate paths R = {r1, r2, ? ? ? , rL}, we want to select a subset of paths R selected ? R, such that certain utility can be maximized. Since our final goal is the author identification task, we define the utility to be the generalization performance (on validation data set) of the task.</p><p>It is worth noting the problem is neither differentiable nor continuous. And the total number of combinations are exponential to the number of candidate paths. So we employ following two steps to select relevant paths in a greedy fashion.</p><p>1 Single path performance. We first run the joint learning with network embedding based on a single path at a time, and then run the experiments for all candidate paths.</p><p>2 Greedy additive path selection. We sort paths according to their performance (from good to poor) obtained from Step 1 above, and gradually add paths into the selected pool. Experiments are run for each additive combination of paths, and the path combination with best performance is selected.</p><p>We need to run experiments (at most) 2N times, where N means the number of candidate paths. Since every experiment takes about 10 minutes in our case (even with millions of nodes and hundreds of millions of links), such selection scheme is affordable and can avoid exponential number of combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we compare the proposed model with baselines, and also evaluate several variants of the proposed model. Case studies are also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>The AMiner citation network <ref type="bibr" target="#b29">[27]</ref> is used throughout our experiments. To prepare for the evaluation, we split all papers into training set and test set according to their publication time. Papers published before 2014 are treated as training set, and papers published in 2014 and 2015 are treated as test set.</p><p>Based on the training papers, a heterogeneous bibliographic network is extracted. We first extract all papers which contain information about its title, authors, references, venue from the dataset. Then we extract keywords by combining unigram and key phrases extracted using method proposed in <ref type="bibr" target="#b16">[14]</ref>. The schema of the network is the same as in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The extracted network contains millions of nodes and tens of millions of links. The detailed statistics of nodes and links for both training and test set can be found in Table <ref type="table" target="#tab_0">1</ref> and 2, respectively. Meta paths augmentation. Other than the length-1 paths presented in the original network, we also consider various of length-2 meta paths as candidate paths for general heterogeneous network embedding. Although other path similarity measures <ref type="bibr" target="#b25">[23]</ref> can be explored, for simplicity, we set weights of a path by the number of path instances. For example, if Tom attended KDD Twice and Jack attended KDD three times, then the path of Tom -KDD -Jack will have a weight of six. The augmented network by adding new meta paths has hundreds of millions of links, much more than the original network. Many of the candidate paths are not symmetric and may contain different information at both sides, so we consider them in both directions. Finally, the detailed statistics of the length-2 paths are presented in Table <ref type="table" target="#tab_2">3</ref>.</p><p>To better understand statistics of the network" Figure <ref type="figure" target="#fig_4">4</ref> shows three different types of degree distributions for papers. As can be seen from the figure, most papers contain quite sparse information of authors, references and keywords: medium 3 authors, 1 reference (many are missing in the data set), and 8 keywords. And this lack of information makes the problem of automatic author identification even harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Experimental Settings</head><p>We mainly consider two types of baselines: (1) the traditional feature-based methods, and (2) the variations of network embedding methods.</p><p>? Supervised feature-based baselines. As widely used in similar author identification/disambiguation problems <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b35">33]</ref>, this thread of methods first extract features for each pair of training data, and then applies supervised learning algorithm to learn some ranking/classification functions. Following them, we extract 20+ related features for each pair of paper and author in the training set (details can be found in appendix). Since the original network only contains true paper-author pairs, in order to get the negative samples, for each paper-author pair we sampled 10 negative pairs by randomly replacing the authors. For the supervised algorithm, we consider Logistic Regression (LR), Support Vector Machines (SVM), Random Forests (RF), and LambdaMART<ref type="foot" target="#foot_1">3</ref> . For all these methods, we use grid search to find their best hyper-parameters, such as regularization penalty, maximum depth of trees, and so on.</p><p>? Task-specific embedding. This method is introduced in Section 3.1. The embeddings of nodes are learned solely based on task-specific embedding architecture.</p><p>? Network-general embedding. This method is introduced in Section 3.2. The embeddings of nodes are learned solely   based on general heterogeneous network embedding, and then the learned embeddings are used to score the author in the same way as in task-specific author identification framework. Since it is not directly combined with author identification task, it cannot perform path selection specific for the task. By default, the paths used for embedding are from original network, i.e. length-1 paths. With length-1 paths, this method is in the same form of PTE <ref type="bibr" target="#b27">[25]</ref>.</p><p>? Pre-training + Task-specific embedding. Pre-training has been found useful to improve neural network based supervised learning <ref type="bibr" target="#b12">[10]</ref>. So instead of training task-specific author identification from randomly initialized embedding vectors, we first pre-train the embedding of nodes using networkgeneral embedding, and then initialize the supervised embedding training with the pre-trained embedding vectors.</p><p>? Proposed combined model. This is our proposed method, which combines both task-specific embedding and meta-path selection-based network-general embedding.</p><p>Candidate authors. There are more than one million authors in the training data, so the total number of candidate authors for each paper is very large. The supervised feature-based baselines cannot scale up to such large amount of candidate set, as it is both very time consuming and storage intensive to extract and store features for all candidate paper-author pairs (which amounts to more than 10 17 pairs). Hence, we conduct comparisons mainly based on a sub-sampled author candidate set, where we randomly sample a set of negative authors, combined with the true authors of the paper to form a candidate set of total 100 authors. For completeness, we also provide both quantitative and qualitative comparisons of different embedding methods on the whole candidate set of a million authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Since the author identification problem is posed as a ranking problem and usually only top returned results are of interest, we adopt two commonly used ranking metrics: Mean Average Precision at k (MAP@k) and Recall at k (Recall@k).</p><p>MAP@K reflects the accuracy of top ranked authors by a model, and can be computed as mean of AP@K for each papers in the test set. The formula for computing AP@K of a single paper is given as follows.</p><formula xml:id="formula_12">AP @K = K k=1 P (k)/min(L, K)<label>(8)</label></formula><p>where P (k) is the precision at cut-off k in the return list. L is the total number of true authors for this test paper. The Recall@K shows the ratio of true authors being retrieved in the top k return results, and can be computed according to:</p><formula xml:id="formula_13">Recall@K = # of true authors at top K # of total true authors<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Meta-Path Selection Results</head><p>We first report experimental results for path selection since the selected paths are used in the joint training of our model. The candidate paths that we consider are all length-1 and length-2 paths presented in Table <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref> Figure <ref type="figure" target="#fig_7">5a</ref> shows the results of single path performance, i.e., the performance when only a single meta-path is used in network-general embedding. Each dot in the plot indicates the performance of author prediction task for the validation dataset. The horizontal line indicates the performance of task-specific only embedding model. Note that paths are sorted according to their performance, and only paths that can help improve the author identification task are shown in the figure . 
Figure <ref type="figure" target="#fig_7">5b</ref> shows the results of additive path selection, which demonstrate the performance of the combined model when metapaths are added gradually. Each bar in the graph shows performance of the joint model based on specific additive selection of paths. Each single path is added to the network-general embedding sequentially according to their rank in the single path performance experiments. For example, the third bar with label "+P1A" includes three paths: A-P-P, A-P-W, and P-A.</p><p>We observe the author identification performance grow first during the first several additive selection of paths, and then it starts to decrease as we add more paths. This suggests that first several paths are most relevant and helpful, and the latter ones can be less relevant, noisy, and thus they are harmful to use in network-general embedding. It also verifies our hypothesis that heterogeneous network embedding based on different meta paths will lead to different  embeddings. Finally we select the first three paths A-P-P, A-P-W, and P-A in joint learning of the proposed model.</p><p>To further investigate the impact of using different meta paths on learning embeddings for the prediction task, we consider several types of paths: (1) the original length-1 network paths presented by network schema in Figure <ref type="figure" target="#fig_1">2</ref>, (2) the augmented paths by combining all length-1 and length-2 paths, and (3) the selected paths by our procedure.</p><p>Table <ref type="table" target="#tab_3">4</ref> shows the results of different embedding models trained based on pre-given meta paths. We observe that by adding all length-2 paths, the results actually become worse, which might be due to the irrelevant or noisy paths. However, this does not mean that consider augmented paths are unnecessary. Using the greedy selected paths (A-P-P, A-P-W, and P-A) from both length-1 and length-2 paths, the performance of all models can be improved, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance Comparison with Baselines</head><p>Table <ref type="table" target="#tab_4">5</ref> shows the performance comparison between baselines and the proposed method. For both pre-train and network-general model, they do not have access to the task-specific path selection, so original length-1 network paths are used.</p><p>Our method significantly outperforms all baselines, including both supervised feature-based baselines and variants of embedding methods. To our surprise, the task-specific embedding model performs quite badly without pre-trained embedding vectors, significantly lower than other methods. We conjecture this is due to overfitting, and can be largely alleviated by pre-training or joint learning with unsupervised network-general embedding.</p><p>To further examine the superior performance of our method compared with traditional methods, we group the papers by its medium author degrees <ref type="foot" target="#foot_2">4</ref> , and report the results on each groups. Figure <ref type="figure">6</ref> shows that our method outperforms baseline methods in almost all groups of papers, but most significantly in those papers that have less frequent authors. This suggests that our method can better understand authors with fewer links. For traditional feature based methods, it is very difficult to extract useful information/feature for them, but our model can still utilize propagation between authors and learn useful embeddings for them.</p><p>Whole author candidate set. To test in real-world author prediction setting, we also conduct evaluation on the whole candidate set including a million of authors for variants of embedding methods. We only compare embedding methods as supervised feature based methods cannot scale up to whole candidate set. The results are shown in Figure <ref type="figure" target="#fig_8">7</ref>. Due to the use of large candidate set, and thus longer evaluation time, we randomly sample 1000 test papers for a single experiment, and results are averaged over 10 experiments. We observe that, among variants of embedding methods, the combined method consistently outperforms other two variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Studies</head><p>We show two types of case studies to demonstrate the performance differences between our proposed method and variants of  embedding methods. The first type of case study shows the ranking of authors given some terms, which is used to see if the learned embedding nodes make sense. And the second type of case study shows the ranking of authors given information of anonymized paper, which is our original task. Table <ref type="table">6</ref> shows the ranking of authors given the term "variational inference". We find from the results, the returned authors of combined methods are most reasonable (i.e., most likely to be the authors of the queried keyword), followed by general network embedding. And the task-specific embedding model itself sometimes give less reasonable results.</p><p>Table <ref type="table">7</ref> shows the ranked authors of some selected papers. Since double-blind review system is still helpful and in someway protects junior researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Many work has been devoted to mining heterogeneous networks in the past few years <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b21">19]</ref>. To study such networks with multiple types of nodes and/or links, meta paths are proposed and studied <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b21">19]</ref>. Many existing work on mining heterogeneous networks rely on feature engineering <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>, while we adopt embedding methods for automatic feature learning.</p><p>Network embedding also attracts lots of attentions in recent years <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b8">6,</ref><ref type="bibr" target="#b9">7]</ref>. Many of these methods are technically inspired by word embedding <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b17">15]</ref>. Different from traditional graph embedding methods <ref type="bibr" target="#b33">[31]</ref>, such as multi-dimensional scaling <ref type="bibr">[8]</ref>, IsoMap <ref type="bibr" target="#b30">[28]</ref>, LLE <ref type="bibr" target="#b20">[18]</ref>, Laplacian Eigenmap <ref type="bibr" target="#b4">[2]</ref>, the network embeddings are more scalable and shown better performance <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>. Some existing network embedding methods are based on homogeneous network <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b28">26]</ref>, while others are based on heterogeneous networks <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b8">6]</ref>. Our work extends existing embedding methods by leveraging meta paths in heterogeneous networks, and use supervised task to guide the selection of meta paths.</p><p>The problem of author identification has been briefly studied before <ref type="bibr" target="#b13">[11]</ref>. And we also notice KDD Cup 2013 has similar author identification/disambiguation problem <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b35">33]</ref>, where participants are asked to predict which paper is truly written by some author. However, different from the KDD Cup, our setting is different from them in the sense that (1) existing authors are unknown in our double-blind setting, and (2) we consider the reference of the paper, which is one of the most important sources of information. Similar problems in social and information networks are also studied, such as collaboration prediction <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b24">22]</ref>. The major difference between those work and ours is the methodology, their methods are mostly based on heavy feature engineering, while ours adopt automatic feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we study the problem of author identification under double-blind review setting, which is posed as author ranking problem under heterogeneous networks. To (1) embed network under the guidance of author identification task, and (2) better exploit heterogeneous networks with multiple types of nodes and links, we propose a task-guided and path-augmented heterogeneous network embedding model. In our model, nodes are first embedded as vectors in latent feature space. Embeddings are then shared and jointly trained by both task-specific and network-general objectives. We extend the existing unsupervised network embedding to incorporate meta paths in heterogeneous networks, and select paths ac-cording to the author identification task. The guidance is provided for learning network embedding, both explicitly in a joint objective and implicitly in path selection. Our experiments demonstrate the usefulness of meta paths in heterogeneous network embedding, and show that by combining both tasks, our model can obtain significantly better accuracy at identifying the true authors comparing to existing methods. Some potential future work includes (1) author set prediction, where the interactions between authors will be considered in the prediction task, and (2) deeper analysis on text, given the full text of papers is given.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of author identification problem.</figDesc><graphic url="image-1.png" coords="1,357.87,207.67,61.14,81.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Network schema of the heterogeneous bibliographic network. Each node denotes a node type, and each link denotes a link type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Task-specific embedding architecture for author identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distributions of numbers of authors, references and keywords.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, 15 paths in total. As introduced in section 3.3.2, a greedy algorithm involving two stages has been used for path selection: (1) single path performance evaluation, and (2) additive path selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 Figure 6 : 3 ( b )</head><label>1063b</label><figDesc>Figure 6: Ranking results for author nodes of different degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Path selection under task guidance. Path names are shorten. X2Y denotes length-2 path; X1Y denotes length-1 path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance comparison on whole million authors candidate set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (a) Choice of different combining factor between network-specific and network-general objectives. (b) Times of speed up versus the number of threads used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Node statistics</figDesc><table><row><cell></cell><cell>Paper</cell><cell>Author</cell><cell cols="3">keyword Venue Year</cell></row><row><cell cols="3">Train 1,562,139 1,003,836</cell><cell>402,687</cell><cell>7,528</cell><cell>60</cell></row><row><cell>Test</cell><cell>33,644</cell><cell>62,030</cell><cell>41,626</cell><cell>868</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Length-1 link statistics</figDesc><table><row><cell></cell><cell>P-A</cell><cell>P-P</cell><cell>P-V</cell><cell>P-W</cell><cell>P-Y</cell></row><row><cell cols="6">Train 4,554,740 6,122,252 1,562,139 12,817,479 1,562,139</cell></row><row><cell>Test</cell><cell>96,434</cell><cell>388,030</cell><cell>235,508</cell><cell>287,885</cell><cell>235,508</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Length-2 link statistics</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of performance under different network paths (each entry is MAP@3 / Recall@3).</figDesc><table><row><cell></cell><cell cols="2">Network-general Pre-train + Task</cell><cell>Combined</cell></row><row><cell>length-1</cell><cell>0.7563 / 0.7105</cell><cell>0.7722 / 0.7234</cell><cell>0.759 / 0.7133</cell></row><row><cell>length-(1+2)</cell><cell>0.7225 / 0.6847</cell><cell cols="2">0.7489 / 0.7082 0.7385 / 0.6973</cell></row><row><cell>Selected</cell><cell>0.7898 / 0.7379</cell><cell cols="2">0.7914 / 0.7413 0.8113 / 0.7548</cell></row><row><cell cols="4">which again demonstrate the path selection can play an important</cell></row><row><cell cols="3">role in learning task-related embeddings.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Author identification performance comparison.</figDesc><table><row><cell cols="2">Models</cell><cell></cell><cell cols="3">MAP@3 MAP@10 Recall@3 Recall@10</cell></row><row><cell cols="2">LR</cell><cell></cell><cell>0.7289</cell><cell>0.7321</cell><cell>0.6721</cell><cell>0.8209</cell></row><row><cell cols="2">SVM</cell><cell></cell><cell>0.7332</cell><cell>0.7365</cell><cell>0.6748</cell><cell>0.8267</cell></row><row><cell cols="2">RF</cell><cell></cell><cell>0.7509</cell><cell>0.7543</cell><cell>0.6921</cell><cell>0.8381</cell></row><row><cell cols="3">LambdaMart</cell><cell>0.7511</cell><cell>0.7420</cell><cell>0.6869</cell><cell>0.8026</cell></row><row><cell cols="3">Task-specific</cell><cell>0.6876</cell><cell>0.7088</cell><cell>0.6523</cell><cell>0.8298</cell></row><row><cell cols="3">Pre-train+Task.</cell><cell>0.7722</cell><cell>0.7962</cell><cell>0.7234</cell><cell>0.9014</cell></row><row><cell cols="3">Network-general</cell><cell>0.7563</cell><cell>0.7817</cell><cell>0.7105</cell><cell>0.8903</cell></row><row><cell cols="3">Combined</cell><cell>0.8113</cell><cell>0.8309</cell><cell>0.7548</cell><cell>0.9215</cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>model</cell><cell></cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell>Task-specific</cell><cell></cell></row><row><cell>mean(performance)</cell><cell>0.04 0.06 0.08 0.10</cell><cell></cell><cell>Network-general Combined</cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell cols="4">map@3 map@5 map@10 recall@3 recall@5 recall@10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>metrics</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The noise distribution only returns nodes of the same type as specified by end-point of path r.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>for LR, SVM, RF, we use scikit learn implementation, and for LambdaMART, we use XGboost implementation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The author degree is calculated based on the number of papers he/she has published in training data.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank anonymous reviewers for helpful suggestions. This work is partially supported by <rs type="funder">NSF</rs> <rs type="grantNumber">CAREER #1453800</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_56Ccy5J">
					<idno type="grant-number">CAREER #1453800</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the information provided for a paper is quite limited (keywords and limited references), and the number of whole candidate author set is more than one million, many of the true authors may not be presented in the top list. However, our combined method can predict true authors more accurately than other methods. Also, we find that most of the top authors in the returned list are related to the paper's topic and true authors, so it is sensible to consider them as potential authors of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Parameter Study and Efficiency Test</head><p>We study the hyper-parameters ?, which is the trade-off term for combing task-specific embedding and network-general embedding. The result is shown in Figure <ref type="figure">8a</ref>. As we can see that the best performance is obtained when we use ? = 0.8, at which both objectives are combined most appropriately.</p><p>Our model can be trained very efficiently with multi-core parallelization. All our experiments are conducted in a desktop with 4 core i7-5860k CPU and 64G memory. The experiments with embedding methods can be finished in about 10 minutes. To conduct a quantitatively experiment, we compare the times of training speedup versus the number of threads used in Figure <ref type="figure">8b</ref>. It is almost </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>Although there is a severe lack of information about papers (e.g. the medium number of references per paper is 1, only keywords are used, and so on), our embedding based algorithm can still identify true authors with reasonable accuracy at top ranks, even with a million of candidate authors. We believe the model can be further improved by utilizing more complete information, and incorporating with more advanced text understanding techniques. For now and near future, a human expert can still be much more accurate at identifying the authors of a paper that he/she may be very familiar with, but algorithms may do a much better job when a paper is in some less familiar domains.</p><p>An interesting observation from both Figure <ref type="figure">6</ref> and Table <ref type="table">7</ref> is that, authors with higher number of past publications are easier for the algorithm to predict, while the authors with few publication records are substantially harder. This suggests that highly-visible authors may be easier to detect, while relatively junior researchers are harder to be identified. From this perspective, we think the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ground-truth Task-specific Network-general Combined</title>
		<author>
			<persName><forename type="first">Yang</forename><forename type="middle">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu S.-D. Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-D</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar R. Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CatchSync: catching sync. behavior in large directed graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ground-truth Task-specific Network-general Combined</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barab?si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of modern physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS&apos;01)</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS&apos;13)</title>
		<meeting><address><addrLine>Lake Tahoe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th International Conference on Computational Statistics</title>
		<meeting>16th International Conference on Computational Statistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entity embedding-based anomaly detection for heterogeneous categorical events</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI&apos;16</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI&apos;16<address><addrLine>Miami</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multidimensional scaling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kdd cup 2013-author-paper identification challenge: second place team</title>
		<author>
			<persName><forename type="first">D</forename><surname>Efimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 KDD Cup 2013 Workshop</title>
		<meeting>the 2013 KDD Cup 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The myth of the double-blind review?: author identification using only citations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combination of feature engineering and ranking models for paper-author identification in kdd cup 2013</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 KDD Cup 2013 Workshop</title>
		<meeting>the 2013 KDD Cup 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature engineering and tree modeling for author-paper identification challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 KDD Cup 2013 Workshop</title>
		<meeting>the 2013 KDD Cup 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining quality phrases from massive text corpora</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD&apos;15)</title>
		<meeting>2015 ACM SIGMOD International Conference on Management of Data (SIGMOD&apos;15)<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS&apos;13)</title>
		<meeting><address><addrLine>Lake Tahoe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD&apos;14)</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD&apos;14)<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic path based personalized recommendation on weighted heterogeneous information networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM&apos;15)</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management (CIKM&apos;15)<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Co-author relationship prediction in heterogeneous bibliographic networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Social Networks Analysis and Mining (ASONAM&apos;11)</title>
		<meeting><address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: principles and methodologies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">When will it happen?: relationship prediction in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM international conference on web search and data mining (WSDM&apos;12)</title>
		<meeting>the fifth ACM international conference on web search and data mining (WSDM&apos;12)<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2011 International Conference on Very Large Data Bases (VLDB&apos;11)</title>
		<meeting>2011 International Conference on Very Large Data Bases (VLDB&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web (WWW&apos;15)</title>
		<meeting>the 24th International Conference on World Wide Web (WWW&apos;15)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD&apos;08)</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD&apos;08)<address><addrLine>Las Vegas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effect of open peer review on quality of reviews and on reviewers&apos; recommendations: a randomised trial</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Godlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">7175</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An efficient method for generating discrete random variables with general distributions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="256" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on web search and data mining (WSDM&apos;14)</title>
		<meeting>the 7th ACM international conference on web search and data mining (WSDM&apos;14)<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The scorecard solution to the author-paper identification challenge</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 KDD Cup 2013 Workshop</title>
		<meeting>the 2013 KDD Cup 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contextual rule-based feature engineering for author-paper identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 KDD Cup 2013 Workshop</title>
		<meeting>the 2013 KDD Cup 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
