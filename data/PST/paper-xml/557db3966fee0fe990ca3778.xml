<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vector Quantization by Deterministic Annealing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, ZEEE</roleName><forename type="first">Kenneth</forename><surname>Rose</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eitan</forename><surname>Gurewitz</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Caltech Concurrent Computation Program</orgName>
								<orgName type="institution" key="instit2">California Institute of Technology</orgName>
								<address>
									<postCode>91 125</postCode>
									<settlement>Pasadena</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>93106</postCode>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Physics</orgName>
								<orgName type="department" key="dep2">Nuclear Research Centre-Negev</orgName>
								<address>
									<postBox>P.O. Box 9001</postBox>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Northeast Parallel Architectures Center</orgName>
								<address>
									<addrLine>111 College Place</addrLine>
									<postCode>13244</postCode>
									<settlement>Syracuse</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vector Quantization by Deterministic Annealing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A58B8727EF5861AF4782BF254C7F4A9</idno>
					<note type="submission">received June 5, 1990; revised November 27, 1991.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vector quantization</term>
					<term>annealing</term>
					<term>maximum entropy</term>
					<term>clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A deterministic annealing approach is suggested to search for the optimal vector quantizer given a set of training data. The problem is reformulated within a probabilistic framework. No prior knowledge is assumed on the source density, and the principle of maximum entropy is used to obtain the association probabilities at a given average distortion. The corresponding Lagrange multiplier is inversely related to the " temperature" and is used to control the annealing process. In this process, as the temperature is lowered, the system undergoes a sequence of "phase transitions" when existing clusters split naturally, without use of heuristics. The resulting codebook is independent of the codebook used to initialize the iterations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HE PROBLEM of vector quantizer design is that of T finding the set of quantization levels (also called reproduction values or cluster representatives), such that a given criterion for the total distortion is minimized, and is thus an optimization problem. Useful distortion criteria are not convex and have several local optima. Traditional iterative methods, such as the early Lloyd-Max scalar quantizer [l], <ref type="bibr">[2]</ref>, and the commonly used generalization, the Linde-Buzo-Gray (LBG) algorithm <ref type="bibr">[3]</ref>, as well as methods for the closely related cluster analysis problem [4] -[6], are sensitive to the choice of initial configuration. The reason is that these are "descent" methods in the sense that at each iteration the defined distortion is reduced, and they tend to get trapped in a local minimum of the distortion surface. Indeed, many heuristic schemes have been suggested for choosing initial configurations to improve the performance.</p><p>The observation of annealing processes in physical chemistry motivates the use of similar concepts to avoid local minima of the distortion. Certain chemical systems can be driven to their low energy states by annealing which is a gradual reduction of temperature, spending a long time at the vicinity of the phase transition points. In the corresponding probabilistic framework, a Gibbs distribution is defined over the set of all possible configurations, and assigns higher probability to configurations of lower energy. This distribution is parameterized by the temperature, and as the temperature is lowered it becomes more discriminating (concentrates most of the probability in a smaller subset of low energy configurations). At the limit of low temperature it assigns nonzero probability only to global minimum configurations. A known technique for nonconvex optimization is stochastic relaxation or simulated annealing <ref type="bibr">[7]</ref>, based on the Metropolis algorithms <ref type="bibr">[8]</ref> for atomic simulations. A sequence of random moves are generated and the decision to accept a move depends on the probability of the resulting configuration. Stochastic relaxation has been applied to a variety of optimization problems, including vector quantization <ref type="bibr">[9]</ref>, <ref type="bibr">[lo]</ref>. However, one must be very careful with the annealing schedule, the rate at which the temperature is lowered. In their work on image restoration, <ref type="bibr">Geman and Geman [ l l ]</ref> have shown that in theory, the global minimum can be achieved if the schedule obeys T O E l/log n, where n is the number of the current iteration (see also derivation of necessary and sufficient conditions for asymptotic convergence of simulated annealing in <ref type="bibr">[12]</ref>). Such schedules are not realistic in many applications.</p><p>Unlike stochastic relaxation where random moves are made on the given energy surface, deterministic annealing (DA) can be viewed as incorporating the "randomness" into the energy function by extracting properties of the macroscopic system from microscopic averages in much the same way as thermodynamic properties are obtained from statistical mechanics. An effective energy function is obtained and is deterministically optimized at each temperature sequentially, starting at high temperatures and going down. This approach was adopted by various researchers [13], <ref type="bibr">[14]</ref>. Our preliminary work on clustering along these lines was briefly summarized in <ref type="bibr">[15]</ref>, <ref type="bibr">[16]</ref>. In a coming paper, <ref type="bibr">Yair,</ref><ref type="bibr">Zeger,</ref><ref type="bibr">and Gersho [17]</ref> add a stochastic component to Kohonen's self organization method [ 181, and then propose a deterministic approximation to it. This independent work shows remarkable intuition, and the resulting algorithm is in a sense a "sequential" version of "batch" algorithms derivable from our proposed method. Here, however, we derive our approach independently of a particular algorithm by starting with a probabilistic formulation of the problem. This allows a mathematical analysis of the process, which clarifies how DA avoids local minima, and shows the relation to statistical mechanics and phase transitions. In particular, the problems associated with the step-size schedule, where a mathematical analysis is lacking <ref type="bibr">[17], [19]</ref>, are circumvented here, as the 0018-9448/92$03.00 0 1992 IEEE process reaches isothermal equilibrium at each temperature as will be explained in this paper.</p><p>Although strongly motivated by the physical analogy, our approach is formally based on principles of information theory. Jaynes [20] has applied Shannon's theory [21] in his information-theoretic formulation of statistical mechanics. In particular he stated the principle of maximum entropy for statistical inference, which is the basis on which we formulate our probabilistic framework. We thus assume no prior knowledge of the densities of the data (if we have such knowledge then we may have to use the principle of minimum cross entropy [22], [231).</p><p>Probability distributions for associating sample points with representatives are taken to be those which maximize the entropy under the constraint of a given average distortion. These are parameterized by the Lagrange multiplier, which is determined by the constraint. This Lagrange multiplier is inversely related to the "temperature" and enables the process of annealing, An effective distortion (free energy) is derived and minimized to find the most probable set of representatives (reproduction values) at each temperature. As the temperature is lowered, there are stages when it becomes advantageous to increase the number of representatives. These are phase transitions in our physical analogy, and the temperature at which each occurs is the corresponding critical temperature. Such a sequence of phase transitions yields a quantization tree or hierarchical clustering.</p><p>Simulations of differing algorithms based on our approach are given. The first solves the problem of "hard clustering" for a known number of representatives. It is demonstrated how the solution is independent of the initial configuration and escapes local minima. Preliminary results for Gauss-Markov sources are also given. The second does not assume knowledge of the number of representatives, and produces a hierarchy of clustering solutions from which one can obtain a set of quantizers at different distortions. The methods are derived for a large class of distortion measures and demonstrated for the commonly used "sum of squared distances" measure.</p><p>Vector quantizer design, as we define it, is a clustering optimization problem. Because of the obvious equivalence, we are using source coding terminology, and clustering (pattern recognition) terminology, interchangingly . In particular, "cluster centers" or "representatives" are the reproduction values or codevectors, source vectors are in fact "data points" in the training set, etc. The clustering terminology is very useful in the description of the annealing process with its hierarchy of fuzzy clustering solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">A FRAMEWORK FOR PROBABILISTIC VECTOR QUANTIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Association Probabilities</head><p>Although quantization essentially implies using decision boundaries, and associating each point in space with exactly one representative, still it is advantageous to formulate the problem within a probabilistic framework. The main principle is that each point is associated in probability with each cluster. The state of the system is given by the set of probability distributions for associating points with clusters. Related formulations can be found in the fuzzy clustering literature [5], <ref type="bibr">[6]</ref>, where instead of these association probabilities, "fuzzy membership in clusters" is defined. Hard clustering is a marginal special case, where each point is deterministically associated with a single cluster. Even though in the end hard classification will obtain minimal distortion, the probabilistic framework is of utmost importance in the search for the optimal set of representatives, especially when we want to generalize from a given training set of data. It should be noted that the formulation here is truly probabilistic. We shall use "fuzzy terminology" where it seems to convey more intuition. In our formulation, a cluster is a set whose members are determined by the realizations of corresponding random variables. We conveniently define a fuzzy cluster as the fuzzy set, where each sample's degree of membership in the fuzzy cluster is exactly the probability of its belonging to the corresponding nonfuzzy cluster. However, no essential use is made of the theory and tools associated with fuzzy sets.</p><p>Let us first assume that the set of quantization levels (the codebook) Y = { y j } is given (we shall, of course, remove this impossible assumption later). Then the expected distortion (energy) is where d ( x , y j ) is the distortion measure for representing data point x by the vector y j , and P( x E C j ) is the probability that x belongs to the cluster of points represented by y j . The summation is over all the given data points and all clusters. As we do not have any prior knowledge about the data distribution, we apply the principle of maximum entropy. In particular, of all possible probability distributions that yield a given expected distortion, we choose the one that maximizes the entropy. Maximizing the entropy under the constraint (1) we obtain the Gibbs distributions:</p><formula xml:id="formula_0">e -P d ( x , Y,) P ( X E C j ) = (2) z x '</formula><p>where Z, is the partition function</p><formula xml:id="formula_1">(3) Zx = C e-Pd(X,Yk). k</formula><p>For a given codebook, it is further assumed that the probabilities relating different x's to their clusters are independent. Hence, the total partition function for a given fixed set Y , i.e., the normalization for the joint distribution of associations of all points, is</p><formula xml:id="formula_2">Z ( Y ) = nzx. X (4)</formula><p>The parameter /3 is the Lagrange multiplier determined by the given value of (D) in (1). In our physical analogy of annealing, /3 is inversely proportional to the temperature. As /3 gets larger, the temperature is lowered and the associations become less "fuzzy." In fact for /3 = 0 each point is equally associated with all clusters (an extremely "fuzzy" association), while for / 3 --t 03 each point belongs to exactly one cluster with probability one (or more precisely, uniformly distributed over the set of equidistant "nearest" clusters). This already gives us some intuition on the annealing to be described later, by viewing it as a process of gradually decreasing the "fuzziness" of the associations. Another way to look at it is as follows: We start with each training sample equally influencing all the representatives, and gradually localize this influence. At the limit, each sample will only influence the nearest representative, i.e., we converge to the traditional methods such as LBG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Most Probable Set of Representatives and the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eflective Distortion</head><p>In this subsection, we extend the derivation, and drop the assumption that the set of representatives is fixed. Instead of considering the association probability of a data point, we consider the probability of an entire instance. An instance of the system is given by a set of representatives Y = { y j } , and a partition via the set of associations V = { u x j ) , where if x e C j , otherwise u x j = (::</p><formula xml:id="formula_3">where 1 F ( Y ) = -;lOgZ(Y).</formula><p>v F ( Y ) as defined here is exactly the free energy in our statistical mechanics analogy. Maximizing the marginal probability P ( Y ) in ( <ref type="formula">9</ref>) requires minimizing the free energy F. Thus, on the one hand we have the distortion D that is minimized to obtain the optimal quantizer for the training set.</p><p>On the other hand, at a given 0, we have an effective distortion, the free energy F , which is minimized to obtain the optimal quantizer (the most probable set of quantizer levels). Moreover, for / 3 + 00, both D and F are minimized by the same Y , which is given probability 1. In this perspective, solving the optimal quantizer for the training set is a special case of the second problem which is parameterized by /3. Note also that in statistical mechanics, the free energy is the quantity that is minimized at isothermal equilibrium, hence, the clear relation to stochastic relaxation.</p><p>In terms of a given distortion measure, the free energy is by (10)</p><p>The set Y of vectors that optimizes the free energy satisfies</p><formula xml:id="formula_4">-F = O , v j ,</formula><p>where this is shorthand notation for differentiation with respect to each component separately. Differentiating (1 1) we obtain a By the same reasoning as in the previous subsection (i.e., by using the pr -of maximum entropy), the probability of this instance is giv&amp; by the&amp;I%s distribution x J after normalization Next, let us consider the following marginal probability:</p><formula xml:id="formula_5">P ( Y ) = C P ( Y , V V ) , (7)</formula><p>where the summation is performed over all legal associations. A legal association V defines a partition, and is such that each data point is assigned to exactly one cluster. By using (6) and then (3) and (4) we obtain the identity The marginal probability of (7) now becomes This can be rewritten as an explicit Gibbs distribution where the expectation is over the distribution of the jth cluster. Note that if we could interchange the expectation and the differentiation operators we would get that y j minimizes the average cluster distortion, which is the centroid condition in LBG [3]. However, the probability distribution over which we perform the expectation depends on y j at finite /3. There are two special cases, the first is 0 = 0 that yields the uniform distribution. The second is 0 -+ a , where the distribution is piecewise constant (a region of 0 and a region of l), i.e., its derivative vanishes almost everywhere, and the process indeed converges to the LBG algorithm.</p><p>We shall assume that there is a unique y that minimizes or equivalently, there is only one optimal vector which represents the entire data set as a single cluster. It can be shown that this assumption is satisfied by all convex distortion measures (e.g., the squared distance measure). This assumption implies that there is a unique solution to (12) at p = 0, which gives therefore the global minimum. At / 3 &gt; 0, a set Y of vectors satisfying (12) corresponds to a local minimum of the free energy and can be obtained using one's favorite iterative method (e.g., gradient descent). In order to avoid arbitrary local minima depending on the initialization of the iterations, we shall apply the concept of annealing as will be explained in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The vth Law Distortion</head><p>important example of the vth law distortion measure</p><p>In this subsection, we apply the general results to the which is the vth power of the I,, norm. The squared distance distortion, v = 2, is apparently the most extensively used distortion measure, and will be particularly discussed here.</p><p>The necessary condition for minimizing the free energy (12) or ( <ref type="formula">13</ref>) for the ith component of the jth vector becomes This shows that the optimal y j is the cluster's symmetry (or antisymmetry) point of the (v -1)th moment.</p><p>In the case of the squared-distance distortion, (15) is rewritten for all i as c P( X E C j ) ( xyj) = 0, X or in the form of (17) Each representative is interpreted as the center of mass of the fuzzy cluster, or the average over all data points, where to each data point we assign its relative weight in the cluster. This is a generalization of the centroid condition of LBG for the squared-distance distortion, and it is similarly related to basic-ISODATA. The center-of-mass condition is obtained from (17) at 0 -, 00</p><formula xml:id="formula_6">1 l i m y . = - x P-00 ' n j XEC,</formula><p>where nj is the population of (number of data points in) the cluster.</p><p>It is also worth noting that in the squared-distance distortion case, the association probabilities take the Gaussian form e -P I X-U, I Z X</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P ( X E C j ) = (19) III. PHASE TRANSITIONS IN THE ANNEALING PROCESS</head><p>At 0 = 0 all data points are equally associated with all representatives (2). For all distortion measures satisfying our assumption of Section II-B, there is only one solution, and regardless of the number of representatives, they will all converge to the same point, the global minimum. We shall interpret identical representatives as representing the same cluster and consider the set Y of vectors without repetitions.</p><p>The number of distinct representatives is the number of "natural clusters." Thus, at 0 = 0 we actually have one natural cluster which is representable by one representative.</p><p>The single solution of 0 = 0 will be a solution for all 0, but it will change from a stable solution (local minimum) into a nonstable one. Hence, at positive 0 we may have several local minima, and the concept of annealing emerges and can be viewed as tracking the minimum, starting at the global minimum and gradually increasing P.</p><p>As long as the number of vectors is not limited a priori, since we avoid repetitions, the number of natural clusters will emerge at every given 0. At = 0 we have one natural cluster consisting of the entire data set, but at some positive 0, not all vectors will converge to the same point and the natural number of clusters will increase. In other words, the cluster will split into smaller clusters, and will thus undergo a phase transition. Each of the resulting smaller clusters will stay intact until 0 reaches some higher value where it will split into yet smaller clusters. Note that these cluster splits are obtained naturally by increasing 0 (i.e., lowering the temperature), and not by using heuristics! The critical temperature is an extremely important value associated with a phase transition. In the rest of this section, we compute the critical 0 for the first phase transition, and obtain an approximation for the following transitions.</p><p>Assuming the vth law distortion of Section II-C, let us derive the critical value 0, at which the first phase transition occurs. At 0 = 0, we have one cluster represented by the symmetry point of the (v -1)th moment. Without loss of generality, we shall take this point to be the origin. The phase transition occurs when the Hessian at the origin is no longer positive-definite; i. positive-semidefinite matrix</p><formula xml:id="formula_7">1 N x A [ i , i ] = -E l x ( i ) l ' -2 , (28) sgn' [ x ( i ) ] [ x u -( i) -( v -1 ) ~' -( i ) y, ( i) ] 4 ( x ,U,)</formula><p>, and C,, is the covariance matrix of the vectors z defined by k+(X,Yk) ( <ref type="formula">21</ref>)</p><formula xml:id="formula_8">z ( i ) = sgn [ x ( i ) ] 1 x ( i ) 1<label>(29)</label></formula><p>where Next, let us consider the derivative of B, at the origin</p><formula xml:id="formula_9">4 ( &amp; r) = exp [ P V C x v -' ( m ) u ( 4 sgnq x ( m ) l } 9 ( Y = 0 ) = P V I x ( / ) I '-'sgn [ x ( l ) ] ,</formula><p>a BX (22) ayj( <ref type="formula">1</ref>)</p><formula xml:id="formula_10">m</formula><p>and where we also cancelled the common factor exp { -0 C , I x( m) I '} from the numerator and the denominator. For convenience, we rewrite the previous as which implies that at the origin,</p><p>where A , ( i) and B, stand for the numerator and denominator, respectively. Note that at the origin</p><formula xml:id="formula_11">B x ( Y = 0) = n,,</formula><p>where nr is the number of representatives. Therefore, for computing the Hessian at the origin, (32)</p><formula xml:id="formula_12">= P Y X 1 x ( l ) 1 1 x ( i ) 1 '-Isgn [ x ( l ) ] sgn [ x ( i ) ] .<label>(30)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ~, ( i ) =E s g n U [ x ( i ) ] [ x u -' ( i ) -( ~-1 ) x ~-~( i ) y , ( i ) ]</head><p>H . . = -X X n:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>' ( 1 + v P ~, x ~--' ( m ) y ~( m ) s g n ~[ x ( m ) ] } .</head><p>(</p><formula xml:id="formula_13">) = o Noting that sgn'[ x ( i ) ] x ' -' ( i ) = sgn [ x ( i ) ] I x ( i ) I X X<label>25</label></formula><p>is obtained by (20), and discarding terms containing second or higher powers of y , we get ~, ( i )</p><formula xml:id="formula_14">= P Y X sgn'[ x ( i ) ] x U ~-' ( i ) ~, x u -' ( m ) X X uj ( m)sgn' [ X ( m ) ] -( v -1) sgn'[ x ( i)] x ' -~( i) y,( i ) . (<label>26</label></formula><formula xml:id="formula_15">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>This result can be rewritten in matrix-vector notation as</p><formula xml:id="formula_16">C A , = N [ PvC,, -( v -l ) " ] ~, ,<label>(27)</label></formula><p>where N is the number of data points, A is the diagonal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>The Hessian H is the large matrix obtained by placing copies of Hjj on the diagonal, and copies of H j j elsewhere.</p><p>We are interested in analyzing the conditions for positivedefinite H . Noting that C,, is a covariance matrix, and therefore positive-semidefinite, it is quite straightforward to show that H is positive-definite, if and only i f the first term in (31) is. We shall use this claim here, and prove it in the Appendix.</p><p>We thus consider the matrix H I = (v -1)A -PvC,,. Both A and C,, depend only on the data set, not on P, so that varying only modifies the balance between these two fixed matrices. At P = 0, H , is positive-definite since A is (we ignore here pathologies such as det A = 0). It stays positive definite until it reaches the point where its determinant vanishes.</p><p>The critical value for P is thus,</p><formula xml:id="formula_17">(33)</formula><p>where kx is the largest eigenvalue of A-'C,,. Moreover, bifurcation occurs along the eigenvector corresponding to the Hessian's zero eigenvalue. This means that the split will be in the direction of the eigenvector of A-'CZz corresponding to Now that we have derived the critical p for the vth law family, let us see its interpretation for the special case of the squared distortion measure (v = 2). In this case, where A, , , , , is now the largest eigenvalue of C,,. We see that in this case the critical temperature is determined by the variance along the major principal axis of the distribution. Furthermore, the split will be initiated in the direction of this principal axis. Finally, as long as we may neglect intercluster influences, this derivation will hold for the following phase transitions, and every cluster will split at the critical temperature corresponding to its variance.</p><p>We now have an approximate idea of how the annealing process works. As 6 is increased, whenever it reaches a value corresponding to the variance along a cluster's major principal axis, this cluster splits into smaller clusters. These clusters stay intact until 6 reaches values corresponding to their (smaller) variances etc. This also indicates how 6</p><p>relates to the cluster variances in the solution. Note that this description is approximate after the first phase transition, because we neglected intercluster influences on the phase transition. It should be a good approximation when the phase transitions are well spaced. Just as in the physical analogy, the critical moments in the process are the phase transitions. Knowing to predict the next critical 6 may allow us to accelerate the process between phase transitions, while being more careful during the transition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATION RESULTS</head><p>The simulation results relate to two algorithms, one for a fixed number of representatives corresponding to the classical problem of vector quantization, and the other is for hierarchical clustering which demonstrates graphically the phase transitions in the process, as well as the means to generate a hierarchy of quantizers at different rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Fixed Number of Representatives</head><p>This algorithm performs hard clustering, and in fact searches for the optimal quantizer for the training set, given a fixed number of quantization levels. This is a well posed problem, and it is in this context that many traditional techniques are proved to converge to a local optimum. Regardless of the initial configuration, at 6 = 0 all the representatives will be at the center of mass of the entire data set. According to our annealing process we gradually increase 0 and reoptimize by solving (13). The system undergoes a sequence of phase transitions until the number of natural representatives reaches the specified number. At the limit of 6 + 03, the associations (2) become hard and each sample point is associated with exactly one representative. At this limit the algorithm becomes exactly the LBG algorithm, i.e., at each iteration every point is assigned to the "nearest" representative (in the sense of minimal distortion), and then a new set of "centroids," minimizing the cluster average distortion (13), is computed. If the distortion measure is the sum of squared distances, then at the limit it is also the basic ISODATA algorithm.</p><p>The simulation example shows how our DA algorithm avoids a typical local minimum "trap" as compared to LBG or basic ISODATA (the sum of squared distances is taken as the distortion measure). The training set was generated from a normal mixture whose density centers are marked by X in the figures. The output of LBG depends on the choice of initial configuration. In Fig. <ref type="figure" target="#fig_5">1</ref>, part (a) shows the LBG result for an initialization consisting of placing the means on a small circle around the center of mass of the distribution. The final location of the representatives is marked by 0 and the distortion is 9024. The output of the annealing algorithm is shown in Fig. l(b), and is independent of the initial configuration. The final distortion here is 7635. This is just one example of a trap. Many such traps may exist in complex nonstationary source distributions. Clearly, LBG may avoid the trap with the "right" initialization. In fact, our annealing process can be viewed as doing just that: generating a good initial configuration as it approaches temperature zero where it becomes LBG. This process is illustrated in the next subsection.</p><p>While the above example demonstrates a "trap," one may wish to consider the performance for sources of more practical interest. We shall give here some preliminary results for a first-order Gauss-Markov source. For Gauss-Markov sources, it is important to realize that since the random vector distribution is Gaussian, the source density itself does not  Hence, S is positive-definite. 2) If P is not positive-definite, then there exists nonzero w such that wTPw I 0. Construct U using n-tuples of the form uj = a j w , such that Hence, S is not positive-definite. 0</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where P( x E C j ) is as in (2) and (3). Equivalently, we obtain D ( Y , V ) = c uxj d ( x , Y,).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>e., it evolves from a local minimum into a nonstable solution. At 0 = 0 and yk = 0, for all k , (15) becomes Let us use the following binomial series expansion for each component of the distortion where sgn ( ab) is the signum function, and for b + 0 we replace it by sgn (a). Using this when differentiating (1 1) and discarding the O( y 2 ( i)) terms, we get aF -a y j ( i ) --v?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>X</head><label></label><figDesc>h x n (24), (27), and (30) we obtain the sub-Hessian with respect to y, at the origin, A X ( 9 avi(l) . (24) The cross components of the Hessian consist only of the second term in (31); i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Hard clustering results. Data is generated from six Gaussian distributions centered at the location marked by " X " . Calculated cluster centroids are marked by "0". Lines are the decision boundaries. (a) LBG. Distortion is 9024. (b) DA. Distortion is 7635.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 ..</head><label>2</label><figDesc>Fig. 2. Clustering at different phases corresponding to Fig. 3. Lines are equiprobability contours, p = 1/2 in (b), and p = 1/3 elsewhere. (a) 1 cluster (0 = 0), (b) 2 clusters (0 = 0.0049), (c) 3 clusters (0 = 0.0056), (d) 4 clusters (0 = 0.0100), (e) 5 clusters (0 = 0.0156), (f) 6 clusters (0 = 0.0347), and (9) 19 clusters (0 = 0.0605).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>uTS,u = 0. Now,VTSV = UTS,U + v T s * u = u T s , v I O .</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>offer many local minima to trap descent methods. In other words, a long training set that closely approximates the source density, will be quantized almost optimally by LBG. However real signals (and especially nonstationary signals) usually do not have such simple distributions. To demonstrate the advantage of DA that avoids local minima, we consider relatively short training sets, and compute the SNR gains in dB for quantizing the training set itself. The results are summarized in Table <ref type="table">I</ref>.</p><p>We used a zero-mean first-order Gauss-Markov source, with autocorrelation coefficient p = 0.9. A training set of 1000 samples arranged in two-dimensional vectors, was quantized at 3 bits/sample (i.e., 64 codevectors). Fifteen , runs of LBG (for different initial codebooks extracted from the training set) were used to compare with the results of DA. Then, a training set of 2000 samples arranged in fourdimensional vectors was quantized at 1.5 bits/sample (i.e., 64 codevectors). The test was also performed with a Gaussian source ( p = 0).</p><p>More extensive simulations are required to reach an accurate evaluation of the performance for different sources and rates. However, these preliminary results indicate the potential gains of the suggested method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hierarchy by Phase Transitions</head><p>This algorithm produces a "fuzzy clustering" hierarchy from which quantizers of successively higher rates can be obtained. It differs from the previous algorithm in its goal which is to find the most probable set of representatives instead of minimizing the distortion for the training set. As with the previous algorithm, we apply our annealing process, where each phase transition is a transition to a quantizer of higher rate. However, each such quantizer is retained with its corresponding p, so that the result is fuzzy clustering with the underlying Gibbs distribution, and the representatives are the fuzzy cluster "centroids. " Such probabilistic quantizers may have an important role in entropy constrained vector quantization as will be reported in a coming paper.</p><p>The quantizer hierarchy is shown in Fig. <ref type="figure">2</ref>. The training set is generated from a mixture of six normal distributions, and we see the quantizers obtained at different phases. The process starts with one natural cluster containing all the training set (shown in Fig. <ref type="figure">2(a)</ref>). At the first phase transition it splits into two clusters (Fig. <ref type="figure">2(b</ref>)), and passes through a sequence of phase transitions until it reaches the "natural" set of six clusters. The next phase transition results in an "explosion" where all clusters split. This is predictable by the analysis of Section 111. Here we have a set of identical isotropic clusters. By (34), we know that the critical temperature will be the same for all these clusters. Moreover, since their covariance matrices are isotropic (C,, = XI), every vector is an eigenvector, so that the split may be initiated in all directions. A phase diagram is given in Fig. <ref type="figure">3</ref>. It shows the behavior of the average distortion throughout the annealing process, and the number of natural clusters at each phase. Note that to obtain hard clustering solutions for each one of the phases, one has to fix the number of representatives and let 0 + 03. V. CONCLUSION Deterministic annealing is suggested as a new approach to vector quantization. The concept of annealing eliminates the sensitivity to the choice of the initial configuration and yields a probabilistic hierarchical clustering process. The process starts at the global minimum at high temperature and tracks the minimum while lowering the temperature. Although motivated by analogy to statistical mechanics, it is based on principles of information theory. No knowledge is assumed on the underlying probability density, and the maximum entropy distribution is used. The phase transitions observed in this process produce quantizers of increasing rate.</p><p>We wish to draw the attention of the readers to a coming paper <ref type="bibr">[24]</ref> where the formulation is extended to constrained clustering. In particular, it suggests the mass-constrained clustering method, which eliminates a weakness in our algorithm, namely, the dependence on the multiplicity of representatives at a given natural cluster. The resulting DA method, takes the cluster masses (the codevector probabilities) into account. Some preliminary results show the advantages of this modification. Future work is needed in order to apply DA to the problem of entropy-constrained vector quantization, and structurally constrained vector quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors are grateful to R. Meir for helpful discussions. K. Rose wishes to thank E. C. Posner for valuable suggestions, and to acknowledge comments made by R. J. McEliece at an early stage of the work, which triggered the shift from statistical physics analogies to information theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this appendix, we prove the claim we used in Section 111, i.e., that H is positive definite, if and only if HI is.</p><p>Claim: Let Q be a positive-semidefinite n x n matrix. Let S be the kn X kn matrix constructed using n X n submatrices according to if i = j , if i # j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s [ i , j ] = { : , + Q '</head><p>Then S is positive-definite iff P is.</p><p>Proof: Consider the decomposition S = SI + S,, where S l [ i , j ] = Q and Let U be an kn-tuple that we can view as the concatenation of k</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Least-squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
	<note>Reprint of the 1957 paper</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quantizing for minimum distortion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="1960-03">Mar. 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A clustering technique for summarizing multivariate data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="153" to="155" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="57" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A convergence theorem for the fuzzy ISODATA clustering algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Equations of state calculations by fast computing machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1087" to="1091" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic relaxation algorithm for improved vector quantizer design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett. I</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Globally optimal vector quantizer design by stochastic relaxation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vaisey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="310" to="322" />
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Infell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984-11">Nov. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial survey of theory and applications of simulated annealing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hajek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th IEEE Conf. Decision Contr</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An analysis of the elastic net approach to the travelling salesman problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computat</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="348" to="358" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical mechanics as the underlying theory of elastic and neural optimization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Simic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="89" to="103" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deterministic annealing approach to clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gurewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="589" to="594" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical mechanics and phase transitions in clustering</title>
	</analytic>
	<monogr>
		<title level="j">Physical Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="945" to="948" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Competitive learning and soft competition for vector quantizer design</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="294" to="309" />
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient algorithms for designing predictive vector quantizers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Self Organization and Associative Memory</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1984-08">1984. Aug. 1986</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="679" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Information theory and statistical mechanics</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Papers on Probability, Statistics and Statistical Physics</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Rosenkrantz</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weaver</surname></persName>
		</author>
		<title level="m">The Mathematical Theory of Communication</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<title level="m">Information Theory and Statistics</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Dover</publisher>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Axiomatic derivation of the principle of maximum entropy and the principle of minimum crossentropy</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Shore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">K</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gurewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="755" to="760" />
			<date type="published" when="1980-01">Jan. 1980. 1992. 1989</date>
		</imprint>
	</monogr>
	<note>IEEE Tran. Inform. Theory. to appear. 896-898</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
