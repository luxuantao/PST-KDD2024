<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-11">11 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jungil</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juhee</forename><surname>Son</surname></persName>
						</author>
						<title level="a" type="main">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-11">11 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.06103v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel endto-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.</p><p>1 Although there is a text preprocessing step in TTS systems, We herein use preprocessed text interchangeably with the word "text".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text-to-speech (TTS) systems synthesize raw speech waveforms from given text through several components. With the rapid development of deep neural networks, TTS system pipelines have been simplified to two-stage generative modeling apart from text preprocessing such as text normalization and phonemization. The first stage is to produce intermediate speech representations such as melspectrograms <ref type="bibr" target="#b35">(Shen et al., 2018)</ref> or linguistic features (Oord   1 Kakao Enterprise, Seongnam-si, Gyeonggi-do, Republic of Korea 2 School of Computing, KAIST, Daejeon, Republic of Korea.</p><p>Correspondence to: Jaehyeon Kim &lt;jay.xyz@kakaoenterprise.com&gt;.</p><p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). <ref type="bibr">et al., 2016)</ref> from the preprocessed text, 1 and the second stage is to generate raw waveforms conditioned on the intermediate representations <ref type="bibr" target="#b27">(Oord et al., 2016;</ref><ref type="bibr" target="#b14">Kalchbrenner et al., 2018)</ref>. Models at each of the two-stage pipelines have been developed independently.</p><p>Neural network-based autoregressive TTS systems have shown the capability of synthesizing realistic speech <ref type="bibr" target="#b35">(Shen et al., 2018;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>, but their sequential generative process makes it difficult to fully utilize modern parallel processors. To overcome this limitation and improve synthesis speed, several non-autoregressive methods have been proposed. In the text-to-spectrogram generation step, extracting attention maps from pre-trained autoregressive teacher networks <ref type="bibr" target="#b31">(Ren et al., 2019;</ref><ref type="bibr" target="#b28">Peng et al., 2020)</ref> is attempted to decrease the difficulty of learning alignments between text and spectrograms. More recently, likelihood-based methods further eliminate the dependency on external aligners by estimating or learning alignments that maximize the likelihood of target mel-spectrograms <ref type="bibr" target="#b42">(Zeng et al., 2020;</ref><ref type="bibr" target="#b26">Miao et al., 2020;</ref><ref type="bibr" target="#b15">Kim et al., 2020)</ref>. Meanwhile, generative adversarial networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> have been explored in second stage models. GAN-based feed-forward networks with multiple discriminators, each distinguishing samples at different scales or periods, achieve high-quality raw waveform synthesis <ref type="bibr" target="#b19">(Kumar et al., 2019;</ref><ref type="bibr" target="#b1">Bińkowski et al., 2019;</ref><ref type="bibr" target="#b18">Kong et al., 2020)</ref>.</p><p>Despite the progress of parallel TTS systems, two-stage pipelines remain problematic because they require sequential training or fine-tuning <ref type="bibr" target="#b35">(Shen et al., 2018;</ref><ref type="bibr" target="#b41">Weiss et al., 2020)</ref> for high-quality production wherein latter stage models are trained with the generated samples of earlier stage models. In addition, their dependency on predefined intermediate features precludes applying learned hidden representations to obtain further improvements in performance. Recently, several works, i.e., FastSpeech 2s <ref type="bibr" target="#b32">(Ren et al., 2021)</ref> and EATS <ref type="bibr" target="#b6">(Donahue et al., 2021)</ref>, have proposed efficient end-to-end training methods such as training over short audio clips rather than entire waveforms, leveraging a mel-spectrogram decoder to aid text representation learning, and designing a specialized spectrogram loss to relax lengthmismatch between target and generated speech. However, despite potentially improving performance by utilizing the learned representations, their synthesis quality lags behind two-stage systems.</p><p>In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Using a variational autoencoder (VAE) <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2014)</ref>, we connect two modules of TTS systems through latent variables to enable efficient end-to-end learning. To improve the expressive power of our method so that high-quality speech waveforms can be synthesized, we apply normalizing flows to our conditional prior distribution and adversarial training on the waveform domain. In addition to generating fine-grained audio, it is important for TTS systems to express the one-to-many relationship in which text input can be spoken in multiple ways with different variations (e.g., pitch and duration). To tackle the one-to-many problem, we also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method captures speech variations that cannot be represented by text.</p><p>Our method obtains more natural sounding speech and higher sampling efficiency than the best publicly available TTS system, Glow-TTS <ref type="bibr" target="#b15">(Kim et al., 2020)</ref> with HiFi-GAN <ref type="bibr" target="#b18">(Kong et al., 2020)</ref>. We make both our demo page and source-code publicly available.<ref type="foot" target="#foot_0">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we explain our proposed method and the architecture of it. The proposed method is mostly described in the first three subsections: a conditional VAE formulation; alignment estimation derived from variational inference; adversarial training for improving synthesis quality. The overall architecture is described at the end of this section. Figures <ref type="figure" target="#fig_0">1a and 1b</ref> show the training and inference procedures of our method, respectively. From now on, we will refer to our method as Variational Inference with adversarial learning for end-to-end Text-to-Speech (VITS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Variational Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">OVERVIEW</head><p>VITS can be expressed as a conditional VAE with the objective of maximizing the variational lower bound, also called the evidence lower bound (ELBO), of the intractable marginal log-likelihood of data log p θ (x|c):</p><formula xml:id="formula_0">log p θ (x|c) ≥ E q φ (z|x) log p θ (x|z)−log q φ (z|x) p θ (z|c)<label>(1)</label></formula><p>where p θ (z|c) denotes a prior distribution of the latent variables z given condition c, p θ (x|z) is the likelihood function of a data point x, and q φ (z|x) is an approximate posterior distribution. The training loss is then the negative ELBO, which can be viewed as the sum of reconstruction loss − log p θ (x|z) and KL divergence log q φ (z|x) − log p θ (z|c), where z ∼ q φ (z|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">RECONSTRUCTION LOSS</head><p>As a target data point in the reconstruction loss, we use a mel-spectrogram instead of a raw waveform, denoted by x mel . We upsample the latent variables z to the waveform domain ŷ through a decoder and transform ŷ to the melspectrogram domain xmel . Then the L 1 loss between the predicted and target mel-spectrogram is used as the reconstruction loss:</p><formula xml:id="formula_1">L recon = x mel − xmel 1 (2)</formula><p>This can be viewed as maximum likelihood estimation assuming a Laplace distribution for the data distribution and ignoring constant terms. We define the reconstruction loss in the mel-spectrogram domain to improve the perceptual quality by using a mel-scale that approximates the response of the human auditory system. Note that the mel-spectrogram estimation from a raw waveform does not require trainable parameters as it only uses STFT and linear projection onto the mel-scale. Furthermore, the estimation is only employed during training, not inference. In practice, we do not upsample the whole latent variables z but use partial sequences as an input for the decoder, which is the windowed generator training used for efficient end-to-end training <ref type="bibr" target="#b32">(Ren et al., 2021;</ref><ref type="bibr" target="#b6">Donahue et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">KL-DIVERGENCE</head><p>The input condition of the prior encoder c is composed of phonemes c text extracted from text and an alignment A between phonemes and latent variables. The alignment is a hard monotonic attention matrix with |c text | × |z| dimensions representing how long each input phoneme expands to be time-aligned with the target speech. Because there are no ground truth labels for the alignment, we must estimate the alignment at each training iteration, which we will discuss in Section 2.2.1. In our problem setting, we aim to provide more high-resolution information for the posterior encoder. We, therefore, use the linear-scale spectrogram of target speech x lin as input rather than the mel-spectrogram. Note that the modified input does not violate the properties of variational inference. The KL divergence is then: The factorized normal distribution is used to parameterize our prior and posterior encoders. We found that increasing the expressiveness of the prior distribution is important for generating realistic samples. We, therefore, apply a normalizing flow f θ <ref type="bibr" target="#b33">(Rezende &amp; Mohamed, 2015)</ref>, which allows an invertible transformation of a simple distribution into a more complex distribution following the rule of change-ofvariables, on top of the factorized normal prior distribution:</p><formula xml:id="formula_2">L kl = log q φ (z|x lin ) − log p θ (z|c text , A), (3) z ∼ q φ (z|x lin ) = N (z; µ φ (x lin ), σ φ (x lin ))</formula><formula xml:id="formula_3">p θ (z|c) = N (f θ (z); µ θ (c), σ θ (c)) det ∂f θ (z) ∂z ,<label>(4)</label></formula><formula xml:id="formula_4">c = [c text , A]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Alignment Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">MONOTONIC ALIGNMENT SEARCH</head><p>To estimate an alignment A between input text and target speech, we adopt Monotonic Alignment Search (MAS) <ref type="bibr" target="#b15">(Kim et al., 2020)</ref>, a method to search an alignment that maximizes the likelihood of data parameterized by a normalizing flow f :</p><formula xml:id="formula_5">A = arg max Â log p(x|c text , Â) = arg max Â log N (f (x); µ(c text , Â), σ(c text , Â)) (5)</formula><p>where the candidate alignments are restricted to be monotonic and non-skipping following the fact that humans read text in order without skipping any words. To find the optimum alignment, <ref type="bibr" target="#b15">Kim et al. (2020)</ref> use dynamic programming. Applying MAS directly in our setting is difficult because our objective is the ELBO, not the exact log-likelihood. We, therefore, redefine MAS to find an alignment that maximizes the ELBO, which reduces to finding an alignment that maximizes the log-likelihood of the latent variables z:</p><formula xml:id="formula_6">arg max Â log p θ (x mel |z) − log q φ (z|x lin ) p θ (z|c text , Â) = arg max Â log p θ (z|c text , Â) = log N (f θ (z); µ θ (c text , Â), σ θ (c text , Â))<label>(6)</label></formula><p>Due to the resemblance of Equation <ref type="formula">5</ref>to Equation <ref type="formula" target="#formula_6">6</ref>, we can use the original MAS implementation without modification. Appendix A includes pseudocode for MAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">DURATION PREDICTION FROM TEXT</head><p>We can calculate the duration of each input token d i by summing all the columns in each row of the estimated alignment j A i,j . The duration could be used to train a deterministic duration predictor, as proposed in previous work <ref type="bibr" target="#b15">(Kim et al., 2020)</ref>, but it cannot express the way a person utters at different speaking rates each time. To generate human-like rhythms of speech, we design a stochastic duration predictor so that its samples follow the duration distribution of given phonemes. The stochastic duration predictor is a flow-based generative model that is typically trained via maximum likelihood estimation. The direct application of maximum likelihood estimation, however, is difficult because the duration of each input phoneme is 1) a discrete integer, which needs to be dequantized for using continuous normalizing flows, and 2) a scalar, which prevents high-dimensional transformation due to invertibility. We apply variational dequantization <ref type="bibr" target="#b10">(Ho et al., 2019)</ref> and variational data augmentation <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> to solve these problems. To be specific, we introduce two random variables u and ν, which have the same time resolution and dimension as that of the duration sequence d, for variational dequatization and variational data augmentation, respectively. We restrict the support of u to be [0, 1) so that the difference d − u becomes a sequence of positive real numbers, and we concatenate ν and d channel-wise to make a higher dimensional latent representation. We sample the two variables through an approximate posterior distribution q φ (u, ν|d, c text ). The resulting objective is a variational lower bound of the log-likelihood of the phoneme duration:</p><formula xml:id="formula_7">log p θ (d|c text ) ≥ E q φ (u,ν|d,ctext) log p θ (d − u, ν|c text ) q φ (u, ν|d, c text )<label>(7)</label></formula><p>The training loss L dur is then the negative variational lower bound. We apply the stop gradient operator (van den <ref type="bibr" target="#b38">Oord et al., 2017)</ref>, which prevents back-propagating the gradient of inputs, to the input conditions so that the training of the duration predictor does not affect that of other modules.</p><p>The sampling procedure is relatively simple; the phoneme duration is sampled from random noise through the inverse transformation of the stochastic duration predictor, and then it is converted to integers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Adversarial Training</head><p>To adopt adversarial training in our learning system, we add a discriminator D that distinguishes between the output generated by the decoder G and the ground truth waveform y.</p><p>In this work, we use two types of loss successfully applied in speech synthesis; the least-squares loss function <ref type="bibr" target="#b25">(Mao et al., 2017)</ref> for adversarial training, and the additional featurematching loss <ref type="bibr" target="#b20">(Larsen et al., 2016)</ref> for training the generator:</p><formula xml:id="formula_8">L adv (D) = E (y,z) (D(y) − 1) 2 + (D(G(z))) 2 , (8) L adv (G) = E z (D(G(z)) − 1) 2 ,<label>(9)</label></formula><formula xml:id="formula_9">L f m (G) = E (y,z) T l=1 1 N l D l (y) − D l (G(z)) 1 (10)</formula><p>where T denotes the total number of layers in the discriminator and D l outputs the feature map of the l-th layer of the discriminator with N l number of features. Notably, the feature matching loss can be seen as reconstruction loss that is measured in the hidden layers of the discriminator suggested as an alternative to the element-wise reconstruction loss of VAEs <ref type="bibr" target="#b20">(Larsen et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Final Loss</head><p>With the combination of VAE and GAN training, the total loss for training our conditional VAE can be expressed as follows:</p><formula xml:id="formula_10">L vae = L recon + L kl + L dur + L adv (G) + L f m (G) (11) 2.5. Model Architecture</formula><p>The overall architecture of the proposed model consists of a posterior encoder, prior encoder, decoder, discriminator, and stochastic duration predictor. The posterior encoder and discriminator are only used for training, not for inference.</p><p>Architectural details are available in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">POSTERIOR ENCODER</head><p>For the posterior encoder, we use the non-causal WaveNet residual blocks used in WaveGlow <ref type="bibr" target="#b30">(Prenger et al., 2019)</ref> and Glow-TTS <ref type="bibr" target="#b15">(Kim et al., 2020)</ref>. A WaveNet residual block consists of layers of dilated convolutions with a gated activation unit and skip connection. The linear projection layer above the blocks produces the mean and variance of the normal posterior distribution. For the multi-speaker case, we use global conditioning <ref type="bibr" target="#b27">(Oord et al., 2016)</ref> in residual blocks to add speaker embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">PRIOR ENCODER</head><p>The prior encoder consists of a text encoder that processes the input phonemes c text and a normalizing flow f θ that improves the flexibility of the prior distribution. The text encoder is a transformer encoder <ref type="bibr" target="#b39">(Vaswani et al., 2017)</ref> that uses relative positional representation <ref type="bibr" target="#b34">(Shaw et al., 2018)</ref> instead of absolute positional encoding. We can obtain the hidden representation h text from c text through the text encoder and a linear projection layer above the text encoder that produces the mean and variance used for constructing the prior distribution. The normalizing flow is a stack of affine coupling layers <ref type="bibr" target="#b5">(Dinh et al., 2017)</ref> consisting of a stack of WaveNet residual blocks. For simplicity, we design the normalizing flow to be a volume-preserving transformation with the Jacobian determinant of one. For the multispeaker setting, we add speaker embedding to the residual blocks in the normalizing flow through global conditioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.">DECODER</head><p>The decoder is essentially the HiFi-GAN V1 generator <ref type="bibr" target="#b18">(Kong et al., 2020)</ref>. It is composed of a stack of trans-posed convolutions, each of which is followed by a multireceptive field fusion module (MRF). The output of the MRF is the sum of the output of residual blocks that have different receptive field sizes. For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input latent variables z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.4.">DISCRIMINATOR</head><p>We follow the discriminator architecture of the multi-period discriminator proposed in HiFi-GAN <ref type="bibr" target="#b18">(Kong et al., 2020)</ref>. The multi-period discriminator is a mixture of Markovian window-based sub-discriminators <ref type="bibr" target="#b19">(Kumar et al., 2019)</ref>, each of which operates on different periodic patterns of input waveforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.5.">STOCHASTIC DURATION PREDICTOR</head><p>The stochastic duration predictor estimates the distribution of phoneme duration from a conditional input h text .</p><p>For the efficient parameterization of the stochastic duration predictor, we stack residual blocks with dilated and depth-separable convolutional layers. We also apply neural spline flows <ref type="bibr" target="#b7">(Durkan et al., 2019)</ref>, which take the form of invertible nonlinear transformations by using monotonic rational-quadratic splines, to coupling layers. Neural spline flows improve transformation expressiveness with a similar number of parameters compared to commonly used affine coupling layers. For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input h text .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We conducted experiments on two different datasets. We used the LJ Speech dataset <ref type="bibr" target="#b12">(Ito, 2017)</ref> for comparison with other publicly available models and the VCTK dataset <ref type="bibr" target="#b40">(Veaux et al., 2017)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing</head><p>We use linear spectrograms which can be obtained from raw waveforms through the Short-time Fourier transform (STFT), as input of the posterior encoder. The FFT size, window size and hop size of the transform are set to 1024, 1024 and 256, respectively. We use 80 bands mel-scale spectrograms for reconstruction loss, which is obtained by applying a mel-filterbank to linear spectrograms.</p><p>We use International Phonetic Alphabet (IPA) sequences as input to the prior encoder. We convert text sequences to IPA phoneme sequences using open-source software <ref type="bibr" target="#b0">(Bernard, 2021)</ref>, and the converted sequences are interspersed with a blank token following the implementation of Glow-TTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>The networks are trained using the AdamW optimizer <ref type="bibr" target="#b23">(Loshchilov &amp; Hutter, 2019)</ref> with β 1 = 0.8, β 2 = 0.99 and weight decay λ = 0.01. The learning rate decay is scheduled by a 0.999 1/8 factor in every epoch with an initial learning rate of 2 × 10 −4 . Following previous work <ref type="bibr" target="#b32">(Ren et al., 2021;</ref><ref type="bibr" target="#b6">Donahue et al., 2021)</ref>, we adopt the windowed generator training, a method of generating only a part of raw waveforms to reduce the training time and memory usage during training. We randomly extract segments of latent representations with a window size of 32 to feed to the decoder instead of feeding entire latent representations and also extract the corresponding audio segments from the ground truth raw waveforms as training targets. We use mixed precision training on 4 NVIDIA V100 GPUs. The batch size is set to 64 per GPU and the model is trained up to 800k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experimental Setup for Comparison</head><p>We compared our model with the best publicly available models. We used Tacotron 2, an autoregressive model, and Glow-TTS, a flow-based non-autoregressive model, as first stage models and HiFi-GAN as a second stage model. We used their public implementations and pre-trained weights. 3  Since a two-stage TTS system can theoretically achieve higher synthesis quality through sequential training, we included the fine-tuned HiFi-GAN up to 100k steps with the predicted outputs from the first stage models. We empirically found that fine-tuning HiFi-GAN with the generated mel-spectrograms from Tacotron 2 under teacher-forcing mode, led to better quality for both Tacotron 2 and Glow-TTS than fine-tuning with the generated mel-spectrograms from Glow-TTS, so we appended the better fine-tuned HiFi- 3 The implementations are as follows: Tacotron 2 : https://github.com/NVIDIA/tacotron2 Glow-TTS : https://github.com/jaywalnut310/glow-tts HiFi-GAN : https://github.com/jik876/hifi-gan GAN to both Tacotron 2 and Glow-TTS.</p><p>As each model has a degree of randomness during sampling, we fixed hyper-parameters that controls the randomness of each model throughout our experiments. The probability of dropout in the pre-net of Tactron 2 was set to 0.5. For Glow-TTS, the standard deviation of the prior distribution was set to 0.333. For VITS, the standard deviation of input noise of the stochastic duration predictor was set to 0.8 and we multiplied a scale factor of 0.667 to the standard deviation of the prior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Speech Synthesis Quality</head><p>We conducted crowd-sourced MOS tests to evaluate the quality. Raters listened to randomly selected audio samples, and rated their naturalness on a 5 point scale from 1 to 5. Raters were allowed to evaluate each audio sample once, and we normalized all the audio clips to avoid the effect of amplitude differences on the score. All of the quality assessments in this work were conducted in this manner.</p><p>The evaluation results are shown in Table <ref type="table" target="#tab_1">1</ref>. VITS outperforms other TTS systems and achieves a similar MOS to that of ground truth. The VITS (DDP), which employs the same deterministic duration predictor architecture used in Glow-TTS rather than the stochastic duration predictor, scores the second-highest among TTS systems in the MOS evaluation. These results imply that 1) the stochastic duration predictor generates more realistic phoneme duration than the deterministic duration predictor and 2) our end-to-end training method is an effective way to make better samples than other TTS models even if maintaining the similar duration predictor architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalization to Multi-Speaker Text-to-Speech</head><p>To verify that our model can learn and express diverse speech characteristics, we compared our model to Tacotron 2, Glow-TTS and HiFi-GAN, which showed the ability to extend to multi-speaker speech synthesis <ref type="bibr" target="#b13">(Jia et al., 2018;</ref><ref type="bibr" target="#b15">Kim et al., 2020;</ref><ref type="bibr" target="#b18">Kong et al., 2020)</ref>. We trained the models on the VCTK dataset. We added speaker embedding to our model as described in Section 2.5. For Tacotron 2, we broadcasted speaker embedding and concatenated it with the encoder output, and for Glow-TTS, we applied the global conditioning following the previous work. The evaluation method is the same as that described in Section 4.1.</p><p>As shown in Table <ref type="table">3</ref>, our model achieves a higher MOS than the other models. This demonstrates that our model learns and expresses various speech characteristics in an end-to-end manner.</p><p>Table <ref type="table">3</ref>. Comparison of evaluated MOS with 95% confidence intervals on the VCTK dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model MOS (CI)</head><p>Ground Truth 4.38 (±0.07) Tacotron 2 + HiFi-GAN 3.14 (±0.09) Tacotron 2 + HiFi-GAN (Fine-tuned) 3.19 (±0.09) Glow-TTS + HiFi-GAN 3.76 (±0.07) Glow-TTS + HiFi-GAN (Fine-tuned) 3.82 (±0.07) VITS 4.38 (±0.06)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Speech Variation</head><p>We verified how many different lengths of speech the stochastic duration predictor produces, and how many different speech characteristics the synthesized samples have.  Similar to <ref type="bibr" target="#b37">Valle et al. (2021)</ref>, all samples here were generated from a sentence "How much variation is there?".</p><p>Figure <ref type="figure" target="#fig_2">2a</ref> shows histograms of the lengths of 100 generated utterances from each model. While Glow-TTS generates only fixed-length utterances due to the deterministic duration predictor, samples from our model follow a similar length distribution to that of Tacotron 2. Figure <ref type="figure" target="#fig_2">2b</ref> shows the lengths of 100 utterances generated with each of five speaker identities from our model in the multi-speaker setting, implying that the model learns the speaker-dependent phoneme duration. F0 contours of 10 utterances extracted with the YIN algorithm <ref type="bibr" target="#b4">(De Cheveigné &amp; Kawahara, 2002)</ref> in Figure <ref type="figure" target="#fig_3">3</ref> shows that our model generates speech with diverse pitches and rhythms, and five samples generated with each of different speaker identities in Figure <ref type="figure" target="#fig_3">3d</ref> demonstrates our model expresses very different lengths and pitches of speech for each speaker identity. Note that Glow-TTS could increase the diversity of pitch by increasing the standard deviation of the prior distribution, but on the contrary, it could lower the synthesis quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Synthesis Speed</head><p>We compared the synthesis speed of our model with a parallel two-stage TTS system, Glow-TTS and HiFi-GAN. We measured the synchronized elapsed time over the entire process to generate raw waveforms from phoneme sequences with 100 sentences randomly selected from the test set of the LJ Speech dataset. We used a single NVIDIA V100 GPU with a batch size of 1. The results are shown in Table <ref type="table" target="#tab_3">4</ref>. Since our model does not require modules for generating predefined intermediate representations, its sampling efficiency and speed are greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">End-to-End Text-to-Speech</head><p>Currently, neural TTS models with a two-stage pipeline can synthesize human-like speech <ref type="bibr" target="#b27">(Oord et al., 2016;</ref><ref type="bibr" target="#b29">Ping et al., 2018;</ref><ref type="bibr" target="#b35">Shen et al., 2018)</ref>. However, they typically require vocoders trained or fine-tuned with first stage model output, which causes training and deployment inefficiency. They are also unable to reap the potential benefits of an end-toend approach that can use learned hidden representations rather than predefined intermediate features. Recently, single-stage end-to-end TTS models have been proposed to tackle the more challenging task of generating raw waveforms, which contain richer information (e.g., high-frequency response and phase) than mel-spectrograms, directly from text. FastSpeech 2s <ref type="bibr" target="#b32">(Ren et al., 2021)</ref> is an extension of FastSpeech 2 that enables end-to-end parallel generation by adopting adversarial training and an auxiliary mel-spectrogram decoder that helps learn text representations. However, to resolve the one-to-many problem, FastSpeech 2s must extract phoneme duration, pitch, and energy from speech used as input conditions in training. EATS <ref type="bibr" target="#b6">(Donahue et al., 2021)</ref> employs adversarial training as well and a differentiable alignment scheme. To resolve the length mismatch problem between generated and target speech, EATS adopts soft dynamic time warping loss that is calculated by dynamic programming. Wave Tacotron <ref type="bibr" target="#b41">(Weiss et al., 2020)</ref> combines normalizing flows with Tacotron 2 for an end-to-end structure but remains autoregressive. The audio quality of all the aforementioned end-to-end TTS models is less than that of two-stage models.</p><p>Unlike the aforementioned end-to-end models, by utilizing a conditional VAE, our model 1) learns to synthesize raw waveforms directly from text without requiring additional input conditions, 2) uses a dynamic programming method, MAS, to search the optimal alignment rather than to calculate loss, 3) generates samples in parallel, and 4) outperforms the best publicly available two-stage models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Variational Autoencoders</head><p>VAEs <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2014)</ref> are one of the most widely used likelihood-based deep generative models. We adopt a conditional VAE to a TTS system. A conditional VAE is a conditional generative model where the observed conditions modulate the prior distribution of latent variables used to generate outputs. In speech synthesis, <ref type="bibr" target="#b11">Hsu et al. (2019)</ref> and <ref type="bibr" target="#b43">Zhang et al. (2019)</ref> combine Tacotron 2 and VAEs to learn speech style and prosody. BVAE-TTS <ref type="bibr" target="#b21">(Lee et al., 2021)</ref> generates mel-spectrograms in parallel based on a bidirectional VAE <ref type="bibr" target="#b17">(Kingma et al., 2016)</ref>. Unlike the previous works that applied VAEs to first stage models, we adopt a VAE to a parallel end-to-end TTS system. <ref type="bibr" target="#b33">Rezende &amp; Mohamed (2015)</ref>, <ref type="bibr" target="#b3">Chen et al. (2017)</ref> and <ref type="bibr" target="#b44">Ziegler &amp; Rush (2019)</ref> improve VAE performance by enhancing the expressive power of prior and posterior distribution with normalizing flows. To improve the representation power of the prior distribution, we add normalizing flows to our conditional prior network, leading to the generation of more realistic samples.</p><p>Similar to our work, <ref type="bibr" target="#b24">Ma et al. (2019)</ref> proposed a conditional VAE with normalizing flows in a conditional prior network for non-autoregressive neural machine translation, FlowSeq. However, the fact that our model can explicitly align a latent sequence with the source sequence differs from FlowSeq, which needs to learn implicit alignment through attention mechanisms. Our model removes the burden of transforming the latent sequence into standard normal random variables by matching the latent sequence with the time-aligned source sequence via MAS, which allows for simpler architecture of normalizing flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Duration Prediction in Non-Autoregressive</head><p>Text-to-Speech</p><p>Autoregressive TTS models <ref type="bibr" target="#b36">(Taigman et al., 2018;</ref><ref type="bibr" target="#b35">Shen et al., 2018;</ref><ref type="bibr" target="#b37">Valle et al., 2021)</ref> generate diverse speech with different rhythms through their autoregressive and several tricks including maintaining dropout probability during inference and priming <ref type="bibr" target="#b9">(Graves, 2013)</ref>. Parallel TTS models <ref type="bibr" target="#b31">(Ren et al., 2019;</ref><ref type="bibr" target="#b28">Peng et al., 2020;</ref><ref type="bibr" target="#b15">Kim et al., 2020;</ref><ref type="bibr" target="#b32">Ren et al., 2021;</ref><ref type="bibr" target="#b21">Lee et al., 2021)</ref>, on the other hand, have been relied on deterministic duration prediction. It is because parallel models have to predict target phoneme duration or the total length of target speech in one feed-forward path, which makes it hard to capture the correlated joint distribution of speech rhythms. In this work, we suggest a flow-based stochastic duration predictor that learns the joint distribution of the estimated phoneme duration, resulting in the generation of diverse speech rhythms in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed a parallel TTS system, VITS, that can learn and generate in an end-to-end manner. We further introduced the stochastic duration predictor to express diverse rhythms of speech. The resulting system synthesizes natural sounding speech waveforms directly from text, without having to go through predefined intermediate speech representations. Our experimental results show that our method outperforms two-stage TTS systems and achieves close to human quality. We hope the proposed method will be used in many speech synthesis tasks, where two-stage TTS systems have been used, to achieve performance improvement and enjoy the simplified training procedure. We also want to point out that even though our method integrates two separated generative pipelines in TTS systems, there remains a problem of text preprocessing. Investigating self-supervised learning of language representations could be a possible direction for removing the text preprocessing step. We will release our source-code and pre-trained models to facilitate research in plenty of future directions.</p><p>HiFi-GAN, respectively, except that we use different input dimension for the decoder and append a sub-discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Prior Encoder and Posterior Encoder</head><p>The normalizing flow in the prior encoder is a stack of four affine coupling layers, each coupling layer consisting of four WaveNet residual blocks. As we restrict the affine coupling layers to be volume-preserving transformations, the coupling layers do not produce scale parameters.</p><p>The posterior encoder, consisting of 16 WaveNet residual blocks, takes linear-scale log magnitude spectrograms and produce latent variables with 192 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Decoder and Discriminator</head><p>The input of our decoder is latent variables generated from the prior or posterior encoders, so the input channel size of the decoder is 192. For the last convolutional layer of the decoder, we remove a bias parameter, as it causes unstable gradient scales during mixed precision training.</p><p>For the discriminator, HiFi-GAN uses the multi-period discriminator containing five sub-discriminators with periods <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">11]</ref> and the multi-scale discriminator containing three sub-discriminators. To improve training efficiency, we leave only the first sub-discriminator of the multi-scale discriminator that operates on raw waveforms and discard two sub-discriminators operating on average-pooled waveforms. The resultant discriminator can be seen as the multi-period discriminator with periods <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">11]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Stochastic Duration Predictor</head><p>Figures 5a and 5b show the training and inference procedures of the stochastic duration predictor, respectively. The main building block of the stochastic duration predictor is the dilated and depth-wise separable convolutional (DDSConv) residual block as in Figure <ref type="figure" target="#fig_4">5c</ref>. Each convolutional layer in DDSConv blocks is followed by a layer normalization layer and GELU activation function. We choose to use dilated and depth-wise separable convolutional layers for improving parameter efficiency while maintaining large receptive field size.</p><p>The posterior encoder and normalizing flow module in the duration predictor are flow-based neural networks and have the similar architecture. The difference is that the posterior encoder transforms a Gaussian noise sequence into two random variables ν and u to express the approximate posterior distribution q φ (u, ν|d, c text ), and the normalizing flow module transforms d − u and ν into a Gaussian noise sequence to express the log-likelihood of the augmented and dequantized data log p θ (d − u, ν|c text ) as described in Section 2.2.2.</p><p>All input conditions are processed through condition encoders, each consisting of two 1x1 convolutional layers and a DDSConv residual block. The posterior encoder and normalizing flow module have four coupling layers of neural spline flows. Each coupling layer first processes input and input conditions through a DDSConv block and produces 29-channel parameters that are used to construct 10 rational-quadratic functions. We set the hidden dimension of all coupling layers and condition encoders to 192. Figure <ref type="figure" target="#fig_5">6a</ref> and 6b show the architecture of a condition encoder and a coupling layer used in the stochastic duration predictor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Side-by-Side Evaluation</head><p>We conducted 7-point Comparative Mean Score (CMOS) evaluation between VITS and the ground truth through 500 ratings on 50 items. Our model achieved -0.106 and -0.270 CMOS on the LJ Speech and the VCTK datasets, respectively, as in Table <ref type="table">5</ref>. It indicates that even though our model outperforms the best publicly available TTS system, Glow-TTS and HiFi-GAN, and achieves a comparable score to ground truth in MOS evaluation, there remains a small preference of raters towards the ground truth over our model.</p><p>Table <ref type="table">5</ref>. Evaluated CMOS of VITS compared to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset CMOS</head><p>LJ Speech -0.106 VCTK -0.262</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Voice Conversion</head><p>In the multi-speaker setting, we do not provide speaker identities into the text encoder, which makes the latent variables estimated from the text encoder learn speaker-independent representations. Using the speaker-independent representations, we can transform an audio recording of one speaker into a voice of another speaker. For a given speaker identity s and an utterance of the speaker, we can attain a linear spectrogram x lin from the corresponding utterance audio. We can transform x lin into a speaker-independent representation e through the posterior encoder and the normalizing flow in the prior encoder:</p><p>z ∼ q φ (z|x lin , s) (12) e = f θ (z|s) (13)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. System diagram depicting (a) training procedure and (b) inference procedure. The proposed model can be viewed as a conditional VAE; a posterior encoder, decoder, and conditional prior (green blocks: a normalizing flow, linear projection layer, and text encoder) with a flow-based stochastic duration predictor.</figDesc><graphic url="image-4.png" coords="3,231.60,38.68,240.02,285.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Comparison of sample duration. Glow-TTS only provides a single value due to the deterministic duration predictor. (b) Comparison of sample duration in different speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Sample duration in seconds on (a) the LJ Speech dataset and (b) the VCTK dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Pitch tracks for the utterance "How much variation is there?". Samples are generated from (a) VITS, (b) Tacotron 2, and (c) Glow-TTS in the single speaker setting and from (d) VITS in the multi-speaker setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Block diagram depicting (a) training procedure and (b) inference procedure of the stochastic duration predictor. The main building block of the stochastic duration predictor is (c) the dilated and depth-wise separable convolutional residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The architecture of (a) condition encoder and (b) coupling layer used in the stochastic duration predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to verify whether our model can learn and express diverse speech characteristics. The LJ Speech dataset consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. The audio format is 16-bit PCM with a sample rate of 22 kHz, and we used it without any manipulation. We randomly split the dataset into a training set (12,500 samples), validation set (100 samples), and test set (500 samples). The VCTK dataset consists of approximately 44,000 short audio clips uttered by 109 native English speakers with various accents. The total length of the audio clips is approximately 44 hours. The audio format is 16-bit PCM with a sample rate of 44 kHz. We reduced the sample rate to 22 kHz.</figDesc><table /><note>We randomly split the dataset into a training set (43,470 samples), validation set (100 samples), and test set (500 samples).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of evaluated MOS with 95% confidence intervals on the LJ Speech dataset. Removing the normalizing flow in the prior encoder results in a 1.52 MOS decrease from the baseline, demonstrating that the prior distribution's flexibility significantly influences the synthesis quality. Replacing the linear-scale spectrogram for posterior input with the mel-spectrogram results in a quality degradation, indicating that the highresolution information is effective for VITS in improving the synthesis quality.</figDesc><table><row><cell>Model</cell><cell>MOS (CI)</cell></row><row><cell>Ground Truth</cell><cell>4.46 (±0.06)</cell></row><row><cell>Tacotron 2 + HiFi-GAN</cell><cell>3.77 (±0.08)</cell></row><row><cell cols="2">Tacotron 2 + HiFi-GAN (Fine-tuned) 4.25 (±0.07)</cell></row><row><cell>Glow-TTS + HiFi-GAN</cell><cell>4.14 (±0.07)</cell></row><row><cell cols="2">Glow-TTS + HiFi-GAN (Fine-tuned) 4.32 (±0.07)</cell></row><row><cell>VITS (DDP)</cell><cell>4.39 (±0.06)</cell></row><row><cell>VITS</cell><cell>4.43 (±0.06)</cell></row></table><note>We conducted an ablation study to demonstrate the effectiveness of our methods, including the normalized flow in the prior encoder and linear-scale spectrogram posterior input. All models in the ablation study were trained up to 300k steps. The results are shown in Table2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>MOS comparison in the ablation studies.</figDesc><table><row><cell>Model</cell><cell>MOS (CI)</cell></row><row><cell>Ground Truth</cell><cell>4.50 (±0.06)</cell></row><row><cell>Baseline</cell><cell>4.50 (±0.06)</cell></row><row><cell cols="2">without Normalizing Flow 2.98 (±0.08)</cell></row><row><cell>with Mel-spectrogram</cell><cell>4.31 (±0.08)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the synthesis speed. Speed of n kHz means that the model can generate n×1000 raw audio samples per second. Real-time means the synthesis speed over real-time.</figDesc><table><row><cell>Model</cell><cell cols="2">Speed (kHz) Real-time</cell></row><row><cell>Glow-TTS + HiFi-GAN</cell><cell>606.05</cell><cell>×27.48</cell></row><row><cell>VITS</cell><cell>1480.15</cell><cell>×67.12</cell></row><row><cell>VITS (DDP)</cell><cell>2005.03</cell><cell>×90.93</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Source-code: https://github.com/jaywalnut310/vits Demo: https://jaywalnut310.github.io/vits-demo/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Sungwon Lyu, Bokyung Son, Sunghyo Chung, and Jonghoon Mo for helpful discussions and advice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of Conditional Variational Autoencoder with Adversarial Learning for</head><p>End-to-End Text-to-Speech</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Monotonic Alignment Search</head><p>We present pseudocode for MAS in Figure <ref type="figure">4</ref>. Although we search the alignment which maximizes the ELBO not the exact log-likelihood of data, we can use the MAS implementation of Glow-TTS as described in Section 2.2.1.</p><p>def monotonic_alignment_search(value):</p><p>"""Returns the most likely alignment for the given log-likelihood matrix. Args: value: the log-likelihood matrix. Its (i, j)-th entry contains the log-likelihood of the j-th latent variable for the given i-th prior mean and variance: .. for x in range(max(0, t_x + y -t_y), min(t_x, y + 1)): if y == 0: # Base case. If y is 0, the possible x value is only 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Configurations</head><p>In this section, we mainly describe the newly added parts of VITS as we followed configurations of Glow-TTS and HiFi-GAN for several parts of our model: we use the same transformer encoder and WaveNet residual blocks as those of Glow-TTS; our decoder and the multi-period discriminator is the same as the generator and multi-period discriminator of Then, we can synthesize a voice ŷ of a target speaker identity ŝ from the representation e through the inverse transformation of the normalizing flow f −1 θ and decoder G:</p><p>Learning speaker-independent representations and using it for voice conversion can be seen as an extension of the voice conversion method proposed in Glow-TTS. Our voice conversion method provides raw waveforms rather than melspectrograms as in Glow-TTS. The voice conversion results are presented in Figure <ref type="figure">7</ref>. It shows a similar trend of pitch tracks with different pitch levels. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><surname>Phonemizer</surname></persName>
		</author>
		<ptr target="https://github.com/bootphon/phonemizer" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High Fidelity Speech Synthesis with Adversarial Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Variational</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BysvGP5ee" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yin, a fundamental frequency estimator for speech and music</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveigné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1917" to="1930" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density estimation using Real NVP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end Adversarial Text-to-Speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rsf1z-JSj87" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Spline Flows</title>
		<author>
			<persName><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Flow++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical Generative Modeling for Controllable Speech Synthesis</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rygkk305YQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><surname>The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dataset</forename><surname>Speech</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative Adversarial networks for Efficient and High Fidelity Speech Synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Hifi</surname></persName>
		</author>
		<author>
			<persName><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Generative Adversarial Networks for Conditional waveform synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Boissiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Z</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Brébisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Melgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14910" to="14921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bidirectional Variational Inference for Non-Autoregressive Text-to-speech</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=o3iritJHLfO" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-autoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><surname>Flowseq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4273" to="4283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flow-TTS: A non-autoregressive network for text to speech based on flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7209" to="7213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural text-to-speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7586" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Voice 3: 2000-Speaker Neural Text-to-Speech</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJtEm4p6Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Fastspeech</surname></persName>
		</author>
		<title level="m">Fast, Robust and Controllable Text to Speech</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3171" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=piLPYqxtWuA" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voiceloop: Voice Fitting and Synthesis via a Phonological Loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkFAWax0-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ig53hpHxS4" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit</title>
		<author>
			<persName><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mariooryad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Wave-Tacotron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03568</idno>
		<title level="m">Spectrogramfree end-to-end text-to-speech synthesis</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aligntts: Efficient feed-forward text-to-speech system without explicit alignment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6714" to="6718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning latent representations for style control and transfer in endto-end speech synthesis</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6945" to="6949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Latent normalizing flows for discrete sequences</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7673" to="7682" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
