<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Dynamic Heterogeneous Graph and Node Importance for Future Citation Prediction</title>
				<funder ref="#_c5CS4Be">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_J4nWt7B">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-27">27 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao</forename><surname>Geng</surname></persName>
							<email>genghao@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Deqing</forename><surname>Wang</surname></persName>
							<email>dqwang@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<email>zhuangfuzhen@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xuehua</forename><surname>Ming</surname></persName>
							<email>xhming@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Du</surname></persName>
							<email>duchenguang@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haolong</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="department" key="dep3">Institute of Artificial Intelligence at Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">SKLSDE</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">CIKM &apos;22</orgName>
								<address>
									<addrLine>October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">CIKM &apos;22</orgName>
								<address>
									<addrLine>October 17-21</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Dynamic Heterogeneous Graph and Node Importance for Future Citation Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-27">27 May 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3511808.3557398</idno>
					<idno type="arXiv">arXiv:2305.17417v1[cs.DL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>citation count prediction</term>
					<term>dynamic heterogeneous graph</term>
					<term>node importance estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate citation count prediction of newly published papers could help editors and readers rapidly figure out the influential papers in the future. Though many approaches are proposed to predict a paper's future citation, most ignore the dynamic heterogeneous graph structure or node importance in academic networks. To cope with this problem, we propose a Dynamic heterogeneous Graph and Node Importance network (DGNI) learning framework, which fully leverages the dynamic heterogeneous graph and node importance information to predict future citation trends of newly published papers. First, a dynamic heterogeneous network embedding module is provided to capture the dynamic evolutionary trends of the whole academic network. Then, a node importance embedding module is proposed to capture the global consistency relationship to figure out each paper's node importance. Finally, the dynamic evolutionary trend embeddings and node importance embeddings calculated above are combined to jointly predict the future citation counts of each paper, by a log-normal distribution model according to multi-faced paper node representations. Extensive experiments on two large-scale datasets demonstrate that our model significantly improves all indicators compared to the SOTA models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Predicting the impact of research papers is of great significance for science researchers to find out the most promising research topic to study and identify significant works from a sea of scientific literature <ref type="bibr" target="#b6">[7]</ref>. As there is no precise definition of the impact of a scientific research, citation counts of scientific papers are usually taken as the estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>However, the task of predicting citation counts is challenging and nontrivial, due to the following reasons. Firstly, as existing publications own citation counts in each year, when a new publication emerging, there does not exist any historical citations, leading to the lack of label information to train the model. Secondly, in the heterogeneous academic network, each paper is associated with heterogeneous information such as authors, venues and fields, and how to make full use of these heterogeneous information turns to be a great challenge. Thirdly, all nodes in the academic network keep continuously evolving with changeable states, making it crucial to properly capture the dynamics. To follow previous footsteps, we summarize the existing methods for citation count prediction task in the following two categories.</p><p>The first category predicts early published papers' citation counts with previous citations available <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. One part of these methods design parametric patterns to model the paper citation trend, including the log-normal intensity function in Wang et al. <ref type="bibr" target="#b31">[32]</ref>, the reinforced Poisson process in Shen et al. <ref type="bibr" target="#b24">[25]</ref> and the recency-weighted effect in Liu et al. <ref type="bibr" target="#b19">[20]</ref>. Other methods utilize neural networks to capture the temporal patterns in historical citations, such as the Recurrent Neural Network (RNN) in Yuan et al. <ref type="bibr" target="#b40">[41]</ref> and the seq2seq framework in Abrishami <ref type="bibr" target="#b0">[1]</ref>. However, there methods are unable to predict citation counts for newly published papers, since there exists no historical citations.</p><p>The second category predicts citation counts for newly published papers without historical citations. As a paper carries multidimensional information including its related authors, keywords, reference papers and venues, some researchers extract hand-crafted features to represent a paper <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. For instance, Dong et al. <ref type="bibr" target="#b6">[7]</ref> represents a paper by 6 types of factors, including author, reference, topic, venue, social and temporal features. Yan et al. <ref type="bibr" target="#b38">[39]</ref> extracts rank-based features such as author rank and venue rank, as a part of paper features. However, these feature engineering methods require expert knowledge and manual labour for feature designing, which is time-consuming and cannot utilize the power of heterogeneous and evolving characteristics of nodes. HINTS <ref type="bibr" target="#b14">[15]</ref> is the first work as we known to design an end-to-end framework for predicting new paper citation counts without feature engineering. Specifically, HINTS takes the whole academic network as a sequence of heterogeneous graphs, and combines the temporally aligned Graph Neural Network (GNN), the Recurrent Neural Network (RNN) and a time series generator to learn representations of the sequence. Although creative and rational, it fails to make full use of heterogeneous academic network features and the significance or popularity of a node in the graph, and thus causing the prediction accuracy lower.</p><p>In this paper, we propose a new framework based on Dynamic Heterogeneous Graph and Node Importance Network, named DGNI, which models the dynamic evolutionary trends of the academic network and each paper's node importance on the global scale to predict future citation counts of newly-published papers. DGNI is divided into three parts: dynamic heterogeneous network embedding module, node importance embedding module, and time series generation module.</p><p>Firstly, in dynamic heterogeneous network embedding module, we use Heterogeneous Graph Neural Network (HGNN) on snapshots of the academic network from different timestamps, together with RNN-based model to jointly model time series features. So the module can capture the dynamic evolutionary trends of the whole academic network before the publication of the papers.</p><p>Since the heterogeneous graph neural network can only capture the local consistency relationship of academic network, neglecting the significance of the global consistency relationship. In node importance embedding module, we propose a node importance embedding module to calculate each paper's node importance on the global scale to capture the global consistency relationship. This is consistent with our intuition that a paper with higher importance and influence in the academic community tends to receive more citations in the subsequent years.</p><p>Finally, in time series generation module, the dynamic evolutionary trends embeddings and node importance embeddings calculated above are transformed into the parameters of the time series generation module, using a simple multilayer perceptron (MLP). Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>, we use a log-normal distribution model to generate the prediction citation counts sequence for each newly published paper.</p><p>In summary, our main contributions can be summarized as follows:</p><p>? We solve the challenging cold start problem in time series citation prediction. To be specific, it refers to the prediction of citation count for newly published articles without historical citation count values. The rest of this paper is organized as follows: Section 2 introduces necessary definitions and makes a formal definition of the problem we tackle. Section 3 introduces the motivation and framework of our proposed model DGNI, and further elaborate each component of our model. Section 4 evaluates the performance of DGNI by experiments and analyses. Section 5 reviews the related works of citation count prediction, node importance estimation and heterogeneous graph representation learning. Section 6 makes a conclusion to the entire paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we introduce necessary definitions used in the paper and make a formal definition of the problem we study. The types of nodes in heterogeneous academic network include paper, venue, field and author. It is widely noticed that the papers are the central nodes and other nodes are neighbors. The types of edge include publish (paper-venue), write (author-paper), contain (paper-field) and cite (paper-paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Metapath and metapath-based subgraph.</head><p>Metapath is defined as a path with the following form:</p><formula xml:id="formula_0">? 1 ? 1 --? ? 2 ? 2 --? ? ? ? ? ? -1 ----? ? ? (abbreviated as ? 1 ? 2 ? ? ? ? ? )</formula><p>, where ? ? ? V, ? ? ? E. The metapath describes a composite relation between node types ? 1 and ? ? , which expresses specific semantics.</p><p>Given a metapath ? ? of a heterogeneous graph G, the metapathbased subgraph of graph Gis defined as a graph composed of all neighbor pairs based on metapath ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Dynamic Heterogeneous Network.</head><p>A dynamic heterogeneous network is a sequence of heterogeneous academic network from 1 to T year:</p><formula xml:id="formula_1">?G ? ? ? ? =1 = {G 1 , G 2 , ..., G ? }, where G ? = (V ? , E ? )(1 ? ? ? ? )</formula><p>is the heterogeneous academic network in t th year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Node importance.</head><p>A node importance ? ? R + is a non-negative real number representing the significance or the popularity of an entity in a knowledge graph. For instance, the gross of the movie or the voting number for the movie on the website can be regarded as the node importance in movie knowledge graphs. The specific importance value of a node is collected from the real scenarios and obtained after the log transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Formalization</head><p>Given a dynamic heterogeneous network ?G ? ? ? ? =1 and a target paper p, paper citation time series prediction aims to learn a function f : (?G ? ? ? ? =1 , ?) ? {? ? +1 ? , ? ? +2 ? , ..., ? ? +? ? } that predicts the citation time series of the target paper p in the following L years after the publication year T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we introduce our proposed model DGNI for citation count prediction. First, we describe our motivation to design the architecture of DGNI, then we present the framework of DGNI, as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. Next, we elaborate the details of three components in DGNI: dynamic heterogeneous network embedding module, node importance embedding module, and time series generation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework of DGNI</head><p>The key idea of our model is to learn a continuously evolving vector representation of each node from the snapshots of the academic network in different periods, so the node representation can reflect the node's evolutionary trend. Powered by such representations, the dynamics of the academic network can be well captured, making it easier to predict future citations for new papers. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, our proposed method DGNI is composed of three modules: dynamic heterogeneous network embedding module, node importance embedding module, and time series generation module.</p><p>According to <ref type="bibr" target="#b14">[15]</ref>, the impact of paper can be predicted by modeling changes of snapshots of dynamic heterogeneous networks in different periods. In our dynamic heterogeneous network embedding module, instead of simply using RGCN <ref type="bibr" target="#b22">[23]</ref> to capture the heterogeneity of the academic network, which keeps distinct non-sharing weights for node types and edge types alone and is insufficient to capture heterogeneous properties, we use the SOTA heterogeneous graph neural network HGT <ref type="bibr" target="#b12">[13]</ref> to encode the dynamics and heterogeneity in each year's heterogeneous network snapshot. The HGT model automatically learns different weights for different types of nodes and relations, and aggregates features accordingly. The node representations learned by HGT serve as dynamic network features before the paper is published.</p><p>Since the heterogeneous graph representation learning algorithm is based on the neighbor aggregation mechanism, it takes only the information of a very limited neighborhood for each node. So it can only capture the local evolutionary trend patterns of academic network, unable to capture global trend patterns. As there are huge number of nodes in the academic network interacting with each other, and each node contribute differently, it makes sense to encode the global evolutionary trends to model each node's importance. A paper node with higher importance tends to receive more citations in the following years and vice versa. In node importance embedding module, we take Personalized PageRank (PPR) <ref type="bibr" target="#b20">[21]</ref> into the graph neural network to reflect the larger neighbor information of different types of relations, capturing the global evolutionary trends. To make full use of the heterogeneity in the graph, we devise a semantic-level attention mechanism to learn the importance of different meta-paths and fuse them automatically. After that, we obtain each new paper's node importance patterns.</p><p>In time series generation module, we use attention mechanism to fuse the dynamic node features and node importance patterns learned by above modules to generate final embeddings of each paper. The paper embeddings serve as the parameters of a parametric citation count generator. Based on the work <ref type="bibr" target="#b31">[32]</ref>, we use a log-normal distribution to encode prior knowledge of citation processes and generate citation count time series in the years immediately following publication. We unfold the details of these three modules in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Heterogeneous Network Embedding Module</head><p>A dynamic heterogeneous academic network refers to a sequence of heterogeneous networks from several years before a paper's publication year T, and it reflects the evolutionary trends of the entire academic network in different periods. For each year's heterogeneous network, to capture the rich semantic heterogeneity information, we use the Transformer <ref type="bibr" target="#b30">[31]</ref>-based heterogeneous graph neural network model HGT <ref type="bibr" target="#b12">[13]</ref> to learn the node representations of each node, which treats one type of node as query to calculate the importance of other types of nodes around it. As each year's network snapshot reflects different parts of the whole dynamic academic network, and should be comparable, we use the same HGT model to encode each static heterogeneous network in each year.</p><p>Secondly, in academic dynamic network, unlike other dynamic networks such as social dynamic network, nodes won't change rapidly. In other words, the characteristics of same nodes in adjacent years tend to be similar. Inspired by this, we introduce a Mean Squared Error (MSE) loss to force embeddings of same nodes in adjacent years to be close to each other, named temporal-aligned loss:</p><formula xml:id="formula_2">L ???? = 1 T -1 T -1 ?? ? =1 1 |? ? ? ? ? +1 | ?? ? ?? ? ?? ? +1 ||? ? ? -? ? +1 ? || 2 2 (1)</formula><p>where ? ? ? denotes node ?'s embedding at year ? (year starts from 1), ? ? denotes the node set in the heterogeneous network of year ?, and T denotes the number of years (observed heterogeneous networks) before the paper's publishment. By encoding static heterogeneous networks using HGT model, we obtain the embeddings of each node in each year. However, since a newly published paper has no historical citations, it does not exist in the dynamic academic network. As a solution, we use the metadata nodes (e.g. authors, venues, keywords, etc.) existing in previous years which are linked to the new paper to generate the "fake" embeddings of the paper node in past timestamps. The reason is that a paper's metadata nodes are often with a long history and have a high probability to exist in previous years' academic networks.</p><p>To fulfil that, we first explore the snapshot network related to the published year of the paper to find out the linked metadata neighbor nodes, denoted as ? ? . Then we look back on each previous year's snapshot network and average the features of these neighbor nodes to get the paper's fake embeddings in each past year. As different types of neighbor nodes may not contribute equally to the impact of the paper, inspired by <ref type="bibr" target="#b14">[15]</ref>, we apply type-aware trainable weights to preserve the unequal contribution of different kinds of metadata neighbor nodes, as follows:</p><formula xml:id="formula_3">? ? ? = ?? ? ?R ?? ? ?? ? ?,? ? ? ? h ?,? |? ? ?,? | ,<label>(2)</label></formula><p>where R denotes the relation set in the network, h ?,? denotes the feature of node ? in year ?, ? ? ?,? denotes the set of neighbor nodes adjacent to the paper ? based on relation ? , and ? ? denotes the learnable weight of relation ? shared in all years of network snapshots. After that, the generated fake embeddings of paper ? in year ?, denoted as ? ? ? , can be obtained.</p><p>We apply Eq. 2 in every timestamp to obtain a sequence of generated fake embeddings of new paper ? as ? ? = {? ? ? , ? ? +1 ? , ..., ? ? -1 ? }, where ? is the first year when paper ?'s metadata nodes can be observed. Then, to model the new paper ?'s temporal trajectory, we temporally encode the embedding sequence ? ? into a single vector ? ? ? through the recurrent neural network LSTM <ref type="bibr" target="#b11">[12]</ref> as follows:</p><formula xml:id="formula_4">? ? ? = LSTM(? 1 ? , ? 2 ? , ..., ? ? ? ).<label>(3)</label></formula><p>After that, we obtain ? ? ? , the dynamic heterogeneous network feature vector of the paper ? before its publication, which reflects new paper's dynamic evolutionary trends. And it will be used in the time series generation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Node Importance Embedding Module</head><p>Since the heterogeneous graph learning algorithm is based on a neighbor aggregation mechanism, it takes only the information of a very limited neighborhood of each node to avoid overfitting and over-smoothing. As a result, the heterogeneous graph neural network can only capture the local consistency relationship of the academic network. However, the global consistency relationship is vitally important. For instance, in an academic network, each scholar can be a member of several communities and can be influenced by his neighborhoods with different distances from local consistency relationship to global consistency relationship, so only considering the local relationship tends to be one-sided.</p><p>To capture the global consistency relationship, we propose a node importance embedding module to calculate each paper's node importance on the global scale. Intuitively, node importance has a close connection with citation counts. A paper with higher node importance has higher academic impacts and tends to receive more citations in the following years.</p><p>In this work, we take Personalized PageRank (PPR) <ref type="bibr" target="#b20">[21]</ref> into the graph neural network to reflect the global consistency relationship of the whole academic network. We define the PPR matrix as:</p><formula xml:id="formula_5">? ??? = ? (? ? -(1 -?)? -1 ?) -1 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where ? is a teleport probability. The PPR representation of node ? refers to the ? ?? row in ? ??? as ? (?) := ? ??? ?,: . As the academic network used in our experiment is too large to compute on the whole graph, following the work of <ref type="bibr" target="#b5">[6]</ref>, we use random walk sampling <ref type="bibr" target="#b9">[10]</ref> as an efficient and scalable algorithm for computing an approximation of PPR. To guarantee the absolute error lower than ? with probability of 1 -1 ? , we need ? ( log ? ? 2 ) random walks. Additionally, we also take the instructions in <ref type="bibr" target="#b5">[6]</ref> to truncate ? ??? to contain only the top ? largest entries for each row ? (?), denoted as ? ?,? ? . However, the above method can only handle the homogeneous graph structure. When it comes to heterogeneous graph, since it contains different types of nodes and links, each node is connected via various types of relations, e.g., meta-paths. Since different metapaths reflect different aspects of the whole graph, and they take unequal contribution to the final result, we compute PageRank respectively in each metapath-based subgraph. Then, inspired by Graph Attention Network (GATv2) <ref type="bibr" target="#b1">[2]</ref>, we propose a novel attention mechanism to learn the importance of different meta-paths and fuse multiple semantics revealed by them.</p><p>Firstly, we extract metapath-based subgraphs by each meta-path, and compute PageRank matrix ? ? ? for each subgraph using Eq. 4.</p><p>Then by modifying the attention mechanism proposed in GATv2 <ref type="bibr" target="#b1">[2]</ref> to <ref type="bibr" target="#b4">(5)</ref> where ? ? ? denotes the attention vector of meta-path ? ? , ? ? denotes the raw feature vector of node ?, ? ? ? denotes the transformation weight matrix of meta-path ? ? , aiming at projecting the raw node feature into the meta-path vector space, and ? ? ? ? denotes the PageRank pattern of node ? in meta-path ? ? . By Eq. 5, we can naturally incorporate the global PageRank patterns into the GAT layer in each subgraph.</p><formula xml:id="formula_7">? ? ? (? ? , ? ? ) = ? ? ? ? LeakyReLU([? ? ? ? ? ||? ? ? ? ? ||? ? ? ? ||? ? ? ? ]),</formula><p>Then, we normalize the attention scores from all neighbors ? ? N ? ? ? within meta-path ? ? , and aggregate these features by learned weights:</p><formula xml:id="formula_8">? ? ? (? ? , ? ? ) = exp(LeakyReLU(? ? ? (? ? , ? ? ))) ? ? N ?? ? exp(LeakyReLU(? ? ? (? ? , ? ? ))) ,<label>(6)</label></formula><formula xml:id="formula_9">? ? ? ? = LeakyReLU( ?? ? ? N ?? ? ? ? ? (? ? , ? ? ) ? ? ? ? ? ? ? ).<label>(7)</label></formula><p>After that, we get node embedding ? ? ? ? for each meta-path ? ? , incorporated with both PageRank patterns and metapath-level representations. Next, in order to fuse these metapath-level representations to get the final node embeddings, we use an attention mechanism similarly, but at the level of meta-path. Specifically, we do a normalization on each meta-path by averaging the node embeddings ? ? ? ? learned before. Then we use an attention vector ? to transform these embeddings into the importance of specific meta-path, denoted as ? ? ? :</p><formula xml:id="formula_10">? ? ? = ? T ? 1 |V | ?? ? ? V ReLU(? ? ? ? ? ? ).<label>(8)</label></formula><p>To obtain the weight of meta-path ? ? , we normalize the above importance of all meta-paths by softmax function:</p><formula xml:id="formula_11">? ? ? = exp(? ? ? ) ? ?=1 exp(? ? ? ) ,<label>(9)</label></formula><p>where ? denotes the number of all meta-paths. Since the learned ? ? ? can be interpreted as the contribution of each meta-path ? ? , we take it as coefficient to fuse semantic-specific embeddings belonging to different meta-paths, to obtain the final embedding as follows:</p><formula xml:id="formula_12">? ? ? = ? ?? ?=1 ? ? ? ? ? ? ? ? .<label>(10)</label></formula><p>The final node embedding ? ? ? reflects the node importance of paper ?, capturing the global consistency relationship. It will be used for the time series generation module, together with node features reflecting new paper's dynamic evolutionary trends described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Time Series Generation Module</head><p>Through above two modules, for a target paper ?, we can generate its dynamic evolutionary trend patterns and node importance patterns in the whole academic network. To combine node dynamic trend patterns and importance patterns, we use an attention mechanism:</p><formula xml:id="formula_13">? ? = ? ? ? ? ? + ? ? ? ? ? (11) ? ? = exp(? T ? ? ? ) exp(? T ? ? ? ) + exp(? T ? ? ? ) ,<label>(12)</label></formula><formula xml:id="formula_14">? ? = exp(? T ? ? ? ) exp(? T ? ? ? ) + exp(? T ? ? ? ) ,<label>(13)</label></formula><p>where ? ? ? refers to the dynamic trend vector of paper ? calculated in Sec. 3.2, ? ? ? refers to the node importance vector of paper ? calculated in Sec. 3.3, and ? denotes the trainable attention weight.</p><p>As described in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>, a new paper's influence will reach the peak within a few years after publication, and will gradually decrease on account of novelty fading and continuous appearance of new ideas and new research topics attracting researchers' attention, known as aging effect. Therefore, we model the citation trajectory of a paper as a log-normal probability along time ?:</p><formula xml:id="formula_15">? ? (?) = 1 ? 2?? ? ? ??? - (ln ? -? ? ) 2 2? 2 ? ,<label>(14)</label></formula><p>where ? ? denotes the mean of the normal distribution, which describes the time required for an article to reach the peak of citation trajectory. ? ? denotes the variance of the normal distribution, which describes the decay rate of paper ?'s citation decrement.</p><p>As discussed in <ref type="bibr" target="#b14">[15]</ref>, the "fitness" makes significant contributions to a paper's citations, so another parameter ? ? is used to model it. Integrated across ? ? , the cumulative number of citations of a paper can be generated by the cumulative distribution function:</p><formula xml:id="formula_16">? ? ? = ? ??? (? ? * ?( ln ? -? ? ? ? )) -1 ,<label>(15)</label></formula><p>where ? ? is the parameter which weights the citation count to model the difference between papers. ? is a scalar that adjusts the weight of the result, which is a hyper-parameter that will be fixed during the model training process. ?(?) is defined as:</p><formula xml:id="formula_17">?(?) = (2?) -1/2 ? ? -? ? -? 2 /2 ??.<label>(16)</label></formula><p>To get three parameters ? ? , ? ? and ? ? to generate each new paper's citation time series, we use three Multilayer Perceptron models (MLP) to transform the final node embedding ? ? of the target paper ? to generate these parameters:</p><formula xml:id="formula_18">? ? = MLP 1 (? ? ),<label>(17)</label></formula><formula xml:id="formula_19">? ? = MLP 2 (? ? ),<label>(18)</label></formula><formula xml:id="formula_20">? ? = MLP 3 (? ? ). (<label>19</label></formula><formula xml:id="formula_21">)</formula><p>After that, we can obtain the cumulative citation of target paper ? in 1-? years after publication, denoted as a sequence ? ? = {? ? ? , ? ? +1 ? , ..., ? ? +? ? }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>The cumulative sequence of target paper ? in 1-? years after publication is calculated by Eq. 15, denoted as {? ? ? , ? ? +1 ? , ..., ? ? +? ? }. The loss function is composed of two parts: prediction loss and temporal-aligned loss.</p><p>The temporal-aligned loss is discussed in Sec. 3.2. As nodes in academic dynamic network won't change rapidly and the characteristics of same nodes in adjacent years tend to be similar, the temporal-aligned loss aims to force embeddings of same nodes in adjacent years to be close to each other, as described in Eq. 1.</p><p>For prediction loss, we adopt the Mean Square Error (MSE) to compare the predicted time series with the ground-truth as prediction loss:</p><formula xml:id="formula_22">L ???? = 1 ? ? ?? ?=1 1 ? ? +? ?? ? =? +1 (? ? ? -?? ? ) 2 , (<label>20</label></formula><formula xml:id="formula_23">)</formula><p>where ? ? ? denotes the ground-truth citation count of paper ? in the ? ?? year, ? denotes the number of prediction years, and ? denotes the total number of papers for prediction. Since the citation counts of different papers vary widely, we conduct log transformation on ground-truth citation counts to smooth rapid changes, i.e. ? t ? = log(? t ? + 1), and make predictions on the logged version of true values.</p><p>The overall model will be optimized by prediction loss (Eq. 20) and temporal-aligned loss (Eq. 1) at the same time. The total loss is defined as follow:</p><formula xml:id="formula_24">L = L ???? + ?L ???? (<label>21</label></formula><formula xml:id="formula_25">)</formula><p>where ? is the hyper-parameter to adjust the proportion of temporalaligned loss in total loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the performance of our proposed model DGNI by experiments on two real-world large-scale datasets. We describe our experimental settings and then show numerical comparison results with other citation count prediction baselines. To help readers understand how DGNI works, we breakdown the model in ablation studies and conduct visualization analyses to prove DGNI's efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>4.1.1 Datasets. We conduct experiments on two real-world datasets APS and AMiner. The statistics of nodes and edges about the two datasets are shown in Table <ref type="table" target="#tab_1">1</ref>. Below we take a brief introduction: APS<ref type="foot" target="#foot_0">1</ref> (American Physical Society) is a dataset that covers publications in the journal of the American Physical Society, including three node types: paper, author, venue. To generate keyword nodes, we extract keywords from the title of papers following the preprocessing procedure proposed by <ref type="bibr" target="#b23">[24]</ref>. In the experiment, We use papers from 2003 to 2008 to build the training set to train the model, papers in 2009 to build the validation set, and papers in 2010 to build the testing set.</p><p>AMiner<ref type="foot" target="#foot_1">2</ref> is a dataset that covers publications in computer science venues <ref type="bibr" target="#b27">[28]</ref>, including four node types: paper, author, venue and keyword in its V11 version. The training, validation and testing datasets are of the same configuration as APS. The distribution of cumulative citation counts of papers in two datasets is shown in Fig. <ref type="figure" target="#fig_2">2</ref>. We can note that the cumulative citations of papers subject to the long-tailed distribution: most papers are rarely cited after publication, and only a fraction of them can receive considerable citations.</p><p>4.1.2 Baselines. Since the "cold start" citation count time series prediction task is a novel problem, there is only one work (i.e. HINTS <ref type="bibr" target="#b14">[15]</ref>) to compare with. Besides, we consider 4 other citation count time series prediction methods for comparison, which are briefly described below.</p><p>? Gradient Boosting Machine (GBM): A gradient boosting model used to model scientific features and predict citation time series. Following <ref type="bibr" target="#b14">[15]</ref>, we extract scientific features that are available in our problem setting or data, to citation time series with XGBoost <ref type="bibr" target="#b4">[5]</ref>.</p><p>? DeepCas <ref type="bibr" target="#b17">[18]</ref>: This model conducts random walk across an information cascade graph to predict popularity. In our experiments, the ego network of a new paper in the publication year is used as the initial cascade graph.</p><p>? HINTS <ref type="bibr" target="#b14">[15]</ref>: A state-of-the-art model for "cold start" citation count time series prediction. This model uses R-GCN <ref type="bibr" target="#b22">[23]</ref> to encode dynamic heterogeneous graph and a log-normal distribution to generate the citation time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Metrics.</head><p>Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>, we use the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to evaluate the accuracy of predictions, which are common choices for regression tasks. MAE and RMSE are defined as follows:</p><formula xml:id="formula_26">MAE(? ? , ? t ) = 1 ? ? ?? ?=1 |? t ? -? ? ? |,<label>(22)</label></formula><formula xml:id="formula_27">RMSE(? ? , ? t ) = 1 ? ? ?? ?=1 (? t ? -? ? ? ) 2 ,<label>(23)</label></formula><p>where ? ? ? and ? t ? denote the ground-truth and the prediction of citation counts of paper ? in the ? ?? year after publication, and ? denotes the total number of papers for prediction.</p><p>4.1.4 Implementation Details. We implement DGNI using PyTorch 1.10.0. For the dynamic heterogeneous network embedding module, the number of historical dynamic heterogeneous networks we use to model is 3, the dimension of output features of HGT is 32, the layer count and attention heads are set to 2 and 4. The node features in dynamic heterogeneous network are randomly initialized using Xavier initialization. The hidden dimension and the layer count of GRU are 32 and 3 respectively.</p><p>For node importance embedding module, the number of attention heads and attention layers are 4 and 2 respectively, the hidden dimension is set to 32. The meta-paths we use are based on paper nodes: PAP (Paper-Author-Paper), PVP (Paper-Venue-Paper), PKP (Paper-Keyword-Paper). For time series generation module, the hidden dimensions of three fully-connected layers are all set to 20. The weight of temporal-aligned loss ? is set to 0.5.</p><p>We set learning rate to 0.001 and model parameter optimizer as Adam. We set batch size to 3000 for both APS and AMiner datasets. All the models predict the citation series of the paper in the first 5 years after publication, and the averages are used as the prediction result. We run each experiment 10 times following the same configuration with different random seeds and take the average of all results as final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Numerical Comparison Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparison with Baselines.</head><p>The prediction results are shown in Table <ref type="table" target="#tab_2">2</ref>. We can summarize that DGNI achieves significant improvement on all the baselines on two datasets in terms of both MAE and RMSE. Since HINTS is the baseline with best performance as we know, for our proposed model DGNI, in terms of MAE, DGNI outperforms HINTS by 10.93% on APS and 11.39% on AMiner. And for RMSE, DGNI outperforms HINTS by 12.90% on APS and 11.31% on AMiner. The results prove the effectiveness and efficacy of our DGNI.</p><p>Compared with HINTS, our use of HGT as the dynamic heterogeneous network encoder has stronger feature expression ability than the use of simple R-GCN. Besides, our proposed node importance embedding module can capture the node importance of different papers and pay more attention on papers with higher reputation, thus boosting the prediction of citation count. The more detailed analyses of these two components are elaborated in Ablation Study in Sec. 4.2.2.</p><p>Additionally, it can be found from annual prediction results that DGNI can achieve the best results also in the annual results and can achieve better performance in early citation prediction than long-term citation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Study on DGNI Components.</head><p>To present more detailed analyses of these components in our proposed DGNI model and find out why DGNI works, we compare DGNI with two variants on APS and AMiner datasets to evaluate the effectiveness of the modules of our model. The two variants are described as follows:</p><p>? DGNI-graph: A variant of our framework, which removes the node importance embedding module and only uses dynamic heterogeneous network for prediction. ? DGNI-inp: A variant of our framework, which removes the dynamic heterogeneous network embedding module and only uses node importance embedding module for prediction.</p><p>The experimental results are shown in Table <ref type="table" target="#tab_3">3</ref>. From the results, we can come to the following conclusions: (1) The whole DGNI can achieve almost best results than all variants on APS and AMiner datasets. It verifies the effectiveness of all our proposed modules in DGNI. <ref type="bibr" target="#b1">(2)</ref> The node importance embedding module has a great impact on the results, because after removing the module, the model accuracy on both datasets gets lower. The reason might be that the node importance can reflect the global consistency relationship of the network and can guide the allocation of focus on different papers. Furthermore, in terms of AMiner dataset, the variant of DGNI-inp has the best performance, the reason might be that in AMiner dataset the global consistency relationship is more important. (3) The use of dynamic heterogeneous network plays a key role in the citation prediction. As after removing the dynamic heterogeneous network embedding module, the model performance on both datasets has a huge decrease. The reason might be that heterogeneous graph neural model can capture the rich structure information in the network. And structure information is the core of heterogeneous academic network and vital for citation prediction.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Prediction Error</head><p>Analysis. In order to analyze the prediction error between papers of different citation counts, we experiment on AMiner dataset and compare the gap between actual value and predicted value. Specifically, we evenly divide the dataset into three parts by papers' citation numbers: low citation number interval (0%-33%), medium citation number interval (33%-66%) and high citation number interval (66%-100%). For visualization analysis, we plot the average of prediction result and actual result in the following 5 years after publication, on the line chart.</p><p>As shown in Fig. <ref type="figure" target="#fig_7">5</ref>, the DGNI model has a better prediction accuracy in the medium citation count range, but a higher prediction error in the low citation count range and high citation count range. The reason might be the long-tailed distribution of paper citations. Exactly, most papers included in the real-world dataset can only receive 0 or 1 citations in the following 5 years after publication, while only a small part of papers can receive higher citation numbers. However, the low citation result is not necessarily depended on authors, venues or fields, since a prominent scholar may also publish papers without citations, and papers in top-ranked journals or conferences often receive no citations either. As a result, on the one hand, the independence between papers and their related metadata brings challenges to the prediction of papers with low citation counts. On the other hand, since papers in high citation count range only constitute a small part of the whole dataset, leading to the poor prediction performance in such a scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>This section reviews three lines of related work: citation time series prediction, heterogeneous graph representation learning, and node importance estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Citation Time Series Prediction</head><p>Citation count prediction includes two categories: using early citation after publication to predict and using information before publication to predict. Parametric approaches uses early citations to model citation trends as a parametric pattern <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. Some researchers use machine learning methods to model the early citations and citation graph after publication, e.g. Abrishami and Aliakbary <ref type="bibr" target="#b0">[1]</ref> used the schema of encoder-decoder to convert the early citations into future citation trends; Li et al. <ref type="bibr" target="#b17">[18]</ref> modeled the early citations of the paper as an information cascade network. There are similar methods like <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref>. However, this kind of method relies on the early citation within 1-3 years after publication, so it can not deal with the cold start problem. Some recent works focus on predicting future citations of new papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>. Li et al. <ref type="bibr" target="#b18">[19]</ref> used peer review text to predict future citations. Xu et al. <ref type="bibr" target="#b36">[37]</ref> used heterogeneous academic network to predict the citations of the paper in ten years. The work <ref type="bibr" target="#b14">[15]</ref> is the first work to generate citation time series for newly-published paper without any leading citation values, and they use dynamic GNN to model the dynamic academic networks before the publication of papers. Despite most of the citation prediction of newly-published papers are proposed, they are not using information effectively. In this paper, we propose a new framework, which uses academic networks and node importance to predict citation of newly-published papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node Importance Estimation</head><p>Early node importance estimation methods used the degree of nodes in the networks to measure the importance of nodes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>. PageRank <ref type="bibr" target="#b20">[21]</ref> is a classic algorithm based on random walk model to propagate the importance of each node to another node with a certain probability. By traversing the entire graph, the importance of each node can be quickly calculated. With the rapid development of deep learning in recent years, some methods based on graph neural networks are proposed. Park et al. <ref type="bibr" target="#b21">[22]</ref> estimated node importance by using graph attention mechanism, and fusing information of neighbor nodes. Huang et al. <ref type="bibr" target="#b13">[14]</ref> utilized a relational graph transformer to learn semantic features, and node2vec <ref type="bibr" target="#b8">[9]</ref> to learn structure features. Then combine these features to get the final node importance values. However, these methods can only predict the node importance at a single time rather than time series. In addition, they cannot model the evolution of dynamic network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Heterogeneous Graph Representation Learning</head><p>Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling graph structured data <ref type="bibr" target="#b34">[35]</ref>. While most GNNs only work for homogeneous graphs, to represent heterogeneous structures and capture the dynamics of network time series, which are more associated with real-world scenarios, researchers have further developed heterogeneous GNNs <ref type="bibr" target="#b32">[33]</ref> and dynamic GNNs <ref type="bibr" target="#b15">[16]</ref>. For example, Schlichtkrull et al. <ref type="bibr" target="#b22">[23]</ref> propose RGCN using multiple weight matrices to project the node embeddings into different relation spaces to capture the heterogeneity of the graph. Wang et al. <ref type="bibr" target="#b33">[34]</ref> propose HAN using a hierarchical attention mechanism to capture both node and semantic importance. Hu et al. <ref type="bibr" target="#b12">[13]</ref> propose HGT treating one type of node as query to calculate the importance of other types of nodes around it, by multi-head attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we develop a framework named DGNI which adaptively fuses the dynamic evolutionary trends and the node importance in the academic network to predict citation time series with parameter-based generator, tackling the problem of cold start citation count prediction. The contrast experiments and ablation experiments have been conducted on two real-world datasets to demonstrate the superiority of our framework and effectiveness of all the components respectively. For future work, we will consider the interaction between the importance of nodes and more effective generator.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2. 1 . 1</head><label>11</label><figDesc>Heterogeneous Academic Network. A heterogeneous academic network is a special kind of heterogeneous information network (HIN), which consists of multiple types of nodes and edges to represent academic networks. It can be defined as a graph G = (V, E) with node mapping function ? : V ? T and edge mapping function ? : E ? R where |T | + |R| &gt;2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The overall architecture of our proposed model DGNI. To predict the citation count time series of a new paper ? published in year ? , DGNI first learns the heterogeneous network features from years before ? to generate the fake embeddings of ? before its publication. Then we compute the node importance of ? by our proposed node importance module. After that, the above embeddings are fed into the final time series generation module to generate the cumulative citation of ? in 1-? years after its publication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of cumulative citation counts within five years after publication.</figDesc><graphic url="image-17.png" coords="6,317.96,452.21,240.24,124.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 3 . 1</head><label>31</label><figDesc>Feature Dimension Reduction Analysis. In order to verify that DGNI can learn expressive embeddings for each paper, we use T-SNE<ref type="bibr" target="#b29">[30]</ref> to project final embeddings ? ? on AMiner dataset into a two-dimensional space, as shown in Fig.3.The figure represent the log-scale cumulative citation count in 5 years after publication. The blue points indicate low-cited papers, while red points indicate high-cited papers. It can be seen that the citation counts from the red points (left-bottom) to the blue points (right-top) decrease gradually, so DGNI can model papers with different citation counts effectively. But there are still many lowly cited papers and high-cited papers distribute together. The reason is that the specific number of citations of the paper are more related to the quality of the paper itself. Therefore, DGNI is more discriminative on the macro scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The T-SNE projection result of final embeddings on AMiner dataset. Each point represent the log-scale cumulative citation count.</figDesc><graphic url="image-18.png" coords="8,100.73,548.25,144.15,120.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 3 . 2</head><label>32</label><figDesc>Normal Distribution Parameter Analysis. Then, we visualize the relationship between the parameters of the normal distribution and the cumulative number of citations, the result is shown as Fig.4. The cumulative citation counts gradually increase from the left to the right. At the same time, the more the citation counts, the larger the parameter of the normal distribution ? ? . In addition, it can be seen that the high-cited papers usually have smaller parameters ?, which indicates that high-cited papers have a larger weight and higher growth rate. The conclusion is in line with the physical meaning of the parameters we proposed in Sec.3.4.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The visualization of parameters. Each point represent the log-scale cumulative citation count.</figDesc><graphic url="image-19.png" coords="8,364.88,377.57,144.15,130.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The predicted citation counts compared with ground-truth of papers with different citation range.</figDesc><graphic url="image-20.png" coords="9,136.71,78.02,336.35,88.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? We propose a novel framework named DGNI for citation time series prediction, which leverages both the local consistency relationship and global consistency relationship of the heterogeneous academic network. Hence our model can make full use of the dynamic academic network and node importance to predict future citation count of newly published papers.</figDesc><table /><note><p>? We conduct extensive experiments on two large-scale realworld academic network datasets, and the experimental results illustrate that our model outperforms the SOTA models by 11.39% improvement in terms of MAE and 12.90% improvement in terms of RMSE.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The Statistics of Datasets.</figDesc><table><row><cell>Dataset</cell><cell>#paper</cell><cell cols="3">#node #author #keyword #venue</cell><cell>#edge</cell></row><row><cell>APS</cell><cell>311,533</cell><cell>161,051</cell><cell>41,126</cell><cell>9</cell><cell>3,250,651</cell></row><row><cell cols="3">AMiner 1,026,795 831,151</cell><cell>34,833</cell><cell>3,673</cell><cell>10,366,576</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison Results of Different Methods over Two Datasets.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="4">MAE year1 year2 year3 year4 year5 overall year1 year2 year3 year4 year5 overall RMSE</cell></row><row><cell></cell><cell>GBM</cell><cell>0.898 0.885 0.900 0.921 1.041</cell><cell>0.934</cell><cell>1.098 1.088 1.105 1.124 1.274</cell><cell>1.139</cell></row><row><cell>APS</cell><cell cols="2">DeepCas 0.931 0.923 0.885 0.855 0.832 HINTS 0.769 0.809 0.825 0.831 0.828</cell><cell>0.904 0.805</cell><cell>1.125 1.139 1.126 1.103 1.062 0.936 0.994 1.019 1.032 1.035</cell><cell>1.104 1.023</cell></row><row><cell></cell><cell>DGNI</cell><cell cols="4">0.608 0.689 0.734 0.766 0.788 0.717 0.748 0.850 0.909 0.949 0.978 0.891</cell></row><row><cell></cell><cell>GBM</cell><cell>0.584 0.920 0.989 1.310 1.260</cell><cell>1.018</cell><cell>0.691 1.031 1.224 1.535 1.621</cell><cell>1.224</cell></row><row><cell>AMiner</cell><cell cols="2">DeepCas 0.948 1.052 1.008 0.898 0.968 HINTS 0.610 0.710 0.751 0.775 0.788</cell><cell>0.981 0.764</cell><cell>1.054 1.260 1.302 1.245 1.265 0.769 0.905 0.966 1.001 1.024</cell><cell>1.258 0.991</cell></row><row><cell></cell><cell>DGNI</cell><cell cols="4">0.491 0.629 0.704 0.759 0.803 0.677 0.606 0.782 0.898 0.986 1.003 0.879</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The Ablation Results of DGNI on Two Datasets.</figDesc><table><row><cell cols="5">Dataset Metric DGNI-graph DGNI-inp DGNI</cell></row><row><cell>APS</cell><cell>MAE RMSE</cell><cell>0.728 0.909</cell><cell>0.805 0.987</cell><cell>0.717 0.891</cell></row><row><cell>AMiner</cell><cell>MAE RMSE</cell><cell>0.699 0.914</cell><cell>0.684 0.866</cell><cell>0.677 0.879</cell></row><row><cell cols="3">4.3 Visualization Analysis</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://journals.aps.org/datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://aminer.org/citation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported by by <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2019YFA0707204</rs> and the <rs type="funder">National Natural Science Foundation of China</rs> under Grant Nos. <rs type="grantNumber">62176014</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_c5CS4Be">
					<idno type="grant-number">2019YFA0707204</idno>
				</org>
				<org type="funding" xml:id="_J4nWt7B">
					<idno type="grant-number">62176014</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting citation counts based on deep neural network learning techniques</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Abrishami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadegh</forename><surname>Aliakbary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Informetrics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="485" to="499" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How Attentive are Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=F72ximsx7C1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepHawkes: Bridging the Gap between Prediction and Understanding of Information Cascades</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Keting Cen, Wentao Robin Ouyang, and Xueqi Cheng</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating Number of Citations Using Author Reputation</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In SPIRE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Personalized Pagerank Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3578" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can scientific impact be predicted?</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="30" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open Access and Global Participation in Science</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Reimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="1025" to="1025" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Topic-sensitive PageRank</title>
		<author>
			<persName><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><surname>Haveliwala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WWW. 2704-2710</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation Learning on Knowledge Graphs for Node Importance Estimation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanren</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="646" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">HINTS: Citation Time Series Prediction for New Publications via Dynamic Heterogeneous Information Network Embedding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="3158" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation Learning for Dynamic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Defining and identifying sleeping beauties in science</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Radicchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="7426" to="7431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepcas: An end-to-end predictor of information cascades</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno>WWW. 577-586</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural citation count prediction model based on peer review text</title>
		<author>
			<persName><forename type="first">Siqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Jing Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4914" to="4924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On predictive patent valuation: Forecasting patent citations and their types</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating node importance in knowledge graphs using graph neural networks</title>
		<author>
			<persName><forename type="first">Namyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="596" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv abs/1703.06103</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated Phrase Mining from Massive Text Corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling and predicting popularity dynamics via reinforced poisson processes</title>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dashun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quantifying the evolution of individual scientific impact</title>
		<author>
			<persName><forename type="first">Roberta</forename><surname>Sinatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dashun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Deville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A L</forename><surname>Barabasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding the impact of early citers on long-term scientific impact</title>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random walk with restart: fast solutions and applications</title>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="327" to="346" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>ArXiv abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quantifying long-term scientific impact</title>
		<author>
			<persName><forename type="first">Dashun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert-L?szl?</forename><surname>Barab?si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv abs/2011.14867</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinggang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On Modeling and Predicting Individual Paper Citation Count over Time</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2676" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Early prediction of scientific impact based on multi-bibliographic features and convolutional neural network</title>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingfeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengsi</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="92248" to="92258" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">To better stand on the shoulder of giants</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In JCDL &apos;12</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Citation count prediction: learning to estimate future citations for literature</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1247" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Citation impact prediction for scientific papers using stepwise regression analysis</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1233" to="1252" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Modeling and predicting citation count via recurrent neural network with long short-term memory</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02129</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Utilizing Citation Network Structure to Predict Citation Counts: A Deep Learning Approach</title>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02647</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
