<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFCNN:A Restricted Convolutional Neural Network for Expert Finding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EFCNN:A Restricted Convolutional Neural Network for Expert Finding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Expert Finding</term>
					<term>Convolution Neural Network</term>
					<term>Similarity Matrix</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expert finding, aiming at identifying experts for given topics (queries) from expert-related corpora, has been widely studied in different contexts, but still heavily suffers from low matching quality due to inefficient representations for experts and topics (queries). In this paper, we present an interesting model, referred to as EFCNN, based on restricted convolution to address the problem. Different from traditional models for expert finding, EFCNN offers an end-to-end solution to estimate the similarity score between experts and queries. A similarity matrix is constructed using experts' document and the query. However, such a matrix ignores word specificity, consists of detached areas, and is very sparse. In EFCNN, term weighting is naturally incorporated into the similarity matrix for word specificity and a restricted convolution is proposed to ease the sparsity. We compare EFCNN with a number of baseline models for expert finding including the traditional model and the neural model. Our EFCNN clearly achieves better performance than the comparison methods on three datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online question-and-answer (QA) has become a more popular way for users to share their experiences and to ask questions on the Internet. For example, Quora.com and Zhihu.com, the most popular websites for sharing and acquiring knowledge, attract users to answer millions of questions per day; Toutiao QA, an up-and-coming mobile social platform, has accumulated 580 million Toutiao users and 300 thousand professional writers (authors). The competitive advantage of the online QA platforms is that they provide high-quality answers for users and offers a new direction for professional knowledge sharing. However, at the same time, it also poses new challenges. One central challenge is finding a way to assign those new questions (queries) to potential experts, referred to as expert finding.</p><p>Expert finding has been studied by researchers from different communities. Several different methods have been proposed. These include keyword-based modeling <ref type="bibr" target="#b1">[2]</ref>, language modeling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>, latent semantic indexing <ref type="bibr" target="#b5">[6]</ref>, and topic modeling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. Most of these methods represent every expert as a document and cast the problem as a document matching problem. In language models, each document is represented by words with their term frequency-inverse document frequency (TF-IDF) <ref type="bibr" target="#b19">[20]</ref> score. Latent semantic indexing learns a low-dimensional representation by decomposing the word feature space, and topic models such as Latent Dirichlet Allocation probabilistically group similar words into topics, and represent documents as distributions over these topics. Obviously, these existing methods mainly represent documents by the frequency or co-frequency of words, but ignore semantic information at the phrase and sentence level. Thus, how to capture and utilize semantic information at the word, phrase, and sentence level of documents remains a challenging problem.</p><p>Given a query paper and a candidate expert pool, we represent each expert as a set of documents he/she has written. As a result, how to estimate the similarity score between these documents and the query paper becomes the central issue. Inspired by the success of convolutional neural networks (CNN) <ref type="bibr" target="#b12">[13]</ref> in image recognition, we cast this task as an "image recognition" and present a method based on restricted convolution. Specifically, the similarity between any word pair of two documents is calculated to generate a similarity matrix. However, this similarity matrix not only ignores the word specificity, but is also very sparse and position-related. Therefore we introduce IDF into the similarity matrix for word specificity and propose a restricted convolution layer to ease the problem of the sparse and position-related matrix. The experiments on three datasets show that our work performs better than the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>In this paper, we define the problem of expert finding and propose a framework based on a restricted convolutional neural network. Based on the similarity matrix, we propose restricted convolution. Compared to a standard convolution, restricted convolution considers the importance of position, and penalizes for similarity far from the center of filters. For taking word specificity into accounts, we further construct a new similarity matrix by combining original similarity matrix and IDF. We prove that the proposed framework can capture and utilize semantic information from word-level to document-level. We compare our framework with several state-of-the-art approaches on three different datasets and experimental results show the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><formula xml:id="formula_0">Let S = {(v i , d i )} N</formula><p>i=1 denote the set of experts and his/her documents, where v i is a candidate expert, d i is the set of support documents authored by (or associated with) expert v i and N is the expert size. The input of our problem also includes a query d q , which can be also viewed as a document. There can be various kinds of documents in different applications. For example, in an academic network, the documents could be papers published by researchers, while in a Quora-like website, the documents could be the questions (or answers) that users have asked (or answered). Given this, we can formally define the expert finding problem as follows:</p><p>Definition 1 Expert Finding. Given a set S and a query d q , the objective here is to learn a function f using documents d i in each expert v i and the query d q , in order to predict a ranked list R(v i , d i ) ⊂ S with |R| = k, which is the top-k relevant experts in S with respect to d q .</p><p>One challenge here is that experts and query documents are two different kinds of entities. This means that they cannot be represented in a common space and the relevance between an expert and query documents cannot be measured directly. An alternative method is to measure the relevance based on expert v i 's documents d i , where experts with a more relevant document to the query should be ranked higher. The central problem is how to model the representations for documents and queries so that the similarity score can be easily estimated. In this paper, we propose a model based on the similarity matrix and restricted convolution to address this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our model</head><p>The basic idea of our model is to cast this problem as an image recognition problem. Our model first constructs a similarity matrix using the embedding of words contained in the document and the query. Viewing the similarity matrix as an image, a restricted convolutional neural network is employed to learn the representations and also predict the relevance score of a candidate to the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embedding and Similarity Matrix</head><p>Word Embedding It is easy to understand that both the query d q and candidate document d i can be represented as word sequences. In order to construct a similarity matrix, a variety of methods can be used to compute the similarity between words. For example, the similarity can be simply defined as 1 or 0 to indicate whether two words are identical; however, that ignores the semantic information between two similar words. Considering the semantic information, we use the word embedding technology, i.e., the Word2Vec model <ref type="bibr" target="#b13">[14]</ref>, to represent each word as a multi-dimensional vector, and then compute the similarity.</p><p>For completeness, we give a brief introduction to the Word2Vec model. Word2Vec employs a neural network to learn word embedding for each word. The neural network architecture (the skip-gram model) consists of an input layer, a projection layer, and an output layer. The objective is to maximize the probability of surrounding words for an input word in the corpus. Therefore, the objective can be written as:</p><formula xml:id="formula_1">1 T T t=1 wj ∈nb(wt) log p(w j |w t )</formula><p>where T is the size of the corpus, nb(w t ) is the set of surrounding words of w t , and |nb(w t )| is determined by the window size in training. The probability p(w j |w t ) is the hierarchical softmax of the word embedding of w j and w t . The authors demonstrate that semantic relationships are often preserved in vector operations on word embeddings, e.g., vec("King") − vec("Man") + vec("Woman") results in a vector that is closest to the vector representation of the word "Queen." Due to its high quality and low computational cost, we use Word2Vec embedding as our preferred embedding.</p><p>S 1 ：Chinese people like to spend the spring festival with their family and friends.</p><p>S 2 ：Chinese people enjoy the spring festival with their friends and family. Similarity Matrix based on Embedding Given word embeddings, there are many measures to obtain the similarity score, such as euclidean distance, cosine metric and dot product. In this paper, cosine metric is adopted to compute the similarity score between two embeddings. Therefore, the similarity matrix M can be written as:</p><formula xml:id="formula_2">M i,j = vec(w i ) T vec(w j ) vec(w i ) • vec(w j ) (1)</formula><p>where w i is the i-th word in query d q , w j is the j-th word in document d i , vec(w) is the Word2Vec embedding of the word w, and • is the norm of Word2Vec embedding.</p><p>In this way, similarity matrix M can provide meaningful matching information between query and document at word, phrase, and sentence level. Take two sentences in Figure <ref type="figure" target="#fig_1">2</ref> as an example, we find that these two sentences are similar at all three mentioned levels. At the word level, these sentences not only have identical word pairs, e.g., "Chinese-Chinese," but also have similar word pairs, e.g., "like-enjoy." At the phrase level, sentences can be broken down into three matching phrase pairs, e.g., "(Chinese people like)-(Chinese people enjoy)." These three mentioned phrase pairs roughly construct sentences, which indicates the similarity at the sentence level.</p><p>Similarity Matrix with IDF Similarity matrix mainly focuses on the word similarity of two documents, ignoring how specific and distinctive a word is. Take two sentences in Figure <ref type="figure" target="#fig_1">2</ref> as an example again, "chinese" and "festival" are always more specific than "people" and "enjoy" and should be given more attention. TF-IDF is the most common measurement for scoring word specificity. In this paper, similarity matrix already contains the whole words in the document, so only the IDF needs to be taken into account. IDF is the logarithmically scaled inverse fraction of the documents that contains the word. There are a whole family of inverse functions, and here we choose the smooth IDF:</p><formula xml:id="formula_3">IDF (w) = log(1 + N n w )</formula><p>where n w is the number of documents containing the word w and N is the total number of documents. Similarity matrix with IDF can be regarded as the supplementary of similarity matrix for word-specific information, but it cannot completely replace the similarity matrix, which is discussed in Section 4.2 . Formally, it can be written as:</p><formula xml:id="formula_4">M IDF i,j = M i,j * IDF (w i ) * IDF (w j )</formula><p>where the smooth IDF will not change the sign of word similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EFCNN: Expert Finding with Restricted CNN</head><p>Restricted Convolution Inspired by the great success of the CNN in image recognition, <ref type="bibr" target="#b16">[17]</ref> views the similarity matrices as images, which can be the input for CNN. However, as shown in Figure <ref type="figure" target="#fig_3">4</ref>, the similarity matrix is slightly different from the traditional image, with its sparse value and regional discontinuity.</p><p>To ease this problem, we adopt an intuitive method named restricted convolution to edit the convolution structure to produce position-based filters for each layer. Specifically, the closer a position is to the central axis, the higher its weight is. In this paper, we use two decay functions, including linear and exponential decay.</p><p>linear decay is a common decay function. If the weight attenuation follows linear decay, the weights w (1,k) ∈ R m×n of k-th filter in the l-th restricted convolutional layer can be written as:</p><formula xml:id="formula_5">w (l,k) i,j = (α + | n 2 − j| n 2 * (1 − α)) * w (l,k) i, n 2<label>(2)</label></formula><p>where α is the decency coefficient of linear decay. exponential decay considers the smooth of decay. If the weight attenuation follows exponential decay, the weights are computed as following:</p><formula xml:id="formula_6">w (l,k) i,j = e −β|j− n 2 | * w (l,k) i, n 2<label>(3)</label></formula><p>where β is the decency coefficient of exponential decay. Obviously, comparing to standard convolutional layer, each filter in restricted convolution only has one column variable in central axis, which will accelerate the training process. In addition, the restricted convolution can also be transposed as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Forward Network Same with standard convolution, the k-th filter in restricted convolutional layer is used to compute dot product between its weights w (l,k) and regions in the input z (l−1) . An element-wise activation function δ is applied to obtain a non-linear feature map z (l,k) . Formally, we have:</p><formula xml:id="formula_7">z (0) = M ⊕ M IDF z (l,k) x,y = δ( c (l−1) −1 t=0 mt−1 i=0 nt−1 j=0 w (l,k) i,j • z (l−1,k) x+i,y+j + b (l,k) )<label>(4)</label></formula><p>where m t , n t denotes the size of t-th filter, c ( l) denotes the number of filters in the l-th layer and b (l,k) is a bias term.</p><p>In addition to reducing the spatial size of the feature maps, max-pooling layers also operate independently on every output of convolutional layers and resize them spatially. Therefore, the output z (l,k) of the max-pooling layer can be written as:</p><formula xml:id="formula_8">z (l,k)</formula><p>x,y = max</p><formula xml:id="formula_9">0≤i&lt;r k max 0≤j&lt;r k z (l−1,k) x•r k +i,y•r k +j<label>(5)</label></formula><p>where r k denotes the size of the k-th pooling filter, which is set to 2 in our model. The final feature maps are then turned into a vector and passed through an MLP with several hidden layers. In this paper, we use only two fully-connected layers. For the final output, a single unit is connected to all units of the last hidden layer:</p><formula xml:id="formula_10">s = W 2 δ(W 1 • z + b 1 ) + b 2<label>(6)</label></formula><p>where W 1 and W 2 are the weights of fully-connected layers with b 1 and b 2 are the bias terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization and Model Training</head><p>As the task is formalized as a ranking problem, we can utilize pairwise ranking loss such as hinge loss for training. Given a triple (d q , d + , d − ), where document d + is ranked higher than document d − with respect to query document d q , the loss function is defined as:</p><formula xml:id="formula_11">Loss(d q , d + , d − ) = max(0, 1 − s(d q , d + ) + s(d q , d − ))</formula><p>where s(d q , d + ) and s(d q , d − ) are the corresponding predicted similarity scores.</p><p>Since the size of expert-finding datasets we use is relatively small, for experiments on these datasets we train our model on a task called citation prediction. Given the abstracts of three documents, the model needs to give higher rank to the document that has citation relationship with the query. Obviously, to complete the task, the model also needs to compute the relevance of two documents.</p><p>The training dataset of our model is collected from an academic search system Aminer <ref type="bibr" target="#b20">[21]</ref>.</p><p>Training is done through stochastic gradient descent over mini-batches, with the Adagrad update rule <ref type="bibr" target="#b4">[5]</ref>. It achieves good performance with a learning rate of 0.001. For regularization, we employ dropout <ref type="bibr" target="#b6">[7]</ref> on the penultimate layer, which prevents co-adaptation of hidden units by randomly dropping out, i.e., set to zero. To avoid over-fitting, we apply an early-stop strategy <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>To evaluate the proposed model, we conduct the experiments of expert finding problem on three datasets.</p><p>Datasets As the paper-reviewer assignment is private and interest-related, there is no publicly labeled dataset. It is difficult to create one as well. Therefore, for the purpose of evaluation, we collect three datasets from an online system and human judgments.</p><p>Paper-Reviewer: This dataset comes from an online system which connects journal editors with qualified journal reviewers. The system recommends journal submissions that are posted by journal editors to qualified reviewers who are willing to review. It includes 540 papers submitted to ten journals and 2,359 experts' invitation responses. Among these responses, 953 are "agree", while the rest are viewed as "decline" (including "unavailable" and "no response"). Basically, we consider "agree" as relevant and "decline" as irrelevant.</p><p>Topic-Expert: This dataset is based on papers from Aminer <ref type="bibr" target="#b20">[21]</ref>. It consists of 86 papers with 20 candidate experts for each query. In this dataset, we follow a traditional expertise matching setting such as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Patent-Relevance: This dataset is based on documents of patents, which comes from the Patent Full-Text Datasets of the United States Patent and Trademark Office. It consists of 67 patent queries with 20 candidate patents for each query.</p><p>In Topic-Expert and Patent-Relevance, we gather relevance judgments from college students and experts on patent analysis as the ground truth. The relevance is simply expressed as binary: relevant or irrelevant. In these three datasets, only the first 64 words are chosen for the abstracts of the papers or the patent documents.</p><p>Comparison Methods We compare the following methods in the experiment:</p><p>-BM25: The relevance score between query q and document d is measured by the BM25 score, where each word in query q is considered as a keyword. The relevance score is defined as:</p><formula xml:id="formula_12">BM25(q, d) = w∈q IDF(w) • N w d • (k 1 + 1) N w d + k 1 • (1 − b + b • N d λ )</formula><p>where IDF(•) is inverse document frequency, N w d is word w's frequency in document d, and N d is the length of d. We set k = 2, b = 0.75, and λ as the average document length; -MixMod <ref type="bibr" target="#b21">[22]</ref>: While another setting is the same as BM25, the relevance score between query q and expert e is defined as:</p><formula xml:id="formula_13">P (q|e) = dj ∈Di k m=1 ti∈q P (t i |θ m )P (θ m |d j )P (d j |e)</formula><p>where P (t i |θ m ) denotes the probability of generating a term given a theme θ m and P (θ m |d j ) denotes the probability of generating a theme given a document d j ; -Doc2Vec <ref type="bibr" target="#b11">[12]</ref>: We represent each document via Paragraph Vector model.</p><p>The similarity score between two documents is produced by the cosine metric of their representations;</p><p>Table <ref type="table">1</ref>. Results of relevance assignment(%). NG is the simplify of NDCG.</p><p>Paper-Reviewer Topic-Expert Patent-Relevance Method NG@1 NG@3 NG@5 NG@3 NG@5 NG@10 NG@3 NG@5 NG@10 BM25 36.9 40. <ref type="bibr" target="#b3">4</ref>  -WMD <ref type="bibr" target="#b9">[10]</ref>: We apply the Word Mover's Distance (WMD) to measure the similarity between two documents. The WMD is the minimum distance required to transport the words from one document to another based on word embeddings; -LSTM-RNN <ref type="bibr" target="#b15">[16]</ref>: <ref type="bibr" target="#b15">[16]</ref> adopts an LSTM to construct sentence representations and uses cosine similarity to output the similarity score; -MatchPyramid <ref type="bibr" target="#b16">[17]</ref>: The MatchPyramid is a standard CNN built on the standard similarity matrix to get the similarity score.</p><p>As for neural models, we can see that Doc2Vec and LSTM-RNN are all sentence representation models, while WMD, MatchPyramid and EFCNN are the interaction-based model.</p><p>Parameter settings In our model, there are two restricted convolutional layers, both having 64 filters. All filters are set to 3 × 7 and Batch Normalization <ref type="bibr" target="#b7">[8]</ref> is adding to all restricted convolutional layers. The number of hidden units of the fully-connected layer is set to 256. And the hyperparameters α and β are set to 0.2 and 2.0 respectively, which is discussed in Section 4.2.</p><p>The Word2Vec embedding is learned on AMiner data <ref type="bibr" target="#b20">[21]</ref>. The embedding is trained using the Skip-gram architecture <ref type="bibr" target="#b13">[14]</ref>. For a fair comparison, word embedding of all comparison models is the same as that of the proposed model and the dimension number of word embedding is set to 150.</p><p>Evaluation Metric Formalized as a ranking problem, the output is a ranked list of experts, where the order depends on the maximum similarity score of their documents regarding the query. The goal is to rank the positive one higher than the negative ones. Therefore, we use NDCG@n <ref type="bibr" target="#b8">[9]</ref> as an evaluation metric. Formally, we have:</p><formula xml:id="formula_14">NDCG@n = n i=1 2 r i −1 log 2 (i+1) |R| i=1 2 R i −1 log 2 (i+1)</formula><p>where r i is the relevance of i-th expert in the output and R represents the list of experts (ordered by their relevant) in the length of n. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Performance Analysis We compare the performance of all methods on three datasets. Table <ref type="table">1</ref> shows the ranking accuracy of different methods in terms of NDCG, where measures are averaged for all queries on each dataset. Roughly speaking, the neural methods (such as WMD and EFCNN) outperform the traditional methods in most cases. Only taking the exact word matching into account, traditional methods will lose important information easily, while neural methods based on word embedding can learn better representations and deal with the mismatch problem effectively. As for neural models, we can see that interaction based models, such as WMD and MatchPyramid, perform better than representation based models. This is mainly because these model can capture more detailed information from the interaction of documents.</p><p>On Paper-Review and Topic-Expert, we also see that our model achieves significant improvement compared to all the baselines. On Paper-Reviewer, EFCNN clearly outperforms the comparison methods in all by 2.2% and 1.9% (p−value 0.01 in both cases by t-test) in terms of NDCG@1 and NDCG@5 respectively. It indicates that restricted convolution deals with sparse but position-depended signals effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How the Similarity Matrix with IDF works</head><p>To have a better understanding of how the similarity matrix with IDF works, we show the pixel images of matrices without/with IDF in Figure <ref type="figure" target="#fig_3">4</ref>. From the pixel image, we can see that similarity matrix focuses more on word similarity. While the similarity matrix with IDF focuses more on details, it will only show the significant result when two words are similar and when both of them are important to the document. These two matrices support each other, and we will lose some information if we drop any one of them.</p><p>Sensitivity Analysis of Hyperparameters Since there are two different decay functions, our model has two versions, denoted as EFCNN-Lin and EFCNN-Exp. There are two hyperparameters α and β in EFCNN-Lin and EFCNN-Exp, respectively. We further study the effect of different choices of α and β. The experimental result is listed in Table <ref type="table" target="#tab_1">2</ref>. The results indicate that the best model setting is always encouraging weight decay. Therefore, the consideration of weight decay along with positions in convolution is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we study the problem of expert finding. We formalize the problem and propose a deep learning model based on restricted convolutional neural networks. We prove that the proposed model can capture the relevant information between two documents. Compared to several state-of-the-art models, our model can significantly improve the performance of expert finding. The problem of expert finding represents an interesting and important research direction. In future work, it would be intriguing to investigate a deep architecture to learn expert representations directly. It would also be interesting to study how to incorporate both network information and content information together to better learn the expert representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of expert finding: each expert has a set of documents; each document has a similarity score with respect to query. Intuitively, expert A is more relevant than expert B regarding the query .</figDesc><graphic url="image-1.png" coords="3,217.68,111.87,179.46,197.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture of EFCNN</figDesc><graphic url="image-2.png" coords="4,150.01,113.08,317.29,164.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of two similar sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The visualization result of two matrices based on a document pair. The brighter the pixel is, the larger value it has. The document pair is as follows: D1: Privacy is an enormous problem in online social networking sites. D2: While online social networks encourage sharing information, they raise privacy issues.</figDesc><graphic url="image-3.png" coords="10,167.71,115.28,281.28,155.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The effect of hyperparameters α and β on Topic-Expert.</figDesc><table><row><cell cols="2">NG@3 NG@10</cell></row><row><cell>EFCNN-Lin(α=0.2) 67.7</cell><cell>70.8</cell></row><row><cell>EFCNN-Lin(α=0.5) 66.2</cell><cell>68.7</cell></row><row><cell>EFCNN-Lin(α=1.0) 64.5</cell><cell>66.4</cell></row><row><cell>EFCNN-Exp(β=1.0) 66.3</cell><cell>68.7</cell></row><row><cell>EFCNN-Exp(β=1.5) 66.5</cell><cell>68.0</cell></row><row><cell>EFCNN-Exp(β=2.0) 67.3</cell><cell>69.5</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Formal models for expert finding in enterprise corpora</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
				<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nevill-Manning</surname></persName>
		</author>
		<title level="m">Recommending papers by mining the web</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Research on expert search at enterprise track of trec</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Formal models for expert finding on dblp bibliography data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automating the assignment of submitted manuscripts to reviewers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual international ACM SIGIR conference on Research development in information retrieval</title>
				<meeting>the 15th annual international ACM SIGIR conference on Research development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-aspect expertise matching for review assignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karimzadehgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Belford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
				<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Expertise modeling for matching papers with reviewers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 13th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Text matching as image recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical language models for expert finding in enterprise corpora</title>
		<author>
			<persName><forename type="first">D</forename><surname>Petkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic early stopping using cross validation: quantifying the criteria</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing &amp; management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A mixture model for expert finding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD&apos;08</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="466" to="478" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
