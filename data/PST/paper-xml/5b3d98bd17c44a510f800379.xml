<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Biased Dropout and Crossmap Dropout: Learning towards Effective Dropout Regularization in Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-04-06">April 6, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alvin</forename><surname>Poernomo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Ubiquitous IT</orgName>
								<orgName type="institution">Dongseo University</orgName>
								<address>
									<settlement>Busan</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dae-Ki</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Ubiquitous IT</orgName>
								<orgName type="institution">Dongseo University</orgName>
								<address>
									<settlement>Busan</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Biased Dropout and Crossmap Dropout: Learning towards Effective Dropout Regularization in Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-04-06">April 6, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">E1CA014F4D2B96832C46B94F45C6F744</idno>
					<idno type="DOI">10.1016/j.neunet.2018.03.016</idno>
					<note type="submission">Preprint submitted to Journal of L A T E X Templates</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dropout</term>
					<term>regularization</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training a deep neural network with a large number of parameters often leads to overfitting problem. Recently, Dropout has been introduced as a simple, yet effective regularization approach to combat overfitting in such models. Although Dropout has shown remarkable results on many deep neural network cases, its actual effect on CNN has not been thoroughly explored. Moreover, training a Dropout model will significantly increase the training time as it takes longer time to converge than a non-Dropout model with the same architecture. To deal with these issues, we address Biased Dropout and Crossmap Dropout, two novel approaches of Dropout extension based on the behaviour of hidden units in CNN model. Biased Dropout divides the hidden units in a certain layer into two groups based on their magnitude and applies different Dropout rate to each group appropriately. Hidden units with higher activation value, which give more contributions to the network final performance, will be retained by a lower Dropout rate, while units with lower activation value will be exposed to a higher Dropout rate to compesate the previous part. The second approach is Crossmap Dropout, which is an extention of the regular Dropout in convolution layer. Each feature map in a convolution layer has a strong correlation between each other, particularly in every identical pixel location in each feature map.</p><p>Crossmap Dropout tries to maintain this important correlation yet at the same time break the correlation between each adjacent pixel with respect to all feature maps by applying the same Dropout mask to all feature map, so that all pixels</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays, deep learning model has been evolving rapidly and showing remarkable results in various areas, including image recognition <ref type="bibr" target="#b0">[1]</ref>, speech recognition <ref type="bibr" target="#b1">[2]</ref>, language modeling <ref type="bibr" target="#b2">[3]</ref>, information retrieval <ref type="bibr" target="#b3">[4]</ref>, etc. Its outstanding ability to learn very complex, both linear and non-linear relationships directly from the data makes deep neural network perfectly suitable to perform intelligent tasks similar to those performed by human brain. Recently, Convolutional neural network (CNN), one of the popular deep learning models which originally introduced for computer vision task, has constantly shown promising results in large areas and applications. With the advancement of GPU-based parallel computing and the availability of large public datasets on the Internet, such as Imagenet, CNN marks its superiority over the conventional computer vision approaches in ILSVRC2012 competition. A typical CNN architecture consists of several layers of alternating convolution and pooling layers at lower stages, which At higher stage, those layers are usually combined with fully connected layers, which correspond to a traditional MLP, to perform the classification task.</p><p>Deep models, including CNN with a large number of parameters has shown to be very powerful, but overfitting becomes a crucial problem as a big network with millions of parameters can be easily overfit, especially when provided with insufficient number of training data. This will lead to a poor prediction performance since the network fails to generalize the unseen or testing data.</p><p>Correspondingly, many methods have been developed for regularizing deep neural networks, including early stopping of training as soon as its performance on validation data becomes degraded. Other methods, like introducing weight penalties and soft weight sharing <ref type="bibr" target="#b4">[5]</ref> have also been proven to improve the generalization performance.</p><p>Recently, Dropout regularization approach has been introduced and considered as the most effective way to reduce overfitting of deep neural network <ref type="bibr" target="#b5">[6]</ref>.</p><p>Inspired by combining and averaging the results of many models to improve the performance of neural network, Dropout network literally tries to combine exponentially many different neural network architectures which share the same weights by randomly omitting a set of hidden units at each training case. Extensive research work have shown that Dropout significantly reduces overfitting and improves generalization performance <ref type="bibr" target="#b6">[7]</ref>. Since then, there have been several extensions to improve the Dropout performance, including DropConnect <ref type="bibr" target="#b7">[8]</ref>. It was proposed as a generalization of Dropout, which introduces sparsity within the weights W rather than the output vectors of a layer.</p><p>In this paper, we address two major issues which can improve the effectiveness of Dropout, especially in CNN models. First, regular Dropout introduces noise within the hidden units by temporarily dropping a random set of units during each training case. Each unit has the same probability p of being omitted during training. Introducing the same proportion of noise to each hidden units could render the effectiveness of Dropout itself, since each unit has different saliency and contribution to the network final performance. Biased Dropout is introduced to address this issue by segmenting the hidden units into two groups, "important" and "less-important" group and assigning a different Dropout rate for each group. The main purpose of this approach is to retain the "important" group by giving a lower Dropout rate as this group contains units with highactivated values, thus considered as important, whose deletion will have a major impact on the network performance. Viewed another way, limiting the noise on the "important" part of the units will make the network learns and converges faster as the parameter update for this important part of the network is not as noisy as using the regular Dropout. The second issue is that Dropouts are commonly used only in the fully connected layers and its effect on convolution layer has not been sufficiently investigated. n convolution layer, units are organized in such planes, called feature maps and each feature map share a strong correlation between each other since all of them are the output of the same input with different filters. Instead of omitting a random set of units in every feature map, Crossmap Dropout tries to break the correlation between each unit with respect to all corresponding feature maps by omitting a random set of units in one feature map and applying the same Dropout mask to all remaining maps. This will ensure that each unit or pixel in the same position of all feature maps, which resembles the same feature, is either dropped or active during the training phase. This paper is structured as follows. Section 2 describes relevant previous work. Section 3 describes the motivation and the details of our proposed approaches. In Section 4, we present our experimental results with different datasets and the comparison with other related approaches. Section 5 describes the analysis and disscussions of our proposed approaches. Section 6 covers the summary of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Dropout as a regularization approach for deep fully connected networks was first introduced by Hinton et al. <ref type="bibr" target="#b5">[6]</ref>. The idea is similar to Bagging approach <ref type="bibr" target="#b8">[9]</ref> where a set of models are trained on different subsets of the same training data and the prediction results of each models are averaged in order to improve the performance. Instead of training many different architectures, which would take a lot of computation time, especially when dealing with very big architectures and be difficult to find the optimal hyperparameter values for each model, Dropout provides a way of approximately combining exponentially many different networks by randomly dropping units with a fixed probability p for each training case. Therefore, training a Dropout neural network with n hidden units can be seen as training a collection of 2 n different "thinned" models with ex-tensive weight sharing. These thinned networks can be combined into a single network at the testing phase by retaining all units and scaling the weights by (1p). It is mentioned in the extension of Srivastava et al. work <ref type="bibr" target="#b6">[7]</ref> that Dropout gives additional performance improvement when is applied to the convolution layers. Following the success of Dropout, several work have been done on improving the performance and efficiency of Dropout.</p><p>Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref> proposed Maxout Network to facilitate optimization and accuracy improvement of Dropout. The maxout unit picks the maximum value within a group of linear pieces as a generalization of rectified activation function which is capable of approximating the arbitrary convex function. They reported state-of-the-art results at that time by combining a convolution maxout network and Dropout approach, including a 0.45% and 2.47% test error rate on MNIST and SVHN dataset respectively. Wan et al. <ref type="bibr" target="#b7">[8]</ref> also proposed DropConnect, an extension of regular Dropout by introducing noise in weight vector, instead of the hidden units. DropConnect introduces a dynamic weight sparsity in the model, modifying e.g. a fully connected layer into a sparsely connected layer with its connections are chosen randomly during traning. Although DropConnect offers a marginally slower training time than the regular Dropout, its performances are proven effective on some cases, including a 1.12% test error rate on MNIST dataset using a normal MLP model and 1.94% test error rate on SVHN dataset using CNN model.</p><p>The idea to give different Dropout rate to each hidden unit has been proposed by Ba and Frey <ref type="bibr" target="#b10">[11]</ref>. They proposed another extension of Dropout, named Adaptive Dropout or Standout, which suggested that finding an optimal Dropout rate for each node based on the results of the previous layers will increase the performance of Dropout. The reason is that there might be certain hidden units that can individually make confident predictions for the presence or absence of an important feature or combination of features. Regular Dropout will ignore this confidence and drop the units with the same p probability at all times.</p><p>Therefore, in Standout approach, Dropout rates are adaptively trained for each hidden node instead of putting a consistent rate. This process is equivalent to learning a separate belief network of Dropout rates on top of the existing neural network. Adaptive Dropout approach has been proven effective on several cases, such as MNIST and NORB datasets, although the additional computation time of binary belief network becomes its drawback.</p><p>Another modification of regular Dropout was introduced by Tompson et al. <ref type="bibr" target="#b11">[12]</ref>, named Spatial Dropout. It is designed for convolution layer, where a random subset of feature maps will be dropped during each training case, instead of a random subset of units. Since natural images exhibit strong spatial correlation and the feature map activations in the convolution layer are also strongly correlated, regular Dropout tends to fail to improve generalization performance in convolution layers. They have found out that Spatial Dropout implementation improves performance on small-training-set-size dataset, such as FLIC (Frames Labeled in Cinema) dataset over the regular Dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Biased Dropout</head><p>In the original Dropout approach, each hidden unit shares a constant probability p of being omitted during training phase, despite that each hidden unit has different contributions to the network performance. Particularly, in convolution layer, a unit with high activation value indicates an important feature, whose deletion might cause a huge information loss and lead to a poor network performance during training. On the contrary, a unit with low activation value gives a small contribution value during training and eventually will receive a small amount of error gradient during backpropagation phase. Thus, we can safely say that this unit has less effect to the overall network performance compared to the previous one. This brings us a question whether it may help to have different Dropout probability rate for those two. Generally, we can distribute the hidden units in a certain layer into two groups. The first group consists of units with high activation value and the other group contains units with low activation value. This concept of unit discrimination approach is adopted from magnitude-based pruning technique <ref type="bibr" target="#b12">[13]</ref>, where it removes weights whose value are lower than a certain threshold to obtain a more compressed network with the same performance as the initiall network after a few re-training steps. Our initial hypothesis is that we can adjust the Dropout rate for each unit group 145 to improve the efficiency of Dropout. Group with high-activated units will be assigned with a low Dropout rate to preserve the important information within.</p><p>On the other hand, group with low-activated units will be exposed to a high Dropout rate in order to compesate the small amount of noise injection on the previous group (see Figure <ref type="figure" target="#fig_0">1</ref>). In the end, the total amount of noise introduced 150 in this layer is as much as the amount of regular Dropout does. Supposed that we have a l th layer with total number of n hidden units and initial p Dropout rate. Biased Dropout segments these units into two groups -g1 as the group with high activation units and g2 with low activation units.</p><p>Having unsymmetric distribution of hidden units due to ReLU activation, we consider to choose median value as the threshold value to segment the units.</p><p>Units with value equal or bigger than the threshold belong to the g1 and the rest belong to the g2. We introduce two different Dropout rate which will be assigned to each group. A lower Dropout rate p1 will be assigned to g1 in order to preserve important information with less noise, while the second group g2 will be exposed to a higher amount of noise with p2 rate. Here, we introduce a new hyperparameter c which linearly correlates p1 and p2 values.</p><formula xml:id="formula_0">p2 = c × p1; c &gt; 1 (1)</formula><p>Using a simple averaging concept, we can enumerate p1 and p2 values using equation as follows:</p><formula xml:id="formula_1">p = (p1 × n1 + p2 × n2)/n (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where p is our initial Dropout rate, n1 and n2 are the number of units in g1 and g2 respectively with n is the total units in the corresponding layer. As for the Biased Dropout steps in testing phase, we only need to scale the weights using p value instead of using p1 and p2 as the overall noise injected in this layer is as much as using the initial Dropout rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Crossmap Dropout</head><p>Crossmap Dropout is a generalization of standard Dropout, which is purposely designed for convolutional layer. The motivation of this idea is because commonly Dropout is applied in fully-connected layer, and in term of architecture, convolution layer is different from the fully-connected layer and so far, the effect of Dropout approach in convolution layer has not been thoroughly explored <ref type="bibr" target="#b13">[14]</ref>. Therefore, we propose a new generalization of Dropout based on the behavior of hidden nodes in the convolution layer. In convolution layers, units are organized in certain planes, called feature maps. These feature maps correspond to different kernels or filters we use in a convolution layer. Within a feature map, all units share the same set of weights. This weight sharing is designed to extract a certain feature from the input by performing the same operation on all different parts of the input image. A complete convolution layer is composed by several feature maps with different sets of weight vectors, in order to extract multiple features at each location of the image. These feature maps are therefore, the result of the same input, with different set of filter corresponds to each of them. Thus, these feature maps share a strong correlation between each other, which resembles the similarity with the color channel spaces in image dimensionality. Crossmap Dropout aims to maintain this correlation between feature maps, yet at the same time, break the correlation between each adjacent unit with respect to all feature maps by introducing noise in such way that all feature maps have the same Dropout mask during each training case. Having this approach instead of the regular Dropout will force the network to not depend on every feature to be present all time, instead of introducing random noise in each feature maps, which results in feature deformation.</p><p>Let X l ∈ R H×W ×D be the output map of l th convolution layer, where H is height dimension, W is width dimension, and D is the number of feature maps/channels. Supposed that we have a set of 3-D convolution filters, which is denoted by f ∈ R H ×W ×D×K where H , W , D are the height, weight, and depth dimension of the filter and K is the number of different convolution filters.</p><p>Using X l feature maps as the input to the next convolution layer (l+1) th , we can define the output maps of this layer, which are denoted by Y l+1 ∈ R H ×W ×K , as follows :</p><formula xml:id="formula_3">Y l+1 i ,j ,k = b k + H i =1 W j =1 D d=1 f i ,j ,d × X l (i +i -1),(j +j -1),d<label>(3)</label></formula><p>where H and W are the height and weight dimensions of the feature maps and K is the number of feature maps corresponding to total K numbers of convolution filters with biases b.</p><p>Applying Dropout to individual units in l th convolution layers is the same as multiplying each element of X l with each element of binary mask M ∈ {0, 1} H×W ×D where the size of mask is the same as the output map X l , with each element of M is drawn independently from Bernoulli distribution with a probability p of being 0.  </p><formula xml:id="formula_4">M l ∼ Bernoulli(p),<label>(4)</label></formula><formula xml:id="formula_5">Xl = X l * M l ,<label>(5)</label></formula><formula xml:id="formula_6">Y l+1 i ,j ,k = b k + H i =1 W j =1 D d=1 f i ,j ,d × Xl (i +i -1),(j +j -1),d<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate our proposed approches with several benchmark datasets on different domains, which are MNIST, CIFAR10, DNA splice junction, and Im-ageNet dataset. All experiments are conducted using Matconvnet library <ref type="bibr" target="#b14">[15]</ref>,</p><p>with mini-batch SGD approach. We do not use batch normalization as an additional regularizator or any optimization approaches suggested by Snoek et al. <ref type="bibr" target="#b15">[16]</ref>. In order to obtain stabler results, we train five independent networks for each approach/ model and report the mean and standard deviation values for all dataset, except for DNA spice junction and ImageNet dataset. In this section, we present some key results that show the effectiveness of our approaches along with the comparison model from other work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MNIST</head><p>The MNIST <ref type="bibr" target="#b16">[17]</ref> is a standard dataset of handwitten digits, which consists of 28x28 pixels of grayscale digit images with 50,000 training, 10,000 validation, and 10,000 test instances. Basically, the task is to classify the images into 10 digit classes (0-9). For this dataset, we use a CNN model with two convolution layers, each followed by ReLU activation and max pooling layer and one fully connected layer. Apart from the ReLU activation function, this is the same architecture used in <ref type="bibr" target="#b5">[6]</ref>. We apply Dropout after each convolution layer (p = 0.25)</p><p>and fully connected layer (p = 0.5). We have not applied any data augmentation approach on this dataset, although it is possible to achieve lower error rates by augmenting the dataset with transformations of the standard set of images <ref type="bibr" target="#b17">[18]</ref> or by averaging multiple deep neural networks with different input preprocessing results <ref type="bibr" target="#b18">[19]</ref>. Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MNIST Basic and MNIST Random Background</head><p>Following the work of <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref>, we use a variety of the original MNIST dataset which include several factors of variation. The MNIST basic dataset is a subset of the original MNIST which has been shuffled randomly from the original splits. This dataset contains the same dimension as the original MNIST, 28x28 pixels gray-scale images of digits, with 10,000 training, 2,000 validation, and 50,000 test samples. The MNIST random background is a variety of MNIST  To provide fair comparison with all methods used in previous work, we perform ten-fold cross-validation approach and report the mean and standart deviation value. We follow a standard CNN model described in Nguyen et al. work <ref type="bibr" target="#b23">[24]</ref>, which consists of two convolution and subsampling layer, followed by a fully connected layer with 100 neurons. They reported a 3.82% average test error rate using CNN model with FC Dropout (p = 0.5) from  Caffe-AlexNet model as our reference model. This model consists of five convolution layers, with max pooling layers after the first and second convolution layer, and followed by two fully connected layers. We apply Dropout at the last convolution layer (p = 0.25) and after each fully connected layer (p = 0.5).</p><p>It is important to note that we do not use batch normalization as additional regularization, since we focus on exploring the efficiency of the original Dropout and our approaches. For the performance results, we report two error rates:</p><p>top-1 and top-5 error rate, where top-5 error rate is the rate which the correct label is not among the five labels considered most probable by the model. Using the aformentioned configuration with Dropout in the fully connected layers gives us a 80.76 % top-5 accuracy, which is very similar to the results reported in Han et al. work <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref> whose model is identical as ours. Implementing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of Biased Dropout</head><p>The experiment results described in the previous section provide strong evidences that Biased Dropout can improve the generalization performance of the network by limiting noise in the "important" units and exposing more noise in the "less-important" units to compesate the other part. To investigate the effect of unit saliency to the network performance, we run an additional experiment of inversed-Biased Dropout where the more important units will be exposed to more noise. As a result, we provide a proof of contradiction of how Biased Dropout can be more effective than the regular Dropout approach.</p><p>Having exposed by more noise in the important group will eventually lead the model to an under-fitting condition (see Figure <ref type="figure" target="#fig_4">3</ref>). faster compared to one using the original Dropout (see Figure <ref type="figure" target="#fig_4">3</ref>). Additionally, choosing an optimal hyperparameter value c is also very crucial to the Biased Dropout performance. Table <ref type="table" target="#tab_7">6</ref> displays the validation performance comparison of the Biased Dropout with multiple hyperparameter value c on each dataset.</p><p>We use three different c values during the validation process, and choose the corresponding value with the highest validation performance.   Compared to Spatial Dropout architecture, one might think that our Crossmap Dropout is similar both in term of concept and practical use. However, it is worth to note that the slight difference in Dropout mask pattern between Spatial Dropout and Crossmap Dropout makes a significant impact on the network performance. We believe that Spatial Dropout introduces too much noise in the convolution maps, which leads to an underfitted model and poor test performance. We can observe the degradation performance of Spatial Dropout in every dataset we used (refer to Table <ref type="table" target="#tab_1">1</ref><ref type="table" target="#tab_2">2</ref><ref type="table" target="#tab_3">3</ref><ref type="table" target="#tab_4">4</ref><ref type="table" target="#tab_5">5</ref>), even when compared to the original Dropout performance. On the other hand, applying a Crossmap Dropout mask in the convolution maps is proven effective on most of our cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance Summary</head><p>From the performance result in Table <ref type="table" target="#tab_1">1</ref>-5, we can observe that our proposed approaches Biased Dropout and Crossmap Dropout offer a higher accuracy performance than the original Dropout. We have performed T-interval test to each dataset (except ImageNet) with 95% confidence interval. We can observe that although the true error interval of Biased and Crossmap Dropout are overlapped with the original Dropout, there is a slight significance of performance given their lower bounds are still lower than the original Dropout's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have presented two new approaches of Dropout generalization to address the current issues in Dropout implementation. We have demonstrated that our Biased Dropout can achieve better network performance by adjusting the number of noise exposure in hidden units effectively. We have also showed that implementing Crossmap Dropout in the convolution layer can boost the network performance, especially in small sized datasets. However, there are obvious drawbacks in our experiments, including the lack of mathematical justification of how Crossmap and Biased Dropout can perform better than the original Dropout, since we only provide the empirical results. Another issue, like optimizing hyperparameter values through validation process, which is computationally expensive, is also very crucial to our results. Hence, further study including mathematical formulation and efficient algorithm design are needed to provide a deeper insight and analysis of these issues. <ref type="bibr" target="#b25">[26]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Regular Dropout (a) drops units with a consistent probability p for all units. Biased Dropout (b) segments units into two groups and applies different rates for each group.</figDesc><graphic coords="8,180.04,338.32,234.43,110.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For a given</head><label></label><figDesc>Dropout mask of size H × W × K, regular Dropout performs {H × W × K} Bernoulli trials corresponding to the size of the mask itself. In Spatial Dropout case, we only perform {K} Bernoulli trials and extend the value across the entire mask. On the other hand, Crossmap Dropout performs {H × W } Bernoulli trials and extends the same values to all K feature maps. The visual comparison between original Dropout, Spatial Dropout, and Crossmap Dropout can be seen in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different Dropout masks when applied to each feature map in a convolution layer. (a) Regular Dropout (b) Spatial Dropout (c) Crossmap Dropout. Note that units in black will receive zero activation or being dropped, and units in white will be active. Only the active units will receive error gradients from the next layer during backpropagation phase.</figDesc><graphic coords="11,186.93,333.02,220.65,55.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 4 .</head><label>4</label><figDesc>Result on DNA splice junction dataset The DNA splice junction dataset is originally obtained from UCI Machine Learning Repository, in which all examples are taken from Genbank 64.1. It consists of 3,190 instances with 60 base pairs of DNA sequence each. Given a sequence of DNA, the boundary between exons (the parts of the DNA sequence retained after splicing) and intron (the parts of the DNA sequence that are spliced out) are introduced. The task is to classify the sequence into 3 categories, which are belong to EI (exon/ intron boundaries or referred to donors), IE (intron/ exon boundaries or referred as acceptors), or N (neither of those boundaries).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Using DNA splice dataset, we compare the network performance of our original Biased Dropout and inversed version of Biased Dropout. Using (b) MNIST and (c) DNA splice junction dataset, we show the convergence properties of the train/ test using original Dropout and Biased Dropout aproach. See text for the discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>we only address a minor modification from the original Dropout, spesifically on the binary mask pattern, Crossmap Dropout has no difference in term of convergence time compared to Dropout. However, our experiment on ILSVRC2012 dataset shows that Crossmap Dropout achieves a similar result with the original Dropout. We believe that the massive amount of iterations in this dataset renders the random pattern in Crossmap Dropout less effective, because the same patterns are likely to occur in the original Dropout training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: MNIST classification error(%) on different Dropout models; errors are estimated us-</cell></row><row><cell></cell><cell cols="2">ing 5 independent networks with a 95% confidence interval. The number in boldface indicates</cell></row><row><cell></cell><cell>the Dropout model with the highest performance.</cell></row><row><cell></cell><cell>MODEL</cell><cell>Error Rate</cell></row><row><cell></cell><cell>Convolution Maxout Network+Dropout (FC) [10]</cell><cell>0.45</cell></row><row><cell></cell><cell>CNN + Dropout (FC Layer)</cell><cell>0.59±0.150</cell></row><row><cell></cell><cell>CNN + Dropout (Convolution + FC Layer)</cell><cell>0.46±0.133</cell></row><row><cell></cell><cell>CNN + Biased Dropout with c = 4 (Convolution + FC</cell><cell>0.45±0.131</cell></row><row><cell></cell><cell>Layer)</cell></row><row><cell></cell><cell>CNN + Spatial Dropout (Convolution) + Dropout (FC)</cell><cell>0.55±0.145</cell></row><row><cell></cell><cell>CNN + Crossmap Dropout (Convolution) + Dropout</cell><cell>0.42±0.127</cell></row><row><cell></cell><cell>(FC)</cell></row><row><cell></cell><cell>CNN + Crossmap Dropout (Convolution) + Biased</cell><cell>0.41±0.125</cell></row><row><cell></cell><cell>Dropout with c = 4 (FC)</cell></row><row><cell></cell><cell cols="2">basic, inserted with a random background, with each pixel value of the back-</cell></row><row><cell></cell><cell cols="2">ground is generated uniformly between 0 and 255. The charateristic of this</cell></row><row><cell></cell><cell cols="2">dataset is identical with the MNIST basic dataset, which consists of 28x28 pix-</cell></row><row><cell></cell><cell cols="2">els gray-scale images of digit with random background. The dataset is split</cell></row><row><cell>250</cell><cell cols="2">into 10,000 training, 2,000 validation, and 50,000 test instances. We use the</cell></row><row><cell></cell><cell cols="2">same CNN architecture as the one used in the original MNIST dataset. We</cell></row><row><cell></cell><cell cols="2">apply Dropout after each convolution layer (p = 0.25) and fully connected layer</cell></row><row><cell></cell><cell cols="2">(p = 0.5). Using Dropout only in the fully connected layer yields us a 1.43%</cell></row><row><cell></cell><cell cols="2">and 5.29% average error rate for MNIST Basic and MNIST random background</cell></row><row><cell>255</cell><cell cols="2">respectively. Extending Dropout in all convolution layers reduces the MNIST</cell></row><row><cell></cell><cell cols="2">Basic average test error rate into 1.22% and MNIST random background into</cell></row><row><cell></cell><cell cols="2">5.12%. Biased Dropout succesfully reduces the average error further into 1.16%</cell></row><row><cell></cell><cell cols="2">and 5.06% for MNIST Basic and random background respectively. Crossmap</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MNIST Basic and MNIST Background classification error(%) on different Dropout models; errors are estimated using 5 independent networks with a 95% confidence interval.The number in boldface indicates the Dropout model with the highest performance. 7% and 18.7% test eror with Dropout and DropConnect respectively in the fully connected layer. Following this work, we are able to achieve a 17.97% average test error rate with FC Dropout and a 0.06% point additional average error reduction by affixing Dropout in the last convolution layer. Substituting Dropout with Biased Dropout reduces the average error rate into 17.88%. In-</figDesc><table><row><cell></cell><cell>MODEL</cell><cell cols="2">MNIST Basic MNIST Background</cell></row><row><cell></cell><cell></cell><cell>Error Rate</cell><cell>Error Rate</cell></row><row><cell></cell><cell>CNN + Dropout (FC Layer)</cell><cell>1.43±0.104</cell><cell>5.29±0.196</cell></row><row><cell></cell><cell>CNN + Dropout (Convolution + FC Layer)</cell><cell>1.22±0.096</cell><cell>5.12±0.193</cell></row><row><cell></cell><cell>CNN + Biased Dropout with c = 2 for Basic</cell><cell></cell></row><row><cell></cell><cell>and c = 4 for Background (Convolution +</cell><cell>1.16±0.094</cell><cell>5.06±0.192</cell></row><row><cell></cell><cell>FC Layer)</cell><cell></cell></row><row><cell></cell><cell>CNN + Spatial Dropout (Convolution) +</cell><cell>1.45±0.105</cell><cell>5.22±0.195</cell></row><row><cell></cell><cell>Dropout (FC)</cell><cell></cell></row><row><cell></cell><cell>CNN + Crossmap Dropout (Convolution) +</cell><cell>1.07±0.090</cell><cell>5.04±0.192</cell></row><row><cell></cell><cell>Dropout (FC)</cell><cell></cell></row><row><cell></cell><cell>CNN + Crossmap Dropout (Convolution) +</cell><cell></cell></row><row><cell></cell><cell>Biased Dropout with c = 2 for Basic and</cell><cell>1.09±0.091</cell><cell>5.08±0.193</cell></row><row><cell></cell><cell>c = 4 for Background (FC)</cell><cell></cell></row><row><cell></cell><cell cols="3">Dropout yields the lowest average error rate for both datasets, which is 1.07% for</cell></row><row><cell>260</cell><cell cols="3">MNIST Basic and 5.04% for MNIST random background. Combining Crossmap</cell></row><row><cell></cell><cell cols="3">and Biased Dropout does not give us the best performance result, although its</cell></row><row><cell></cell><cell cols="3">average error performance result yields 0.13% point reduction for MNIST Basic</cell></row><row><cell></cell><cell cols="3">and 0.04% point reduction for MNIST random background compared to the</cell></row><row><cell></cell><cell>original Dropout.</cell><cell></cell></row><row><cell>265</cell><cell>4.3. Result on CIFAR10 dataset</cell><cell></cell></row><row><cell></cell><cell cols="3">The CIFAR10 [22] dataset consists of 32x32 pixel of RGB natural color im-</cell></row><row><cell></cell><cell cols="3">ages with 50,000 training and 10,000 test samples. The task is to classify the</cell></row></table><note><p>jecting a random-patterned noise with Crossmap Dropout in the convolution layer yields a 17.80% average error rate and combining it with Biased Dropout in the fully connected layer presents the lowest average error rate of 17.55% from all of our models.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: CIFAR10 classification error(%) on different Dropout models; errors are estimated</cell></row><row><cell></cell><cell cols="2">using 5 independent networks with a 95% confidence interval</cell></row><row><cell></cell><cell>MODEL</cell><cell>Error Rate</cell></row><row><cell></cell><cell>CNN + Dropout (FC Layer, 0.001 learning rate, 150</cell><cell>19.7</cell></row><row><cell></cell><cell>epochs) [8]</cell></row><row><cell></cell><cell>CNN + DropConnect (FC Layer, 0.001 learning rate, 150</cell><cell>18.7</cell></row><row><cell></cell><cell>epochs) [8]</cell></row><row><cell></cell><cell>CNN + Dropout (Fully Connected Layer)</cell><cell>17.97±0.753</cell></row><row><cell></cell><cell>CNN + Dropout (Convolution + FC Layer)</cell><cell>17.91±0.752</cell></row><row><cell></cell><cell>CNN + Biased Dropout with c = 2 (Convolution + FC</cell><cell>17.88±0.751</cell></row><row><cell></cell><cell>Layer)</cell></row><row><cell></cell><cell>CNN + Spatial Dropout (Convolution) + Dropout (FC)</cell><cell>19.21±0.772</cell></row><row><cell></cell><cell>CNN + Crossmap Dropout (Convolution) + Dropout</cell><cell>17.80±0.749</cell></row><row><cell></cell><cell>(FC)</cell></row><row><cell></cell><cell>CNN + Crossmap Dropout (Convolution) + Biased</cell><cell>17.55±0.746</cell></row><row><cell></cell><cell>Dropout with c = 2 (FC)</cell></row><row><cell></cell><cell cols="2">the ten-fold cross-validation results. Corresponding to their work, we obtain a</cell></row><row><cell></cell><cell cols="2">3.31% average error rate and by extending Dropout to all convolution layers</cell></row><row><cell>300</cell><cell cols="2">(p = 0.25), we reduce the average error rate by 0.06% point. Biased Dropout</cell></row><row><cell></cell><cell cols="2">and Crossmap Dropout succesfully decrease the average error rate into 3.13%</cell></row><row><cell></cell><cell cols="2">and 2.63% respectively. Additional experiment of combining Crossmap Dropout</cell></row><row><cell></cell><cell cols="2">in convolution layers and Biased Dropout in the fully connected layer achieves</cell></row><row><cell></cell><cell>the lowest average error rate so far by 2.13%.</cell></row><row><cell>305</cell><cell>4.5. Result on ImageNet dataset</cell></row><row><cell></cell><cell cols="2">We further examine the performance of our proposed approaches on the Ima-</cell></row><row><cell></cell><cell cols="2">geNet ILSVRC2012 dataset, which has 1.2 million training and 50,000 validation</cell></row><row><cell></cell><cell cols="2">examples. ImageNet ILSVRC2012 dataset, which is a subset of the original Im-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: DNA splice junction classification error(%) on different Dropout models; errors are</cell></row><row><cell cols="2">estimated using ten-fold cross-validation with a 95% confidence interval</cell></row><row><cell>MODEL</cell><cell>Error Rate</cell></row><row><cell>CNN Network with Dropout in FC Layer [24]</cell><cell>3.82</cell></row><row><cell>CNN + Dropout (Fully Connected Layer)</cell><cell>3.31±1.96</cell></row><row><cell>CNN + Dropout (Convolution + FC Layer)</cell><cell>3.25±1.94</cell></row><row><cell>CNN + Biased Dropout with c = 4 (Convolution + FC</cell><cell>3.13±1.91</cell></row><row><cell>Layer)</cell><cell></cell></row><row><cell>CNN + Spatial Dropout (Convolution) + Dropout (FC)</cell><cell>3.28±1.95</cell></row><row><cell>CNN + : Crossmap Dropout (Convolution) + Dropout</cell><cell>2.63±1.76</cell></row><row><cell>(FC)</cell><cell></cell></row><row><cell>CNN + : Crossmap Dropout (Convolution) + Biased</cell><cell>2.13±1.58</cell></row><row><cell>Dropout with c = 4 (FC)</cell><cell></cell></row><row><cell cols="2">ageNet dataset, consists of roughly 1,000 images in each of 1,000 categories. In</cell></row><row><cell cols="2">this experiment, we use MatconvNet-AlexNet model, which is the same as the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>AlexNet top-1 and top-5 (%) validation result using different models.</figDesc><table><row><cell>MODEL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>It is obvious that the model is not able to generalize the training data, which results in poor test performance. Biased Dropout, on the other hand, can perform relatively better when compared to the regular Dropout performance. Furthermore, Biased Dropout does not slow the training convergence as much as the regular Dropout, making it applicable to be combined with other regularization techniques at the same time without suffering too much underfitting or information loss during train-</figDesc><table /><note><p>ing. Using MNIST and DNA Splice junction dataset as our comparison models, we can observe that our model with Biased Dropout can converge 1.5 -2 times</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Biased Dropout validation set performance on each dataset with different hyperparameter c values. The number in boldface indicates the optimal validation result with the corresponding c values.</figDesc><table><row><cell>Dataset</cell><cell>c = 2</cell><cell>c = 4</cell><cell>c = 6</cell></row><row><cell>MNIST</cell><cell>0.47</cell><cell>0.46</cell><cell>0.47</cell></row><row><cell>MNIST Basic</cell><cell>1.16</cell><cell>1.26</cell><cell>1.27</cell></row><row><cell>MNIST Background</cell><cell>5.09</cell><cell>5.06</cell><cell>5.66</cell></row><row><cell>CIFAR10</cell><cell>17.85</cell><cell>19.04</cell><cell>19.65</cell></row><row><cell>DNA splice junction</cell><cell>2.81</cell><cell>3.13</cell><cell>3.44</cell></row><row><cell>ImageNet</cell><cell cols="3">41.96/19.14 41.82/19.30 41.73/18.77</cell></row><row><cell>5.2. Effect of Crossmap Dropout</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">From our experiment results, we can observe that Crossmap Dropout seems</cell></row><row><cell cols="4">to be able to provide more accurate data generalization, especially on small sized</cell></row><row><cell cols="4">datasets which require a relatively small number of iterations to train. Since</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weightsharing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<title level="m">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Maxout networks., ICML (3)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3084" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards dropout training for convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cuda-convnet</title>
		<imprint>
			<date type="published" when="2012">2012. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dna sequence classification by convolutional neural network</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Lumbanraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abapihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page">280</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient L 1 regularized logistic regression</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
