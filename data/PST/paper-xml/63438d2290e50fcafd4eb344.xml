<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EMPOWERING GRAPH REPRESENTATION LEARNING WITH TEST-TIME GRAPH TRANSFORMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-26">26 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
							<email>tzhao@snap.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Snap Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EMPOWERING GRAPH REPRESENTATION LEARNING WITH TEST-TIME GRAPH TRANSFORMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-26">26 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.03561v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTRANS which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTRANS on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTRANS performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings. Code is released at https://github.com/ChandlerBang/GTrans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph representation learning has been at the center of various real-world applications, such as drug discovery <ref type="bibr" target="#b8">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b17">Guo et al., 2022)</ref>, recommender systems <ref type="bibr" target="#b61">(Ying et al., 2018;</ref><ref type="bibr" target="#b10">Fan et al., 2019;</ref><ref type="bibr" target="#b41">Sankar et al., 2021)</ref>, forecasting <ref type="bibr" target="#b46">(Tang et al., 2020;</ref><ref type="bibr" target="#b6">Derrow-Pinion et al., 2021)</ref> and outlier detection <ref type="bibr">(Zhao et al., 2021a;</ref><ref type="bibr" target="#b5">Deng &amp; Hooi, 2021)</ref>. In recent years, there has been a surge of interest in developing graph neural networks (GNNs) as powerful tools for graph representation learning <ref type="bibr">(Kipf &amp; Welling, 2016a;</ref><ref type="bibr" target="#b48">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b19">Hamilton et al., 2017;</ref><ref type="bibr" target="#b54">Wu et al., 2019)</ref>. Remarkably, GNNs have achieved state-of-the-art performance on numerous graph-related tasks including node classification, graph classification and link prediction <ref type="bibr" target="#b4">(Chien et al., 2021;</ref><ref type="bibr" target="#b63">You et al., 2021;</ref><ref type="bibr">Zhao et al., 2022b)</ref>.</p><p>Despite the enormous success of GNNs, recent studies have revealed that their generalization and robustness are immensely challenged by the data quality <ref type="bibr">(Jin et al., 2021b;</ref><ref type="bibr" target="#b28">Li et al., 2022)</ref>. In particular, GNNs can behave unreliably in scenarios where sub-optimal data is presented:</p><p>1. Distribution shift <ref type="bibr">(Wu et al., 2022a;</ref><ref type="bibr">Zhu et al., 2021a)</ref>. GNNs tend to yield inferior performance when the distributions of training and test data are not aligned (due to corruption or inconsistent collection procedure of test data). 2. Abnormal features <ref type="bibr">(Liu et al., 2021a)</ref>. GNNs suffer from high classification errors when data contains abnormal features, e.g., incorrect user profile information in social networks. 3. Adversarial structure attack <ref type="bibr" target="#b76">(Z?gner et al., 2018;</ref><ref type="bibr" target="#b29">Li et al., 2021)</ref>. GNNs are vulnerable to imperceptible perturbations on the graph structure which can lead to severe performance degradation.</p><p>To tackle these problems, significant efforts have been made on developing new techniques from the modeling perspective, e.g., designing new architectures and employing adversarial training strategies <ref type="bibr" target="#b56">(Xu et al., 2019;</ref><ref type="bibr">Wu et al., 2022a)</ref>. However, employing these methods in practice may be</p><formula xml:id="formula_0">Transform [?] [?] [?] [?] [?] [?] [?] [x] [?] [x] [?] [?] [x] [?] [x] Node feature [?] [?]</formula><p>Test Accuracy GCN: 44.3% GAT: 21.2% APPNP: 48.3% AirGNN: 58.5%</p><formula xml:id="formula_1">[x] Node connection Original Graph [?] [?] [x] [?] [x] [?] [?] [x] [?] [?] [?] [x] [?] [x]</formula><p>[?]</p><p>[?]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refined Graph</head><p>Figure <ref type="figure">1</ref>: We study the test-time graph transformation problem, which seeks to learn a refined graph such that pre-trained GNNs can perform better on the new graph compared to the original. Shown: An illustration of our proposed approach's empirical performance on transforming a noisy graph.</p><p>infeasible, as they require additional cost of changing model architectures or re-training model parameters, especially for well-trained large-scale models. The problem is further exacerbated when adopting these techniques for multiple architectures. By contrast, this paper seeks to investigate approaches that can be readily used with a wide variety of pre-trained models and test settings for improving model generalization and robustness. Essentially, we provide a data-centric perspective to address the aforementioned issues by modifying the graph data presented at test-time. Such modification aims to bridge the gap between training data and test data, and thus enable GNNs to achieve better generalization and robust performance on the new graph. Figure <ref type="figure">1</ref> visually describes this idea: we are originally given with a test graph with abnormal features where multiple GNN architectures yield poor performance; however, by transforming the graph prior to inference (at test-time), we enable these GNNs to achieve much higher accuracy.</p><p>In this work, we aim to develop a data-centric framework that transforms the test graph to enhance model generalization and robustness, without altering the pre-trained model. In essence, we are faced with two challenges: (1) how to model and optimize the transformed graph data, and (2) how to formulate an objective that can guide the transformation process. First, we model the graph transformation as injecting perturbation on the node features and graph structure, and optimize them alternatively via gradient descent. Second, inspired by the recent progress of contrastive learning, we propose a parameter-free surrogate loss which does not affect the pre-training process while effectively guiding the graph adaptation. Our contributions can be summarized as follows:</p><p>2021), i.e., small perturbations on the input graph can mislead GNNs into making wrong predictions. Several works make efforts towards developing new GNNs or adversarial training strategies to defend against attacks <ref type="bibr" target="#b56">(Xu et al., 2019;</ref><ref type="bibr" target="#b72">Zhu et al., 2019;</ref><ref type="bibr">Jin et al., 2021a;</ref><ref type="bibr">b)</ref>. Instead of altering model training behavior, our work aims to modify the test graph to correct adversarial patterns.</p><p>Graph Structure Learning &amp; Graph Data Augmentation. Graph structure learning and graph data augmentation both aim to improve GNNs' generalization performance by augmenting the (training) graph data <ref type="bibr">(Zhao et al., 2022a)</ref>, either learning the graph from scratch <ref type="bibr" target="#b13">(Franceschi et al., 2019;</ref><ref type="bibr" target="#b22">Jin et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr">Zhao et al., 2021b)</ref> or perturbing the graph in a rule-based way <ref type="bibr" target="#b38">(Rong et al., 2020;</ref><ref type="bibr" target="#b12">Feng et al., 2020;</ref><ref type="bibr" target="#b7">Ding et al., 2022)</ref>. While our work also modifies the graph data, we focus on modifying the test data and not impacting the model training process.</p><p>Test-time Training. Our work is also related to test-time training <ref type="bibr" target="#b44">(Sun et al., 2020;</ref><ref type="bibr" target="#b50">Wang et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b66">Zhang et al., 2021;</ref><ref type="bibr">2022)</ref>, which has raised a surge of interest in computer vision recently. To handle out-of-distribution data, <ref type="bibr" target="#b44">Sun et al. (2020)</ref> propose the pioneer work of test-time training (TTT) by optimizing feature extractor via an auxiliary task loss. However, TTT alters training to jointly optimize the supervised loss and auxiliary task loss. To remove the need for training an auxiliary task, Tent <ref type="bibr" target="#b50">(Wang et al., 2021)</ref> proposes to minimize the prediction entropy at test-time. Tent works by adapting the parameters in batch normalization layers, which may not always be employed by modern GNNs. In this work, we focus on a novel perspective of adapting the test graph data, which makes no assumptions about the particular training procedure or architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We start by introducing the general problem of test-time graph transformation (TTGT). While our discussion mainly focuses on the node classification task where the goal is to predict the labels of nodes in the graph, it can be easily extended to other tasks. </p><formula xml:id="formula_2">L (f ? * (g(G Te )), Y Te ) s.t. g(G Te ) ? P(G Te ), with ? * = arg min ? L (f ? (G Tr ), Y Tr ) ,<label>(1)</label></formula><p>where L denotes the loss function measuring downstream performance; P(G Te ) is the space of the modified graph, e.g., we may constrain the change on the graph to be small.</p><p>To optimize the TTGT problem, we are faced with two critical challenges: (1) how to parameterize and optimize the graph transformation function g(?); and (2) how to formulate a surrogate loss to guide the learning process, since we do not have access to the ground-truth labels of test nodes. Therefore, we propose GTRANS and elaborate on how it addresses these challenges as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CONSTRUCTING GRAPH TRANSFORMATION</head><p>Let G Te = {A, X} denote the test graph, where A ? {0, 1} N ?N is the adjacency matrix, N is the number of nodes, and X ? R N ?d is the d-dimensional node feature matrix. Since the pre-trained GNN parameters are fixed at test time and we only care about the test graph, we drop the subscript in G Te and Y Te to simplify notations in the rest of the paper.</p><p>Construction. We are interested in obtaining the transformed test graph G Te = g(A, X) = (A , X ). Specifically, we model feature modification as an additive function which injects perturbation to node features, i.e., X = X + ? X ; we model the structure modification as A = A ? ? A 1 , where ? stands for an element-wise exclusive OR operation and ? A ? {0, 1} N ?N is a binary matrix. In other words, (? A ) ij = 1 indicates an edge flip. Formally, we seek to find ? A and ? X that can minimize the objective function:</p><p>arg min</p><formula xml:id="formula_3">? A ,? X L (f ? (A ? ? A , X + ? X ), Y) s.t. (A ? ? A , X + ? X ) ? P(A, X),<label>(2)</label></formula><p>where ? A ? {0, 1} N ?N and ? X ? R N ?d are treated as free parameters. Further, to ensure we do not heavily violate the original graph structure, we constrain the number of changed entries in the adjacency matrix to be smaller than a budget B on the graph structure, i.e., ? A ? B. We do not impose constraints on the node features to ease optimization. In this context, P can be viewed as constraining ? A to a binary space as well as restricting the number of changes.</p><p>Optimization. The optimization for ? X is easy since the node features are continuous. The optimization for ? A is particularly difficult in that (1) ? A is binary and constrained; and (2) the search space of N 2 entries is too large especially when we are learning on large-scale graphs.</p><p>To cope with the first challenge, we relax the binary space to [0, 1] N ?N and then we can employ projected gradient descent (PGD) <ref type="bibr" target="#b56">(Xu et al., 2019;</ref><ref type="bibr" target="#b15">Geisler et al., 2021)</ref> to update ? A :</p><formula xml:id="formula_4">? A ? ? P (? A -?? ? A L(? A ))</formula><p>(3) where we first perform gradient descent with step size ? and call a projection ? P to project the variable to the space P. Specifically, given an input vector p, ? P (?) is expressed as:</p><formula xml:id="formula_5">? P (p) ? ? [0,1] (p), If 1 ? [0,1] (p) ? B; ? [0,1] (p -?1) with 1 ? [0,1] (p -?1) = B, otherwise,<label>(4)</label></formula><p>where ? [0.1] (?) clamps the input values to [0, 1], 1 stands for a vector of all ones, and ? is obtained by solving the equation 1 ? [0,1] (p -?1) = B with the bisection method <ref type="bibr" target="#b33">(Liu et al., 2015)</ref>. To keep the adjacency structure discrete and sparse, we view each entry in A ? ? A as a Bernoulli distribution and sample the learned graph as</p><formula xml:id="formula_6">A ? Bernoulli(A ? ? A ).</formula><p>Furthermore, to enable efficient graph structure learning, it is desired to reduce the search space of updated adjacency matrix. One recent approach of graph adversarial attack <ref type="bibr" target="#b15">(Geisler et al., 2021)</ref> proposes to sample a small block of possible entries from the adjacency matrix and update them at each iteration. This solution is still computationally intensive as it requires hundreds of steps to achieve a good performance. Instead, we constrain the search space to only the existing edges of the graph, which is typically sparse. Empirically, we observe that this simpler strategy still learns useful structure information when combined with feature modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PARAMETER-FREE SURROGATE LOSS</head><p>As discussed earlier, the proposed framework GTRANS aims to improve the model generalization and robustness by learning to transform the test graph. Ideally, when we have test ground-truth labels, the problem can be readily solved by adapting the graph to result in the minimum cross entropy loss on test samples. However, as we do not have such information at test-time, it motivates us to investigate feasible surrogate losses to guide the graph transformation process. In the absence of labeled data, recently emerging self-supervised learning techniques <ref type="bibr" target="#b55">(Xie et al., 2021;</ref><ref type="bibr" target="#b35">Liu et al., 2022b)</ref> have paved the way for providing self-supervision for TTGT. However, not every surrogate self-supervised task and loss is suitable for our transformation process, as some tasks are more powerful and some are weaker. To choose a suitable surrogate loss, we provide the following theorem.</p><p>Theorem 1. Let L c denote the classification loss and L s denote the surrogate loss, respectively. Let ?(G) denote the correlation between ? G L c (G, Y) and ? G L s (G), and let denote the learning rate for gradient descent. Assume that L c is twice-differentiable and its Hessian matrix satisfies</p><formula xml:id="formula_7">H(G, Y) 2 ? M for all G. When ?(G) &gt; 0 and &lt; 2?(G) ? G Lc(G,Y) 2 M ? G Ls(G) 2</formula><p>, we have</p><formula xml:id="formula_8">L c (G -? G L s (G) , Y) &lt; L c (G, Y) .<label>(5)</label></formula><p>The proof can be found in Appendix A.1. Theorem 1 suggests that when the gradients from classification loss and surrogate loss have a positive correlation, i.e., ? (G) &gt; 0, we can update the test graph by performing gradient descent with a sufficiently small learning rate such that the classification loss on the test samples is reduced. Hence, it is imperative to find a surrogate task that shares relevant information with the classification task. To empirically verify the effectiveness of Theorem 1, we adopt the surrogate loss in Equation (6) as L s and plot the values of ?(G) and L c on one test graph in Cora in Figure <ref type="figure" target="#fig_0">2</ref>. We can observe that a positive ?(G) generally reduces the test loss.</p><p>Results on different losses can be found in Appendix D.9 and similar patterns are exhibited. Parameter-Free Surrogate Loss. As one popular self-supervised paradigm, graph contrastive learning has achieved promising performance in various tasks <ref type="bibr" target="#b20">(Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b63">You et al., 2021;</ref><ref type="bibr">Zhu et al., 2021b)</ref>, which indicates that graph contrastive learning tasks are often highly correlated with downstream tasks. This property is desirable for guiding TTGT as suggested by Theorem 1. At its core lies the contrasting scheme, where the similarity between two augmented views from the same sample is maximized, while the similarity between views from two different samples is minimized. However, the majority of existing graph contrastive learning methods cannot be directly applied to our scenario, as they often require a parameterized projection head to map augmented representations to another latent space, which inevitably alters the model architecture. Thus, we design a parameter-free surrogate loss which removes the projection head. Specifically, we apply an augmentation function A(?) on input graph G and obtain an augmented graph A(G). The node representations obtained from the two graphs are denoted as Z and ?, respectively; z i and ?i stand for the i-th node representation taken from them, respectively. We adopt DropEdge <ref type="bibr" target="#b38">(Rong et al., 2020)</ref> as the augmentation function A(?), and the node representations are taken from the second last layer of the trained model. Essentially, we maximize the similarity between original nodes and their augmented view while penalizing the similarity between the nodes and their negative samples:</p><formula xml:id="formula_9">min L s = N i=1 (1 - ? i z i ?i z i ) - N i=1 (1 - z i z i zi z i ),<label>(6)</label></formula><p>where {z i |i = 1, . . . , N } are the negative samples for corresponding nodes, which are generated by shuffling node features <ref type="bibr" target="#b49">(Velickovic et al., 2019)</ref>. In Eq. ( <ref type="formula" target="#formula_9">6</ref>), the first term encourages each node to be close while the second term pushes each node away from the corresponding negative sample. Note that (1) L s is parameter-free and does not require modification of the model architecture, or affect the pre-training process;</p><p>(2) there could be other self-supervised signals for guiding the graph transformation, and we empirically compare them with the contrastive loss in Appendix D.9. We also highlight that our unique contribution is not the loss in Eq. ( <ref type="formula" target="#formula_9">6</ref>) but the proposed TTGT framework as well as the theoretical and empirical insights on how to choose a suitable surrogate loss. Furthermore, the algorithm of GTRANS is provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FURTHER ANALYSIS</head><p>In this subsection, we study the theoretical property of Eq. ( <ref type="formula" target="#formula_9">6</ref>) and compare the strategy of adapting data versus that of adapting model. We first demonstrate the rationality of the proposed surrogate loss through the following theorem. Theorem 2. Assume that the augmentation function A(?) generates a data view of the same class for the test nodes and the node classes are balanced. Assume for each class, the mean of the representations obtained from Z and ? are the same. Minimizing the first term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is approximately minimizing the class-conditional entropy H(Z|Y ) between features Z and labels Y .</p><p>The proof can be found in Appendix A.2. Theorem 2 indicates that minimizing the first term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) will approximately minimize H(Z|Y ), which encourages high intra-class compactness, i.e., learning a low-entropy cluster in the embedded space for each class. Notably, H(Z|Y ) can be rewritten as H(Z|Y ) = H(Z) -I(Z, Y ). It indicates that minimizing Eq. ( <ref type="formula" target="#formula_9">6</ref>) can also help promote I(Z, Y ), the mutual information between the hidden representation and downstream class. However, we note that only optimizing this term can result in collapse (mapping all data points to a single point in the embedded space), which stresses the necessity of the second term in Eq. ( <ref type="formula" target="#formula_9">6</ref>).</p><p>Next, we use an illustrative example to show that adapting data at test-time can be more useful than adapting model in some cases. </p><formula xml:id="formula_10">Illustration. Let x1 = Agg(x 1 , {x i |i ? N 1 }) and x2 = Agg(x 2 , {x j |j ? N 2 }).</formula><p>For simplicity, we consider the following mean square loss as the classification error:</p><formula xml:id="formula_11">= 1 2 Trans(x 1 )) -y 1 ) 2 + (Trans(x 2 ) -y 2 ) 2 . (7)</formula><p>It is easy to see that reaches its minimum when Trans(x 1 ) = y 1 and Trans(x 2 ) = y 2 . In this context, it is impossible to find ? such that Trans(?) can map x 1 , x 2 to different labels since it is not a one-to-many function. However, since y 1 and y 2 are in the label space of training data, we can always modify the test graph to obtain newly aggregated features x 1 , x 2 such that Trans(x 1 ) = y 1 and Trans(x 2 ) = y 2 , which minimizes . In the extreme case, we may drop all node connections for the two nodes, and let x 1 ? x 1 and x 2 ? x 2 where x 1 and x 2 are the aggregated features taken from the training set. Hence, adapting data can achieve lower classification loss.</p><p>Remark 1. Note that the existence of two nodes with the same aggregated features but different labels is not rare when considering adversarial attack or abnormal features. We provide a figurative example in Figure <ref type="figure" target="#fig_5">6</ref> in Appendix A.3: the attacker injects one adversarial edge into the graph and changes the aggregated features x1 and x2 to be the same.</p><p>Remark 2. When we consider x1 = x2 , y 1 = y 2 , whether we can find ? satisfying Trans(x 1 ) = y 1 and Trans(x 2 ) = y 2 depends on the expressiveness of the the transformation function. If it is not powerful enough (e.g., an under-parameterized neural network), it could fail to map different data points to different labels. On the contrary, adapting the data does not suffer this problem as we can always modify the test graph to satisfy Trans(x 1 ) = y 1 and Trans(x 2 ) = y 2 . Remark 3. The above discussion can be easily extended to nonlinear GNN by considering x1 , x2 as the output before the last linear layer of GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GENERALIZATION ON OUT-OF-DISTRIBUTION DATA</head><p>Setup. Following the settings in EERM <ref type="bibr">(Wu et al., 2022a)</ref>, which is designed for node-level tasks on OOD data, we validate GTRANS on three types of distribution shifts with six benchmark datasets:</p><p>(1) artificial transformation for Cora <ref type="bibr" target="#b60">(Yang et al., 2016)</ref> and Amazon-Photo <ref type="bibr" target="#b42">(Shchur et al., 2018)</ref>, (2) cross-domain transfers for Twitch-E and FB-100 (Rozemberczki et al., 2021a) <ref type="bibr" target="#b31">(Lim et al., 2021)</ref>, and</p><p>(3) temporal evolution for Elliptic <ref type="bibr" target="#b37">(Pareja et al., 2020)</ref> and OGB-Arxiv <ref type="bibr" target="#b21">(Hu et al., 2020)</ref>. Moreoever, Cora and Amazon-Photo have 1/1/8 graphs for training/validation/test sets. The splits are 1/1/5 on Twitch-E, 3/2/3 on FB-100, 5/5/33 on Elliptic, and 1/1/3 on OGB-Arxiv. More details on the datasets are provided in Appendix C. We compare GTRANS with four baselines: empirical risk minimization (ERM, i.e., standard training), data augmentation technique DropEdge <ref type="bibr" target="#b38">(Rong et al., 2020)</ref>, test-timetraining method Tent <ref type="bibr" target="#b50">(Wang et al., 2021)</ref>, and the recent SOTA method EERM <ref type="bibr">(Wu et al., 2022a)</ref> which is exclusively developed for graph OOD issue. Further, we evaluate all the methods with four popular GNN backbones including GCN (Kipf &amp; Welling, 2016a), GraphSAGE <ref type="bibr" target="#b19">(Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b48">(Veli?kovi? et al., 2018)</ref>, and GPR <ref type="bibr" target="#b4">(Chien et al., 2021)</ref>. Their default setup follows that in EERM<ref type="foot" target="#foot_2">2</ref> . We refer the readers to Appendix C.1 for more implementation details of baselines and GTRANS. Notably, all experiments in this paper are repeated 10 times with different random seeds. Due to page limit, we include more baselines such as SR-GNN <ref type="bibr">(Zhu et al., 2021a)</ref> and UDA-GCN <ref type="bibr" target="#b51">(Wu et al., 2020)</ref> in Appendix D.1.</p><p>Results. Table <ref type="table" target="#tab_2">1</ref> reports the averaged performance over the test graphs for each dataset as well as the averaged rank of each algorithm. From the table, we make the following observations:</p><p>(a) Overall Performance. The proposed framework consistently achieves strong performance across the datasets: GTRANS achieves average ranks of 1.0, 1.7, 2.0 and 1.7 with GCN, SAGE, GAT and GPR, respectively, while the corresponding ranks for the best baseline EERM are 2.9, 3.4, 3.0 and 2.0. Furthermore, in most of the cases, GTRANS significantly improves the vanilla baseline (ERM) by a large margin. Particularly, when using GCN as backbone, GTRANS outperforms ERM by 3.1%, 5.0% and 2.0% on Cora, Elliptic and OGB-Arxiv, respectively. These results demonstrate the effectiveness of GTRANS in tackling diverse types of distribution shifts.   Extra Running Time (s) Total GPU Memory (GB) Cora Photo Ellip. Arxiv Cora Photo Ellip. Arxiv EERM 25.9 396.4 607.9 -2.5 10.5 12.8 &gt;32 GTRANS 0.3 0.5 0.6 2.6 1.4 1.5 1.3 3.9</p><p>Table 2: Efficiency comparison. GTRANS is more time-and memory-efficient than EERM.</p><p>advantage of the information from test graphs. As a test-time training method, Tent also performs well in some cases, but Tent only adapts the parameters in batch normalization layers and cannot be applied to models without batch normalization.</p><p>We further show the performance on each test graph on Cora with GCN in Figure <ref type="figure" target="#fig_2">3</ref> and the results for other datasets are provided in Appendix D.4. We observe that GTRANS generally improves over individual test graphs within each dataset, which validates the effectiveness of GTRANS.</p><p>Efficiency Comparison. Since EERM performs the best among baselines, Table <ref type="table">2</ref> showcases the efficiency comparison between our proposed GTRANS and EERM on the largest test graph in each dataset. The additional running time of GTRANS majorly depends on the number of gradient descent steps. As we only use a small number (5 or 10) throughout all the experiments, the time overhead brought by GTRANS is negligible. Compared with the re-training method EERM, GTRANS avoids the complex bilevel optimization and thus is significantly more efficient. Furthermore, EERM imposes a considerably heavier memory burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ROBUSTNESS TO ABNORMAL FEATURES</head><p>Setup. Following the setup in AirGNN <ref type="bibr">(Liu et al., 2021a)</ref>, we evaluate the robustness in the case of abnormal features. Specifically, we simulate abnormal features by assigning random features taken from a multivariate standard Gaussian distribution to a portion of randomly selected test nodes. Note that the abnormal features are injected after model training (at test time) and we vary the ratio of noisy nodes from 0.1 to 0.4 with a step size of 0.05. This process is performed for four datasets: the original version of Cora, Citeseer, Pubmed, and OGB-Arxiv. In these four datasets, the training graph and the test graph have the same graph structure but the node features are different. Hence, we use the training classification loss combined with the proposed contrastive loss to optimize GTRANS. We use GCN as the backbone model and adopt four GNNs as the baselines including GAT <ref type="bibr" target="#b48">(Veli?kovi? et al., 2018)</ref>, APPNP <ref type="bibr" target="#b27">(Klicpera et al., 2018)</ref>, AirGNN and AirGNN-t. Cor.</p><p>Elli.</p><p>Fb.</p><p>Arx.</p><p>Twi.</p><p>Adapting Model ERM Adapting Data (c) OOD Setting Results. For each model, we present the node classification accuracy on both abnormal nodes and all test nodes (i.e., both normal and abnormal ones) in Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_7">8</ref> (See Appendix D.5), respectively. From these figures, we have two observations. First, GTRANS significantly improves GCN in terms of the performance on abnormal nodes and all test nodes for all datasets across all noise ratios. For example, on Cora with 30% noisy nodes, GTRANS improves GCN by 48.2% on abnormal nodes and 31.0% on overall test accuracy. This demonstrates the effectiveness of the graph transformation process in GTRANS in alleviating the effect of abnormal features. Second, GTRANS shows comparable or better performance with AirGNNs, which are the SOTA defense methods for tackling abnormal features. It is worth mentioning that AirGNN-t improves AirGNN by tuning its hyper-parameter at test time, which aligns with our motivation that test-time adaptation can enhance model test performance. To further understand the effect of graph transformation, we provide the visualization of the test node embeddings obtained from abnormal graph (0.3 noise ratio) and transformed graph for Cora in Figures <ref type="figure" target="#fig_8">5a</ref> and<ref type="figure" target="#fig_4">5b</ref>, respectively. We observe that the transformed graph results in well-clustered node representations, which indicates that GTRANS can promote intra-class compactness and counteract the effect of abnormal patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ROBUSTNESS TO ADVERSARIAL ATTACK</head><p>Setup. We further evaluate GTRANS under the setting of adversarial attack where we perturb the test graph, i.e., evasion attack. Specifically, we use PR-BCD <ref type="bibr" target="#b15">(Geisler et al., 2021)</ref>, a scalable attack method, to attack the test graph in OGB-Arxiv. We focus on structural attacks, and vary the perturbation rate, i.e., the ratio of changed edges, from 5% to 25% with a step of 5%. Similar to Section 4.2, we adopt the training classification loss together with the proposed contrastive loss to optimize GTRANS. We use GCN as the backbone and employ four robust baselines implemented by the adversarial attack repository DeepRobust <ref type="bibr" target="#b30">(Li et al., 2020)</ref> including GAT <ref type="bibr" target="#b48">(Veli?kovi? et al., 2018)</ref>, RobustGCN <ref type="bibr" target="#b72">(Zhu et al., 2019)</ref>, SimPGCN <ref type="bibr">(Jin et al., 2021a)</ref> and GCNJaccard <ref type="bibr" target="#b56">(Xu et al., 2019)</ref> as comparisons. Among them, GCNJaccard pre-processes the attacked graph by removing edges where the similarities of connected nodes are less than a threshold; we tune this threshold at test time based on the performance on both training and validation nodes.</p><p>Results. Table <ref type="table" target="#tab_3">3</ref> reports the performances under structural evasion attack. We observe that GTRANS consistently improves the performance of GCN under different perturbation rates of adversarial attack. Particularly, GTRANS improves GCN by a larger margin when the perturbation rate is higher. For example, GTRANS outperforms GCN by over 40% under the 25% perturbation rate. Such observation suggests that GTRANS can counteract the devastating effect of adversarial attacks. In addition, the best performing baseline GCNJaccard also modifies the graph at test time, which demonstrates the importance of test-time graph adaptation. Nonetheless, it consistently underperforms our The results on all test nodes in Table <ref type="table" target="#tab_4">4</ref>. Note that "Tr" stands for GNNs used in TTGT while "Te" denotes GNNs used for obtaining predictions on the transformed graph; "Noisy" indicates the performance on the noisy graph. We observe that the transformed graph yields good performance even outside the scope it was optimized for. We anticipate that such transferability can alleviate the need for costly re-training on new GNNs.</p><p>Adapting Model vs. Adapting Data. We empirically compare the performance between adapting data and adapting model and consider the OOD and abnormal feature settings. Specifically, we use GCN as the backbone and adapt the model parameters by optimizing the same loss function as used in GTRANS. The results are shown in Figures <ref type="figure" target="#fig_8">5c</ref> and<ref type="figure" target="#fig_4">5d</ref>. In OOD setting, both adapting model and adapting data can generally improve GCN's performance. Since their performances are still close, it is hard to give a definite answer on which strategy is better. However, we can observe significant performance differences when the graph contains abnormal features: adapting data outperforms adapting model on 3 out of 4 datasets. This suggests that adapting data can be more powerful when the data is perturbed, which aligns with our analysis in Section 3.3.</p><p>Learning Features v.s. Learning Structure. Since our framework learns both node features and graph structure, we investigate when one component plays a more important role than the other.</p><p>Our results are shown in Tables <ref type="table" target="#tab_2">16</ref> and<ref type="table" target="#tab_14">17</ref> in Appendix D.8. From the tables, we observe that (1) while each component can improve the vanilla performance, feature learning is more crucial for counteracting feature corruption and structure learning is more important for defending structure corruption; and (2) combining them generally yields a better or comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>GNNs tend to yield unsatisfying performance when the presented data is sub-optimal. To tackle this issue, we seek to enhance GNNs from a data-centric perspective by transforming the graph data at test time. We propose GTRANS which optimizes a contrastive surrogate loss to transform graph structure and node features, and provide theoretical analysis with deeper discussion to understand this framework. Experimental results on distribution shift, abnormal features and adversarial attack have demonstrated the effectiveness of our method. In the future, we plan to explore more applications of our framework such as mitigating degree bias and long-range dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>A.1 THEREOM 1</p><p>Theorem 1. Let L c denote the classification loss and L s denote the surrogate loss, respectively. Let ?(G) denote the correlation between ? G L c (G, Y) and ? G L s (G), and let denote the learning rate for gradient descent. Assume that L c is twice-differentiable and its Hessian matrix satisfies</p><formula xml:id="formula_12">H(G, Y) 2 ? M for all G. When ?(G) &gt; 0 and &lt; 2?(G) ? G Lc(G,Y) 2 M ? G Ls(G) 2</formula><p>, we have</p><formula xml:id="formula_13">L c (G -? G L s (G) , Y) &lt; L c (G, Y) .<label>(8)</label></formula><p>Proof. Given that L c is differentiable and twice-differentiable, we perform first-order Taylor expansion with Lagrange form of remainder at G -? G L s (G):</p><formula xml:id="formula_14">Lc (G -?GLs (G) , Y) (9) = Lc (G, Y) -? (G) ?GLc (G, Y) 2 ?GLs (G) 2 + 2 2 ?GLs(G) H(G -??GLs(G), Y)?GLs(G),</formula><p>where ? ? (0, 1) is a constant given by Lagrange form of the Taylor's remainder (here we slightly abuse the notation), and ? (G) is the correlation between</p><formula xml:id="formula_15">? G L c (G, Y) and ? G L s (G): ? (G) = ? G L c (G, Y) T ? G L s (G) ? G L c (G, Y) 2 ? G L s (G) 2 . (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Before we proceed to the next steps, we first show that given a vector p and a symmetric matrix A, the inequality p Ap ? p 2 2 A 2 holds:</p><formula xml:id="formula_17">p Ap = ? i p u i u i p (Performing SVD on A, i.e., A = ? i u i u i ) = ? i v i v i (Let v = U p, where U = [u 1 ; u 2 ; . . . ; u n ]) ? ? max v i v i = ? max v 2 2 = ? max U p 2 2 = ? max p 2 2 (U is an orthogonal matrix) = p 2 2 A 2<label>(11)</label></formula><p>Since the Hessian matrix is symmetric, we can use the above inequality to derive:</p><formula xml:id="formula_18">Lc (G -?GLs (G) , Y) (12) ? Lc (G, Y) -? (G) ?GLc (G, Y) 2 ?GLs (G) 2 + 2 2 ?GLs(G) 2 2 H(G -??GLs(G), Y) 2.</formula><p>Then we rewrite Eq. ( <ref type="formula">12</ref>) as:</p><formula xml:id="formula_19">L c (G -? G L s (G) , Y) -L c (G, Y) ? -? (G) ? G L c (G, Y) 2 ? G L s (G) 2 + 2 2 ? G L s (G) 2 2 H(G -?? G L s (G), Y) 2 . (13) Given H(G, Y) 2 ? M , we know L c (G -? G L s (G) , Y) -L c (G, Y) ? -? (G) ? G L c (G, Y) 2 ? G L s (G) 2 + 2 M 2 ? G L s (G) 2 2 . (<label>14</label></formula><formula xml:id="formula_20">) By setting = 2?(G) ? G Lc(G,Y) 2 M ? G Ls(G) 2</formula><p>, we have</p><formula xml:id="formula_21">L c (G -? G L s (G) , Y) -L c (G, Y) = 0.<label>(15)</label></formula><p>Therefore, when</p><formula xml:id="formula_22">&lt; 2?(G) ? G Lc(G,Y) 2 M ? G Ls(G) 2</formula><p>and ?(G) &gt; 0, we have</p><formula xml:id="formula_23">L c (G -? G L s (G) , Y) &lt; L c (G, Y) .<label>(16)</label></formula><p>A.2 THEREOM 2</p><p>Theorem 2. Assume that the augmentation function A(?) generates a data view of the same class for the test nodes and the node classes are balanced. Assume for each class, the mean of the repre-sentations obtained from Z and ? are the same. Minimizing the first term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is approximately minimizing the class-conditional entropy H(Z|Y ) between features Z and labels Y .</p><p>Proof. For convenience, we slightly abuse the notations to replace zi zi and ?i ?i with z i and ?i , respectively. Then we have z i = ?i = 1. Let Z k denote the set of test samples from class k;</p><formula xml:id="formula_24">thus |Z k | = N K . Let c</formula><p>= denote equality up to a multiplicative and/or additive constant. Then the first term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) can be rewritten as:</p><formula xml:id="formula_25">N i=1 (1 -? i z i ) = N i=1 1 -? i z i c = K k=1 1 |Z k | zi?Z k -? i z i (17)</formula><p>Let c k be the mean of hidden representations from class k; then we have</p><formula xml:id="formula_26">c k = 1 |Z k | zi?Z k z i = 1 |Z k |</formula><p>?i? ?k ?i . Now we build the connection between Eq. ( <ref type="formula">17</ref>) and</p><formula xml:id="formula_27">K i=1 zi?Z k z i -c k 2 : K i=1 zi?Z k z i -c k 2 (18) = K i=1 zi?Z k z i 2 -2 zi?Z k z i c k + |Z k | c k 2 = K i=1 ? ? zi?Z k z i 2 -2 1 |Z k | zi?Z k ?i? ?k ? i z i + 1 |Z k | zi?Z k ?i? ?k ? i z i ? ? = K i=1 ? ? zi?Z k z i 2 - 1 |Z k | zi?Z k ?i?Zk ? i z i ? ? c = K i=1 ? ? 1 |Z k | zi?Z k ?i? ?k z i 2 - 1 |Z k | zi?Z k ?i?Zk ? i z i ? ? = K i=1 ? ? 1 |Z k | zi?Z k ?i? ?k z i 2 -? i z i ? ? c = K i=1 ? ? 1 |Z k | zi?Z k ?i? ?k -? i z i ? ?<label>(19)</label></formula><p>By comparing Eq. ( <ref type="formula">17</ref>) and Eq. ( <ref type="formula" target="#formula_27">19</ref>), the only difference is that Eq. ( <ref type="formula" target="#formula_27">19</ref>) includes more positive pairs for loss calculation. Hence, minimizing Eq. ( <ref type="formula">17</ref>) can be viewed as approximately minimizing Eq. ( <ref type="formula" target="#formula_27">19</ref>) or Eq. ( <ref type="formula">18</ref>) through sampling positive pairs. As demonstrated in the work <ref type="bibr" target="#b0">(Boudiaf et al., 2020)</ref>, Eq. ( <ref type="formula">18</ref>) can be interpreted as a conditional cross-entropy between Z and another random variable Z, whose conditional distribution given Y is a standard Gaussian centered around c</p><formula xml:id="formula_28">Y : Z | Y ? N (c Y , I): zi?Z k z i -c k 2 = H(Z | Y ) + D KL (Z|| Z | Y ) ? H(Z | Y )<label>(20)</label></formula><p>Hence, minimizing the first term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is approximately minimizing H(Z|Y ).</p><p>Discussion: We note that the assumption "the mean of the representations obtained from Z and ? are the same" can be inferred by the first assumption about data augmentation. Let p k (X) denote the distribution of samples with class k and let x ? p k (X) denote the sample with class k. Recall that we assume the data augmentation function A(?) is strong enough to generate a data view that can simulate the test data from the same class. In this regard, the new data view can be regarded as an independent sample from the same class, i.e., A(x) ? p k (X). Hence, the expectation of Z and ? is the same and we would approximately have that "the mean of Z and ? is the same for each class". Particularly, when the number of samples is relatively large, the mean of Z ( ?) would be close to the true distribution mean. For example, on one graph of Cora, the mean absolute difference between the two mean representations of Z and ? are [0.018, 0.009, 0.021, 0.024, 0.016, 0.014, 0.0, 0.016, 0.023] for each class, which are actually very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 A FIGURATIVE EXAMPLE</head><p>In Figure <ref type="figure" target="#fig_5">6</ref>, we show an example of adversarial attack which causes the aggregated features for two nodes to be the same. Given two nodes x 1 and x 2 and their connections, we are interested in predicting their labels. Assume a mean aggregator is used for aggregating features from the neighbors. Before attack, the aggregated features for them are x1 = [0.45] and x2 = [0.53] while after attack the aggregated features become the same x1 = x2 = [0.45]. In this context, it is impossible to learn a classifier that can distinguish the two nodes. </p><formula xml:id="formula_29">Attack [0.3] [0.3] [0.8] [0.2] [0.7] [0.7] [0] [0.2] ? ? ? ? [0.3] [0.3] [0.8] [0.2] [0.7] [0.7] [0.2] ? ? ? ? ? # ? = 0.45 , ? # ? = 0.53 ? # ? = 0.45 , ? # ? = 0.45 [0]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ALGORITHM</head><p>We show the detailed algorithm of GTRANS in Algorithm 1. In detail, we first initialize ? A and ? X as zero matrices and calculate L s based on Eq. ( <ref type="formula" target="#formula_9">6</ref>). Since we alternatively optimize ? A and ? X , we update ? X every ? 1 epochs and update ? A every ? 2 epochs. When the optimization is done, we sample the discrete graph structure for K times and select the one that results in the smallest L s as the final adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DATASETS AND HYPER-PARAMETERS</head><p>In this section, we reveal the details of reproducing the results in the experiments. We will release the source code upon acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 OUT-OF-DISTRIBUTION (OOD) SETTING</head><p>The out-of-distribution (OOD) problem indicates that the model does not generalize well to the test data due to the distribution gap between training data and test data <ref type="bibr">(Yang et al., 2021a)</ref>, which is also referred to as distribution shifts. Numerous research studies have been conducted to explore this problem and propose potential solutions <ref type="bibr" target="#b14">(Ganin et al., 2016;</ref><ref type="bibr">Zhu et al., 2021a;</ref><ref type="bibr">Yang et al., 2021b;</ref><ref type="bibr">c;</ref><ref type="bibr">Wu et al., 2022a;</ref><ref type="bibr">Liu et al., 2022a;</ref><ref type="bibr" target="#b2">Chen et al., 2022;</ref><ref type="bibr" target="#b1">Buffelli et al., 2022;</ref><ref type="bibr" target="#b16">Gui et al., 2022;</ref><ref type="bibr">Wu et al., 2022b;</ref><ref type="bibr" target="#b64">You et al., 2023)</ref>. In the following, we introduce the datasets used for evaluating the methods that tackle the OOD issue in graph domain.</p><p>Dataset Statistics. For the evaluation on OOD data, we use the datasets provided by <ref type="bibr">Wu et al. (2022a)</ref>. The dataset statistics are shown in Table <ref type="table">5</ref>, which includes three distinct type of distribution shifts: (1) artificial transformation which indicates the node features are replaced by synthetic spurious features; (2) cross-domain transfers which means that graphs in the dataset are from different domains and (3) temporal evolution where the dataset is a dynamic one with evolving nature. Notably, we use the datasets provided by <ref type="bibr">Wu et al. (2022a)</ref>, which were adopted from the For the baseline methods, we tuned their hyper-parameters based on the validation performance. For DropEdge, we search the drop ratio in the range of <ref type="bibr">[0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 0.7]</ref>. For Tent, we search the learning rate in the range of <ref type="bibr">[1e-2, 1e-3, 1e-4, 1e-5, 1e-6]</ref> and the running epochs in <ref type="bibr">[1,</ref><ref type="bibr">10,</ref><ref type="bibr">20,</ref><ref type="bibr">30]</ref>. For EERM, we followed the instruction provided by the original paper. For GTRANS, we alternatively optimize node features for ? 1 = 4 epochs and optimize graph structure ? 2 = 1 epoch. We adopt DropEdge as the augmentation function A(?) and set the drop ratio to 0.5. We use Adam optimizer for both feature learning and structure learning. We further search the learning rate of feature adaptation ? 1 in <ref type="bibr">[5e-3, 1e-3, 1e-4, 1e-5, 1e-6]</ref>, learning rate of structure adaptation ? 2 in [0.5, 0.1, 0.01], the modification budget B in [0.5%, 1%, 5%] of the original edges, total epochs T in <ref type="bibr">[5,</ref><ref type="bibr">10]</ref>. We note that the process of tuning hyper-parameters is quick due to the high efficiency of test-time adaptation as we demonstrated in Section 4.1.</p><p>Evaluation Protocol. For ERM (standard training), we train all the GNN backbones using the common cross entropy loss. For DropEdge, we drop a certain amount of edges at each training epoch. For EERM, it optimizes a bi-level problem to obtain a trained classifier. Note that the aforementioned three methods do not perform any test-time adaptation and their model parameters are fixed during test. For the two test-time adaptation methods, Tent and GTRANS, we first obtain the GNN backbones pre-trained from ERM and adapt the model parameters or graph data at test time, respectively. Furthermore, Tent minimizes the entropy loss while GTRANS minimizes the contrastive surrogate loss.</p><p>Quantifying Distribution Shifts. Following SR-GNN <ref type="bibr">(Zhu et al., 2021a)</ref>, we adopt central moment discrepancy (CMD) <ref type="bibr" target="#b65">(Zellinger et al., 2017)</ref> as the measurement to quantify the distribution shifts in different graphs. Specifically, given a pre-trained model, we obtain its hidden representation on the training graph and test graphs, denoted as Z tr and Z te . Then we calculate their distance by the CMD metric, i.e., CMD(Z tr ), Z te . We show the results in Table <ref type="table">6</ref> and we can observe certain distribution shifts as these values are not small. Let's take the OGB-Arxiv dataset as an example, where we select papers published before 2011 for training, 2011-2014 for validation, and within 2014-2016/2016-2018/2018-2020 for test. In this context, the distribution shift is from the temporal change. In Table <ref type="table" target="#tab_5">7</ref>, we show the CMD values, ERM performance and GTRANS performance. From the table, we can find that (1) the CMD value on the validation graph is essentially smaller than those on test graphs; and (2) GCN performances on test graphs (with larger shifts) are lower than that on the validation graph.</p><p>Table <ref type="table">6</ref>: CMD values on each individual graph based on the pre-trained GCN.</p><formula xml:id="formula_30">GraphID G 0 G 1 G 2 G 3 G 4 G 5 G 6 G 7 G 8</formula><p>Amz-Photo  Hyper-Parameter Settings. We closely followed AirGNN <ref type="bibr">(Liu et al., 2021a)</ref> to set up the hyperparameters for the baselines:</p><p>(a) GCN: the architecture setup is 2 layers with 64 hidden units without batch normalization for Cora, Citeseer and Pubmed, and 3 layers with 256 hidden units with batch normalization for OGB-Arxiv. The learning rate is set to 0.01. (b) GAT: the architecture setup is 2 layers with 8 hidden units in each of the 8 heads without batch normalization for Cora, Citeseer and Pubmed, and 3 layers with 32 hidden units in each of the 8 heads with batch normalization for OGB-Arxiv. The learning rate is set to 0.005. (c) APPNP: the architecture setup is 2-layer transformation with 64 hidden units and 10-layer propagation without batch normalization for Cora, Citeseer and Pubmed; the architecture is set to 3-layer transformation with 256 hidden units and 10-layer propagation with batch normalization for OGB-Arxiv. The learning rate is set to 0.01. (d) AirGNN: The architecture setup is the same as APPNP and the hyper-parameter ? is set to 0.3 for OGB-Arxiv and 0.5 for other datasets. (e) AirGNN-t: The architecture setup is the same as AirGNN but we tune the hyper-parameter ? in AirGNN based on performance on the combination of training and validation nodes at test stage. This is because the test graph has the same graph structure as the training graph; thus we can take advantage of the label information of training nodes (as well as validation nodes) to tune the hyper-parameters. Specifically, we search ? in the range of [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] for each noise ratio.</p><p>For the setup of GTRANS, we alternatively optimize node features for ? 1 = 4 epochs and optimize graph structure ? 2 = 1 epoch. We adopt DropEdge as the augmentation function A(?) and set the drop ratio to 0.5. We use Adam optimizer for both feature learning and structure learning. We further search the learning rate of feature adaptation ? 1 in [1, 1e-1, 1e-2], total epochs T in <ref type="bibr">[10,</ref><ref type="bibr">20]</ref>. The modification budget B to 5% of the original edges and the learning rate of structure adaptation ? 2 is set to 0.1. It is worth noting that we use a weighted combination of contrastive loss and training classification loss, i.e., L train + ?L s , instead of optimizing the contrastive loss alone. We adaopt this strategy because that the training graph and the test graph were the same graph before the injection of abnormal features. Here the ? is tuned in the range of [1e-2, 1e-3, 1e-4]. We study the effects of contrastive loss and training classification loss in Appendix D.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 ADVERSARIAL ATTACK</head><p>Dataset Statistics. We used OGB-Arxiv for the adversarial attack experiments and the dataset statistics can be found in Table <ref type="table" target="#tab_6">8</ref>. Again, we only have one test graph for this dataset.</p><p>Hyper-Parameter Settings. The setup of GCN and GAT is the same as that in the setting of abnormal features. For the defense methods including SimPGCN, RobustGCN and GCNJaccard, we use the DeepRobust <ref type="bibr" target="#b30">(Li et al., 2020)</ref> library to implement them. For GCNJaccard, we tune its threshold hyper-parameter in the range of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. The hyper-parameter is also tuned based on the performance of training and validation nodes (same as Appendix C.2). Note that the popular defenses ProGNN <ref type="bibr" target="#b22">(Jin et al., 2020)</ref> and GCNSVD <ref type="bibr" target="#b9">(Entezari et al., 2020)</ref> were not included because they throw OOM error due to the expensive eigen-decomposition operation.</p><p>We use the official implementation of the scalable attack PR-BCD <ref type="bibr" target="#b15">(Geisler et al., 2021)</ref> to attack the test graph. We note that when performing adversarial attacks, the setting is more like transductive setting where the training graph and test graph are the same. However, the test graph becomes different from the training graph after the attack. Since the training graph and test graph were originally the same graph, we use a weighted combination of contrastive loss and training classification loss, i.e., L train + ?L s , instead of optimizing the contrastive loss alone. For the setup of GTRANS, we alternatively optimize node features for ? 1 = 1 epoch and optimize graph structure ? 2 = 4 epochs. We fix the learning rate of feature adaptation ? 1 to 1e-3, learning rate of structure adaptation ? 2 to 0.1, ? to 1, total epochs T to 50 and modification budget B to 30% of the original edges.</p><p>C.4 HARDWARE AND SOFTWARE CONFIGURATIONS.</p><p>We perform experiments on NVIDIA Tesla V100 GPUs. The GPU memory and running time reported in Table <ref type="table">2</ref> are measured on one single V100 GPU. Additionally, we use eight CPUs, with the model name as Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz. The operating system we use is CentOS Linux 7 (Core).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 COMPARISON TO GRAPH DOMAIN ADAPTATION</head><p>Our work is related to graph domain adaptation (GraphDA) <ref type="bibr" target="#b43">(Shen et al., 2020;</ref><ref type="bibr" target="#b51">Wu et al., 2020;</ref><ref type="bibr">Zhu et al., 2021a)</ref>, but they are also highly different. In Table <ref type="table">9</ref>, we summarize the differences between GraphDA and GTRANS. In detail, there are the following differences:</p><p>(a) Data and losses: GraphDA methods optimize the loss function based on both labeled source data (training data) and unlabeled target data (test data), while GTRANS only requires target data during inference. Hence, GraphDA methods are infeasible when access to the source data is prohibited such as online service. (b) Parameter: To our best knowledge, existing GraphDA methods are model-centric approaches while GTRANS is a data-centric approach. GTRANS adapts the data instead of the model, which can be more useful in some settings as we showed in the Example of Section 3.3. (c) Efficiency: GraphDA is indeed a training-time adaptation and for each given test graph, it would require training the model on the source and target data. Thus, it is much less efficient than GTRANS, especially when we have multiple test graphs (e.g., 33 test graphs for Elliptic).</p><p>Table <ref type="table">9</ref>: Comparison between GraphDA and GTRANS. They differ by their data and losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting Source Target Train Loss Test Loss Parameter Efficiency</head><formula xml:id="formula_31">GraphDA G tr G te L(G tr , Y tr ) + L(G tr , G te ) - f ? Low GTRANS - G te - L(G te ) G te High</formula><p>To compare their empirical performance, we include two GraphDA methods (SR-GNN <ref type="bibr">(Zhu et al., 2021a)</ref> and UDA-GCN <ref type="bibr" target="#b51">(Wu et al., 2020)</ref>) and one general domain adaptation method (DANN <ref type="bibr" target="#b14">(Ganin et al., 2016)</ref>). SR-GNN regularizes the model's performance on the source and target domains. Note that SR-GNN is originally developed under the transductive setting where the training graph and test graph are the same. To apply SR-GNN in our OOD setting, we assume the test graph is available during the training stage of SR-GNN, as typically done in domain adaptation methods. UDA-GCN is another work that tackles graph data domain adaptation, which exploits local and global information for different domains. In addition, we also include DANN, which adopts an adversarial domain classifier to promote the similarity of feature distributions between different domains. We followed the authors' suggestions in their paper to tune the hyper-parameters and the results are shown in Table <ref type="table" target="#tab_7">10</ref>. On the one hand, we can observe that GraphDA methods generally improve the performance of GCN under distribution shift and SRGNN is the best performing baseline. On the other hand, GTRANS performs the best on all datasets except Amz-Photo. On Amz-Photo, GTRANS does not improve as much as SR-GNN, which indicates that joint optimization over source and target is necessary for this dataset. However, recall that domain adaptation methods are less efficient due to the joint optimization on source and target: the adaptation time of SR-GNN on the 8 graphs of Amz-Photo is 83.5s while that of GTRANS is 4.9s (plus pre-training time 10.1s). Overall, test-time graph transformation exhibits strong advantages of effectiveness and efficiency. Our work is also relevant to graph structure learning (GSL) <ref type="bibr" target="#b13">(Franceschi et al., 2019;</ref><ref type="bibr" target="#b22">Jin et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr">Zhao et al., 2021b;</ref><ref type="bibr">Rozemberczki et al., 2021b;</ref><ref type="bibr" target="#b18">Halcrow et al., 2020;</ref><ref type="bibr" target="#b11">Fatemi et al., 2021)</ref> which learns the graph structure during the training time while not adapting the graph structure at test stage. Our proposed test-time graph transform is essentially different from these works as we do not modify the training data but only the test data. It can be of interest to adopt GSL method at test time by also adapting the test graph structure. However, most existing GSL methods optimize the cross entropy loss defined on the labels to update graph structure, thus not applicable in the absence of test labels. One exception is SLAPS <ref type="bibr" target="#b11">(Fatemi et al., 2021)</ref> which utilizes a selfsupervised loss together with the cross entropy loss to optimize the graph structure. However, the default setting in SLAPS is generating structure for raw data points (with no given graph structure). Hence, using SLAPS for our settings requires considerable changes. Furthermore, we highlight two more weaknesses of SLAPS compared to GTRANS.</p><p>(a) Introducing additional parameters. SLAPS uses a denoising loss as self-supervision. In detail, it first injects noise into node features and trains a denoising autoencoder to denoise the noisy features. This introduces additional parameters from the denoising autoencoder and inevitably changes the model architecture. (b) Not learning features. As other GSL methods, SLAPS does not learn node features. We argue that feature learning is highly important under the abnormal feature setting as shown in Table <ref type="table" target="#tab_2">16</ref>. For example, structure learning only improves GCN by 2% on OGB-Arxiv while feature learning can improve GCN by 20%. Thus, without the feature learning component, the performance will significantly drop when encountering noisy features.</p><p>Since GTRANS is highly versatile and we can use any self-supervised loss as the surrogate loss, we can simply replace the contrastive loss in Eq. ( <ref type="formula" target="#formula_9">6</ref>) with the denoising loss of SLAPS instead of paying considerable efforts in adjusting SLAPS. We refer to the loss used for denoising as SLAPS loss and adopt it for TTGT. Note that we first train the parameters of the DAE used for denoising while keeping the pre-trained model fixed. Then we fix both DAE and the pre-trained model and optimize the test graph for TTGT. The results are shown in Table <ref type="table" target="#tab_8">11</ref>. From the table, we can observe that the SLAPS loss (or feature denoising loss) does not work as well as the contrastive loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 COMPARISON TO AD-GCL</head><p>Next, we compare our method with a graph contrastive learning method with learnable augmentation AD-GCL <ref type="bibr" target="#b45">(Suresh et al., 2021)</ref>. Since AD-GCL is originally designed for graph classification as a pre-training strategy, the direct empirical comparison between AD-GCL and GTRANS is not easy. However, due to the flexibility of GTRANS, we can integrate AD-GCL into our TTGT framework, denoted as TTGT+AD-GCL. We present the empirical results in Table <ref type="table" target="#tab_9">12</ref>. We can observe that TTGT+AD-GCL generally performs worse than GTRANS except on Amz-Photo, which indicates that GTRANS is a stronger realization of TTGT. Furthermore, we highlight some key differences between it and GTRANS.</p><p>(a) AD-GCL requires optimization of a min-max problem which involves parameters of graph structure and model. Thus, adopting it for TTGT would change the pre-trained model architecture. (b) AD-GCL only augments the graph structure while not learning the features. We argue that feature learning is highly important under the abnormal feature setting as shown in Table <ref type="table" target="#tab_2">16</ref>. For example, structure learning only improves GCN by 2% on OGB-Arxiv while feature learning can improve GCN by 20%. Thus, without the feature learning component, the performance will significantly drop when encountering noisy features. (c) According to Eq. ( <ref type="formula">9</ref>) in the AD-GCL paper, it calculates the similarities between all samples within each mini-batch. When we increase the batch size, we would easily get the out-ofmemory issue while a small mini-batch will slow down the learning process. As a consequence, TTGT+AD-GCL is less efficient than GTRANS: the adaptation time of TTGT+AD-GCL on OGB-Arxiv is 12.7s while that of GTRANS is 2.6s.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 ABNORMAL FEATURES</head><p>For each model, we present the node classification accuracy on all test nodes (i.e., both normal and abnormal ones) in Figure <ref type="figure" target="#fig_7">8</ref>. GTRANS significantly improves GCN in terms of the performance on all test nodes for all datasets across all noise ratios. For example, on Cora with 30% noisy nodes, GTRANS improves GCN by 31.0% on overall test accuracy. These results further validate that the proposed GTRANS can produce expressive and generalizable representations. To understand the modifications made on the graph, we compare several properties among clean graph, attacked graph (20% perturbation rate), graph obtained by GCNJaccard, and graph obtained by GTRANS in Table <ref type="table" target="#tab_10">13</ref>. We follow the definition in <ref type="bibr" target="#b73">(Zhu et al., 2020)</ref> to measure homophily; "Pairwise Feature Similarity" is the averaged feature similarity among all pairs of connected nodes; "#Edge+/-" indicates the number of edges that the modified graph adds/deletes compared to the clean graph. From Table <ref type="table" target="#tab_10">13</ref>, we observe that first, adversarial attack decreases homophily and feature similarity, but GTRANS and GCNJaccard promote such information to defend against adversarial patterns. Second, both GTRANS and GCNJaccard focus on deleting edges from the attacked graph, but GCNJaccard removes a substantially larger amount of edges, which may destroy clean graph structure and lead to suboptimal performance.  <ref type="table" target="#tab_2">16</ref> and<ref type="table" target="#tab_14">17</ref>, respectively. Note that "None" indicates the vanilla GCN without any test-time adaptation; "A " or "X " is the variants of GTRANS which solely learns structure or node features; "Both" indicates the method GTRANS that learn both structure and node features. From Table <ref type="table" target="#tab_2">16</ref>, we observe that (1) while both feature learning and structure learning can improve the vanilla performance, feature (2) combining them does not seem to further improve the performance but it achieves a comparable performance to sole feature learning. From Table <ref type="table" target="#tab_14">17</ref>, we observe that (1) while both feature learning and structure learning can improve the vanilla performance, structure learning is more powerful than feature learning; and (2) combining them can further improve the performance. From these observations, we conclude that (1) feature learning is more crucial for counteracting feature corruption and structure learning is more important for defending structure corruption; and (2) combining them always yields a better or comparable performance.</p><p>Table <ref type="table" target="#tab_2">16</ref>: Ablation study on feature learning and structure learning for abnormal feature setting. While both feature learning and structure learning can improve the vanilla performance, feature learning is more powerful than structure learning. Combining them does not seem to further improve the performance but it achieves a comparable performance to sole feature learning.    <ref type="bibr" target="#b11">(Fatemi et al., 2021)</ref> utilizes self-supervision to guide the graph structure learning process. Specifically, it injects random noise into node features and employs a denoising autoencoder (DAE) to denoise the node features. We refer to the loss used for denoising as SLAPS loss and adopt it for TTGT. Note that we first train the parameters of the DAE used for denoising while keeping the pre-trained model fixed. Then we fix both DAE and the pre-trained model and optimize the test graph for TTGT.</p><p>We summarize the results in Table <ref type="table" target="#tab_15">18</ref>. From the table, we observe that in most of the cases, the above three losses underperform our proposed surrogate loss and even degrade the vanilla performance. It validates the effectiveness of our contrastive loss in guiding the test-time graph transformation. Gradient Correlation. In Figure <ref type="figure" target="#fig_0">2</ref>, we have empirically verified the effectiveness of Theorem 1 when adopting the surrogate loss in Eq. (6) as L s . We further plot the values of ?(G) with different surrogate losses (i.e., entropy, reconstruction and SLAPS) and L c on one test graph in Cora in Figure <ref type="figure" target="#fig_9">9</ref>. We can observe that a positive ?(G) generally reduces the test classification loss. For example, when using entropy loss, the test loss generally reduces when ?(G) is positive and starts to increase after ?(G) becomes negative. In this subsection, we examine the sensitivity of GTRANS' performance with respect to the perturbation budget, i.e., hyper-parameter B. Specifically, we vary the value of B in the range of {0.5%, 1%, 5%, 10%, 20%, 30%} and perform experiments on the OGB-Arxiv dataset for the three settings in Table <ref type="table" target="#tab_16">19</ref>. Specifically, "Abn. Feat" stands for abnormal feature setting with 30% noisy feature while "Adv. Attack" stands for the adversarial attack setting with 20% perturbation rate.</p><p>From the table, we can observe budget B has a smaller effect on OOD and abnormal feature settings while highly impacting the performance under structural adversarial attack. This is because most of the changes made by adversarial attack are edge injections as shown in Table <ref type="table" target="#tab_10">13</ref>, and we need to use a large budget B to remove adversarial patterns. By contrast, GTRANS is much less sensitive to the value of B in the other two settings. In Eq. ( <ref type="formula" target="#formula_9">6</ref>), used DropEdge as the augmentation function A(?) to obtain the augmented view. In practice, the choice of augmentation can be flexible and here we explore two other choices: node dropping <ref type="bibr" target="#b62">(You et al., 2020)</ref> and subgraph sampling <ref type="bibr">(Zhu et al., 2021b)</ref>. We perform experiments on OOD setting with GCN as the backbone model and report the results in Table <ref type="table" target="#tab_17">20</ref>. Specifically, we adopt a ratio of 0.05 for node dropping, and ratios of 0.05 and 0.5 for DropEdge. From the table, we can observe that (1) GTRANS with any of the three augmentations can greatly improve the performance of GCN under distribution shift, and (2) different augmentations lead to slightly different performance on different datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Positive ?(G) can help reduce test loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Comparison to other baselines. Both DropEdge and EERM modify the training process to improve model generalization. Nonetheless, they are less effective than GTRANS, as GTRANS takes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on Cora under OOD. GTRANS improves GCN on most test graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Node classification accuracy on abnormal (noisy) nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a)(b) T-SNE visualizations of embedding obtained from abnormal graph and transformed graph on Cora. (c)(d) Comparison between adapting data and adapting model at test time. Note that AirGNN-t tunes the message-passing hyper-parameter in AirGNN at test time. For a fair comparison, we tune AirGNN-t based on the performance on both training and validation nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Given two nodes x 1 and x 2 and their connections, we are interested in predicting their The color indicates the node label and "[0.3]" suggests that the associated node feature is 0.3. Assume a mean aggregator is used for aggregating features from the neighbors. Left: we show the clean graph without adversarial attack. The aggregated features for the two center nodes are x1 = [0.45] and x2 = [0.53]. Right: we show the attacked graph where the red edge indicates the adversarial edge injected by the attacker. The aggregated features for the two center nodes become x1 = [0.45] and x2 = [0.45].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Classification performance on individual test graphs within each dataset for OOD setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Overall node classification accuracy under the setting of abnormal features. GTRANS significantly improves the performance of GCN on both abnormal nodes and overall nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )</head><label>a</label><figDesc>Reconstruction Loss. Data reconstruction is considered as a good self-supervised signal and we can adopt link reconstruction (Kipf &amp; Welling, 2016b) as the guidance. Minimizing the reconstruction loss is equivalent to maximizing the similarity for connected nodes, which encourages the connected nodes to have similar representations. (b) Entropy Loss. Entropy loss calculates the entropy of the model prediction. Minimizing the entropy can force the model to be certain about the prediction. It has been demonstrated effective in Tent<ref type="bibr" target="#b50">(Wang et al., 2021)</ref> when adapting batch normalization parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The relationship between ?(G) and L c when adopting different surrogate losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Example. Let N i denote the neighbors for node x i . If there exist two nodes with the same aggregated features but different labels, i.e., Agg(x 1 , {x i |i ? N 1 }) = Agg(x 2 , {x j |j ? N 2 }), y 1 = y 2 , adapting the data {x i |i = 1, . . . , K} can achieve lower classification error than adapting the model f ? at test stage.</figDesc><table><row><cell>Given test samples {x i |i = 1, . . . , K}, we consider a linearized GNN</cell></row><row><cell>f ? which first performs aggregation through a function Agg(?, ?) and then transforms the aggregated</cell></row><row><cell>features via a function Trans(?). Hence, only the function Trans(?) is parameterized by ?.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Average classification performance (%) on the test graphs. Rank indicates the average rank of each algorithm for each backbone. OOM indicates out-of-memory error on 32 GB GPU memory. The proposed GTRANS consistently ranks the best compared with the baselines. * / * * indicates that GTrans outperforms ERM at the confidence level 0.1/0.05 from paired t-test. 94.66?0.63 * * 55.88?3.10 * * 54.32?0.60 41.59?1.20 * * 60.42?0.86 * 67?0.74 * * 96.37?1.00 * * 66.43?2.57 * * 51.16?1.72 43.76?1.25 * * 58.59?1.07</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Backbone Method</cell><cell cols="2">Amz-Photo Cora</cell><cell>Elliptic</cell><cell>FB-100</cell><cell>OGB-Arxiv Twitch-E</cell><cell>Rank</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell>ERM</cell><cell>93.79?0.97</cell><cell>91.59?1.44</cell><cell>50.90?1.51</cell><cell cols="2">54.04?0.94 38.59?1.35</cell><cell>59.89?0.50</cell><cell>3.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DropEdge 92.11?0.31</cell><cell>81.01?1.33</cell><cell>53.96?4.91</cell><cell cols="2">53.00?0.50 41.26?0.92</cell><cell>59.95?0.39</cell><cell>3.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tent</cell><cell>94.03?1.07</cell><cell>91.87?1.36</cell><cell>51.71?2.00</cell><cell cols="2">54.16?1.00 39.33?1.40</cell><cell>59.46?0.55</cell><cell>3.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EERM</cell><cell>94.05?0.40</cell><cell>87.21?0.53</cell><cell>53.96?0.65</cell><cell cols="2">54.24?0.55 OOM</cell><cell>59.85?0.85</cell><cell>2.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">GTRANS 94.13?0.77  1.0</cell></row><row><cell></cell><cell></cell><cell>SAGE</cell><cell>ERM</cell><cell>95.09?0.60</cell><cell>99.67?0.14</cell><cell>56.12?4.47</cell><cell cols="2">54.70?0.47 39.56?1.66</cell><cell>62.06?0.09</cell><cell>3.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DropEdge 92.61?0.56</cell><cell>95.85?0.30</cell><cell>52.38?3.11</cell><cell cols="2">54.51?0.69 38.89?1.74</cell><cell>62.14?0.12</cell><cell>4.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tent</cell><cell>95.72?0.43</cell><cell>99.80?0.10</cell><cell>55.89?4.87</cell><cell cols="2">54.86?0.34 39.58?1.26</cell><cell>62.09?0.09</cell><cell>2.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EERM</cell><cell>95.57?0.13</cell><cell>98.77?0.14</cell><cell>58.20?3.55</cell><cell cols="2">54.28?0.97 OOM</cell><cell>62.11?0.12</cell><cell>3.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">GTRANS 96.91?0.68  *  *  99.45?0.13</cell><cell cols="3">60.81?5.19  *  *  54.64?0.62 40.39?1.45  *  *  62.15?0.13  *</cell><cell>1.7</cell></row><row><cell></cell><cell></cell><cell>GAT</cell><cell>ERM</cell><cell>96.30?0.79</cell><cell>94.81?1.28</cell><cell>65.36?2.70</cell><cell cols="2">51.77?1.41 40.63?1.57</cell><cell>58.53?1.00</cell><cell>3.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DropEdge 90.70?0.29</cell><cell>76.91?1.55</cell><cell>63.78?2.39</cell><cell cols="2">52.65?0.88 42.48?0.93</cell><cell>58.89?1.01</cell><cell>3.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tent</cell><cell>95.99?0.46</cell><cell>95.91?1.14</cell><cell>66.07?1.66</cell><cell cols="2">51.47?1.70 40.06?1.19</cell><cell>58.33?1.18</cell><cell>3.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EERM</cell><cell>95.57?1.32</cell><cell>85.00?0.96</cell><cell>58.14?4.71</cell><cell cols="2">53.30?0.77 OOM</cell><cell>59.84?0.71</cell><cell>3.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">GTRANS 96.2.0</cell></row><row><cell></cell><cell></cell><cell>GPR</cell><cell>ERM</cell><cell>91.87?0.65</cell><cell>93.00?2.17</cell><cell>64.59?3.52</cell><cell cols="2">54.51?0.33 44.38?0.59</cell><cell>59.72?0.40</cell><cell>2.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">DropEdge 88.81?1.48</cell><cell>79.27?1.39</cell><cell>61.02?1.78</cell><cell cols="2">55.04?0.33 43.65?0.77</cell><cell>59.89?0.05</cell><cell>3.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tent 3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EERM</cell><cell>90.78?0.52</cell><cell>88.82?3.10</cell><cell>67.27?0.98</cell><cell cols="2">55.95?0.03 OOM</cell><cell>61.57?0.12</cell><cell>2.0</cell></row><row><cell>Accuracy</cell><cell>0.80 0.85 0.90 0.95 0.75</cell><cell cols="5">GTRANS 91.93?0.73 T2 T3 T4 T5 ERM 69.03?2.33  T1 93.05?2.02 T6 T7 T8 GTrans</cell><cell></cell></row></table><note><p>* * * 54.38?0.31 46.00?0.46 * * 60.11?0.53 * * 1.7 3 Tent cannot be applied to models which do not contain batch normalization layers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Node classification accuracy (%) under different perturbation (Ptb.) rates of structure attack.Interpretation.To understand the modifications made on the graph, we compare several properties among clean graph, attacked graph (20% perturbation rate), graph obtained by GCNJaccard, and graph obtained by GTRANS in Table13in Appendix D.6. First, adversarial attack decreases homophily and feature similarity, but GTRANS and GCNJaccard promote such information to alleivate the adversarial patterns. Our experiment also shows that GTRANS removes 77% adversarial edges while removing 30% existing edges from the attacked graph. Second, both GTRANS and GCNJaccard focus on deleting edges from the attacked graph, but GCNJaccard removes a substantially larger amount of edges, which may destroy clean graph structure and lead to sub-optimal performance.</figDesc><table><row><cell>Ptb. Rate</cell><cell>GCN</cell><cell>GAT</cell><cell cols="3">RobustGCN SimPGCN GCNJaccard</cell><cell>GTRANS</cell></row><row><cell>5%</cell><cell cols="2">57.47?0.54 64.56?0.43</cell><cell>61.55?1.20</cell><cell>61.30?0.42</cell><cell>65.01?0.26</cell><cell>66.29?0.25</cell></row><row><cell>10%</cell><cell cols="2">47.97?0.65 61.20?0.70</cell><cell>58.15?1.55</cell><cell>57.01?0.70</cell><cell>63.25?0.30</cell><cell>65.16?0.52</cell></row><row><cell>15%</cell><cell cols="2">38.04?1.22 58.96?0.59</cell><cell>55.91?1.27</cell><cell>54.13?0.73</cell><cell>61.83?0.29</cell><cell>64.40?0.38</cell></row><row><cell>20%</cell><cell cols="2">29.05?0.73 57.29?0.49</cell><cell>54.39?1.09</cell><cell>52.26?0.87</cell><cell>60.57?0.34</cell><cell>63.44?0.50</cell></row><row><cell>25%</cell><cell cols="2">19.58?2.32 55.86?0.53</cell><cell>52.76?1.44</cell><cell>50.46?0.85</cell><cell>59.17?0.39</cell><cell>62.95?0.67</cell></row><row><cell cols="7">proposed GTRANS, indicating that a learnable transformation function is needed to achieve better</cell></row><row><cell cols="6">robustness under adversarial attacks, which GCNJaccard does not employ.</cell></row><row><cell cols="2">4.4 FURTHER ANALYSIS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Transferability.Cross-Architecture Transferability. Since the outcome of GTRANS is a refined graph, it can conceptually be employed by any GNN model. Thus, we can transform the graph based on one pre-trained GNN and test the transformed graph on another pre-trained GNN. To examine such transferability, we perform experiments GCN, APPNP, AirGNN and GAT under the abnormal feature setting with 30% noisy nodes on Cora.</figDesc><table><row><cell>Tr\Te</cell><cell>GCN APPNP AirGNN GAT</cell></row><row><cell>GCN</cell><cell>67.36 70.65 70.84 58.62</cell></row><row><cell cols="2">APPNP 67.87 70.39 69.59 64.46</cell></row><row><cell cols="2">AirGNN 68.00 70.37 72.68 64.93</cell></row><row><cell>GAT</cell><cell>54.85 60.37 65.22 54.60</cell></row><row><cell>Noisy</cell><cell>44.29 48.26 58.51 21.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>CMD values and the performances of ERM and GTRANS on OGB-Arxiv</figDesc><table><row><cell></cell><cell>6.4</cell><cell>5.1</cell><cell>5.5</cell><cell>3.7</cell><cell>2.8</cell><cell>3.7</cell><cell>3.9</cell><cell>6.6</cell><cell>-</cell></row><row><cell>Cora</cell><cell>5.4</cell><cell>4.2</cell><cell>4.8</cell><cell>6.3</cell><cell>5.5</cell><cell>4.8</cell><cell>4.6</cell><cell>5.4</cell><cell>-</cell></row><row><cell>Elliptic</cell><cell cols="9">80.2 90.8 114.3 86.5 789.3 781.6 99.4 100.4 150.6</cell></row><row><cell>OGB-Arxiv</cell><cell cols="3">14.7 20.6 10.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FB-100</cell><cell cols="3">29.7 16.9 32.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Twitch-E</cell><cell>8.6</cell><cell>6.1</cell><cell>9.0</cell><cell>8.4</cell><cell>9.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Method</cell><cell cols="9">2011-2014 (Val) 2014-2016 2014-2016 2016-2018 2018-2020</cell></row><row><cell>CMD</cell><cell>2.5</cell><cell></cell><cell></cell><cell>14.7</cell><cell>14.7</cell><cell></cell><cell>20.6</cell><cell></cell><cell>10.4</cell></row><row><cell>ERM</cell><cell cols="2">45.32?0.50</cell><cell cols="7">41.29?1.13 41.29?1.13 38.69?1.33 35.78?1.81</cell></row><row><cell>GTRANS</cell><cell cols="2">45.82?0.38</cell><cell cols="7">44.03?0.95 44.03?0.95 41.90?1.28 38.81?1.47</cell></row><row><cell cols="3">C.2 ABNORMAL FEATURES</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>Dataset Statistics. In these two settings, we choose the original version of popular benchmark datasets Cora, Citeseer, Pubmed and OGB-Arxiv. The statistics for these datasets are shown in Table</p>8</p>. Note that we only have one test graph, and the injection of abnormal features or adversarial attack happens after the training process of backbone model, which can be viewed as evasion attack.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Dataset statistics for experiments on abnormal features and adversarial attack.</figDesc><table><row><cell>Dataset</cell><cell>Classes</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="4">Features Training Nodes Validation Nodes Test Nodes</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2708</cell><cell>5278</cell><cell>1433</cell><cell>20 per class</cell><cell>500</cell><cell>1000</cell></row><row><cell>Citeseer</cell><cell>6</cell><cell>3327</cell><cell>4552</cell><cell>3703</cell><cell>20 per class</cell><cell>500</cell><cell>1000</cell></row><row><cell>Pubmed</cell><cell>3</cell><cell>19717</cell><cell>44324</cell><cell>500</cell><cell>20 per class</cell><cell>500</cell><cell>1000</cell></row><row><cell>OGB-Arxiv</cell><cell>40</cell><cell cols="2">169343 1166243</cell><cell>128</cell><cell>54%</cell><cell>18%</cell><cell>28%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Performance comparison between GTRANS and graph domain adaptation methods.</figDesc><table><row><cell>Method</cell><cell>Amz-Photo Cora</cell><cell>Elliptic</cell><cell>FB-100</cell><cell>OGB-Arxiv</cell><cell>Twitch-E</cell></row><row><cell>ERM</cell><cell cols="5">93.79?0.97 91.59?1.44 50.90?1.51 54.04?0.94 38.59?1.35 59.89?0.50</cell></row><row><cell cols="6">UDA-GCN 91.70?0.35 92.65?0.46 51.57?1.31 54.11?0.54 39.43?0.71 52.12?0.38</cell></row><row><cell>DANN</cell><cell cols="5">94.08?0.21 92.89?0.64 53.00?0.97 51.53?1.47 36.60?1.26 60.13?0.53</cell></row><row><cell>SRGNN</cell><cell cols="5">94.64?0.17 94.08?0.28 51.94?0.81 54.08?1.10 38.92?0.65 59.21?0.51</cell></row><row><cell>GTRANS</cell><cell cols="5">94.13?0.77 94.66?0.63 55.88?3.10 54.32?0.60 41.59?1.20 60.42?0.86</cell></row><row><cell cols="4">D.2 COMPARISON TO GRAPH STRUCTURE LEARNING</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>Comparison between SLAPS loss and our contrastive loss.</figDesc><table><row><cell></cell><cell>Amz-Photo</cell><cell>Cora</cell><cell>Elliptic</cell><cell>FB-100</cell><cell>OGB-Arxiv</cell><cell>Twitch-E</cell></row><row><cell>None</cell><cell cols="6">93.79?0.97 91.59?1.44 50.90?1.51 54.04?0.94 38.59?1.35 59.89?0.50</cell></row><row><cell>SLAPS</cell><cell cols="6">93.97?1.04 91.41?1.23 50.54?1.81 54.08?0.76 41.38?1.35 59.85?0.68</cell></row><row><cell cols="7">L s in Eq. (6) 94.13?0.77 94.66?0.63 55.88?3.10 54.32?0.60 41.59?1.20 60.42?0.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Comparison between GTRANS and AD-GCL under the TTGT framework.To show the performance on individual test graphs, we choose GCN as the backbone model and include the box plot on all test graphs within each dataset in Figure7. We observe that GTRANS generally improves over each test graph within each dataset, which validates the effectiveness of test-time graph transformation.</figDesc><table><row><cell></cell><cell>Amz-Photo</cell><cell>Cora</cell><cell>Elliptic</cell><cell>FB-100</cell><cell>OGB-Arxiv</cell><cell>Twitch-E</cell></row><row><cell>ERM</cell><cell cols="6">93.79?0.97 91.59?1.44 50.90?1.51 54.04?0.94 38.59?1.35 59.89?0.50</cell></row><row><cell cols="7">TTGT+AD-GCL 94.96?0.52 92.38?1.35 54.38?2.77 53.81?0.87 39.16?0.98 59.78?0.65</cell></row><row><cell>GTRANS</cell><cell cols="6">94.13?0.77 94.66?0.63 55.88?3.10 54.32?0.60 41.59?1.20 60.42?0.86</cell></row><row><cell cols="4">D.4 OUT-OF-DISTRIBUTION (OOD) SETTING</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>Statistics of modified graphs. GTRANS promotes homophily and feature similarity.Since we optimized a combined loss in the settings of abnormal features and adversarial attack, we now perform ablation study to examine the effect of each component. We choose GCN as the backbone model and choose 0.3 noise ratio for abnormal features. The results for abnormal features and adversarial attack are shown in Tables14 and 15, respectively. "None" indicates the vanilla GCN without any test-time adaptation and "Combined" indicates jointly optimizing a combination of the two losses. From the two tables, we can conclude that (1) both L s and L train help counteract abnormal features and adversarial attack; and (2) optimizing the combined loss generally outperforms optimizing L s or L train alone.D.8 ABLATION STUDY ON FEATURE LEARNING AND STRUCTURE LEARNINGIn this subsection, we investigate the effects of the feature learning component and structure learning component. We show results for abnormal features and adversarial attack in Tables</figDesc><table><row><cell></cell><cell cols="4">GTRANS GCNJaccard Attacked Clean</cell></row><row><cell>Homophily</cell><cell>0.689</cell><cell>0.636</cell><cell>0.548</cell><cell>0.654</cell></row><row><cell>Pairwise Feature Similarity</cell><cell>0.825</cell><cell>0.863</cell><cell>0.809</cell><cell>0.827</cell></row><row><cell>#Edges</cell><cell>1,945k</cell><cell>1,754k</cell><cell>2,778k</cell><cell>2,316k</cell></row><row><cell>#Edge+</cell><cell>108k</cell><cell>118k</cell><cell>463k</cell><cell>-</cell></row><row><cell>#Edge-</cell><cell>479k</cell><cell>679k</cell><cell>0.6k</cell><cell>-</cell></row><row><cell cols="2">D.7 ABLATION STUDY ON SURROGATE LOSS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 14 :</head><label>14</label><figDesc>Performance comparison when optimizing different losses for abnormal feature setting. Both L s and L train help counteract abnormal features; optimizing the combined loss generally outperforms optimizing L s or L train alone.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">All Test Nodes</cell><cell></cell><cell></cell><cell cols="2">Abnormal Nodes</cell><cell></cell></row><row><cell>Dataset</cell><cell>None</cell><cell>Ls</cell><cell>Ltrain</cell><cell>Combined</cell><cell>None</cell><cell>Ls</cell><cell>Ltrain</cell><cell>Combined</cell></row><row><cell cols="9">OGB-Arxiv 44.29?1.20 46.70?1.20 64.60?0.22 64.64?0.24 31.50?1.12 35.22?1.17 57.54?0.93 57.69?0.93</cell></row><row><cell>Citeseer</cell><cell cols="8">39.26?2.02 45.41?2.71 54.97?1.55 52.54?1.08 17.30?1.86 32.93?2.81 42.67?2.78 44.10?2.97</cell></row><row><cell>Cora</cell><cell cols="8">36.35?1.87 48.71?3.02 66.77?2.54 67.29?1.44 15.80?2.33 35.40?4.05 61.67?3.64 63.90?2.55</cell></row><row><cell>Pubmed</cell><cell cols="8">62.72?1.20 65.49?1.65 66.56?0.64 70.55?1.55 36.47?1.85 56.77?3.60 60.20?1.97 67.93?2.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 15 :</head><label>15</label><figDesc>Performance comparison when optimizing different losses for adversarial attack setting. Both L s and L train help counteract adversarial attack; optimizing the combined loss generally outperforms optimizing L s or L train alone. r denotes the perturbation rate.</figDesc><table><row><cell>Loss</cell><cell>r=5%</cell><cell>r=10%</cell><cell>r=15%</cell><cell>r=20%</cell><cell>r=25%</cell></row><row><cell>None</cell><cell cols="5">57.47?0.54 47.97?0.65 38.04?1.22 29.05?0.73 19.58?2.32</cell></row><row><cell>L s</cell><cell cols="5">62.40?0.45 59.76?0.93 57.85?1.03 55.26?1.35 52.64?2.35</cell></row><row><cell>L train</cell><cell cols="5">65.54?0.25 64.00?0.31 62.99?0.34 61.95?0.40 61.55?0.58</cell></row><row><cell cols="6">Combined 66.29?0.25 65.16?0.52 64.40?0.38 63.44?0.50 62.95?0.67</cell></row><row><cell cols="3">learning is more powerful than structure learning;</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>As there can be other choices to guide our test-time graph transformation process, we examine the effects of other self-supervised signals. We choose the OOD setting to perform experiments and consider the following two parameter-free self-supervised loss:</figDesc><table><row><cell></cell><cell></cell><cell cols="2">All Test Nodes</cell><cell></cell><cell></cell><cell cols="2">Abnormal Nodes</cell><cell></cell></row><row><cell>Dataset</cell><cell>None</cell><cell>A</cell><cell>X</cell><cell>Both</cell><cell>None</cell><cell>A</cell><cell>X</cell><cell>Both</cell></row><row><cell cols="9">OGB-Arxiv 44.29?1.20 46.02?1.09 64.88?0.23 64.64?0.24 31.50?1.12 31.96?1.05 58.12?0.83 57.69?0.93</cell></row><row><cell>Citeseer</cell><cell cols="8">39.26?2.02 39.67?1.96 54.99?1.55 54.97?1.55 17.30?1.86 17.13?1.81 42.73?2.81 42.67?2.78</cell></row><row><cell>Cora</cell><cell cols="8">36.35?1.87 37.02?1.82 67.40?1.62 67.29?1.44 15.80?2.33 15.67?2.15 64.17?3.18 63.90?2.55</cell></row><row><cell>Pubmed</cell><cell cols="8">62.72?1.20 62.50?1.21 70.53?1.52 70.55?1.55 36.47?1.85 36.57?1.96 67.90?2.07 67.93?2.11</cell></row><row><cell cols="6">D.9 COMPARING DIFFERENT SELF-SUPERVISED SIGNALS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 17 :</head><label>17</label><figDesc>Ablation study on feature learning and structure learning for adversarial structural attack setting. While both feature learning and structure learning can improve the vanilla performance, structure learning is more powerful than feature learning. Combining them can further improve the performance.</figDesc><table><row><cell>Param</cell><cell>r=5%</cell><cell>r=10%</cell><cell>r=15%</cell><cell>r=20%</cell><cell>r=25%</cell></row><row><cell>None</cell><cell cols="5">57.47?0.54 47.97?0.65 38.04?1.22 29.05?0.73 19.58?2.32</cell></row><row><cell>X</cell><cell cols="5">64.16?0.24 61.59?0.29 60.07?0.32 59.04?0.49 58.82?0.68</cell></row><row><cell>A</cell><cell cols="5">65.93?0.32 64.31?0.71 63.14?0.39 61.42?0.58 60.18?1.53</cell></row><row><cell>Both</cell><cell cols="5">66.29?0.25 65.16?0.52 64.40?0.38 63.44?0.50 62.95?0.67</cell></row><row><cell cols="2">(c) SLAPS Loss. SLAPS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18 :</head><label>18</label><figDesc>Comparison of different self-supervised signals for OOD setting. The reconstruction loss and entropy loss generally underperform our proposed loss. None 93.79?0.97 91.59?1.44 50.90?1.51 54.04?0.94 38.59?1.35 59.89?0.50 Recon 93.77?1.01 91.37?1.41 49.33?1.37 53.94?1.03 44.93?4.06 59.17?0.77 Entropy 93.67?0.98 91.54?1.14 49.93?1.56 54.29?0.97 41.11?2.19 59.48?0.64 SLAPS 93.97?1.04 91.41?1.23 50.54?1.81 54.08?0.76 41.38?1.35 59.85?0.68 L s in Eq. (6) 94.13?0.77 94.66?0.63 55.88?3.10 54.32?0.60 41.59?1.20 60.42?0.86</figDesc><table><row><cell>Amz-Photo</cell><cell>Cora</cell><cell>Elliptic</cell><cell>FB-100</cell><cell>OGB-Arxiv</cell><cell>Twitch-E</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 19 :</head><label>19</label><figDesc>The change of model performance when varying budget B on OGB-Arxiv.</figDesc><table><row><cell>Setting</cell><cell cols="6">B=0.5% B=1% B=5% B=10% B=20% B=30%</cell></row><row><cell>OOD</cell><cell>40.52</cell><cell>40.69</cell><cell>41.32</cell><cell>41.40</cell><cell>41.70</cell><cell>41.65</cell></row><row><cell>Abn. Feat.</cell><cell>64.78</cell><cell>64.80</cell><cell>64.64</cell><cell>64.60</cell><cell>64.57</cell><cell>64.57</cell></row><row><cell>Adv. Attack</cell><cell>56.66</cell><cell>56.89</cell><cell>58.30</cell><cell>59.93</cell><cell>62.31</cell><cell>63.47</cell></row><row><cell cols="6">D.11 DIFFERENT AUGMENTATIONS IN CONTRASTIVE LOSS</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 20 :</head><label>20</label><figDesc>Performance of GTRANS with different augmentation used in contrastive loss. Node Dropping 94.45?0.70 95.00?0.65 56.57?2.99 54.15?0.60 39.95?1.11 60.38?0.74 Subgraph Sampling 94.18?0.75 94.95?0.64 55.40?3.00 54.51?0.56 41.44?1.17 60.52?0.80 DropEdge (0.05) 94.43?0.68 95.10?0.66 56.78?2.86 54.17?0.60 40.19?1.08 60.31?0.74 DropEdge (0.5) 94.13?0.77 94.66?0.63 55.88?3.10 54.32?0.60 41.59?1.20 60.42?0.86 ERM 93.79?0.97 91.59?1.44 50.90?1.51 54.04?0.94 38.59?1.35 59.89?0.50</figDesc><table><row><cell>Augmentation</cell><cell>Amz-Photo</cell><cell>Cora</cell><cell>Elliptic</cell><cell>FB-100</cell><cell>OGB-Arxiv</cell><cell>Twitch-E</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>(A ? ? A )ij can be implemented as</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>-(A + ? A )ij if (A + ? A )ij ? 1, otherwise (A + ? A )ij.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We note that the GCN used in the experiments of EERM does not normalize the adjacency matrix according to its open-source code. Here we normalize the adjacency matrix to make it consistent with the original GCN.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOLWEDGEMENT</head><p>This research is supported by the National Science Foundation (NSF) under grant numbers IIS1845081, IIS1928278, IIS1955285, IIS2212032, IIS2212144, IOS2107215, and IOS2035472, the Army Research Office (ARO) under grant number W911NF-21-1-0198, MSU Foundation, the Home Depot, Cisco Systems Inc, Amazon Faculty Award, Johnson &amp; Johnson and Snap Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>To the best of our knowledge, there are no ethical issues with this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>To ensure reproducibility of our experiments, we provide our source code at https://github. com/ChandlerBang/GTrans. The hyper-parameters are described in details in the appendix. We also provide a pseudo-code implementation of our framework in the appendix.</p><p>Algorithm 1: GTRANS for Test-Time Graph Transformation 1 Input: Pre-trained model f ? and test graph dataset GTe = (A, X ). 2 Output: Model prediction ? and transformed graph G = (A , X ). 3 Initialize ? A and ? X as zero matrices 4 for t = 0, . . . , T -1</p><p>Compute Ls(? A , ? X ) as shown in Eq. ( <ref type="formula">6</ref>)</p><p>aforementioned references with manually created distribution shifts. Note that there can be multiple training/validaiton/test graphs. Specifically, Cora and Amazon-Photo have 1/1/8 graphs for training/validation/test sets. Similarly, the splits are 1/1/5 on Twitch-E, 3/2/3 on FB-100, 5/5/33 on Elliptic, and 1/1/3 on OGB-Arxiv. Hyper-Parameter Setting. For the setup of backbone GNNs, we majorly followed Wu et al. ( <ref type="formula">2022a</ref>):</p><p>(a) GCN: the architecture setup is 5 layers with 32 hidden units for Elliptic and OGB-Arxiv, and 2 layers with 32 hidden units for other datasets, and with batch normalization for all datasets. The learning rate is set to 0.001 for Cora and Amz-Photo, 0.01 for other datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets. (b) GraphSAGE: the architecture setup is 5 layers with 32 hidden units for Elliptic and OGB-Arxiv, and 2 layers with 32 hidden units for other datasets, and with batch normalization for all datasets. The learning rate is set to 0.01 for all datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets. (c) GAT: the architecture setup is 5 layers for Elliptic and OGB-Arxiv, and 2 layers for other datasets, and with batch normalization for all datasets. Each layer contains 4 attention heads and each head is associated with 32 hidden units. The learning rate is set to 0.01 for all datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets. (d) GPR: We use 10 propagation layers and 2 transformation layers with 32 hidden units. The learning rate is set to 0.01 for all datasets; the weight decay is set to 0 for Elliptic and OGB-Arxiv, and 0.001 for other datasets. Note that GPR does not contain batch normalization layers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses</title>
		<author>
			<persName><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?me</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masud</forename><surname>Imtiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayed</forename><surname>Ben</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="548" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Buffelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Vandin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Invariance principle meets out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05441</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19314" to="19326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph neural network-based anomaly detection in multivariate time series</title>
		<author>
			<persName><forename type="first">Ailin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4027" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eta prediction with graph neural networks in google maps</title>
		<author>
			<persName><forename type="first">Austin</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Wiltshire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3767" to="3776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data augmentation for deep graph learning: A survey</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08235</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">All you need is low (rank) defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">Negin</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slaps: Self-supervision improves structure learning for graph neural networks</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22667" to="22681" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22092" to="22103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11960</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Franc ?ois Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robustness of graph neural networks at scale</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hakan S ?irin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7637" to="7649" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiner</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Good</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08452</idno>
		<title level="m">A graph out-of-distribution benchmark</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graphbased molecular representation learning</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhao</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04869</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grale: Designing networks for graph learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Mosoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2523" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10203</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Node similarity preserving graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on web search and data mining</title>
		<meeting>the 14th ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial attacks and defenses on graphs</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial attack on large scale graph</title>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenfang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deeprobust: A pytorch library for adversarial attacks and defenses</title>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06149</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on nonhomophilous graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Confidence may cheat: Self-training on graph neural networks under distribution shift</title>
		<author>
			<persName><forename type="first">Hongrui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1248" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparsity-aware sensor collaboration for linear coherent estimation</title>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swarnendu</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makan</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pramod</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2582" to="2596" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph neural networks with adaptive residual</title>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hfkER_KJiNw" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 2021a</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph selfsupervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ttt++: When does self-supervised test-time training fail or thrive?</title>
		<author>
			<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Bastien Van Delft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Bellot-Gurlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21808" to="21820" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkx1qkrKPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pathfinder discovery networks for neural message passing</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Englert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2547" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph neural networks for friend ranking in large-scale social platforms</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2535" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial deep network embedding for cross-network node classification</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Lai</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kup-Sze</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2991" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Test-time training with self-supervision for generalization under distribution shifts</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9229" to="9248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial graph augmentation to improve graph contrastive learning</title>
		<author>
			<persName><forename type="first">Susheel</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15920" to="15933" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Knowing your fate: Friendship, action and temporal explanations for user engagement prediction on social apps</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2269" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Social structure of facebook networks</title>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Traud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Graph attention networks</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep graph infomax. ICLR (Poster)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tent: Fully test-time adaptation by entropy minimization</title>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=uXl3bZLkr3c" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive graph convolutional networks</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1457" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=FQOC5u-1egI" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2022a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hGXij5rfiHw" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2022b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10757</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04214</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Generalized out-of-distribution detection: A survey</title>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11334</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bridging the gap between few-shot and manyshot learning via distribution calibration</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9830" to="9843" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph domain adaptation via theory-grounded spectral regularization</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschl?ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><surname>Memo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09506</idno>
		<title level="m">Test time robustness via adaptation and augmentation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A synergistic approach for graph anomaly detection with pattern mining and feature learning</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In AAAI, 2021b</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Graph data augmentation for graph machine learning: A survey</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08871</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning from counterfactual links for link prediction</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26911" to="26926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Shift-robust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bylnx209YX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
