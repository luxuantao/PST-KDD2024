<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hardware Evaluation of Cache Partitioning to Improve Utilization and Energy-Efficiency while Preserving Responsiveness</title>
				<funder ref="#_AgEZwRw">
					<orgName type="full">Microsoft</orgName>
				</funder>
				<funder ref="#_TAsnR7x">
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_2quJxGK">
					<orgName type="full">Spanish Ministry of Science</orgName>
				</funder>
				<funder>
					<orgName type="full">Par Lab affiliates Nokia, NVIDIA, Oracle</orgName>
				</funder>
				<funder ref="#_wpkNVhC">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_GWC3ktr">
					<orgName type="full">U.C. Discovery</orgName>
				</funder>
				<funder>
					<orgName type="full">Samsung</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Henry</forename><surname>Cook</surname></persName>
							<email>hcook@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department Universitat Politecnica de Catalunya</orgName>
								<orgName type="laboratory">The Parallel Computing Laboratory ? Computer Architecture Department CS Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jordi Girona</orgName>
								<address>
									<addrLine>1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miquel</forename><surname>Moreto</surname></persName>
							<email>mmoreto@ac.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department Universitat Politecnica de Catalunya</orgName>
								<orgName type="laboratory">The Parallel Computing Laboratory ? Computer Architecture Department CS Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jordi Girona</orgName>
								<address>
									<addrLine>1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Bird</surname></persName>
							<email>slbird@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department Universitat Politecnica de Catalunya</orgName>
								<orgName type="laboratory">The Parallel Computing Laboratory ? Computer Architecture Department CS Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jordi Girona</orgName>
								<address>
									<addrLine>1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khanh</forename><surname>Dao</surname></persName>
							<email>khanhdao@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department Universitat Politecnica de Catalunya</orgName>
								<orgName type="laboratory">The Parallel Computing Laboratory ? Computer Architecture Department CS Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jordi Girona</orgName>
								<address>
									<addrLine>1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
							<email>pattrsn@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">EECS Department Universitat Politecnica de Catalunya</orgName>
								<orgName type="laboratory">The Parallel Computing Laboratory ? Computer Architecture Department CS Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Jordi Girona</orgName>
								<address>
									<addrLine>1-3</addrLine>
									<postCode>08034</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hardware Evaluation of Cache Partitioning to Improve Utilization and Energy-Efficiency while Preserving Responsiveness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computing workloads often contain a mix of interactive, latency-sensitive foreground applications and recurring background computations. To guarantee responsiveness, interactive and batch applications are often run on disjoint sets of resources, but this incurs additional energy, power, and capital costs. In this paper, we evaluate the potential of hardware cache partitioning mechanisms and policies to improve efficiency by allowing background applications to run simultaneously with interactive foreground applications, while avoiding degradation in interactive responsiveness. We evaluate these tradeoffs using commercial x86 multicore hardware that supports cache partitioning, and find that real hardware measurements with full applications provide different observations than past simulation-based evaluations. Co-scheduling applications without LLC partitioning leads to a 10% energy improvement and average throughput improvement of 54% compared to running tasks separately, but can result in foreground performance degradation of up to 34% with an average of 6%. With optimal static LLC partitioning, the average energy improvement increases to 12% and the average throughput improvement to 60%, while the worst case slowdown is reduced noticeably to 7% with an average slowdown of only 2%. We also evaluate a practical low-overhead dynamic algorithm to control partition sizes, and are able to realize the potential performance guarantees of the optimal static approach, while increasing background throughput by an additional 19%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Energy efficiency and predictable response times are firstorder concerns across the entire computing spectrum, rang-ing from mobile clients to warehouse-scale cloud computers.</p><p>For mobile devices, energy efficiency is critical, as it affects both battery life and skin temperature, and predictable response times are essential for a fluid user interface. Some mobile systems have gone so far as to limit which applications can run in the background <ref type="bibr">[1]</ref> to preserve responsiveness and battery life.</p><p>In warehouse-scale computing, energy inefficiencies increase electricity consumption and operational costs and can significantly impact the capital costs of the infrastructure needed to distribute power and cool the servers <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b15">17]</ref>. As with mobile devices, researchers have also found unpredictable response times to be very expensive in warehousescale computing <ref type="bibr" target="#b29">[31]</ref>. In one example, inserting delays in a search engine slowed down user response time by more than the delay, which decreased the number of overall searches, user satisfaction, and revenue; the results were so negative that the researchers stopped their experiment early <ref type="bibr" target="#b29">[31]</ref>. To preserve responsiveness, cloud computing providers often dedicate large clusters to single applications, despite the hardware being routinely utilized at only 10% to 50% <ref type="bibr" target="#b0">[2]</ref>.</p><p>In this paper, we study the potential to reduce the waste from resources held idle in the name of responsiveness by co-scheduling background applications with foreground applications. Although some application workloads already include parallelized codes, few applications scale perfectly with increasing core count, leading to underutilized resources, and potentially providing an opportunity to increase system efficiency through consolidation. But this benefit can only be realized if there is minimal degradation in responsiveness of latency-sensitive user-facing tasks. In the client, the goal is to complete background work while the foreground task is active, so that the mobile device can quickly return to a very low-power hibernation mode, thereby extending battery life. In the cloud, the goal is to co-schedule background tasks, such as indexing, with user-facing web applications to obtain the greatest value from the huge sunk investment in machines, power distribution, and cooling.</p><p>Low-priority background tasks degrade the responsiveness of a high-priority foreground task primarily through contention for shared hardware resources, such as on-chip shared cache or off-chip DRAM bandwidth. In this pa-per, we study partitioning the capacity of a shared lastlevel cache (LLC) as a means to potentially mitigate the negative performance effects of co-scheduling, and design a lightweight practical algorithm to dynamically divide the LLC among applications. Our study uses a prototype commercial x86 multicore processor (Sandy Bridge) executing multiple large parallel applications taken from several modern benchmark suites, and we measure power and energy to explore the potential benefits of LLC partitioning. While other mechanisms to mitigate such degradation are the subject of active research <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b21">23]</ref> (in particular, techniques for LLC partitioning <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref>), the past work has predominantly considered sequential applications and has mostly been simulation-based, which limits the size of possible workloads and the ability to accurately measure performance and energy. We found that our conclusions differ as a result of these differences in experimental methods.</p><p>In our analysis, we found significant potential for consolidation of applications without harming their responsiveness. The considerable size of the LLC (6 MB) makes cache partitioning unnecessary in many cases-a result not observed by previous work because the studies mostly simulated much smaller cache sizes (1-2 MB). Without any partitioning, we found that nearly 50% of the 45 benchmarks tested slowed down less than 2.5% with a co-scheduled background application. This is unsurprising considering that, when studied individually, we found that 44% of applications had working sets that fit in 1 MB and 78% fit in 3 MB. Overall, consolidation without LLC partitioning provided a 10% energy improvement and a 54% performance improvement. However, sharing did occasionally result in significant slowdowns, of up to nearly 35% in the worst case, with an average slowdown of 6%.</p><p>Co-scheduling applications with optimal static LLC partitioning increased the average energy improvement to 12% and the average performance improvement to 60%, while more effectively protecting the foreground application-the average slowdown was just 2% and the worst case only 7%. For 16% of the cases (almost exclusively those running with a low-scalability sensitive application), LLC partitioning provided even more significant speedups-20% on average.</p><p>We also introduce and evaluate a practical online dynamic partitioning algorithm, and show that it maintains foreground application performance to within 1% of the optimal static partition, while increasing the background computation throughput by 19% on average and as much as 2.5? in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">EXPERIMENTAL METHODOLOGY</head><p>In this section, we describe the hardware platform and benchmarks used in our evaluation. While this paper does not evaluate server hardware, we believe the approach and resulting conclusions are just as relevant for the cloud, although the quantitative results would surely change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Platform Configuration</head><p>We use a prototype version of Intel's Sandy Bridge x86 processor that is similar to the commercially available client chip, but with additional hardware support for way-based LLC partitioning. By using a real hardware prototype, we are able to run complete, full-sized applications for realistic time scales on a standard operating system and accurately measure performance and energy.</p><p>The Sandy Bridge client chip has 4 quad-issue out-of-order superscalar cores, each of which supports 2 hyperthreads using simultaneous multithreading <ref type="bibr" target="#b16">[18]</ref>. Each core has private 32 KB instruction and data caches, as well as a private 256 KB non-inclusive L2 cache. The LLC is a 12-way setassociative 6 MB inclusive cache, shared by all cores via a ring interconnect. All cache levels are write-back.</p><p>The cache partitioning mechanism is way-based and works by modifying the cache-replacement algorithm. Each core can be assigned a subset of the 12 ways in the LLC. Way allocations can be completely private, completely shared, or overlapping. Although all cores can hit on data stored in any way, a core can only replace data in its assigned ways. Data is not flushed when the way allocation changes.</p><p>We use a customized BIOS that enables the cache partitioning mechanism, and run unmodified Linux-2.6.36 for all of our experiments. We use the Linux taskset command to pin applications to sets of hyperthreads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Performance and Energy Measurement</head><p>To measure application performance, we use the libpfm library <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b25">27]</ref>, built on top of the perf_events infrastructure introduced in Linux 2.6.31, to access performance counters available on the machine <ref type="bibr" target="#b17">[19]</ref>.</p><p>To measure on-chip energy, we use the energy counters available on Sandy Bridge to measure the consumption of the entire socket and also the total combined energy of cores, private caches, and the LLC. We access these counters using the Running Average Power Limit (RAPL) interfaces <ref type="bibr" target="#b17">[19]</ref>. The counters measure power at a 1/2 16 second granularity.</p><p>In addition, we use a FitPC external multimeter to measure the power consumed by the entire system at the wall socket with a 1 second granularity. We correlate the wall power data with the data collected from the hardware energy counters using time stamps. We observed less than one second of delay in these measurements consistently across all experiments. Together, these mechanisms allow us to collect accurate energy readings over the entire course of an application's execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Description of Workloads</head><p>We build our workload using a wide range of codes from three different popular benchmark suites: SPEC CPU 2006 <ref type="bibr" target="#b30">[32]</ref>, DaCapo <ref type="bibr" target="#b4">[6]</ref>, and PARSEC <ref type="bibr" target="#b2">[4]</ref>. We include some additional applications to broaden the scope of the study, and microbenchmarks to exercise certain system features.</p><p>The SPEC CPU2006 benchmark suite <ref type="bibr" target="#b30">[32]</ref> is a CPUintensive, single-threaded benchmark suite, designed to stress a system's processor, memory subsystem and compiler. Using the similarity analysis performed by Phansalkar et al. <ref type="bibr" target="#b26">[28]</ref>, we subset the suite, selecting 4 integer benchmarks (astar, libquantum, mcf, omnetpp) and 4 floatingpoint benchmarks (cactusADM, calculix, lbm, povray). Based on the characterization study by Jaleel <ref type="bibr" target="#b19">[21]</ref>, we also pick 4 extra floating-point benchmarks that stress the LLC: GemsFDTD, leslie3d, soplex and sphinx3. When multiple input sets and sizes are available, we pick the single ref input indicated by Phansalkar et al. <ref type="bibr" target="#b26">[28]</ref>. SPEC CPU was the only benchmark suite used in many previous characterizations of LLC partitioning <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref>, but only a few hundred million instructions were simulated in these studies.</p><p>We include the DaCapo Java benchmark suite as a representative of a managed-language workloads. The managed nature of the Java runtime environment has been shown to make a significant difference in some scheduling studies <ref type="bibr" target="#b10">[12]</ref>. We use the latest 2009 release, which consists of a set of open-source, real-world applications with non-trivial memory loads, including both client and server-side applications.</p><p>The PARSEC benchmark suite is intended to be representative of parallel real-world applications <ref type="bibr" target="#b2">[4]</ref>. PARSEC programs use various parallelization approaches, including data-and task-parallelization. We use the pthreads version for all benchmarks, with the exception of freqmine, which is only available in OpenMP. Although reduced input sets are available for simulation studies, we use the largest input sets designed for native execution. Previous characterizations of PARSEC have found it to be sensitive to cache capacity <ref type="bibr" target="#b2">[4]</ref>, but also resilient to performance degradation in the face of intra-application cache sharing <ref type="bibr" target="#b37">[39]</ref>, .</p><p>We add four additional parallel applications from local researchers to represent important algorithms at the core of future applications: Browser_animation is a multithreaded kernel representing a browser layout animation <ref type="bibr" target="#b23">[25]</ref>; G500_csr code is a breadth-first search graph algorithm <ref type="bibr" target="#b1">[3]</ref>; Paradecoder is a parallel speech-recognition application that takes audio waveforms of human speech and infers the most likely word sequence intended by the speaker <ref type="bibr" target="#b8">[10]</ref>; Stencilprobe simulates heat transfer in a fluid using a parallel stencil kernel over a regular grid <ref type="bibr" target="#b20">[22]</ref>.</p><p>We also add two microbenchmarks that stress the memory system: stream_uncached is a memory and on-chip bandwidth hog that continuously brings data from memory without caching it, while ccbench explores arrays of different sizes to determine the structure of the cache hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PERFORMANCE STUDIES</head><p>Our first set of experiments explores the sensitivity of each application to different resources in the system: hyperthreads, LLC capacity, prefetcher configurations, and the on-chip LLC bandwidth and off-chip DRAM bandwidth. We then use machine learning to cluster applications based on their resource requirements, and select a set of representative applications for further evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Thread Scalability</head><p>We first study parallel scalability for a fixed problem size. Figure <ref type="figure" target="#fig_0">1</ref> shows the speedup of each application as we increase its allocation from 1 to 8 threads. When adding new threads, we first assign both hyperthreads available in one core before moving on to the next core. For example, allocations with four threads correspond to running on both hyperthreads of two cores. This method fits our scenario of consolidating applications in a multiprogrammed environment, where different applications should be pinned to disjoint cores to avoid thrashing at inner cache levels <ref type="bibr" target="#b34">[36]</ref>. Many PARSEC applications scale well (Fig. <ref type="figure" target="#fig_0">1a</ref>): six benchmarks scale up over 4?, four benchmarks between 3-4?, and just three show more modest scaling factors (2-3?). For the majority of these applications, we can see that performance keeps growing at a similar rate up to at least 8 threads. The DaCapo applications in Fig. <ref type="figure" target="#fig_0">1b</ref> are largely less scalable than the PARSEC applications. Only two applications show speedups over 4?, with two others between 2-3?, and ten between 1-2.3?. Furthermore, the performance of all the low-scalability applications saturates after 4 or 6 threads. The intrinsic parallelism available in some of the DaCapo benchmarks together with the scalability bottlenecks for garbage collectors explain this behavior <ref type="bibr" target="#b13">[15]</ref>. Finally, the scalability results for the additional parallel applications and microbenchmarks are presented in Figure <ref type="figure" target="#fig_0">1c</ref>. The microbenchmarks are single-threaded, while the parallel applications are all memory-bandwidth-bound on this platform (we have observed parallel speedups on other platforms), explaining the limited scalability measured.</p><p>We now classify applications according to their thread scalability. Table <ref type="table" target="#tab_0">1</ref> groups applications in each suite into three categories: applications with low scalability, applications that scale up to a reduced number of threads, and applications that continue to scale up with the number of threads. There are noticeable differences between suites, with PARSEC clearly being the most scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Last-Level Cache Sensitivity</head><p>We next evaluate how sensitive the benchmarks are to amount of LLC capacity allocated to them. Taking advantage of the partitioning mechanism in the LLC, we change the LLC space allocated to a given application from 0.5 MB to 6 MB. In the interests of space, we show only the behavior of three representative applications in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Unsurprisingly, the first conclusion that we can draw is that running an application inside a 0.5 MB direct-mapped LLC is always detrimental. In addition to conflict misses from the direct mapping, inclusivity issues for inner cache levels lead to significant increases in execution time. The second observation is that the LLC is clearly overprovisioned for these applications. We found 44% of the applications only require 1 MB to reach their maximum performance, while 78% require less than 3 MB. Others have observed similar behavior for cloud computing applications <ref type="bibr" target="#b12">[14]</ref>.</p><p>Finally, we do not observe clear knees in execution time as we increase allocated LLC capacity. Previous simulationbased studies took advantage of these knees to propose dynamic cache partitioning techniques <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref>. In contrast, performance improves smoothly with the allocated LLC capacity for all applications. The combination of memorymapping functions, randomized LLC-indexing functions, pre-fetchers, pseudo-LRU eviction policies, as well as having multiple threads simultaneously accessing the LLC, serve to remove clear working-set knees in the real system.</p><p>Next, we classify applications into 3 categories according to their LLC sensitivity, ignoring the pathological directmapped 0.5 MB case. Low utility applications (e.g., swaptions) yield the same performance despite increased available LLC space. Saturated utility applications (e.g., tomcat) benefit from extra LLC space up to a saturation point. Finally, high utility applications (e.g., 471.omnetpp) always benefit from more LLC space. Figure <ref type="figure" target="#fig_1">2</ref> shows a representative application for each category. We observe that increasing the number of threads assigned to an application decreases LLC sensitivity. Additional cores result in larger aggregate private cache (L1 and L2) and a greater overlap of memory accesses, thereby reducing pressure on the LLC.</p><p>Table <ref type="table" target="#tab_1">2</ref> categorizes the benchmarks in each suite according to their LLC utility. We highlight applications with more than 10 LLC accesses per kilo-instruction in bold, because these applications may cause bandwidth contention and pol- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prefetcher Sensitivity</head><p>We now characterize the sensitivity of applications to hardware prefetching configurations, because some prefetchers are a shared resource that cannot be partitioned (unlike hyperthreads and LLC). In a multi-programmed environment, access streams from different applications could impact sensitive applications if they degrade prefetcher efficacy.</p><p>There are four distinct hardware prefetchers on Sandy Bridge platforms: 1) Per-core Data Cache Unit (DCU) IPprefetchers look for sequential load history to determine whether to prefetch the data to the L1 caches; 2) DCU streamer prefetchers detect multiple reads to a single cache line in a certain period of time and choose to load the following cache lines to the L1 data caches; 3) Mid-Level cache (MLC) spatial prefetchers detect requests to two successive cache lines and are triggered if the adjacent cache lines are accessed; 4) MLC streaming-prefetchers work similarly to the DCU streamer-prefetchers, which predict future access patterns based on the current cache line reads. We can activate or deactivate each prefetcher by setting the corresponding machine state register (MSR) bits <ref type="bibr" target="#b17">[19]</ref>.</p><p>Figure <ref type="figure">3</ref>   when all prefetchers are active normalized to the configuration with all prefetchers disabled. In general, the applications are more sensitive to the DCU spatial prefetcher, but the MLC prefetcher is also important for some applications. Nearly all applications are insensitive to the prefetcher configuration (36 out of 46). In PARSEC, only facesim and streamcluster benefit from the prefetchers. No DaCapo applications benefit significantly from the prefetchers, and lusearch performance even degrades when the prefetchers are active. In contrast, SPEC benchmarks are more sensitive to prefetching, particularly 450.soplex, 459.GemsFDTD, 462.libquantum and 470.lbm. The majority of the additional applications also benefit from the prefetchers. However, we also observed that increasing the number of threads in the system reduces overall prefetcher effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LLC and DRAM Bandwidth Sensitivity</head><p>We also characterize how sensitive applications are to any memory bandwidth contention, because bandwidth is a shared resource that cannot be partitioned. Access We characterize applications according to their performance when running together with a bandwidth-hogging microbenchmark (stream_uncached), which uses specially tagged load and store instructions to stream through memory without caching data in the LLC. Bandwidth-sensitive applications will suffer from being run concurrently with this benchmark. Figure <ref type="figure" target="#fig_2">4</ref> shows the increase in execution time of all applications when running with stream_uncached. Only two PARSEC applications suffer (fluidanimante and streamcluster), while DaCapo applications are not affected much by bandwidth contention. In the case of SPEC, some benchmarks are not affected at all (436.cactusADM, 453.povray, 454.calculix, 473.astar) and others are heavily affected (450.soplex, 459.gemsFDTD, 462.libquantum, 470.lbm). In contrast, all the added parallel applications are bandwidth sensitive. In general, the applications are more sensitive to bandwidth contention than to prefetcher configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Clustering Analysis</head><p>To reduce our study to a feasible size, we use the application characterization studies described above to select a subset of the benchmarks representative of different application resource behaviors. Following in the footsteps of <ref type="bibr" target="#b26">[28]</ref>, we use machine learning to select representative benchmarks. We use a hierarchical clustering algorithm <ref type="bibr" target="#b26">[28]</ref> with the singlelinkage method from the Python library scipy-cluster .</p><p>We create a feature vector of 19 values for each application using the measurements from the previous subsections: 1) execution time as we increase the number of threads (7 features); 2) execution time as we increase the LLC size (10 features); 3) prefetcher sensitivity (1 feature); and 4) bandwidth sensitivity(1 feature). All metrics are normalized to the interval [0, 1]. The clustering algorithm finds the small- est Euclidean distance of a pair of feature vectors and forms a cluster containing that pair. It continues selecting the next smallest distance between a pair and forms another cluster. Linkage criteria can be used to adjust cluster formation.</p><p>The single-linkage we selected uses the minimum distance between a pair of objects in different clusters to determine the distance between them.</p><p>Figure <ref type="figure">5</ref> shows the dendogram for the studied applications. The Y-axis represents the linkage-distance between applications. Applications within a distance of 0.9 are assigned the same cluster and colored to match. The first two clusters contain applications with low thread scalability. The first cluster is more sensitive to LLC space, but less sensitive to bandwidth and the prefetcher. Applications in the third cluster present high thread scalability and low cache utility and are insensitive to the prefetcher. The last three clusters are comprised of applications with saturated thread scalability, but different cache utility. The fourth cluster is more sensitive to cache space than the rest, the fifth is insensitive to cache space, and the sixth is insensitive to bandwidth contention. There is also a cluster with only one application (fluidanimate), which stands apart as it only runs correctly when allocated a power-of-2 number of threads. Due to this irregularity, we do not consider this cluster any further in our analysis. Table <ref type="table" target="#tab_4">3</ref> lists representative applications by cluster for each benchmark suite. Applications highlighted in bold are closest to the centroid of the cluster. We select these applications to represent the cluster in our consolidation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ENERGY VERSUS PERFORMANCE</head><p>Our next experiments explore the power, energy, and performance tradeoffs available in our system.</p><p>Controlling the number of cores assigned to an application, and the frequency at which those cores run, is the most well-studied technique to control energy consumption. However, it is worth noting that making energy-efficient optimizations sometimes involves counter-intuitive choices. For example, activating additional cores or raising frequency increases power consumption, but can often result in lower overall energy consumption per task, since the task may finish earlier allowing the system go into a much lower energy sleep state. This operating scenario is often described as race-to-halt, where the best energy efficiency is obtained by optimizing for the highest performance to more quickly complete a task and then move to a sleep state to save energy. However, allocating cores that do not improve performance can decrease energy efficiency. A memory-bound application is unlikely to see any performance benefit if run at a higher The results also illustrate that race-to-halt is the optimal energy strategy.</p><p>frequency or allocated additional cores, but would consume more energy while waiting for data to be provided by the memory system. Cache capacity allocation decisions are usually more straightforward, and typically only impact energy by changing the number of LLC misses an application incurs. LLC misses can increase energy consumption both by requiring additional data to be fetched from DRAM, and by the increased program runtime this might cause. Socket power does not change as a function of the cache allocated, since current hardware cannot turn off power to a portion of the cache. This limitation makes it desireable to try to reassign underutilized capacity to another application instead.</p><p>To better understand the space of possible performance and energy tradeoffs, we execute each representative with every possible thread and way allocation and measure the performance and energy. Each benchmark is tested with 1-8 threads and 1-12 cache ways (96 different allocations). Figure <ref type="figure" target="#fig_3">6</ref> shows plots of the runtime, LLC misses per kilo instruction (MPKI), and total socket and wall energy consumption of all possible resource allocations for the six cluster representatives. When considering execution time versus miss rate, we can see that some applications have runtimes that are tightly correlated with miss rate (429.mcf and fop), while others are insensitive (ferret and dedup), or see diminishing returns (459.GemsFDTD and batik).</p><p>When considering energy, our measurements strongly suggest that race-to-halt is the right optimization strategy for nearly all of our benchmarks. This is specially significant in the case of the wall energy, since the energy consumed in other parts of the system adds to total energy consumption. While there are a spread of points to consider when picking an allocation that minimizes LLC miss rate at a particular execution time, for nearly all benchmarks this curve narrows significantly when energy is factored in. We see this effect because in general miss rates are correlated with both increased energy and increased execution time, limiting the possibilities for a high-miss-rate allocation to have lower energy via a faster runtime, or a faster-runtime allocation to expend more energy than it saves.</p><p>We also can see many resource allocations achieve near optimal execution time, indicating that there should be spare resources available for background work to use when the application is running. Figure <ref type="figure" target="#fig_5">7</ref> illustrates this point by showing the contour plots of the wall energy for each benchmark. We obtained very similar figures when considering runtime and socket energy. Significantly, many applications have one or more energy-optimal configurations that are not the largest allocation, indicating that most applications do not require the whole system to achieve peak performance. Some applications do not benefit from more than one thread (429.mcf and 459.GemsFDTD), others require all threads to minimize energy consumption (dedup and ferret), and some applications have a range of possible thread counts that maximize energy-efficiency (batik and fop). More importantly, all of them can yield some space in the LLC without affecting their performance, ranging from 0.5 MB (429.mcf) to 4 MB (batik and ferret). This resource gap between equally optimal allocations presents us with an opportunity: we could run additional work concurrently on the remaining LLC and core resources, assuming we can prevent destructive interference.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MULTIPROGRAM ANALYSES</head><p>In this section, we show it is often possible to take advantage of the excess resources to save energy and improve system throughput, without impacting foreground application performance, by reducing the foreground application's allocation and running a concurrent background application. For some combinations of applications, we can effectively consolidate applications without partitioning the LLC. However, other combinations require LLC partitioning to protect the foreground application's performance. We examine the relative effectiveness of uneven, even and no cache partitioning in terms of energy, background throughput, and average and worst-case foreground performance degradation.</p><p>We run each multithreaded application with 4 threads on 2 cores with 2 active HTs. Some applications did exhibit slightly different cache scalability for 2, 4, 6, or 8 cores, so we used the 4-core values in our clustering analysis, ensuring a representative set of pairing scenarios for this study. We found that cases where applications were assigned odd numbers of hyperthreads (meaning that two applications were sharing a core) had many performance anomalies due to sharing of the L1 resources and thus chose to study splitting the cores evenly to more clearly observe the effects of consolidating applications in the LLC. Exploring alternative core allocations is the subject of future work <ref type="bibr" target="#b3">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Shared Cache Performance Degradation</head><p>To begin, we execute all possible pairs of applications together with no cache partitioning. In addition to LLC capacity, both applications are sharing the on-chip interconnection network to access the LLC and off-chip memory bandwidth. Figure <ref type="figure" target="#fig_6">8</ref> shows a heat map of the relative ex-  ecution time of the foreground application for all possible pairs of applications. The values are normalized to the execution time of the application when running alone on the system with 2 cores with 2 active HTs each. For example, canneal's execution time when running with streamcluster in the background increases 29% (dark red), while the execution time of streamcluster is affected much less (8.3%, bright red) when running with canneal in the background.</p><p>It is important to note that these relationships can be asymmetric. Some applications are sensitive to contention for the shared hardware resources (represented by a darker average vertical column). These benchmarks are more affected when running together with a background application. There is one sensitive application in PARSEC (stream-cluster), none in DaCapo, and 5 in SPEC CPU 2006 (437.leslie, 450.soplex, 459.GemsFDTD, 462.libquantum, and 470.lbm). All the new applications except ccbench are also sensitive. The average slowdown for these benchmarks is over 10%.</p><p>Independent of sensitivity, applications can also be aggressive and affect the performance of foreground applications if run in the background (presented by a darker average horizontal row). In our results, we find SPEC CPU 2006 and the additional parallel applications tend to be more aggressive. DaCapo benchmarks, on the other hand, only slightly affect other foreground applications. The applications that are the most significant aggressors (causing an average slowdown over 10%) are canneal, lusearch, 471.omnetpp, Pa-raDecoder, browser_animation and stream_uncached. Some applications are not affected when sharing the machine with a background application. Twenty-two of the 45 applications slow down less than 2.5% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mitigating Degradation with Partitioning</head><p>We consider three cache partitioning policies in this section. As the baseline, we use data from the previous subsection, where both applications share the entire cache without partitioning (shared). The other two approaches are to statically partition the cache between the two applications, in one case splitting the LLC evenly (fair), and in the second case giving each application some uneven allocation (biased). We evaluate all possible biased allocations and report results for the one that is the best (i.e., among allocations with minimum foreground performance degradation, select the one that maximizes background performance). Figure <ref type="figure" target="#fig_7">9</ref> presents the effectiveness of these three policies at preserving the performance of the representative applications. We run two applications simultaneously, each with 4 hyperthreads on two cores. All values are normalized to the execution time of the foreground application alone. For some foreground applications (C3, C5, C6), less than 5% degradation is present for the shared case, indicating their limited sensitivity. Thus, the improvement provided by partitioning is minor. For some applications pairs, degradation is somewhat or completely mitigated by biased partitioning, but not mitigated by fair partitioning. Overall, we find that biased partitioning results in lower average (2.3%) and worst case (7.4%) slowdown than shared (5.9% and 34.5% respectively). Half of the applications have effectively no slowdown with the biased partitioning as compared with only a quarter for shared. Fair is close to shared in terms of average slowdown (6.1%), but with a better worse case (16.3%).</p><formula xml:id="formula_0">C1+C1 C1+C2 C1+C3 C1+C4 C1+C5 C1+C6 C2+C1 C2+C2 C2+C3 C2+C4 C2+C5 C2+C6 C3+C1 C3+C2 C3+C3 C3+C4 C3+C5 C3+C6 C4+C1 C4+C2 C4+C3 C4+C4 C4+C5 C4+C6 C5+C1 C5+C2 C5+C3 C5+C4 C5+C5 C5+C6 C6+C1 C6+C2 C6+C3 C6+C4 C6+C5 C6+C6 Average 0.9 1.0 1.1 1.2 1.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rel. Execution Time</head><note type="other">Shared Fair Biased</note><p>The fact that degradation remains present in some cases, even though we have sequestered the applications on disjoint sets of cores and (in the biased case) provided them with optimally-sized cache allocations, implies that either the cache is just not large enough to accommodate both applications or bandwidth contention on the on-chip ring interconnect or off-chip DRAM interface is to blame. Prior work <ref type="bibr" target="#b21">[23]</ref> has proposed mechanisms to partition such bandwidth resources, but unfortunately they are not available on extant hardware. While there is little we can do to mitigate the contention that exists on our platform, there are a number of points where energy optimization through consolidation is still possible.</p><p>We also examined more extreme cases with one foreground application and two or more copies of the background applications continuously running. However, adding additional applications only further increased contention for cache capacity and DRAM bandwidth. As expected the benchmarks already experiencing degradation with one background application, slowed down further when more were added. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Energy Efficiency of Consolidation</head><p>Figure <ref type="figure" target="#fig_0">10</ref> compares running each application once, one after another, on the whole machine, with the different partitioning policies that run both applications once, concurrently, on the machine. The theoretical upper bound in energy savings is 50% and decreases as the applications have more disparate execution lengths. We measure an average energy improvement of 12% and a maximum of 37% for biased. The two other policies have similar but slightly lower improvements. The results differ mainly when 459.GemsFDTD is running with fop, dedup or batik.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> shows the weighted speedup of the application pairs running together as compared to each running alone. The results show an average speedup of 60% using biased partitioning and slightly lower for shared and fair. In 16% of the cases (mainly involving 459.GemsFDTD), biased provides significant improvements over shared (20% on average).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DYNAMIC CACHE PARTITIONING</head><p>In the previous section, we saw the potential for consolidation and found that for a static partition, biased partitioning provides the lowest average and worst-case foreground degradation, and the highest average performance and energy improvements. However, choosing the best partition would require testing all possible allocations, which is infeasible in practice, and such an optimal static allocation still cannot take into account phase-based behavior of the applications. In this section, we present our implementation of a dynamic cache-capacity allocation framework controlled by a utility-based policy to maximize background throughput and mitigate performance penalties without an oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Opportunity</head><p>Applications often have phases with very different resource requirements. For example, Figure <ref type="figure" target="#fig_9">12</ref> shows the number of LLC misses per kilo-instruction (MPKI) for different cache allocations for 429.mcf. This application transitions 5 times between low LLC MPKI and high LLC MPKI phases. In the phases with high MPKI, 429.mcf requires 4.5 MB (9 ways) to reach 95% of its maximum performance, while in the other phases, only 1.5 MB (3 ways) are required. The phases with low MPKI represent opportunities to reduce the amount of LLC allocated to the application without negatively impacting its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Phase-Detection Framework</head><p>We have created a software framework to monitor behavior and respond to phase changes by reallocating cache resources. We use libpfm to monitor the application's performance. The framework runs on the hyperthreads assigned to the application, and has negligible impact on application performance when passively monitoring behavior. The framework detects phase changes by looking for changes in LLC misses per kilo-instruction over a 100 millisecond interval. Algorithm 6.1 shows the phase-detection pseudocode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dynamic Partitioning Algorithm</head><p>When a phase change is detected, a dynamic reallocation algorithm is activated to determine the LLC capacity required by the new phase. Specifically, when the foreground using partitioning to provide a minimum performance to applications <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b24">26]</ref>. However, these proposals were all evaluated on a simulator, and all but <ref type="bibr" target="#b18">[20]</ref> used sequential applications -leading to different conclusions.</p><p>The 6 MB LLC in our system is often large enough for two applications to share without degradation, irrespective of cache partitioning policy. This behavior was not observed in previous studies based on simulators because the studies limited their cache sizes to 1-2 MB, which is closer to the working-set size of many of the studied applications.</p><p>Even in cases where degradation is present, LLC partitioning algorithms provide performance improvements for only a minority of our workloads. Consequently, relative to prior work, we measure reduced average performance improvements of LLC partitioning algorithms over naive sharing strategies (less than 3%). Simulation-based studies have reported much larger performance improvements <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b28">30]</ref>, likely due to the reduced LLC capacity in their simulations, or possibly because they do not consider randomized LLCindexing functions, memory-mapping functions, prefetchers, or memory-bandwidth contention.</p><p>Several groups have studied the impact of workload consolidation in shared caches <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39]</ref> for datacenter applications. They do not explore hardware solutions such as cache partitioning, or client-side workloads.</p><p>Other authors make use of page coloring to partition the LLC by sets <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b32">34]</ref>. Cho and Lin <ref type="bibr" target="#b7">[9]</ref> experiment with page coloring on a simulator to reduce access latency in distributed caches. Tam et al. <ref type="bibr" target="#b32">[34]</ref> map pages into each core's private color using static decisions with Linux on a POWER5. Lin et al. <ref type="bibr" target="#b22">[24]</ref> evaluate a dynamic scheme using page coloring on an Intel Xeon 5160 processor. However, there is a significant performance overhead inherent to changing the color of a page. Another challenge is that the number of available partitions is a function of page size, so increasing page size can make page coloring ineffective or impossible <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b32">34]</ref>. The experiments on real machines use only 2 running threads and make decisions only every several seconds. In contrast, our approach can change LLC partitions much more quickly and with minimal overhead.</p><p>Others have proposed hardware support to obtain the cache miss rate, IPC curves or hardware utilization information <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b27">29]</ref> to provide QoS. These approaches require hardware modifications and will not work on current processors. Chanda et al. <ref type="bibr" target="#b6">[8]</ref> explored on-line methods to extrapolate these curves, but their approach has significant complexity and overhead. Tam et al. <ref type="bibr" target="#b33">[35]</ref> use performance counters on POWER5 to predict miss curves for different page coloring assignments. They use a stack simulator and an LLC address trace to obtain the miss curve and statically assign colors. Others have measured LLC misses to identify thrashing applications <ref type="bibr" target="#b36">[38]</ref> or predict contention <ref type="bibr" target="#b11">[13]</ref> enabling the scheduler to pick co-running applications with more affinity. Xie and Loh <ref type="bibr" target="#b36">[38]</ref> further use the LLC measurements to partition the cache according to their classification of applications as thrashing or non-thrashing. LLC misses can also be used to change the replacement policy of the LLC to partition the cache at finer granularity <ref type="bibr" target="#b28">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">DISCUSSION AND CONCLUSIONS</head><p>This paper measures the energy and performance of an LLC-partitioning scheme on real x86 hardware using a diverse set of full-sized serial and parallel applications. These measurements show that optimizing for performance is still also the optimal strategy for energy optimization, but that a significant number of applications do not need to run on all the available cores or with all the LLC capacity to reach an energy-optimal execution point. Based on this characterization, we evaluated the potential to co-schedule different applications using LLC partitioning and CPU binding techniques. We succeed in significantly reducing energy consumption (12% on average) and increasing performance (60% on average), while minimizing the performance degradation of the foreground applications (2% on average and 7% worst case) using LLC partitioning.</p><p>We found that for around half of our workloads, cache partitioning is unnecessary: most parallel applications do not need more than 1 MB LLC to reach their maximum performance. Consequently, the 6 MB LLC in our system (and other current systems) is typically enough for two applications to share without degradation irrespective of cache partitioning policy. Simulation studies using cache sizes closer to working set sizes (1-2 MB), show excessive interference and hence greater potential improvement from cache partitioning (&gt;10%).</p><p>We have also frequently been told by industry partners that cache partitioning will be less effective than shared caching because shared caching naturally adjusts the allocation for each application dynamically, whereas cache partitioning does not let other applications reclaim unused resources. Overall, while we find that naive LLC sharing can often be effective, there can be a significant downside in foreground performance degradation (35% worst case). Although responsiveness may not matter in some domains, it has become an increasingly crucial factor in the userexperience for both mobile and warehouse-scale computing <ref type="bibr">[1,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b29">31]</ref>. Additionally, we found that cache partitioning was actually more effective than shared caching on average: across the entire suite of applications, it provided increased performance (60%) and energy (12%) improvements versus shared caching (54% performance improvement and 10% energy) without the potential performance degradation. The worst case was only 7%, and average foreground degradation was just 2%. Furthermore, LLC partitioning algorithms present significant performance improvements (20% on average) in 16% of workloads (workloads comprised of sensitive and aggressive LLC applications).</p><p>Static partitioning is impractical, as it would require profiling of all applications before deployment and cannot react to application phases or input-dependent behavior. We found that a simple dynamic adjustment algorithm provided an effective consolidation solution. Our lightweight online dynamic partitioning algorithm maintained foreground performance to within 2% of the best static partitioning, while further increasing the background computation throughput 19% on average and by as much as 2.5? in some cases.</p><p>Finally, we determined that partitioning or other qualityof-service mechanisms for memory bandwidth could potentially be a further effective hardware addition to consider on future systems. All of the worst-case foreground slowdowns with cache partitioning (and without) were from the applications shown to be the most sensitive to memory bandwidth. The slowdowns occurred even when the background application did not have high bandwidth demands -implying that in order to to achieve robust performance isolation, latency quality-of-service in particular would need to improve.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Normalized speed up as we increase the number of threads allocated to each application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Applications representative of different LLC allocation sensitivities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Increase in execution time when running with a bandwidth hog microbenchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example performance, energy and miss rate of resource allocations. Configurations with different numbers of hyperthreads are represented with different shapes. Shade represents cache allocation: darker configurations have a smaller cache allocation. All of the applications have multiple configurations that are optimal, indicating the potential for consolidation.The results also illustrate that race-to-halt is the optimal energy strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Wall energy contour plots for the cluster representatives. Darker colors represent higher energy consumptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Normalized execution time of the foreground application (X-axis), when run concurrently with a background application (Y-axis). Spaces represented as a 20% slowdown may be greater than 20%</figDesc><graphic url="image-1.png" coords="8,92.05,56.39,160.79,139.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Effect of different consolidation approaches on the foreground application in the presence of different, continuously-running background applications. Normalized the foreground application running alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: 429.mcf LLC MPKI phase changes with different static and dynamic LLC allocations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of thread scalability</figDesc><table><row><cell>Suite</cell><cell cols="4">Low scalability Saturated scalability High scalability</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">blackscholes, body-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>track,</cell><cell>facesim,</cell></row><row><cell>PARSEC</cell><cell>-</cell><cell>canneal, dedup, ray-trace</cell><cell cols="2">ferret, vips, x264, fluidanimate,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">freqmine, stream-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">cluster, swaptions</cell></row><row><cell>DaCapo</cell><cell>h2, tradebeans, tradesoap</cell><cell>avrora, eclipse, fop, jython, batik, luindex, lusearch</cell><cell cols="2">pmd, sunflow, tom-cat, xalan</cell></row><row><cell>SPEC</cell><cell>all</cell><cell>-</cell><cell>-</cell></row><row><cell>Parallel applica-tions</cell><cell>paradecoder</cell><cell>browser animation, g500, stencilprobe</cell><cell>-</cell></row><row><cell>?bench-marks</cell><cell>ccbench, stream uncached</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of LLC allocation sensitivity.</figDesc><table><row><cell>Suite</cell><cell>Low</cell><cell></cell><cell>Saturated</cell><cell>High</cell></row><row><cell></cell><cell cols="2">blackscholes, bodytrack,</cell><cell></cell></row><row><cell></cell><cell cols="2">dedup, ferret, fluidani-</cell><cell></cell></row><row><cell>PARSEC</cell><cell cols="2">mate, freqmine, raytrace,</cell><cell cols="2">canneal, facesim x264</cell></row><row><cell></cell><cell>vips,</cell><cell>streamcluster,</cell><cell></cell></row><row><cell></cell><cell cols="2">swaptions,</cell><cell></cell></row><row><cell cols="3">DaCapo avrora, sunflow</cell><cell>batik, h2, jython, luindex, tomcat, tradesoap</cell><cell>eclipse, lusearch, pmd, fop, tradebeans, xalan</cell></row><row><cell></cell><cell cols="2">436.cactusADM, 437.les-</cell><cell></cell></row><row><cell></cell><cell>lie3d,</cell><cell>450.soplex,</cell><cell></cell></row><row><cell>SPEC</cell><cell cols="2">453.povray, 454.calculix, 459.GemsFDTD,</cell><cell>429.mcf, 473.as-tar, 482.sphinx3</cell><cell>471.omnetpp</cell></row><row><cell></cell><cell cols="2">462.libquantum,</cell><cell></cell></row><row><cell></cell><cell>470.lbm</cell><cell></cell><cell></cell></row><row><cell>Parallel applica-tions</cell><cell></cell><cell>-</cell><cell>paradecoder, stencilprobe</cell><cell>browser ani-mation, g500</cell></row><row><cell>?bench-marks</cell><cell></cell><cell>-</cell><cell>ccbench, stream uncached</cell><cell>-</cell></row></table><note><p>lute the LLC for other applications even if they do not benefit (in terms of execution time) from the allocated space. Overall, we find PARSEC applications have much more relaxed LLC requirements than the other suites. SPEC applications rarely require a large LLC despite having a significant number of LLC accesses.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows the execution time of the applications</figDesc><table><row><cell>1.2</cell><cell></cell></row><row><cell>1.0</cell><cell></cell></row><row><cell>0.8</cell><cell></cell></row><row><cell>0.6</cell><cell></cell></row><row><cell>0.4</cell><cell></cell></row><row><cell>0.2</cell><cell></cell></row><row><cell cols="2">blackscholes bodytrack canneal dedup facesim ferret fluidanimate freqmine raytrace streamcluster swaptions vips x264 avrora batik eclipse fop h2 jython luindex lusearch pmd sunflow tomcat tradebeans tradesoap xalan 429.mcf 436.cactusADM 437.leslie3d 450.soplex 453.povray 454.calculix 459.GemsFDTD 462.libquantum 470.lbm 471.omnetpp 473.astar 482.sphinx3 g500_csr ParaDecoder stencilprobe ccbench str_uncached 0.0 browser_anime PARSEC DACAPO SPEC PAR ?</cell></row><row><cell cols="2">Figure 3: Normalized execution time when enabling all</cell></row><row><cell>prefetchers enabled w.r.t. all prefetchers disabled.</cell><cell></cell></row><row><cell>2.00 2.25</cell><cell>3.8</cell></row><row><cell>1.75</cell><cell></cell></row><row><cell>1.50</cell><cell></cell></row><row><cell>1.25</cell><cell></cell></row><row><cell>1.00</cell><cell></cell></row><row><cell>0.75</cell><cell></cell></row><row><cell>0.50</cell><cell></cell></row><row><cell>0.25</cell><cell></cell></row><row><cell cols="2">blackscholes bodytrack canneal dedup facesim ferret fluidanimate freqmine raytrace streamcluster swaptions vips x264 avrora batik eclipse fop h2 jython luindex lusearch pmd sunflow tomcat tradebeans tradesoap xalan 429.mcf 436.cactusADM 437.leslie3d 450.soplex 453.povray 454.calculix 459.GemsFDTD 462.libquantum 470.lbm 471.omnetpp 473.astar 482.sphinx3 g500_csr ParaDecoder stencilprobe ccbench str_uncached 0.00 browser_anime PARSEC DACAPO SPEC PAR ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Clustering based on execution time, LLC space, memory bandwidth, and prefetcher sensitivity. All applications with the same color belong to the same cluster.</figDesc><table><row><cell></cell><cell>2.5</cell></row><row><cell>Linkage Distance</cell><cell>1.0 1.5 2.0</cell></row><row><cell></cell><cell>0.5</cell></row><row><cell></cell><cell>473.astar 429.mcf 437.leslie3d 453.povray 454.calculix 436.cactusADM h2 ParaDecoder 471.omnetpp 482.sphinx3 462.libquantum 450.soplex 459.GemsFDTD 470.lbm ccbench str uncached fluidanimate facesim streamcluster freqmine swaptions vips ferret tomcat bodytrack sunflow pmd x264 xalan g500?csr browser anim fop eclipse raytrace blackscholes avrora dedup canneal stencilprobe tradebeans lusearch tradesoap jython luindex batik 0.0</cell></row><row><cell cols="2">Figure 5:</cell></row></table><note><p>streams from concurrent applications could cause performance degradation of sensitive applications if they oversubscribe particular network links, memory channels, or Miss Status Handling Registers (MSHRs).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Cluster representatives</figDesc><table><row><cell>Suite</cell><cell cols="2">Cluster 1 Cluster 2</cell><cell>Cluster</cell><cell>Cluster</cell><cell>Cluster</cell><cell>Cluster</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell cols="2">PARSEC -</cell><cell>-</cell><cell cols="2">ferret x264</cell><cell cols="2">dedup -</cell></row><row><cell cols="2">DaCapo h2</cell><cell>-</cell><cell cols="2">sunflow fop</cell><cell cols="2">avrora batik</cell></row><row><cell>SPEC</cell><cell cols="2">429.mcf 459.gems-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>FDTD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Parallel</cell><cell>Para-</cell><cell>-</cell><cell>-</cell><cell>browser</cell><cell>stencil-</cell><cell>-</cell></row><row><cell>Applica-</cell><cell>Decoder</cell><cell></cell><cell></cell><cell>anima-</cell><cell>probe</cell><cell></cell></row><row><cell>tions</cell><cell></cell><cell></cell><cell></cell><cell>tion</cell><cell></cell><cell></cell></row><row><cell>?bench-</cell><cell>-</cell><cell>ccbench</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>marks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9.">ACKNOWLEDGEMENTS</head><p>We would especially like to thank everyone at <rs type="institution">Intel</rs> who made it possible for us to use the cache-partitioning machine in this paper, including <rs type="person">Opher Kahn</rs>, <rs type="person">Andrew Herdrich</rs>, <rs type="person">Ravi Iyer</rs>, <rs type="person">Gans Srinivasa</rs>, <rs type="person">Mark Rowland</rs>, <rs type="person">Ian Steiner</rs> and <rs type="person">Henry Gabb</rs>. We would also like to <rs type="person">Scott Beamer</rs>, <rs type="person">Chris Celio</rs>, <rs type="person">Shoaib Kamil</rs>, <rs type="person">Leo Meyerovich</rs>, and <rs type="person">David Sheffield</rs> for allowing us to study their applications. Additionally, we would like to thank our colleagues in the <rs type="institution">Par Lab</rs> for their continual advice, support, and, feedback. Research supported by <rs type="funder">Microsoft</rs> (<rs type="grantNumber">Award 024263</rs>) and <rs type="funder">Intel</rs> (<rs type="grantNumber">Award 024894</rs>) funding and by matching funding by <rs type="funder">U.C. Discovery</rs> (Award <rs type="grantNumber">DIG07-10227</rs>). Additional support comes from <rs type="funder">Par Lab affiliates Nokia, NVIDIA, Oracle</rs>, and <rs type="funder">Samsung</rs>. <rs type="person">M. Moreto</rs> was supported by the <rs type="funder">Spanish Ministry of Science</rs> under contract <rs type="grantNumber">TIN2012-34557</rs>, a <rs type="grantName">MEC/Fulbright Fellowship</rs>, and by an <rs type="grantName">AGAUR award</rs> (<rs type="grantNumber">BE-DGR 2010</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AgEZwRw">
					<idno type="grant-number">Award 024263</idno>
				</org>
				<org type="funding" xml:id="_TAsnR7x">
					<idno type="grant-number">Award 024894</idno>
				</org>
				<org type="funding" xml:id="_GWC3ktr">
					<idno type="grant-number">DIG07-10227</idno>
				</org>
				<org type="funding" xml:id="_2quJxGK">
					<idno type="grant-number">TIN2012-34557</idno>
					<orgName type="grant-name">MEC/Fulbright Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_wpkNVhC">
					<idno type="grant-number">BE-DGR 2010</idno>
					<orgName type="grant-name">AGAUR award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>application starts or changes phase, the framework gives the application as much cache as possible (11 ways on our machine). The framework then gradually reduces the application's LLC allocation until negative performance effect is observed (MPKI goes up). The background application(s) are given the remaining LLC resources. On a reallocation, the data is not flushed since the partitioning mechanism only affects the replacement algorithm, which limits the performance overhead of reallocating. Algorithm 6.2 shows the pseudocode for our reallocation algorithm.</p><p>This algorithm uses several hysteresis effects in order to preserve the foreground application's performance. When dealing with applications with rapidly fluctuating MPKI rate, the framework will assign a LLC allocation that might be unnecessarily large. Additionally, data remaining in deallocated ways can hide the performance effects of the reallocation, allowing too much shrinkage. However, as soon as another application evicts the leftover data, a phase change will be detected and reallocation will return the foreground application to a suitable capacity.</p><p>A sensitivity study to set the MPKI derivative thresholds for phase detection and allocation size found selected parameters: M P KI T HR1 = 0.02, M P KI T HR2 = 0.02, and M P KI T HR3 = 0.05. We've found the results largely insensitive to small parameter changes.</p><p>The algorithm is applicable in cases where there are multiple background applications, as long as they can all be treated as peers. By pinning the background peers to certain cores, their accesses to the cache all become routed to the same partition, within which they contend for capacity. This extension does not affect the performance monitoring and cache reallocation performed on behalf of the foreground application. Supporting multiple latency-sensitive applications would require a more complex algorithm, as it is entirely possible for them to oversubscribe the cache, and in this case some component of the system would have to judge their relative utility <ref type="bibr" target="#b3">[5]</ref>. We do believe that the performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rel. Background Throughput Shared Dynamic</head><p>Figure <ref type="figure">13</ref>: Summary of background rate improvement compared to the best static cache allocation for the foreground application. Our dynamic cache-partitioning approach improves background throughput an average of 19%. Sharing the cache improves throughput an average of 53%, but provides no performance isolation.</p><p>monitoring aspect of our algorithm would be important to making such a judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Efficacy</head><p>Across our benchmarks, we find that the framework is able to achieve foreground performance within 2% of the best static allocation. Figure <ref type="figure">13</ref> shows the improved background throughput as a result of the dynamic adaptation. In some cases we see significant throughput increases (up to 2.5x), resulting in a 19% throughput on average across all the pairs. For many cases, the limited number of phases in the foreground application or its high sensitivity to LLC allocation size does not allow for additional improvements over the best static policy. However, even in those cases the dynamic partitioning provides value, because it is a realizable way to achieve the performance of the optimal static allocation without application profiling. For comparison, using a shared LLC without partitioning results in a 53% improvement over the best static allocation. However, as shown before, this scenario can often result in significant performance loss (up to 35%) for the foreground application. As we saw in Section 4, performance improvements translate directly to energy improvements in our system, so we do not show the energy results.</p><p>While the dynamic partitioning mechanism does not always select the best cache capacity allocation for any one phase, overall it is still able to mitigate foreground degradation and increase background throughput. A suboptimal selection during some phases has minimal impact on foreground application latency, and overall the capacity allocated to the foreground application is smaller than when it is given its optimal static allocation for the entire runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Several authors have evaluated way-partitioned caches with multiprogrammed workloads <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref> including</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Searching for a parent instead of fighting over children: A fast breadth-first search implementation for graph500</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno>UCB/EECS-2011-117</idno>
		<imprint>
			<date type="published" when="2011-11">Nov 2011</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<title level="m">Benchmarking Modern Multiprocessors</title>
		<imprint>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PACORA: Dynamically Optimizing Resource Allocations for Interactive Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-04">April 2013</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The DaCapo benchmarks: Java benchmarking development and analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="169" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predictable Performance in SMT Processors: Synergy between the OS and SMTs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M W</forename><surname>Knijnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="785" to="799" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting inter-thread cache contention on a chip multi-processor architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="340" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Managing distributed, shared l2 caches through os-level page allocation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="455" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Opportunities and challenges of parallelizing speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oei</surname></persName>
		</author>
		<editor>HotPar</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perfmon2: a flexible performance monitoring interface for linux</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eranian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ottawa Linux Symposium</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="269" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Looking back and looking forward: power, performance, and upheaval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="105" to="114" />
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Managing contention for shared resources on multicore processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuravlev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">O</forename><surname>Ko?berber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessing the scalability of garbage collectors on many cores</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gidra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sopena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLOS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A framework for providing quality of service in chip multi-processors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computer Architecture -A Quantitative Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel 64 and ia-32 architectures optimization reference manual</title>
		<author>
			<persName><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Intel 64 and ia-32 architectures software developer&apos;s manual</title>
		<author>
			<persName><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">QoS policies and architecture for cache/memory in CMP platforms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Memory characterization of workloads using instrumentation-driven simulation -a pin-based memory characterization of the spec cpu2000 and spec cpu2006 benchmark suites</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>VSSAD, Intel Corporation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Stencil probe</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<ptr target="http://www.cs.berkeley.edu/~skamil/projects/stencilprobe/" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Globally-synchronized frames for guaranteed quality-of-service in on-chip networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2008-02">feb. 2008</date>
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel schedule synthesis for attribute grammars</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Meyerovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Torok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FlexDCP: a QoS framework for CMP architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="86" to="96" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Perfmon2 webpage. perfmon2.sourceforge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis of redundancy and application balance in the spec cpu2006 benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="412" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="423" to="432" />
			<pubPlace>MICRO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vantage: Scalable and Efficient Fine-Grain Cache Partitioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA)</title>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The user and business impact of server delays, additional bytes, and http chunking in web search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brutlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation. SPEC CPU 2006 benchmark suite</title>
		<ptr target="http://www.spec.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A New Memory Monitoring Scheme for Memory-Aware Scheduling and Partitioning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Managing shared l2 caches on multicore systems in software</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WIOSCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rapidmrc: approximating l2 miss rate curves on commodity systems for online optimizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The impact of memory subsystem resource sharing on datacenter applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Characterization and dynamic mitigation of intra-application cache interference</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ISPASS</title>
		<imprint>
			<biblScope unit="page" from="2" to="11" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable shared-cache management by containing thrashing workloads</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HiPEAC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="262" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Does cache sharing on modern CMP matter to the performance of contemporary multithreaded programs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
