<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High Frequency Performance Monitoring via Architectural Event Measurement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chutitep</forename><surname>Woralert</surname></persName>
							<email>woralec@clarkson.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Clarkson University</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Bruska</surname></persName>
							<email>james.bruska@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Independent Researcher</orgName>
								<address>
									<settlement>Syracuse</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
							<email>cliu@clarkson.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Clarkson University</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lok</forename><surname>Yan</surname></persName>
							<email>lok.yan@us.af.mil</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Air Force Research Lab</orgName>
								<address>
									<settlement>Rome</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High Frequency Performance Monitoring via Architectural Event Measurement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/IISWC50251.2020.00020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hardware Performance Counters</term>
					<term>Kernel Module</term>
					<term>Performance Monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Obtaining detailed software execution information via hardware performance counters is a powerful analysis technique. The performance counters provide an effective method to monitor program behaviors; hence performance bottlenecks due to hardware architecture or software design and implementation can be identified, isolated and improved on. The granularity and overhead of the monitoring mechanism, however, are paramount to proper analysis. Many prior designs have been able to provide performance counter monitoring with inherited drawbacks such as intrusive code changes, a slow timer system, or the need for a kernel patch. In this paper, we present K-LEB (Kernel -Lineage of Event Behavior), a new monitoring mechanism that can produce precise, non-intrusive, low overhead, periodic performance counter data using a kernel module based design. Our proposed approach has been evaluated on three different case studies to demonstrate its effectiveness, correctness and efficiency. By moving the responsibility of timing to kernel space, K-LEB can gather periodic data at a 100μs rate, which is 100 times faster than other comparable performance counter monitoring approaches. At the same time, it reduces the monitoring overhead by at least 58.8%, and the difference between the recorded performance counter readings and those of other tools are less than 0.3%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There is significant demand to improve various performance metrics such as speed and energy efficiency within modern computing systems. Associated with that, effective performance evaluation has been a cornerstone of program and architecture optimization for decades. Instrumentation tools such as PIN <ref type="bibr" target="#b0">[1]</ref>, Intel VTune <ref type="bibr" target="#b1">[2]</ref>, and OProfile <ref type="bibr" target="#b2">[3]</ref> greatly help programmers identify performance bottlenecks, thus improving the execution efficiency of their code. In order to locate the performance bottlenecks, these tools can profile both high-level constructs such as threads and locks as well as low-level details such as instructions executed and retired. The finer grained performance details are commonly gathered using hardware performance counters that are built into modern processors.</p><p>High-level analysis, while powerful, often requires the software being analyzed to either be changed (e.g., recompiled) or instrumented (e.g., binary translation). For example, the Callgrind Profiler <ref type="bibr" target="#b3">[4]</ref> provides accurate control flow behavior; however, Callgrind is built on top of Valgrind <ref type="bibr" target="#b4">[5]</ref>, which uses dynamic binary instrumentation (DBI) to monitor programs. Programs are first converted from their native instruction set (e.g., x86) into the VEX Intermediate Representation (IR), then instrumentation instructions can be inserted into the VEX IR, and finally the instrumented VEX is recompiled back down to the native instruction set for execution. This entire process can produce significant overhead, which makes online analysis with software-based profiling for fine-grained events sub-optimal. On the other hand, performance counters collect data via dedicated circuitry in the processor itself, allowing for collection with nearly negligible overhead.</p><p>Many tools have been developed to provide a high-level API to control the low-level performance counters. They are represented by Perf <ref type="bibr" target="#b5">[6]</ref>, PAPI <ref type="bibr" target="#b6">[7]</ref>, and LiMiT <ref type="bibr" target="#b7">[8]</ref>. Each of these tools use different mechanisms to interact with the monitored programs, control the dedicated counter registers, and provide the collected data back to the user. The tools listed above, however, have inherited drawbacks, such as intrusive code changes, a slow timer system, or the need for a kernel patch. More importantly, some of them add drastic overhead during profiling by using expensive kernel calls to control and read the performance counters. For example, prior research has indicated that popular programs, such as Firefox, could not even run properly when profiled with PAPI-C due to its overhead <ref type="bibr" target="#b7">[8]</ref>.</p><p>This overhead issue becomes even more critical when trying to perform online decision making based on the performance counter data. Recent research has started to explore the use of performance counters for various online decision making applications, which can be seen in malware detection by Demme et al. <ref type="bibr" target="#b8">[9]</ref>, online program verification by Bruska et al. <ref type="bibr" target="#b9">[10]</ref>, scheduling techniques by Torres et al. <ref type="bibr" target="#b10">[11]</ref>, and dynamic power estimation by Liu et al. <ref type="bibr" target="#b11">[12]</ref>. Across all these scenarios, the overhead and the collection rate can dictate whether the proposed implementation is feasible or not.</p><p>Aside from overhead, intrusiveness (e.g., requiring source code changes or not) is another consideration. Some profiling tools such as PAPI and LiMiT require modifying the source code as a prerequisite. This can be useful because it integrates program control flow information with the hardware event information. However, this might not always be possible since there are certain times when the source code is not available. For example, it could be legacy code where the source code is nowhere to be found; another scenario is when third-party intellectual property is provided in binary format only. Removing the source requirement also opens up new use cases such as for operating systems to monitor a process's dynamic behavior as part of just-in-time (JIT) optimizations. As outlined above, DBI can be used to instrument binaries without source, but with significant overheads.</p><p>In this work, we introduce K-LEB (Kernel -Lineage of Event Behavior), a performance counter based profiling tool that utilizes a kernel space collection system to produce precise, nonintrusive, low overhead, high periodicity performance counter data. The advantages of our proposed scheme are:</p><p>-access to the source code is not needed for high periodicity profiling; -the profiling frequency is much faster than other periodic performance counter monitoring tools; -user programs can be profiled on an already running kernel as K-LEB uses a kernel module; -the overhead produced is significantly less than other available performance counter monitoring tools. This was determined by evaluating K-LEB with three case studies to demonstrate its effectiveness, correctness and efficiency.</p><p>The rest of this paper is organized as follows. In Section II, we describe the performance counters and the evolution of their usage. Section III describes the methodology and techniques used in K-LEB . The case studies and overhead evaluation are presented in Sections IV and V. We discuss the hardware limitations of performance counters and the potential difficulties of performance counter approach to online decision making in Section VI. The conclusion and closing thoughts are given in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, we give an overview of the performance monitoring unit and performance counters, the overhead of different monitoring tools, and the timing constraints associated with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>The performance monitoring unit (PMU) is a special piece of on-chip hardware that can monitor architectural and microarchitectural hardware events. The PMU normally consists of two main components: configuration registers and counter registers. Configuration registers are used to control the collection system. This includes which hardware events are being collected, start and stop commands, privilege level, and more. Counter registers, on the other hand, are used to count the number of occurrences for each of the specified hardware events based on the configuration registers.</p><p>The type and number of hardware events available to be monitored vary based on the processor type. For example, modern Intel processors can collect from more than three hundred unique hardware events <ref type="bibr" target="#b12">[13]</ref>. On the other hand, the ARM Cortex-A9 processor PMU can count any of the 58 events available <ref type="bibr" target="#b13">[14]</ref>. It is not possible to monitor all of these events at the same time though. There are six counter registers available in Cortex-A9. For the modern Intel microarchitecture family, such as Nehalem, Skylake, and Cascade Lake, there are seven counter registers available, three of which are fixed performance counters that only monitor predefined events: instructions retired, unhalted core clock cycles, and unhalted clock cycles at the time stamp clock rate. The other four counters are user configurable or programmable, such as the number of load/store instructions, cache misses, branch mispredictions, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overhead Constraints</head><p>Nowadays, there are several open-source software tools that can be used to collect performance counter samples. One of the most popular tools is Perf <ref type="bibr" target="#b5">[6]</ref>. The Perf Linux tool is built on the perf events interface in the Linux kernel. Perf provides a simple command-line interface that allows the user to collect performance counter samples. Perf has two collection modes: perf stat and perf record. The perf stat mode gathers overall statistical hardware event counts for the program in counting mode. The perf record mode gathers the hardware event counts in sampling mode, which then generates a file that can be analyzed by using perf report. Perf does provide the ability to count more events than available counter registers, as it virtualizes these registers and uses time multiplexing to monitor multiple hardware events with the same register. But this comes with the cost of decreased accuracy and increased overhead.</p><p>Another popular tool is PAPI <ref type="bibr" target="#b6">[7]</ref>. It provides an API that allows the user to attain fine-grained control of performance counter sample collection by using library functions to dictate event monitoring time frames withing the source code of the monitored program. This extra control is incredibly beneficial, but also comes at a cost; it uses expensive system calls to enable and read the performance counters. It also requires the availability of the source code in order to monitor the program, which is not always possible to attain for legacy or closedsource programs.</p><p>This was improved upon when LiMiT was introduced by Demme et al. <ref type="bibr" target="#b7">[8]</ref>. They addressed the expensive system call problem. LiMiT is a kernel patch that allows the user to access the performance counters directly from user-land without having to invoke a system call. This avoids the overhead of having to context switch into the kernel every time the user wants to collect performance counters. This still comes at a cost though. It requires the user to patch the kernel, which might not be convenient or possible. It also doesn't allow the monitoring of software on systems that are already running stably and cannot be restarted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Timing Constraints</head><p>One key feature of the profiling tools, besides the overhead, is the rate at which profiling can occur while maintaining proper program functionality. This is even more critical in timer-based profiling tools. Tools such as Perf can only operate with timer speeds of 10ms or slower <ref type="bibr" target="#b5">[6]</ref>. This is due to the limitation of the Linux user space timing systems. Unfortunately, this is not fast enough for certain applications.</p><p>This limitation can be seen in prior scheduling research, such as the work by Torres et al. <ref type="bibr" target="#b14">[15]</ref>. In their research, they used performance counter data to control the affinity of eight virtual machine processes in order to minimize contention for processor resources. They chose to use longer collection periods in an attempt to reduce overhead. But if the overhead problem were to be mitigated then the sampling frequency would become of utmost importance. In addition to this, they suggested to move the performance measurements into the operating system in order to enhance the scheduler predictions. Thus, a movement to high frequency sampling would be required due to the Linux scheduler operating on 1-4ms process evaluation frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this work we propose a new collection mechanism that is designed to address the limitations of prior performance counter collection tools. The K-LEB (Kernel -Lineage of Event Behavior) tool is a kernel module with the ability to collect performance counter samples in high frequency time series. Figure <ref type="figure" target="#fig_0">1</ref> illustrates an overview of the K-LEB system architecture. The figure shows the two major K-LEB components. First is the K-LEB kernel module which takes care of performance counter collection by communicating with the PMU. The module will isolate the performance counter data of the "Monitored Process" from other processes by only enabling the collection during the period that the "Monitored process" is actively running. Second is the user space "Controller Process" that is used to configure and communicate with the K-LEB kernel module. The configuration includes select the "Monitored Process" as the target process to collect performance counter measurements on. Other configuration options include the hardware events to be monitored as well as the recording frequency. The user can issue the command to the kernel module through the "Controller Process" to start/stop collecting the hardware events.</p><p>The first important characteristic of K-LEB is of nonintrusiveness. This means the host kernel does not need to be patched, nor does the program of monitored process. Neither source is required. As a kernel module, K-LEB uses existing kernel APIs to perform monitoring. It can be used on an already running or stable system. K-LEB solves the source code limitation by using a periodic collection design. In this way, the user no longer needs to carefully place performance counter control API calls (e.g., start, read, and stop) into the source or instrument the binary. K-LEB collects the counts at regular intervals.</p><p>In periodic recording, granularity or recording frequency becomes more significant. Perf is only able to sample every 10ms due to the limitations of user space timers. Certain programs would finish running within 10ms. By moving the responsibility of timing to kernel space and using a high resolution timer (HRTimer), our monitoring system can gather periodic data at a 100μs rate. By design, K-LEB uses process identifiers (PIDs) to identify the program to monitor and a timer frequency to determine how often to record the hardware events.</p><p>The process flow in Figure <ref type="figure" target="#fig_1">2</ref> shows how each of these concepts comes together to form the K-LEB system. First, the initial PID, selected hardware events, and the timer speed are all passed in from user space using an ioctl system call. This initializes the resources and parameters for the kernel module, marked as <ref type="bibr" target="#b0">(1)</ref>.</p><p>The PID is then tracked for the life of the process. In order to attain process isolation while collecting events, the module stops counting when the process is scheduled out. Figure <ref type="figure" target="#fig_2">3</ref> shows such interactions between controller process and monitored process. It does this by attaching kernel probes to the context switch handler of the scheduler. Since a single application can have multiple PIDs, we also collect and use information such as process name, process id, parent process id, and process states to effectively trace the process, and its children. While the monitored process is actively running, marked as (2), the module will enable performance counter collection and start the HRTimer. The HRTimer will periodically trigger a hardware interrupt that collects the performance counter data and stores them in an assigned buffer in kernel memory. In theory, the HRTimer for modern systems can be precise up to a nanosecond level; in practice, there are several factors that can affect the timer's precision such as clock jitter, context switches, and data processing. Due to these factors, we do not recommend running data collection period faster than 100μs on the HRTimer. Although a faster collection rate can be used, it can introduce noises to the data. When the monitored process gets scheduled out by the scheduler, marked as (3), the module will disable the performance counter collection and stop the HRTimer to isolate the performance counter data of monitored process from other processes.</p><p>Since kernel developers highly recommend against directly accessing files in kernel space, hardware event counts are logged to the file system by the controller process in user space. This means that the controller process must make expensive system calls to tap into the kernel and extract the data samples. To limit the number of system calls, K-LEB uses a temporary buffer in kernel memory to pool the samples until the controller process is naturally scheduled to run and make the periodic read system calls to extract the data. In an effort to prevent the rare case where the controller process is starved and cannot extract the kernel data samples fast enough, we implemented a safety mechanism to temporarily stop the module from collecting more data until sufficient space in the buffer is freed up. When the controller process finally extracts the data and clears the buffer, K-LEB will continue collecting the data automatically.</p><p>After the process exits or the user issues the stop monitoring command through the controller process, marked as (4), the ioctl system call will be used to stop monitoring the process and release resources in the kernel module. All the data samples in the kernel module will then be sent to user space for further processing and analysis, marked as <ref type="bibr" target="#b4">(5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ANALYSIS / CASE STUDIES</head><p>In this section we used K-LEB in three different case studies, i.e., LINPACK, Docker containers and Meltdown. These case studies are used to verify that K-LEB 1) can perform as expected on a traditional highperformance workload where no source code is available, 2) can be used to profile Docker containers without requiring any changes or instrumentation, 3) can use the high sampling rate (100μs) to perform online/live analysis. Their corresponding results are presented in the respective order. The experiments were performed locally on an Intel Core i7-920 @ 2.67GHz processor with 6GB DDR3 memory @ 1066 MHz running Ubuntu 16.04 with Linux kernel version 4.13.0-15. The results were verified on Amazon Web Services using Intel Xeon Platinum 8259CL @ 2.50GHz processor running Ubuntu 16.04 with Linux kernel version 4.4.0-1112aws. There was less than 1% difference in the counts, therefore we only present the local results in this section. The LiMiT patch is running on Ubuntu 12.04 with Linux kernel version 2.6.32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LINPACK Benchmark</head><p>In the first experiment, we used K-LEB to collect the performance counter samples for LINPACK <ref type="bibr" target="#b15">[16]</ref>. The core of the LINPACK benchmark is to solve a dense system of linear equations. By measuring the time to factor and solve the system and convert that time into a performance rate, LINPACK is commonly used to evaluate high performance computing machines and supercomputers, for example, those on Top500.org. In order to demonstrate the non-intrusive nature of K-LEB , we directly downloaded the binary executable of LINPACK from the Intel MKL Benchmarks Suite <ref type="bibr" target="#b16">[17]</ref> and captured the performance counter samples.</p><p>For this experiment K-LEB was set up to collect the performance counter data from LINPACK periodically on a 10ms timer to accommodate the relatively long running time. Since LINPACK solves dense systems of linear equations, we set up K-LEB to monitor the hardware events of arithmetic multiplication, load, and store. The experiment consisted of 10 trials with a problem size of 5000 to calculate the average performance in FLOPS (FLoating-point Operations Per Second). Figure <ref type="figure">4</ref> shows the performance counter trends for LINPACK, averaged over the 10 trial runs. From the performance counter samples collected, we can clearly see how K-LEB is able to capture the phase behavior of LINPACK as a software program. During the initialization process, LINPACK works in kernel level to extract configuration parameters, so we don't see an increase in the hardware counts on the first few samples. Then, we see a sharp increase in LOAD and STORE event counts during the benchmark parameters set up. This also explains why there is only a small number of ARTIH MUL (arithmetic multiplications) during the same period. Starting at about 200 samples, the real computation begins. For the major part of the program which is the linear equations solving period, we can see a clear phase transition from loading data and computation, followed by a storing phase afterwards. The pattern repeats for the rest of the program, which matches the characteristic of the LINPACK benchmark. This demonstrates K-LEB 's validity and effectiveness.</p><p>In Table <ref type="table" target="#tab_0">I</ref> we compared FLOPS numbers among different profiling tools, all using the same 10ms sample rate, to derive the performance loss. It shows that when running LINPACK without profiling, the raw performance is 37.24 GFLOPS. When we use K-LEB to collect performance counter samples, we achieved a very small performance loss of 0.64%, compared with the 7.08% of perf stat and 0.96% of perf record. Note how K-LEB 's performance loss is comparable to that of perf record, which uses a sampling method to estimate the hardware count. K-LEB is able to generate data samples in time series to provide precise (instead of estimate) counts. This is comparable to perf stat but with significantly less performance loss. This experiment shows that K-LEB can generate time series samples with non-intrusiveness where no source code is available, while having minimal impact on program performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Docker Containers</head><p>Docker <ref type="bibr" target="#b17">[18]</ref> is a popular virtualization tool that provides OS-level virtualization for software. Docker provides isolated working environments by packing all the necessary code and dependencies into a single Docker image. This allows the software to easily migrate from one working environment to another, regardless of the infrastructure. A Docker image can be deployed and run in separate environments called containers on top of the Docker engine.</p><p>In this case study we demonstrate how K-LEB can capture performance counter samples from the running Docker containers natively without interrupting the workloads. The counts are then used to categorize the docker images into memory-intensive and computation-intensive workloads. We downloaded the most popular Docker images from the official Docker Hub <ref type="bibr" target="#b18">[19]</ref> and then employed K-LEB to collect the hardware event counts for each Docker image program. The hardware events collected are the number of LLC (Last-Level Cache) misses and the number of instructions retired so we can calculate the Misses per Kilo Instructions (MPKI). The results are presented in Figure <ref type="figure" target="#fig_3">5</ref>. Muralidhara et al. <ref type="bibr" target="#b19">[20]</ref> classified programs with MPKI higher than 10 as high memory-intensity programs and the programs with MPKI lower than 10 as computation-intensive programs.</p><p>Our data shows that the interpreter Docker images, such as Ruby, Golang, Python, have very low MPKI, less than 1 on average. They can be categorized as computation-intensive Docker images. Other Docker images such as Mysql, Traefik, and Ghost, despite having higher MPKI than the interpreter Docker images, are still categorized as computation-intensive programs due to having an MPKI of less than 10 on average. On the contrary, the web-server Docker images such as Apache, Nginx, and Tomcat have an MPKI much higher than 10 and thus are categorized as memory-intensive Docker images.</p><p>We did another round experiment, running Docker containers on the newer hardware platform on Amazon Web Services. What we found is that even though the absolute values of cache misses collected by K-Leb vary due to different hardware configuration and the cache structure of the processor, the dockers programs still follow the same trend in terms of their LLC MPKI from low to high, which is consistent across different processors.</p><p>Torres et al. and Arteaga et al. have demonstrated how such classification integrated with scheduling techniques can help improve the throughput of the workloads in their case studies <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>. These classifications can be valuable to cloud service providers as the performance counter data can be used to assist the scheduler to balance their workloads across processor cores based on the type of resources required. The scheduler can colocate computation-intensive programs or containers with the memory-intensive ones on the same core, while scheduling the programs that require the same type of resources on different cores. K-LEB is an enabling factor to perform all these tasks in an online fashion.</p><p>This experiment demonstrates that K-LEB can be used for online workload characterization via gatherings performance counter data for programs running in Docker, despite the fact that it is only provided with a binary container. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Meltdown Attack</head><p>Side channel attacks are an emerging threat that are notoriously difficult to detect with program profiling and control flow verification, because the attacks don't necessarily interfere with the program execution itself. Meltdown <ref type="bibr" target="#b21">[22]</ref> is a classic example. Meltdown operates as a cache side-channel attack by allowing the attacker to infer/steal the values of normally inaccessible memory locations (e.g., ones that belong to the kernel or other processes) from user space. This occurs because the actual data values of privileged operations (kernel operations that should be hidden) are reflected in which LLC lines are evicted during speculative or out-of-order (OoO) execution.</p><p>This speculative execution feature was designed to allow modern processors to move ahead with the instruction execution while waiting for data to arrive to the CPU from memory. This reduces stalling and waiting times. As the CPU looks ahead and executes instructions out of order, there is a likely chance that memory access instructions in the instruction queue (reorder buffer) have already been executed out of order and the state of the LLC has changed. In the case where OoO instructions throw an access exception, the OoO unit will clear the instruction pipeline and deem the temporary state as 'discarded'. However, the cache state is not reverted and remains changed. This is where Meltdown comes in and pulls the data from the cache by determining which cache line has changed. This in turn provides insights into the memory location's data value.</p><p>This exploit alone impacted many of the widely used Intel processors between the late 1990s and early 2018. It left millions of computers vulnerable. In this study, we will show the benefits of using K-LEB to monitor a program that is being attacked with the Meltdown exploit.</p><p>Since the Meltdown attack mostly revolves around cache exploitation, we set up K-LEB to monitor the cache events such as LLC references and LLC misses. For the experiment, we used the Meltdown example from <ref type="bibr" target="#b22">[23]</ref>. We ran the sample secret string printing program without Meltdown and collected its performance counter samples, then we ran the same program but using Meltdown to extract and print out the secret string value and monitored its hardware event counts again.</p><p>The results are compared and summarized in Figure <ref type="figure" target="#fig_4">6</ref>. The hardware event counts presented are the average over 100 rounds for each program. We can see that the program with the Meltdown attack attached to it has significantly higher counts for LLC references and LLC misses than the program without Meltdown. The Meltdown attack added more execution time to the program and resulted in many more samples being collected as shown in Figure <ref type="figure" target="#fig_5">7</ref>.</p><p>We used a very small timer interval of 100μs to provide time series data for the program without Meltdown. The program has a short execution time, less than 10ms. This is important because a profiling tool like perf stat cannot generate multiple data points due to the timer being triggered every 10ms. Intuitively, the extra cache operations are coming from the Flush+Reload side-channel attack since it has to rapidly perform a cache flush and reload to determine the memory location of the data. This results in abnormally high LLC misses and LLC references ratio during the point of attack in Figure <ref type="figure" target="#fig_5">7</ref>. We also calculated the MPKI for both Meltdown and Non-Meltdown programs from the performance counter data collected. On average a program without Meltdown has 7.52 MPKI, while the program with Meltdown saw a sharp increase to 27.53 MPKI on average. This is another demonstration of the increased cache memory access for the programs with Meltdown.</p><p>The number of LLC references and LLC misses can vary based on the configuration and cache structure of the processors. However, we are still able to detect the changes in the LLC misses and LLC references ratio, and MPKI during the point of attack using K-LEB across different processors we used.</p><p>Another experiment we conducted was to show that K-LEB is more advanced than Perf in terms of time granularity. In comparison with the Perf method, it can only provide one performance counter sample for the same duration, which merely indicates whether an attack has happened or not. K-LEB generates more precise time series data as shown in Figure <ref type="figure" target="#fig_5">7</ref>, which allows us to be able to identify the point of attack more clearly and at the early stage of the attack during the program execution. This gives K-LEB the potential to be used for hardware event based anomaly detection system. However, building such a detection system is outside the scope of this work.  V. OVERHEAD STUDIES In this section, we performed run-time overhead testing of K-LEB against other comparable profiling tools, i.e., Perf, PAPI, and LiMiT, by measuring the execution time of a matrix multiplication program provided by <ref type="bibr">Intel [24]</ref> and comparing the run-time across the tools. Matrix multiplication programs offer flexibility to adjust the program execution time while also providing source code which is suitable for those profiling tools that require the source code modification.</p><p>Table <ref type="table" target="#tab_0">II</ref> shows the percentage overhead comparison across performance counter collection tools. The numbers presented here are the average over 100 runs of a program using triple nested loop to perform matrix multiplication for each profiling tool. We compared the tools that support timer-based collection, i.e., K-LEB and Perf, by running them at the same sample rate. Since neither PAPI nor LiMiT support timer-based collection, for these tools we modified the source code to collect performance counter samples at multiple strategic points in the program so that the numbers of data samples obtained are approximately the same as those of the timer-based tools. With a comparable number of collected samples, PAPI has the highest overhead of 6.43%. LiMiT has a significantly less overhead of 4.08%. This is due to PAPI having to constantly invoke system calls to acquire the data samples whereas LiMiT acquires the samples directly from user space without having to invoke system calls.</p><p>From the table, we can see that K-LEB has a very low overhead of 0.68% at the 10ms sample rate, in comparison with 6.01% for perf stat. K-LEB shows 58.8% decrease in performance overhead when comparing to the next best tool, i.e., perf record, in term of performance overhead. K-LEB mitigates the overhead effectively by collecting and keeping the performance counter data in kernel space. This reduced the expensive overhead from extra system calls. It also allows the scheduler to determine the best time to execute the controller program and gather the performance counter data from kernel space to user space for logging.</p><p>Figure <ref type="figure" target="#fig_6">8</ref> shows the box and whisker plot of the normalized execution time of the test program for each collection tool. K-LEB has a smaller spread in execution time in comparison to the other collection tools. This indicates that K-LEB has the least interference on the running process and the least overhead, demonstrating that K-LEB is more consistent than the other tools.</p><p>We also performed overhead testing on matrix multiplication using Intel MKL dgemm routine which has a significantly shorter execution time: less than 100ms in comparison to the 2s required by the traditional triple nested loop approach mentioned above. Intel MKL [24] is a math library that provides mathematical routines optimized for Intel and supported processors architectures. Intel MKL can be found as a part of Intel Parallel Studio XE or Intel System Studio. It can also be found as stand alone package in Intel Performance Libraries <ref type="bibr" target="#b23">[25]</ref>. In Table <ref type="table" target="#tab_0">III</ref>, the numbers presented are the average result of 100 runs of the matrix multiplication program using the Intel MKL dgemm routine to perform matrix multiplication for each profiling tool. Due to an unsupported OS and kernel version for LiMiT, no data from LiMiT is presented here for Intel MKL. From Table <ref type="table" target="#tab_0">III</ref>, even though K-LEB saw an increase  in overhead from 0.68% of triple nested loop to 1.13% of MKL dgemm at 10ms sample rate, K-LEB still has the lowest overhead in comparison with other profiling tools, i.e., 7.64% for perf stat and 2.00% for perf record. We can see that the tools that require expensive system calls such as PAPI still have the highest overhead of 21.40% when collecting comparable number of samples. With the short execution time of Intel MKL, we can see how expensive the system calls are with regards to overhead.</p><p>Figure <ref type="figure">9</ref> depicts the percentage difference in hardware event count of K-LEB in comparison with other HPC collection tools. We mainly chose architectural events for comparison, such as the number of load and store instructions, as these hardware events are more stable and deterministic than microarchitectural events, which have dependency on the context and the state of the hardware <ref type="bibr" target="#b24">[26]</ref>. Due to this characteristic of the architectural events, these hardware event count is consistent across different processors we used in the experiments.</p><p>K-LEB shows less than 0.0008% difference in hardware event count on deterministic events, such as Branch, Load, Store, and Instruction retired, when compared to perf stat. While perf record uses sampling method to collect its hardware event counts, it still has close approximate hardware event count</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. K-LEB Architecture.</figDesc><graphic url="image-1.png" coords="3,325.53,71.90,216.26,142.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. K-LEB Process Flow.</figDesc><graphic url="image-2.png" coords="3,313.46,422.26,240.40,157.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. K-LEB Process Interaction.</figDesc><graphic url="image-3.png" coords="4,55.15,72.00,240.29,95.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. LLC misses per instruction for workloads running on Docker</figDesc><graphic url="image-5.png" coords="5,313.53,527.99,240.40,150.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Meltdown comparison.</figDesc><graphic url="image-6.png" coords="6,337.63,359.69,192.35,115.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Meltdown vs Non-Meltdown via K-LEB</figDesc><graphic url="image-7.png" coords="6,337.63,513.55,192.35,115.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Performance overhead comparison across performance counter collection tools.</figDesc><graphic url="image-9.png" coords="7,325.34,264.45,216.49,130.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. event</head><label></label><figDesc>Fig. event count of K-LEB in comparison to other performance counter collection tools.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I LINPACK</head><label>I</label><figDesc>GFLOPS RESULTS ACROSS DIFFERENT PROFILING TOOLS</figDesc><table><row><cell>Profiling Tools</cell><cell cols="4">No profiling K-Leb Perf stat Perf record</cell></row><row><cell>GFlops</cell><cell>37.24</cell><cell>37.00</cell><cell>34.78</cell><cell>36.89</cell></row><row><cell>Performance Loss (%)</cell><cell>0</cell><cell>0.64</cell><cell>7.08</cell><cell>0.96</cell></row></table><note>Fig. 4. LINPACK behavior in hardware performance counter samples using K-LEB</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:40:57 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:40:57 UTC from IEEE Xplore. Restrictions apply. with less than 0.15% difference on deterministic events when compared to K-LEB . And when comparing different hardware events across all collection tools, K-LEB still shows less than 0.3% difference in hardware event counts with any of the other tools. This demonstrates K-LEB achieves accuracy while still maintaining minimal overhead. K-LEB 's overhead, just like other timer base profiling tools, depends on the sample rate. The finer the granularity, the more samples it will produce. At the same time it introduces more overhead. This can be seen in Tables II and III. In the end, it is up to the users to determine at what level they want to monitor, given the trade-off between overhead and the granularity of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LIMITATIONS</head><p>This new technique for performance measurement of K-LEB has its limitations bounded by hardware. First, the number of hardware events that can be monitored concurrently is limited by the number of counter registers available for each specific processor family. The most recent generations of Intel processors only have four programmable and three fixed counter registers. This is a rather small number of events when trying to identify causes of program behavior. Normally this is solved by using sequential runs for profiling (e.g., one run measures events A, B, C and D while the next measures events W, X, Y and Z); however, this methodology proves difficult when trying to perform online or runtime analysis. Perf solves this by using time multiplexing to estimate the different hardware events. This estimation may not be suitable for measurement systems that require precision though.</p><p>Second, the maximum timing frequency of the collection system is also limited due to the processor hardware. The more interrupts there are, the more times the processor has to save its state in order to collect the performance counter data and reset the timer. This means that the overhead will rapidly increase after 100μs intervals as it interrupts the running process more and more often. Also, the high resolution timer is only able to operate at a frequency of approximately 100μs with a reasonable amount of jitter. This could have a major impact on systems when trying to profile how a program operates. For example, even a 1% jitter could cause the collection mechanism to shift an entire time step off with only 100 iterations. For a system that operates at a frequency of 10,000 samples per second, that jitter would be devastating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work we introduce K-LEB (Kernel-Lineage of Event Behavior), a kernel module based mechanism for performance counter collection. K-LEB 's periodic collection operates at a rate that is 100 times faster than other comparable monitoring tools. K-LEB is non-intrusive as it does not require the source code of the profiled program. This is helpful in situations where only the program binary is available due to legacy code or IP protection. Our experimental results show that K-LEB only introduces 0.68% overhead on average in comparison to 4.08% of LiMiT, 6.01% of Perf, and 6.43% of PAPI at 10ms sample rate when running the test case of a triple nested loop matrix multiplication program, with at most 0.3% difference in actual hardware event counts collected during the experiments when compared with other tools. This new approach allows users to better measure performance and behavioral characteristics of programs. As a result, many other subject areas that benefit from using performance data, such as binary instrumentation, malware detection and scheduling techniques, could advance as well. For those that would like to use K-LEB to monitor their own system, the code can be found at https://github.com/ camel-clarkson/K-LEB. Instructions on how to compile and use the system are provided as well. ACKNOWLEDGMENT Mr. Chutitep Woralert is supported by Nicklas-Ignite Research Fellowship, Clarkson University. The authors want to thank Mr. Caleb DeLaBruere who worked on this project briefly and gave the team the inspiration for the name of our scheme. The authors also want to thank Dr. Gildo Torres for the technical discussion that greatly helped the development work. Any opinions, findings and conclusions in this paper are those of the authors and do not necessarily reflect the views of the US Air Force and/or US Government. Approved for public release: 88ABW-2020-2104.</p><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:40:57 UTC from IEEE Xplore. Restrictions apply.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pin a dynamic binary instrumentation tool</title>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/pin-a-dynamic-binary-instrumentation-tool.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intel vtune profiler</title>
		<ptr target="https://www.intel.com/content/www/us/en/develop/tools/vtune-profiler.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Oprofile</title>
		<ptr target="https://oprofile.sourceforge.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequential performance analysis with callgrind and kcachegrind</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weidendorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools for High Performance Computing</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Resch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Keller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Himmler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Krammer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schulz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="93" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Valgrind: A framework for heavyweight dynamic binary instrumentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nethercote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIG-PLAN Conference on Programming Language Design and Implementation, PLDI &apos;07</title>
				<meeting>the 28th ACM SIG-PLAN Conference on Programming Language Design and Implementation, PLDI &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The new linux&apos;perf&apos;tools</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>De Melo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting performance data with papi-c</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terpstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jagode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools for High Performance Computing</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Resch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schulz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009. 2010</date>
			<biblScope unit="page" from="157" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rapid identification of architectural bottlenecks via precise event counting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual International Symposium on Computer Architecture, ISCA &apos;11</title>
				<meeting>the 38th Annual International Symposium on Computer Architecture, ISCA &apos;11<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the feasibility of online malware detection with performance counters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maycock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waksman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture, ISCA &apos;13</title>
				<meeting>the 40th Annual International Symposium on Computer Architecture, ISCA &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Verification of OpenSSL version via hardware performance counters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Blasingame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Disruptive Technologies in Sensors and Sensor Systems</title>
				<editor>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Blowers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Williams</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10206</biblScope>
			<biblScope unit="page" from="52" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The impact on the performance of co-running virtual machines in a virtualized environment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Adaptive Resource Management and Scheduling for Cloud Computing, ARMS-CC&apos;16</title>
				<meeting>the Third International Workshop on Adaptive Resource Management and Scheduling for Cloud Computing, ARMS-CC&apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dynamic power estimation with hardware performance counters support on multi-core platform</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<editor>Advanced Computer Architecture (J. Wu, H. Chen, and X. Wang</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="177" to="189" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intel 64 and ia-32 architectures software developer manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">tech. rep</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Intel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cortex-A9 Technical Reference Manual, Revision: r2p0</title>
		<ptr target="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0388e/BEHEDIHI.html" />
		<imprint/>
		<respStmt>
			<orgName>ARM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive virtual machine management in the cloud: A performance-counter-driven approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJSSOE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intel® optimized linpack benchmark for linux</title>
	</analytic>
	<monogr>
		<title level="m">tech. rep., Intel Corporation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel® math kernel library (intel® mkl) benchmarks suite</title>
		<imprint>
			<publisher>Intel Corporation</publisher>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Docker documentation</title>
		<ptr target="https://docs.docker.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explore -docker hub</title>
		<ptr target="https://hub.docker.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing memory interference in multicore systems via application-aware memory channel partitioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Muralidhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="374" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative virtual machine scheduling on multi-core multi-threading systemsa feasibility study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thanarungroj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Micro Architectural Support for Virtualization, Data Center Computing, and Cloud</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meltdown: Reading kernel memory from user space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Prescher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mangard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th USENIX Security Symposium (USENIX Security 18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Developer guide for intel® math kernel library 2020 for linux</title>
		<ptr target="https://github.com/IAIK/meltdown" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Intel Corporation</publisher>
		</imprint>
	</monogr>
	<note>Meltdown proof-of-concept. tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Intel® performance libraries</title>
		<ptr target="https://software.seek.intel.com/performance-libraries" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can data-only exploits be detected at runtime using hardware events? a case study of the heartbleed vulnerability</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Hardware and Architectural Support for Security and Privacy</title>
				<meeting>the Hardware and Architectural Support for Security and Privacy<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016. 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
