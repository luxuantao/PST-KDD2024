<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Developing a Local Least-Squares Support Vector Machines-Based Neuro-Fuzzy Model for Nonlinear and Chaotic Time Series Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arash</forename><surname>Miranian</surname></persName>
							<email>ar.miranian@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mechanical Engineering</orgName>
								<orgName type="department" key="dep2">Pardis Branch</orgName>
								<orgName type="institution">Islamic Azad University</orgName>
								<address>
									<settlement>Pardis New City, Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Majid</forename><surname>Abdollahzade</surname></persName>
							<email>m.abdollahzade@pardisiau.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mechanical Engineering</orgName>
								<orgName type="department" key="dep2">Pardis Branch</orgName>
								<orgName type="institution">Islamic Azad University</orgName>
								<address>
									<settlement>Pardis New City, Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Developing a Local Least-Squares Support Vector Machines-Based Neuro-Fuzzy Model for Nonlinear and Chaotic Time Series Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">45EB10433F744AE050DB950CB9D5362C</idno>
					<idno type="DOI">10.1109/TNNLS.2012.2227148</idno>
					<note type="submission">received March 8, 2012; revised October 22, 2012; accepted October 22, 2012. Date of publication December 11, 2012; date of current version January 11, 2012.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hierarchical binary tree (HBT)</term>
					<term>least-squares support vector machines (LSSVMs)</term>
					<term>local neuro-fuzzy (LNF) models</term>
					<term>prediction</term>
					<term>time series</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Local modeling approaches, owing to their ability to model different operating regimes of nonlinear systems and processes by independent local models, seem appealing for modeling, identification, and prediction applications. In this paper, we propose a local neuro-fuzzy (LNF) approach based on the least-squares support vector machines (LSSVMs). The proposed LNF approach employs LSSVMs, which are powerful in modeling and predicting time series, as local models and uses hierarchical binary tree (HBT) learning algorithm for fast and efficient estimation of its parameters. The HBT algorithm heuristically partitions the input space into smaller subdomains by axis-orthogonal splits. In each partitioning, the validity functions automatically form a unity partition and therefore normalization side effects, e.g., reactivation, are prevented. Integration of LSSVMs into the LNF network as local models, along with the HBT learning algorithm, yield a high-performance approach for modeling and prediction of complex nonlinear time series. The proposed approach is applied to modeling and predictions of different nonlinear and chaotic real-world and hand-designed systems and time series. Analysis of the prediction results and comparisons with recent and old studies demonstrate the promising performance of the proposed LNF approach with the HBT learning algorithm for modeling and prediction of nonlinear and chaotic systems and time series.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>L OCAL modeling approaches (LMAs) are very popular since they can deal with complex identification, prediction, and control problems by dividing them into a set of smaller and simpler subproblems. In fact, the main advantage of the LMAs is that the identification and prediction of complex nonlinear processes is alleviated by the integration of structured knowledge about the process <ref type="bibr" target="#b0">[1]</ref>. The ability to describe different operating regimes of a system or process by different local models (LMs) has made LMAs appealing to many researchers to date.</p><p>LMAs have been developed under different names and titles <ref type="bibr" target="#b1">[2]</ref>, such as local model networks <ref type="bibr" target="#b2">[3]</ref>, Takagi-Sugeno (TS) fuzzy models <ref type="bibr" target="#b3">[4]</ref>, piecewise linear models <ref type="bibr" target="#b4">[5]</ref>, and local regression <ref type="bibr" target="#b5">[6]</ref>.</p><p>The LMAs can be distinguished according to the strategy of combining LMs. Soft partitioning and hard switching are two dominant strategies adopted by the LMAs <ref type="bibr" target="#b1">[2]</ref>. The soft partitioning strategy, if it arises from a fuzzy-inference-system formulation, constitutes an appealing class of LMAs, called local neuro-fuzzy (LNF) models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>. As two important and salient properties, the LNF models allow incorporation of arbitrary LM types and can use neural network learning algorithms for estimation of their parameters <ref type="bibr" target="#b1">[2]</ref>.</p><p>Among the many proposed LNF models, the pioneering local linear neuro-fuzzy (LLNF) model and its local linear model tree (LOLIMOT) learning algorithm, developed by Nelles <ref type="bibr" target="#b1">[2]</ref>, are among the most popular approaches for identification and prediction in nonlinear systems. The LLNF models have been used in various applications such as time series prediction <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref> and system identification <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The LLNF models with LOLIMOT learning algorithm, proposed by Nelles, employ linear functions as LMs and Gaussian validity functions for deciding upon the contribution of LMs to the final output for identification and prediction applications based on a divide-and-conquer strategy <ref type="bibr" target="#b13">[14]</ref>. However, when the nonlinearity and complexity of a process or underlying system is very high, linear function may fail to precisely model the process or system, or a large number of LMs may be required.</p><p>In addition to the inefficiency of the linear functions as LMs in case of highly nonlinear systems, the LOLIMOT learning algorithm exhibits some drawbacks, too. First, due to the normalization of validity functions in each iteration of the LOLIMOT algorithm, the normalization side effects such as reactivation may occur. Second, in each iteration of the learning procedure, the parameters of the new and all existing LMs and their validity functions should be re-estimated. This slows down the learning procedure when the dimension of the input space is high or a large number of LMs are required.</p><p>Other local linear models based on wavelet functions have also been proposed. For instance, Wang et al. <ref type="bibr" target="#b14">[15]</ref> developed a wavelet neural network (WNN) for approximating nonlinear multivariable functions. As an extension to WNN, the local linear WNN (LLWNN) was proposed <ref type="bibr" target="#b15">[16]</ref>. Although these methods are considered LMAs, they do not have the strict interpretation of TS fuzzy systems. However, a strong similarity is still preserved.</p><p>To overcome the aforementioned shortcomings of the LLNF model and LOLIMOT algorithm, in this paper we propose an LNF approach based on the least-squares support vector machines (LSSVMs) and hierarchical binary tree (HBT) learning algorithm for prediction of nonlinear and chaotic time series and processes. In the proposed LNF approach, the LSSVMs, recognized as powerful time series prediction techniques <ref type="bibr" target="#b16">[17]</ref>, are incorporated as LMs into the LNF model. In HBT algorithm, which is motivated by the LOLIMOT, normalization of the validity functions is not required and hence, its side effects are avoided. As another advantage, in each iteration of the HBT algorithm, only parameters of the newly generated LMs are computed and there is no need for parameter estimation for existing LMs. This property adds to the speed of the learning algorithm. Comprehensive simulations on seven benchmark nonlinear and chaotic time series demonstrated the promising and remarkable performance of the proposed LNF approach and HBT learning algorithm.</p><p>The rest of this paper is organized as follows. In Section II, mathematical description of the LNF models is presented. Section III provides the necessary formulations for incorporation of LSSVMs as LMs into the LNF model. The learning algorithm for training the proposed LNF model is presented in Section IV. The results of predictions of seven benchmark time series, including the Mackey-Glass time series, sunspot number series, laser intensity time series, Henon map, Ikeda map, Lorenz model, and Rossler model are provided in Section V. Finally, conclusions are drawn in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LOCAL NEURO-FUZZY MODELS</head><p>Neuro-fuzzy (NF) models are fuzzy models that are not solely designed by the expert knowledge, but are at least partly learned from data. In fact, an NF model is a fuzzy system drawn in a neural network structure and therefore learning methods already developed for neural networks can be applied to the NF model. Therefore, the NF systems inherit the learning capability of the neural networks as well as the logicality and transparency in the fuzzy systems <ref type="bibr" target="#b1">[2]</ref>. Local NF models are an appealing class of the NF systems and work on the basis of the interpolation of the LMs <ref type="bibr" target="#b13">[14]</ref>. In the LNF approach, the whole input space is partitioned into a set of subregions, each of which is determined by an LM and its corresponding validity function. Interestingly, the procedure of input space partitioning allows us to describe a complex nonlinear process by creating a number of simpler LMs, whose parameters are easily identifiable.</p><p>The general mathematical expression for an LNF with p-dimensional input, u = [u 1 , u 2 , . . . , u p ] tr , and M LMs is given by</p><formula xml:id="formula_0">ŷ = M i=1 h i (u) i (u) (1)</formula><p>where, h i (•) is a nonlinear function describing i th local model (LM i ), i is the corresponding validity function of LM i , and ŷ is the LNF model's output. In order to have a smooth transition between LMs, the validity functions are smooth and take values between 0 and 1. Furthermore, the validity functions must form a partition of unity to have reasonable interpretation of LMs</p><formula xml:id="formula_1">M i=1 i (u) = 1. (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The architecture of the LNF model, described by <ref type="bibr" target="#b0">(1)</ref>, is illustrated by Fig. <ref type="figure" target="#fig_0">1</ref>. Based on this figure, the total output of the LNF model can be represented as the weighted sum of the outputs of all LMs</p><formula xml:id="formula_3">ŷ = M i=1 ŷi i (u) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where ŷi is the output of LM i which is described by h i (u).</p><p>It is interesting to note that the arbitrary nonlinear functions h i (•), and as a result arbitrary LMs, can be utilized in the LNF model structure. This outstanding feature allows us to choose complex LMs in order to better model and describe complex and highly nonlinear processes and systems.</p><p>For local estimation of LMs using T data samples, local quadratic error minimization problems, for each LM, are defined as</p><formula xml:id="formula_5">min i I i = T t =1 i (u (t)) e 2 (t) , i = 1, . . . , M (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where i is the parameter vector for LM i , e (t) = y (t)-ŷ (t), Y = [y (1) , . . . , y (T )] is the vector containing target outputs, and ŷ is the output of the LNF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LSSVMS AS LOCAL MODELS</head><p>Support vector machines (SVMs) have the ability to accurately model and predict time series when underlying system processes are nonlinear, nonstationary, and undefined a priori. SVMs have also been proved to outperform other nonlinear techniques, including neural network-based techniques such as multilayer perceptrons (MLPs) <ref type="bibr" target="#b16">[17]</ref>. SVMs have also found applications in classification <ref type="bibr" target="#b17">[18]</ref>, feature selection <ref type="bibr" target="#b18">[19]</ref>, and approximate solutions to differential equations <ref type="bibr" target="#b19">[20]</ref>. Later, the LSSVMs were introduced, which replaced the -insensitive loss function defined by Vapnic by the quadratic loss function in the optimization problem associated with the SVMs. Here, we consider LSSVMs as LMs to be used in the structure of the local NF model because of the aforementioned advantages of SVMs and straightforward parameter estimation when quadratic loss function is selected.</p><p>Support vector machines, developed by Vapnic, are based on the statistical learning theory <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref> and recognized as an efficient and powerful regression and time series prediction technique. From a time series prediction point of view, the purpose of SVMs is to define a function that produces outputs as close as possible to the actual values of the time series over the prediction horizon. Given a training set of T data points {u (t) , y (t)} T t =1 , where u ∈ R p , y ∈ R, the SVMs try to construct a predictor function expressed by</p><formula xml:id="formula_7">y = f (u) = w tr φ (u) + b (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where f (•) : R p → R is the predictor function, φ (•) : R p → R nh is a nonlinear mapping to high, and potentially infinite, dimensional feature space, w is the weight vector, b is referred to as bias parameter, and the superscript "tr" stands for vector transposition. Now, the problem is finding the optimal weights w and bias b. This is done by minimization of weights as well as training error by considering a quadratic loss function. Therefore, the overall goal is minimization of the regularized risk function defined by <ref type="bibr" target="#b21">[22]</ref> min</p><formula xml:id="formula_9">w,b,e R = 1 2 w 2 + γ 1 2 T t =1 e (t) 2<label>(6)</label></formula><p>such that</p><formula xml:id="formula_10">y (t) = w tr φ (u (t)) + b + e (t) , t = 1, . . . ,T.<label>(7)</label></formula><p>Here, γ is referred to as regularization constant. Minimization of weights w, which results in the flatness property of the weights, is intended for reducing the overfitting of the data and minimization of bad generalization effects <ref type="bibr" target="#b16">[17]</ref>. Incorporation of the quadratic loss function into the regularized risk function (R) leads us to the class of LSSVMs. The -insensitive loss function, defined by Vapnic, is another commonly used loss function for the estimation of parameters in SVMs <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, the optimization problem stated in ( <ref type="formula" target="#formula_9">6</ref>) and ( <ref type="formula" target="#formula_10">7</ref>) should be modified to incorporate LSSVMs as LMs in the LNF model <ref type="bibr" target="#b0">(1)</ref>. Therefore, the optimization problem in ( <ref type="formula" target="#formula_9">6</ref>) and ( <ref type="formula" target="#formula_10">7</ref>) is modified on the basis of the local quadratic error minimization formulation defined by (4) as min</p><formula xml:id="formula_11">w i ,b i ,e i R i = 1 2 w i 2 + γ i 1 2 T t =1 i (u (t)) e i (t) 2 , i = 1, . . . , M<label>(8)</label></formula><p>such that</p><formula xml:id="formula_12">y (t) = w tr i φ i (u (t)) + b i + e i (t) , t = 1, . . ., T.<label>(9)</label></formula><p>Clearly, for each LM, an individual and independent optimization problem, as stated by ( <ref type="formula" target="#formula_11">8</ref>) and ( <ref type="formula" target="#formula_12">9</ref>) should be solved. Note that when the parameters of the LSSVMs are estimated, it is assumed that validity functions i are known in advance.</p><p>Let us focus on solving optimization problem ( <ref type="formula" target="#formula_11">8</ref>) and ( <ref type="formula" target="#formula_12">9</ref>) for the LM i . We derive the dual problem for ( <ref type="formula" target="#formula_11">8</ref>) and ( <ref type="formula" target="#formula_12">9</ref>) by constructing the Lagrangian</p><formula xml:id="formula_13">L i = R i - T t =1 α i,t w tr i φ i (u (t)) + b i + e i (t) -y (t) (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where α i,t are Lagrange multipliers associated with i th LM.</p><p>The conditions for optimality are derived by setting the Lagrangian derivative with respect to w i , b i , e i (t), and α i,t to zero</p><formula xml:id="formula_15">∂ L i ∂w i = 0 → w i = T t =1 α i,t φ i (u (t)) (<label>11</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">∂ L i ∂b i = 0 → T t =1 α i,t y (t) = 0 (<label>12</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">∂ L i ∂e i (t) = 0 → α i,t = γ i ( i (u (t)) e i (t)) , t = 1, . . . ,T (13) ∂ L i ∂α i,t = 0 → w tr i φ i (u (t)) + b i + e i (t) -y (t) = 0 t = 1, . . . ,T.<label>(14)</label></formula><p>By eliminating variables w i and e i (t), the following solution can be obtained for α i,t and b i :</p><formula xml:id="formula_20">0 1 tr T 1 T i + U (γ i , i (u)) b i α i = 0 Y tr (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>where</p><formula xml:id="formula_22">1 T = ⎡ ⎣ 1,1, . . . , 1 T ⎤ ⎦ tr α i = α i,1 , . . . , α i,T tr . U (γ i , i (u)) is expressed as U (γ i , i (u)) = diag 1 γ i i (u (1)) , . . . , 1 γ i i (u (T )) (<label>16</label></formula><formula xml:id="formula_23">)</formula><formula xml:id="formula_24">and i, mn = φ i (u (m)) tr φ i (u (n)) , m, n = 1, . . . , T .</formula><p>Now, by finding the solution of ( <ref type="formula" target="#formula_20">15</ref>) and substituting into (11) and ( <ref type="formula" target="#formula_7">5</ref>), the following results are obtained:</p><formula xml:id="formula_25">y i = T t =1 α i,t φ i (u) tr φ i (u (t)) + b i (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>where y i is the output of i th LM. Note that <ref type="bibr" target="#b16">(17)</ref> relies on dot product of nonlinear mapping functions φ i (u) tr and φ i (u (t)). If a kernel function K i (u (m), u (n)) satisfies Mercer's conditions, then one can apply kernel trick and state <ref type="bibr" target="#b21">[22]</ref> </p><formula xml:id="formula_27">K i (u (m) , u (n)) = φ i (u (m)) tr φ i (u (n)). (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>Interestingly, by defining a suitable kernel function (i.e., one that satisfies Mercer's conditions), no explicit construction of the nonlinear mapping φ (•) is needed. Therefore, the solution ( <ref type="formula" target="#formula_25">17</ref>) can be modified to</p><formula xml:id="formula_29">y i = f i (u) = T t =1 α i,t K i (u, u (t)) + b i . (<label>19</label></formula><formula xml:id="formula_30">)</formula><p>Radial basis function (RBF) kernel is one of the most common kernels since the Mercer's conditions hold for all parameters of an RBF kernel. The RBF kernel is expressed as</p><formula xml:id="formula_31">K (u, u (t)) = exp -u -u (t) 2 σ 2 i (<label>20</label></formula><formula xml:id="formula_32">)</formula><p>where σ i is the bandwidth of the RBF kernel.</p><p>According to ( <ref type="formula" target="#formula_11">8</ref>) and ( <ref type="formula" target="#formula_31">20</ref>), γ i and σ i are the only unknown parameters that must be set prior to the construction of the local LSSVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LEARNING PROCEDURE</head><p>In Section III, the procedure of estimating local LSSVMs from observation data was presented. However, for identification of an LNF mode, parameters of both LMs and validity functions should be determined. In this paper, we propose the heuristic, fast, and efficient HBT learning algorithm for training the proposed LNF model with LSSVMs as LMs.</p><p>The HBT learning algorithm is a heuristic tree-construction algorithm motivated by the local linear model tree (LOLIMOT) learning algorithm <ref type="bibr" target="#b13">[14]</ref>. The idea of binary tree is based on the concept of decision tree, introduced by Breiman et al. <ref type="bibr" target="#b22">[23]</ref>. The HBT learning algorithm tries to partition the input space into the hyper-rectangular subdomains by axisorthogonal splits. In each iteration of the algorithm, the worst performing LM is divided into two halves. Therefore, two new LMs are generated and their associated validity functions determined. This procedure continues until the desirable performance is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchical Partitioning of Input Space</head><p>The architecture of the HBT algorithm is represented by a growing binary tree example, shown in Fig. <ref type="figure">2</ref>. In this architecture, each node i corresponds to the partitioning of the input space into two areas by the splitting function ψ i (u) and its counterpart 1 -ψ i (u). It is seen that the two splitting functions automatically sum up to unity; therefore no normalization is needed and its undesirable side effects, such as reactivation in LOLIMOT <ref type="bibr" target="#b1">[2]</ref>, are avoided.</p><p>The partitioned regions are further subdivided by succeeding nodes, if any, and the binary tree ends with a set of end nodes, called leaves. Each leaf represents an LM and its contribution to the overall model output is given by its validity function, which is computed by multiplication of all splitting functions from the root of the tree to the corresponding leaf. For the binary tree in Fig. <ref type="figure">2</ref> with nine nodes and five leaves, the </p><formula xml:id="formula_33">ψ 1 1 -ψ 1 1 -ψ 2 ψ 2 ψ 3 1 -ψ 3 1 -ψ 6 ψ 6</formula><p>Leaf Root Node Fig. <ref type="figure">2</ref>. Hierarchical model structure with five LMs (at leaves).</p><p>validity functions can be expressed as follows:</p><formula xml:id="formula_34">4 (u) = (1 -ψ 2 (u)) (1 -ψ 1 (u)) (21) 5 (u) = (ψ 2 (u)) (1 -ψ 1 (u)) (22) 7 (u) = ψ 3 (u) ψ 1 (u) (23) 8 (u) = (1 -ψ 6 (u)) (1 -ψ 3 (u)) ψ 1 (u) (24) 9 (u) = ψ 6 (u) (1 -ψ 3 (u)) ψ 1 (u). (<label>25</label></formula><formula xml:id="formula_35">)</formula><p>It can be easily verified that</p><formula xml:id="formula_36">i={4,5,7,8,9} i (u) = 1.<label>(26)</label></formula><p>If the validity functions are known, the model's output can be computed as the sum of LMs' outputs, weighted by their validity functions</p><formula xml:id="formula_37">ŷ = i∈ h i (u) i (u) (<label>27</label></formula><formula xml:id="formula_38">)</formula><p>where is the set of indices of the leaf nodes. In Fig. <ref type="figure">2</ref>, = {4, 5, 7, 8, 9}. Sigmoid splitting functions are suitable for heuristic axis-orthogonal partitioning of the input space, since proper multiplication of sigmoid functions can easily generate the local validity functions. A sigmoid splitting function is expressed as</p><formula xml:id="formula_39">ψ i (u) = 1 1 + e -s i (di,0+di,1u1+•••+di,pu p ) (<label>28</label></formula><formula xml:id="formula_40">)</formula><p>where the direction vector</p><formula xml:id="formula_41">d i = [d i,1 , …, d i, p</formula><p>] tr sets the direction of split, the position parameter d i,0 determines the position of the split, and the smoothness parameter s i determines the smoothness of the split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Procedure</head><p>The HBT learning algorithm works on the basis of the structure of the growing binary tree introduced in Section IV-A. In each iteration of the HBT algorithm, the worst performing local model (LM wt ) is determined using loss function <ref type="bibr" target="#b3">(4)</ref>. Then the validity region (premise space) of the LM wt is divided into two equal hyperrectangles through axis-orthogonal splits. The divisions in all dimensions of the input space are tried, and the best division that leads to the highest improvement in model performance is performed. Since the splits are axis-orthogonal, all entries of the direction vector, except the one that corresponds to the division dimension, are zero. For instance, if division in the second dimension is intended, the direction vector would be d i = [0 1, . . ., 0] tr 1× p . Hence, the direction vector of the splitting function can be determined heuristically and without running any time-consuming optimization procedure. The positioning parameter d i,0 is set equal to the center of the LM wt at the split dimension, and the smoothness parameter is proportional to the length of the newly generated hyper-rectangle. Once the splitting function ψ i (u) is determined, its counterpart 1-ψ i (u) can be computed easily.</p><p>When the splitting functions, and consequently the validity functions, are determined, the parameters of the corresponding local LSSVMs can be estimated through <ref type="bibr" target="#b14">(15)</ref>. The HBT learning algorithm can be summarized as follows.</p><p>1) Start With the Initial Model: Set M = 1, start with a single LM whose validity function [ 1 (u) = 1] covers the whole input space, and estimate the model parameters using <ref type="bibr" target="#b14">(15)</ref>. 2) Find the Worst LM: Calculate a loss function defined by <ref type="bibr" target="#b4">(5)</ref> for each of LM i , i = 1, …, M, and find the worst performing LM wt . 3) Check All Divisions: The validity region of the LM wt must be divided into two equal hyper-rectangles (two new validity regions) in all of p dimensions. a) Construct splitting functions: For each of p divisions, a splitting function must be determined. The positioning parameter d i,0 is set equal to the center of the LM wt at the split dimension, all entries of the direction vector, except the one that corresponds to the division dimension, are zero and the smoothness parameter is simply set proportional to the length of the newly generated hyper-rectangle. Once the splitting function ψ i is determined, calculate its counterpart 1 -ψ i . b) Compute validity functions: Determine the validity functions of two newly generated validity regions by multiplication of all splitting functions from the root of the tree to the corresponding leaf. c) Estimate parameters of new LMs: Parameters of the two LMs, corresponding to the two newly generated validity regions, must be estimated using <ref type="bibr" target="#b14">(15)</ref>. Other existing LMs remain unchanged. Finally, the loss function for the current overall model must be computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Find the Best Division:</head><p>The best LMs related to the lowest loss function value must be determined. The number of LMs is incremented: M → M + 1. If the termination criterion, e.g., a desired level of validation error or model's complexity, is met, then stop; otherwise go to step 2.</p><p>Maximum generalization and noteworthy forecasting performance are among the salient features of the model identified by HBT learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS AND DISCUSSION</head><p>In order to evaluate the proposed local LSSVM-based NF model, comprehensive experiments and simulations based on seven nonlinear and chaotic benchmark time series were conducted. The first case study applies the proposed approach to the Mackey-Glass time series, which is an artificially generated time series and a commonly used benchmark time series by researchers for evaluating time series prediction approaches. The annually recorded sunspot number, as a realworld time series, is predicted in the second case study. The third case study analyzes the laser intensity time series, which has been obtained from laboratory measurements and exhibits complex and chaotic behavior. The last case study includes modeling and prediction of four chaotic processes related to a group of benchmark equation-based nonlinear dynamical systems.</p><p>Besides, in all case studies, results produced by LLNF with LOLIMOT learning algorithm are also reported. This is done to draw comparison between the LLNF and the proposed LNF models as well as between the LOLIMOT and HBT learning algorithms.</p><p>It must be noted that, in all case studies, the last 10% of the training data is used as the validation data to select best model structure. The validation data is not used during the training, and the model with lowest validation error is selected as the best model, which is used to predict test data.</p><p>For numerical assessment of the prediction's accuracy, the following error criteria are used.</p><p>1) Root-mean-square error (RMSE)</p><formula xml:id="formula_42">RMSE = 1 N N n=1 y (n) -ŷ (n) 2 . (<label>29</label></formula><formula xml:id="formula_43">)</formula><p>2) Normalized mean-square error (NMSE)</p><formula xml:id="formula_44">NMSE = N n=1 y (n) -ŷ (n) 2 (y (n) -ȳ) 2 ȳ = 1 N n n=1 y (n) (<label>30</label></formula><formula xml:id="formula_45">)</formula><p>where y (n) and ŷ (n) are the actual and forecasted outputs at sample n, respectively, and N is the number of forecasted samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prediction of Mackey-Glass Time Series</head><p>Mackey-Glass time series is generated using the following time-delay differential equation, introduced as a model for white blood cell production:</p><formula xml:id="formula_46">dx dt = 0.2x (t -τ ) 1 + x 10 (t -τ ) -0.1x (t). (<label>31</label></formula><formula xml:id="formula_47">)</formula><p>This time series is chaotic for τ &gt; 16.8 and exhibits no clearly defined period <ref type="bibr" target="#b23">[24]</ref>. This benchmark series has been used in numerous neural network and fuzzy modeling researches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b37">[38]</ref>. The series is very sensitive to the initial conditions. Here, we choose x (0) = 1.2 and τ = 17 to generate 1000 data samples by Runge-Kutta integration of the ( <ref type="formula" target="#formula_46">31</ref>) with a step size of 1.</p><p>The standard input variables for this case are x (t -18) , x (t -12) , x (t -6), and x (t) for predicting x (t + 6) (i.e., this is a case of six-step-ahead prediction). From 1000 generated observations, the first 500 sets of data are used as the training set and the last 500 are employed as test series.</p><p>For this case study, the LNF model with six neurons (i.e., six local LSSVMs) produced the lowest validation error, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The actual and predicted test series are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. For a numerical evaluation and for the purpose of comparison, the training and test RMSEs of the proposed approach and a number of methods available in the literature are presented in Table <ref type="table">I</ref>. Based on the presented results in Table <ref type="table">I</ref>, the proposed LNF approach has much better performance over all other compared methods for both training and test data. The difference in performance between the proposed LNF model and LLNF model is also noticeable. Note that the reported results for LLNF with LOLIMOT algorithm were obtained for 70 neurons, which are much more than 6 neurons for the proposed LNF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prediction of Sunspot Number Time Series</head><p>Let us now consider the sunspot time series which is a real-world highly-complex and nonstationary time series. This series corresponds to an annual average relative number of sunspots observed. A considerable number of CI-based approaches have studied the sunspot numbers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b43">[44]</ref>. The annually recorded sunspot time series from 1700 to 1979 is considered here <ref type="bibr" target="#b44">[45]</ref>. The input variables are x (t -4) , x (t -3) , x (t -2), and x (t -1) where the output is x (t). The whole sunspot series is divided into a training set, from 1700 to 1920, and two test sets, from 1921 to 1955  -0.02 Sixth-order polynomial <ref type="bibr" target="#b15">[16]</ref> -0.04 Linear prediction method <ref type="bibr" target="#b15">[16]</ref> -0.55 Product T-norm <ref type="bibr" target="#b24">[25]</ref> -0.09 Classical RBF (with 23 neurons) <ref type="bibr" target="#b25">[26]</ref> -0.0114 PG-RBF network <ref type="bibr" target="#b26">[27]</ref> -0.0028 Genetic algorithm and fuzzy system <ref type="bibr" target="#b27">[28]</ref> -0.049 Neural tree model <ref type="bibr" target="#b28">[29]</ref> -0.0069 WNN <ref type="bibr" target="#b15">[16]</ref>+gradient 0.0067 0.0071 WNN <ref type="bibr" target="#b15">[16]</ref>+hybrid 0.0056 0.0059 LLWNN <ref type="bibr" target="#b15">[16]</ref>+gradient 0.0038 0.0041 LLWNN <ref type="bibr" target="#b15">[16]</ref>+hybrid 0.0033 0.0036 IT2FNN-3 <ref type="bibr" target="#b29">[30]</ref> -0.0020 MSBFNN <ref type="bibr" target="#b30">[31]</ref> -0.0024 SEIT2FNN <ref type="bibr" target="#b31">[32]</ref> -0.0034 FLNFN-CCPSO <ref type="bibr" target="#b32">[33]</ref> 0.00827 0.00842 NFIS-SEELA <ref type="bibr" target="#b33">[34]</ref> 0.00745 0.00747 Recurrent ANFIS <ref type="bibr" target="#b34">[35]</ref> -0.0013 ANFIS <ref type="bibr" target="#b35">[36]</ref> 0.00160 0.00156 RBF network <ref type="bibr" target="#b36">[37]</ref> -0.0015 FWNN-S <ref type="bibr" target="#b37">[38]</ref> 0.00124 0.00109 FWNN-R <ref type="bibr" target="#b37">[38]</ref> 0.00231 0.00232 FWNN-M <ref type="bibr" target="#b37">[38]</ref> 0.00129 0.00114 LLNF 0.0013 0.0020 Proposed LNF approach 0.00070 0.00079 and from 1956 to 1979, respectively. It must be noted that the second test series is atypical of the data on the whole <ref type="bibr" target="#b37">[38]</ref>.</p><p>The training procedure resulted in the best LNF model with seven neurons. Then, the trained model was used to predict the sunspot number for test set 1 and test set 2. The actual and predicted outputs for test series 1 and 2 are illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>.  It is worth noting that performance of the most compared approaches deteriorates for the second test series, while the proposed LNF model still keeps its accuracy even for the second test series which is atypical of the data on the whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction of Laser-Generated Time Series</head><p>The laser data is a univariate time record of the observed laser intensity measured in a physics laboratory experiment <ref type="bibr">[46]</ref>. The data was recorded from a far-infrared (FIR) laser in a chaotic state. The measurements were made on an 81.5-micrometer 14 NH3 continuous wave (FIR) laser. The experimental signal-to-noise ratio was about 300. The laser intensity data has a complicated and chaotic pattern of behavior. Furthermore, it is highly predictable on the shortest time scales (relatively simple oscillations), but has global Target and predicted test series for laser intensity time series (100 one-step-ahead predictions). events that are harder to predict (the rapid decay of the oscillations) <ref type="bibr">[46]</ref>. Fig. <ref type="figure" target="#fig_5">6</ref> shows the laser intensity data.</p><p>The total amount of data includes 1100 samples. The first 1000 data samples were used for training the proposed LNF model and the rest were used for testing the performance of the trained model. The learning procedure of the LNF model by the HBT algorithm was terminated after construction of six local LSSVMs and their corresponding validity functions. The trained model was applied to the test data to produce predictions for the data not previously seen by the model. The obtained predictions along with the target laser intensities are illustrated in Fig. <ref type="figure">7</ref>. It is obvious that the proposed LNF approach has successfully captured the complicated and nonlinear dynamics of the laser intensity time series, since it produced prediction outputs that match the target value very well. The numerical comparison presented in Table <ref type="table">III</ref> indicates the noticeable performance of the proposed LNF approach and its superiority over other compared methods.  <ref type="bibr" target="#b45">[47]</ref> 0.00088 0.00093 RNN-BPTT <ref type="bibr" target="#b45">[47]</ref> 0.00032 0.01092 RNN-RTRL <ref type="bibr" target="#b45">[47]</ref> 0.00036 0.00876 RNN-EKF <ref type="bibr" target="#b45">[47]</ref> 0.00035 0.00436 RBLM-RNN <ref type="bibr" target="#b45">[47]</ref> 0.00045 0.00060 LLNF 0.00068 0.00104 Proposed LNF approach 0.00037 0.00053 </p><formula xml:id="formula_48">1 50 100 150<label>200 250 -3 -2 -</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Prediction of Chaotic Processes</head><p>The last case study addresses prediction of time series produced by four chaotic processes, namely Henon map, Ikeda map, Lorenz model, and Rossler model, all of which are recognized as nonlinear dynamical systems. For each chaotic process, 1250 samples were generated; the first 1000 samples were used as training set and the remaining 250 samples applied as test data to evaluate performance of the proposed LNF model. In order to have a fair comparison with the approaches proposed in <ref type="bibr" target="#b45">[47]</ref>, white noise with standard deviation of 0.05 was added to the training set of each process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Prediction of Henon Map:</head><p>The Henon map is a discretetime dynamical system and one of the most frequently studied examples of dynamical systems that exhibit chaotic behavior. Henon map is identified by the following set of governing equations <ref type="bibr" target="#b46">[48]</ref>:</p><formula xml:id="formula_49">x n+1 = y n + 1 -αx 2 n y n+1 = βx n . (<label>32</label></formula><formula xml:id="formula_50">)</formula><p>The time series was generated by setting α = 0.4 and β = 0.3.   <ref type="bibr" target="#b45">[47]</ref> 0.00032 0.00081 RNN-BPTT <ref type="bibr" target="#b45">[47]</ref> 0.00055 0.00110 RNN-RTRL <ref type="bibr" target="#b45">[47]</ref> 0.00058 0.00103 RNN-EKF <ref type="bibr" target="#b45">[47]</ref> 0.00043 0.00072 RBLM-RNN <ref type="bibr" target="#b45">[47]</ref> 0.00036 0.00068 LLNF 0.00049 0.00077 Proposed LNF approach 0.00018 0.00044 predicted values. Furthermore, the performance of the proposed approach was assessed in terms of NMSE, as presented in Table <ref type="table">IV</ref>. This table also provides comparisons with several other methods developed in <ref type="bibr" target="#b45">[47]</ref>. Clearly, the proposed LNF model outperforms other approaches for both training and test data. Based on the results presented in Table <ref type="table">IV</ref>, the test NMSE obtained by proposed LNF approach is about two-thirds of the NMSE achieved by RBLM-RNN.</p><p>2) Prediction of Ikeda Map: Ikeda map is another discretetime dynamical system, proposed by Ikeda et al. <ref type="bibr" target="#b47">[49]</ref> for modeling multistability and chaos in a ring cavity with a nonlinear medium. The Ikeda map is given by</p><formula xml:id="formula_51">x n+1 = 1 + μ (x n cos ρ n -y n sin ρ n ) y n+1 = μ (x n sin ρ n + y n cos ρ n ) (<label>33</label></formula><formula xml:id="formula_52">)</formula><p>where ρ n = 0.4 -b/(1 + x 2 n + y 2 n ). We set b = 6 and μ = 0.9 to produce a chaotic time series.</p><p>The target and predicted values of Ikeda map time series for 250 test samples are shown in Fig. <ref type="figure" target="#fig_8">9</ref>. A zoomed-in region is also shown in this figure to better illustrate the accuracy of the predictions. The training and test NMSE, as well as comparison with the approaches developed in <ref type="bibr" target="#b45">[47]</ref>, are presented in Table <ref type="table">V</ref>.</p><p>3) Prediction of Lorenz Model: The Lorenz model is an example of a nonlinear dynamic system corresponding  <ref type="bibr" target="#b45">[47]</ref> 0.00049 0.00127 RNN-BPTT <ref type="bibr" target="#b45">[47]</ref> 0.00058 0.00162 RNN-RTRL <ref type="bibr" target="#b45">[47]</ref> 0.00056 0.00159 RNN-EKF <ref type="bibr" target="#b45">[47]</ref> 0.00043 0.00101 RBLM-RNN <ref type="bibr" target="#b45">[47]</ref> 0.00048 0.00081 LLNF 0.00052 0.0010 Proposed LNF approach 0.00029 0.00037 to the long-term behavior of the Lorenz oscillator. The Lorenz oscillator is a 3-D continuous dynamical system that exhibits chaotic flow <ref type="bibr" target="#b47">[49]</ref>. The Lorenz model is expressed by the following mathematical equation:</p><formula xml:id="formula_53">ẋ = σ (y -x) ẏ = x (ρ -z) -y ż = x y -βz (34)</formula><p>where x, y, and z are real functions of time and σ = 10, ρ = 8/3, and β = 28. The data was generated by Runge-Kutta integration of the Lorenz equations with a step size of 0.01. Fig. <ref type="figure" target="#fig_10">10</ref> shows target and predicted values for the Lorenz time series. The numerical assessment of the predictions and comparison with the approaches developed in <ref type="bibr" target="#b45">[47]</ref> are also presented in Table <ref type="table">VI</ref>. It is interesting to note the considerable difference between test NMSE of the proposed LNF and that of the RBLM-RNN, as the best approach in [47] (6.4 × 10 -5 versus 9.0 × 10 -4 ). This comparison demonstrates the superior and outstanding performance of the proposed LNF model in modeling and predicting nonlinear and chaotic time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Prediction of Rossler Model:</head><p>The fourth and last example of nonlinear dynamical systems deals with Rossler model. Rossler model is a system of three ordinary differential equations which define a continuous dynamical system that exhibits chaotic dynamics associated with the fractal properties of the Rossler attractor <ref type="bibr" target="#b47">[49]</ref>. This map is described by the following differential equations:    <ref type="bibr" target="#b45">[47]</ref> 0.00033 0.00096 RNN-BPTT <ref type="bibr" target="#b45">[47]</ref> 0.00056 0.00185 RNN-RTRL <ref type="bibr" target="#b45">[47]</ref> 0.00057 0.00172 RNN-EKF <ref type="bibr" target="#b45">[47]</ref> 0.00036 0.00121 RBLM-RNN <ref type="bibr" target="#b45">[47]</ref> 0.00036 0.00090 LLNF 0.00013 0.00029 Proposed LNF approach 0.000017 0.000064 proposed LNF model compared to the approaches developed in <ref type="bibr" target="#b45">[47]</ref>.</p><formula xml:id="formula_54">ẋ = -y -z ẏ = x -ay ż = b + z (x -c) (<label>35</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Influence of Noise</head><p>The results of comprehensive simulations were presented in Section V-A to V-D. However, the influence of measurement  <ref type="bibr" target="#b45">[47]</ref> 0.00047 0.00101 RNN-BPTT <ref type="bibr" target="#b45">[47]</ref> 0.00070 0.00311 RNN-RTRL <ref type="bibr" target="#b45">[47]</ref> 0.00071 0.00312 RNN-EKF <ref type="bibr" target="#b45">[47]</ref> 0.00060 0.00191 RBLM-RNN <ref type="bibr" target="#b45">[47]</ref> 0.00057 0.00092 LLNF 0.000048 0.000071 Proposed LNF approach 0.0000065 0.000015 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Burden</head><p>Analysis of the computational burden of the proposed approach during training and test phase is also important and of interest. For this purpose, the times required during training and test for each of the seven time series are summarized in Table <ref type="table">IX</ref>. All Simulations were carried out on a PC with a 2-GHz processor and 2 MB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper proposed a local LSSVM-based NF network with the HBT learning algorithm for modeling and predicting nonlinear and chaotic time series and processes. The local characteristics of the proposed LNF approach allows integrating any arbitrary model as LMs of the LNF network, which can be identified independently and through experiment data. The LSSVMs, which are powerful in modeling and predicting time series, were integrated into the proposed LNF network as LMs. Furthermore, we proposed the heuristic learning algorithm of HBT as the training algorithm of the proposed local LSSVM-based NF approach. The HBT learning algorithm heuristically partitions the input space into smaller subdomains by axis-orthogonal splits. In each partitioning, the validity functions automatically form a unity partition and therefore no normalization side effects occur. Furthermore, in each iteration of the HBT algorithm, only the parameters of the newly generated LMs and their corresponding validity functions are estimated, and all other parameters of the whole model remain unchanged. This interesting property results in the speed improvement of the training procedure. The proposed LNF approach was employed for modeling and predicting a wide range of different nonlinear time series and chaotic systems and processes. The investigated time series included Sunspot number time series, laser intensity time series, Mackey-Glass time series, Henon map, Ikeda map, Lorenz model, and Rossler model; the first two are recognized as real-world nonlinear time series, while the last five are known to be dynamical systems with chaotic behaviors. The obtained prediction results and the comprehensive comparisons with some recently published studies revealed the outstanding performance of the proposed LNF approach in modeling and prediction of nonlinear, chaotic, and complex time series and processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Structure of the LNF model with p inputs and M LMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training and validation RMSE for the Mackey-Glass time series.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Target and predicted test series for the Mackey-Glass time series (500 six-step-ahead predictions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5.Target and predicted test series for sunspot number time series (59 one-step-ahead predictions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Laser intensity time series.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8.Target and predicted test series for Henon map time series (250 one-step-ahead predictions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>depicts the target and predicted values for test data of series generated by Henon map. The zoomed region in this figure shows the close match between actual and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Target and predicted test series for Ikeda map time series (250 onestep-ahead predictions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>)where x, y, and z are real functions of time and the attractor shows chaotic behavior for a = 0.2, b = 0.2, and c = 4.6. The data was generated by Runge-Kutta integration of the Rossler equations with a step size of 0.01.The target and predicted test data of Rossler model are shown in Fig.11, demonstrating successful performance of the proposed LNF approach in capturing and modeling the dynamical behavior of this nonlinear chaotic system. The training and test NMSE and the comparisons provided in Table VII indicate the far better prediction performance of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10.Target and predicted test series for Lorenz model time series (250 one-step-ahead predictions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11.Target and predicted test series for Rossler model time series (250 one-step-ahead predictions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>numerical comparison based on the NMSE with several approaches is made in Table II. The superior performance of the proposed LNF approach is noticeable in this table.</figDesc><table><row><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="3">PERFORMANCE COMPARISON FOR SUNSPOT</cell><cell></cell></row><row><cell cols="3">NUMBER TIME SERIES PREDICTION</cell><cell></cell></row><row><cell>Method</cell><cell>Training NRMSE</cell><cell>NMSE for Test 1</cell><cell>NMSE for Test 2</cell></row><row><cell>Tong and Lim [39]</cell><cell>0.097</cell><cell>0.097</cell><cell>0.28</cell></row><row><cell>Weigend [40]</cell><cell>0.082</cell><cell>0.086</cell><cell>0.35</cell></row><row><cell>Svarer [41]</cell><cell>0.090</cell><cell>0.082</cell><cell>0.35</cell></row><row><cell>Transversal net [42]</cell><cell>0.0987</cell><cell>0.0971</cell><cell>0.3724</cell></row><row><cell>Recurrent net [42]</cell><cell>0.1006</cell><cell>0.0972</cell><cell>0.4361</cell></row><row><cell>RFNN [43]</cell><cell>-</cell><cell>0.074</cell><cell>0.21</cell></row><row><cell>ANFIS [36]</cell><cell>0.0550</cell><cell>0.1915</cell><cell>0.4068</cell></row><row><cell>FENN [44]</cell><cell>-</cell><cell>-</cell><cell>0.18</cell></row><row><cell>FWNN-S [38]</cell><cell>0.0895</cell><cell>0.1093</cell><cell>0.1510</cell></row><row><cell>FWNN-R [38]</cell><cell>0.0796</cell><cell>0.1099</cell><cell>0.2549</cell></row><row><cell>FWNN-M [38]</cell><cell>0.0828</cell><cell>0.0973</cell><cell>0.1988</cell></row><row><cell>LLNF</cell><cell>0.0714</cell><cell>0.087</cell><cell>0.1033</cell></row><row><cell>Proposed LNF approach</cell><cell>0.050</cell><cell>0.063</cell><cell>0.078</cell></row></table><note><p>A</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>on modeling and prediction accuracy has not been fully investigated yet. To do this, the time series generated by Rossler model is considered. Then the total series (including both training and test series) is distorted by white noise with standard deviations of 0.01, 0.02, 0.03, 0.04, and 0.05 to produce five distorted series. The prediction results for each distorted series are presented in TableVIII. For the standard deviation of white noise from 0.00 (noiseless series) to 0.05, the training RMSE ranges from 0.0144 to 0.0375 and the test RMSE ranges from 0.0149 to 0.0421. Note that for all levels of noise, the training and test RMSEs are of the same order.</figDesc><table><row><cell></cell><cell cols="2">TABLE VIII</cell></row><row><cell cols="4">INFLUENCE OF NOISE ON PERFORMANCE OF THE PROPOSED</cell></row><row><cell cols="4">LNF MODEL FOR ROSSLER TIME SERIES</cell></row><row><cell cols="2">White Noise Standard Deviation</cell><cell cols="2">Training RMSE</cell><cell>Test RMSE</cell></row><row><cell>0.00 (noiseless series)</cell><cell></cell><cell>0.0144</cell><cell>0.0149</cell></row><row><cell>0.01</cell><cell></cell><cell>0.0173</cell><cell>0.0185</cell></row><row><cell>0.02</cell><cell></cell><cell>0.0215</cell><cell>0.0222</cell></row><row><cell>0.03</cell><cell></cell><cell>0.0265</cell><cell>0.0290</cell></row><row><cell>0.04</cell><cell></cell><cell>0.0322</cell><cell>0.0350</cell></row><row><cell>0.05</cell><cell></cell><cell>0.0375</cell><cell>0.0421</cell></row><row><cell></cell><cell cols="2">TABLE IX</cell></row><row><cell cols="4">COMPUTATIONAL BURDEN OF PROPOSED LNF APPROACH</cell></row><row><cell>Case Study</cell><cell cols="3">Training Time Test Time</cell></row><row><cell>Mackey-Glass</cell><cell></cell><cell>0.25</cell><cell>0.13</cell></row><row><cell>Sunspot number</cell><cell></cell><cell>0.12</cell><cell>0.08</cell></row><row><cell>Laser intensity</cell><cell></cell><cell>0.22</cell><cell>0.15</cell></row><row><cell>Henon map</cell><cell></cell><cell>0. 18</cell><cell>0.13</cell></row><row><cell>Ikeda map</cell><cell></cell><cell>0. 18</cell><cell>0.13</cell></row><row><cell>Lorenz model</cell><cell></cell><cell>0. 18</cell><cell>0.13</cell></row><row><cell>Rossler model</cell><cell></cell><cell>0.18</cell><cell>0.13</cell></row></table><note><p><p>IN TERMS OF TRAINING AND TEST TIME (IN SECONDS)</p>noise</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He was a Design Engineer in transmission substations design and construction projects from 2010 to 2011. He joined the Institute for International Energy Studies, Tehran, Iran, in 2011, as a Senior Researcher and focused on modeling and prediction of energy (particularly oil and gas) price and demand and published several papers based on the findings of his researches. He has been involved in several research projects, centering around developing computational intelligence-based approaches for modeling, identification, prediction, and control. Currently, he is working on developing novel local modeling approaches for identification, prediction and control in nonlinear systems and processes and is in collaboration with the Department of Mechanical Engineering, Pardis Branch, Islamic Azad University, Tehran. He is conducting researches on simulation and control in multiterminal DC power systems. His current research interests include computational intelligence and its applications in the area of nonlinear system identification and control, with a focus on electric power systems, signal processing, and time series prediction.</p><p>Majid Abdollahzade (M'11) received the B.Sc. degree in mechanical engineering from the K. N. Toosi University of Technology, Tehran, Iran, in 2005, and the M.Sc. degree from the University of Tehran, Tehran, in 2008, respectively. He is currently pursuing the Ph.D. degree in mechanical engineering with the K. N. Toosi University of Technology.</p><p>He joined the Institute for International Energy Studies, Tehran, in 2010, as a Senior Researcher, where he is working on energy (oil and gas) price and demand modeling and simulation. He is with the Department of Mechanical Engineering, Pardis Branch, Islamic Azad University, Tehran. He has been involved in various research projects in the area of nonlinear system identification, modeling and prediction of energy price and demand and design of decision support systems for commodity market regulation using computational intelligence techniques. He has published more than 20 papers in international journals and conference proceedings. His current research interests include nonlinear system identification, computational intelligence, control, signal processing, and time series prediction.</p><p>Mr. Abdollahzade is a member of the IEEE Computational Intelligence Society. He is a reviewer of several international journals in the area of computational intelligence and forecasting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonlinear identification with local model networks using GTLS techniques and equality constraints</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hametner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jakubeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1406" to="1418" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Nelles</surname></persName>
		</author>
		<title level="m">Nonlinear System Identification: From Classical Approaches to Neural Networks and Fuzzy Models</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive local model networks with higher degree polynomials</title>
		<author>
			<persName><forename type="first">O</forename><surname>Banfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nelles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the interpretation and identification of dynamic Takagi-Sugeno fuzzy models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="313" />
			<date type="published" when="2000-06">Jun. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Piecewise-linear approximation of non-linear models based on probabilistically/possibilistically interpreted intervals&apos; numbers (INs)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Kaburlasos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="5060" to="5076" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image interpolation via regularized local linear regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3455" to="3469" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application of emotional learning fuzzy inference systems and locally linear neuro-fuzzy models for prediction and simulation in dynamic systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdollahzade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Faraji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf. Fuzzy Syst</title>
		<meeting>IEEE Int. Joint Conf. Fuzzy Syst<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mid-term energy demand forecasting by hybrid neuro-fuzzy models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdollahzade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miranian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solar activity forecast: Spectral analysis and neurofuzzy prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shafiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Atmosph. Solar-Terrest. Phys</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="595" to="603" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Input variables selection using mutual information for neuro fuzzy modeling with the application to time series forecasting</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirmomeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1121" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forecasting energy consumption using fuzzy transform and local linear neuro fuzzy models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdollahzade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miranian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An experimental nonlinear system identification based on local linear neuro-fuzzy models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nourzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fatehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Labibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Araabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Ind. Technol</title>
		<meeting>IEEE Int. Conf. Ind. Technol</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2274" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identification, prediction and detection of the process fault in a cement rotary kiln by locally linear neurofuzzy technique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fatehi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Process Control</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locally linear neuro-fuzzy (LLNF) electricity price forecasting in deregulated power markets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdollahzade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahjoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zaringhalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miranian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innovat. Comput., Inf. Control</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A wavelet neural network for the approximation of nonlinear multivariable function</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inst. Electr. Eng. Jpn</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="193" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>C</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Time-series prediction using a local linear wavelet neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="449" to="465" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Time series prediction using support vector machines: A survey</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Sapankevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="38" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources: A domain-dependent regularization approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="518" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiclass feature selection with kernel grammatrix-based criteria</title>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1611" to="1623" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximate solutions to ordinary differential equations using least squares support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrkanoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Falck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1356" to="1367" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Gestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<title level="m">Least Squares Support Vector Machines</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth &amp; Brooks</publisher>
			<pubPlace>Monterey, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chaotic time series prediction based on a novel robust echo state network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="787" to="799" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating fuzzy rules by learning from examples</title>
		<author>
			<persName><forename type="first">L.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1414" to="1427" />
			<date type="published" when="1992-12">Nov.-Dec. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Radial basis function based adaptive fuzzy systems their application to system identification and prediction</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="325" to="339" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Time series analysis using normalized PG-RBF network with regression weights</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pomares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bernier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Pelayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="285" />
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Forecasting time series with genetic fuzzy predictor ensembles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="535" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonlinear system modeling via optimal design of neural trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="137" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hybrid learning algorithm for a class of interval type-2 fuzzy neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodríguez-Díaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2175" to="2193" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A fuzzy neural network with fuzzy impact grades</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hengjie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3098" to="3122" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A self-evolving interval type-2 fuzzy neural network with online structure and parameter learning</title>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1424" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A hybrid of cooperative particle swarm optimization and cultural algorithm for neural fuzzy networks and its prediction applications</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient self-evolving evolutionary learning for neurofuzzy inference systems</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1476" to="1490" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent type ANFIS using local search technique for time series prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vairappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Asia Pacific Conf. Circuits Syst</title>
		<meeting>IEEE Asia Pacific Conf. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2008-08">Aug. 2008</date>
			<biblScope unit="page" from="380" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ANFIS: Adaptive-network-based fuzzy inference systems</title>
		<author>
			<persName><forename type="first">J.-S</forename><forename type="middle">R</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="665" to="685" />
			<date type="published" when="1993-06">May-Jun. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The effect of different basis functions on a radial basis function network for time series prediction: A comparative study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harpham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2161" to="2170" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fuzzy wavelet neural network models for prediction and identification of dynamical systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oysal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1599" to="1609" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Threshold autoregression, limit cycles and cyclical data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="292" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Predicting the future: A connectionist approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigned</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
		<idno>PARC-SSL-90-20</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>PDP Research Group</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On design and evaluation of tapped-delay neural network architectures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Svarer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Netw</title>
		<meeting>IEEE Int. Conf. Neural Netw<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09">Sep. 1993</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evolving recurrent perceptrons for time-series modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Waagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="38" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evolutionary algorithm-based learning of fuzzy neural networks. Part 2: Recurrent fuzzy neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Alieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Guirimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fazlollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Aliev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A new neural network structure for temporal signal processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process.</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3341" to="3344" />
			<date type="published" when="1997">1997</date>
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sunspot Numbers</title>
		<ptr target="http://www.ngdc.noaa.gov/stp/solar/ssndata.html" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>National Geophysical Data Center</publisher>
			<pubPlace>Boulder, CO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recursive Bayesian recurrent neural networks for time-series modeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mirikitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikolaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="274" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The Computational Beauty of Nature: Computer Explorations of Fractals, Chaos, Complex Systems, and Adaptation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Flake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Ludge</surname></persName>
		</author>
		<title level="m">Nonlinear Laser Dynamics: From Quantum Dots to Cryptography</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
