<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A C C E P T E D M A N U S C R I P T</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-24">June 24, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">New York University Abu Dhabi</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Digital Technologies</orgName>
								<orgName type="institution">Northumbria University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">New York University Abu Dhabi</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
							<email>yfang@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NYU Multimedia and Visual Computing Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">New York University Abu Dhabi</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A C C E P T E D M A N U S C R I P T</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-24">June 24, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">A6E5D9960268F02D57812DE244044693</idno>
					<idno type="DOI">10.1016/j.imavis.2016.06.007</idno>
					<note type="submission">Preprint submitted to Image and Vision Computing ACCEPTED MANUSCRIPT resentation techniques.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human action recognition</term>
					<term>Handcrafted features</term>
					<term>Deep Learning</term>
					<term>Convolutional neural network</term>
					<term>Dictionary learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human action recognition is an important branch among the studies of both human perception and computer vision systems. Along with the development of artificial intelligence, deep learning techniques have gained remarkable reputation when dealing with image categorization tasks (e.g., object detection and classification). However, since human actions normally present in the form of sequential image frames, analyzing human action data requires significantly increased computational power than still images when deep learning techniques are employed. Such a challenge has been the bottleneck for the migration of learning-based image representation techniques to action sequences, so that the old fashioned handcrafted human action representations are still widely used for human action recognition tasks. On the other hand, since handcrafted representations are usually ad-hoc and overfit to specific data, they are incapable of being generalized to deal with various realistic scenarios. Consequently, resorting to deep learning action representations for human action recognition tasks is eventually a natural option. In this work, we provide a detailed overview of recent advancements in human action representations. As the first survey that covers both handcrafted and learning-based action representations, we explicitly discuss the superiorities and limitations of exiting techniques from both kinds. The ultimate goal of this survey is to provide comprehensive analysis and comparisons between learning-based and handcrafted action representations respectively, so as to inspire action recognition researchers towards the study of both kinds of rep-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human perception is collectively the organization, identification and interpretation of feelings that humans acquire from the surrounding environment <ref type="bibr">[1]</ref>. The investigation of human perception systems has been a challenging topic in many modern science subjects, including psychology <ref type="bibr">[2]</ref>, cognitive science <ref type="bibr">[3]</ref>, neuroscience <ref type="bibr" target="#b4">[4]</ref> and biology <ref type="bibr">[5]</ref>. At a low level, as one of human perceptions, the human vision system can receive a series of observations on an agent's body movements. Such observations are then passed to an intermediate level of human perception system, where predictions of movement categories can be made, e.g., running, waving hand, and jumping. Humans' daily activities are based on signals acquired from a large number of visual perceptions. Imagine that you are now playing basketball with another player on the play-ground. He is dribbling the ball in front of you while you are trying to defend him. At the lowest level of your visual perception system, most details of the other player's movements are perceived, e.g., his legs are straight, or his left hand is up and his right hand is down. At an intermediate level, you can tell that he is standing straightly and dribbling with his left hand. At the highest level, more advanced signals, such as he is more likely to pass the ball in the following move since his legs are not in a speeding up position, can be received based on intermediate perceptions. While one of the most important tasks of computer vision is to mimic the way how humans perceive the surrounding environment and make predictions accordingly, some existing computer vision technologies are biologically inspired, such as the Convolutional Neural Network (CNN) <ref type="bibr" target="#b6">[6]</ref>, which has gained a remarkable reputation for image-based categorization tasks in terms of performance. Similar as image classification, the ability of correctly recognizing human actions is a basic component of human perception system. In order to develop a robust action recognition system that can achieve a similar level of human performance, computer vision researchers have paid significant efforts in the past decades. Unfortunately, due to the challenging issues, such as high environment complexity and high intra-class action variations, what we can achieve today is still far from what a mature human perception sys-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>tem could do. Since the 1980s, when the research field of action recognition first drew the attention of computer science researchers, action representations mainly rely on statistics of gradients <ref type="bibr" target="#b7">[7]</ref>, combinations of global filters <ref type="bibr" target="#b8">[8]</ref>, depth images and skeletons <ref type="bibr" target="#b9">[9]</ref>, etc, which can be together referred as handcrafted features. Given the earlier stated importance of biologically inspired learning features, a growing number of learning-based video representations are altering the dominant position of old fashioned handcrafted features. In this survey, we aim to provide a comprehensive investigation of existing action representation and recognition approaches. In the remaining part of this section, we provide discussions on the scope of action recognitionrelated technologies that are covered in this survey and how the contents of this survey are structured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Scope of study</head><p>Discussions in the survey concentrate on the study of action representation, which is seemingly a narrow investigation scope, but essentially is the core of action recognition. The term "action" is always confused with similar terms "gesture", "interaction" and "activity". To clarify, action is defined as intentional, purposive, conscious and subjectively meaningful activity, where the above stated four terms can be associated with a ascending order of complexity levels: "gesture", "action", "interaction" and "activity" <ref type="bibr" target="#b10">[10]</ref>. Even though this survey mainly focuses on representation techniques which are based on action-related inputs, relevant representation methods (e.g., gesture representations), which are versatile for actions, are also covered. In the literature, there are several existing surveys on the topic of vision-based action recognition. These surveys are either structured following different taxonomies or emphasizing on different coverage areas. For example, both surveys by Aggarwal and Ryoo <ref type="bibr" target="#b10">[10]</ref> and Cheng et al. <ref type="bibr" target="#b11">[11]</ref> follow the taxonomy that divides action recognition approaches into single-layered approaches and hierarchical approaches; Moeslund et al. <ref type="bibr" target="#b12">[12]</ref> and Poppe <ref type="bibr" target="#b13">[13]</ref> follow the hierarchy of action primitive, action and activity; in an earlier work of Aggarwal and Cai <ref type="bibr" target="#b14">[14]</ref>, human motion analysis is discussed from three subtopics, which are 1) human body parts-based motion analysis, 2) moving human tracking and 3) image sequences-based human recognition. From another perspective, previous studies investigate the action recognition problem with different interest parts. For example, Cedras and Shah <ref type="bibr" target="#b15">[15]</ref> mainly study motion-based action recognition approaches; Gavrila <ref type="bibr" target="#b16">[16]</ref> investigate human body and hands tracking, and tracking-based recognition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT</head><p>approaches. A more comprehensive summary of taxonomies and interests of studies based on previous relevant survey works are provided in Table <ref type="table" target="#tab_0">1</ref>. Due to the fact that human action is a collection of various human body movements, an obvious trend can be discovered from Table <ref type="table" target="#tab_0">1</ref> that a significant number of work focuses on the investigation of human motions.</p><p>In recent years, the advent of large-scale training data has enabled CNN to significantly boost the performance of image classification on challenging tasks, such as ImageNet <ref type="bibr" target="#b17">[17]</ref>. Such success has inspired researchers to follow a similar methodology to extract robust representations from action videos, so that an increasing number of methods that aim to utilize learning-based representations for action recognition have been proposed. While there exists a gap between the coverage of previous surveys on action recognition and the recently emerged learning-based action representations, our work only focuses on action representations and covers both handcrafted and learningbased approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Survey structure</head><p>We structure this survey based on two main approaches of action representations, which are handcrafted approaches and learning-based approaches. In Section 2, we review the recent advancements of handcrafted action representations from four subcategories, including spatial-temporal volume-based approaches, depth image-based approaches, trajectory-based approaches and global approaches, where a discussion is given towards the end of Section 2 to summarize the cons and pros of listed handcrafted approaches. In Section 3, we provide a comprehensive investigation of existing learning-based action representation approaches, including both non-neural network-based approaches and neural network-based inspired approaches, where the focus is allocated to the latter. For non-neural network-based approaches, we selectively list genetic programming-based action representations and dictionary learning-based action representations, and for neural network-based approaches, a finer taxonomy is employed to review these approaches from the aspects of static frames-based approaches, frame transformations-based approaches, handcrafted features-based approaches, 3D CNN-based approaches and hybrid models, where certain overlapping may exist between these taxonomies. A discussion and comparison over both handcrafted action representations and learning-based action representations is also given towards the end of Section 3. Finally, a conclusion is given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surveys</head><p>Taxonomy Interest of study Cheng et al. <ref type="bibr" target="#b11">[11]</ref> Single-layered approaches and hierarchical approaches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handcrafted approaches</head><p>Moeslund et al. <ref type="bibr" target="#b12">[12]</ref> Hierarchy of action primitive, action and activity Human motion capture and analysis Aggarwal and Cai <ref type="bibr" target="#b14">[14]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Handcrafted action representations</head><p>Handcrafted action representations have been taking the dominated position along with the developments of action recognition. Before the emergence of deep learning approaches, the most popular action recognition frameworks follow an old-fashioned pipeline, which first extracts local statistics from both spatial saliences and action motions, then combines these local statistics into video-level representations and feeds to discriminative classifiers (such as Support Vector Machines (SVM) <ref type="bibr" target="#b18">[18]</ref>). The low-level features are normally built on the pixel-level, and are carefully designed to deal with challenging issues, such as occlusions <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref> and viewpoint changes <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>. In the remaining part of this section, we review and summarize some popular handcrafted action features from the following aspects: 1) spatial-temporal volume-based approaches, 2) skeleton-based approaches, 3) trajectory-based approaches and 4) global approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spatial-temporal volume-based approaches</head><p>In those early ages of the time when action recognition problems start attracting computer vision researchers' attentions, the most popular way of developing an action recognition follows the common trend in object recognition that establishes the categorization frameworks based on detected spatio-temporal interest points <ref type="bibr" target="#b26">[26]</ref>. Significant attempts are paid to develop effective detectors, including <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. While the majority of early interest point detectors are based on the extensions of scale invariant feature transformation (SIFT) <ref type="bibr" target="#b30">[30]</ref>-like 2D interest point detectors, Dollar et al. <ref type="bibr" target="#b31">[31]</ref> propose an alternative method of characterizing 3D cuboids of spatio-temporal volumes surrounding each detected 3D interest point, where interest points can be found by either detecting corners <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> in the spatiotemporal space or using the Laplacian of Gaussian (LoG) <ref type="bibr" target="#b34">[34]</ref> to obtain the local maximal response. Low-level action features, such as histograms of optical flows (HoF) <ref type="bibr" target="#b35">[35]</ref>, histograms of oriented gradients (HoG) <ref type="bibr" target="#b36">[36]</ref>, 3D extended HOG (HoG3D) <ref type="bibr" target="#b7">[7]</ref>, are then extracted from those sparsely detected spatial-temporal volumes and projected to video-level representations through coding schemes, such as the bag-of-words (BoW) model <ref type="bibr" target="#b37">[37]</ref>, sparse coding (SC) <ref type="bibr" target="#b38">[38]</ref> or locality-constrained linear coding (LLC) <ref type="bibr" target="#b39">[39]</ref>.</p><p>While the sparse spatial-temporal volume approaches are technically sound and getting widely popularized, some later works suggest that improved performance can be achieved by adopting densely sampled spatial-temporal vol-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T umes. For example, Wang et al. <ref type="bibr" target="#b40">[40]</ref> compare the performance of dense action representations and sparse representations in a local action feature evaluation work, where the BoW-based densely sampled approaches achieve better performance when either using HoG/HoF features or HoG3D features. Wainland et al. <ref type="bibr" target="#b19">[19]</ref> also propose a HoG-based dense action representation that achieves outstanding performance when dealing with the occlusion problem in action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Depth image-based approaches</head><p>Due to the emergence and popularization of depth cameras <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref> tasks that are relevant to pose estimation have been significantly simplified. A remarkable work that has to be mentioned is the real-time single imagebased body part estimation algorithm <ref type="bibr" target="#b9">[9]</ref>, which is also the core technique that has been adopted by the human-machine interaction system of one of those best-seller gaming machines, Xbox, based on the Kinect depth camera <ref type="bibr" target="#b44">[44]</ref>. Even that the work introduced in <ref type="bibr" target="#b9">[9]</ref> is not directly applied to action recognition tasks, it has been the basis of the following depth input-based action recognition approaches. While previous depth image-based approaches suffer from difficulties in either the efficiency or the re-initialization precess, the proposed method can achieve fast and robust body part estimations based on single images and consumer hardwares. The body part estimation task is treated as a depth image pixel-level classification task, where the computational efficiency is mainly guaranteed by the utilization of random decision forests. Reliable body part proposal that describe positions of 3D skeleton joints can be obtained at the output end of the algorithm. An illustration of how human skeletons can be obtained based on depth images is given inf Figure . 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><note type="other">ACCEPTED MANUSCRIPT</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Trajectory-based approaches</head><p>Action trajectories can be computed by tracking human body joints along the action videos. Over the recent years, trajectory-based action representations have been proved as the most successful handcrafted shallow representations. Wang et al. <ref type="bibr" target="#b45">[45]</ref> introduce a densely sampled rich feature that integrates HOG, HOF and motion boundary histogram (MBH) <ref type="bibr" target="#b46">[46]</ref>, where trajectories are obtained by tracking densely sampled points using optical flow. Since the global smoothness constraints are imposed to the trajectories, the resulting trajectories are much more robust. After the emergence of the dense trajectories-based action representation, a large amount of attempts have been paid, targeting on further improve the action recognition performance with more advanced trajectories-based approaches. Along with the improved performance when incorporating with a high level of density</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>of trajectories within the video, high computational costs can be naturally anticipated in the end. In order to alleviate the computation complexity, Vig et al. <ref type="bibr" target="#b47">[47]</ref> employ saliency-maps to extract salient regions within the frames where actions are performed. Based on the saliency-maps derived from human eye movements, a significant amount of dense trajectories can be discarded, while the action recognition performance can still remain at a high level. Improved performance over <ref type="bibr" target="#b45">[45]</ref> can be even observed when pruning proportions that vary between 20% ∼ 50%. In order to enhance action representations against camera movements, Jiang et al. <ref type="bibr" target="#b48">[48]</ref> utilize both local and global reference points that operate on the top of action trajectories to characterize action motions. Similarly, by investigating motions generated by camera movements, Wang et al. <ref type="bibr" target="#b49">[49]</ref> further improve the performance of their previous dense trajectory work <ref type="bibr" target="#b45">[45]</ref> by differentiating motions caused by human movements from motions caused by camera movements, where camera motions can be estimated by matching feature points across different frames using the speeded up robust features (SURF) <ref type="bibr" target="#b50">[50]</ref> and dense optical flows <ref type="bibr" target="#b35">[35]</ref>. Just like the dense trajectories <ref type="bibr" target="#b45">[45]</ref>, the improved dense trajectories <ref type="bibr" target="#b49">[49]</ref> soon become the most popularized action representations among all handcrafted approaches. On the top of the improved dense trajectory features, Peng et al. <ref type="bibr" target="#b51">[51]</ref> further build a multi-layer stacked fisher vector (FV) <ref type="bibr" target="#b52">[52]</ref> approach that stacking FVs obtained from the improved dense trajectory features within sub-volumes to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Global approaches</head><p>Almost all the above mentioned approaches operate within a local video volume. On the contrary, there are also some global approaches that extract action representations from the holistic action videos. While local approaches can well preserve the local structures within video volumes and being insensitive to partial occlusions, the main advantage of global approaches, on the other hand, is to well capture the global structure of actions. Also, since global approaches normally take the whole action video as the framework input and perform pooling or filtering on the video pixel level, it always results in a relatively simple architecture and thus requires less computational costs. Zhen et al. <ref type="bibr" target="#b53">[53]</ref> propose a spatio-temporal steerable pyramid (STSP) action representation. By decomposing spatio-temporal volumes into bandpassed sub-volumes, spatio-temporal patterns can present in multiple pyramid scales. Three-dimensional separable steerable filters are then employed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>to the band-passed sub-volumes, and the outputs of these filters are squared, summed and max-pooled to generate video-level action representations. Shao et al. <ref type="bibr" target="#b54">[54]</ref> present a spatio-temporal Laplacian pyramid coding (STLPC) method for obtaining holistic action representations. STLPC decomposes each video sequence into a set of band-pass-filtered components and localizes spatio-temporal pyramid features that reside at different scales, so that the motion information can be effectively encoded. After the bandpass-filtering stage, a bank of 3D 3D-Gabor filters <ref type="bibr" target="#b55">[55]</ref> is employed to each level of the Laplacian pyramid, where max pooling is employed to each filter band over spatio-temporal neighborhoods in order to capture both spatial structure and temporal motion information.</p><p>Some learning-based approaches <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b58">[58]</ref> also share the characteristics as the above mentioned global approaches, e.g., inputting the whole video sequence and capturing the global structure, however, we exclude these learning-based approaches from this subsection and place them in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Discussions</head><p>Along with the developments of handcrafted action features, previous approaches pay most efforts on capturing the motion information embedded in sequential body movements. The current successes of handcrafted features are achieved step-by-step by a wide range of techniques, such as optical flows, MBH, HOG, HOF and dense trajectories. So far, the most effective action feature is the IDT <ref type="bibr" target="#b49">[49]</ref> and its extended stacking version <ref type="bibr" target="#b51">[51]</ref> that works along with fisher vectors. The successes of handcrafted are also impacting learning-based action representations. Some existing learning-based action representation is built on handcrafted features. On the other hand, handcrafted action features also have some limitations. Since the most effective existing handcrafted features are local features, and follow the densely sample strategy (such as IDT), one major limitation for these approaches is the high computation complexity that are unavoidable in both the training and testing phases. Such a limitation can hinder handcrafted action features from many real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning-based action representations</head><p>While handcrafted action features normally operate at video pixel levels and measure the low-level statistics of either spatial body shapes or temporal motions, recently, more advanced approaches that either operate on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>the top of handcrafted action features or establish end-to-end action recognition frameworks from the pixel-level to action categories have been proposed. We summarize existing learning-based action representations as non-neural network-based representations and neural network-based representations. Intuitively, the difference with non-neural network-based approaches and neural network-based approaches is that the latter is designed to mimic the way of how humans observe the world from a biological perspective. As introduced in the Section 1, deep learning approaches, such as CNN, consist of multiple layers, where these layers are expected to function in a similar manner as neural layers of the human perception system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Genetic programming-based approaches</head><p>Genetic programming (GP) <ref type="bibr" target="#b59">[59]</ref>, as an evolutionary method, has been employed to a wide range of visual categorization tasks. Following the Darwinian principle of natural selection to automatically search a space of possible solutions without any prior knowledge, GP relies on a natural and random process that escapes traps by which handcrafted approaches may be captured. In a GP algorithm, a group of primitive operators is first employed to randomly assemble computational programs as initialization. Then, evolutions are performed over the population using either crossover or mutation strategies through reproduction with single or pair parents chosen stochastically <ref type="bibr" target="#b60">[60]</ref>. Liu et al. utilize a population of primitive 3D operators, such as 3D-Gabor filter <ref type="bibr" target="#b55">[55]</ref> and wavelet <ref type="bibr" target="#b61">[61]</ref>, to evolve the motion features from both colors and optical flow fields. The GP fitness function that calculates the average cross-validation classification error using a SVM on the training data is employed in the evolutionary architecture. By finishing the entire evolution procedure, the selected solution is a set of cascaded operator that can directly perform on action sequences, which results in an efficient feature extraction step in the querying phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dictionary learning-based approaches</head><p>Dictionary learning is a popular type of representation learning method, which in most cases refers to learning sparse representations of the input data in the form of a linear combination of basis dictionary atoms. Since researchers find that the sparse representation of signals are particularly effective when dealing with data categorization tasks, dictionary learning techniques have been widely applied in a wide range of computer vision applications, including image classification, saliency detection, action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT</head><p>The well known Bag-of-Words (BoW) model <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref> is based on a particular type of dictionary learning method, which, instead of using a combination of basis dictionary atoms, assigns each input signal to a single basis dictionary atom. The BoW model is widely used by local approaches to generate global data representations, thus the BoW model can be employed by the majority approaches introduced in Section 2.1, 2.2 and 2.3. In order to obtain more discriminative action representations through sparse coding, Guha et al. <ref type="bibr" target="#b64">[64]</ref> propose to learn sparse representation for action recognition and show improved performance over the BoW model. The classical objective function for sparsity-based dictionary learning contains a reconstruction error term and a regularization term, where the later penalizes on the number of selected basis dictionary atoms using l 0 -norm or l 1 -norm. Beyond the classical sparsitybased dictionary learning approaches, some variants are proposed to deal with advanced requirements, such as enforcing discriminative power to the learned sparse codes and minimizing cross-domain discrepancies for transfer learning tasks. Zheng et al. <ref type="bibr" target="#b65">[65]</ref> learn a cross-view dictionary pair over actions captured from different observation points, and encode actions within each specific view with the corresponding dictionary, so that the encoded action representations can be view-invariant. Zhu and Shao <ref type="bibr" target="#b66">[66]</ref> present a weakly-supervised dictionary learning approach to adapt knowledge of one action dataset to the other action dataset. In addition to the commonly used reconstruction error term, the dictionary learning function has a discriminative term and a cross-domain discrepancy term, so that the cross-domain smoothness property can be guaranteed in the learned action codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Neural network-based approaches</head><p>The main focus of this work is neural network-based features, which will be explicitly discussed in the remaining part of this section. As a popular biologically inspired technique nowadays, deep learning is one machine learning algorithm that attempts to model high-level abstraction of data using hierarchical structures. In contrast to handcrafted features, deep learning performs more intellectual leaning and contains hierarchical feature extraction layers that contain much more trainable parameters than shallow architectures, e.g., kernel machines <ref type="bibr" target="#b67">[67]</ref>. Based on the success of applying deep learning techniques to image-level categorization tasks, some recent works have been using similar learning-based representations for action recognition, where these works are summarized according to the following directions: 1) learning from video frames, 2) learning from frame transformations, 3) learning from hand-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>crafted features, 4) three-dimensional convolutional networks and 5) hybrid models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Learning from static frames</head><p>While there are several existing well established deep learning frameworks for image feature extraction, e.g., OverFeat <ref type="bibr" target="#b68">[68]</ref> and Caffe <ref type="bibr" target="#b69">[69]</ref>, applying any of these frameworks to frames within an action video and extract action features at the frame-level can be a natural and the most straightforward option.</p><p>Ning et al <ref type="bibr" target="#b70">[70]</ref> decompose the video-based analysis of embryos development problem into frame-level 2D images, and apply two-dimensional CNN to various stages (from fertilization to four-cell stage) of the development process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Learning from frame transformations</head><p>A Restricted Boltzmann Machine (RBM) <ref type="bibr" target="#b71">[71]</ref> is a generative stochastic artificial neural network that can provide a "deep architecture" by successively composing several RBMs. Based on RBM, Taylor et al. <ref type="bibr" target="#b72">[72]</ref> propose an unsupervised approach for learning spatio-temporal features (STF) using a gated Restricted Boltzmann Machine (GRBM)-based <ref type="bibr" target="#b73">[73]</ref> convolutional architecture <ref type="bibr" target="#b6">[6]</ref> to extract motion-sensitive action features from neighboring image pairs. Specifically, the GRBM architecture is first proposed by Memisevic and Hinton <ref type="bibr" target="#b6">[6]</ref> to describe the probabilistic model of learning rich and distributed representations of image transformations. Such a generative model tries to predict the next frame image in a stream of observations based on the current frame, so as to perform "mapping" between both frames, and subsequently extract stream features based on frame transformations. By extending the GRBM model to image patches at identical spatial locations in sequential frames, GRBM can naturally capture the transformations of successive image pairs. In order to avoid the limitations of training GRBMs on isolated patches as in <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b74">[74]</ref>, Taylor et al. <ref type="bibr" target="#b72">[72]</ref> extend a GRBM to a convolutional GRBM (convGRBM) by incorporating with the convolutional architecture <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b75">[75]</ref>, where weights at multiple locations within an image are shared by a convGRBM. The convGRBM operates in a multi-stage architecture. At the lowest layer, convGRBM extracts motion-sensitive features from every neighboring frame pairs. At the intermediate layer, spatio-temporal cues are captured by 3D spatio-temporal filters, which are extended from the 2D spatial filter by Jarrett et al. <ref type="bibr" target="#b76">[76]</ref>. After performing normalization,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>average spatial pooling and an additional max pooling in the temporal dimension, action representations can be obtained by the fully-connected layers and action label is at the topmost layer (softmax). The low layer conGRBM is trained unsupervised and separately with upper layers, where backpropagations are performed on upper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Learning from handcrafted features</head><p>Another natural choice of computing learning-based action representations is to perform learning on the top of handcrafted features. In fact, the majority of early attempts that aim to address action recognition problem using learning-based representations lie in this category. This subsection needs to be distinguished from the previous section, "handcrafted representationsbased action recognition", because the former discusses how the learning network can be established based on existing low-level features while the latter discusses how low-level features can be extracted from raw pixel-level video data. Kim et al. <ref type="bibr" target="#b77">[77]</ref> propose a modified convolutional neural network (MCNN)-based action feature extraction and classification framework. At the low-level, action information are captured by handcrafted features. Specifically, When an action performed by an agent is presented in a 3D video, a sequence of the agent's outer boundary, which can be considered as a 2D contour in the spatial plane, generates a spatio-temporal volume, where the outer boundary information are extracted using three-dimensional Gabor filters <ref type="bibr" target="#b78">[78]</ref>. Thus, in each spatio-temporal volume, actions are presented in a view-invariant form. In order to diminish the post-normalization location variances, the extended three-dimensional convolutional neural network is applied to each spatio-temporal volume, and subsequently action features can be extracted from a set of hierarchical layers based on the agent's outer boundary features. A 3DCNN consists two convolution layers and two subsampling layers, where each convolution layer or subsampling layer consists of two sub-layers. The extracted features are then fed into a discriminative classification model <ref type="bibr" target="#b79">[79]</ref>.</p><p>Jhuang et al. <ref type="bibr" target="#b80">[80]</ref> present a biologically inspired system (BIS) that utilizes a feedforward hierarchy of spatio-temporal feature detectors of increasing complexity to measure motion-direction sensitive units, which lead to position-invariant spatio-temporal feature detectors. Motivated by the scale and position invariant features <ref type="bibr" target="#b81">[81]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b83">[83]</ref>, a vector of scale and position invariant features is obtained by computing a global max for each feature map at the top of the hierarchy. The main weakness of this approach is that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>it requires carefully handcrafted spatio-temporal feature detectors. Wu and Shao <ref type="bibr" target="#b84">[84]</ref> propose a hierarchical parametric networks (HPN) based on skeleton features. By replacing the RBM in <ref type="bibr" target="#b85">[85]</ref> with a multilayer network, the HPN approach can serve as a better model for estimating emission probability of hidden Moarkov models <ref type="bibr" target="#b86">[86]</ref> and achieve improved performance over other well established methods. Similar as <ref type="bibr" target="#b9">[9]</ref>, the approach introduced in [84] also operates on depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Three-dimensional convolutional networks</head><p>The first attempt that aims to develop a three-dimensional convolutional neural networks (3D CNN) and performs 3D convolution along both spatial and temporal dimensions at the pixel level is introduced in <ref type="bibr" target="#b56">[56]</ref>. By applying multiple distinct convolutional operations at identical input locations, and subsequently extracting features from multiple information channels, action representations obtained by such a 3D CNN approach contain a variety of information. In a 2D CNN <ref type="bibr" target="#b6">[6]</ref>, convolution is performed in the spatial domain, where features are extracted from neighboring units that share the same feature map in the previous layer. On the other hand, convolution is performed in both spatial and temporal dimensions using 3D cubes, which are generated by stacking multiple contiguous frames. The proposed architecture of 3D CNN consists of 7 layers including the input layer, which is hardwired to three convolution layers and two subsampling layers in with an alternating order. The last layer consists of 128 feature maps, and is fully connected to all feature maps in the previous layer. The main premise of the 3D CNN architecture comes from the feed-forward nature, which enables efficient feature extraction in the recognition phase. An illustration of the 3D CNN architecture used in <ref type="bibr" target="#b56">[56]</ref> is given in Figure . 3.</p><p>Among the first batch of attempts that address action representations with learning-based methods, Le et al. <ref type="bibr" target="#b57">[57]</ref> propose the hierarchical invariant spatio-temporal (HIST) action features. The feature learning framework is established based on the independent subspace analysis (ISA) <ref type="bibr" target="#b87">[87]</ref>, which is a two-layered network and normally used for extracting features from 2D images. Since the ISA training process is less efficient, especially when handling large-scale video data, the migration of ISA to the video domain is assisted by the principle component analysis (PCA) <ref type="bibr" target="#b88">[88]</ref>. The complete HIST framework consists of several ISAs, where each ISA is firstly trained on small input patches (flattened vectors from sequences of image patches) and then propagate responses to the next-layer ISA with reduced dimensions by PCA. Inherit from the unsupervised learning property of ISA, HIST also operates in an unsupervised manner over action videos, so that it can leverage the massively available unlabeled online video data. Baccouche et al. <ref type="bibr" target="#b58">[58]</ref> present sequential deep learning (SDL) approach that adopts a similar strategy of extending CNN to a three-dimensional scenario as in Kim et al. <ref type="bibr" target="#b77">[77]</ref>. Such an extension is rather straightforward in <ref type="bibr" target="#b58">[58]</ref>, since the two-dimensional convolutions are simply replaced by threedimensional convolutions. On the other hand, <ref type="bibr" target="#b58">[58]</ref> differs from <ref type="bibr" target="#b77">[77]</ref> in a way that the CNN architecture directly operates on raw video pixels in the former scenario while the CNN framework is established on the top of handcrafted low-level features in the latter. The construction of the CNN architecture in <ref type="bibr" target="#b58">[58]</ref> is also different from those in <ref type="bibr" target="#b77">[77]</ref> by following the order of two alternating convolutional layers, a rectification layer, a sub-sampling layer, a second alternating convolutional layer, a second sub-sampling layer, a third convolutional layer and two neuron layers. The training process follows a standard online backpropagation with momentum algorithm <ref type="bibr" target="#b6">[6]</ref>. Once action features are extracted using the three-dimensional CNN architecture, a sequential action labeling scheme is utilized. Instead of utilizing small sized spatiotemporal volumes to generate three-dimensional regions for CNN learning, Baccouche et al. <ref type="bibr" target="#b58">[58]</ref> capture the features' temporal evolution over time by adapting CNN to sequential data, where the recurrent neural networks (RNN) <ref type="bibr" target="#b89">[89]</ref> with one hidden layers of Long Short-Term Memory (LSTM) <ref type="bibr" target="#b90">[90]</ref>. Apart from the LSTM model <ref type="bibr" target="#b58">[58]</ref>, some other RNN-based approaches are proposed for human action recognition tasks, including the hierarchical recurrent neural network for skeleton-based representation <ref type="bibr" target="#b91">[91]</ref> and the dif-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>ferential recurrent neural networks <ref type="bibr" target="#b91">[91]</ref>. However, when comparing with the LSTM model, regular RNN is incapable of capturing long-term dependencies between frames, so that these models are theoretically inferior. Karpathy et al. <ref type="bibr" target="#b92">[92]</ref> conduct a comprehensive study that provides three CNN-based action recognition approaches and extensively evaluates all these approaches. The main focus of this study is to investigate the best approach to incorporate with the motion information for recognizing actions, and how much improvement the motion information can boost from applying CNNs on static video frames. Based on extensive investigations on a new large-scale action video dataset, which consists of 1 million YouTube videos belonging to 487 classes, and many other real-world on-line videos, Karpathy et al. <ref type="bibr" target="#b92">[92]</ref> discover that most videos normaly present in a highly inconsistent nature, and thus cannot be easily processed with fixed-sized architectures. Consequently, the network is designed to learn spatio-temporal features by connecting several contiguous frames in time and plugging in the network. In order to better analyze the benefits that come from the motion information, Karpathy et al. <ref type="bibr" target="#b92">[92]</ref> present three CNN-based learning strategies: Early Fusion, Late Fusion and Slow Fusion. Illustrations of these fusion strategies are given in the reprinted Figure . 4 from <ref type="bibr" target="#b92">[92]</ref>. Based on the single frame architecture, which is equivalent to applying CNNs to 2D images, Late Fusion enforces two separate CNNs on two apart frames (e.g., from a distance of 15 frames) and connects both networks in the first fully connected layer, so that action motion characteristics can be captured at a global level. Early Fusion, on the other hand, modifies the 2D single-frame based convolution window to include the temporal dimension, and feed these 3D cubes to the first convolutional layer. The Early Fusion strategy is equivalent to the work <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b58">[58]</ref>, in terms of how the motion information are captured. As a compromise between Early Fusion and Late Fusion, Slow Fusion (SFCNN) progressively connects adjacent frames from convolutional layers in both spatial and temporal dimensions. These three types of fusion strategies can also be generalized to other learning-based action representation approaches. Details of such a taxonomy will be given in the discussion subsection below. Due to the high computational requirements of CNN-based approaches, especially when being extended to incorporate the additional dimension, efforts have also been paid on how to speedup the training process in <ref type="bibr" target="#b92">[92]</ref>, where a modified architecture that adopts both the context stream and the fovea stream is used in the implementation. Specially, in the context stream, the input video clips are down sampled to half of the original spatial resolution,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T while in the fovea stream, the centering regions of the original frame are cropped at an identical size of the down sampled resolution as in the context stream. The activations from both streams are concatenated and fed into the first fully connected layer for generating the final action representation. Interestingly, experimental results suggest that performing CNNs on individual static video frames achieves similar performance as performing CNNs on a stack of frames, which suggest that motion information is not well captured by the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hybrid models</head><p>Different from the two-stream architecture of the multiresultion strategy in <ref type="bibr" target="#b92">[92]</ref> that aims to speed up the training process with such a architecture design, a two-stream convolutional networks approach is recently proposed by Simonyan and Zisserman <ref type="bibr" target="#b93">[93]</ref> to learn action representations by decomposing action videos into both spatial and temporal components. In stead of implicitly estimating the motion as in <ref type="bibr" target="#b92">[92]</ref> and <ref type="bibr" target="#b58">[58]</ref>, the temporal stream architecture takes a stack of optical flow displacement fields <ref type="bibr" target="#b94">[94]</ref> between several adjacent frames as inputs, and train the convolutional networks on the top of optical flows. Several configurations of convolutional networks are further proposed in <ref type="bibr" target="#b93">[93]</ref>, including the optical flow stacking, which uses dense optical flows of neighboring pairs of consective frames, the trajectory stacking, which samples the flow across several frames along the motion trajectories, and the bi-directional optical flow, which computes both the forward and backward directions of optical flows in either a dense manner or a trajectorybased sampling manner. Training the convolutional networks is conducted</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCEPTED MANUSCRIPT</head><p>in a multi-task learning setting <ref type="bibr" target="#b95">[95]</ref>, where the task is set to classify videos in both the HMDB-51 dataset <ref type="bibr" target="#b96">[96]</ref> and the UCF-101 dataset <ref type="bibr" target="#b97">[97]</ref> by equipping two softmax classifcation layers on the top of the last fully connected layer, so that one layer computes the HMDB-51 classification score and the other computes the UCF-101 classification score. The training procedures of the both streams are generally the same, and similar to the network architectures of <ref type="bibr" target="#b98">[98]</ref> and <ref type="bibr" target="#b99">[99]</ref>, where the temporal network can be considered as an adaptation of <ref type="bibr" target="#b100">[100]</ref> to the temporal domain. While the spatial network performs 2D convolutions on static action frames, the 3D convolution architecture is also similar to the architectures used in <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b58">[58]</ref> and <ref type="bibr" target="#b92">[92]</ref>. In terms of how convolutional networks are applied to the video data, the proposed two-stream approach can be considered as a hybrid model of learning from both handcrafted features and raw pixels.</p><p>Motivated by <ref type="bibr" target="#b93">[93]</ref> and the success of the shallow handcrafted video representation <ref type="bibr" target="#b45">[45]</ref>, Wang et al. <ref type="bibr" target="#b101">[101]</ref> propose a trajectory-pooled two-stream deep convolution descriptor (TDD). By adopting a similar network architecture as the two-stream learning architecture in <ref type="bibr" target="#b93">[93]</ref> and performing training on the combination of the HMDB-51 dataset <ref type="bibr" target="#b96">[96]</ref> and the UCF-101 dataset, the trained convolutional networks can be considered as generic feature extractors, which are then used to compute multi-scale convolutional feature maps from each video in the target dataset. Meanwhile, improved trajectories are computed using the method in <ref type="bibr" target="#b45">[45]</ref> from each action video. Trajectorypooling is then performed by pooling local convolutional network responses along the trajectories in the spatio-temporal space, where the final TDD descriptors are obtained from the pooling results. Compared to <ref type="bibr" target="#b93">[93]</ref>, <ref type="bibr" target="#b45">[45]</ref> inclines more towards the handcrafted feature side, since that in addition to the optical flows used when training the two-stream network, dense trajectories are utilized for pooling the deep convolutional descriptors. On the top of TDDs, Wang et al. <ref type="bibr" target="#b101">[101]</ref> further leverage the discriminative Fisher vector <ref type="bibr" target="#b102">[102]</ref> for encoding the TDDs, where their reported leading performance is achieved by early fusing TDD and improved dense trajectory features using the Fisher vector representations. However, since fisher vector representation is not used for obtaining the classification results in <ref type="bibr" target="#b93">[93]</ref>, there is no evidence to support the superiority in terms of network architectures of TDD over the original two-stream convolutional networks <ref type="bibr" target="#b93">[93]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T </p><note type="other">ACCEPTED MANUSCRIPT</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>The above mentioned learning-based action representations cover the majority of most popular approaches that are based on network architectures and published after 2010. Based on how learnings are performed over action videos, we put these approaches in multiple branches, including learning from static frames, learning from frame transformations, learning from handcrafted features, learning with three dimensional neural networks and learning with hybrid models. In fact, these taxonomies overlap with each other. For example, the works <ref type="bibr" target="#b93">[93]</ref> and <ref type="bibr" target="#b101">[101]</ref> are categorized as hybrid models, while their learning architectures also take handcrafted features as inputs; the unsupervised learning approach in <ref type="bibr" target="#b72">[72]</ref> also cooperates with the threedimensional convolutional network; and in addition to <ref type="bibr" target="#b72">[72]</ref>, the majority of above mentioned approaches include elements that aim to learn from action motions. Thus, some other taxonomies can be used for discriminating different learning-based action representations. We illustrate these expanded taxonomies in Figure <ref type="figure" target="#fig_4">5</ref>, and list the corresponding belongingness of learningbased action representations.</p><p>The majority of popular existing learning-based action representations are either directly applying CNN to video frames or variants of CNN for learning spatio-temporal features. Network architectures of some popular learning-based representations that are established based on CNN are listed in Table <ref type="table" target="#tab_3">3</ref>. For other learning-based methods <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b57">[57]</ref>, since the CNN architecture is not utilized in their learning frameworks, we do not list them in Table <ref type="table" target="#tab_3">3</ref>. The listed network architectures vary in the input data types, 65.4% Two-stream+SVM Simonyan and Zisserman <ref type="bibr" target="#b93">[93]</ref> 88.0% TDD+FV Wang et al. <ref type="bibr" target="#b101">[101]</ref> 90.3% TDD+IDT+FV Wang et al. <ref type="bibr" target="#b101">[101]</ref> 91.5% Feature pruning Liu et al. <ref type="bibr" target="#b109">[109]</ref> 71.2% HIST Le et al., <ref type="bibr" target="#b57">[57]</ref> YouTube 75.8% IDT+SFV Peng et al. <ref type="bibr" target="#b51">[51]</ref> 93.38% HOG/HOF + KM + SVM Wang et al. <ref type="bibr" target="#b40">[40]</ref> 47.4% VideoDarwin Fernando et al. <ref type="bibr" target="#b110">[110]</ref> Hollywood2 73.7% IDT+FV Wang et al. <ref type="bibr" target="#b45">[45]</ref> 64.3% STF Taylor et al. <ref type="bibr" target="#b72">[72]</ref> 46.6% STIP+BoW Kuehne et al. <ref type="bibr" target="#b96">[96]</ref> 23.0% Motionlets Wang et al. <ref type="bibr" target="#b105">[105]</ref> 42.1% DT+BoW Wang et al. <ref type="bibr" target="#b106">[106]</ref> 46.6% DT+MVSV Cai et al. <ref type="bibr" target="#b107">[107]</ref> 55.9% IDT+FV Wang et al. <ref type="bibr" target="#b45">[45]</ref> 57.2% IDT+HSV Peng et al. <ref type="bibr" target="#b108">[108]</ref> HMDB-51 <ref type="bibr" target="#b96">[96]</ref> 61.1% Two-stream+SVM Simonyan and Zisserman <ref type="bibr" target="#b93">[93]</ref> 63.2% TDD+FV Wang et al. <ref type="bibr" target="#b101">[101]</ref> 63.2% TDD+IDT+FV Wang et al. <ref type="bibr" target="#b101">[101]</ref> 65.9% VideoDarwin Fernando et al. <ref type="bibr" target="#b110">[110]</ref> 63.7% IDT+SFV Peng et al. <ref type="bibr" target="#b51">[51]</ref> 66.79%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT</head><p>layer numbers, layer components and orders of how different components are connected. So far, there does not exist a widely acknowledged network architecture for learning action representations. The SFCNN approach utilizes the maximum layer numbers among listed works, however, the proposed network achieves equivalent results as the network that operates on individual video frames, which suggest that motion information is not effectively learned in such a network. Also, based on the fact that the SFCNN achieves a significantly worse result than the best handcrafted shallow representations <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b51">[51]</ref>, SFCNN fails to demonstrate that a network with long cascaded layers can lead to better performance. The network architectures used in both <ref type="bibr" target="#b93">[93]</ref> and <ref type="bibr" target="#b101">[101]</ref> are based on the image visualizing and understanding network <ref type="bibr" target="#b99">[99]</ref>, and are perceptually effective since they achieve leading performance among learning-based action representations. Unfortunately, since the motion information is still captured by handcrafted features in both approaches, and the reported result on the spatial stream network is significantly worse than either the best handcrafted shallow representations or their proposed two-stream model, there is no proof that the improved performance in <ref type="bibr" target="#b93">[93]</ref> and <ref type="bibr" target="#b101">[101]</ref> come from the superiority of the network architecture. Unlike spatial CNNs, which can be trained using large-scale image data (such as ImageNet <ref type="bibr" target="#b17">[17]</ref>), most available datasets for action classification are still rather small, e.g., the UCF-101 dataset <ref type="bibr" target="#b97">[97]</ref>, which consists of 9.5K videos from 101 categories, and the HMDB-51 dataset <ref type="bibr" target="#b96">[96]</ref>, which consists of 3.7K videos from 51 categories. In order to increase the number of available videos for training, a straightforward option is to simply combine both datasets for training, however, this is not practical due to the intersection between the sets of classes. Consequently, recent approaches <ref type="bibr" target="#b93">[93]</ref>, <ref type="bibr" target="#b92">[92]</ref> seek more principled ways of combining existing action datasets using either multi-task learning <ref type="bibr" target="#b95">[95]</ref>, <ref type="bibr" target="#b111">[111]</ref>, <ref type="bibr" target="#b112">[112]</ref> or transfer learning <ref type="bibr" target="#b113">[113]</ref>, <ref type="bibr" target="#b114">[114]</ref>, <ref type="bibr" target="#b115">[115]</ref>. In the multitask learning setting of <ref type="bibr" target="#b95">[95]</ref>, the architecture of the convolutional network is modified so that 2 softmax classification layers can be cascaded to the top of the layer fully connected layer, where one soft layer handles the classification task of the HMDB-51 dataset while the other handles the classification task in the UCF-101 dataset. On the other hand, in the transfer learning setting employed by Wang et al. <ref type="bibr" target="#b101">[101]</ref>, the model is trained on the UCF-101 dataset, and extended to the HMDB-51 dataset for action feature extraction and action classification. So far, the largest available video dataset is the Sports-1M dataset <ref type="bibr" target="#b92">[92]</ref>, which consists of 1 million YouTube videos annotated with 487 categories. As claimed in <ref type="bibr" target="#b92">[92]</ref>, since the YouTube video IDs of UCF-101</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><p>M A N U S C R I P T A C C E P T E D M A N U S C R I P T videos are not available, there is no rigorous guarantee that the HMDB-51 dataset and UCF-101 dataset do not overlap with each other. Unfortunately, even by ruling out the dataset intersections between the Sports-1M dataset and the UCF-101 dataset, the model trained on the Sports-1M dataset is still not able to lead the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this survey, we provide a comprehensive study over the state-of-theart action representations, covering both handcrafted representations and learning-based representations. Due to the success of deep learning in image categorization tasks, increasing attentions have been paid on migrating the deep learning architectures from image-level visual categorization tasks to video-level action sequences recognition tasks. On the handcrafted representation side, we deliver both the foundations of classical action representations and recent advancements in action representations. So far, handcrafted action features have achieved remarkable performance on a variety of action recognition tasks and made significant contributions to the action recognition society. However, since the most effective handcrafted action features follow a densely-sampled local strategy, high computational complexity is unavoidable in both training and testing phases, which make these handcrafted approaches inapplicable for real-world applications. Learning-based approaches, on the other hand, can avoid such high computation by employing simple network architectures with learned parameters. With the rapid developments of learning-based action representations, the most effective learning-based approach can outperform the best performance achieved by handcrafted action features, though these learning-based approaches still rely on handcrafted features. However, the performance of pure learningbased approaches that directly from videos still fall far behind action recognition researchers' expectations. The reasons can be two folds: 1) unlike those successful cases in the image domain, the majority of existing learningbased approaches are not trained on large-scale data, which can result in the insufficiency of the networks' generalization ability; 2) in order to balance the computational costs, down-sampling strategies are widely utilized in learning-based action representations, where the information loss during the down-sampling procedures could cause different levels of performance degradation. In the future, we can anticipate the emergence of more advanced action representations that are able to achieve improved performance while </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of (a): the spatial-temporal interest points-based approach, and (b): the spatial-temporal volume-based approach. (reprint from Gilbert et al. [29] and Dollar et al. [31], respectively).</figDesc><graphic coords="8,130.19,118.79,365.30,183.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of how skeletons can be obtained based on depth images. (reprint from Shotton et al. [9]).</figDesc><graphic coords="9,203.27,118.19,219.50,155.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The 3D CNN architecture used in [56]. (reprint from Ji et al. [56]).</figDesc><graphic coords="17,130.19,119.03,365.66,125.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of different fusion strategies for incorporating with the temporal dimension (reprint from Karpathy et al. [92]).</figDesc><graphic coords="19,130.19,118.31,365.66,126.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Expanded taxonomies of learning-based action recognition representations.</figDesc><graphic coords="21,130.19,117.59,365.66,148.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of taxonomies and interests of studies based on previous relevant surveys.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons between state-of-the-art learning-based action repre-</figDesc><table><row><cell>sentations and handcrafted representations</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Dataset</cell><cell>Performance</cell></row><row><cell>STF Taylor et al. [72]</cell><cell></cell><cell>90.0%</cell></row><row><cell>SS (Short Sequence) Schindler and Gool</cell><cell></cell><cell>95.04%</cell></row><row><cell>[103]</cell><cell></cell><cell></cell></row><row><cell>BIS Jhuang et al. [80]</cell><cell>KTH [104]</cell><cell>91.70%</cell></row><row><cell>SDL Baccouche et al. [58]</cell><cell></cell><cell>94.39%</cell></row><row><cell>STIP+BoW Kuehne et al. [96]</cell><cell></cell><cell>43.9%</cell></row><row><cell>Motionlets Wang et al. [105]</cell><cell></cell><cell>63.3%</cell></row><row><cell>DT+BoW Wang et al. [106]</cell><cell></cell><cell>79.9%</cell></row><row><cell>DT+MVSV Cai et al. [107]</cell><cell></cell><cell>83.5%</cell></row><row><cell>IDT+FV Wang et al. [45]</cell><cell></cell><cell>85.9%</cell></row><row><cell>IDT+HSV Peng et al. [108]</cell><cell>UCF-101 [97]</cell><cell>87.9%</cell></row><row><cell>SFCNN Karpathy et al. [92]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Inputs and network architectures of some popular action feature learning approaches that are established on CNN. Layer numbers are reported by including the input layers for all methods. Definitions of notations used for describing network architectures are as follows: "H" denotes the hardwired layer, "C" denotes the convolutional layer, "N" denotes the normalization layer, "S" denotes the sub-sampling layer, "R" denotes the rectification layer, "P" denotes the pooling layer and "F" denotes the fully connected layer.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>Layer</cell><cell>Architecture</cell></row><row><cell></cell><cell></cell><cell>num-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ber</cell><cell></cell></row><row><cell>3D CNN Ji et al. [56]</cell><cell>Action videos</cell><cell>7</cell><cell>H⇒C⇒S⇒C⇒S⇒C⇒F</cell></row><row><cell>MCNN Kim et al. [77]</cell><cell>Action</cell><cell>5</cell><cell>H⇒C⇒S⇒C⇒S⇒</cell></row><row><cell></cell><cell>descriptors</cell><cell></cell><cell></cell></row><row><cell></cell><cell>obtained from</cell><cell></cell><cell></cell></row><row><cell></cell><cell>3D Gabor filter</cell><cell></cell><cell></cell></row><row><cell>SDL Baccouche et al.</cell><cell>Action videos</cell><cell>10</cell><cell>H⇒C×2⇒R×2⇒S×2⇒C⇒R</cell></row><row><cell>[58]</cell><cell></cell><cell></cell><cell>⇒S⇒C⇒N⇒N</cell></row><row><cell>SFCNN Karpathy et</cell><cell>Multiresolution</cell><cell>13</cell><cell>H⇒C⇒N⇒P⇒C⇒N⇒P⇒C</cell></row><row><cell>al. [92]</cell><cell>Action videos</cell><cell></cell><cell>⇒C⇒C⇒P⇒F⇒F</cell></row><row><cell>Two-stream Simonyan</cell><cell>Action videos</cell><cell>8</cell><cell>H⇒C⇒C⇒C⇒C⇒C⇒F⇒F</cell></row><row><cell>and Zisserman [93]</cell><cell>and optical</cell><cell></cell><cell></cell></row><row><cell></cell><cell>flows</cell><cell></cell><cell></cell></row><row><cell>TDD Wang et al.</cell><cell>Trajectory-</cell><cell>8</cell><cell>H⇒C⇒C⇒C⇒C⇒C⇒F⇒F</cell></row><row><cell>[101]</cell><cell>pooled action</cell><cell></cell><cell></cell></row><row><cell></cell><cell>videos and</cell><cell></cell><cell></cell></row><row><cell></cell><cell>optical flows</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Reference</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Daniel Schacter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><surname>Psychology</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Worth</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mirror neuron system and action recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Buccino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Binkofski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and language</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="370" to="376" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Brain signatures of meaning access in action word recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pulvermüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shtyrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ilmoniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="884" to="892" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audiovisual mirror neurons and action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Umiltà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nanetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fogassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gallese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental brain research</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="628" to="636" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Biology of progesterone action during pregnancy recognition and maintenance of pregnancy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in bioscience: a journal and virtual library</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1879" to="1898" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action mach a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Saudagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Namuduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Buckles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.05964</idno>
		<title level="m">Advances in Human Action Recognition: A Survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of advances in visionbased human motion capture and analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human motion analysis: A review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="428" to="440" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion-based recognition a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cedras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: A survey</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="98" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making action recognition robust to occlusions and viewpoint changes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Özuysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Viewpoint dependence in scene recognition</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Diwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Mcnamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="302" to="307" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning the viewpoint manifold for action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Babbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3d exemplars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">View invariance for human action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An efficient dense and scaleinvariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale invariant action recognition using compound features mined from dense spatio-temporal corners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A fast operator for detection and precise location of distinct points, corners and centres of circular features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Förstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gülch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intercommission Conference on Fast Processing of Photogrammetric Data</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<title level="m">Alvey vision conference</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Laplacian-of-Gaussian kernel: a formal analysis and design procedure for fast, accurate convolution and fullframe output</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sotak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="189" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Histograms of oriented optical flow and binet-cauchy kernels on nonlinear dynamical systems for the recognition of human actions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">/images/tex/484. gif&quot; alt=&quot;\ rm K&quot;¿-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>¡ img src=</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>British Machine Vision Conference</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real time motion capture using a single time-of-flight camera</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human pose estimation from a single view point, real-time range sensor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constrained optimization for human pose estimation from depth sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fujimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Microsoft kinect for Xbox 360</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<publisher>PC Mag. Com</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Space-variant descriptor sampling for action recognition based on saliency and eye movements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Trajectory-based modeling of human actions with motion reference points</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Action recognition by spatio-temporal oriented energies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="295" to="309" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatio-temporal Laplacian pyramid coding for action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="817" to="827" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mathematical description of the responses of simple cortical cells*</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marčelja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1297" to="1300" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Behavior Understanding</title>
		<imprint>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Genetic programming: an introduction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nordin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Francone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Morgan Kaufmann San Francisco</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Feature learning for image classification via multiobjective genetic programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1371" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Burrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Introduction to wavelets and wavelet transforms: a primer</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure, Word</title>
		<imprint>
			<date type="published" when="1954">1954</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised spectral dual assignment clustering of human actions in context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning sparse representations for human action recognition, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1576" to="1588" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cross-View Action Recognition via a Transferable Dictionary Pair</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Large-scale kernel machines</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caffe</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional Architecture for Fast Feature Embedding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Toward automatic phenotyping of developing embryos from videos</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delhomme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Barbano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1360" to="1371" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unsupervised learning of image transformations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning transformational invariants from natural movies</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Convolutional networks and applications in vision</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Human action recognition using a modified convolutional neural network, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1233" to="1258" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A weighted FMM neural network and its application to face detection</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multiclass object recognition using sparse, localized hmax features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Object recognition with features inspired by visual cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state Markov chains, The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<title level="m">Natural Image Statistics: A Probabilistic Approach to Early Computational Vision</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geladi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and intelligent laboratory systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="52" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning precise timing with LSTM recurrent networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<title level="m">A dataset of 101 human actions classes from videos in the wild</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Action Recognition With Trajectory-Pooled Deep-Convolutional Descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schüldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3d parts for human motion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4506</idno>
		<title level="m">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Multi-task linear discriminant analysis for multi-view action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Activity Recognition: Linking Low-level Sensors to High-level Intelligence</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Cross-domain activity recognition</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">We provide a comprehensive stud over the state-of-the-art action representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Deep learning-based representations are compared to handcrafted representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Pros and cons of current deep learning-based approaches are discussed</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
