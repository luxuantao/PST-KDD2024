<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cache Coherence Protocol and Memory Performance of the Intel Haswell-EP Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Molka</surname></persName>
							<email>danial.molka@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universit?t Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Hackenberg</surname></persName>
							<email>daniel.hackenberg@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universit?t Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Sch?ne</surname></persName>
							<email>robert.schoene@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universit?t Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wolfgang</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
							<email>wolfgang.nagel@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universit?t Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cache Coherence Protocol and Memory Performance of the Intel Haswell-EP Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICPP.2015.83</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge in the design of contemporary microprocessors is the increasing number of cores in conjunction with the persevering need for cache coherence. To achieve this, the memory subsystem steadily gains complexity that has evolved to levels beyond comprehension of most application performance analysts. The Intel Haswell-EP architecture is such an example. It includes considerable advancements regarding memory hierarchy, on-chip communication, and cache coherence mechanisms compared to the previous generation. We have developed sophisticated benchmarks that allow us to perform in-depth investigations with full memory location and coherence state control. Using these benchmarks we investigate performance data and architectural properties of the Haswell-EP microarchitecture, including important memory latency and bandwidth characteristics as well as the cost of core-to-core transfers. This allows us to further the understanding of such complex designs by documenting implementation details the are either not publicly available at all, or only indirectly documented through patents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND MOTIVATION</head><p>The driving force of processor development is the increasing transistor budget which is enabled by Moore's Law. Progress on the core level is still being made. However, most of the additional space is used to increase the core count of contemporary server processors. The peak performance scales linearly with the increasing core count. However, some resources are typically shared between all cores, for example the last level cache and the integrated memory controllers. These shared resources naturally limit scalability to some extend and can create bottlenecks. A profound understanding of these effects is required in order to efficiently use the cache hierarchy and avoid limitations caused by memory accesses.</p><p>Contemporary multi-socket x86 servers use point-to-point connections between the processors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. The available memory bandwidth scales with the number of processors as each processor has an integrated memory controller. Unfortunately, the distance between the requesting core and the memory controller influences the characteristics of memory accesses. Accesses to the local memory are generally faster than remote memory accesses which require additional transfers via the inter-processor connections. This Non-Uniform Memory Access (NUMA) behavior also affects the performance of parallel applications. Typically, each processor is a single NUMA node. However, processors with multiple NUMA domains already exist in the form of multi-chip-modules, e.g., the AMD Opteron 6200 series. Haswell-EP supports a Cluster-on-Die mode <ref type="bibr" target="#b2">[3]</ref> that splits a single processor into two NUMA domains.</p><p>The distributed caches in Haswell-EP based systems are kept coherent using a snooping based protocol <ref type="bibr" target="#b1">[2]</ref>. Two snooping modes-source snooping and home snooping-are available. An in-memory directory as well as directory caches are used to mitigate the performance impact of the coherence protocol. In this Paper we analyze the impact of the coherence protocol on the latencies and bandwidths of core-to-core transfers and memory accesses. We compare the three available configurations in a dual socket Haswell-EP system using micro-benchmarks. Furthermore, we investigate the influence on the performance of parallel applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Kottapalli et al. <ref type="bibr" target="#b3">[4]</ref> describe the directory assisted snoop broadcast protocol (DAS)-an extension of the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>. The DAS protocol uses an in-memory directory to accelerate memory accesses in multi-socket Intel systems. Moga et al. <ref type="bibr" target="#b4">[5]</ref> introduce an directory cache extension to the DAS protocol which accelerates accesses to cache lines that are forwarded from caches in other NUMA nodes. Both extensions are implemented in the Haswell-EP micro-architecture <ref type="bibr" target="#b2">[3]</ref>. Geetha et al. <ref type="bibr" target="#b5">[6]</ref> describe a modified version of the forward state in the MESIF protocol which increases the ratio of requests the local caching agent (CA) can service without snooping.</p><p>Synthetic benchmarks are an important tool to analyze specific aspects of computer systems. In previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> we presented micro-benchmarks to measure the characteristics of distributed caches in NUMA systems with x86 64 processors. They use data placement and coherence state control mechanisms to determine the core-to-core transfers and memory accesses considering the coherence state of the data.</p><p>Application based benchmarks are often used to analyze the performance of parallel architectures. M?ller et al. introduce SPEC OMP2012 <ref type="bibr" target="#b8">[9]</ref>-a benchmark suite for shared memory systems. It contains 14 OpenMP parallel applications from different scientific fields and also covers OpenMP 3.0 features. In <ref type="bibr" target="#b9">[10]</ref> SPEC MPI2007 is introduced. It is based on 13 scientific applications that use MPI for parallelization. The scope of SPEC MPI2007 extends to large scale distributed memory systems. However, both suites can be used to evaluate shared memory systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HASWELL-EP ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ISA Extensions and Core Design</head><p>The Intel Haswell micro-architecture is the successor of Sandy Bridge. As shown in Table <ref type="table" target="#tab_2">I</ref>, the out-of-order execution is enhanced significantly in the Haswell micro-architecture. There are more scheduler and reorder buffer (ROB) entries, larger register files, and more load/store buffers in order to extract more instruction level parallelism. Haswell's scheduler can issue eight micro-ops each cycle. The two new issue ports provide an additional ALU and add a third address generation unit. The support of FMA instructions doubles the theoretical peak performance. The data paths to the L1 and L2 cache are widened as well to provide the execution units with more data. However, if 256 bit instructions are detected, the base frequency is reduced to the AVX base frequency and the available turbo frequencies are restricted <ref type="bibr" target="#b10">[11]</ref>. Furthermore, the uncore frequency is adapted to the workload dynamically by the hardware <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Uncore Design</head><p>Haswell-EP is available in three variants [16, Section 1.1]an eight-core die (4, 6, 8-core SKUs 1 ), a 12-core die (10, 12core SKUs), and an 18-core die (14, 16, 18-core SKUs) <ref type="bibr" target="#b10">[11]</ref>. The eight-core die uses a single bi-directional ring interconnect. The 12-and 18-core dies use a partitioned design as depicted in Figure <ref type="figure">1</ref>. In that case, eight cores, eight L3 slices,  one memory controller, the QPI interface, and the PCIe controller are connected to one bi-directional ring. The remaining cores (4 or 10), L3 slices, and the second memory controller are connected to another bi-directional ring. Both rings are connected via two bi-directional queues. Some SKUs use partially deactivated dies. We are unaware of a specification which cores are deactivated in specific models. It is possible that this is determined individually during the binning process in order to improve yield. Thus, it can not be ruled out that processors of the same model exhibit different characteristics depending on how balanced the rings are populated. The ring topology is hidden from the operating system in the default configuration, which exposes all cores and resources in a single NUMA domain. An optional Cluster-on-Die (COD) mode can be enabled in the BIOS. This mode splits each processor into two clusters as depicted in Figure <ref type="figure">1</ref>. The clusters contain an equal number of cores and are exposed to the operating system as two NUMA nodes. However, the software view on the NUMA topology actually does not match  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HASWELL-EP CACHE COHERENCE MECHANISMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MESIF Implementation</head><p>Cache coherence is maintained using the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>. It inherits the modified, exclusive, shared, and invalid states from MESI and adds the state forward to enable cacheto-cache transfers of shared cache lines. The forward state designates one shared copy as being responsible to forward the cache line upon requests. The protocol ensures that at most one copy in the forward state exists at any one time. It does not ensure that the data is provided by the closest available copy.</p><p>The protocol is implemented by caching agents (CAs) within each L3 slice and home agents (HAs) within each memory controller as depicted in Figure <ref type="figure" target="#fig_1">3</ref>. The cores send requests to the caching agents in their node. The responsible CA is derived from a requested physical address using a hash function <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">Section 2.3]</ref>. It provides data from its appendent L3 slice or obtains a copy from another core's L1 or L2 cache as indicated by the core valid bits <ref type="bibr" target="#b6">[7]</ref>. In case of an L3 miss, the caching agent forwards the request to the home agent which provides the data from memory. The caching agents in other nodes need to be checked as well which can be implemented using source snooping (see Section IV-B) or home snooping (see Section IV-C).</p><p>The MESIF protocol can be augmented with directory support <ref type="bibr" target="#b3">[4]</ref>. The "directory assisted snoop broadcast protocol" stores 2-bit of directory information for each cache line in the memory ECC bits <ref type="bibr" target="#b16">[17</ref>  that a potentially modified copy exists in another node. The state shared indicates that multiple clean copies exist. This information can be used to limit the amount of snoops sent. This is particularly important in systems with several sockets, since broadcasts quickly become expensive for an increasing number of nodes. On the other hand, necessary snoops are delayed until the directory lookup is completed. It is also possible to send snoops to all caching agents in parallel to the directory access. In that case the home agent can forward the data from memory without waiting for the snoop responses if the directory state allows it. However, the snoop traffic is not decreased in that case. According to [16, Section 2.5], the directory should not be used in typical two-socket systems.</p><p>Our test system does not expose a BIOS option to manually enable directory support, but it is automatically enabled in COD mode (see Section IV-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Source Snoop Mode</head><p>In the source snoop mode of the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>, snoops are sent by the caching agents. In case of an L3 miss, the caching agent broadcasts a snoop requests to the appropriate caching agents in all other nodes as well as to the home agent in the home node. If another caching agent has a copy of the cache line in state modified, exclusive, or forward it will be forwarded directly to the requester. All caching agents also notify the home agent about their caching status of the requested cache line. The home agent collects all snoop responses, resolves conflicts, provides data from memory if necessary, and completes the transaction. This mode has the lowest possible latency. However, it generates a lot of traffic on the interconnect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Home Snoop Mode</head><p>The MESIF protocol <ref type="bibr" target="#b1">[2]</ref> also supports a home snoop mode in which snoops are sent by the home agents. In that case, the caching agent that requests a cache line does not broadcast snoops. Instead it forwards the request to the home agent in the home node which then sends snoops to other caching agents. The transaction then continues as in the source snoop variant. The forwarding to the home node adds latency. However, the home agent can implement a directory to improve performance (see Section IV-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cluster on Die Mode (COD)</head><p>In the COD mode each processor is partitioned as depicted in Figure <ref type="figure">1</ref> resulting in a system with four NUMA nodes as shown in Figure <ref type="figure">2b</ref>. The coherence protocol then uses a home snoop mechanism with directory support as described in Section IV-A. Furthermore, Haswell-EP also includes directory caches to accelerate the directory lookup <ref type="bibr" target="#b2">[3]</ref>. However, with only 14 KiB per home agent these caches are very small. Therefore, only highly contended cache lines are entered in the so-called "HitME" cache <ref type="bibr" target="#b4">[5]</ref>. These are also referred to as migratory cache lines <ref type="bibr" target="#b2">[3]</ref>, i.e., cache lines that are frequently transferred between nodes. Consequently, only accesses that already require snoops result in allocations in the HitME cache (see <ref type="bibr">Fig 3.</ref> in <ref type="bibr" target="#b4">[5]</ref>). Since cache lines that are in the state remote-invalid do not require snoops, the first access to such a line can transfer a cache line to a remote caching agent without entering it in the directory cache in the home node. An entry in the directory cache can only be allocated if cache lines are forwarded between caching agents in different nodes and the requesting caching agent is not in the home node. If the directory cache contains an entry for a cache line, the corresponding entry in the in-memory directory is in the snoop-all state.</p><p>The directory cache stores 8-bit presence vectors that indicate which nodes have copies of a cache line <ref type="bibr" target="#b2">[3]</ref>. When a request arrives at the home node, the directory cache is checked first. If it contains an entry for the requested line, snoops are sent as indicated by the directory cache. If the request misses in the directory cache, the home agent reads the status from the in-memory directory bits and sends snoops accordingly, or directly sends the data from memory if no snoops are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BENCHMARKING METHODOLOGY A. Tests System</head><p>Our test system contains two 12-core Haswell-EP processors (see Table <ref type="table" target="#tab_2">II</ref>). Each socket has four DDR4 memory channels running at 2133 MHz. Thus, 68.3 GB/s of memory bandwidth are available per socket. The sockets are connected with two QPI links that operate at 9.6 GT/s. Each link provides a bi-directional bandwidth of 38.4 GB/s. Both links combined enable 76.8 GB/s of inter-socket communication (38.4 GB/s in each direction).</p><p>The coherence protocol can be influenced by the Early Snoop and COD mode settings in the BIOS <ref type="foot" target="#foot_0">2</ref> . The default configuration is Early Snoop set to auto (enabled) and COD mode set to disabled which results in a source snoop behavior. If Early Snoop is disabled a home snoop mechanism is used. If COD mode is enabled the newly introduced Cluster-on-Die mode <ref type="bibr" target="#b2">[3]</ref> is activated. In that case the setting for Early Snoop has no influence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Microbenchmark Design</head><p>We use an extended version of the synthetic microbenchmarks presented in <ref type="bibr" target="#b6">[7]</ref>. They include data placement and coherence state control mechanisms that place cache lines in a fully specified combination of core id, cache level, and coherence state. The data set size determines the cache level in which the data is located. If the data set does not fit into a certain cache level, optional cache flushes can be used to evict all cache lines from higher cache levels into the cache level that is large enough. The latency and bandwidth measurements can be performed on the core that performed the data placement to evaluate the local cache hierarchy. The measurement can also be performed on another core in order to estimate the performance of core-to-core transfers. The bandwidth can also be measured for concurrent accesses of multiple cores. Furthermore, the libnuma based memory affinity control enables the analysis of NUMA characteristics for main memory accesses.</p><p>The coherence state of the date can be controlled as well.</p><p>? State modified is enforced by: 1) writing to the data.</p><p>? State exclusive is generated by: 1) writing the data which invalidates all other copies, 2) removing the modified copy using clflush, 3) reading the data. ? State shared and forward are created by: 1) caching data in state exclusive, 2) another core reading the data. The order of accesses determines which core gets the forward copy. It is also possible to define a list of cores that should share the data in order to span multiple nodes.</p><p>The benchmarks are designed to operate in a stable environment, i.e., at fixed frequencies. We therefore disable the Turbo Boost feature and set the core frequency to the nominal 2.5 GHz. However, the uncore frequency scaling and frequency reductions for AVX workloads can influence the results. Occasionally the bandwidth benchmarks show better performance than presented in this paper. The higher performance levels are not reliably reproducible. We selected measurements for the typical case, i.e., curves that do not show frequency related jumps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LATENCY RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Source Snoop Mode</head><p>Latency measurements for the default configuration are depicted in Figure <ref type="figure" target="#fig_2">4</ref>. The access times of 1.6 ns (4 cycles), 4.8 ns (12 cycles), and 21.2 ns (53 cycles) for reads from the local L1, L2, and L3 are independent of the coherence state. In contrast, core-to-core transfers show noticeable differences for modified, exclusive, and shared cache lines.</p><p>If another core contains a modified copy in the L1 or L2 cache, the corresponding core valid bit is set in the L3 cache. Thus, the caching agent snoops the core which then forwards the cache line to the requester. The latency is approximately 53 ns and 49 ns in that case for transfers from the L1 and L2 cache, respectively. If the other core has already evicted the modified cache lines, the inclusive L3 cache services the requests without delay (21.2 ns). This is possible because the write back to the L3 also clears the core valid bit.</p><p>Unmodified data is always delivered from the inclusive L3 cache. However, accesses to exclusive cache lines have a higher latency of 44.4 ns if they have been placed in the L3 cache by another core. This is because the caching agent has to snoop the other core as the state could have changed to modified. This also happens if the core does not have a copy anymore as exclusive cache lines are evicted silently which does not clear the core valid bit. If multiple core valid bits are set, core snoops are not necessary as the cache line can only be in the state shared. In that case the L3 cache forwards a copy without delay (21.2 ns).</p><p>Accesses to cache lines in the other socket follow the same pattern. Modified cache lines in a remote L1 or L2 cache have to be forwarded from the core that has the only valid copy in that case. The latency is 113 ns from the L1 and 109 ns from the L2. If a valid copy exists in the L3 cache it is forwarded from there with a latency of 86 ns for immediate replies or 104 ns if a core snoop is required. The memory latency is 96.4 ns for local and 146 ns for remote accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Home Snoop Mode</head><p>Figure <ref type="figure" target="#fig_3">5</ref> depicts the difference between source snooping (default) and home snooping (Early Snoop disabled). As expected, all accesses that are handled by the L3's caching agents without any external requests are not affected by the snoop behavior. However, the latency of remote cache access is noticeably higher if home snooping is used. It increases from 104 ns to 115 ns (+10.5%). The local memory latency increases from 96.4 ns to 108 ns (+12%). Both effects can be explained by the delayed snoop broadcast in the home snoop protocol. This also shows that directory support is not activated as the local memory latency would not increase if it was enabled. The remote memory latency of 146 ns is identical to source snoop mode, as the request is sent from the requesting caching agent directly to the remote home agent in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cluster-on-Die Mode</head><p>Activating the COD mode doubles the amount of possible distances. In addition to local accesses, core-to-core transfers within the node, and transfers via QPI we also have to consider  transfers between the clusters in a socket as well as intersocket communication between nodes that are not directly connected via QPI. Figure <ref type="figure" target="#fig_4">6</ref> shows the clearly distinguishable performance levels. Transfers between on-chip clusters are much faster than QPI transfers. The L3 forwards cache lines slightly faster (18.0 or 37.2 ns compared to 21.2 or 44.4 ns with COD disabled). However, the inclusive behavior of the L3 cache is limited to the cores within a NUMA node. Coreto-core transfers between the nodes involve the home node of the data, with latencies increasing from 18.0 and 37.2 ns to 57.2 and 73.6 ns if data is delivered from the L3 of the second node. Remote L3 access latency increases from 86 ns (modified) and 104 ns (exclusive) to 90 and 104 ns, 96 and 111 ns, or 103 and 118 ns depending on the number of hops. In these measurements the home agent is in the same node as the caching agents that forward the data. The latency further increases if three nodes are involved.</p><p>The memory latency with COD drops from 96.4 to 89.6 ns for local accesses. Accesses to memory attached to the second node are slower but still slightly faster than local accesses with COD disabled. If memory from the second processor is used, the latency depends on the number of hops within the nodes: 141 ns for 1-hop-connections (node0-node2), 147 ns for two hops (node0-node3 and node1-node2) and 153 ns for three hops (node1-node3).</p><p>The asymmetrical chip layout which is mapped to a bal-anced NUMA topology causes performance variations. The six cores in the first node (core 0-5 in Figure <ref type="figure">1a</ref>) are connected to the same ring, thus have similar performance characteristics.</p><p>The average distance to the individual L3 slices is almost identical for all cores. Consequently, the average time for forwarding the request from the core to the responsible caching agent is almost independent of the location on the ring (assuming data is distributed evenly). The distance from the caching agent to the home agents also does not depend on the requesting core's location on the ring, as the location of the agents is determined by the physical address (hash function selects the CA, NUMA node determines HA). In the second node however, the situation is different. Two cores are connected to the first ring, but allocate memory from the memory controller connected to the second ring by default (assuming NUMA aware software). On average, two thirds of the cores' memory requests are serviced by the four caching agents connected to the second ring. For the four cores connected to the second ring, one third of their requests is serviced by the two caching agents on the first ring.</p><p>Table <ref type="table" target="#tab_6">III</ref> shows the different performance characteristics in comparison to the source snooping and home snooping modes on our test system. The latency reduction for local accesses is substantial for the cores in the first node (-15% for L3 and -7.1% for memory), slightly lower for the four cores in the second node on the second ring (-13.2% and -6.2%), and only small for the two cores in the second node but on the first ring (-5.6% and -2.5%). In contrast, remote cache accesses are up to 15.4% slower. Remote memory latency varies between 3.4% faster and 4.8% slower. These cases only cover transfers that do not require broadcasts. Figure <ref type="figure">7</ref> depicts measurements of accesses from node0 to data that has been used by two cores somewhere in the system which also covers situations where broadcasts are required. Small data set sizes show an unexpected behavior if the data is cached in forward state outside the home node. Surprisingly, the latency for L1 and L2 accesses is significantly lower than for L3 accesses 5 . Performance counter readings 6  show that data is in fact forwarded from the memory in the home node for data set sizes up to 256 KiB 7 . According to <ref type="bibr" target="#b5">[6]</ref>, this is possible if the in-memory directory indicates that cache lines are in state shared. This explains why data is delivered faster if node0 is the home node as no QPI transfer is involved. However, if the in-memory directory state would be responsible for this behavior, it would not depend on the data set size. This suggests that the effect is caused by the directory cache. For larger data set sizes an increasing number of cache lines is forwarded by the other node 8 as expected according to the state specifications in the MESIF protocol <ref type="bibr" target="#b1">[2]</ref>. For 2.5 MiB 5 The behavior doesn't change if hardware prefetchers are disabled in BIOS 6 event MEM LOAD UOPS L3 MISS RETIRED:REMOTE DRAM 7 the variation below 256 KiB also occurs if all caches are flushed and data is read directly from local DRAM, thus it is apparently caused by DRAM characteristics, e.g., the portion of accesses that read from already open pages 8 indicated by MEM LOAD UOPS L3 MISS RETIRED:REMOTE FWD and above the percentage of DRAM responses are negligible. The gradual decline matches the assumption that the directory cache is involved and the hit rates diminish. We therefore conclude that the AllocateShared policy <ref type="bibr" target="#b4">[5]</ref> of the directory cache is implemented and an entry is allocated if the cache lines are transferred to another node in state forward. This sets the in-memory directory to state snoop-all. However, the presence vector in the directory cache indicates that the cache line is shared and allows forwarding the valid memory copy as long as the entry is not evicted. Table <ref type="table" target="#tab_7">IV</ref> details the L3 latencies for a core in node0 that accesses shared cache lines with different distributions among the nodes. The values express the performance for data set sizes above 2.5 MiB, i.e., the divergent behavior for smaller data sets is not considered here. If a valid copy is available in node0, it is forwarded to the requesting core with the same 18.0 ns latency discussed above (case COD, first node in Table <ref type="table" target="#tab_6">III</ref>). Interestingly, this also is the case for accesses to shared cache lines in the local L1 or L2 cache if the forward copy is in another node. This indicates, that in this case the responsible caching agent is notified in order to reclaim the forward state. In the cases on the diagonal, a forward copy is found in the home node (the local snoop in the home node is carried out independent of the directory state <ref type="bibr" target="#b4">[5]</ref>). The results are equal to the forwarding of modified lines from the home node (see Figure <ref type="figure" target="#fig_4">6a</ref>). The remaining cases show the performance of forwarding cache lines from another node which is snooped by the home node. The range from 162 to 177 ns is caused by the different distances between the involved nodes. The worst case of 177 ns is more than twice as high as the 86 ns in the default configuration.</p><p>Table <ref type="table" target="#tab_8">V</ref> shows the impact of the directory protocol on memory accesses. For data sets larger than 15 MiB data was read by multiple cores but is already evicted from the L3 caches. The diagonal represents cases where data is only shared within the home node. In that case no directory cache entries are allocated and the in-memory directory remains in state remote-invalid. Thus, the data is delivered by the home node without a broadcast. All other cases apparently require a broadcast. Thus, the directory state has to be snoop all. This is another indication that the AllocateShared policy <ref type="bibr" target="#b4">[5]</ref> of the directory cache is implemented, i.e., directory cache entries are allocated if a cache line is entered in state forward in another node. Consequently, the in-memory state is changed to snoop all instead of shared which would be used without the directory cache <ref type="bibr" target="#b3">[4]</ref>. Thus, the home node broadcasts a snoop request which adds between 78 and 89 ns to the latency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. BANDWIDTH RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Threaded Bandwidths</head><p>Figure <ref type="figure" target="#fig_5">8</ref> shows the single threaded memory read bandwidth in the default configuration. Using 256 bit wide load instructions, data can be read from the L1 and L2 cache with 127.2 and 69.1 GB/s from the local L1 and L2 cache, respectively. The L1 and L2 measurements show unusually high variability which is probably caused by frequency changes between the nominal and the AVX base frequency. If 128 bit instructions are used, the bandwidth is limited to 77.1 and 48.2 GB/s as they do not fully utilize the width of the data paths. Accesses beyond the local L2 cache show little differences between AVX and SSE loads.</p><p>The core-to-core bandwidth reflect the behavior that has already been observed in the latency measurements. Only modified cache lines are actually forwarded from a core's L1 or L2 cache. The bandwidth is 7.8 and 10.6 GB/s for onchip transfers and 6.7 and 8.1 GB/s for transfers between the sockets. If modified cache lines are evicted to the L3 cache, they can be read with 26.2 GB/s from the local and 9.1 GB/s remote L3. Accesses to exclusive cache lines are always serviced by the L3. However, the bandwidth only reaches 26.2 GB/s for accesses to cache lines in the local L3 that have been evicted by the requesting core. If another core needs to be snooped, the bandwidth is limited to 15.0 GB/s for the local L3 and 8.7 GB/s for the remote L3. Since exclusive cache lines are evicted silently, the penalty also occurs if the other core does not contain a copy anymore.</p><p>Figure <ref type="figure" target="#fig_6">9</ref> shows the read bandwidth of shared cache lines. Local L1 and L2 accesses only achieve the performance measured for modified or exclusive, if the forward copy is in the requesting core's node. If the forward copy is held by the other processor, the L1 and L2 bandwidth are limited to the L3 bandwidth of 26.2 GB/s. As already mentioned in Section VI-C, this suggests that the L3 is notified of the repeated access in order to reclaim the forward state. Shared data can be read with 26.2 GB/s from the local L3 and 9.1 GB/s from the remote L3 without snooping other cores, as the data is guaranteed to be valid.</p><p>Bandwidths are also affected by the coherence protocol configuration as detailed in Table <ref type="table" target="#tab_9">VI</ref>. Deactivating Early Snoop decreases the local memory bandwidth from 10.3 to 9.6 GB/s. On the other hand, there is a small increase in the remote L3 and memory bandwidths. The COD mode again shows significant performance variations between cores in the different nodes within the chip. Local L3 and memory bandwidth benefit from enabling COD mode. The L3 bandwidth  increases significantly in the first node (+10.6%). However, the performance in the second node does not benefit that much (+3.8%/+5.3% depending on the ring to which the core is connected). The local memory bandwidth increases by more than 20% in all cases. On the other hand, remote cache and memory bandwidth are reduced in COD mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Aggregated Bandwidths</head><p>As in earlier ring based designs, the L3 cache scales well with the number of cores. The performance is identical for source snoop and home snoop mode. The read bandwidth scales almost linearly from 26.2 GB/s for one core to 278 GB/s with 12 cores (23.2 GB/s per core). While the measurements using only a few cores are reproducible, the measured bandwidth for 7 to 12 cores strongly differs between measurements. Up to 343 GB/s have been reached in our experiments. We attribute this unpredictable behavior to the uncore frequency scaling that dynamically increases the L3 bandwidth. The write bandwidths scale from 15 GB/s to 161 GB/s-with occasional performance boosts up to 210 GB/s. In COD mode 154 GB/s of read and 94 GB/s of write bandwidth are available per node. The difference between the nodes on a chip is negligible.</p><p>Table <ref type="table" target="#tab_9">VII</ref> shows the bandwidth scaling for the source snoop and home snoop mode. The read bandwidth from local memory is lower for up to seven concurrently reading cores if Early Snoop is disabled. For eight and more cores the effect is negligible. Approximately 63 GB/s are reached in both configurations. The write bandwidth is 7.7 GB/s for a single core. It increases up to 26.5 GB/s using five cores and declines afterwards. Using all cores we measure 25.8 GB/s. The read bandwidth from remote memory is much higher if Early Snoop is disabled. It reaches 30.6 GB/s compared to only 16.8 GB/s in the default configuration. The COD mode bandwidths are detailed in Table <ref type="table" target="#tab_9">VIII</ref>. The local bandwidth per node is 32.5 GB/s. The bandwidth of node-to-node transfers is 18.8 GB/s within on chip and-depending on the number of hops-15.6 or 14.7 GB/s for transfers between the sockets. The SPEC OMP2012 results for the home snoop mode (Early Snoop disabled) are almost identical to the default configuration. 12 out of 14 benchmarks are within +/-2% of the original runtime. 362.fma3d and 371.applu331 show a ca. 5% reduction of the runtime if Early Snoop is disabled. If COD mode is enabled, the runtime of these two benchmarks increases noticeably-up to 23% for 371.apply. They are apparently influenced by the worst case latency penalties of COD mode. No benchmark in the SPEC OMP2012 suite benefits from enabling COD mode.</p><p>The SPEC MPI2007 results are very uniform. Disabling Early Snoop has a tendency to slightly decrease the performance while enabling COD mode mostly increases the performance, i.e., the benchmarks reflect the changes in local memory latency and bandwidth. This is to be expected, since MPI programs primarily use local memory.   Understanding the design and operation principles of microprocessor memory hierarchy designs is paramount for most application performance analysis and optimization tasks. In this paper we have presented sophisticated memory benchmarks along with an in-depth analysis of the memory and cache design of the Intel Haswell-EP micro-architecture. The increasing number of cores on a processor die strongly increases the complexity of such systems, e.g. due to the use of directory-based protocols that used to be applicable only to large multi-chip-systems. Our work reveals that providing cache coherence in such a system does have considerable overhead and presents scalability challenges, in particular for latency sensitive workloads. Furthermore, the performance variations that have been introduced with the Haswell-EP architecture due to complex and unpredictable frequency control mechanisms may turn out to be equally challenging for any optimization task.</p><p>The different coherence protocol modes have a significant impact on the latencies and bandwidths of core-to-core transfers as well as memory accesses on Haswell-EP. The default source snoop mode is optimized for latency, which-according to our application benchmarks-seems to be a good choice. The home snooping mode enables higher bandwidth for intersocket transfers, creating significant advantages in our microbenchmarks. However, it does not have a large benefit in our benchmark selection. On the other hand, the latency of the local memory is increased which reduces the performance of NUMA optimized workloads.</p><p>The optional COD mode reduces the local memory latency and improves its bandwidth which slightly increases performance in cases. However, mapping the asymmetrical chip layout to a balanced NUMA topology creates performance variations between the cores with latency reductions between 5 and 15% depending on the location of the core on the chip. The benefit of COD mode would probably be more substantial if the hardware topology would match the software visible NUMA nodes. Moreover, the complex transactions in the coherence protocol-especially those that involve three nodes-can result in severe performance degradations. The worst case latency of core-to-core transfers is doubled. Furthermore, the memory latency increases significantly if the directory information is outdated due to silent evictions from the L3 cache. However, our measurements with shared memory parallel applications show that the performance impact is mostly negligible. Only one of the SPEC OMP2012 benchmarks shows a significant performance degradation.</p><p>Our conclusion is that processors with single-chip NUMA and directory support may not yet be mandatory, but will probably become standard in the near future. Our resultsboth regarding fundamental operational principles and performance impacts-are crucial to understand these systems, which represents an important pre-requisite for most performance optimization and performance modeling tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Haswell-EP block diagram: There are two rings with one memory controller (IMC) each. QPI and PCIe links are connected to the first ring. The rings are connected with bi-directional queues. In the default configuration all cores can access the whole L3 cache and memory is interleaved over all four channels. The Cluster-on-Die mode splits the chip into two NUMA nodes which share one memory controller each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The cores forward requests to the caching agents within their node. If the request cannot be serviced locally, it is forwarded to the home agent. Snoops of the CAs in the peer nodes are either handled by the CA or the HA (see Section IV-B and Section IV-C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Memory read latency in default configuration (source snoop): Comparison of accesses to a core's local cache hierarchy (local) with accesses to cache lines of another core in the same NUMA node (within NUMA node) as well as accesses to the second processor (other NUMA node (1 hop QPI)).</figDesc><graphic url="image-63.png" coords="5,60.27,535.66,232.39,112.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Memory read latency: Comparison of source snoop and home snoop, cached data in state exclusive</figDesc><graphic url="image-64.png" coords="5,314.16,561.27,232.27,111.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Memory read latency: Comparison of accesses to a core's local cache hierarchy (local) with accesses to cache lines of other cores in the same NUMA node (within NUMA node) and on-chip transfers between the clusters (other NUMA node (1 hop on-chip)) as well as transfers between the two sockets. This includes connections between the two nodes that are coupled directly via QPI (other NUMA node (1 hop QPI)) as well as transfers that include additional hops between the clusters within the sockets. (other NUMA node (2 hops) / other NUMA node (3 hops)). The measurements use the first core in every node to perform the data placement.</figDesc><graphic url="image-71.png" coords="6,58.87,72.93,237.24,114.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Memory read bandwidth in default configuration (source snoop): Comparison of accesses to a core's local cache hierarchy (local) with accesses to cache lines of another core in the same NUMA node (within NUMA node) as well as accesses to the second processor (other NUMA node (1 hop QPI)).</figDesc><graphic url="image-88.png" coords="8,58.03,209.62,237.24,115.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Memory read bandwidth in default configuration (source snoop): Accesses to shared cache lines.</figDesc><graphic url="image-89.png" coords="8,314.44,73.01,237.13,112.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Coherence protocol configuration vs. application performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table I compares both architectures. The frontend is similar to Sandy Bridge. Instructions are fetched in 16 byte windows and decoded into micro-ops by four decoders. The instruction set architecture (ISA) has been extended to support AVX2 and FMA3 instructions. AVXwhich is available since Sandy Bridge-introduced 256 bit floating point instructions. AVX2 increases the width of integer SIMD instructions to 256 bit as well. The FMA3 instruction set introduces fused multiply-add instructions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF SANDY BRIDGE AND HASWELL MICRO-ARCHITECTURE</figDesc><table><row><cell>Micro-architecture</cell><cell>Sandy Bridge</cell><cell>Haswell</cell></row><row><cell>References</cell><cell>[13], [14, Section 2.2]</cell><cell>[15], [14, Section 2.1]</cell></row><row><cell>Decode</cell><cell cols="2">4(+1) x86/cycle</cell></row><row><cell>Allocation queue</cell><cell>28/thread</cell><cell>56</cell></row><row><cell>Execute</cell><cell>6 micro-ops/cycle</cell><cell>8 micro-ops/cycle</cell></row><row><cell>Retire</cell><cell cols="2">4 micro-ops/cycle</cell></row><row><cell>Scheduler entries</cell><cell>54</cell><cell>60</cell></row><row><cell>ROB entries</cell><cell>168</cell><cell>192</cell></row><row><cell>INT/FP registers</cell><cell>160/144</cell><cell>168/168</cell></row><row><cell>SIMD ISA</cell><cell>AVX</cell><cell>AVX2</cell></row><row><cell>FPU width</cell><cell>2? 256 Bit (1? add, 1? mul)</cell><cell>2? 256 Bit FMA</cell></row><row><cell>FLOPS/cycle</cell><cell>16 single / 8 double</cell><cell>32 single / 16 double</cell></row><row><cell>Load/store buffers</cell><cell>64/36</cell><cell>72/42</cell></row><row><cell>L1D accesses</cell><cell>2? 16 byte load +</cell><cell>2? 32 Byte load +</cell></row><row><cell>per cycle</cell><cell>1? 16 byte store</cell><cell>1? 32 Byte store</cell></row><row><cell>L2 bytes/cycle</cell><cell>32</cell><cell>64</cell></row><row><cell>Memory</cell><cell>4? DDR3-1600</cell><cell>4? DDR4-2133</cell></row><row><cell>Channels</cell><cell>up to 51.2 GB/s</cell><cell>up to 68.2 GB/s</cell></row><row><cell>QPI speed</cell><cell>8 GT/s (32 GB/s)</cell><cell>9.6 GT/s (38.4 GB/s)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III LATENCY</head><label>III</label><figDesc>IN NANOSECONDS; RESULTS FOR L3 CACHE LINES ARE FOR STATE exclusive. Memory read latency: core in node0 accesses cache lines with forward copies in another node (F:) which also have different home nodes (H:). The home nodes also contain a copy which is in the shared state if the forward copy is in another node. The sharing also affects the memory latency because of stale state in the in-memory directory.</figDesc><table><row><cell>Fig. 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>source</cell><cell>default configuration</cell><cell>Early Snoop disabled</cell><cell>first node</cell><cell cols="2">COD mode second node first ring (core 6 and 7) second ring (cores 8-11)</cell></row><row><cell></cell><cell>local</cell><cell></cell><cell>21.2</cell><cell>18.0 (-15%)</cell><cell>20.0 (-5.6%)</cell><cell>18.4 (-13.2%)</cell></row><row><cell>L3</cell><cell>remote first node remote 2nd node</cell><cell>104</cell><cell>115 (+12%)</cell><cell>104 (+/-0) 113 (+8.7%)</cell><cell>108 (+3.8%) 118 (+13.5%)</cell><cell>111 (+6.7%) 120 (+15.4%)</cell></row><row><cell></cell><cell>local</cell><cell>96.4</cell><cell>108 (+10.5%)</cell><cell>89.6 (-7.1%)</cell><cell>94.0 (-2.5%)</cell><cell>90.4 (-6.2%)</cell></row><row><cell>memory</cell><cell>remote first node remote 2nd node</cell><cell>146</cell><cell>148 (+1.3%)</cell><cell>141 (-3.4%) 147 (+0.7%)</cell><cell>145 (-0.7%) 151 (+3.4%)</cell><cell>148 (+1.3%) 153 (+4.8%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV LATENCY</head><label>IV</label><figDesc>IN NANOSECONDS FOR ACCESSES FROM A CORE IN NODE0 TO L3 CACHE LINES WITH MULTIPLE SHARED COPIES.</figDesc><table><row><cell>node with</cell><cell cols="4">home node (shared copy)</cell></row><row><cell>forward copy</cell><cell>node0</cell><cell>node1</cell><cell>node2</cell><cell>node3</cell></row><row><cell>node0</cell><cell>18.0</cell><cell>18.0</cell><cell>18.0</cell><cell>18.0</cell></row><row><cell>node1</cell><cell>18.0</cell><cell>57.2</cell><cell>170</cell><cell>177</cell></row><row><cell>node2</cell><cell>18.0</cell><cell>166</cell><cell>90.0</cell><cell>166</cell></row><row><cell>node3</cell><cell>18.0</cell><cell>169</cell><cell>162</cell><cell>96.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V LATENCY</head><label>V</label><figDesc>IN NANOSECONDS FOR ACCESSES FROM A CORE IN NODE0 TO DATA IN MAIN MEMORY THAT HAS BEEN SHARED BY MULTIPLE CORES.</figDesc><table><row><cell>node that had</cell><cell></cell><cell cols="2">home node</cell><cell></cell></row><row><cell>forward copy</cell><cell>node0</cell><cell>node1</cell><cell>node2</cell><cell>node3</cell></row><row><cell>node0</cell><cell>89.6</cell><cell>182</cell><cell>222</cell><cell>236</cell></row><row><cell>node1</cell><cell>168</cell><cell>96.0</cell><cell>222</cell><cell>236</cell></row><row><cell>node2</cell><cell>168</cell><cell>182</cell><cell>141</cell><cell>236</cell></row><row><cell>node3</cell><cell>168</cell><cell>182</cell><cell>222</cell><cell>147</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI SINGLE</head><label>VI</label><figDesc>THREADED READ BANDWIDTH IN GB/S; RESULTS FOR L3 CACHE LINES ARE FOR STATE exclusive.</figDesc><table><row><cell></cell><cell>source</cell><cell>default configuration</cell><cell>Early Snoop disabled</cell><cell>first node</cell><cell cols="2">COD mode second node first ring (core 6 and 7) second ring (cores 8-11)</cell></row><row><cell></cell><cell>local</cell><cell>26.2</cell><cell></cell><cell>29.0</cell><cell>27.2</cell><cell>27.6</cell></row><row><cell>L3</cell><cell>remote first node remote 2nd node</cell><cell>8.8</cell><cell>8.9</cell><cell>8.7 8.3</cell><cell>8.3 8.0</cell><cell>8.4 8.1</cell></row><row><cell></cell><cell>local</cell><cell>10.3</cell><cell>9.5</cell><cell>12.6</cell><cell>12.4</cell><cell>12.6</cell></row><row><cell>memory</cell><cell>remote first node remote 2nd node</cell><cell>8.0</cell><cell>8.2</cell><cell>8.3 8.0</cell><cell>7.8 7.4</cell><cell>8.1 7.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>order to evaluate the influence of the coherence protocol mode on the performance of shared memory and message passing applications. Both suites have been compiled using Intel Composer XE 2013 SP1 update 3. The results (median of 3 iterations) are depicted in Figure VIII. Threads or processes are pinned to individual cores via KMP AFFINITY and -bind-to-core, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VIII. APPLICATION PERFORMANCE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>We have selected the SPEC OMP2012 and SPEC MPI2007</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>application benchmarks in</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell></row><row><cell cols="6">MEMORY READ BANDWIDTH (GB/S) SCALING IN COD MODE</cell></row><row><cell>source</cell><cell>1</cell><cell cols="4">number of concurrently reading cores 2 3 4 5</cell><cell>6</cell></row><row><cell>local memory</cell><cell cols="2">12.6</cell><cell>24.3</cell><cell>30.6</cell><cell>32.5</cell></row><row><cell>node0-node1</cell><cell cols="2">7.0</cell><cell>15.2</cell><cell>18.6</cell><cell>18.8</cell></row><row><cell>node0-node2</cell><cell cols="2">5.9</cell><cell>12.8</cell><cell>15.4</cell><cell>15.6</cell></row><row><cell>node0-node3 node1-node3</cell><cell cols="2">5.5</cell><cell>12.2</cell><cell>14.4</cell><cell>14.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Advanced ? Chipset Configuration ? North Bridge ? QPI Configuration ? QPI General Configuration</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>2.1 GHz base frequency for AVX workloads<ref type="bibr" target="#b10">[11]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Uncore frequency scaling automatically adjusts frequency<ref type="bibr" target="#b11">[12]</ref> </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The amd opteron northbridge architecture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2007">03 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">An Introduction to the Intel QuickPath Interconnect, Intel</title>
		<imprint>
			<date type="published" when="2009-01">January 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Intel xeon e5-2600 v3 (haswell) architecture &amp; features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karedla</surname></persName>
		</author>
		<ptr target="http://repnop.org/pd/slides/PDHaswellArchitecture.pdf" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Extending a cache coherency snoop broadcast protocol with directory information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kottapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neefs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagaraj</surname></persName>
		</author>
		<ptr target="https://www.google.com.tr/patents/US20120047333" />
		<imprint>
			<biblScope unit="volume">340</biblScope>
		</imprint>
	</monogr>
	<note>Patent, 2, 2012, uS Patent App. 12/860</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Allocation and write policy for a glueless area-efficient directory cache for hotly contested cache lines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandviwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Geetha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hum</surname></persName>
		</author>
		<ptr target="https://www.google.com.tr/patents/US8631210" />
		<imprint>
			<biblScope unit="volume">210</biblScope>
		</imprint>
	</monogr>
	<note>Patent, 1, 2014, uS Patent 8,631</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving value of forward state by increasing local caching agent forwarding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Geetha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neefs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Achtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jung</surname></persName>
		</author>
		<idno>App. PCT/US2012/020</idno>
		<ptr target="https://www.google.com.tr/patents/WO2013103347A1?cl=en" />
		<imprint>
			<biblScope unit="volume">408</biblScope>
		</imprint>
	</monogr>
	<note>Patent, 7, 2013, wO Patent</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory performance and cache coherency effects on an Intel Nehalem multiprocessor system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sch?ne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2009.22</idno>
		<ptr target="http://dx.doi.org/10.1109/PACT.2009.22" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing cache architectures and coherency protocols on x86-64 multicore SMP systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1669112.1669165</idno>
		<ptr target="http://dx.doi.org/10.1145/1669112.1669165" />
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spec omp2012 -an application benchmark suite for parallel systems using openmp</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robichaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shelepugin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waveren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenMP in a Heterogeneous World, ser</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Chapman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Massaioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>M?ller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Rorro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7312</biblScope>
			<biblScope unit="page" from="223" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SPEC MPI2007 -an application benchmark suite for parallel systems using mpi</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Waveren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ponder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/xeon-e5-v3-spec-update.pdf" />
		<title level="m">Intel? Xeon? Processor E5 v3 Product Family -Processor Specification Update, Intel</title>
		<imprint>
			<date type="published" when="2015">1 2015</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An energy efficiency feature survey of the intel haswell processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sch?ne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ilsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schuchart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Workshop on High-Performance, Power-Aware Computing (HPPAC&apos;15)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fully integrated multi-cpu, processor graphics, and memory controller 32-nm processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehalel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Altshuler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fayneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zelikson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="2012">1 2012</date>
		</imprint>
	</monogr>
	<note>Solid-State Circuits</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Optimization Reference Manual, Intel</title>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" />
		<imprint>
			<date type="published" when="2014-09">Sep 2014</date>
			<biblScope unit="page" from="248966" to="248996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Haswell: The fourth-generation intel core processor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hammarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hunsaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>D'sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chennupaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Micro, IEEE</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2014-03">Mar 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/zip/xeon-e5-v3-uncore-performance-monitoring.zip" />
		<title level="m">Intel Xeon Processor E5 v3 Family Uncore Performance Monitoring Reference Manual, Intel</title>
		<imprint>
			<date type="published" when="2014-09">9 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Intel? Xeon? Processor E5-1600, E5-2400, and E5-2600 v3 Product Families</title>
		<imprint>
			<publisher>Intel Corporation</publisher>
			<date type="published" when="2015-01">Jan 2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="http://www.bull.com/extreme-computing/download/S-bullxR421-en3.pdf" />
		<title level="m">Bull SAS, data sheet</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>bullx r421 e4 accelerated server</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
