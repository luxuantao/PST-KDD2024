<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AstriFlash A Flash-Based System for Online Services</title>
				<funder ref="#_yZB3Z5z">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
				<funder ref="#_4fPr59Z">
					<orgName type="full">FNS</orgName>
				</funder>
				<funder ref="#_2DgqsAX">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_a8T2EKH">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Gupta</surname></persName>
							<email>siddharth.gupta@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Yunho</forename><surname>Oh</surname></persName>
							<email>oh@korea.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Yan</surname></persName>
							<email>l.yan@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Sutherland</surname></persName>
							<email>mark.sutherland@alumni.epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
							<email>abhishek@cs.yale.edu</email>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
							<email>babak.falsafi@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Hsu</surname></persName>
							<email>peter.hsu@phaa.eu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Peter Hsu &amp; Associates</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AstriFlash A Flash-Based System for Online Services</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern datacenters host datasets in DRAM to offer large-scale online services with tight tail-latency requirements. Unfortunately, as DRAM is expensive and increasingly difficult to scale, datacenter operators are forced to consider denser storage technologies. While modern flash-based storage exhibits ?s-scale access latency, which is well within the tail-latency constraints of many online services, traditional demand paging abstraction used to manage memory and storage incurs high overheads and prohibits flash usage in online services. We introduce AstriFlash, a hardware-software co-design that tightly integrates flash and DRAM with ns-scale overheads. Our evaluation of server workloads with cycle-accurate full-system simulation shows that AstriFlash achieves 95% of a DRAM-only system's throughput while maintaining the required 99th-percentile tail latency and reducing the memory cost by 20x.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As billions of online users generate data daily, computer system designers struggle to architect datacenters that can manage ever-increasing datasets in a high-performance and cost-effective manner <ref type="bibr" target="#b10">[11]</ref>. To provide online services with high throughput and low tail latency, modern datacenters host the majority of data in memory <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[58]</ref> using TBs of DRAM per server <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b64">[66]</ref>. Unfortunately, DRAM accounts for a significant fraction of the overall server cost <ref type="bibr" target="#b6">[7]</ref> and is not scaling in capacity <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b62">[64]</ref>. Thus, datacenter operators are forced to consider denser technologies to host online services <ref type="bibr" target="#b21">[22]</ref>.</p><p>NAND flash is a suitable alternative as it enjoys 50x price ($/GB) improvement over DRAM <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b33">[34]</ref>, but with 1000x higher latency <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b65">[67]</ref>, <ref type="bibr" target="#b78">[80]</ref>. We believe that tighter integration of flash with DRAM might be a potential solution for two reasons. First, many modern online services have msscale end-to-end tail-latency constraints <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, which allows them to absorb few ?s-scale flash accesses <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Second, object popularity and request distributions for datacenter workloads are inherently skewed <ref type="bibr" target="#b62">[64]</ref>, <ref type="bibr" target="#b71">[73]</ref>, <ref type="bibr" target="#b73">[75]</ref>, <ref type="bibr" target="#b74">[76]</ref>, thus allowing hosting the hot fraction of the dataset in DRAM that serves most requests and filters the bandwidth required from the backing flash. The above observations should permit the design of a cost-effective two-tier hierarchy where a capacity-constrained DRAM caches the hot fraction of the dataset stored in a capacity-scaled flash layer.</p><p>The central obstacle to such a design today is the reliance on the traditional OS abstraction of demand paging for moving data between memory and flash. While paging was originally introduced for devices with ms-scale access latencies (e.g., disks), modern flash-based devices provide ?50 ?s access latency. As a consequence, archaic OS paging mechanisms incur performance overheads unsuitable for the tight taillatency constraints of online services <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Previous proposals combat the performance overheads by either accelerating paging <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> or providing direct access to flash <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, but still have a significant performance degradation compared to DRAM-only systems, or bypass paging and virtual memory altogether <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b63">[65]</ref>.</p><p>We propose AstriFlash, a hardware-software co-designed system that tightly integrates flash and DRAM to achieve DRAM-like performance with capacity and cost benefits of flash while maintaining the abstraction of virtual memory. We identify that paging overheads can be divided into core-side and memory-side arising from task switching and memory management. As a solution, we employ DRAM as a hardwaremanaged cache (e.g., Intel Knights Landing <ref type="bibr" target="#b66">[68]</ref>) to eliminate the OS memory-management overheads and enable near-DRAM capacity management and data movement. We also hide the flash access latency using fast user-level thread switches triggered on a DRAM-cache miss instead of the traditional OS-based context switches, thus enabling efficient asynchronous flash accesses. While prior proposals <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b63">[65]</ref> typically focus on optimizing one class of overheads, AstriFlash achieves better performance by addressing core-side and memory-side overheads synergistically. Overall, AstriFlash efficiently absorbs the ?s-scale flash latency by providing crossstack integration with ns-scale overheads. Such integration requires a novel re-design of three essential techniques:</p><p>1) While switch-on-miss architectures have been studied for ns-scale memory stalls <ref type="bibr" target="#b16">[17]</ref> and ms-scale disk stalls <ref type="bibr" target="#b69">[71]</ref>, AstriFlash requires absorbing ?s-scale flash accesses in online services using flexible user-level threads. In contrast to a limited number of hardware threads and expensive OS-based context switches, user-level threads enable low-cost contexts to efficiently overlap flash accesses by directly switching threads in 100 ns on a DRAM-cache miss. AstriFlash also provides hardware support to ensure forward progress and prevent starvation, thus upholding the tail latency of online services.</p><p>2) AstriFlash revisits classic proposals on memory traps <ref type="bibr" target="#b60">[62]</ref> as it requires tolerating ?s-scale DRAM-cache misses instead of rare ms-scale page faults using OS support. Accommodating frequent DRAM-cache misses in modern out-of-order (OoO) cores requires efficient microarchitectural support to revert committed stores residing in the Store Buffer (SB). AstriFlash extends existing speculation proposals <ref type="bibr" target="#b75">[77]</ref> to cover the SB, thus allowing reverting stores without OS support.</p><p>3) AstriFlash avoids OS-based memory management using a hardware-managed DRAM cache and provides microarchitectural support for managing 100s of concurrent misses. While traditional on-chip cache designs implement costly SRAMbased structures to support 10s of concurrent misses for ns-scale cache-refill latency, the DRAM cache can have 100s of concurrent misses because of the ?s-scale cache refills from flash. To the best of our knowledge, while previous DRAMcache proposals <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b61">[63]</ref> do not provide such support, AstriFlash provides novel microarchitectural support to implement an in-DRAM miss status table to track concurrent misses at low cost.</p><p>Overall, AstriFlash provides a flash-based system for online services where a fast-but-expensive DRAM contains the hot fraction, and a slow-but-cheaper flash contains the dataset, thus reducing the memory cost by 20x. Our evaluation shows that AstriFlash achieves ?95% of the throughput of a DRAM-only system while maintaining the 99th-percentile latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FLASH-INTEGRATED HIERARCHIES</head><p>NAND flash offers 50x cost improvement <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b33">[34]</ref> but incurs 1000x higher latency (50 ?s) than DRAM <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b65">[67]</ref>, <ref type="bibr" target="#b78">[80]</ref>. While various online services with ms-scale tail latency constraints can absorb a few ?s-scale flash accesses <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, replacing all DRAM with flash will result in unacceptable performance. Therefore, a careful combination of flash and DRAM is required to reduce costs while maintaining acceptable performance. Flash is commercialized as a storage device called Solid State Drive (SSD) with legacy I/O interfaces that preclude its use in online services. Therefore, tighter flash integration while maintaining DRAM-like performance requires the following considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Identifying the required DRAM-to-flash ratio</head><p>Memory hierarchies are designed using caching principles to exploit the locality present in data accesses. Faster devices are used as a cache to serve frequently accessed data while backing slower devices serve data in case of a cache miss <ref type="bibr" target="#b46">[47]</ref>. In datacenter workloads, a small fraction of the dataset can absorb most of the data accesses. Such an access pattern can be exploited by hosting the hot fraction of the dataset in DRAM while the backing flash contains the whole dataset <ref type="bibr" target="#b71">[73]</ref>, <ref type="bibr" target="#b72">[74]</ref>, <ref type="bibr" target="#b73">[75]</ref>, <ref type="bibr" target="#b74">[76]</ref>. Each DRAM miss requires fetching the requested data from flash. Similar to previous studies <ref type="bibr" target="#b71">[73]</ref>, <ref type="bibr" target="#b74">[76]</ref>, we examine the DRAM miss ratio while varying the DRAM-to-flash capacity ratio for CloudSuite <ref type="bibr" target="#b22">[23]</ref> workloads.</p><formula xml:id="formula_0">BW Flash = BW DRAM Cache Block Size ? Miss Rate ? Page Size (1)</formula><p>We also study the tradeoff between DRAM capacity and flash bandwidth required to refill the DRAM. We calculate the flash bandwidth required per core using Equation 1 with 0.5 GBps as average DRAM bandwidth <ref type="bibr" target="#b72">[74]</ref>, <ref type="bibr" target="#b73">[75]</ref>, 4KB and 64B as the page and cache block size. Page is the smallest data unit in DRAM but is decided to be larger than the cache block to capture spatial locality in the lower levels of the memory hierarchy where temporal locality is scarce. Large pages also reduce the tracking metadata required for all pages in DRAM. Figure <ref type="figure" target="#fig_0">1</ref> shows the average cache miss ratio across workloads and the required flash bandwidth for different DRAM capacities. Similar to previous studies <ref type="bibr" target="#b71">[73]</ref>, <ref type="bibr" target="#b74">[76]</ref>, the miss rates flatten around 3% of DRAM capacity, which requires 60 GBps of flash bandwidth for a 64-core system <ref type="bibr" target="#b2">[3]</ref>. With PCIe Gen5 specifications <ref type="bibr" target="#b19">[20]</ref> providing up to 128 GBps bandwidth, it is possible to meet the flash bandwidth requirements for high core counts using multiple SSDs. To reduce the bandwidth requirements further, we can use a larger DRAM cache, employ smaller pages, or use optimizations such as Footprint Cache <ref type="bibr" target="#b35">[36]</ref>.</p><p>Henceforth, we assume a system with 1TB dataset hosted in flash and a DRAM cache with 3% capacity (i.e., 32GB), which requires 60 GBps aggregate flash bandwidth for 64 cores. As flash is 50x cheaper than DRAM, this configuration reduces the memory cost by 20x compared to a 1TB DRAM system <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b64">[66]</ref>. Today, we can already implement such a flash-based system with existing OS paging support. However, our workloads indicate that each core encounters a DRAM miss every 5-25 ?s, thus causing the ?s-scale paging support to become the performance bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Programming abstractions for flash</head><p>Data movement between flash and DRAM layers can either be orchestrated by programmers or transparently done by the OS using virtual memory <ref type="bibr" target="#b14">[15]</ref>. While the latter places a lower burden on the programmer, its reliance on demand paging makes it more challenging to guarantee good performance. To stress-test AstriFlash, we do not expect the programmer to control data movement between the DRAM and flash layers.</p><p>Data movement in the case of virtual memory is transparent to programmers, and demand paging is used to retrieve data from flash. All pages are mapped into the application's Virtual Address (VA) space, but only a subset may be present in the Physical Address (PA) space, i.e., DRAM. When the application accesses a page not present in DRAM, the OS copies the required page from flash to DRAM and maps it to the correct VA. The application is oblivious to the physical location of a page and uses the same VA for each page throughout execution. Therefore, virtual memory provides ease of programmability by enabling permanent VA addresses for data.</p><p>Data movement in case of explicit I/O is orchestrated by programmers. The application manages data transfer between flash and DRAM using system calls or user-space I/O <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b67">[69]</ref>, and can issue an explicit request to copy a page from flash into a self-managed memory pool. Complex indexing structures <ref type="bibr" target="#b43">[44]</ref> are used to track pages in both DRAM and flash, and the limited capacity of the memory pool is exposed to the application. Thus, as DRAM pages are not guaranteed to remain at the same index throughout execution, the application cannot rely on permanent VA addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Overheads of demand paging</head><p>Modern memory hierarchies use flash as a logical extension of memory. Demand paging abstractions use DRAM as an OS-managed cache for flash, where DRAM is exposed to the OS through a physical address space, and page tables map virtual addresses to physical addresses. When the application accesses a virtual address that is not mapped to a physical address, a page-fault exception is triggered, and the OS fetches the required page from flash and installs it in the physical address space, potentially evicting another page. After initiating the page fault, the OS also performs a context switch, thus overlapping the flash access with useful work <ref type="bibr" target="#b49">[50]</ref>.</p><p>Traditional demand paging abstraction for I/O was originally built for dealing with ms-scale access latencies (e.g., disks) where the device overheads overshadowed the ?s-scale software overheads. However, as modern devices (e.g., flash) exhibit ?s-scale access latencies, paging becomes a critical performance bottleneck <ref type="bibr" target="#b11">[12]</ref>. We categorize these overheads into memory-side representing flash-access scheduling and memory-capacity management, and core-side representing task management and context switches.</p><p>Every page fault requires the OS to schedule an I/O request to fetch the requested page based on the SSD protocol (e.g., NVMe [57]). Checking the page cache and executing the OS storage stack and NVMe driver can consume up to 10 ?s <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b63">[65]</ref>. Even with recent proposals that include lean software stacks <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b67">[69]</ref> or allow direct user access to flash using normal loads and stores <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, systems still incur ?s-scale overheads. As flash accesses are 1000x longer  than DRAM accesses, frequently accessed pages should be maintained in DRAM for fast access. Migrating a page to DRAM might also require evicting another page, where victim selection is performed using complex policies in the OS with ?s-scale overhead. Page migration also requires updating the address mappings in the page tables. Keeping the TLBs coherent with the page tables requires a global TLB shootdown, which removes the old entries from all the TLBs. Modern TLB shootdowns are a broadcast operation, thus scaling poorly with the number of cores and incurring over 10 ?s in latency <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Recent proposals <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b45">[46]</ref> attempt to reduce the overhead by batching multiple page faults together. However, for frequent flash accesses, the number of overall shootdowns grows with the core count, thus accumulating high memory-side overhead.</p><p>As OoO pipelines cannot hide 50 ?s long flash access latency, the OS provides the abstraction of asynchronous flash accesses. The OS triggers a context switch on each page fault to overlap flash latency with useful work and maintain high system throughput. Previous proposals <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref> without context switches suffer a 5-10x performance degradation compared to DRAM-only systems. However, each context switch has ?5 ?s of core-side overhead <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b63">[65]</ref>, <ref type="bibr" target="#b70">[72]</ref> because of complex scheduling policies in the OS.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> compares asynchronous flash accesses with traditional paging overheads and an ideal system with no paging overhead. With 3% DRAM capacity, modern datacenter workloads have a DRAM miss every ?10 ?s per thread accompanied by 10 ?s of page fault and context switch overhead <ref type="bibr" target="#b63">[65]</ref>. Moreover, TLB shootdowns and OS mechanisms result in global synchronization and thus do not scale with the number of cores. Clearly, ?s-scale paging overhead causes high throughput degradation. To address these challenges, we propose AstriFlash, a hardware-software co-designed flashbased system for online services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ASTRIFLASH</head><p>This section describes the key insights for AstriFlash, provides an overview of its design, and discusses the tradeoffs against DRAM-only and flash-based memory hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Key insights</head><p>AstriFlash 1) uses lightweight user-level thread switches to hide flash accesses and maintain tail-latency constraints, and 2) eliminates traditional OS paging overheads using an accelerated DRAM miss handler.</p><p>The first insight aims at reducing the core-side overheads and is based on queuing characteristics at the tail of the responselatency distribution in online services with ms-scale Service-Level Objectives (SLO) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The response latency of requests is dominated by queuing delay because a system that implements a single-server queuing model requires the younger requests to wait for older requests to finish and free up the server. With asynchronous flash accesses and lightweight thread-switching support, a single physical server can instead function as a logical multi-server queuing model. Therefore, older requests waiting for pending flash accesses can free up the server instead of blocking younger requests, thus removing the request-level head-ofthe-line blocking and allowing AstriFlash to maintain similar overall response latency as a DRAM-only system. This insight applies best at high loads when there are multiple outstanding requests to cover the flash access latency.</p><p>The second insight aims at eliminating the memory-side overheads and asserts that conventional OS demand paging abstractions are fundamentally not scalable in systems with high paging frequency because of synchronization in either software (i.e., multiple OS threads modifying kernel data structures like page tables) or hardware (i.e., broadcast-based TLB shootdowns), thereby limiting throughput as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Memory mapping flash and encapsulating the memory-management activity in an accelerated miss handler between the DRAM cache and flash enables removing the paging overheads and maximizing throughput.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> presents the analytical latency and throughput of AstriFlash, a DRAM-only system, a traditional swap-based system (OS-Swap), and a system with synchronous flash accesses (Flash-Sync). DRAM-only and Flash-Sync represent an M/M/1 queuing system where all the requests always run to completion. In contrast, AstriFlash and OS-Swap represent an M/M/k queuing system where k requests are required to overlap the flash accesses. We assume that every 10 ?s of execution triggers a flash access that takes 50 ?s to complete. Flash-Sync has the worst performance with &gt;80% throughput degradation as each flash access is synchronous. Though OS-Swap performs asynchronous flash accesses, it suffers from severe core-side and memory-side overheads (10 ?s per flash access) because of page fault handling and context switches that cause ?50% throughput degradation. AstriFlash has little overhead from flash access and thread switching, and thus the throughput approaches that of a DRAM-only system. Our analysis indicates that an application with flash accesses every ?10 ?s of execution requires a SLO of 40x the average service time to perform within ?20% of the DRAM-only system. The performance gap shrinks as the SLO is relaxed or more physical servers are added. Overall, AstriFlash can sustain ms-scale SLOs for online services while achieving DRAMlike throughput using lightweight, user-level thread switching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design overview</head><p>Figure <ref type="figure" target="#fig_5">4a</ref>, 4b, and 4c depict the design overview of OS-Swap, Flash-Sync, and AstriFlash respectively. On the core side, AstriFlash provides fast switching among user-space threads at a ?s-scale granularity to efficiently overlap flash accesses with useful work. On the memory side, AstriFlash allows mapping flash into the physical address space while using DRAM as a hardware-managed cache (3% of the flash size).</p><p>1) Core-side design: AstriFlash provides novel hardwaresoftware co-design <ref type="bibr" target="#b28">[29]</ref> to hide asynchronous flash accesses efficiently. While OoO cores are provisioned to hide synchronous DRAM accesses, traditional disk accesses are long and infrequent enough to perform a context switch in the OS and hide the latency. However, in AstriFlash, we face ?s-scale stalls which cannot be handled efficiently either in hardware or software <ref type="bibr" target="#b11">[12]</ref>. Previous proposals <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b74">[76]</ref> expose the flash access latency to the cores and force them to wait for a response, thus losing throughput. In contrast, AstriFlash provides a novel ?s-scale switch-on-miss architecture that allows switching user-level threads on a DRAM miss to hide flash accesses efficiently and achieve DRAM-like throughput. While switchon-miss architectures <ref type="bibr" target="#b16">[17]</ref> have been studied only in the context of ns-scale memory stalls and batch workloads like SPEC, AstriFlash needs to absorb ?s-scale stalls and support latencysensitive workloads. As online services can incur multiple outstanding flash accesses, flexible user-level threads are a costeffective way to provide the traditional sequential execution abstraction to programmers and manage 100s of execution contexts <ref type="bibr" target="#b63">[65]</ref>, while hardware threads are limited in number. AstriFlash uses a simple user-level thread library <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b59">[61]</ref> to switch between threads in 100 ns, which is 50x faster than context switches, and 5x faster than recent proposals <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b63">[65]</ref>. AstriFlash also provides architectural support in OoO cores to invoke the user-level thread scheduler on a DRAMcache miss, where the scheduler stops the running thread and schedules the next available thread. When the original thread is rescheduled after the flash access completes, it resumes from the instruction that caused the DRAM-cache miss and successfully reads the required data from the DRAM cache. AstriFlash also provides scheduling policies and architectural support to minimize request starvation.  The ?s-scale switch-on-miss design requires microarchitectural changes because the existing OoO cores assume that all successfully launched memory instructions finish synchronously. In contrast, on a DRAM-cache miss, AstriFlash aborts the responsible instruction, e.g., a committed store residing in the Store Buffer, and resets the core state to that of the last finished instruction before invoking the user-level thread scheduler. While a similar problem has been studied in the context of rare memory traps, previous proposals <ref type="bibr" target="#b60">[62]</ref> rely on the OS to handle the trap. However, as DRAM-cache misses happen every ?10 ?s, AstriFlash cannot rely on the heavyweight OS mechanisms and requires microarchitectural support. As any memory instruction can potentially trigger a DRAM-cache miss and thus has to be aborted, instructions can only retire after the previous instruction completes, therefore disabling the performance critical memory reorderings from relaxed memory consistency models <ref type="bibr" target="#b54">[55]</ref> and resulting in slow, sequentially-consistent execution. However, the memory reorderings can be performed speculatively if a rollback mechanism ensures correctness in case of aborts and discards all the speculative instructions. The speculation succeeds if the memory instruction completes, thus allowing the following speculatively-executed instructions to retire legally. Such speculation mechanisms <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b75">[77]</ref> have been extensively studied in the context of memory consistency models and require extra tracking structures per core.</p><p>2) Memory-side design: As memory management and flash interaction contribute ?5 ?s of paging overhead per flash access, AstriFlash employs a hardware-managed DRAM cache to provide efficient near-DRAM capacity management and orchestration of data movement, thus eliminating the traditional OS paging overheads. Removing the explicit DRAM-capacity management in the OS also eliminates OS synchronization due to page-table modifications and TLB shootdowns for data movement between DRAM and flash, while substituting OS-based page replacement policies with cache eviction policies and I/O scheduling logic with hardware-triggered flash accesses. AstriFlash requires DRAM controllers to look up the DRAM cache and determine hit or miss decisions that are communicated to the requesting core, while the misses cause the corresponding pages to be fetched from flash.</p><p>Incorporating a GB-scale DRAM cache requires careful consideration in picking the page size. Traditional SRAM caches have 64B blocks as they need a small block size to track multiple independent data items to benefit from the temporal locality closer to the cores. In contrast, the DRAM cache being the last level in the cache hierarchy should be tailored for spatial locality as temporal locality is scarce. Moreover, having 64B blocks in a 32GB cache will require ?4GB of tags which is impractical. Therefore, the DRAM cache in AstriFlash should employ a larger page size such as 4KB. However, even with a 4KB block size, we still need 64MB of tags which will be prohibitively expensive to hold in an SRAM tag array. Therefore, we conservatively employ designs that hold the tags in the DRAM cache at the cost of serialized tag and data lookup <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. System designers can pick alternative DRAM-cache designs <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b61">[63]</ref> if they can accommodate the required SRAM tag array.</p><p>Similar to the non-blocking on-chip SRAM caches, the DRAM cache also needs to track multiple concurrent misses. While the on-chip caches use expensive SRAM structures (MSHRs) to track 10s of concurrent misses, the DRAM cache can have 100s of concurrent misses because of the long flash access latency and thus cannot rely on expensive SRAM structures. AstriFlash provides scalable bookkeeping of the DRAM-cache misses using hardware support for an in-DRAM miss status table, which is inexpensive compared to the typical CAM-based MSHR solutions. To the best of our knowledge, previous DRAM-cache proposals <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b61">[63]</ref> suffer from similar problems but do not provide any solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ASTRIFLASH IMPLEMENTATION</head><p>This section describes the detailed implementation of AstriFlash mechanisms in the same sequence as required in the DRAM-cache miss-handling control path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Flash addressing and memory mapping</head><p>AstriFlash uses existing PCIe mechanisms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref> to create memory mappings for flash. PCIe devices have Base Address  Registers (BARs) which contain the base address and offset of the address space assigned to an endpoint device such as an SSD. At boot time, the OS reads these BARs and acknowledges them as the physical address space assigned to the SSD. Then, the OS can use the page tables to directly map virtual addresses to these physical addresses, thereby exposing the SSD as a memory device. The obtained physical addresses are used to look up data in the cache hierarchy and are equivalent to the Logical Page Numbers of the SSD used internally in Flash Translation Layer and wear-leveling.</p><p>Address translation for servers provisioned with TBs of DRAM <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b64">[66]</ref> is an important problem <ref type="bibr" target="#b26">[27]</ref> as modern TLB hierarchies cannot provide enough coverage. AstriFlash can benefit from previously proposed solutions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b79">[81]</ref>, where Midgard <ref type="bibr" target="#b26">[27]</ref> in particular is an excellent fit because it relies on large cache hierarchy capacity to reduce address translation overheads. AstriFlash and DRAM-only system have similar address translation overheads because both the hot data and corresponding page tables are served from the on-chip or DRAM caches. However, in the case of cold data accesses, both the data and the required page tables might have to be retrieved from flash. While AstriFlash's switch-on-miss architecture can provide DRAM-like throughput, serving page table from flash using a serialized page table walk might cause the application to violate its SLO. To address this challenge, AstriFlash employs available hardware and OS support to ensure that page tables are always DRAM resident.</p><p>On the hardware side, we use a hybrid-DRAM architecture similar to Intel's Knights Landing <ref type="bibr" target="#b66">[68]</ref> in which the DRAM is split into a cache and a flat space that the OS can use directly. On the OS side, recent proposals <ref type="bibr" target="#b1">[2]</ref> have engineered Linux to ensure page tables are allocated in specific DRAM nodes. Therefore, the OS can assuredly place page tables in a DRAM partition that is directly exposed to the OS. Such a design requires the DRAM controller to logically partition the available number of DRAM rows into flat and cached parts at boot time. The flat rows do not contain tags and expose a unique physical address range to the OS using BARs, while the cached rows contain tags and should be probed using the physical address range exposed by the flash BARs. For every memory access, if the requested physical address belongs to BARs dedicated to the flat rows, then the data is retrieved from the required flat row as per traditional DRAM design. Otherwise, the cached rows are searched for the required page using the set index and tag bits. Overall, AstriFlash can place page tables directly in DRAM and provide the address translation characteristics of a traditional DRAM-only system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DRAM-cache organization</head><p>AstriFlash uses a DRAM-cache design <ref type="bibr" target="#b34">[35]</ref> with 4KB page granularity instead of block-based caches <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b61">[63]</ref>, therefore enabling a simple page fetch from flash on a DRAM-cache miss. Each DRAM row is equivalent to a set in a set-associative cache and contains both tag and data fields, as shown in Figure <ref type="figure" target="#fig_6">5a</ref>. We implement two DRAM-cache controllers, where the frontside controller manages the DRAM-cache accesses, while the backside controller manages the DRAM-cache misses.</p><p>1) Frontside controller (FC): Figure <ref type="figure" target="#fig_6">5b</ref> depicts that FC handles all DRAM data requests from the on-chip caches. It calculates hit/miss decisions by looking up the tags and sends replies for each request. We design FC by extending a traditional DRAM controller which inherits typical DRAM commands and scheduling policies. When FC receives a request, it calculates the set index (row number) from the address. It then checks if the row contains the page using a Row Address Strobe (RAS) operation to fetch the row into the DRAM row buffer, followed by a Column Address Strobe (CAS) operation to fetch the tags and compare them against the requested address. The tags contain the physical addresses of the pages as exposed by flash and each tag occupies 8B. Therefore, each tag column (64B) can map up to 8 ways. If one of the tags matches, FC fetches the requested data with further CAS operations and sends it to the LLC. If no tag matches the requested address, FC sends a miss request to the backside controller's queue to fetch the requested page from flash. If the queue is full, FC stalls while waiting for free entries in the queue. Once the backside controller accepts the miss request, FC generates and sends a miss response for the data request to the LLC.</p><p>2) Backside controller (BC): Figure <ref type="figure" target="#fig_6">5c</ref> depicts that BC interacts with flash and manages the metadata required for handling misses. As flash accesses are long (50 ?s), BC has ample time to perform miss-handling operations. We propose implementing BC as programmable logic using microcode or software <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b44">[45]</ref> instead of hardwired FSMs, thus enabling the implementation of flexible and complex policies.</p><p>BC issues a 4KB read request to fetch a page from flash using the physical address. The on-chip network routes the requests generated by BC to the PCIe controller, which forwards them to flash <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>. To receive the requested page, BC needs to secure available space in the corresponding DRAM set and might require evicting an existing page. After BC requests the page from flash, it identifies a victim page and copies it to the evict buffer. If the evicted page is dirty, it is written back to flash off the critical path. Once the requested page arrives, BC installs the data and tag in the designated set and way.</p><p>Traditionally, on-chip caches manage misses with Miss Status Handling Registers (MSHRs). As MSHRs are CAM-based and expensive, there are only tens of MSHRs per cache. In AstriFlash, as each miss lasts for 50 ?s, it is possible to have hundreds of concurrent misses, which makes it too expensive to implement enough MSHRs. Instead, AstriFlash tracks the outstanding misses in a specialized DRAM row called Miss Status Row (MSR). The MSR stores miss-handling entries containing the addresses and metadata of missing pages. To allow fast searches, we design the MSR as a set-associative structure where each entry is 8B and can be retrieved with a CAS operation. Once a DRAM-cache miss is detected, BC checks the MSR for a pending miss to the same page to avoid issuing duplicate requests to flash. If BC finds an existing entry, it discards the request; otherwise, it locates free entries in the set and allocates a new entry for the request. In case of no free entries, BC waits for pending flash requests to finish and free an MSR entry. Once the requested page arrives, BC removes the matching MSR entry, thus indicating miss completion.</p><p>In AstriFlash, the DRAM cache buffers all write operations that happen only on dirty page evictions, leading to fewer flash writebacks that are de-prioritized against reads. Flash writes are expensive as they can trigger garbage collection required for wear leveling, incurring up to 100ms of latency <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b78">[80]</ref>. To minimize garbage collection overheads, we suggest employing previous proposals <ref type="bibr" target="#b78">[80]</ref> that perform block erasure only in the local plane, thus reducing wear-leveling latency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ?s-scale switch-on-miss architecture</head><p>Once FC detects a DRAM-cache miss, it sends a miss signal to the core to trigger a thread switch instead of the traditional synchronous wait for data.</p><p>1) Sending a miss signal to the core: As the memory request corresponding to the DRAM-cache miss cannot be completed immediately and should be executed later, all resources allocated to the memory request should be reclaimed for the system to progress. E.g., if MSHRs retain memory requests that miss in the DRAM cache, all the MSHRs will eventually get occupied, and the on-chip caches will block. The mechanism required here is similar to the existing DRAM ECC error interface. When a non-correctable DRAM ECC error occurs, the DRAM controller generates an exception for the requesting core, and all the resources allocated to the request in the cache hierarchy are reclaimed <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b68">[70]</ref>. AstriFlash piggybacks miss signaling on the same mechanism, freeing up the allocated MSHRs at each cache level and sending the miss signal up the hierarchy towards the requesting core.</p><p>2) Triggering a thread switch: The miss signal triggers a user-level thread switch after reaching the core. Similar to previous proposals <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b53">[54]</ref>, our mechanism for thread switching requires a new Handler Address Register and Resume Register as part of the architectural state. For each process, the handler address register contains the virtual address of the user-level handler, which will trigger a thread switch using the user-level thread library. For security purposes, this register can only be written in privileged mode, thus requiring an additional system call to verify and install a legitimate handler address. In contrast, the resume register can be read and written in user mode. Both these registers are part of the normal process state and will change on a context switch.</p><p>The core-side MSHRs track the memory requests sent to the cache hierarchy and can link the incoming miss signal back to the triggering instruction, which is assumed to be in the ROB as shown in Figure <ref type="figure" target="#fig_7">6</ref>. Once the miss-triggering instruction is identified and all the older instructions have retired, the ROB is flushed and the Program Counter (PC) is set to the handler address. The PC of the miss-triggering instruction is saved in the resume register so that the thread can later resume from the same instruction. Thus, the CPU is not stalled for flash accesses and the control is passed back to the program.</p><p>3) Forward progress guarantees: The execution of multiple threads can cause a deadlock in AstriFlash. When rescheduling a miss-generating thread, the requested page might have already been evicted because of DRAM-cache contention among multiple threads and cores, thus preventing the thread from progressing. To handle such cases, AstriFlash includes an architectural mechanism to force forward progress of threads when they are rescheduled so that they are not de-scheduled again before retiring at least one instruction. Therefore, even if the requested page is not present in DRAM when the thread is rescheduled, the thread is not switched out and blocks the core while waiting for the flash response. Moreover, AstriFlash can also expose such contention events to software for management at a higher level. AstriFlash implements this mechanism by adding a forward progress bit to the resume register. When a thread is rescheduled and the scheduler needs to force it to make forward progress, it stores the PC of the resuming instruction in the resume register and sets the forward progress bit. If this bit is set, the new memory request for the resuming instruction is forced to complete synchronously at FC, even if the DRAM cache misses. Therefore, the resuming instruction will block the core until it receives the requested data from the memory hierarchy, after which it retires and unsets the forward progress bit. Thus, the scheduler can flexibly choose between thread switches and forward progress while using contention information for scheduling or OS monitoring.</p><p>4) Precise exceptions and speculative stores: In modern processors, each core has a Store Buffer (SB) to collect stores that have retired but not completed. As stores do not produce direct register values for younger instructions, the store can be retired once its value and address are obtained and it is at the head of the ROB, and is sent to the SB where it awaits completion. In AstriFlash, as the DRAM cache might miss for store accesses, a thread switch will be triggered on corresponding stores. However, if the store has already retired from the pipeline and is resident in the SB, it cannot be discarded using existing speculation mechanisms. Therefore, as it is always possible for a store and the following instructions to be aborted due to a DRAM-cache miss, the OoO core is forced to run in a sequentially-consistent manner, thus prohibiting any memory reorderings of relaxed memory consistency models <ref type="bibr" target="#b54">[55]</ref> and the non-speculative retirement of younger instructions. We employ post-retirement speculation techniques <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b75">[77]</ref> that regain the performance of relaxed memory consistency models by speculatively reordering memory operations.</p><p>Based on ASO <ref type="bibr" target="#b75">[77]</ref>, we expand the speculation mechanisms already present in the ROB to cover the SB so that we can abort the "speculative" stores in case of a DRAM-cache miss. For typical ROB exceptions/speculation, each instruction's physical register mappings are kept until it retires from the ROB. In case of an exception or misspeculation, the core reverts to the older mappings before the instruction and discards newer mappings. We extend the speculation mechanism so that the mappings for a store are only freed when it leaves the SB. We assume a 4-way OoO ARM A76 core with 128-entry ROB, 32-entry SB, and a base 128-entry Physical Register File (PRF). We analyze our workloads to find that an average of four registers are modified between two stores, requiring four additional physical registers per store in the SB. Therefore, a 32-entry SB requires 32*4=128 additional registers in our PRF, equivalent to 1KB of additional SRAM. Finally, each store entry in the SB requires a map table to track the associated physical registers, where each map table entry represents 8-bit PRF indices for 32 registers, therefore requiring 32*32*8b = 1KB of SRAM. As PRF and map tables consume most of the additional silicon in ASO, we discount the silicon required for any additional microarchitectural structures. Based on 7nm SRAM density projections, modern silicon layouts can provide 2MB SRAM/mm 2 while Cortex A76 core is 1.3mm 2 in size. Therefore, our 2KB overhead per core occupies 0.001mm 2 (0.1% of Cortex A76), which might be acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Incorporating user-level threads</head><p>AstriFlash uses a user-level threading library that interacts with the hardware to provide hardware-triggered ?s-scale thread switches on a DRAM-cache miss. The thread scheduler is designed to ensure that online services can satisfy their taillatency requirements. We implement the proposed user-level threading library in C and Assembly and evaluate it with a cycle-accurate full-system simulator described in section V.</p><p>1) User-level threads: For each physical core, we assume a single global queue that receives jobs from the clients. The userlevel scheduler picks new jobs from this queue and executes them on user-level worker threads. The same scheduler manages the context of each thread and switches among them for cooperative multithreading. This scheduling model <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b59">[61]</ref> allows the applications to be easily ported to the AstriFlash infrastructure using the traditional notion of threads.</p><p>The scheduler cannot preempt the user-level threads directly as jobs are much smaller than the typical OS time quantum. Therefore, AstriFlash executes the jobs on a run-to-completion basis, except when they trigger a DRAM-cache miss and have to wait for data access from flash to complete. AstriFlash allows the user-level scheduler to be triggered on a DRAM-cache miss, which can be enabled by installing the scheduler handler's address in the handler address register. Once the scheduler is triggered, it deschedules the running thread that suffered the DRAM-cache miss and schedules a ready thread, thus overlapping the wait for the flash reply with useful work. The scheduler backs up the context for the running thread, consisting of the general-purpose registers and AstriFlash-specific resume register, and stores it on the thread stack. Logically, on a DRAM-cache miss, the running thread is halted and is stored in a pending job queue, as shown in Figure <ref type="figure" target="#fig_9">8</ref>. The pending queue's size is limited so that pending jobs do not exceed the  tail-latency requirements. Once the pending queue is full, any new DRAM-cache misses result in the scheduler waiting for the flash response for the oldest job.</p><p>2) Priority scheduling with aging: The user-level thread library schedules new and pending jobs where the new jobs have never been scheduled for execution, while the pending jobs were halted after being scheduled because they suffered a DRAM-cache miss. The scheduler is optimized to provide the service latency distribution that matches the ideal Flash-Sync system (Figure <ref type="figure" target="#fig_5">4b</ref>), where the jobs wait for flash to respond.</p><p>We implement a priority scheduler that assigns a default priority of one to pending jobs and a higher priority of two to new jobs to overlap the flash access latency with useful work. However, similar to traditional priority scheduling <ref type="bibr" target="#b69">[71]</ref>, AstriFlash also needs to handle the starvation problem. In a skewed job distribution, it is possible to have consecutive new jobs that do not face DRAM-cache misses, causing a simpler scheduler to starve the pending queue. However, Aging policies can be used to prevent such starvation. Each job entry records a timestamp when it enters the pending queue. When picking a new job to execute, the scheduler checks the head of the pending queue. If the age of the head job is greater than the average flash response time, then the scheduler picks it to execute; otherwise, it picks a new job, as shown in Figure <ref type="figure" target="#fig_9">8</ref>. To combat latency spikes due to garbage collection and flashside queuing, it is possible to program the backside controller and create a notification mechanism using queue pairs that can notify the core upon page arrivals from flash, similar to modern storage response arrivals <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b67">[69]</ref>. The scheduler can then read the queue pairs and schedule the corresponding thread. As datacenter jobs typically take 10-100 ?s to finish, which is comparable to the flash latency itself, the pending jobs can be scheduled soon after their data arrives from flash. In this manner, the user-level thread scheduler maintains a service latency distribution similar to the Flash-Sync system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applications and system architecture</head><p>We implement a user-level threading library that spawns 32-64 user threads per core (depending on the workload) and provides fast switching amongst them. We evaluate Silo and Masstree workloads from Tailbench <ref type="bibr" target="#b39">[40]</ref> and port them to our threading library with few changes. We also evaluate five workloads from a microbenchmark suite <ref type="bibr" target="#b27">[28]</ref> to capture data structure access patterns along with highlevel database operations such as TATP and TPCC. In Array Swap, each operation swaps two array elements, generating  and 'neworder' transactions for items in a database. We model data accesses with an analytical Zipfian distribution so that the benchmarks trigger a DRAM-cache miss every 5-25 ?s. Our workloads mimic limited write traffic as identified by previous proposals <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b74">[76]</ref>, resulting in infrequent garbage collection events and practical endurance/lifetime for flash. We model 16? ARM Cortex-A76 cores with 1MB LLC per core. We scale down our 1TB dataset for 64 cores to a 256GB dataset for 16 cores and use an 8GB (3%) DRAM cache while flash stores the 256GB dataset. Both the DRAM cache and flash use the standard page size of 4KB. The frontside controller is an FSM that extends the traditional DRAM controller and uses FR-FCFS scheduling. We model one cycle each to issue commands for opening DRAM rows and columns, sending hit/miss responses to the on-chip caches, and miss requests to the backside controller. In contrast to the frontside controller, the backside controller is programmable and is slower in issuing requests. We model three cycles each for issuing DRAM commands and sending requests to flash.</p><p>For the AstriFlash scheduler, we implement job-based priority scheduling as described in subsection IV-D. We model a large job queue to evaluate the maximum throughput the system can sustain while also monitoring the service time of each job. Apart from actual work, the service time includes the wait time in case of a DRAM-cache miss but does not include the time spent waiting in the job queue. To measure the tail latency distribution, we use a Poisson process to model request arrival times and measure both the queuing time in the job queue and the service time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluated configurations</head><p>We use QFlex <ref type="bibr" target="#b57">[59]</ref>, a cycle-accurate full-system simulator based on Flexus <ref type="bibr" target="#b76">[78]</ref> to evaluate AstriFlash. Table I lists the detailed simulation parameters used in QFlex. We evaluate the following configurations:</p><p>1) DRAM-only represents ideal performance, as all the data is served from DRAM without any flash accesses. 2) AstriFlash represents our proposal, where DRAM acts as a hardware cache and contains the hot data, while the backing flash contains the whole dataset. A priority scheduler is used for switching among user-level threads, and each switch costs around 100ns. 3) AstriFlash-Ideal represents the AstriFlash design with no cost associated with thread switching. 4) AstriFlash-noPS represents AstriFlash with a FIFO scheduling policy instead of Priority Scheduling. 5) AstriFlash-noDP represents AstriFlash without DRAM partitioning, and thus TLB misses can incur flash-based page-table walks (subsection IV-A). 6) OS-Swap represents traditional systems where the OS uses paging to swap pages between the DRAM and flash (subsection III-A). 7) Flash-Sync represents FlatFlash <ref type="bibr" target="#b0">[1]</ref>, a latency-optimized system where the core waits for flash accesses to complete synchronously, thus resulting in a 50 ?s delay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>This section describes our evaluation of AstriFlash based on cycle-accurate simulation. The evaluation results include throughput, service time, and tail latency comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Throughput comparison</head><p>We first evaluate the throughput of AstriFlash compared to a DRAM-only system. Figure <ref type="figure" target="#fig_10">9</ref> shows that AstriFlash achieves an average of 95%, while AstriFlash-Ideal achieves 96% of the DRAM-only system's throughput. The 5% throughput loss is because of the DRAM-cache tag comparisons and response wait, pipeline flush for every DRAM-cache miss, and the scheduler overhead for switching threads.</p><p>For each DRAM-cache miss, DRAM-side tag checking is an overhead because it does not result in data access and will be repeated later. When the miss signal is sent to the core, the pipeline is flushed to redirect control to the user-level handler. As modern processors feature 100s of ROB entries, each flush loses useful work done by the OoO pipeline resulting in throughput degradation. As TPCC is the most computationally intensive workload, there is higher throughput degradation as  each ROB flush is comparatively costlier. Finally, the user-level thread scheduler also causes throughput degradation as each switch on a DRAM-cache miss takes 100ns.</p><p>The OS-Swap configuration achieves 58% of the DRAMonly system's throughput as it has high page fault and context switch overheads for each DRAM-cache miss. The Flash-Sync configuration achieves only 27% of the DRAM-only system's throughput because the core has to wait for flash to respond to each DRAM-cache miss. Overall, AstriFlash achieves DRAMlike throughput while reducing the memory cost by 20x using a small DRAM cache and cost-effective flash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Service-latency comparison</head><p>AstriFlash must schedule both new and pending jobs fairly so that the service latency of the pending jobs does not lead to SLO violations. We show that AstriFlash -along with an optimized priority-based scheduling policy -achieves a similar latency distribution as in the Flash-Sync system. Table <ref type="table" target="#tab_5">II</ref> compares the 99th-percentile latency from AstriFlash against AstriFlash-noPS that does not use the Priority Scheduler and AstriFlash-noDP that does not have DRAM partitioning, thus resulting in flash-based page-table walks. We normalize all latencies to the 99th-percentile latency of Flash-Sync as it represents the ideal latency when accessing flash. Compared to Flash-Sync, AstriFlash has a 2% latency degradation as once the requested page arrives, the non-preemptive scheduler might have to wait for the current job to finish before it can schedule the pending job. In contrast, AstriFlash-noPS has a ?7x latency degradation as the scheduler executes new jobs even if the requested page for a pending job has arrived and only checks the pending queue when encountering a miss. Thus, the priority scheduler prevents the pending queue from starvation by checking it after every request. AstriFlash-noDP has a ?70% latency degradation as page-table entries for cold pages have to be fetched from flash, which affects the 99thpercentile latency and can violate the SLO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tail-latency comparison</head><p>We study the tail-latency distribution for TATP, as it represents the short database operations present in datacenter workloads and takes ten ?s on average. We use a Poisson process to model a bursty request arrival distribution and sweep the average inter-arrival time between requests from zero to ten ?s, where each value represents a different request load in the system. Figure <ref type="figure" target="#fig_11">10</ref> represents the tail-latency distribution of the DRAM-only system and AstriFlash. The X-axis represents the system throughput normalized to the maximum throughput of the DRAM-only system, while the Y-axis represents the 99th-percentile latency normalized to the average service time of the DRAM-only system, as previously shown in Figure <ref type="figure" target="#fig_3">3</ref>. AstriFlash has higher 99th-percentile latency even at low loads with negligible queuing because of requests that require flash access. However, as the load increases, queuing latency also increases allowing the AstriFlash switch-on-miss architecture to overlap the flash access latency with queuing latency. Therefore, AstriFlash with 93% throughput matches the tail latency of a DRAM-only system with 96% throughput, thus maintaining the same tail latency with only 3% less throughput and 20x less memory cost. The tail latency for the same load is worse because each DRAM-cache miss flushes the ROB, and the following thread switch destroys the on-chip cache/TLB locality, incurring more on-chip misses. It is also possible that requests which have already faced a long queuing delay further incur a DRAMcache miss, thus exacerbating their overall response latency. Even though the service latency is higher as AstriFlash includes the flash access time, the queueing latency is significantly lower because fast DRAM-miss-triggered thread switches reduce the overall queueing, thus maintaining the same response time. Overall, as AstriFlash achieves near-DRAM 99th-percentile response latency, it enables online services to serve data directly from flash while maintaining the overall tail-latency constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Garbage collection overheads</head><p>Garbage collection events in flash may block incoming read requests. For a flash with 256GB capacity, garbage collection blocks 4% of read/write requests <ref type="bibr" target="#b78">[80]</ref>, which impacts the tail latency. As AstriFlash uses a 1TB flash with more chips, the number of blocked requests reduces by more than 4x the 256GB flash, affecting less than 1% of the total requests. Moreover, AstriFlash can employ previously proposed local garbage collection algorithms to further enforce tail latency <ref type="bibr" target="#b78">[80]</ref>. Finally, as flash writes are asynchronous, garbage collection generally happens off the critical path, thus preventing AstriFlash from suffering severe tail latency degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>AstriFlash is inspired by various previous proposals: Flash integration: Flash-based memory systems significantly improve performance compared to disks. SSDAlloc <ref type="bibr" target="#b7">[8]</ref> proposes a hybrid DRAM/flash memory manager while using flash as a log-structured page store. FlashMap <ref type="bibr" target="#b30">[31]</ref> maps flash into a unified address space with DRAM. 2B-SSD <ref type="bibr" target="#b8">[9]</ref> proposes accessing the same file with two separate byte-based and blockbased I/O paths by utilizing a byte-addressable SSD and MMIO. FlatFlash <ref type="bibr" target="#b0">[1]</ref> proposes a horizontally-tiered flash-based memory system to use DRAM as a cache for flash with customized page promotion techniques. A similar system was proposed by PageSeer <ref type="bibr" target="#b42">[43]</ref> in the context of NVM. Overall, these proposals aim for a tighter integration of flash with the CPU.</p><p>Emerging memory technologies: NVM provides lower latency, higher bandwidth, and better endurance than flash <ref type="bibr" target="#b27">[28]</ref>. Eisenman et al. <ref type="bibr" target="#b21">[22]</ref> propose an NVM-based memory system for Facebook's workloads. They also study trade-offs across capacity, latency, and persistence and propose using NVM as a software-controlled cache between DRAM and flash. While NVM provides various attractive performance use cases, it does not benefit from economies of scale as of now and thus has severe volatility in its cost.</p><p>User-level threading and killer microseconds: AstriFlash's switch-on-miss architecture is inspired by informing memory operations <ref type="bibr" target="#b29">[30]</ref> with lightweight multi-threading <ref type="bibr" target="#b53">[54]</ref>. As lean user-level threading libraries <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b59">[61]</ref> provide fast ns-scale thread switching, they are also useful for hiding DRAM accesses, e.g., co-routines for databases <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b58">[60]</ref>. Duplexity <ref type="bibr" target="#b52">[53]</ref> shares resources in OoO cores with other smaller cores to hide ?s-scale stalls, while Cho et al. <ref type="bibr" target="#b18">[19]</ref> apply systemlevel modifications to get rid of ?s-scale stalls. AIFM <ref type="bibr" target="#b63">[65]</ref> uses highly-optimized user-level preemptions to migrate pages between local and remote devices efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We proposed AstriFlash, a flash-based system for online services that eliminates traditional demand paging overheads by employing a hardware/software co-designed ?s-scale switchon-miss architecture. AstriFlash serves data directly out of flash and achieves DRAM-like performance, thus allowing integration of denser and slower memory technologies for online services. Overall, AstriFlash reduces the memory cost by 20x, providing a solution for future TB-scale memory systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Miss rate and flash bandwidth vs. DRAM capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Asynchronous flash accesses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: 99th-percentile latency normalized to the average service time of a DRAM-only system with a single physical server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>Access DRAM cache: Miss! 2 Notify backside controller of miss 3 Send page request to flash 4 Send miss signal to core 5 Switch to another thread 6 Receive page from flash F Frontside controller B Backside controller (c) AstriFlash.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of hybrid memory system designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: AstriFlash DRAM cache structure and controller logic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Switch-on-miss hardware-software interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Aborting speculative stores on a DRAM-cache miss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Priority-based thread scheduling in AstriFlash.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Throughput comparison of different configurations normalized to a DRAM-only system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Comparison of 99th-percentile latency normalized to average service time of a DRAM-only system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>System parameters for simulation on QFlex. both reads and writes. Red Black Tree (RBT) and Hash Table perform data structure lookups with pointer chasing behavior. TATP and TPCC execute 'update subscriber data'</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Comparison of 99th-percentile service latency normalized to the Flash-Sync configuration service latency.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Alexandros Daglis</rs>, <rs type="person">Mario Drumond</rs>, <rs type="person">Emilien Guandalino</rs>, <rs type="person">Slim Fatnassi</rs>, and the anonymous reviewers for their feedback and support. This work was partially supported by <rs type="funder">FNS</rs> projects "<rs type="projectName">Hardware/Software Co-Design for In-Memory Services</rs>" (<rs type="grantNumber">200020B 188696</rs>) and "<rs type="projectName">Memory-Centric Server Architecture for Datacenters</rs>" (<rs type="grantNumber">200021 165749</rs>), a <rs type="grantName">Qualcomm Innovation Fellowship</rs> (<rs type="grantNumber">461760</rs>), and the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korean government (MSIT)</rs> (<rs type="grantNumber">2021R1G1A1094978</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_4fPr59Z">
					<idno type="grant-number">200020B 188696</idno>
					<orgName type="project" subtype="full">Hardware/Software Co-Design for In-Memory Services</orgName>
				</org>
				<org type="funded-project" xml:id="_2DgqsAX">
					<idno type="grant-number">200021 165749</idno>
					<orgName type="grant-name">Qualcomm Innovation Fellowship</orgName>
					<orgName type="project" subtype="full">Memory-Centric Server Architecture for Datacenters</orgName>
				</org>
				<org type="funding" xml:id="_a8T2EKH">
					<idno type="grant-number">461760</idno>
				</org>
				<org type="funding" xml:id="_yZB3Z5z">
					<idno type="grant-number">2021R1G1A1094978</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FlatFlash: Exploiting the Byte-Accessibility of SSDs within a Unified Memory-Storage Hierarchy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H M O</forename><surname>Abulila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mailthody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 24th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASPLOS-XXIV</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="971" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory Machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXV)</title>
		<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="283" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Amd Epyc</surname></persName>
		</author>
		<ptr target="https://www.amd.com/en/products/cpu/amd-epyc-7742" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing the TLB Shootdown Algorithm with Page Access Tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2017 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 13th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="95" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Anton</forename><surname>Shilov</surname></persName>
		</author>
		<ptr target="https://www.tomshardware.com/news/analysts-predict-ssd-prices-may-halve-by-mid-2023" />
		<title level="m">Analysts Predict SSD Prices May Halve by Mid-2023</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Amazon EC2 On-Demand Pricing</title>
		<author>
			<persName><surname>Aws</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ec2/pricing/on-demand" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SSDAlloc: Hybrid SSD/RAM Memory Management Made Easy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 8th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2B-SSD: The Case for Dual, Byte-and Block-Addressable Solid-State Drives</title>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 45th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="425" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">EC2 High Memory Update: New 18 TB and 24 TB Instances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barr</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/blogs/aws/ec2-high-memory-update-new-18-tb-and-24-tb-instances/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attack of the killer microseconds</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient virtual memory for big memory servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 40th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">IX: A Protected Dataplane Operating System for High Throughput and Low Latency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Operating System Design and Implementation</title>
		<meeting>the 11th Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Architectural and Operating System Support for Virtual Memory, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">InvisiFence: performance-transparent memory ordering in conventional multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 36th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved Multithreading Techniques for Hiding Communication Latency in Multiprocessors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ranade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 19th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 24th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASPLOS-XXIV</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="107" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taming the Killer Microsecond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Honarmand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 51st Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="627" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PCIe 5.0 is just beginning to come to new PCs, but version 6.0 is already here</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<ptr target="https://arstechnica.com/gadgets/2022/01/pci-express-6-0-spec-is-finalized-doubling-bandwidth-for-ssds-gpus-and-more/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The tail at scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing DRAM footprint with NVM in facebook</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EuroSys Conference</title>
		<meeting>the 2018 EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">O</forename><surname>Koc ?berber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 17th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASPLOS-XVII</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is SC + ILP=RC</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gniady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 26th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accelerating Database Operations Using a Network Processor</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st international workshop on Data management on new hardware</title>
		<meeting>the 1st international workshop on Data management on new hardware</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 14th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASPLOS-XIV</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rebooting Virtual Memory with Midgard</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Payer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 48th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="512" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed Logless Atomic Durability with Persistent Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="466" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward Dark Silicon in Servers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6" to="15" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Informing Memory Operations: Providing Memory Performance Feedback in Modern Processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 23rd International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unified address translation for memory-mapped SSDs with FlashMap</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 42nd International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="580" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient IO with io uring</title>
		<author>
			<persName><surname>Io Uring</surname></persName>
		</author>
		<ptr target="https://kernel.dk/iouring.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Memory Systems: Cache, DRAM, Disk</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">DRAM price increases will ease</title>
		<author>
			<persName><forename type="first">James</forename><surname>Carbone</surname></persName>
		</author>
		<ptr target="https://electronics-sourcing.com/2022/05/12/dram-price-increases-will-ease/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unison Cache: A Scalable and Effective Die-Stacked DRAM Cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Die-stacked DRAM caches for servers: hit ratio, latency, or bandwidth? have it all with footprint cache</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 40th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="404" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Why Latency Impacts SSD Performance More Than Bandwidth Does</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hruska</surname></persName>
		</author>
		<ptr target="https://www.extremetech.com/computing/325146-why-latency-impacts-ssd-performance-more-than-bandwidth-does" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting Coroutines to Attack the &quot;Killer Nanoseconds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">F</forename><surname>Minhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Levandoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Nishanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1702" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shinjuku: Preemptive Scheduling for ?second-scale Tail Latency</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kaffes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mazi?res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Symposium on Networked Systems Design and Implementation</title>
		<meeting>the 16th Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tailbench: a benchmark suite and evaluation methodology for latency-critical applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the 2016 IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ReFlex: Remote Flash ? Local Flash</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXII</title>
		<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXII</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pocket: Elastic Ephemeral Storage for Serverless Analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stuedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pfefferle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Symposium on Operating System Design and Implementation</title>
		<meeting>the 13th Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="427" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PageSeer: Using Page Walks to Trigger Page Swaps in Hybrid Memory Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kokolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IEEE Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>the 25th IEEE Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reaping the performance of fast NVM storage with uDepot</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kourtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koltsidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th USENIX Conference on File and Storage Technologie (FAST)</title>
		<meeting>the 17th USENIX Conference on File and Storage Technologie (FAST)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meet the walkers: accelerating index traversals for in-memory databases</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">O</forename><surname>Koc ?berber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="468" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LATR: Lazy Translation Coherence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</title>
		<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXIII)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="651" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hints for Computer System Design</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Lampson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 9th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="33" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Architecting phase change memory as a scalable dram alternative</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 36th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Case for Hardware-Based Demand Paging</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 47th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1103" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Asynchronous I/O Stack: A Low-latency Kernel I/O Stack for Ultra-Low Latency SSDs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2019 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="603" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficiently enabling conventional block sizes for very large die-stacked DRAM caches</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards energy-proportional datacenter memory with mobile DRAM</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Nothaft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Periyathambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 39th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enhancing Server Efficiency in the Face of Killer Microseconds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IEEE Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>the 25th IEEE Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="185" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Software-Controlled Multithreading Using Informing Memory Operations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Ramkissoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>the 6th IEEE Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Primer on Memory Consistency and Cache Coherence, Second Edition, ser. Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scaleout NUMA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASPLOS-XIX</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fast crash recovery in RAMCloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">QFlex</title>
		<ptr target="https://qflex.epfl.ch" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Parallel Systems Architecture Lab (PARSA)</orgName>
		</respStmt>
	</monogr>
	<note>EPFL</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Interleaving with coroutines: a systematic and practical approach to hide memory latency in index joins</title>
		<author>
			<persName><forename type="first">G</forename><surname>Psaropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Legler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Arachne: Core-Aware Thread Management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Speiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Symposium on Operating System Design and Implementation</title>
		<meeting>the 13th Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="145" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Tolerating Late Memory Traps in ILP Processors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 26th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fundamental Latency Trade-off in Architecting DRAM Caches: Outperforming Impractical SRAM-Tags with a Simple and Practical Design</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 45th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="235" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scalable high performance main memory system using phase-change memory technology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 36th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">AIFM: High-Performance, Application-Integrated Far Memory</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Symposium on Operating System Design and Implementation</title>
		<meeting>the 14th Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="315" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Announcing the general availability of 6 and 12 TB VMs for SAP HANA instances on Google Cloud Platform</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/blog/products/sap-google-cloud/announcing-the-general-availability-of-6-and-12tb-vms-for-sap-hana-instances-on-gcp" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Intel Skylake</title>
		<author>
			<persName><surname>Skylake</surname></persName>
		</author>
		<ptr target="https://www.7-cpu.com/cpu/Skylake.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Knights Landing: Second-Generation Intel Xeon Phi Product</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sodani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gramunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vinod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chinthamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hutsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Storage Performance Development Kit</title>
		<author>
			<persName><surname>Spdk</surname></persName>
		</author>
		<ptr target="https://spdk.io/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Memory Errors in Modern Systems: The Good, The Bad, and The Ugly</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Debardeleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stearley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 20th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASPLOS-XX</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="297" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Modern operating systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Tanenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Pearson Prentice-Hall</publisher>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The context-switch overhead inflicted by hardware interrupts (and the enigma of do-nothing loops)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Computer Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Design guidelines for high-performance SCM hierarchies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Pnevmatikatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Memory Systems (MemSys)</title>
		<meeting>the International Symposium on Memory Systems (MemSys)</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Memory systems and interconnects for scale-out servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<ptr target="http://infoscience.epfl.ch/record/211040" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">EPFL Thesis</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fat Caches for Scale-Out Servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Architecting a hardware-managed hybrid DIMM optimized for cost/performance</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haukness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haywood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Linstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bronner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Memory Systems (MemSys)</title>
		<meeting>the International Symposium on Memory Systems (MemSys)</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mechanisms for store-wait-free multiprocessors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 34th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="266" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">SimFlex: Statistical Sampling of Computer System Simulation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">ARM Cortex A76</title>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/armholdings/microarchitectures/cortex-a76" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Conference on File and Storage Technologie</title>
		<meeting>the 15th USENIX Conference on File and Storage Technologie</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Translation ranger: operating system support for contiguity-aware TLBs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 46th International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="698" to="710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
