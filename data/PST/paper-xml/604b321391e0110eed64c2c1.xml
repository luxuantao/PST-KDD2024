<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hurdles to Progress in Long-form Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-10">10 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
							<email>kalpesh@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country>♦ Google Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
							<email>aurkor@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country>♦ Google Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hurdles to Progress in Long-form Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-10">10 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.06332v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends:</p><p>(1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / test overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We provide suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Long-form question answering (LFQA) integrates the retrieval component of open-domain QA, which involves searching a large external knowledge source for documents relevant to a given question, with a text generation component to produce paragraph-length answers. Significant progress has been made on open-domain QA datasets such as Natural Questions <ref type="bibr" target="#b21">(Kwiatkowski et al., 2019)</ref>, whose questions are answerable with short phrases and entities, by leveraging dense retrieval techniques like ORQA <ref type="bibr" target="#b23">(Lee et al., 2019)</ref>, REALM <ref type="bibr" target="#b13">(Guu et al., 2020)</ref>, and DPR <ref type="bibr" target="#b18">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b26">Lewis et al., 2020c;</ref><ref type="bibr" target="#b16">Izacard and Grave, 2020)</ref>. Methods inspired by these results have recently been combined with pretrained language models <ref type="bibr" target="#b25">(Lewis et al., 2020b;</ref><ref type="bibr" target="#b31">Petroni et al., 2020)</ref> and applied to the Reddit-derived "Explain Like I'm Five" (ELI5) dataset <ref type="bibr" target="#b9">(Fan et al., 2019)</ref>, which is the only publicly-available large-scale LFQA dataset.</p><p>The recently proposed KILT benchmark <ref type="bibr" target="#b31">(Petroni et al., 2020)</ref>, which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated answers (ROUGE-L against reference answers) and retrieved documents (R-precision against humanannotated relevant documents). In this paper, we build a state-of-the-art system 1 for ELI5 by using a sparse Transformer variant <ref type="bibr" target="#b37">(Roy et al., 2020)</ref> to condition over Wikipedia paragraphs returned by a REALM-style retriever <ref type="bibr" target="#b13">(Guu et al., 2020)</ref>.</p><p>However, despite its success on the KILT leaderboard, our system does not actually use the documents that it retrieves! To measure the effect of retrieval on generation quality, we design a control experiment in which retrieved documents are replaced with randomly-sampled documents at inference time. Results from both human A/B tests and automatic metrics like ROUGE-L demonstrate that conditioning on random documents has almost no effect on generated answer quality (Figure <ref type="figure" target="#fig_1">1c</ref>). We recommend that future LFQA research report the results of such control experiments in addition to reporting generation and retrieval quality. How can a system using random retrieval perform well on ELI5? Our analysis reveals that this result is partially due to significant train / validation overlap in the ELI5 dataset (Figure <ref type="figure" target="#fig_1">1a</ref>), which eliminates the need for external retrieval. A human study shows that at least 81% of validation questions have a paraphrase in the training set, and almost all validation questions are topically similar to a training set question. While <ref type="bibr" target="#b9">Fan et al. (2019)</ref> attempted to identify and remove question overlap using TF-IDF similarity, more complex semantic matching methods &amp; human verification is needed to address this issue in future LFQA datasets.</p><p>Digging deeper, we identify fundamental issues with using ROUGE-L to evaluate generated answer quality (Figure <ref type="figure" target="#fig_1">1b</ref>). Simple baselines such as just repeatedly copying the question, or choosing a random training set answer, can outperform LFQA systems such as RAG <ref type="bibr" target="#b26">(Lewis et al., 2020c)</ref> in terms of ROUGE-L. On the other hand, our system achieves higher ROUGE-L than reference human-written answers, which is misleading since human A/B testers strongly prefer reference answers to our system's. We conclude that ROUGE-L is not a reliable metric to evaluate LFQA due to its large and relatively unconstrained output space (e.g., compared to translation or summarization), and we offer suggestions for better automatic &amp; human evaluations to enable meaningful progress on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A state-of-the-art LFQA system</head><p>The ELI5 task <ref type="bibr" target="#b9">(Fan et al., 2019)</ref> asks models to generate paragraph-length answers to open-ended questions that often rely on world knowledge (e.g., how do jellyfish function without brains or nervous systems?). LFQA systems thus benefit from conditioning answer generation on relevant documents from the web (such as the Wikipedia article about jellyfish). While large-scale pretrained language models store surprising amounts of world knowledge within their parameters <ref type="bibr" target="#b32">(Petroni et al., 2019;</ref><ref type="bibr" target="#b36">Roberts et al., 2020)</ref>, external document retrieval not only augments this intrinsic knowledge but also grounds model outputs in a knowledge source, which provides interpretability.</p><p>In this section, we describe our proposed LFQA system, which conditions answer generation on Wikipedia articles identified by a pretrained retriever. We use a dense retriever trained by scaling up a distantly supervised algorithm from Jernite (2020). Since retrieved articles can be quite long and often exceed the maximum sequence length of pretrained models like BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, we use a sparse-attention variant of the Transformer to allow modeling over longer sequences. Our system sets a new state of the art on ELI5, although we question the significance of this result in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retriever</head><p>We begin by specifying our dense retriever ("contrastive REALM" or C-REALM), which returns documents related to an input question. Consider a corpus of long-form questions and answers, represented by (q i , a i ) N i=1 . Our retriever uses q i as a query to retrieve K documents (r i,j ) K j=1 from a knowledge corpus (Wikipedia), which is enabled by an encoder network that projects both questions and candidate documents to a 128-d shared embedding space. Like REALM <ref type="bibr" target="#b13">(Guu et al., 2020)</ref>, our encoder is a BERT-base Transformer <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> with a final projection layer.</p><p>Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) that uses gold answers for distant supervision. The key idea is to push the encoded vector for a question close to a vector representation of its groundtruth answer(s), but away from all other answer vectors in the mini-batch (negative examples). Intuitively, this method works because both ELI5 answers and external documents are of paragraph length (documents are paragraph-length chunks from Wikipedia). Concretely, we optimize the loss,</p><formula xml:id="formula_0">loss = − (q i ,a i )∈B log exp q i • a i a j ∈B exp q i • a j</formula><p>where B is the mini-batch and q i , a i are the encoded vector representations for (q i , a i ). This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> and dense retriever training <ref type="bibr" target="#b18">(Karpukhin et al., 2020)</ref>. Scaling up from Jernite (2020), we use large mini-batches of size 12K, and initialize our retriever with the REALM CCNews checkpoint <ref type="bibr" target="#b13">(Guu et al., 2020)</ref>. 2 These design decisions greatly improve retriever quality, as we observe in an ablation study (see Appendix A.3). During inference, a MIPS search is conducted with the ScaNN library <ref type="bibr" target="#b11">(Guo et al., 2020)</ref> to efficiently find the top K documents; we use K = 7 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generator</head><p>We next describe our generator model, which conditions its generated answers on retrieved documents returned by C-REALM. We use the Routing Transformer (RT) from <ref type="bibr" target="#b37">Roy et al. (2020)</ref>, which is the current state of the art in long-form language modeling. The RT is a sparse attention model that employs local attention as well as mini-batch k-means clustering to better model long-range dependencies 2 Jernite (2020) use batch size 512, initialize with BERT.</p><p>in sequences (attention maps in Appendix A.2). Long-form language models such as RT are wellsuited to ELI5 as the task requires conditioning answer generation not only on a short question but also many lengthy retrieved documents.</p><p>We pretrain our RT model on PG-19, a longform language modeling benchmark (Rae et al., 2020) created from approximately 28,000 Project Gutenberg books published before 1919. PG-19 has 1.9B tokens and an average context size of 69K words. While this data is out-of-domain for ELI5, we choose it to encourage long &amp; coherent generation. Our RT is a 22-layer model with 1032 hidden units (486M parameters), maximum sequence length of 8192 tokens, and a vocabulary of 98K subwords. <ref type="foot" target="#foot_0">3</ref> We fine-tune our model in a decoderonly fashion <ref type="bibr">(Liu et al., 2018;</ref><ref type="bibr" target="#b46">Wolf et al., 2018)</ref> by concatenating the top K retrieved documents to the question [r i,K , r i,K−1 ... r i,1 , q i , a i ] and training the model to predict tokens of the answer a i . We do not backpropagate gradients through the retriever. <ref type="foot" target="#foot_1">4</ref> Retrievals slightly improve perplexity (18.1 vs 17.8) as seen in <ref type="bibr" target="#b44">Wang and McAllester (2020)</ref>, but do not improve generations ( §3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Main Experiments</head><p>Dataset &amp; Evaluation: We evaluate our model on the KILT validation &amp; test subsets of ELI5 <ref type="bibr" target="#b31">(Petroni et al., 2020)</ref>, since the original ELI5 dataset does not have human annotations to measure retriever performance (split sizes in Appendix A.1). Answer quality is measured by the maximum overlap of generations with a set of gold answers in terms of unigram F1 score and ROUGE-L <ref type="bibr" target="#b28">(Lin, 2004)</ref>. <ref type="bibr" target="#b31">Petroni et al. (2020)</ref> collected human annotations of Wikipedia articles which support ELI5 gold answers, which enables measuring retrieval quality by computing R-precision (if the top-1 retrieval matches the annotation) and Recall@5 using the top-5 retrievals. Finally, the KILT benchmark combines R-prec. and ROUGE-L to measure the overall performance of the system by "KILT ROUGE-L". This metric is similar to ROUGE-L, but assigns a score of 0 whenever the top-1 retrieval does not match the gold annotation. Table <ref type="table">1</ref>: Results on the KILT test set for ELI5 for (1) retrieval performance, using R-precision and Recall@5 (RPrec, R@5), and (2) generation quality, using ROUGE-L (R-L). These scores are combined to produce the final metric KILT R-L (KRL). We outperform prior work on both generation &amp; combined scores.</p><p>Baselines: We compare our model with the other entries on the ELI5 KILT leaderboard which are either generation-only, like T5-base <ref type="bibr" target="#b34">(Raffel et al., 2020)</ref> and BART <ref type="bibr" target="#b25">(Lewis et al., 2020b)</ref>, or variants of BART using retrieval such as RAG <ref type="bibr" target="#b26">(Lewis et al., 2020c)</ref> and BART + DPR <ref type="bibr" target="#b31">(Petroni et al., 2020)</ref>. These systems are based on massive pretrained language models, with similar number of parameters as our model (details in Appendix A.4).</p><p>Results: Table <ref type="table">1</ref> contains our results on the test set of the ELI5 (also on the public KILT leaderboard). We present four variants of our system, using a different retriever during inference (REALM or C-REALM), and different nucleus sampling p values <ref type="bibr" target="#b14">(Holtzman et al., 2020)</ref>. All variants outperform prior work in generation quality, with lowerentropy models (p = 0.6) performing best.<ref type="foot" target="#foot_2">5</ref> C-REALM performs competitively to RAG and DPR despite being only distantly supervised, and outperforms REALM. Our proposed RT+C-REALM system achieves a new state-of-the-art on combined performance (KILT R-L). Generations from our model are provided in Figure <ref type="figure">2</ref> and Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis</head><p>In this section, we conduct a thorough analysis of our model's usage of retrievals (Section 3.1), the impact of overlap in ELI5's train / validation / test Q: Why are almost all boats white?</p><p>A: Boats are generally painted white for obvious reasons, as they are inexpensive and easy to clean. Ships are also often painted to provide a visible visual contrast to the sea. Boats also often have high visibility ports to allow for easy international trade. In addition, many boats have an airbreathing function that makes them easier to spot and stay afloat, which is a good thing if you're a pirate or trying to get into some country's waters. Lastly, many ships are built in the last few decades, so they are not as exotic looking as they were thirty years ago. And lastly, many ships are made out of materials that are easy to work with, and so many of them are painted white as a practical matter. There are many, many more reasons for why ships are white, but those are the ones I know of. I'm sure there are many other reasons, but those are the ones I know of. Edit: I forgot to mention that the other answers about visibility and easy to see ports are also true. Ships are painted to appear to be larger, and more expensive. That makes them appear more impressive, and so they are more likely to be bought by people who want to show off their wealth and status.</p><p>Figure <ref type="figure">2</ref>: Example generation from our LFQA system with p = 0.9. Generations are long &amp; coherent, but suffer from repetition towards the end. (more in Appendix A.5 and attached data supplementary material).</p><p>folds (Section 3.2), issues with ROUGE-L and performance bounds (Section 3.3), and the difficulty in human evaluation for this task (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Are generations grounded in retrieval?</head><p>While our retrieval-augmented system achieves state-of-the-art performance, we find little evidence that it is actually using the retrieved documents. To measure this, we run an ablation study where at inference time we replace retrieved paragraphs with randomly sampled paragraphs from Wikipedia.</p><p>We compare this Random baseline with our original system (Predicted) in terms of generation quality as well as the n-gram overlap between the generation and the retrieved paragraphs.</p><p>Generations are similar irrespective of type of retrievals: We present our results in For p = 0.9 pred. random 31% ( <ref type="formula">52</ref>) 37% ( <ref type="formula">63</ref>) 32% (54) pred. gold ans. 17% (49) 72% (203) 11% ( <ref type="formula">31</ref>) Table <ref type="table">3</ref>: Human evaluation results with exact number of ratings shown in (•). Annotators are shown a question along with two answers (A, B) in random order and ask them to choose one (details in Appendix A.7). For both model variants (p = 0.6, 0.9), we see (1) little difference between generations conditioned on predicted (pred.) or random (rand.) retrievals; (2) strong preference for gold answers over generations. due to stopwords (e.g., prepositions, punctuation) and entities which are copied from the question. To tackle this issue, in Table <ref type="table" target="#tab_3">4</ref> we measure the fractions of lemmatized nouns, proper nouns and numbers in the generated answer which are present in the predicted retrievals but not in the question. We notice similar trends as before, with only small differences between the two systems. Finally, there is almost no correlation (Spearman ρ = 0.09) between the Predicted model's generation quality and the amount of unigram overlap between its outputs and the retrieved documents (scatter plots in Appendix A.9), strengthening our hypothesis that generations are not grounded in retrievals. 7   Human evaluation validates our findings: As ROUGE-L and n-gram overlap have major limitations for LFQA (Section 3.3), we perform additional human A/B testing on the output of Random and Predicted. Specifically, we ask human 7 All these trends persist even on questions for which our retriever predicts the ground-truth document (Appendix A.9) vs qn. vs predicted retr. vs random retr.</p><p>but  volunteers<ref type="foot" target="#foot_4">8</ref> to choose between answers generated by the two systems (presented in random order).</p><p>As seen in Table <ref type="table">3</ref>, humans struggle to choose which of the two answers is more relevant to the question. For both model variants (p = 0.6, 0.9), there is a less than 7% preference for a particular answer type, with humans preferring answers (by 6%) from the Random model for p = 0.9! Other systems also have this issue, possibly due to source-reference divergence and trainvalidation overlap: We note that this issue is not unique to our system -other systems on the KILT leaderboard like BART + DPR and RAG actually perform worse than their no-retrieval counterpart (BART) in generation quality, as shown in Table <ref type="table">1</ref>. Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020).<ref type="foot" target="#foot_5">9</ref> A possible explanation for this issue is high source-reference divergence, a common problem in table-to-text generation <ref type="bibr" target="#b45">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b40">Tian et al., 2019)</ref>.</p><p>In Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_3">4</ref>, we measure the n-gram overlap of top-ranked gold validation answers (Gold Ans) with predicted retrievals. This overlap is low and similar to that of our generations, which we suspect encourages our model to ignore retrievals. A second explanation is the large amount of train-validation overlap (Section 3.2), which eliminates the need for retrieval. Takeaway (better evaluation of grounding): For evaluating LFQA, it is important to run control experiments with random retrievals &amp; measure grounding of generations in retrieval. While the KILT benchmark does attempt to measure the combined retrieval + generation performance via KILT RL, it does not check whether the generations actually used the retrievals. In other words, one can submit independent retrieval &amp; generation systems, but still perform well on the combined score. This may not be an issue for short-form QA tasks like Natural Questions, since the gold answer is often exactly contained as a span in the gold retrieval. Also, as retrieval might be less important for large language models with parametric knowledge <ref type="bibr" target="#b36">(Roberts et al., 2020)</ref>, the KILT-RL strategy of simply aggregating top-1 retrieval score with ROUGE-L unfairly penalizes systems not relying on retrieval.<ref type="foot" target="#foot_8">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training / Validation Overlap</head><p>Our experiments in Section 3.1 show that model performance is mostly unchanged by conditioning generation on randomly sampled retrievals instead of predictions from C-REALM. Despite not using retrievals, we observe qualitatively that our model displays a large amount of parametric knowledge ("Faraday Cage" in Figure <ref type="figure" target="#fig_1">1c</ref>), which is surprising since it was pretrained on novels from Project Gutenberg (not Wikipedia). In this section, we discover that a major reason for ignoring retrievals is the large amount of train / validation overlap in ELI5. While <ref type="bibr" target="#b9">Fan et al. (2019)</ref> attempted to fix this issue through TF-IDF overlap, this method is insufficient to identify all question paraphrases, as we find significant overlap between the training set and the KILT validation set of ELI5.<ref type="foot" target="#foot_9">13</ref> ELI5 is not the only dataset with substantial train / test overlap: <ref type="bibr" target="#b27">Lewis et al. (2020d)</ref> identify similar issues with short-form QA datasets like Natural Questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding similar questions &amp; measuring overlap:</head><p>We use our retriever C-REALM to retrieve similar questions from the training set, since it has learned to map questions to a feature-rich embedding space.</p><p>For each validation question, we retrieve the 7 most similar training set questions. We use both human and automatic evaluation to calculate the amount of overlap. For human evaluation, we show annotators on Amazon Mechanical Turk<ref type="foot" target="#foot_10">14</ref> a validation set question and a retrieved training set question, and ask them to annotate the pair as 0: No paraphrase relationship; 1: on similar topics, but different questions; 2: approximately the same question (an adaptation of the paraphrase evaluation of <ref type="bibr" target="#b20">Kok and Brockett, 2010)</ref>. We take 300 validation set questions and ask three crowd-workers to rate them against retrieved training questions on this scale, and consider the label with majority rating. To improve quality, we manually verify their annotations.</p><p>Table <ref type="table" target="#tab_5">5</ref> shows that 81% of validation set questions have at least one paraphrase in the training set, while all annotated questions have at least one topically similar question in the training set, which indicates substantial training / validation overlap. The experiment had "fair agreement" with a Fleiss κ of 0.29 <ref type="bibr" target="#b10">(Fleiss, 1971;</ref><ref type="bibr" target="#b22">Landis and Koch, 1977)</ref>.</p><p>As manually annotating question overlap can be expensive and time-consuming, we also experiment with automatic overlap detection qns with at least one train set paraphrase 81% qns with at least one train set topically similar 100% % of all pairs marked paraphrases 39.5% % of all pairs marked topically similar 47.8% % of all pairs marked as non-paraphrases 12.7% methods. In particular, we use a RoBERTa-large binary classifier <ref type="bibr" target="#b30">(Liu et al., 2019)</ref> fine-tuned on the Quora Question Paraphrase (QQP) dataset <ref type="bibr" target="#b15">(Iyer et al., 2017)</ref> from the GLUE benchmark <ref type="bibr" target="#b43">(Wang et al., 2019)</ref>. For 43.6% of the ELI5 validation set, this classifier marked at least one retrieved question as a paraphrase (46% for the 300 questions we annotated). Qualitatively, we notice that this classifier often mis-classifies retrieved questions that are valid paraphrases but exhibit significant lexical or syntactic divergence. This observation, along with the smaller fraction of valid paraphrases in the QQP training set (37%), partially explains the gap between automatic &amp; human evaluations.</p><p>Using retrieved QA for generation: Since ELI5 contains significant amount of overlap between the training and validation sets, a system can simply copy the answers of retrieved training set questions instead of actually doing generation. Table <ref type="table" target="#tab_6">6</ref> shows that by using the longest answer within the top-K retrieved questions, we outperform two prior systems (RAG, BART + DPR) that use retrieval-augmented generation. As an upper bound, we also consider a system which uses the best possible answer to retrieved training set questions in terms of ROUGE-L (best top-K train answer). This system gets 28.5 ROUGE-L, outperforming all others.</p><p>ELI5 performance on overlapping QA: Finally, we measure the performance difference between validation questions that overlap with the training set vs. those that do not. In Appendix A.6 we notice large differences of 6.6 RPrec, 8.1 R@5 in retrieval performance favoring the overlap subset, but only a small generation score gain of 0.8 F1, 0.4 R-L (which may be misleading, Section 3.3).</p><p>Takeaway (careful held-out curation): Based on our findings, we suggest that more careful dataset curation for LFQA tasks is needed to prevent du-plicates. We acknowledge the efforts from <ref type="bibr" target="#b9">Fan et al. (2019)</ref> to fix this issue, and suggest alternative methods to control overlap and focus on evaluating generalization in held-out sets: (1) retrieving paraphrases and running human validation to eliminate them; or (2) holding out entire genres or domains to reduce the possibility of overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ROUGE-L Bounds on ELI5 Performance</head><p>We have seen that simply copying the answer of a close question paraphrase from the training set achieves 28.5 ROUGE-L with an optimal selection among retrieved questions and outperforming all computational models. But how "good" is this absolute number? What are some suitable upper &amp; lower bounds to ROUGE-L scores on ELI5? Is ROUGE-L an informative metric for LFQA?</p><p>Lower bounds are trivial baselines used to test the vulnerability of datasets or metrics to simple heuristic strategies that do not actually perform the task.</p><p>Recent examples include hypothesis-only baselines for natural language inference <ref type="bibr" target="#b12">(Gururangan et al., 2018)</ref> and passage-only baselines for reading comprehension <ref type="bibr" target="#b19">(Kaushik and Lipton, 2018)</ref>. We evaluate two ROUGE-L lower bounds on ELI5:</p><p>(1) copy the question 5 times and concatenate, as longer outputs boost ROUGE-L (Appendix A.8);</p><p>(2) retrieve a random training set answer.</p><p>Our first baseline contains entities often present in the gold answer, but without actually answering the question. Our second baseline follows the "style" of an answer but is completely off-topic.</p><p>As an upper bound, we estimate the ROUGE-L of gold answers themselves. On an average, there are 12 gold answers per question, so we measure the ROUGE-L of the longest gold answer with respect to the other gold answers. We also measure the maximum pairwise ROUGE-L between two gold answers for the same question. <ref type="foot" target="#foot_11">15</ref> We only calculate upper bounds for the validation set, since the gold answers of the KILT test set are hidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lower bounds beat prior work, upper bounds have low ROUGE-L:</head><p>We compare our bounds with actual retrieval augmented generation systems in Table <ref type="table" target="#tab_6">6</ref>. Both our lower bounds (random training answer, copy input) are quite competitive, outperforming RAG <ref type="bibr" target="#b26">(Lewis et al., 2020c)</ref> and performing close to BART + DPR <ref type="bibr" target="#b31">(Petroni et al., 2020)</ref> without actually answering the question! This shows that ROUGE-L is fairly sensitive to simply copying entities from the question as well as stylistic properties of ELI5. On the other hand, upper bounds (longest gold answer) perform worse than our system (21.2 vs 24.4).</p><p>Suspecting that this result is misleading, we run another human A/B test by showing volunteers a question and asking them to choose between answers generated by our system and the longest gold answer, shuffled at random. 16 As seen in Table <ref type="table">3</ref>, the majority of humans prefer the gold reference answers vs generations (68% vs 14% for p = 0.6). In interviews with human annotators after completing the task, they reported that both answers were often fluent and stylistically similar, but one eventually veered off-topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Takeaway (better automatic metrics needed):</head><p>Our experiments demonstrate that computing the ROUGE-L of generations against gold answers is not a meaningful way to evaluate LFQA systems, since it is not selective enough to differentiate between valid/invalid answers. There is a very small margin of improvement between trivial lower bounds and strong upper bounds, with the absolute scores of upper bounds being quite low. We suspect this is due to the long length of answers and fairly unconstrained and large output space. 16 Human A/B testing details in Appendix A.7.</p><p>The ELI5 dataset has several open-ended questions with many plausible answers (like What causes traffic?), often involving analogies. A possible fix is a sentence-level evaluation and then aggregating scores across generated sentences, but appropriate penalties are needed for lack of diversity <ref type="bibr" target="#b50">(Zhu et al., 2018</ref>) and short lengths. Other possible fixes include learning task-specific metrics to measure semantic overlap <ref type="bibr" target="#b38">(Sellam et al., 2020)</ref> or metrics to check factual correctness <ref type="bibr" target="#b48">(Zhang et al., 2020)</ref> and faithfulness to input <ref type="bibr">(Wang et al., 2020;</ref><ref type="bibr" target="#b8">Durmus et al., 2020;</ref><ref type="bibr" target="#b49">Zhou et al., 2020)</ref>. Ultimately, all automatic metrics have their limitations, and human evaluation is necessary <ref type="bibr" target="#b3">(Celikyilmaz et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Difficulty of Human Evaluation</head><p>To better understand the inherent difficulty of evaluation in ELI5, we interviewed human annotators (of Table <ref type="table">3</ref>) and found two challenges:</p><p>(1) Unfamiliarity with question topics: Annotators were often unfamiliar with the technical topics discussed in some questions, which makes it hard to assess answer correctness. The ELI5 dataset has questions in a wide variety of topics (History, Politics, Biology etc.), while most annotators were CS graduate students. While we did allow annotators to use Wikipedia, they mentioned domain-experts will be better judges of answer quality.</p><p>(2) Length of Answers: Annotators mentioned the paragraph-long length of answers made the task quite challenging. Annotators reported taking an average of 2 minutes per answer pair, many of which required careful thought &amp; concentration. This was especially difficult when only part of the answer was correct and the rest had contradictions or repetitions, a common theme in our generations.</p><p>Takeaway: Human evaluation is challenging but necessary for evaluating LFQA. Crowd-workers are unlikely to spend time reading &amp; analyzing long text <ref type="bibr">(Akoury et al., 2020)</ref>. Hence, it is imperative to design simpler evaluations. One effort in this direction is <ref type="bibr" target="#b7">Dugan et al. (2020)</ref>, who reveal one generated sentence at a time and estimate system quality based on the number of sentences which fooled humans. Another promising direction is extrinsic evaluation <ref type="bibr" target="#b3">(Celikyilmaz et al., 2020)</ref> where humans actually interact with systems in real-world scenarios such as the Alexa Prize <ref type="bibr" target="#b35">(Ram et al., 2018)</ref> or STORIUM <ref type="bibr">(Akoury et al., 2020)</ref>.</p><p>We present a "retrieval augmented" generation system that achieves state of the art performance on the ELI5 long-form question answering dataset. However, an in-depth analysis reveals several issues not only with our model, but also with the ELI5 dataset &amp; evaluation metrics. We hope that the community works towards solving these issues so that we can climb the right hills and make meaningful progress.</p><p>A Appendices for "Hurdles to Progress in Long-form Question Answering" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training &amp; Model Details</head><p>All our models are developed and trained using TensorFlow 1.15 <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and Tensor2Tensor <ref type="bibr" target="#b41">(Vaswani et al., 2018)</ref>. Our implementations are based on the open-source codebases of REALM<ref type="foot" target="#foot_13">18</ref> and the Routing Transformer. <ref type="foot" target="#foot_14">19</ref>Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search). Since our retriever is frozen, we do not use the document index refresher available in their codebase.</p><p>Retriever: Our retriever is trained on 64 Google Cloud TPUs for a total of 4k steps and a batch size of 12288. We do early stopping on the validation data (with a smaller batch size of 512 due to smaller P100 GPU memory).</p><p>Our model converges quite fast, reaching its best performance in 1.5k steps (in 43 minutes) and needing 103 minutes for the full set of 4k steps.</p><p>Generator: Our generator is trained on 64 Google Cloud TPUs, for a total of 100k steps on the ELI5 training set. We use the pg19_local_cluster8k configuration available in the Routing Transformer implementation.</p><p>Besides the default hyperparameters, setting 15% input, attention and ReLU dropout was critical to prevent overfitting on the training set. We use a learning rate of 5e-5. Our retrievals, questions and answers are truncated / padded to 288 subword tokens (using the PG19 subword tokenizer). We use a minibatch size of 128 QA pairs, which corresponds to 332k tokens per mini-batch (of which, the loss is computed over the last 288 answer tokens, or 37k total tokens). We do not compute loss over padded tokens, and use special symbols to separate different parts of the input context. We reverse the retrieved paragraphs in context since the model uses local attention layers, and we wanted higher ranked retrievals to appear closer to the answer tokens. Our models take about 30 hours to finish 100k steps (0.92 steps / second).</p><p>Attention Maps: We show the 2D plots of our generator's attention maps in Figure <ref type="figure" target="#fig_3">3</ref>.  Hyperparameter Choices: We experimented with several different pretraining strategies (using Wikipedia), smaller model variants and hyperparameter choices manually in preliminary experiments. All these experiments performed quite poorly on ELI5, producing very short and sometimes incoherent responses. Finally, switching to a Routing Transformer model which was pretrained on a longform language modeling dataset (PG-19) significantly improved generation quality. Hyperparameters for this pretrained model (like hidden size / number of layers) were manually chosen with model capacity in mind. For our final experiments with this pretrained model we did not perform any hyperparameter search during training, primarily due to the expensive setup required to train the system. During inference, we tuned the nucleus sampling value from 0.0 to 1.0 in increments of 0.1, choosing the value with the best validation set performance. Our hyperparameter choices for contrastive learning on the retriever have been discussed in an ablation study in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Ablation Study of C-REALM</head><p>One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in <ref type="bibr" target="#b17">Jernite (2020)</ref>. This method uses in-batch negative sampling, making minibatch size a critical hyperparameter for better constrastive learning. We perform controlled experiments initializing our retrievers with REALM-CCNews <ref type="bibr" target="#b13">(Guu et al., 2020)</ref> and varying batch size and keeping all other hyperparameters consistent.</p><p>In Table <ref type="table">7</ref>, we notice a steady increase in performance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) from 512 to 1024. Finally, in preliminary experiments we saw no benefit of more intelligent negative sampling schemes.</p><p>Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 <ref type="bibr" target="#b17">(Jernite, 2020)</ref> 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2</p><p>Table <ref type="table">7</ref>: The effect of minibatch size on the validation performance of C-REALM. As a baseline, we also add the retrieval performance of the REALM pretrained model which is used as an initialization.</p><p>Next, we investigate the effect of initialization on the training of C-REALM. Unlike Jernite (2020) who initialize their model with BERT, before training we initialize our retriever with a pretrained self-supervised retriever. As a baseline, we initialize our model with ICT, a weaker self-supervised retriever introduced in <ref type="bibr" target="#b23">Lee et al. (2019)</ref>. Both models are trained with minibatch sizes of 12228. In Table <ref type="table">8</ref>, we notice a large improvement in performance when using a better initialization, confirming our design decisions.</p><p>Initialization R-Prec. R@5 REALM (pretrained) 6.6 14.9</p><p>ICT <ref type="bibr" target="#b23">(Lee et al., 2019)</ref> 9.3 16.5 REALM <ref type="bibr" target="#b13">(Guu et al., 2020)</ref> 13.3 21.2</p><p>Table <ref type="table">8</ref>: The effect of initialization on C-REALM. As a baseline, we also add the retrieval performance of the REALM-CCNews pretrained model without any finetuning on ELI5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Number of trainable parameters</head><p>In Table <ref type="table" target="#tab_8">9</ref> we present the number of trainable parameters in our model compared to baselines on the leaderboard. Our generator is slightly larger than the models used in prior work, but we utilize a smaller retriever due to the shared query and candidate encoders in REALM. Overall, our system has a similar total number of parameters as baseline models like RAG and BART + DPR.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Generations from our System</head><p>More generations have been provided (along with retrievals, highlighted to show n-gram overlap) in the supplementary material (data) as HTML files.</p><p>We also present a few samples in Table <ref type="table" target="#tab_6">16</ref>.</p><p>A.6 Performance on overlapping QA</p><p>In Table <ref type="table">10</ref> we compare ELI5 performance on the overlapping and non-overlapping subsets of the validation data. Since we only have human annotations for 300 questions (making the non-overlap subset have only 53 samples), we present this analysis using the QQP classifier's outputs as well and refer to those numbers in the main text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Human Evaluation Setup</head><p>We conducted several A/B tests between variants of our model using human annotators. We asked a total of 20 participants for help who voluntarily agreed to help with the annotation process. Most participants were English-speaking graduate students in computer science. In every test, participants were shown a question along with two answers (generated by different systems) presented in a random order. They were then asked to choose which generation (1) answered the question better / which answer was more relevant to the question;</p><p>(2) was more coherent / had less repetition;</p><p>(3) was more factually correct. Since some annotators had a limited time, we asked them to prioritize question (1) over ( <ref type="formula">2</ref>) / (3). Annotators were allowed to select "Tie" if they could not choose between the systems. We also permitted them to use search engines, but suggested restricting search to Wikipedia. We present all our results in Table <ref type="table" target="#tab_5">15</ref>. We also interviewed some participants after the annotation process and discuss our findings in Section 3.4. Note that while these A/B tests help us understand which system is relatively better, they do not provide an absolute measure of performance (Celikyilmaz et al., 2020) -annotators reported that there were cases where both answers were very good and other cases where both were very poor. This is a limitation of A/B testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Effect of length on ROUGE-L</head><p>In this section we measure the effect of outputs lengths on ROUGE-L scores. To conduct this experiment, we truncate generations by our system to a fixed fraction of tokens across all instances. As we see in documents. Overall, trends are consistent with our observations in Section 3.1.</p><p>Scatter plots between generation quality and unigram overlap with retrievals: We present this scatter plot in Figure <ref type="figure" target="#fig_5">4</ref>. There is virtually no correlation between the two quantities, with Spearman ρ = 0.09.</p><p>Instances with correct predicted retrieval: In Table <ref type="table" target="#tab_11">12</ref>, we present results similar to Section 3.1 considering only those instances where at least one retrieved document matched the gold annotation (roughly 23% instances). We also present a scatter plot on the same set of instances in Figure <ref type="figure" target="#fig_7">5</ref> and note a low correlation of ρ = 0.13.</p><p>Experiments with p = 0.9: We conduct additional experiments studying our model variant with higher nucleus sampling values. As we saw in Section 2.3, these generations tend to be more fluent and coherent, but less relevant to the question. In Table <ref type="table">13</ref> and Table <ref type="table" target="#tab_13">14</ref> we find consistent trends as Section 3.1, with very little difference between models conditioned on retrievals from C-REALM and random retrievals.   Table <ref type="table">13</ref>: Comparison of generations (with p = 0.9) conditioned on retrievals from C-REALM (Predicted) and randomly chosen retrievals (Random). Notice very small differences in: (1) ROUGE-L vs gold answers (R-L); (2) n-gram overlap (n-g) with retrievals predicted by C-REALM (vs predicted retr.). Gold answers also have a similar overlap with predicted retrievals. To control for overlap due to stopwords, we also add n-gram overlaps with the randomly sampled retrievals.     <ref type="table">13</ref> measuring the unigram overlap of nouns/numbers in the generations with the input question (vs qn.), retrievals predicted by C-REALM (vs predicted retr.) and randomly sampled retrievals (vs random retr.). Similar to Table 13, notice very little difference with and without retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Figure 1: A summary of the major hurdles (a-d) to progress in long-form question answering with ELI5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A. 1</head><label>1</label><figDesc>Dataset StatisticsWe downloaded the ELI5 dataset<ref type="bibr" target="#b9">(Fan et al., 2019)</ref> from the KILT Github repository (https:// github.com/facebookresearch/KILT). The dataset has 272,634 training examples, 1,507 validation examples and 600 test examples (answers hidden). The dataset uses English.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Figures (from Roy et al., 2020) showing 2-D attention schemes for the sparse attention mechanism used in Routing Transformer. Lower layers pool in local information via sliding window local attention (Subfigure3a) while upper layers gather global information for every token via clustering (Sub-figure3b).</figDesc><graphic url="image-1.png" coords="12,317.03,218.76,87.08,91.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Scatter plot for generations from the p = 0.6 model between generative quality (ROUGE-L vs reference on X-axis) and grounding with retrieval (unigram overlap with retrieved documents on Y-axis). The plot shows no correlation between the two quantities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Scatter plot for generations from the p = 0.6 model between generative quality (ROUGE-L vs reference on X-axis) and grounding with retrieval (unigram overlap with retrieved documents on Y-axis). Unlike Figure4, this plot only considers those cases where C-REALM predicted the correct retrieval. The plot shows very little correlation between the two quantities (Spearman ρ = 0.13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Despite not being conditioned on any meaningful retrievals, the Random retrieval model has similar ROUGE-L scores as our Predicted system. Moreover, generations from the Random and Predicted models have similar amounts of 1-gram and 2gram overlap with the paragraphs retrieved by C-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">vs predicted retr. vs random retr.</cell></row><row><cell></cell><cell cols="2">R-L</cell><cell>1-g</cell><cell>2-g</cell><cell>1-g</cell><cell>2-g</cell></row><row><cell cols="3">Predicted 24.42</cell><cell>52.3</cell><cell>9.0</cell><cell>38.8</cell><cell>3.9</cell></row><row><cell>Random</cell><cell cols="2">24.20</cell><cell>51.2</cell><cell>8.5</cell><cell>38.5</cell><cell>3.9</cell></row><row><cell cols="2">Gold Ans</cell><cell>-</cell><cell>54.1</cell><cell>9.1</cell><cell>40.2</cell><cell>3.8</cell></row><row><cell cols="7">Table 2: Comparison of generations (with p = 0.6)</cell></row><row><cell cols="7">conditioned on predicted retrievals (Predicted) and ran-</cell></row><row><cell cols="7">domly chosen retrievals (Random). Notice small dif-</cell></row><row><cell cols="7">ferences in: (1) ROUGE-L vs gold answers (R-L); (2)</cell></row><row><cell cols="7">n-gram overlap (n-g) with predicted retrievals (vs pre-</cell></row><row><cell cols="7">dicted retr.). Gold answers also have a similar overlap</cell></row><row><cell cols="7">with predicted retrievals. To control for stopwords, we</cell></row><row><cell cols="5">show overlaps with the random retrievals.</cell><cell></cell></row><row><cell>A</cell><cell>B</cell><cell></cell><cell>Prefer A</cell><cell>Prefer B</cell><cell></cell><cell>Tie</cell></row><row><cell cols="2">For p = 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">pred. random</cell><cell></cell><cell cols="4">40% (78) 33% ( 64) 27% (51)</cell></row><row><cell cols="7">pred. gold ans. 14% (29) 68% (138) 18% (36)</cell></row></table><note>REALM, despite the fact that the Random model does not actually see the retrieved paragraphs.6  The n-gram overlaps are possibly overestimates</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>not in qn. but not in qn.</figDesc><table><row><cell cols="3">lemmatized nouns, proper nouns, numbers only</cell><cell></cell></row><row><cell cols="2">Predicted 13.4%</cell><cell>34.4%</cell><cell>11.9%</cell></row><row><cell>Random</cell><cell>13.7%</cell><cell>31.7%</cell><cell>12.1%</cell></row><row><cell>Gold Ans</cell><cell>8.3%</cell><cell>28.8%</cell><cell>15.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>A fine-grained version of Table2measuring the unigram overlap of nouns/numbers in the generations with the input question (vs qn.), retrievals predicted by C-REALM (vs predicted retr.) and randomly sampled retrievals (vs random retr.). Similar to Table2, notice very little difference with and without retrieval.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: A human evaluation measuring the amount of</cell></row><row><cell>overlap between validation set questions (qns) and re-</cell></row><row><cell>trieved questions from the training set.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Upper (↑) and lower (↓) bounds to performance on ELI5. Lower bounds have been submitted to the public KILT leaderboard, as "Metrics Test".</figDesc><table><row><cell></cell><cell>Validation</cell><cell>Test</cell><cell></cell></row><row><cell>Scheme</cell><cell>F1 R-L</cell><cell cols="2">F1 R-L</cell></row><row><cell>random train answer (↓)</cell><cell cols="3">17.8 16.2 17.1 15.5</cell></row><row><cell>copy input (↓)</cell><cell cols="3">16.6 20.0 14.8 16.9</cell></row><row><cell>RAG (2020c)</cell><cell cols="3">17.2 16.1 14.5 14.1</cell></row><row><cell>BART + DPR (2020)</cell><cell cols="3">18.8 18.5 17.9 17.4</cell></row><row><cell>longest top-1 train answer</cell><cell cols="3">25.2 20.7 21.6 18.7</cell></row><row><cell>longest top-7 train answer</cell><cell cols="3">26.9 21.1 22.0 18.5</cell></row><row><cell>RT + C-REALM (ours)</cell><cell cols="3">25.6 24.4 22.9 23.2</cell></row><row><cell cols="2">best top-1 train answer (↑) 25.9 22.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">best top-7 train answer (↑) 31.5 28.5</cell><cell>-</cell><cell>-</cell></row><row><cell>longest gold answer (↑)</cell><cell>26.7 21.2</cell><cell>-</cell><cell>-</cell></row><row><cell>best gold answer (↑)</cell><cell>29.5 26.2</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>The number of parameters used by our model and baselines. Our generator is slightly bigger than other submissions on the leaderboard, but we use a smaller retriever with a similar sized index.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc></figDesc><table><row><cell>in the Truncate column, shorter</cell></row><row><cell>generations tend have lower ROUGE-L. To disen-</cell></row><row><cell>tangle the effects of length and content, we also</cell></row><row><cell>measure the generation quality by repeating the</cell></row><row><cell>truncated generations several times until it matches</cell></row><row><cell>the original generation length. In the Repeat 1/f</cell></row><row><cell>times column, we notice a gap between our model's</cell></row><row><cell>original generation (24.4 ROUGE-L) and the equal-</cell></row><row><cell>length truncated generations with repetition. These</cell></row><row><cell>results indicate that while length helps improve</cell></row><row><cell>ROUGE-L scores, simple repetition is insufficient.</cell></row><row><cell>A.9 More experiments on measuring</cell></row><row><cell>retrieval grounding of generations</cell></row><row><cell>In this section we provide some more experiments</cell></row><row><cell>testing the grounding of generations in retrieved</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Comparison of generations conditioned on retrievals from C-REALM (Predicted) and randomly chosen retrievals (Random), for those cases where C-REALM predicted the correct retrieval. Notice very small differences in generation quality (R-L) as well as the fraction of n-grams (n-g) in the generation overlapping with retrievals predicted by C-REALM (vs predicted retr.). To control for overlap due to stopwords, we also add n-gram overlaps with the randomly sampled retrievals.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">vs predicted retr. vs random retr.</cell></row><row><cell>Retriever</cell><cell>R-L</cell><cell>1-g</cell><cell>2-g</cell><cell>1-g</cell><cell>2-g</cell></row><row><cell cols="3">p = 0.6, correct retrieval examples</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Predicted 23.74</cell><cell cols="2">54.4 10.0</cell><cell>39.7</cell><cell>4.3</cell></row><row><cell>Random</cell><cell>23.91</cell><cell>52.5</cell><cell>9.6</cell><cell>38.8</cell><cell>4.0</cell></row><row><cell cols="3">p = 0.9, correct retrieval examples</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Predicted 22.40</cell><cell>54.9</cell><cell>9.2</cell><cell>40.9</cell><cell>4.3</cell></row><row><cell>Random</cell><cell>22.22</cell><cell>54.7</cell><cell>9.2</cell><cell>41.1</cell><cell>4.2</cell></row><row><cell></cell><cell></cell><cell cols="4">vs predicted retr. vs random retr.</cell></row><row><cell></cell><cell>R-L</cell><cell>1-g</cell><cell>2-g</cell><cell>1-g</cell><cell>2-g</cell></row><row><cell cols="2">Predicted 22.62</cell><cell>53.9</cell><cell>8.7</cell><cell>40.7</cell><cell>4.1</cell></row><row><cell>Random</cell><cell>22.56</cell><cell>53.1</cell><cell>8.4</cell><cell>40.7</cell><cell>4.1</cell></row><row><cell>Gold Ans</cell><cell>-</cell><cell>54.1</cell><cell>9.1</cell><cell>40.2</cell><cell>3.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>A fine-grained version of Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">Our hyperparameters have been chosen manually with minimal tuning. See Appendix A.2 for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">We tried training the retriever jointly with RT using the attention bias scheme proposed in MARGE<ref type="bibr" target="#b24">(Lewis et al., 2020a)</ref>. This improved perplexity only in autoencoding settings where the gold answer itself is used as a retrieval query (like the setup in<ref type="bibr" target="#b24">Lewis et al., 2020a)</ref>, which is not valid in LFQA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">As in<ref type="bibr" target="#b14">Holtzman et al. (2020)</ref>, a human study reveals that higher entropy (p = 0.9) answers are slightly more coherent and sensible, but lower entropy answers (p = 0.6) are more relevant to the question (details in Appendix A.7).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">Corresponding experiments with the p = 0.9 variant of our model are presented in Appendix A.9.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4">Details of our experimental setup in Appendix A.7.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5">https://huggingface.co/qa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6">While we do not have access to generations from baselines on the KILT leaderboard, example generations from the demo of the BART model in<ref type="bibr" target="#b17">Jernite (2020)</ref> are significantly shorter (59 words avg.) than our generations (187 words avg.).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7">11 https://eval.ai/web/challenges/ challenge-page/689/leaderboard/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8">190912  Another issue of KILT-RL is ignoring non top-1 retrievals, penalizing models using multiple retrievals together in context.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9">The ELI5 demo from Jernite (2020) also retrieves the top-1 similar training set question. Qualitatively, we found many validation examples had near-identical train paraphrases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10"> 14  We pay workers 4 cents per question pair ($8-12 / hr). We only hire workers from USA, UK and Australia with a 95% or higher approval rating and at least 1000 approved HITs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11">Note that different gold answers were not written independently as Reddit users writing answers can read existing answers and may want to provide a non-overlapping perspective. Due to the high train/valid overlap, the best top-7 retrieved answer could be a better upper bound since it is from another Reddit post (and performs better than best gold answer).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_12">https://www.gstatic.com/ gumdrop/sustainability/ google-2019-environmental-report.pdf specifically designed for machine learning applications". These accelerators run on Google Cloud, which has "matched 100% of its electricity consumption with renewable energy purchases, and has committed to fully decarbonize its electricity supply by 2030" (https://cloud.google. com/sustainability). More details on training time are provided in Appendix A.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_13">https://github.com/google-research/ language/tree/master/language/realm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_14">https://github.com/google-research/ google-research/tree/master/routing_ transformer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>First and foremost, we thank all the twenty people who volunteered to help out with with the human annotation experiments. We are very grateful to Vidhisha Balachandran, Niki Parmar and Ashish Vaswani for weekly meetings discussing progress and the REALM team (Kenton Lee, Kelvin Guu, Ming-Wei Chang and Zora Tung) for help with their codebase and several useful discussions which helped us improve our experiments. We are grateful to Tu Vu for help with the QQP classifier. We thank Jules Gagnon-Marchand and Sewon Min for suggesting useful experiments on checking ROUGE-L bounds. Finally we thank Shufan Wang, Andrew Drozdov, Nader Akoury and the rest of the UMass NLP group for helpful discussions and suggestions at various stages in the project.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 State-of-the-art as of March 10, 2021 -the "Google Research &amp; UMass Amherst" team entry on https: //evalai.cloudcv.org/web/challenges/ challenge-page/689/leaderboard/1908</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Our system faces a similar set of issues as most modern text generation technology, like fabrication of facts <ref type="bibr" target="#b47">(Zellers et al., 2019)</ref>, potential for misuse <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> and reflecting biases prevalent on Reddit (the ELI5 dataset has been built using the r/ELI5 subreddit). In our work, we attempted to make text generators more factually grounded by conditioning generations on retrieved Wikipedia articles, hoping to reduce fact fabrication. Unfortunately, a thorough analysis (Section 3.1) has revealed that our system is still not grounding its generations in retrievals, and we have recommended the design of better metrics to measure factual correctness to tackle this issue.</p><p>Our final models were trained using 64 Google Cloud TPUs for a total of 32 hours. As mentioned in the Google 2019 environment report, 17 "TPUs are highly efficient chips which have been  Q: What causes the trail behind jets at high altitude?</p><p>A: The same thing that causes the clouds you see at the start of a storm to form in the first place. The air pressure changes as the jet moves through the air, causing it to cool down and condense into water droplets. These droplets are in fact a lot bigger than the water droplets in the clouds, so when the plane passes through them, they can cause the air pressure to change, causing the water droplets to condense into clouds. This is what you see when you look at a cloud formation from the ground. In addition to this, when the plane passes through the clouds, it also moves through the air, causing it to change its temperature, causing the water droplets to condense and form clouds. These clouds are often quite large, and can block out the sun. This is what you see when you look at a contrail. Finally, the water vapor that forms at high altitudes is cooled by the air above, and as it cools, it condenses into clouds. This is what you see when you look at a cumulonimbus cloud. These clouds are even larger than the water droplets in the clouds, and they block out the sun.</p><p>Q: How do shampoo and conditioner work?</p><p>A: The key ingredients in shampoo and conditioner are usually surfactants, they tend to attract oil from your skin. Shampoo will usually contain an anti-dandruff agent (dandruff is caused by fungi in your skin which cause dandruff). They also contain some moisturizing agents (moisturizers are things that stay in your skin longer and protect your scalp from cleaning), and a lot of anti-dandruff (dandruff is a side effect of high doses of both surfactants and moisturizers) Conditioner will contain moisturizing agents, like a skin moisturizer. They tend to cause oils to be separated from your hair (e.g. wash your hair with conditioner instead of shampoo). They both have a foaming agent that makes them foam up (usually Hydrogen Peroxide). They also have a foaming agent that stops dirt from attaching to your hair. Some conditioner will also suppress the action of the shampoo (to help prevent dirt from attaching to your hair A: Depends what you define as "purpose". There is no specific purpose for the British monarch -rather, they are figureheads that serve a role. The role is to have a high degree of influence over a country, though the rules of the monarchy can somewhat vary from country to country, and the ceremony of "kissing the royal corpse" does not always involve kissing the monarch. Whether that's the only reason for the custom, or if it was the only reason, I can't say, but that's the reasoning. When the U.K. was in full power, the monarch was a puppet of Parliament, and the powers of the monarch were transferred to the Prime Minister and thus to the Prime Minister's deputy, who then became the Prime Minister. Since then, Parliament has been able to vote on legislation that goes through the monarch, although they may still act as the monarch's representative in negotiating treaties, which can have very very deep consequences. The Queen's role, as a representative of Britain, doesn't necessarily involve her formal approval of any of the laws or legislation that goes through Parliament, though. All that's left is for the legislature to pass laws Q: What exactly is fire, in detail? How can light and heat come from something we can't really touch?</p><p>A: Fire is the chemical reaction of fuel <ref type="bibr">(oxygen)</ref> with (what we call) impurities (ash, soot, oil etc). These impurities are created due to a number of things. If you were to study the chemical composition of a log (or a candle) and analyse it you would discover the chemical formula (A, C, and G) which is a simplified version of what actually happens. There is always fuel, and there is always the ash/soot. Wood is made up of mostly carbon and hydrogen and these are the fuels that create the fire. Hydrogen and oxygen are reactive, and they form water (H2O) and CO2 (CO2). If we split a log into smaller pieces, we start to see the process of combustion as it occurs. The carbon is burned, and with the explosion of CO, H2O and H2CO2, we get water and carbon dioxide (mostly). If we smash a log together, we start to see what happens when we also hit another log with another log...imagine smashing two sand castles together. We see how the sand castles and brick castles form and collapse at different rates...then it starts to burn. The smoke, ash and flames are the actual combustion of fuel.</p><p>Q: What goes on in those tall tower buildings owned by major banks?</p><p>A: The actual buildings are not there for profit but for show. There are many real world historical buildings that have little to no effect other than being a symbol of an economic entity or symbol of an era or location, and nothing more. For example look at Sears, Sears didn't care what went on inside, it was all about the _appearance_ of its location, the prestige of the location, the facilities and so on. It didn't care about how long it took it to operate, it was about how much people would pay to go see it. Sears was a landmark as a cultural movement and other big companies followed suit, so if you want to see a building you've never seen before, you have to go see Sears, just like you have to see a Toyota Camry for Toyota Camry. They used to be all about building new factories, some of them if I recall, but now that they're bigger, that means that more factory jobs are coming to them. You've probably seen them in stores as stores where people buy and sell stuff, so there aren't that many places for them to come from. Instead, it's just for show, a symbol of rich people.</p><p>Table <ref type="table">16</ref>: Example generations from our LFQA system with p = 0.9.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nanyun Peng, and Mohit Iyyer. 2020. Storium: A dataset and evaluation platform for machinein-the-loop story generation</title>
		<author>
			<persName><forename type="first">Nader</forename><surname>Akoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Hood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14799</idno>
		<title level="m">Evaluation of text generation: A survey</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
				<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RoFT: A tool for evaluating human detection of machine-generated text</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kirubarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>System Demonstrations. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization</title>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
				<meeting>the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ELI5: Long form question answering</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
				<meeting>the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><surname>Joseph L Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">378</biblScope>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
				<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">REALM: Retrieval-augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
				<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornél</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01282</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Explain anything like i&apos;m five: A model for open domain long form question answering</title>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<ptr target="https://yjernite.github.io/lfqa.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How much reading does reading comprehension require? a critical investigation of popular benchmarks</title>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hitting the right paraphrases in good time</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
				<meeting>the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-training via paraphrasing</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
				<meeting>the<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Question and answer test-train overlap in open-domain question answering datasets</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02637</idno>
		<imprint>
			<date type="published" when="2020">2020d</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02252</idno>
		<title level="m">Kilt: a benchmark for knowledge intensive language tasks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anu</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raefer</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Nunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Nagar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03604</idno>
		<title level="m">Conversational ai: The science behind alexa prize</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
				<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How to compare summarizers without target length? pitfalls, solutions and re-examination of the neural summarization literature</title>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</title>
				<meeting>the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sticking to the facts: Confident decoding for faithful data-to-text generation</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08684</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>CoRR, abs/1803.07416</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Asking and answering questions to evaluate the factual consistency of summaries</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
				<meeting>the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On-the-fly information retrieval augmentation for language models</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events</title>
				<meeting>the First Joint Workshop on Narrative Understanding, Storylines, and Events</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
				<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transfertransfo: A transfer learning approach for neural network based conversational agents</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS CAI Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9054" to="9065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Optimizing the factual correctness of a summary: A study of summarizing radiology reports</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Merck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">Bao</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
				<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Detecting hallucinated content in conditional neural sequence generation</title>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02593</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Texygen: A benchmarking platform for text generation models</title>
		<author>
			<persName><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
