<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Matrix Scaling and Balancing via Box Constrained Newton&apos;s Method and Interior Point Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>micohen@mit.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aleksander</forename><forename type="middle">M</forename><surname>Ądry</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
							<email>tsipras@mit.edu</email>
							<affiliation key="aff2">
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
							<email>avladu@mit.edu</email>
							<affiliation key="aff3">
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Matrix Scaling and Balancing via Box Constrained Newton&apos;s Method and Interior Point Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C87486CE391893A07816510C507C9042</idno>
					<idno type="DOI">10.1109/FOCS.2017.88</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>matrix scaling</term>
					<term>matrix balancing</term>
					<term>Newton&apos;s method</term>
					<term>interior point methods</term>
					<term>SDD solver</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper 1 , we study matrix scaling and balancing, which are fundamental problems in scientific computing, with a long line of work on them that dates back to the 1960s. We provide algorithms for both these problems that, ignoring logarithmic factors involving the dimension of the input matrix and the size of its entries, both run in time O m log κ log 2 (1/ε) where ε is the amount of error we are willing to tolerate. Here, κ represents the ratio between the largest and the smallest entries of the optimal scalings. This implies that our algorithms run in nearly-linear time whenever κ is quasi-polynomial, which includes, in particular, the case of strictly positive matrices. We complement our results by providing a separate algorithm that uses an interior-point method and runs in time O(m 3/2 log(1/ε)).</p><p>In order to establish these results, we develop a new secondorder optimization framework that enables us to treat both problems in a unified and principled manner. This framework identifies a certain generalization of linear system solving that we can use to efficiently minimize a broad class of functions, which we call second-order robust. We then show that in the context of the specific functions capturing matrix scaling and balancing, we can leverage and generalize the work on Laplacian system solving to make the algorithms obtained via this framework very efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Matrix balancing and scaling are problems of fundamental importance in scientific computing, as well as in statistics, operations research, image reconstruction, and engineering. The literature on these problems <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> is truly extensive and dates back to 1960s. They both are key primitives in most mainstream numerical software packages (MATLAB, R, LAPACK, EISPACK) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Also, both these problems can be seen as task in which we are aiming to find diagonal scalings of a given matrix so that the rescaled matrix gains some favorable structure.</p><p>More specifically, in the matrix scaling problem, we are given a nonnegative matrix A, and our goal is to find diagonal matrices X, Y such that the matrix XAY has prescribed row and column sums. The most common instance of this problem is the one where we want to scale the matrix so 1 The full version of this paper is available as <ref type="bibr" target="#b0">[1]</ref>.</p><p>to make it doubly stochastic -in other words, we want to make all row and column sums be equal to one. This procedure has been repeatedly used since as early as 1937 in a number of diverse areas, such as telecommunication <ref type="bibr" target="#b20">[21]</ref>, engineering <ref type="bibr" target="#b12">[13]</ref>, statistics <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b9">[10]</ref>, machine learning <ref type="bibr" target="#b22">[23]</ref>, and even computational complexity <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. A standard application for scaling is preconditioning linear system solving. Given a linear system Ax = b, one can produce a solution by computing Y(XAY) -1 Xb, since applying the inverse of XAY is more numerically stable procedure than directly applying the inverse of A <ref type="bibr" target="#b10">[11]</ref>. Another example application, which commonly occurs in statistics, is iterative proportional fitting. This primitive is often used for standardizing cross-tabulations and has been studied since 1912 <ref type="bibr" target="#b25">[26]</ref>. Even more interestingly, matrix scaling turned out to have surprising connections to fundamental problems in the theory of computation. Notably, in <ref type="bibr" target="#b23">[24]</ref>, it is observed that scaling can be used to approximate the permanent of any nonnegative matrix within a multiplicative factor of e n . Furthermore, deciding whether the permanent of a bipartite graph's adjacency matrix is 0 or at least 1 is equivalent to deciding whether that graph contains a perfect matching. Such scaling-based method can, as a matter of fact, be used to compute maximum matchings in bipartite graphs, which is a classic and intensely studied problem in graph algorithms <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For more history and information on this problem, we refer the reader to Idel's extensive survey <ref type="bibr" target="#b14">[15]</ref>, or <ref type="bibr" target="#b29">[30]</ref> for a list of applications. Now, in the matrix balancing problem, we are, again, given a nonnegative matrix A, and our goal here is to find a diagonal matrix D such that the matrix DAD -1 is balanced, that is the sum of each row is equal to the sum of the corresponding column. This procedure has been introduced first by Osborne <ref type="bibr" target="#b1">[2]</ref>, who was using it to precondition matrices in order to increase the accuracy of the eigenvalue computation. (Note that the balancing operation does not change the eigenvalues of the matrix.) The initially proposed algorithm for it was based on a simple iterative approach, and was then followed by a long series of improvements and extensions. The initial work on this problem focused on a variant in which one aims to balance 2 -norms of rows and columns. It turns out, however, that the 1 -norm- based version we study here is equivalent. In fact, balancing problems with respect to p norms, with constant p ≥ 1, are all reducible to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Previous Work</head><p>The early methods used for solving these problems -Osborne's iteration for balancing, and the RAS method for scaling -are simple iterative algorithms. However, merely the task of analyzing their convergence turned out to be a major challenge. Significant effort has gone into understanding their convergence <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and providing better analyses or better iterative methods resulted in a long line of work in this context.</p><p>The major shortcoming of the methods obtained so far for exactly solving the problem (depending only logarithmically on 1/ε) is their very large running time. In the following discussion we ommit runtime factors that depend (logarithmically) on the size of the input entries. For matrix scaling, Kalantari and Kachiyan <ref type="bibr" target="#b33">[34]</ref> obtained an algorithm that finds an ε-approximate solution and runs in time O(n 4 log(1/ε)), where n denotes the dimension of the matrix (we can assume the matrix is square w.l.o.g.) and ε is the desired accuracy parameter. This algorithm was based on the ellipsoid method. These authors also proposed -but not formally analyzed -an algorithm based on interior point method, which they expected to run in time O(m 3.5 log(1/ε)), where m denotes the number of non-zero entries of the input matrix. Then, Nemirovsky and Rothblum <ref type="bibr" target="#b34">[35]</ref> analyzed an interior point method-based algorithm which run in time O(m 4 log(1/ε)). Finally, Linial, Samorodnitsky, and Wigderson <ref type="bibr" target="#b23">[24]</ref> gave an O(n 7 log(1/ε)) time algorithm that is also strongly polynomial, in the sense that it does not depend at all on the size of input entries.</p><p>For the case of matrix balancing, Parlett and Reinsch <ref type="bibr" target="#b2">[3]</ref> provided an iterative method based on Osborne's iteration, without proving convergence. Then, Grad <ref type="bibr" target="#b3">[4]</ref> proved that Osborne's iteration converges in the limit. The first polynomial time bound was obtained by Kalantari, Khachiyan, and Shokoufandeh <ref type="bibr" target="#b6">[7]</ref>, who gave an algorithm with running time O(n 4 log(1/ε)).</p><p>Alternatively, if one is interested in the regime where the running time is allowed to depend polynomially -instead of logarithmically -on the (inverse of the) desired accuracy of the solution, there are algorithms that have an even better dependence on the other parameters. Specifically, the current state-of-the-art is given by Linial, Samorodnitsky, and Wigderson <ref type="bibr" target="#b23">[24]</ref>, who obtain O(n 3 ε -2 ) running time for the scaling problem. In the case of the balancing problem, recently, Ostrovsky, Rabani, and Yousefi <ref type="bibr" target="#b32">[33]</ref> made a significant progress by obtaining running times of O(m + nε -2 ) and O(n 3.5 ε -1 ).</p><p>Finally, another important line of work in this domain was focused on the related ∞ variant of the balancing problem, where the maximum entry of each row is required to be equal to the maximum entry of the corresponding column. Schneider and Schneider <ref type="bibr" target="#b7">[8]</ref> gave a non-iterative algorithm running in time O(n 4 ), improved to O(mn + n 2 ) by Young, Tarjan, and Orlin <ref type="bibr" target="#b35">[36]</ref>. More recently, Schulman and Sinclair <ref type="bibr" target="#b31">[32]</ref> provided an analysis of the classical Osborne-Parlett-Reinsch obtaining a running time of O(n 2 m), and gave a version of it with running time O(n 3 log(1/ε)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Contributions</head><p>We provide algorithms for both matrix scaling and balancing problems.</p><p>For the matrix scaling problem, we establish an algorithm that runs in time</p><formula xml:id="formula_0">O m log(κ(U * ) + κ(V * )) log 2 s A ε ,</formula><p>where U * and V * are the optimal scaling matrices, κ(•) is the maximum ratio between the diagonal entries of its argument, s A is the sum of the entries in the input matrix, and ε is the measure of the target error of the scaling, formally defined in Definition 5.</p><p>For the matrix balancing problem, we establish a running time of</p><formula xml:id="formula_1">O m log κ(D * ) log 2 w A ε ,</formula><p>where w A is the ratio of the sum of the entries to the minimum nonzero entry, D * is the optimal balancing matrix, κ(•) has the same meaning as above, and ε is the measure of the balancing error, as formally defined in Definition 19. Notably, our running times depend logarithmically on both the target accuracy and the magnitude of the entries in the optimal balancing or scaling. This implies that if the optimal solution has quasi-polynomially bounded entries, our algorithms run in nearly linear time O(m log(1/ε)) (ignoring logarithmic factors involving the entries of the input matrix). This includes, for instance, the case when input matrix has all its entries positive or, in case of matrix balancing, if there just exists a single row/column pair with all positive entries. However, there are matrices for which κ can be exponentially large (in n). For the case of such matrices we develop algorithms with negligible dependence on κ. These algorithms are based on interior point methods, with appropriately chosen barriers, commonly used in exponential programming <ref type="bibr" target="#b36">[37]</ref>. We show that the linear system solves required by the interior point method every iteration can be reduced via Schur complementing to approximately solving a Laplacian system, which can be done in nearly linear time using any standard Laplacian solver <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. This yields a running time of</p><formula xml:id="formula_2">O m 3/2 log w A ε ,</formula><p>where w A is the ratio between the largest and smallest nonzero entry of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Our Approach</head><p>We approach the scaling and balancing problems by developing a continuous optimization based perspective on them. More precisely, we solve both matrix scaling and balancing problems by casting them as tasks of minimizing certain corresponding convex functions. In fact, in the case of the balancing problem, that function is directly inspired by the one used in <ref type="bibr" target="#b6">[7]</ref>; for the scaling problem, it is function derived from the one used in <ref type="bibr" target="#b33">[34]</ref>.</p><p>Since our goal is to obtain logarithmic -instead of polynomial -dependence on the (inverse of the) desired accuracy ε, it would be tempting to use well-known tools for convex programing, such as ellipsoid method or interior point method. However, these methods are, a priori, computationally expensive. This motivates us to look for different, more direct approaches.</p><p>To this end, we develop a technique for minimizing a broader class of functions that we call second-order robust (with respect to ∞ ). Intuitively, this class corresponds to functions whose Hessians do not change too much within any unit ∞ -ball. And the consequence of that property that will be crucial for us is that local quadratic approximation of such functions at any given point is relatively accurate within the unit ∞ neighborhood of that point. As a result, iteratively optimizing the local approximation around the current point, while staying within that ∞ neighborhood, will be guaranteed to make progress towards minimizing the function. This iterative procedure can be viewed as a "box-constrained" variant of the Newton's method.</p><p>A priori, performing a single step of such a boxconstrained Newton's method, i.e., minimizing a quadratic function subject to box constraints might be a computationally costly task. We show, however, that it suffices to implement a weaker primitive, which we call a k-oracle. That primitive corresponds to (approximately) minimizing a quadratic function within a region that is within a factor of k larger than the target ∞ -ball. Once such a k-oracle is implemented efficiently, we can compute the global optimum of our second-order robust function using a small number of calls to it. More precisely, we show that one can minimize a convex function f that is second-order robust with respect to ∞ to within ε additive error from optimum in</p><formula xml:id="formula_3">O (kR ∞ + 1) log f (x 0 ) -f (x * ) ε (1)</formula><p>iterations, where each iteration consists of one call to the k-oracle, x 0 is the starting point, x * is the minimizer of f , and R ∞ is the ∞ radius of the level set of x 0 .</p><p>In the light of the above, the main technical difficulty remaining is obtaining an efficient implementation of a koracle. We show that for functions whose Hessian is symmetrically diagonally dominant, with nonzero off-diagonal entries, or SDD for short <ref type="foot" target="#foot_0">2</ref> , we can implement a k-oracle, with k = O(log n), in time that is nearly linear in the sparsity of the Hessian. We build here on the strategy underlying the Laplacian solver of Lee, Peng and Spielman <ref type="bibr" target="#b44">[45]</ref>. Specifically, we carefully lift the solutions corresponding to coarser (and smaller) approximations of the underlying matrix to the desired solutions corresponding to the initial matrix in a way that does not allow these lifted solutions to exceed the boundaries of a O(log n)-radius ∞ -ball.</p><p>Once the above optimization framework is developed, applying it to the scaling and balancing problems is fairly straightforward. It boils down to verifying that the functions that capture the respective problems are indeed second-order robust and have an SDD Hessian, and then bounding all the relevant quantities that (1) involves.</p><p>Independent Work: Finally, we note that Allen-Zhu, Li, Oliveira, and Wigderson <ref type="bibr" target="#b45">[46]</ref> obtained independently very similar results for the exact version of the problem. The running time of the algorithms they develop have a bit worse dependence on m, but they were able to establish better absolute bounds on κ (in terms of the problem parameters and the magnitude of the input entries) for the general, nondoubly stochastic variant of the matrix scaling problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Roadmap</head><p>The rest of the paper is organized as follows. First, we introduce relevant notation and concepts in Section II. Then, in Section III we formally introduce the class of convex functions we call second-order robust with respect to ∞ . For these, we develop a specific optimization primitive called box-constrained Newton method.</p><p>We describe how we can apply the primitive from Section III to matrix balancing and scaling in Section IV, by reducing these problem to a convex function minimization with favorable structure. In order to complete our algorithm, in Section V, we show how to efficiently implement an iteration of the box-constrained Newton in the special case where the Hessian of the function is SDD. In Section VI we provide a different approach for balancing and scaling based on interior point methods. Complete proofs and technical details are presented in the full version of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>Vectors: We let 0, 1 ∈ R n denote the all zeros and all ones vectors, respectively. When it is clear from the context, we apply scalar operations to vectors with the interpretation that they are applied coordinate-wise.</p><p>Matrices: We write matrices in bold. We use I to denote the identity matrix, and 0 to denote the zero matrix. Given a matrix A, we denote its number of nonzero entries by nnz(A). When it is clear from the context, we use m to denote the the number of nonzeros; similarly, we use n to denote the dimension of the ambient space. We denote by s A the sum of entries of A, by A the minimum nonzero entry of A, and by w A the ratio between these quantities. We use supp(A) to denote the set of pairs of indices (i, j) corresponding to the nonzero entries of A. Given a matrix A, we define r A = A 1 to be the vector consisting of row sums, and c A = A 1 to be the vector consisting of column sums. For a positive diagonal matrix A we denote the maximum ratio between its diagonal elements by κ(A).</p><p>Positive Semidefinite Ordering and Approximation: For symmetric matrices A, B ∈ R n×n we use A B to represent the fact that that x Ax ≤ x Bx, for all x. A symmetric matrix A ∈ R n×n is positive semidefinite (PSD) if A 0. We use , , ≺ in a similar fashion. For vectors x, we define the norm x A = √ x Ax. Given two PSD matrices A and B, and a parameter α &gt; 0, we use</p><formula xml:id="formula_4">A ≈ α B to denote the fact that e -α • B A e α • B.</formula><p>Laplacian and SDD matrices: A family of matrices that will play an important role in this paper are symmetric diagonally dominant (SDD) matrices. These are matrices A, that symmetric and, moreover, have each diagonal entry be larger than the sum of absolute values of the corresponding row entries. That is, for every i</p><formula xml:id="formula_5">A ii ≥ j =i |A ij |.</formula><p>A special case of SDD matrices are Laplacian matrices, which have negative off-diagonal entries and the sum of each row is required to be zero. The crucial fact about these matrices is that one can exploit their structure to solve linear systems in them in time that is only nearly linear <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>Diagonal Matrices: For x ∈ R n we denote by D(x) ∈ R n×n the diagonal matrix where D(x) ii = x i . Given a nonnegative diagonal matrix D, we use κ(D) to denote the ratio between its largest and smallest entry. We will overload notation and, for any matrix</p><formula xml:id="formula_6">A ∈ R n×n , use D(A) to denote the main diagonal of A, that is (D(A)) ii = A ii and (D(A)) ij = 0 for i = j.</formula><p>Gradients and Hessians: Given a function f we denote by ∇f (x) its gradient at x, and by ∇ 2 f (x) its Hessian at x. When the function is clear from the context, we also use H x to denote its Hessian at x.</p><p>Block Matrices: As part of our algorithms, we will consider partitioning the coordinates of vectors into sets of indices F and C. When we compute the quadratic form of a matrix with these vectors, we need to be able to reason about how values in each component interact with the rest of the vector. For that reason it is convenient to denote the block form notation for a matrix A as:</p><formula xml:id="formula_7">A = A [F,F ] A [F,C] A [C,F ] A [C,C] .</formula><p>Schur Complements: For a matrix A ∈ R n×n and a partition of its indices (F, C), the Schur complement of F in A is defined as</p><formula xml:id="formula_8">Sc(A, F ) def = A [C,C] -A [C,F ] A -1 [F,F ] A [F,C] .</formula><p>The exact use of Schur complements will become clear in Sections V,VI. These are objects that naturally arise during Gaussian elimination for the solution of linear systems. By pivoting out variables F the remaining system to solve for variables of C is exactly the Schur complement of F in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BOX-CONSTRAINED NEWTON METHOD FOR SECOND-ORDER ROBUST FUNCTIONS</head><p>The central element of our approach is developing an efficient second-order method based minimization framework for a broad class of functions that we will call secondorder robust with respect to ∞ . To motivate the choice of this class, recall that second-order methods for function minimization are iterative in nature, and they boil down to repeated minimizing the local quadratic approximation of the function around the current point. Consequently, in order to obtain meaningful guarantees about the progress made by such methods, one needs to ensure that this local quadratic approximation constitutes a good approximation of the function not only at the current point but also in a reasonably large neighborhood of that point. The most natural way to obtain such a guarantee is to ensure that the Hessian of the function (which is the basis of our local quadratic approximations) does not change by more than a constant factor in that neighborhood. As a result, the functions we are interested in optimizing in this paper are the ones that satisfy that property in an ∞ -ball around the current point. This is formalized in the following definition.</p><p>Definition 1 (Second-Order Robust w.r.t. ∞ ). We say that a convex function</p><formula xml:id="formula_9">f : R n → R is second-order robust (SOR) with respect to ∞ if, for any x, y ∈ R n such that x - y ∞ ≤ 1, ∇ 2 f (x) ≈ 2 ∇ 2 f (y), that is, 1 e 2 ∇ 2 f (x) ∇ 2 f (y) e 2 ∇ 2 f (x) .</formula><p>Note that the size of the ∞ -ball, as well as the exact factor by which the Hessian is allowed to change, are chosen somewhat arbitrarily -all choices of the constants can be made equivalent via an appropriate rescaling. Moreover, even if these quantities are not constant, they would only appear in the running time as a small polynomial factor. Now, the above definition suggests a natural framework for optimizing such functions. Namely, in every iteration, we optimize a local quadratic approximation of the function within a unit ∞ -ball around the current point. As we will see shortly, this approach can be rigorously analyzed. In particular, our key technical result is that if we apply this approach to an SOR function whose Hessians has additionally a special structure, i.e., those for which the Hessian is, essentially, a symmetric diagonally dominant (SDD) matrix, we can implement every iteration in time nearly linear in the number of nonzero entries of the Hessian. This leads to running time bounds captured by the following theorem.</p><p>Theorem 2 (Minimizing Second-Order Robust Functions w.r.t ∞ ). Let f : R n → R be a second-order robust (SOR) function with respect to ∞ , such that its Hessian is symmetric diagonally dominant (SDD) with nonpositive off-diagonals, and has m nonzero entries. Given a starting point</p><formula xml:id="formula_10">x 0 ∈ R n we can compute a point x, such that f (x) -f (x * ) ≤ ε, in time O (m + T )R ∞ log f (x 0 ) -f (x * ) ε , where x * is a minimizer of f , R ∞ = sup x:f (x)≤f (x0) x - x * ∞</formula><p>is the ∞ diameter of the corresponding level set of f , and T is the time required to compute the Hessian.</p><p>Note that the bounds provided by the above theorem are, in a sense, the best possible for any kind of approach that relies on repeated minimization of a local approximation of a function in an ∞ -ball neighborhood. In particular, as each step can make a progress of at most 1 in ∞ -norm towards the optimal solution, one would expect the total number of steps to be Ω(R ∞ ).</p><p>It turns out that the above theorem is all we need to establish our results for scaling and balancing problems (except the ones relying on the interior point method). That is, these results can be obtained by direct application of the above theorem to an appropriate SOR function. We provide the details in Section IV. Now, the first step to proving the above Theorem 2 is to view each iteration of our iterative minimization procedure as a call to a certain oracle problem. Definition 3. We say that a procedure O is a k-oracle for a class of matrices M, if on input (A, b), where A ∈ M ⊆ R n×n , and b ∈ R n , returns a vector z satisfying 1) z ∞ ≤ k, and 2)</p><formula xml:id="formula_11">1 2 z Az + b z ≤ 1 2 • min z ∞ ≤1 1 2 z Az + b z .</formula><p>Note that the minimum of the left-hand side of Condition (2) above is always non-positive. This is desired, since this expression is supposed to measure our function minimization progress.</p><p>Observe that minimizing the function 1 2 z Az + b z without any constraints on z corresponds to solving a linear system Az = -b. So, one can view the k-oracle problem as a certain generalization of linear system solving. Specifically, it is a task in which we aim to find a point in the ∞ -ball of diameter k around the origin that is closest (in a certain sense) to the solution to that linear system.</p><p>One can view the parameter k as the measure of the "quality" of our k-oracle. The smaller it is, the faster convergence the overall procedure will have. Importantly, however, the value of k impacts only the convergence and not the quality of the final solution. The following theorem makes this relationship precise.</p><p>Theorem 4. Let f : R n → R be a function that is second-order robust with respect to ∞ . Let O be a koracle for {∇ 2 f (x) : x ∈ R n }, along with an initial point x 0 ∈ R n and an accuracy parameter ε. Let</p><formula xml:id="formula_12">R ∞ = sup x:f (x)≤f (x0) x -x * ∞ , where x * is a mini- mizer of f . Then one can produce a solution x T satisfying f (x T ) -f (x * ) ≤ ε using O (kR ∞ + 1) log f (x 0 ) -f (x * ) ε calls to O.</formula><p>In Section V we design an efficient k-oracle, with k = O(log n), for the family of SDD matrices. Combining Theorem 38 with Theorem 4 immediately gives the proof of Theorem 2. We remark that while Theorems 2, 4 are stated and proved for functions defined over R n , they can be extended in a straightforward way to hold when f is defined over an arbitrary closed, convex set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MATRIX SCALING AND BALANCING</head><p>Having developed our main optimization primitives, we can develop efficient algorithms for matrix scaling and matrix balancing. Our approach is essentially the same for both problems, and differs only in technical details.</p><p>At the high level, we will construct convex functions with optima corresponding to exact scaling/balancing of the matrix. Moreover, the gradient of these functions at a specific scaling/balancing of the matrix will be directly related to the quality of this particular scaling/balancing. This will allow us to prove that approximately optimal points correspond to ε-balancing/ε-scaling. The fact that that these functions are second-order robust with respect to ∞ makes it sufficient to apply the optimization method from Section III. To complete the algorithm and its running time analysis, we need then to address two issues.</p><p>Firstly, proving running time bounds for this method requires an upper bound on the ∞ radius of the level set of the initial point, i.e. the R ∞ parameter defined in Theorem 4. Depending on the structure of the matrix, there are several different bounds that one can prove, depending only on parameters of the original problem. However, the most interesting case is when we are promised that the exact scaling/balancing of the matrix is "small" (in the sense that the ratio between factors is, say, polynomial). In that case, we can regularize the function to turn this promise into a guarantee for the size of the level set without sacrificing too much accuracy. Moreover, by using a simple doubling approach, we can make the algorithm not require explicit knowledge of such a parameter, and it will only appear as a factor in the final runnning time of the algorithm.</p><p>Secondly we need to ensure that we can efficiently implement k-oracles for the Hessians of these functions. In our case, this boils down to proving that the Hessians are SDD matrices with sparsity equal to that of the input matrix, and then build on the existing Laplacian solving work. For the remainder of this section, we define the convex functions that we need optimize, show how to regularize them, and prove bounds on the corresponding R ∞ parameters. We describe and sketch the analysis for the implementation of a O(log n)-oracle in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Matrix Scaling</head><p>We now formally define the scaling problem, along with the notion of ε-scaling.</p><p>Definition 5 (Matrix Scaling). Let A ∈ R n×n be a nonnegative matrix and r, c ∈ R n be vectors such that n i=1 r i = n j=1 c i , and r ∞ , c ∞ ≤ 1 3 . We say that two nonnegative diagonal matrices X and Y (r, c)-scale A if the matrix M = XAY satisfies M 1 = r and M 1 = c, i.e. row i sums to r i and column j sums to c j for every i, j. Definition 7 (Scalable and Almost-Scalable Matrices). A nonnegative matrix A, is called (r, c)-scalable, if there exist X and Y that (r, c)-scale A. It is called almost (r, c)scalable if for every ε &gt; 0, there exist X ε and Y ε that ε-(r, c) scale A.</p><p>We will cast matrix scaling as a convex optimization problem and show that applying the method from section III yields a good approximate scaling.  This implies that if U * and V * are, say, quasipolynomially bounded, we can find an approximate scaling in nearly linear time. If fact, we can generalize this statement to obtain a similar result for the case of approximate scalings. This is made precise in Theorem 9.</p><p>1) Matrix Scaling via Convex Optimization: Recall that we want to encode the matrix scaling problem as a an instance of minimizing of a certain convex function. Given the input matrix A, the function we want to consider is:</p><formula xml:id="formula_13">f (x, y) = 1≤i,j≤n A ij e xi-yj - ⎛ ⎝ 1≤i≤n r i x i - 1≤j≤n c j y j ⎞ ⎠ .</formula><p>(2) We want to argue now that computing an (approximate) scaling of the matrix A can indeed be recovered from an (approximate) minimum of the above function. Specifically, we want to establish the following theorem.</p><p>Theorem 9. Suppose that there exist a point</p><formula xml:id="formula_14">z * ε = (x * ε , y * ε ) for which f (z * ε ) -f * ≤ ε 2 /(3n) and z * ε ∞ ≤ B. Then we can compute an ε-(r, c) scaling of A in time O mB log 2 (s A /ε) .</formula><p>The proof is straightforward given the lemmas below and is presented in the full version of the paper. First, we will prove that approximate optimality of f implies an approximate scaling of the matrix. Lemma 10. Let A be an ε-scalable matrix. Let f * = inf (x,y) f (x, y). Then, a pair of vectors (x, y) satisfying f (x, y)f * ≤ ε 2 /3n, for 0 &lt; ε ≤ 1, yields an ε-(r, c) scaling of A:</p><formula xml:id="formula_15">M = D(exp(x)) • A • D(exp(y)) .</formula><p>Note that we compare the value of f (x, y) to its infimum, as for the case of almost scalable matrices it is possible that this value is attained only to the limit.</p><p>To prove this lemma, we first look at the first and second order derivatives of f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 11. Let M be the matrix obtained by scaling A with vectors (x, y), i.e. M = D(exp(x)) • A • D(exp(y)).</head><p>The gradient and Hessian of f satisfy the identities:</p><formula xml:id="formula_16">∇f (x, y) = r M -c M - r -c , ∇ 2 f (x, y) = D(r M ) -M -M D(c M ) .</formula><p>We can observe that any (x, y) for which ∇f (x, y) is equal to 0 yields diagonal matrices that exactly scale A. Moreover, this statement also holds in an approximate sense. One can prove that a large gradient in 2 norm implies that the current point is far from optimal in function value. Making this statement precise, allows us to prove Lemma 10. The technical details can be found in the full version of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Regularization for Solving via Box-Constrained Newton Method:</head><p>It is straightforwards to verify that the function we are minimizing (defined in Equation <ref type="formula" target="#formula_38">2</ref>), satisfies the requirements necessary for us to be able to apply the tools from Section III.</p><p>Lemma 12. The function f defined in (2) is convex, secondorder robust with respect to ∞ , and its Hessian is SDD.</p><p>One should observe, however, that Theorem 4 requires bounding the radius of the entire level set containing our initial point and not merely the distance to some (approximate) minimizer of our function f . This means that the existence of an (approximate) minimizer that is close to our initial point is not sufficient to apply Theorem 4. To circumvent that problem, we regularize the function f by adding to it a term that, on one hand, has a relatively small impact on the additive error we can achieve, but, on the other hand, ensures that the entire relevant level set is contained in some sufficiently small ∞ -ball around our initial point. The following lemma makes these statements precise.</p><formula xml:id="formula_17">Lemma 13. Let z * ε = (x * ε , y * ε ) be a point for which f (z * ε ) - f * ≤ ε 2 /(3n) and z * ε ∞ ≤ B.</formula><p>Then, the regularization of f defined as</p><formula xml:id="formula_18">f (x, y) = f (x, y) + ε 2 36n 2 e B ⎛ ⎝ i (e xi + e -xi ) + j (e yj -e -yj ) ⎞ ⎠ (3)</formula><p>satisfies the following properties 1) f is second-order robust with respect to ∞ and its Hessian is SDD, 2) f (z) ≤ f (z), and there is a point</p><formula xml:id="formula_19">z * such that f ( z * ) ≤ f * + ε 2 9n , 3) for all z such that f (z ) ≤ f (0), z ∞ = O(B log(ns A /ε)).</formula><p>Theorems 8 and 9 follow from applying Theorem 2 to the regularized function defined in Equation <ref type="formula">3</ref>, and then combining it with the guarantees of Lemmas 10 and 13. We note that we don't need an explicit knowledge of an a priori bound on B. We can simply run our algorithm repeatedly, doubling our guess at the value of B each time. This will not increasing the overall running time by more than a factor of two.</p><p>3) Bounding the Magnitude of the Optimal and Approximately Optimal Scalings for Doubly Stochastic Scaling: In order to provide bounds for the magnitude of the scaling factors that only depend on the parameters of the initial problem, we refer to the following lemmas from <ref type="bibr" target="#b33">[34]</ref> for the case of double stochastic (i.e. (1,1)) scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 14 (Lemma 1 of [34]). If A is strictly positive, then it can be scaled to doubly stochastic by diagonal matrices U, V with log(κ(U) + κ(V)) ≤ O(log(w A )).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 15 (Corollary 1 of [34]). If A is scalable, then it can be scaled to doubly stochastic by diagonal matrices U, V with log(κ(U) + κ(V)) ≤ O(n log(w A )).</head><p>For almost scalable matrices, there can be arbitrarily good solutions, using arbitrarily large scaling factors. To prove bounds on the runtime of finding an approximate doublystochastic matrix, we will have to explicitly demonstrate an vector that approximately minimizes function f while having small ∞ norm. Lemma 16. If A is almost-doubly-stochastic scalable, then there exist points (x, y) such that f (x, y) -</p><formula xml:id="formula_20">f * ≤ ε 2 /3n, such that (x, y) ∞ ≤ O(n log(nw A /ε)).</formula><p>For the general case of (r, c)-scaling we refer to the recent lemmas from the parallel work of <ref type="bibr" target="#b45">[46]</ref>. The assumption that the scaling targets are integral is mild, since one can approximate real numbers by rational ones which can then by scaled to be integral (the dependence on this scaling is logarithmic).</p><formula xml:id="formula_21">Lemma 17 (Lemma 3.3 of [46]). If A is almost (r, c)- scalable with r, c being integral, then it can be ε-scaled by diagonal matrices U, V with log(κ(U) + κ(V)) ≤ O(n log(nw A r 1 /ε)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Matrix Balancing</head><p>Our approach for the balancing problem is completely analogous to the one we used for the scaling problem. There are only minor technical differences. To state them, we first formally define the problem and the notion of approximation we are considering for it.</p><p>Definition 18 (Matrix Balancing). Let A be a square nonnegative matrix. We say that A is balanced if the sum of each row is equal to the sum of the corresponding column, i.e. r A = c A . We say that a nonnegative diagonal matrix D balances A if the matrix M = DAD -1 is balanced.</p><p>Definition 19 (ε-Balanced Matrix <ref type="bibr" target="#b6">[7]</ref>). We say that a nonnegative matrix</p><formula xml:id="formula_22">M ∈ R n×n is ε-balanced if r M -c M 2 1≤i,j≤n M ij = n i=1 ((r M ) i -(c M ) i ) 2 1≤i,j≤n M ij ≤ ε.</formula><p>Observe that this definition is invariant to a global scaling of all the entries of the matrix by some factor. There is a very simple condition that characterizes the set of matrices that can be balanced</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 20 ([7]). A nonnegative matrix A ∈ R n×n can be balanced if and only if the graph with adjacency matrix A is strongly connected.</head><p>In the case when the graph is not strongly connected, the matrix can have its rows and columns rearranged so as to be written as a lower triangular block matrix with strongly connected diagonal blocks. The reason no exact balancing exists is that off diagonal block elements will always create imbalances. This, however, is not an obstacle for approximately balancing the matrix. Once we balance the diagonal blocks, we can set all of the off-diagonal block entries to a very small value, say ε/n, so that they don't cause significant imbalances. This corresponds to implicitly scaling the block rows and collumns by a very large amount, making the off-diagonal entries arbitrarily close to zero. Therefore, since the case of matrices that cannot be exactly balanced is easy to detect, and can be easily reduced to the exactly balanceable case, from now on we consider only matrices that can be balanced, and therefore represent strongly connected graphs.</p><p>We can now state our main theorem for this section, which follows our initial discussion.</p><p>Theorem 21. Let A be a matrix that can be balanced by the diagonal matrix D * . Then, we can compute an εapproximate balancing of A in time</p><formula xml:id="formula_23">O(m log κ(D * ) log 2 (w A /ε)) .</formula><p>This immediately implies that if D * is, say, quasipolynomially conditioned, we can find an approximate balancing in nearly linear time.</p><p>Again, we can generalize this result to hold for approximate balancings. We make this statement precise in Theorem 22.</p><p>1) Reducing Matrix Balancing to Convex Optimization: Similarly to the case of the scaling problem, we encode this problem as a minimization of an appropriately constructed convex function. The function we consider here is</p><formula xml:id="formula_24">f (x) = 1≤i,j≤n A ij e xi-xj ,<label>(4)</label></formula><p>and this function was already defined in <ref type="bibr" target="#b6">[7]</ref>. Similarly to the case of matrix scaling, we will show that (approximately) minimizing this function corresponds to (approximately) balancing the matrix A. For the rest of this section, we will define f * to be the infimum value of f in its domain, that is f * = inf x f (x). The main theorem of this section is the following.</p><p>Theorem 22. Suppose that there exists a point x such that f (x) ≤ f * + ε 2 A /24, and x ∞ ≤ B. Then, we can compute an ε-approximate balancing of A in time</p><formula xml:id="formula_25">O(mB log 2 (w A /ε)) .</formula><p>Similarly to the matrix scaling case, the proof of this theorem follows directly from the key lemmas presented below. First, we prove that small additive error in function optimization implies an approximate balancing for A.</p><p>Lemma 23. Consider a matrix A and the corresponding function f . Any vector x satisfying f (x)f * ≤ ε 2 A /8 yields an ε-approximate balancing of A:</p><formula xml:id="formula_26">M = D(exp(x)) • A • D(exp(-x)) .</formula><p>Proving the lemma requires computing the first and second order derivatives of f . Lemma 24. Let M be the matrix obtained by balancing A with the vector x, which corresponds to M = D(exp(x)) • A • D(exp(-x)). The gradient and Hessian of f satisfy the identities:</p><formula xml:id="formula_27">∇f (x) = r M -c M , ∇ 2 f (x) = D(r M + c M ) -(M + M ) .</formula><p>Intuitively, since the gradient is 0 precisely when the corresponding point produces an exact balancing, a small gradient should imply a good approximate balancing. This guides the proof of Lemma 23. We will prove that a large gradient corresponds to being able to significantly decrease the function value, thus contradicting the approximate optimality of the point.</p><p>2) Regularization for Solving via Box-Constrained Newton Method: We observe that the function f defined in (4) satisfies all the conditions required to efficiently minimize it using the method we described in Section III.</p><p>Lemma 25. The function f is convex, second-order robust with respect to ∞ , and its Hessian is SDD.</p><p>The method we described in Section III depends on a promise concerning the point we initialize it with. Recall that in order to apply Theorem 2 we require an upper bound on the size of the ∞ -ball containing the level set of the initial point. In order to provide good bounds, we regularize f . The description and effect of this regularization in captured in the following lemma.</p><p>Lemma 26. Suppose that there exists a point x such that f (x) ≤ f * + ε 2 A /24, and x ∞ ≤ B. Then, the regularization of f is defined as</p><formula xml:id="formula_28">f (x) = f (x) + ε 2 A 48ne B n i=1 (e xi + e -xi )<label>(5)</label></formula><p>and satisfies the following properties: 1) f is second-order robust with respect to ∞ and has a SDD Hessian, 2) f (x) ≤ f (x), and if x * is the minimizer of f , then</p><formula xml:id="formula_29">f ( x * ) ≤ f (x * ) + ε 2 A /24, 3) for all y such that f (y) ≤ f (0), y -x * ∞ = O(B log(nw A /ε)).</formula><p>In particular, this lemma implies that approximately optimizing the regularized function will still produce an approximately balanced matrix.</p><p>Theorem 21 then follows by applying Theorem 2 to the regularized function defined in Lemma 26, and combining it with the error guarantee of Lemma 23. Similarly to the case of the scaling problem, we don't need to know any a priori bound on B. Just trying increasingly larger value of B (i.e., doubling our guess at each iteration) is sufficient.</p><p>3) Bounding the Condition Number of the Optimal Balancing: As we saw above, the running time given by Theorem 21 depends logarithmically on κ(D * ), where D * is the matrix that achieves the optimal balancing. This parameter can be upper bound using certain input-dependent quantities: Lemma 27. Let A ∈ R n×n be a nonnegative matrix. Suppose that the graph with adjacency matrix A is strongly connected, and every vertex can reach every other vertex within at most k hops. Then the matrix D * that perfectly balances A has log κ(D * ) = O(k log w A ).</p><p>The lemma yields the following upper bound on the value of κ(D * ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTING AN O(log n)-ORACLE IN NEARLY LINEAR TIME</head><p>In Section IV we reduced the balancing and scaling problems to the approximate minimization of second-order robust functions with respect to the ∞ norm. All that is left to have a complete algorithm, we need a fast procedure to implement a k-oracle as in Definition 3. Namely, show how to construct an O(log n)-oracle for the problem,</p><formula xml:id="formula_30">min x ∞ ≤1 x Mx + b, x , (<label>6</label></formula><formula xml:id="formula_31">)</formula><p>where M is an SDD matrix. For this section, whenever we say that a matrix is SDD we will also imply that the offdiagonal entries are nonpositive.</p><p>One possible approach, is to use standard convex optimization reductions to turn this problem into the minimization of the maximum of an ∞ norm and an 2 norm subject to linear constraints. This problem can be solved in time O(mn 1/3 ) using the multiplicative weights framework as applied in <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. The resulting algorithm for implementing the k-oracle would take time O(m+n 4/3 ), by taking advantage of spectral sparsification algorithms <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Instead, we will come up with a faster algorithm.</p><p>Our approach, based on the Lee-Peng-Spielman solver <ref type="bibr" target="#b44">[45]</ref>, is to identify large sets of vertices where the problem is "easy" to solve and then deal with the rest of the graph (reduced in size) recursively. The particular notion of "easy" we are going to use, is that of strong diagonal dominance. Definition 29. A matrix M is α-strongly diagonally dominant (α-SDD), if for all i</p><formula xml:id="formula_32">M ii ≥ (1 + α) j =i |M ij |.</formula><p>The reason that such matrices enable us to solve the corresponding problems fast is that they can be wellapproximated by a diagonal matrix.</p><p>Lemma 30. Every α-SDD matrix M, with diagonal D(M), satisfies</p><formula xml:id="formula_33">1 - 1 1 + α D(M) M 1 + 1 1 + α D(M).</formula><p>In our context, problems in the form of Equation <ref type="formula" target="#formula_30">6</ref>, where M is an α-SDD matrix for some α ≥ Ω(1), can be turned into well conditioned quadratic minimization problems for which we can apply standard linearly convergent algorithms. For a more detailed description and analysis of such algorithms can be found in <ref type="bibr" target="#b51">[52]</ref>.</p><p>Lemma 31. There is an algorithm FASTSOLVE, that given an Ω(1)-SDD matrix M, and ε &gt; 0, returns a point x, such that x ∞ ≤ 2, and</p><formula xml:id="formula_34">x M x + x, b ≤ (1 -ε) min x ∞ ≤2 x Mx + x, b in time O(m log(1/ε))</formula><p>, where m is the number of nonzero entries of M.</p><p>An even simpler case is when the matrix is of size 1, in which case the problem can be exactly solved in constant time:</p><p>Lemma 32. There is an algorithm TRIVIALSOLVE, that given a 1 by 1 matrix M returns an x optimizing x Mx + b, x over the interval [-1, 1].</p><p>A key insight of <ref type="bibr" target="#b44">[45]</ref> is that one can find Ω(1)-SDD submatrices of M of size Ω(n). We denote such a subset by F and V \ F by C. To ensure that solving the problem for x F will not interfere with our solution x C we map a solution xC supported only on coordinates of C to a solution x C through a linear mapping P. If P were the energy minimizing extension of voltages on C to voltages on V ,</p><formula xml:id="formula_35">(Px C ) F = M -1 [F,F ] M [F,C] xC , we would have that x F and x C are M-orthogonal, since x F MPx C = 0. Then, optimizing over xC involves the quadratic P MP which is exactly equal to M [C,C] - M [C,F ] M -1 [F,F ] M [F,C] = Sc(M, F ).</formula><p>Applying this proccess recursively leads to the notion of vertex sparsifier chains that we will heavily rely on. Definition 33 (Definition 5.7 of <ref type="bibr" target="#b44">[45]</ref>). For any SDD matrix M (0) , a vertex sparsifier chain of M (0) with parameters α i ≥ 4 and 1/2 ≥ ε i &gt; 0, is a sequence of matrices and subsets (M (1) , . . . , M (d) ; F 1 , . . . , F d-1 ) such that 1) M (1) </p><formula xml:id="formula_36">≈ ε0 M (0) , 2) M (i+1) ≈ εi Sc(M (i) , F i ), 3) M (i)</formula><p>[Fi,Fi] is α i -strongly diagonally dominant, and 4) M (d) has size 1.</p><p>To be able to reason about the approximation guarantees of the chain as a whole we will use an error-quantifying definition.</p><p>Definition 34 (Definition 5.9 of <ref type="bibr" target="#b44">[45]</ref>). An ε-vertex sparsifier chain of an SDD matrix M (0) of work W , is a vertex sparsifier chain of M (0) with parameters α i ≥ 4 and</p><formula xml:id="formula_37">1/2 ≥ ε i &gt; 0 that satisfies 1) 2 d-1 i=0 ε i ≤ ε, 2) d-1 i=0 m i log αi ε -1 i ≤ W , where m i is the number of nonzeros in L (i) .</formula><p>Finally, the construction of such chains, as well as their error guarantees have been already analyzed in <ref type="bibr" target="#b44">[45]</ref> and can be used in a black-box manner. Since we cannot exactly compute the energy minizing mapping P, we will define an approximate mapping that suffices for our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 36. A linear mapping P is an ε-approximate voltage extension from</head><formula xml:id="formula_38">C to V according to L if for any xC ∈ R |C| , 1) ( P -P)x C M ≤ ε Px C M ,<label>2</label></formula><p>) P is the identity on coordinates in C 3) the coordinates of Px C are convex combination of the coordinates of xC and 0. where P is the energy-minimizing extension.</p><p>We will construct such a mapping through a simple averaging scheme. First we set the voltage of every vertex in F to be the weighted average of its neighbors in C. Then at every step we replace its voltage by the weighted average of all its neighbors. (Here, excess diagonal is treated as an edge to a vertex with voltage 0.) We do so for O(log(1/ε)) iterations. We formally state the procedure and prove its correctness in the full version of the paper.</p><p>It is easy to see that all steps of the algorithm are linear maps, and we can therefore also implement its transpose. Lemma 37. For any SDD matrix M, given an Ω(1)-SDD subset F and some ε &gt; 0 one can apply an ε-approximate voltage extension mapping in time O(m log(1/ε)).</p><p>Having expressed all of the components of our approach, stating the algorithm is simple. Given the decomposition of the problem the vertex sparsifier chain provides, we will solve the smallest problem and then iteratively combine it with the solution of the submatrices along the chain. The algorithm is formally described in Figure <ref type="figure" target="#fig_6">1</ref>, and the main claim in Theorem 38.</p><p>OPTIMIZECHAIN((M (1) , . . . , M (d) ; F 1 , . . . , F d-1 ; ε 0 , . . . , ε d-1 ), b) 1) b (1) ← b/e ε0 2) For i ← 1, . . . , d -1 a) P (i) ← APPROXMAPPING(M (i) , F i , ε i ) b) b (i+1) ← ( P (i) ) b (i) /(e εi (1 + ε i + ε 2 i )) 3) x (d) ← TRIVIALSOLVE(M (d) , b (d) ) 4) For i ← d -1, . . . , 1 a) x (i) </p><formula xml:id="formula_39">C ← P (i) x (i+1) b) x (i) F ← FASTSOLVE(M (i) [Fi,Fi] , b (i) Fi , ε i ) c) x (i) ← x (i) C + x (i) F 5) return x (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MATRIX SCALING AND BALANCING WITH</head><p>EXPONENTIAL CONE PROGRAMMING The algorithm developed in the previous sections is essentially optimal in the regime where the ratio between the scaling factors is relatively small (say polynomial in n). Since there are matrices for which this ratio is exponential, we develop a complementary algorithm with negligible runtime dependence on this ratio, at the cost of a mild increase in the dependence on m. The algorithm is based on interior point methods.</p><p>Although interior point methods would seem like a natural option for the problems of matrix scaling and balancing, standard formulations require solving linear systems involving various rescalings of the input matrix. A priori, it is not clear whether these can be solved faster than matrix multiplication time. However, it turns out that a somewhat nonstandard formulation requires solving linear systems for more structured matrices. Particularly, we see that these matrices admit a decomposition involving only matrices that are easy to invert (triangular matrices, solvable by back substitution, and SDD matrices which can be tackled via a standard Laplacian solver). Notably, a similar observation was made by Daitch and Spielman <ref type="bibr" target="#b52">[53]</ref>, in the case of interior point methods applied to flow problems on graphs. <ref type="bibr" target="#b33">[34]</ref> also consider a formulation similar to ours for the matrix scaling problem, however they do not prove exact convergence bounds or state the algorithm rigorously. Moreover, since nearly-linear time SDD solvers where not known at the time, their algorithm provided no benefit compared to other approaches.</p><p>The main result of this section is the following. This is as a matter of fact a consequence of the fact that a specific class of functions, which capture both balancing and scaling, can be minimized efficiently. We capture this result in the following Theorem. A ij e xi-xjd, x , <ref type="bibr" target="#b6">(7)</ref> and let B x be a positive real number. There exists an algorithm which, for any ε &gt; 0, finds a vector x such that f (x)f (x * (Bx) ) ≤ ε (where x * (Bx) is the optimum of f over the region x ∞ ≤ B x ) in time</p><formula xml:id="formula_40">O m 3/2 log 2 + B x + s A ε + d 1 ε .</formula><p>Using this result, one can then conclude the proof of Theorem 39.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>58th</head><label></label><figDesc>Annual IEEE Symposium on Foundations of Computer Science 0272-5428/17 $31.00 © 2017 IEEE DOI 10.1109/FOCS.2017.88</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 6</head><label>6</label><figDesc>(ε-(r, c) scaling). Given nonnegative A and positive diagonal matrices X, Y, we say that (X, Y) is an ε-(r, c) scaling (or ε-scaling, when r and c are clear from the context) for matrix A if the matrix M = XAY satisfies r Mr 2 2 + c Mc 2 2 ≤ ε .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 8 .</head><label>8</label><figDesc>Let A be a matrix, that has an (r, c) scaling(U * , V * ). Then, we can compute an ε-(r, c) scaling of A in time O m log(κ(U * ) + κ(V * )) log 2 (s A /ε) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3</head><label></label><figDesc>In literature we also encounter this problem for non-square matrices; however solving squares is sufficient, since given A ∈ R n×c , we can reduce to this instance by scaling the square matrix 0c,c A A 0r,r . The upper bound on r and c is harmless, since for larger values we can always shrink all of A, r, c and ε by the same factor in order to enforce this constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Corollary 28 .</head><label>28</label><figDesc>If A is a balanceable matrix, and D * perfectly balances it, then log κ(D * ) = O(n log w A ). If A is strictly positive, then log κ(D * ) = O(log w A ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 35 (</head><label>35</label><figDesc>Theorem 5.10 of<ref type="bibr" target="#b44">[45]</ref>). Every SDD matrix M of dimension n has a δ-vertex sparsifier chain of work O(n) and d ≤ O(log n), for any constant 0 &lt; δ ≤ 1. Such a chain can be constructed in time, O(m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Optimizing a vertex sparsifier chain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 39 .</head><label>39</label><figDesc>Given a nonnegative matrixA ∈ R n×n , one can: 3/2 log(w A ε -1 )) , 2) if the matrix is almost (r, c)-scalable, compute a ε-(r, c)-scaling in time O(m 3/2 log(s A ε -1 )) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 40 .</head><label>40</label><figDesc>Let A ∈ R n×n be a nonnegative matrix with m nonzero entries, let f be the functionf (x) = (i,j)∈supp(A)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Such matrices can essentially be viewed as a Laplacian matrix plus a nonnegative diagonal.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENTS MC was supported by the National Science Foundation under Grant No. 1111109 and Grant No. 1553428, and by the National Defense Science and Engineering Graduate Fellowship. AM and DT were supported by the National Science Foundation under Grant No. 1553428. AV was supported by the National Science Foundation under Grant No. 1111109 and Grant No. 1553428.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Matrix scaling and balancing via box constrained newton&apos;s method and interior point methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02310</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On pre-conditioning of matrices</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="338" to="345" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Balancing a matrix for calculation of eigenvalues and eigenvectors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Parlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="304" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matrix balancing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="280" to="284" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Concerning diagonal similarity of irreducible matrices</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hartfiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="425" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Line-sum-symmetric scalings of square nonnegative matrices</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Eaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">G</forename><surname>Rothblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming Essays in Honor of George B. Dantzig Part II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="124" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the complexity of matrix balancing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khachiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="463" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Max-balancing weighted directed graphs and matrix scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of Operations Research</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="208" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Concerning nonnegative matrices and doubly stochastic matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sinkhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A relationship between arbitrary positive matrices and doubly stochastic matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sinkhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="876" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rounding errors in algebraic processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wilkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On pairs of multidimensional matrices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="263" to="268" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A note on approximations to discrete probability distributions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="386" to="392" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Additive decomposition of nonnegative matrices with applications to permanents and scalingt</title>
		<author>
			<persName><forename type="first">S</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear and Multilinear Algebra</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="78" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A review of matrix scaling and sinkhorn&apos;s normal form for matrices and positive maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Idel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06349</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>Mathworks</surname></persName>
		</author>
		<ptr target="https://www.mathworks.com/help/matlab/ref/eig.html" />
		<title level="m">eig -eigenvalues and eigenvectors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">balance -diagonal scaling to improve eigenvalue accuracy</title>
		<ptr target="https://www.mathworks.com/help/matlab/ref/balance.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Balance a square matrix via lapack&apos;s dgebal</title>
		<author>
			<persName><surname>Rdocumentation</surname></persName>
		</author>
		<ptr target="https://www.rdocumentation.org/packages/expm/versions/0.99-1.1/topics/balance" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Package &apos;expm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Goulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dutang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Firth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stadelmann</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/expm/expm.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Balancing and conditioning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Blackford</surname></persName>
		</author>
		<ptr target="http://www.netlib.org/lapack/lug/node94.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">De ingenieur 52</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Kruithof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
	<note>E15-E25</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<title level="m">The Analysis of Cross-Classified Categorical Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deterministic strongly polynomial algorithm for matrix scaling and approximate permanents</title>
		<author>
			<persName><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samorodnitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;98: Proceedings of the 30th Annual ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="644" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classical deterministic complexity of edmonds&apos; problem and quantum entanglement</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gurvits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;03: Proceedings of the 35th Annual ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the methods of measuring association between two attributes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">U</forename><surname>Yule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="579" to="652" />
			<date type="published" when="1912">1912</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum matching and a polyhedron with 0, 1-vertices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards B</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="55" to="56" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster scaling algorithms for network problems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Gabow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1013" to="1036" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Navigating central path with electrical flows: From flows to matchings, and back</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comparative study of algorithms for matrix balancing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Zenios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="455" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Balancing sparse matrices for computing eigenvalues</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear algebra and its applications</title>
		<imprint>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="261" to="287" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis of a classical matrix preconditioning algorithm</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinclair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;15: Proceedings of the 47th Annual ACM on Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="831" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matrix balancing in L p norms: Bounding the convergence rate of osborne&apos;s iteration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yousefi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA&apos;17: Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the complexity of nonnegative-matrix scaling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khachiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linear Algebra and its applications</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On complexity of matrix scaling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="page" from="435" to="460" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster parametric shortest path and minimum-balance algorithms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Orlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="221" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lectures on modern convex optimization: analysis, algorithms, and engineering applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;04: Proceedings of the 36th Annual ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Approaching optimality for solving SDD systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koutis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS&apos;10: Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A nearly m log n-time solver for SDD linear systems</title>
	</analytic>
	<monogr>
		<title level="m">FOCS&apos;11: Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A simple, combinatorial algorithm for solving SDD systems in nearly-linear time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Orecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;13: Proceedings of the 45th Annual ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="911" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Solving sdd linear systems in nearly m log 1/2 n time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;14: Proceedings of the 46th Annual ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparsified cholesky and multigrid solvers for connection laplacians</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;16: Proceedings of the 48th Annual ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Approximate gaussian elimination for laplacians: Fast, sparse, and simple</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sachdeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS&apos;16: Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sparsified cholesky solvers for sdd linear systems</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08204</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Much faster algorithms for matrix scaling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02315</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Electrical flows, Laplacian systems, and faster approximation of maximum flow in undirected graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kelner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;11: Proceedings of the 43rd ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Runtime guarantees for regression problems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Theoretical Computer Science, ITCS &apos;13</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">January 9-12, 2013. 2013</date>
			<biblScope unit="page" from="269" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph sparsification by effective resistances</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1913" to="1926" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Constructing linear-sized spectral sparsification in almost-linear time</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;17: Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An SDP-based algorithm for linear-sized spectral sparsification</title>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;17: Proceedings of the 49th Annual ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="250" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex programming volume i: Basic course</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Faster approximate lossy generalized flow via interior point algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Daitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC &apos;08: Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="451" to="460" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
