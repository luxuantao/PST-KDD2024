<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-02">2 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shihao</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-02">2 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.00552v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graphstructured data, and have achieved state-of-the-art performance on all sorts of practical tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are plagued to overfitting if not regularized properly. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an L 0 -norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy / insignificant edges, and thus focus computation on more important portion of a graph. Extensive experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50%-80% edges from large graphs, such as PPI and Reddit, while retaining similar classification accuracies. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that sparsifies graphs for the purpose of identifying important relationship between nodes and for robust training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph-structured data is ubiquitous in many real-world systems, such as social networks <ref type="bibr" target="#b20">[21]</ref>, biological networks <ref type="bibr" target="#b29">[30]</ref>, and citation networks <ref type="bibr" target="#b17">[18]</ref>, etc. Graphs can capture interactions (i.e., edges) between individual units (i.e., nodes) and encode data from irregular or non-Euclidean domains to facilitate representation learning and data analysis. Many tasks, from link prediction <ref type="bibr" target="#b22">[23]</ref>, graph classification <ref type="bibr" target="#b3">[4]</ref> to node classification <ref type="bibr" target="#b27">[28]</ref>, can be naturally per-formed on graphs, where effective node embeddings that can preserve both node information and graph structure are required. To learn from graph-structured data, typically an encoder function is needed to project high-dimensional node features into a low-dimensional embedding space such that "semantically" similar nodes are close to each other in the low-dimensional Euclidean space (e.g., by dot product) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, various Graph Neural Networks (GNNs) have been proposed to learn such embedding functions. Traditional node embedding methods, such as matrix factorization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref> and random walk <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6]</ref>, only rely on adjacent matrix (i.e., graph structure) to encode node similarity. Training in an unsupervised way, these methods employ dot product or co-occur on short random walks over graphs to measure the similarity between a pair of nodes. Similar to word embeddings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>, the learned node embeddings from these methods are simple look-up tables. Other approaches exploit both graph structure and node features in a semi-supervised training procedure for node embeddings <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref>. These methods can be classified into two categories based on how they manipulate the adjacent matrix:</p><p>(1) spectral graph convolution networks, and (2) neighbor aggregation or message passing algorithms. Spectral graph convolution networks transform graphs to the Fourier domain, effectively converting convolutions over the whole graph into element-wise multiplications in the spectral domain. However, once the graph structure changes, the learned embedding functions have to be retrained or finetuned. On the other hand, the neighbor aggregation algorithms treat each node separately and learn feature representation of each node by aggregating (e.g., weighted-sum) over its neighbors' features. Under the assumption that connected nodes should share similar feature representations, these message passing algorithms leverage local feature aggregation to preserve the locality of each node, which is a generalization of classical convolution on images to irregular graph-structured data. For both the categories of GNN algorithms, they can stack k layers on top of each other and aggregate features from k-hop neighbors. Among all the GNN algorithms, the neighbor aggregation algorithms have proved to be more effective and flexible. In particular, Graph Attention Networks (GATs) <ref type="bibr" target="#b23">[24]</ref> use attention mechanism to calculate edge weights at each layer based on node features, and attend adaptively over all neighbors of a node for representation learning. To increase the expressiveness of the model, GATs further employ multi-head attentions to calculate multiple sets of attention coefficients for aggregation. Although multi-head attentions improve prediction accuracies, our analysis of the learned coefficients shows that multi-head attentions usually learn very similar (sometimes almost identical) coefficient distributions. This indicates that there might be some significant redundancy in the GAT modeling. In addition, GATs cannot assign an unique attention score for each edge because multiple attention coefficients are generated (from multi-heads) for an edge per layer and the same edge at different layers might receive different attention coefficients. For example, for a 2-layer GAT with 8-head attentions, each edge receives 16 different attention coefficients. The redundancy in the GAT modeling not only adds significant overhead to computation and memory usage but also increases the risk of overfitting. To mitigate these issues, we propose to simplify the architecture of GATs such that only one single attention coefficient is assigned to each edge across all GNN layers. To further reduce the redundancy among edges, we incorporate a sparsity constraint into the attention mechanism of GATs. Specifically, we optimize the model under an L 0 -norm regularization to encourage model use as fewer edges as possible. As we only employ one attention coefficient for each edge across all GNN layers, what we learn is an edge-sparsified graph with redundant edges removed. As a result, our Sparse Graph Attention Networks (SGATs), as shown in Figure <ref type="figure" target="#fig_0">1</ref>, outperform the original GATs in two aspects: (1) SGATs can identify noisy/insignificant edges of a graph such that a sparsified graph structure can be discovered while preserving a similar representation capability; and (2) SGATs simplify the architecture of GATs, and this reduces the risk of overfitting while achieving similar or sometimes even higher accuracies than the original GATs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>In this section, we first introduce our notation and then review prior works related to the neighbor aggregation methods on graphs. Let G = (V, E) denote a graph with a set of nodes V = {v 1 , • • • , v N }, connected by a set of edges E ⊆ V × V . Node features are organized in a compact matrix X ∈ R N ×D with each row representing the feature vector of one node. Let A ∈ R N ×N denote the adjacent matrix that describes graph structure of G: A ij = 1 if there is an edge e ij from node i to node j, and 0 otherwise. By adding a self-loop to each node, we use Ã = A + I N to denote the adjacency matrix of the augmented graph, where I N ∈ R N ×N is an identity matrix.</p><p>For a semi-supervised node classification task, given a set of labeled nodes {(v i , y i ), i = 1, • • • , n}, where y i is the label of node i and n &lt; N , we learn a function f (X, A, W ), parameterized by W , that takes node features X and graph structure A as inputs and yields a node embedding matrix H ∈ R N ×D for all nodes in V ; subsequently, H is fed to a classifier to predict the class label of each unlabeled node. To learn the model parameter W , we typically minimize an empirical risk over all labeled nodes:</p><formula xml:id="formula_0">R(W ) = 1 n n i=1 L (f i (X, A, W ), y i ) ,<label>(1)</label></formula><p>where f i (X, A, W ) denotes the output of f (X, A, W ) for node i and L(•) is a loss function, such as the cross-entropy loss that measures the compatibility between model predic-tions and class labels. There are many different GNN algorithms that can solve Eq. 1. However, the main difference among these algorithms is how they define the encoder function f (X, A, W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Neighbor Aggregation Methods</head><p>The most effective and flexible graph learning algorithms so far follow a neighbor aggregation mechanism. The basic idea is to learn a parameter-sharing aggregator, which takes as inputs feature vector x i of node i and its neighbors' feature vectors {x j , j ∈ N i } and outputs a new feature vector for node i. Essentially, the aggregator function aggregates lower-level features of a node and its neighbors and generates high-level feature representations. The popular Graph Convolution Networks (GCNs) <ref type="bibr" target="#b10">[11]</ref> fall into the category of neighbor aggregation. For a 2-layer GCN, its encoder function can be expressed as: f (X, A, W ) = softmax Âσ( ÂXW (0) )W (1) , <ref type="bibr" target="#b1">(2)</ref> where Â = D−1/2 Ã D−1/2 , Dii = j Ãij , and W (•) s are the learnable parameters of GCNs. Apparently, GCNs define the aggregation coefficients as the symmetrically normalized adjacency matrix Â, and these coefficients are shared across all GCN layers. More specifically, the aggregator of GCNs can be expressed as</p><formula xml:id="formula_1">h (l+1) i = σ   j∈Ni Âij h (l) j W (l)   ,<label>(3)</label></formula><p>where h</p><p>(l) j is the hidden representation of node j at layer l, h (0) = X, and N i denotes the set of all the neighbors of node i, including itself.</p><p>Since a fixed adjacency matrix Â is used for feature aggregation, GCNs can only be used for the transductive learning tasks, and if the graph structure changes, the whole GCN model needs to be retrained or fine-tuned. To support inductive learning, GraphSage <ref type="bibr" target="#b6">[7]</ref> proposes to learn parameterized aggregators (e.g., mean, max-pooling or LSTM aggregator) that can be used for feature aggregation on unseen nodes or graphs. To support large-scale graph learning tasks, GraphSage uniformly samples a fixed number of neighbors per node and performs computation on a subsampled graph at each iteration. Although it can reduce computational cost and memory usage significantly, its accuracies suffer from uniform sampling and partial neighbor aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Attention Networks</head><p>Recently, attention networks have achieved state-of-theart results in many computer vision and natural language processing tasks, such as image captioning <ref type="bibr" target="#b26">[27]</ref> and machine translation <ref type="bibr" target="#b0">[1]</ref>. By attending over a sequence of inputs, attention mechanism can decide which parts of inputs to look at in order to gather the most useful information. Extending the attention mechanism to graph-structured data, Graph Attention Networks (GATs) <ref type="bibr" target="#b23">[24]</ref> utilize an attentionbased aggregator to generate attention coefficients over all neighbors of a node for feature aggregation. In particular, the aggregator function of GATs is similar to that of GCNs:</p><formula xml:id="formula_2">h (l+1) i = σ   j∈Ni a (l) ij h (l) j W (l)   ,<label>(4)</label></formula><p>except that (1) a (l) ij is the attention coefficient of an edge e ij at layer l, assigned by an attention function other than by a predefined Â, and (2) different layers utilize different attention functions, while GCNs share a predefined Â across all layers.</p><p>To increase the capacity of attention mechanism, GATs further exploit multi-head attentions for feature aggregation: each head works independently to aggregate information, and all the outputs of multi-heads are then concatenated to form a new feature representation for the next layer. In principle, the learned attention coefficient can be viewed as an importance score of an edge. However, since each edge receives multiple attention coefficients at a layer and the same edge at a different layer has a different set of attention coefficients, GATs cannot assign an unique importance score to quantify the significance of an edge. Built on top of GATs, our SGATs introduce a sparse attention mechanism via an L 0 -norm regularization for feature aggregation. Furthermore, we only assign one attention coefficient (or importance score) to each edge across all layers. As a result, we can identify important edges of a graph and remove redundant ones for the purpose of computational efficiency and robust training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse Graph Attention Networks</head><p>The key idea of our Sparse Graph Attention Networks (SGATs) is that we can attach a binary gate to each edge to determine if an edge will be used for neighbor aggregation. We optimize the SGAT model under an L 0 -norm regularized loss function such that we can use as fewer edges as possible to achieve similar or better classification accuracies. We first introduce our sparse attention mechanism, and then describe how the binary gates can be optimized via stochastic binary optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>To identify important edges of a graph and remove noisy/insignificant ones, we attach a binary gate z ij ∈ {0, 1} to each edge e ij ∈ E such that z ij controls if edge e ij will be used for neighbor aggregation 1 . This corresponds to attach a set of binary masks to the adjacent matrix A:</p><formula xml:id="formula_3">Ā = A Z, Z ∈ {0, 1} M , (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>where M is the number of edges in graph G. Since we want to use as fewer edges as possible for semi-supervised node classification, we train model parameters W and binary masks Z by minimizing the following L 0 -norm regularized empirical risk:</p><formula xml:id="formula_5">R(W, Z) = 1 n n i=1 L (f i (X, A Z, W ), y i )+λ Z 0 (6) = 1 n n i=1 L (f i (X, A Z, W ), y i )+λ (i,j)∈E 1 [zij =0] ,</formula><p>where Z 0 denotes the L 0 -norm of binary masks Z, i.e., the number of non-zero elements in Z (edge sparsity), 1 [c] is an indicator function that is 1 if the condition c is satisfied, and 0 otherwise, and λ is a regularization hyperparameter that balances between data loss and edge sparsity. For the encoder function f (X, A Z, W ), we define the following attention-based aggregation function:</p><formula xml:id="formula_6">h (l+1) i = σ   j∈Ni a ij h (l) j W (l)   ,<label>(7)</label></formula><p>where a ij is the attention coefficient assigned to edge e ij across all layers. This is in a stark contrast to GATs, in which a layer-dependent attention coefficient a (l) ij is assigned for each edge e ij at layer l.</p><p>To compute attention coefficients, we simply calculate them by a row-wise normalization of A Z, i.e.,</p><formula xml:id="formula_7">a ij = normalize (A ij z ij ) = A ij z ij k∈Ni A ik z ik .<label>(8)</label></formula><p>As the center node i by default is important to itself, we set z ii to 1 so that it can preserve its own information. Compared to GATs, we don't use softmax to normalize attention coefficients since z ij ∈ {0, 1} by definition and usually</p><formula xml:id="formula_8">A ij ≥ 0 such that their product A ij z ij ≥ 0 .</formula><p>Similar to GATs, we can also use multi-head attentions to increase the capacity of our model. We thus formulate a multi-head SGAT layer as:</p><formula xml:id="formula_9">h (l+1) i = K k=1 σ   j∈Ni a ij h (l) j W (l) k   , (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where K is the number of heads, represents concatenation, a ij is the attention coefficients computed by Eq. 8, 1 Note that edges e ij and e ji are treated as two different edges and therefore have their own binary gates z ij and z ji , respectively. and W (l) k is the weight matrix of head k at layer l. Note that only one set of attention coefficients a ij is calculated for edge e ij , and they are shared among all heads and all layers. With multi-head attention, the final returned output, h (l+1) i , will consist of KD features (rather than D ) for each node.</p><p>Why can we use one set of coefficients for multi-head attention? This is based on our observation that all GAT heads tend to learn attention coefficients with similar distributions, and thus there might be some significant redundancy in the GAT modeling. In addition, using one set of attention coefficients isn't rare at all as GCNs use a shared Â across all layers and are very competitive to GATs in terms of classification accuracies. While GCNs use one set of predefined aggregation coefficients, SGATs learn the coefficients from a sparse attention mechanism. We believe it is the learned attention coefficients instead of multi-set attention coefficients that leads to the improved performance of GATs over GCNs, and the benefit of multi-set attention coefficients might be very limited and could be undermined by the risk of overfitting due to increased complexity. Therefore, the benefits of using one set of attention coefficients over the original multi-set coefficients are at least twofold: (1) one set of coefficients is computationally K times cheaper than multiple sets of coefficients; and (2) one set of coefficients can be interpreted as edge importance scores such that they can be used to identify important edges and remove noisy/insignificant edges for computational efficiency and robust training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MODEL OPTIMIZATION</head><p>Stochastic Variational Optimization To optimize Eq. 6, we need to compute its gradient w.r.t. binary masks Z. However, since Z is a set of binary variables, neither the first term nor the second term is differentiable. Hence, we resort to approximation algorithms to solve this binary optimization problem. Specifically, we approximate Eq. 6 via an inequality from stochastic variational optimization <ref type="bibr" target="#b1">[2]</ref>: Given any function F(z) and any distribution q(z), the following inequality holds:</p><formula xml:id="formula_11">min z F(z) ≤ E z∼q(z) [F(z)],<label>(10)</label></formula><p>i.e., the minimum of a function is upper bounded by its expectation.</p><p>Since z ij ∀(i, j) ∈ E is a binary random variable, we assume z ij is subject to a Bernoulli distribution with parameter π ij ∈ [0, 1], i.e. z ij ∼ Ber (z ij ; π ij ). Thus, we can upper bound Eq. 6 by its expectation:</p><formula xml:id="formula_12">R(W, π) = 1 n n i=1 E q(Z|π) L(f i (X, A Z, W ), y i ) + λ (i,j)∈E π ij .<label>(11)</label></formula><p>Now the second term of Eq. 11 is differentiable w.r.t. the new model parameters π. However, the first term is still problematic since the expectation over a large number of binary random variables Z is intractable, and thus its gradient does not allow for an efficient computation.</p><p>The Hard Concrete Gradient Estimator We therefore need further approximation to estimate the gradient of the first term of Eq. 11 w.r.t. π. Fortunately, this is a well-studied problem in machine learning and statistics with many gradient estimators existing for this discrete latent variable model, such as REINFORCE <ref type="bibr" target="#b25">[26]</ref>, Gumble-Softmax <ref type="bibr" target="#b8">[9]</ref>, REBAR <ref type="bibr" target="#b21">[22]</ref>, RELAX <ref type="bibr" target="#b4">[5]</ref> and the hard concrete estimator <ref type="bibr" target="#b11">[12]</ref>. We choose the hard concrete estimator due to its superior performance in our experiments and relatively straightforward implementation. Specifically, the hard concrete estimator employs a reparameterization trick to approximate the original optimization problem Eq. 11 by a close surrogate function:</p><formula xml:id="formula_13">R(W, log α) = 1 n n i=1 E u∼U (0,1) L f i (X, A g(f (log α, u)), W ), y i + λ (i,j)∈E σ log α ij − β log −γ ζ<label>(12)</label></formula><p>with</p><formula xml:id="formula_14">f (log α, u) = σ((log u−log(1−u)+log α)/β)(ζ −γ)+γ, g(•) = min(1, max(0, •)),</formula><p>where U(0, 1) is a uniform distribution in the range of</p><formula xml:id="formula_15">[0, 1], σ(x) = 1 1+exp(−x)</formula><p>is the sigmoid function, and β = 2/3, γ = −0.1 and ζ = 1.1 are the typical parameter values of the hard concrete distribution. For more details on the hard concrete gradient estimator, we refer the readers to <ref type="bibr" target="#b11">[12]</ref>.</p><p>During training, we optimize log α ij for each edge e ij . At the test phrase, we generate a deterministic mask Ẑ by employing the following equation:</p><formula xml:id="formula_16">Ẑ = min(1, max(0, σ((log α)/β)(ζ − γ) + γ)),<label>(13)</label></formula><p>which is the expectation of Z under the hard concrete distribution q(Z| log α). Due to the hard concrete approximation, ẑij is now a continuous value in the range of [0, 1].</p><p>Ideally, the majority elements of Ẑ will be zeros, and thus many edges can be removed from the graph.</p><p>Inductive Model of log α The learning of binary masks Z discussed above is transductive, by which we can learn a binary mask z ij for each edge e ij in the training graph G. However, this approach cannot generate new masks for edges that are not in the training graph. A more desirable approach is inductive that can be used to generate new masks for new edges. This inductive model of log α can be implemented as a generator, which takes feature vectors of a pair of nodes as input and produce a binary mask as output. We model this generator simply as</p><formula xml:id="formula_17">log α ij = b T (x i W (0) 0 x j W (0) 0 ),<label>(14)</label></formula><p>where b ∈ R D is the parameter of the generator and W (0) 0</p><p>is the weight matrix of head 0 at layer 0. To integrate this generator into an end-to-end training pipeline, we define this generator to output log α ij . Upon receiving log α ij from the mask generator, we can sample a mask ẑij from the hard concrete distribution q(z| log α ij ). The set of sampled mask Ẑ is then used to generate an edge-sparsified graph for the downstream applications. The full pipeline of SGATs is shown in Figure <ref type="figure" target="#fig_0">1</ref>. In our experiments, we use this inductive SGAT pipeline for semi-supervised node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>To demonstrate SGAT's ability of identifying important edges for feature aggregation, we conduct a series of experiments on synthetic and real-world semi-supervised node classification benchmarks, including transductive learning tasks and inductive learning tasks. We compare our SGATs with the state-of-the-art GNN algorithms: GCNs <ref type="bibr" target="#b10">[11]</ref>, GraphSage <ref type="bibr" target="#b6">[7]</ref> and GATs <ref type="bibr" target="#b23">[24]</ref>. For a fair comparison, our experiments closely follow the configurations of the competing algorithms. We plan to release our code to public to facilitate the research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DATASETS</head><p>We evaluate our algorithm on seven established semisupervised node classification benchmarks, whose statistics are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Transductive learning tasks Three citation network datasets: Cora, Citeseer and Pubmed <ref type="bibr" target="#b17">[18]</ref> and two copurchase graph datasets: Amazon Computers and Amazon Photo <ref type="bibr" target="#b18">[19]</ref> are used to evaluate the performance of our algorithm in the transductive learning setting, where test graphs are included in training graphs for feature aggregation and thus facilitates the learning of feature representations of test nodes for classification. The citation networks have low degrees (e.g., only 1-2 edges per node), while the co-purchase datasets have higher degrees (e.g., 15-18 edges per node) such that we can demonstrate SGAT's performance on sparse graphs and dense graphs. For the citation networks, nodes represent documents, edges denote Inductive learning tasks Two large-scale graph datasets: PPI <ref type="bibr" target="#b29">[30]</ref> and Reddit <ref type="bibr" target="#b6">[7]</ref> are used to evaluate the performance of SGATs in the inductive learning setting, where test graphs are excluded from training graphs for model training and the feature representations of test nodes have to be generated from trained aggregators for classification. In this case, our inductive experiments closely follow the settings of GraphSage <ref type="bibr" target="#b6">[7]</ref>. The protein-protein interaction (PPI) dataset consists of graphs corresponding to different human tissues. Positional gene sets, motif gene sets and immunological signatures are extracted as node features and 121 gene ontology categories are used as class labels.</p><p>There are in total 24 subgraphs in the PPI dataset with each subgraph containing 3k nodes and 100k edges on average. Among 24 subgraphs, 20 of them are used for training, 2 for validation and the rest of 2 for test. For the Reddit dataset, each node represents a reddit post and two nodes are connected when the same user comments on both posts. The node features are made up with the embedding of the post title, the average embedding of all the post's comments, the post's score and the number of comments made on the post. There are 41 different communities in the Reddit dataset corresponding to 41 categories.The task is to predict which community a post belongs to. This is a large-scale graph learning benchmark that contains over 100 million edges and about 250 edges per node, and therefore a high edge redundancy is expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MODLES AND EXPERIMENTAL SETUP</head><p>Models A 2-layer SGAT with a 2-head attention at each layer is used for feature aggregation, followed by a softmax classifier for node classification. We use ReLU <ref type="bibr" target="#b13">[14]</ref> as the activation function and optimize the models with the Adam optimizer <ref type="bibr" target="#b9">[10]</ref> with the learning rate of lr = 1e − 2. We compare SGATs with GCNs and GATs in terms of node classification accuracies. Since SGATs produce edgesparsified graphs, we also report edge redundancy of the sparsified graph, i.e., the percentage of edges removed from the original graph. We implemented our SGATs with the DGL library <ref type="bibr" target="#b24">[25]</ref>. Hyperparameters We tune the performance of SGATs based on the hyperparameters of GATs since SGATs are built on top of GATs. For a fair comparison, we also run 1-head and 2-head GAT models with the same architecture as SGATs to illustrate the impact of sparse attention models vs standard dense attention models. To prevent models from overfitting on small datasets, L 2 regularization and dropout <ref type="bibr" target="#b19">[20]</ref> are used. Dropout is applied to the inputs of all layers and the attention coefficients. For the large-scale datasets, such as PPI and Reddit, we don't use L 2 regularization or dropout as the models have enough data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Synthetic Dataset</head><p>To illustrate the idea of SGAT, we first demonstrate it on a synthetic dataset -Zachary's Karate Club <ref type="bibr" target="#b28">[29]</ref>, which is a social network of a karate club of 34 members with links between pairs of members representing who interacted outside the club. The club was split into two groups later due to a conflict between the instructor and the administrator. The goal is to predict the groups that all members of the club joined after the split. This is a semi-supervised node classification problem in the sense that only two nodes: the instructor (node 0) and the administrator (node 33) are labeled and we need to predict the labels of all the other nodes.</p><p>We train a 2-layer SGAT with a 2-head attention at each layer on the dataset. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the evolution of the graph at different training epochs, the corresponding classification accuracies and number of edges kept in the graph. As can be seen, as the training proceeds, some insignificant edges are removed and the graph is getting sparser; at the end of training, SGAT removes about 46% edges while retaining an accuracy of 96.88% (i.e., only one node is misclassified), which is the same accuracy achieved by GCNs and other competing algorithms that utilize the full graph for prediction. In addition, the removed edges have an intuitive explanation. For example, the edge from node 16 to node 6 is removed while the reversed edge is kept. Apparently, this is because node 6 has 4 neighbors while node 16 has only 2 neighbors, and thus removing one edge between them doesn't affect node 6 too much while may be catastrophic to node 16. Similarly, the edges between node 27, 28 and node 2 are removed. This might be because node 2 has an edge to node 0 and has no edge to node 33, and therefore node 2 is more like to join node 0's group and the edges to nodes 27 and 28 are not important or might be due to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Seven Benchmarks</head><p>We evaluate the performance of SGATs on seven semisupervised node classification benchmarks with the results summarized in Table <ref type="table" target="#tab_1">2</ref>. For a fair comparison, we run each experiments 20 times with different random weight initializations and report the mean accuracies.</p><p>Comparing SGAT with GCN, we note that SGAT outperforms GCN on the PPI dataset significantly while being similar on all the other six benchmarks. Comparing SGAT with GraphSage, SGAT again outperforms GraphSage on PPI by a significant margin. Comparing SGAT with GAT, we note that they achieve very competitive accuracies on all six benchmarks except Reddit, where the original GAT is "out of memory" and SGAT can perform successfully due to its simplified architecture and about 80% edge reduction. Another advantage of SGAT over GAT is the regularization effect of the L 0 -norm on the edges. To demonstrate this, we test two GAT variants : GAT-1head and GAT-2head that have the similar architectures as SGAT-1head and SGAT-2head but with different attention mechanisms (i.e., standard dense attention vs. sparse attention). As we can see, on the Reddit dataset, the sparse attention-based SGATs outperform GATs by 2-3% while sparsifying the graph by about 80%. Overall, SGAT is very competitive against GCN, Graph-Sage and GAT in terms of classification accuracies, while being able to remove different percentages of edges from small and large graphs. More specifically, on the three small citation networks SGATs learn that majority of the edges are critical to maintain competitive accuracies as the original graphs are already very sparse, and therefore SGATs remove only 1-2% edges. On the other hands, on the rest of large or dense graphs, SGATs identify significant amounts of redundancies in edges (e.g., 40-80%), and removing them incurs no or minor accuracy losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Edge Redundancy Analysis</head><p>Lastly, we analyze the edge redundancy identified by SGATs. Figures <ref type="figure" target="#fig_3">3 and 4</ref> illustrate the evolution of num-  ber of edges used by SGATs in Cora and PPI during training and test. As we can see, SGAT removes 2% edges from Cora slowly during training epochs, while it removes 49.3% edges from PPI dramatically, indicating a significant edge redundancy in the PPI benchmark.</p><p>To demonstrate SGAT's accuracy of identifying important edges from a graph, Figure <ref type="figure" target="#fig_4">5</ref> shows the evolution of classification accuracies on the PPI test dataset when different percentages of edges are removed from the graph. We compare three different strategies of selecting edges for removal: (1) top-k% edges sorted descending by log α ij , (2) bottom-k% edges sorted descending by log α ij , and (3) uniformly random k%. As we can see, SGAT identifies important edges accurately as removing them from the graph incurs a dramatically accuracy loss as compared to random edge removal or bottom-k% edge removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">CONCLUSION</head><p>We propose sparse graph attention networks (SGATs) that incorporate a sparse attention mechanism into graph attention networks (GATs) via an L 0 -norm regularization on the number of edges of a graph. To assign a single attention coefficient to each edge, SGATs further simplify the architecture of GATs by sharing one set of attention coefficients across all heads and all layers. This reduces the risk of overfitting, and leads to a more efficient graphlearning algorithm that achieves very competitive accuracies while being able to identify important edges and sparsify large graphs dramatically (e.g., 50-80%). To the best of our knowledge, this is the first graph learning algorithm that sparsifies graphs for the purpose of robust training.</p><p>As for future extensions, we plan to investigate more efficient gradient estimators for stochastic binary optimization to improve the effectiveness of SGATs. We also plan to extend the framework to learn graph structure from data directly when graph structure isn't available in the first place.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The overview of SGATs. By attaching a binary mask to each edge, SGATs utilize a sparse attention mechanism (as the output of the mask generator) to guide model to remove noisy/insignificant edges and yield an edge-sparsified graph. In the plot above, the dashed lines denote removed edges from the graph. More details are described in Sec. 3.</figDesc><graphic url="image-1.png" coords="2,150.18,77.14,121.71,133.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The evolution of the graph of Zachary's Karate Club at different training epochs. SGAT can remove 46% edges from the graph while retaining almost the same accuracy at 96.88%. Nodes 0 and 33 are the labeled nodes, and the colors show the ground-truth labels. The video can be found at https://youtu.be/3Jhr26lXRl8.</figDesc><graphic url="image-9.png" coords="7,50.11,71.38,495.00,124.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The evolution of (top) classification accuracy and (bottom) number of nonzero attention coefficients as a function of training epochs on test subgraphs of the PPI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The evolution of number of edges used for neighbor aggregation as a function of training epochs (top) on the Cora training graph and (b) on the PPI training graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The evolution of classification accuracies on the PPI test dataset when different percentages of edges are removed from the graph. Three different strategies of selecting edges for removal are considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of the datasets used in the experiments For the co-purchase datasets, nodes represent products, edges indicate that two products are frequently purchased together, and node features are the bag-of-words representations of product reviews; similarly, the goal is to classify products into different product categories. Our experiments closely follow the transductive learning setups of<ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19]</ref>. For all these datasets, 20 nodes per class are used for training, 500 nodes are used for validation, and 1000 nodes are used for test.</figDesc><table><row><cell></cell><cell>Tasks</cell><cell>Nodes</cell><cell cols="4">Edges Features Classes Average neighbor size</cell></row><row><cell>Cora</cell><cell>transductive</cell><cell>2,708</cell><cell>13,264</cell><cell>1,433</cell><cell>7</cell><cell>2.0</cell></row><row><cell>Citeseer</cell><cell>transductive</cell><cell>3,327</cell><cell>12,431</cell><cell>3,703</cell><cell>6</cell><cell>1.4</cell></row><row><cell>Pubmed</cell><cell>transductive</cell><cell>19,717</cell><cell>108,365</cell><cell>500</cell><cell>3</cell><cell>2.3</cell></row><row><cell cols="2">Amazon computers transductive</cell><cell>13,381</cell><cell>505,474</cell><cell>767</cell><cell>10</cell><cell>18.4</cell></row><row><cell>Amazon photo</cell><cell>transductive</cell><cell>7,487</cell><cell>245,812</cell><cell>745</cell><cell>8</cell><cell>15.9</cell></row><row><cell>PPI</cell><cell>inductive</cell><cell>56,944</cell><cell>818,716</cell><cell>50</cell><cell>121</cell><cell>6.7</cell></row><row><cell>Reddit</cell><cell>inductive</cell><cell cols="2">232,965 114,848,857</cell><cell>602</cell><cell>41</cell><cell>246.0</cell></row><row><cell cols="3">citation relationship between two documents, and node fea-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tures are the bag-of-words representations of document con-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tents; the goal is to classify documents into different cate-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gories.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracies on seven semi-supervised node classification benchmarks. Results of GCNs on PPI and Reddit are trained in a transductive way. The results annotated with * are from our experiments, and the rest of results are from the corresponding papers. OOM indicates "out of memory".</figDesc><table><row><cell></cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell><cell>Amazon computer</cell><cell>Amazon Photo</cell><cell>PPI</cell><cell>Reddit</cell></row><row><cell>GCN</cell><cell>81.5%</cell><cell>70.3%</cell><cell>79.0%</cell><cell>81.5%</cell><cell>91.2%</cell><cell cols="2">50.9%  *  94.38%  *</cell></row><row><cell>GAT</cell><cell>84.0%</cell><cell>72.5 %</cell><cell>79.0%</cell><cell>-</cell><cell>-</cell><cell>97.3%</cell><cell>OOM</cell></row><row><cell>GAT-1head  *</cell><cell>83.3 %</cell><cell>66.8%</cell><cell>77.1%</cell><cell>81.3%</cell><cell>89.7%</cell><cell>85.6%</cell><cell>92.6%</cell></row><row><cell>GAT-2head  *</cell><cell>83.8%</cell><cell>67.5%</cell><cell>77.4%</cell><cell>82.4%</cell><cell>90.4%</cell><cell>97.1%</cell><cell>93.5%</cell></row><row><cell>GraphSage</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.2%</cell><cell>95.4%</cell></row><row><cell>SGAT-1head  *</cell><cell>83.1%</cell><cell>67.4%</cell><cell>77.2%</cell><cell>81.1%</cell><cell>89.5%</cell><cell>86.0%</cell><cell>94.9%</cell></row><row><cell>SGAT-2head  *</cell><cell>84.2%</cell><cell>68.2 %</cell><cell>77.6%</cell><cell>81.8%</cell><cell>89.9%</cell><cell>96.6%</cell><cell>95.2%</cell></row><row><cell>Edge Redundancy  *</cell><cell>2.0%</cell><cell>1.2%</cell><cell>2.2%</cell><cell>63.6%</cell><cell>42.3%</cell><cell>49.3%</cell><cell>80.8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* From our experiments.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04855</idno>
		<title level="m">Stochastic variational optimization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International on Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Backpropagation through the void: Optimizing control variates for black-box gradient estimation</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dami</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2006">2017. 1, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through l 0 regularization</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data mining (KDD)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Galileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gunnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Relational Representation Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rebar: Lowvariance, unbiased gradient estimates for discrete latent variable models</title>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1992-05">May 1992. 5</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups</title>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Anthropological Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
