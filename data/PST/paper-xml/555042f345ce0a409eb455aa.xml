<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Quality Depth Map Upsampling and Completion for RGB-D Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
							<email>jspark@rcv.kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
							<email>brown@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<email>iskweon77@kaist.ac.kr</email>
						</author>
						<author>
							<persName><roleName>Dr</roleName><forename type="first">Farhan</forename><forename type="middle">A J</forename><surname>Baqai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y.-W</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName><surname>Kweon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">Advanced Institute of Science and Technology</orgName>
								<address>
									<postCode>305-701</postCode>
									<settlement>Daejeon</settlement>
									<country>Korea, Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119077</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Toshiba Research and Development Center</orgName>
								<address>
									<settlement>Kawasaki</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Science and Technology</orgName>
								<address>
									<postCode>1992</postCode>
									<settlement>Daejeon</settlement>
									<country>the Korea Advanced, Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High-Quality Depth Map Upsampling and Completion for RGB-D Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D1842C1F43719C4C359056425368B90C</idno>
					<idno type="DOI">10.1109/TIP.2014.2361034</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depth map upsampling</term>
					<term>depth map completion</term>
					<term>RGB-D cameras</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes an application framework to perform high-quality upsampling and completion on noisy depth maps. Our framework targets a complementary system setup, which consists of a depth camera coupled with an RGB camera. Inspired by a recent work that uses a nonlocal structure regularization, we regularize depth maps in order to maintain fine details and structures. We extend this regularization by combining the additional high-resolution RGB input when upsampling a lowresolution depth map together with a weighting scheme that favors structure details. Our technique is also able to repair large holes in a depth map with consideration of structures and discontinuities utilizing edge information from the RGB input. Quantitative and qualitative results show that our method outperforms existing approaches for depth map upsampling and completion. We describe the complete process for this system, including device calibration, scene warping for input alignment, and even how our framework can be extended for video depthmap completion with the consideration of temporal coherence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>captured by a dedicated sensor. Depending on the distance of objects in a scene, the captured light wave is delayed in phase compared to the original emitted light wave. By measuring the phase delay the distance at each pixel can be estimated. Active pattern cameras emit a pre-defined pattern into a scene and use a camera to observe the deformation and translation of the pattern to determine depth map. This can be done using an infrared projector and camera to avoid human detection and interference with other visible spectrum imaging devices.</p><p>The quality of the depth maps captured by these 3D cameras, however, are relatively low. In the 3D-ToF cameras, the resolution of the captured depth maps are typically less than 1/4th the resolution of a standard definition video camera. In addition, the captured depth maps are often corrupted by significant amounts of noise. In active pattern cameras, although the captured depth map has a comparable resolution to the resolution of RGB cameras, the depth map usually contains many holes due to the disparity between the IR projector and the IR camera. In addition, the captured depth map contains quantization errors and holes due to the estimation errors in pattern matching and the presence of bright light sources and non-reflective objects in a scene.</p><p>The goal of this paper is to propose a method to estimate a high quality depth map from the depth sensors through upsampling and completion. To aid this procedure, an auxiliary high-resolution conventional camera is coupled with the depth camera to synchronously capture the scene. Related work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[29]</ref> also using coupled device setups for depth map upsampling have focused primarily on image filtering techniques such as joint bilateral filtering <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref> or variations. Such filtering techniques can often over-smooth results, especially in areas of structure details.</p><p>We formulate the depth map upsampling and completion problem using constrained optimization. Our approach is inspired by the recent success of nonlocal structure regularization for depth map construction from depth-from-defocus <ref type="bibr" target="#b7">[8]</ref>. In particular, we describe how to formulate the problem into a least-squares optimization that combines nonlocal structure regularization together with an edge weighting scheme that further reenforces fine details. We also employ scene warping to better align the imagery to the auxiliary camera input. While our methodology is heuristic in nature, the result is a system that is able to produce high-quality depth maps superior in quality to prior work. In addition, our approach can be easily extended to incorporate simple user markup to correct errors along discontinuity boundaries without explicit image segmentation (e.g. Fig. <ref type="figure" target="#fig_0">1</ref>). (a) Low-resolution depth map (enlarged using nearest neighbor upsampling), (b) high-resolution RGB image, (c) result from <ref type="bibr" target="#b28">[29]</ref>, (d) our result. User scribble areas (blue) and the additional depth sample (red) are highlighted. The dark areas in (c) are the areas without depth samples after registration. Full resolution comparisons are provided in the supplemental materials.</p><p>A shorter version of this work appeared in <ref type="bibr" target="#b21">[22]</ref>, which focuses on the depth map upsampling problem for 3D-ToF cameras. This paper shows that the framework is also applicable in depth map completion for active pattern cameras (e.g. Microsoft Kinect) with additional implementation details, discussions and experiments for depth map completion. A new technical section is included that extends our framework to depth video completion. Our new experimental results demonstrate high quality temporarily coherent depth video, which out-performs our single frame approach in <ref type="bibr" target="#b21">[22]</ref> for the depth video completion problem.</p><p>The remainder of our paper is organized as follow: In Sec. II, we review related works in depth map upsampling and completion. Our system setup and preprocessing steps are presented in Sec. III, followed by the optimization framework in Sec. IV. In Sec. V, we present our experimental results. Finally, we conclude our work in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Previous work on depth map upsampling and completion can be classified as either image fusion techniques that combine the depth map with the high quality RGB image or super-resolution techniques that merge multiple misaligned low quality depth maps. Our approach falls into the first category of image fusion which is the focus of the related work presented here.</p><p>Image fusion approaches assume there exists a joint occurrence between depth discontinuities and image edges and those regions of homogenous color have similar 3D geometry <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Representative image fusion approaches include <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, Diebel and Thrun performed upsampling using an MRF formulation with the data term computed from the depth map and weights of the smoothness terms between estimated high-resolution depth samples derived from the high-resolution image. Yang et al. <ref type="bibr" target="#b28">[29]</ref> used joint bilateral filtering <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref> to fill the hole and interpolate the high-resolution depth values. Since filtering can often over-smooth the interpolated depth values, especially along the depth discontinuity boundaries, they quantized the depth values into several discrete layers. Joint bilateral filtering was applied at each layer with a final post processing step to smoothly fuse the discrete layers. This work was later extended by <ref type="bibr" target="#b29">[30]</ref> to use a stereo camera for better discontinuity detection in order to avoid over-smoothing of depth boundaries. Chan et al. <ref type="bibr" target="#b2">[3]</ref> introduced a noise-aware bilateral filter that decides how to blend between the results of standard upsampling or joint bilateral filtering depending on the depth map's regional statistics. Dolson et al. <ref type="bibr" target="#b5">[6]</ref> also used a joint bilateral filter scheme, however, their approach includes additional time stamp information to maintain temporal coherence for the depth map upsampling in video sequences.</p><p>The advantage of these bilateral filtering techniques is that they can be performed quickly; e.g. Chan et al. <ref type="bibr" target="#b2">[3]</ref> reported near real-time speeds using a GPU implementation. However, the downside is that they can still over-smooth fine details. Our approach is more related to <ref type="bibr" target="#b4">[5]</ref> because we formulate the problem using a constrained optimization scheme. However, our approach incorporates a nonlocal structure (NLS) term to help preserve local structures. This additional NLS term was inspired by Favaro <ref type="bibr" target="#b7">[8]</ref>, which demonstrated that NLS filtering is useful in maintaining fine details even with noisy input data. We also include an additional weighting scheme based on derived image features to further reinforce the preservation of fine detail. In addition, we perform a warping step to better align the low-resolution and high-resolution input. Huhle et al. <ref type="bibr" target="#b10">[11]</ref> proposes NLS based filtering algorithm targeting a sparse 3D point cloud. However, data alignment and outlier rejection are not needed in this approach. It also only considers the NLS term, while our approach uses several additional terms. Our experimental results on ground truth data shows that our application framework can outperform existing techniques for the majority of scenes with various upsampling factors. Since our goal is high-quality depth maps, the need for manual cleanup for machine vision related input is often unavoidable. Another advantage of our approach is that it can easily incorporate user markup to improve the results.</p><p>Recently, the Kinect sensor has been widely used in the research community. The Kinect's depth accuracy and mechanism are extensively analyzed in <ref type="bibr" target="#b15">[16]</ref>. In <ref type="bibr" target="#b11">[12]</ref>, Izadi et al. proposed KinectFusion for static scene modeling. This approach aligns multi-view depth maps into pre-allocated volumetric representation by using a signed-distance function. It is adequate for modeling objects within a few cubic meters <ref type="bibr" target="#b19">[20]</ref>. Also, this method can be applied to depth map refinement since it averages noisy depth maps from multiple view points. Compared to Izadi et al. <ref type="bibr" target="#b11">[12]</ref>, we focus our work as a depth map upsampling and completion for a single RGB-D image configuration. While our configuration is more restrictive, several research groups have utilized Kinect to build RGB-D datasets <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b23">[24]</ref> under various scenes and objects for scene categorization, object recognition and segmentation. Since the Kinect depth map tends to have holes, it is necessary to fill in these missing depth values before they can be used for recognition and segmentation tasks. In this paper, using the same optimization framework for depth map upsampling, we demonstrate our approach can achieve high quality depth map completion for these Kinect RGB-D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM SETUP AND PREPROCESSING</head><p>In this section, we describe our RGB-D imaging step and the preprocessing step to register the depth camera and conventional RGB camera and to perform initial outlier rejection on the depth input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Configuration</head><p>Figure <ref type="figure" target="#fig_1">2</ref> (a) shows our hardware configuration consisting of a 3D-ToF camera and a high-resolution RGB camera. For the ToF camera, we use a SwissRanger T M SR4000 <ref type="bibr" target="#b0">[1]</ref> which captures a 176 × 144 depth map. For the RGB camera, we use the Point Grey Research Flea RGB camera with a resolution of 1280 ×960 pixels. Figure <ref type="figure" target="#fig_1">2 (c)</ref> shows the Microsoft T M Kinect depth range sensor. The Microsoft Kinect is equipped with an IR pattern projector, an IR camera for depth measurement, and an RGB camera. The resolution of the captured depth map depends on the resolution of the IR camera. The default resolution of the Kinect is 640 × 480 and it is capable to capture depth map and RGB image up to 1280 × 1024 (at a low frame rate) resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Camera Calibration</head><p>Since the RGB-D data captured from the system in Sec. III-A have slightly different viewpoints (both for 3D-ToF setting and Kinect), we need to register the camera  according to the depth values from the low-resolution depth map. This process requires intrinsic and extrinsic camera parameters.</p><p>In this work, we utilize prior art for the heterogeneous camera rig calibration. For 3D-ToF camera setting, we use the method proposed by Jung et al. <ref type="bibr" target="#b13">[14]</ref>. This method uses a planar calibration pattern consisting of holes (Figure <ref type="figure" target="#fig_1">2 (b)</ref>). This unique calibration pattern allows us to detect the positions on the planar surface that are observed by the 3D-ToF camera. In the case of the Kinect, we utilize the Kinect calibration Toolbox <ref type="bibr" target="#b9">[10]</ref> which simultaneously estimates the camera parameters and fixed pattern noise of the depth map. In this case, a conventional checkerboard is utilized.</p><p>The captured depth map does not response linearly to the actual depth in the real world. To calibrate the response of the depth camera, we capture a scene with a planar object and move the planar object towards the camera. The actual movement of the planar object is recorded and is used to fit a depth response curve to linearize the depth map before warping the depth map to align with the RGB image.</p><p>Besides our proposed calibration method, Pandey et al. <ref type="bibr" target="#b20">[21]</ref> introduces an automatic extrinsic calibration method for RGB-D cameras. They optimize 6D extrinsic parameters by maximizing mutual information. The mutual information is (d) Edge saliency map. (e) Guided depth map by using bicubic interpolation of (a). (f) Our upsampling result without the guided depth map weighting, depth bleeding occurred in highly textured regions. (g) Our upsampling result with guided depth map weighting. (h) Ground truth. We subsampled the depth value of a dataset from Middlebury to create the synthetic low-resolution depth map. The magnification factor in this example is 5×. The sum of squared difference (SSD) between (f) and (g) comparing to the ground truth are 31.66 and 24.62 respectively. Note that the depth bleeding problem in highly textured regions has been improved. evaluated by measuring the correlation between surface reflectivity from a range scanner (or known as intensity image in ToF camera case) and intensity values from an RGB camera in their joint histogram. Talyor and Nieto <ref type="bibr" target="#b24">[25]</ref> introduced a method that also takes the intrinsic calibration parameters into account. They use normalized mutual information to avoid drift and apply a particle swam optimization <ref type="bibr" target="#b14">[15]</ref> for robust estimation. Compared to the automatic methods, our procedure requires a calibration pattern as shown in Fig. <ref type="figure" target="#fig_1">2 (b)</ref> and<ref type="figure">(d)</ref>. The usage of the calibration pattern allows us to achieve higher accuracy in calibration. However, in the case when precalibration is impossible, the automatic approaches can be a good substitution for our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth Map Registration</head><p>With the estimated camera parameters, we first undistort the depth map and RGB image. After that, the linearized depth map is back-projected as a point cloud and points are projected into the RGB camera coordinate. Numerically, for any point, x t = (u, v) , on the low-resolution depth map with depth value d t , we can compute its corresponding position in the high-resolution RGB image by the following equation:</p><formula xml:id="formula_0">sx c = K c R | t K d -1 [x t d t ]<label>(1)</label></formula><p>where K c and K d are the intrinsic parameters of the RGB and depth camera respectively, and R and t are the rotation and translation matrices which describe the rotation and translation of the the RGB camera and the depth camera with respect to the 3D world coordinate. We obtain the scaling term s by calculating the relative resolution between the depth camera and the RGB camera. Since the depth map from the depth camera is noisy, we impose a neighborhood smoothness regularization using the thin-plate spline to map the low-resolution depth map to the high-resolution image. The thin-plate spline models the mapping of 3D points by minimizing the following equation:</p><formula xml:id="formula_1">arg min α j ||x j - i α i R(x j -x i )|| 2<label>(2)</label></formula><p>where α = {α i } is a set of mapping coefficients that defines the warping of 3D space, R(r ) = r 2 log r is the radial basis kernel of the thin-plate spline, and {i, j } are index of correspondents</p><formula xml:id="formula_2">{x i = [sx c d] , x i = [x t d] }</formula><p>given by Eq. ( <ref type="formula" target="#formula_0">1</ref>). Thus, for any 3D point, x, the new location after warping is given by:</p><formula xml:id="formula_3">i α i R(x -x i ) (3)</formula><p>With the thin-plate spline regularization, the entire point cloud is warped as a 3D volume. This can effectively suppress individual mapping errors caused by noisy estimation of depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Outliers Detection</head><p>Since there is disparity between the depth camera and the RGB camera, occlusion/disocclusion occurs along depth discontinuities. In addition, we found that the depth map from the 3D-ToF camera contains depth edges that are blurred by mixing the depth values of two different depth layers along depth boundaries. <ref type="foot" target="#foot_0">1</ref> These blurred depth boundaries are unreliable and should be removed before our optimization framework.</p><p>For each depth sample in the depth image, we reproject its location to the RGB image using the method described in Sec. III-C. The reprojected depth map allows us to determine occluded and disoccluded regions. We expand the detected occluded and disoccluded regions such that the unreliable depth samples along depth discontinuities are rejected. To demonstrate the idea, we show a synthetic example in Fig. <ref type="figure" target="#fig_2">3</ref>. Fig. <ref type="figure" target="#fig_2">3 (a)</ref> shows the depth map of the synthetic scene and Fig. <ref type="figure" target="#fig_2">3 (b)</ref> show the reprojected depth map. The occluded regions and disoccluded regions are detected and shown in Fig. <ref type="figure" target="#fig_2">3 (c</ref>) and Fig. <ref type="figure" target="#fig_2">3 (d</ref>) respectively. Since our optimization method handles both depth upsampling and depth completion simultaneously in the same framework, the rejected depth samples along depth boundaries will not degrade the performance of upsampling. Instead, we found that our framework performs worse if the outliers were not rejected before optimization framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OPTIMIZATION FRAMEWORK</head><p>This section describes our optimization framework for upsampling the low-resolution depth map given the aligned sparse depth samples and the high-resolution RGB image. Note that this framework is also applied to depth map completion. Similar to the previous image fusion approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b28">[29]</ref>, we assume the co-occurrences of depth boundaries and image boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Objective Function</head><p>We define the objective function for depth map upsampling and completion as follows:</p><formula xml:id="formula_4">E(D) = E d (D) + λ s E s (D) + λ N E NLS (D) (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where E d (D) is the data term, E s (D) is the neighborhood smoothness term, and E NLS (D) is a NLS regularization. The term λ s and λ N are the relative weights to balance the three terms. Note that the smoothness term and NLS terms could be combined into a single term, however, we keep them separate here for sake of clarity. Our data term is defined according to the initial sparse depth map:</p><formula xml:id="formula_6">E d (D) = p∈G (D( p) -G( p)) 2 , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where G is a set of pixels, which has the initial depth value, G( p). Our smoothness term is defined as:  where N ( p) is the first order neighborhood of p, w pq is the confidence weighting that will be detailed in the following section, and W p = q w pq is a normalization factor. Combining Eq. ( <ref type="formula" target="#formula_6">5</ref>) and Eq. ( <ref type="formula" target="#formula_8">6</ref>) forms a quadratic objective function which is similar to the objective function in <ref type="bibr" target="#b17">[18]</ref>. The work in <ref type="bibr" target="#b17">[18]</ref> was designed to propagate sparse color values to a gray high-resolution image that is similar in nature to our problem of propagating sparse depth values to the highresolution RGB image.</p><formula xml:id="formula_8">E s (D) = p q∈N ( p) w pq W p (D( p) -D(q)) 2 , (<label>6)</label></formula><p>The difference between our method and that of <ref type="bibr" target="#b17">[18]</ref> is the definition of w pq . Work in <ref type="bibr" target="#b17">[18]</ref> defined w pq using intensity difference between the first order neighborhood pixels to preserve discontinuities. We further combine segmentation, color information, and edge saliency as well as the bicubic upsampled depth map to define w pq . The reason for this is that we find the first order neighborhood does not properly consider the image structure. As the result, propagated color information in <ref type="bibr" target="#b17">[18]</ref> was often prone to bleeding errors near fine detail. In addition, we include a NLS regularization term that helps to preserve thin structures by allowing the pixels on the same nonlocal structure to reinforce with each other within a larger neighborhood. We define the NLS regularization term using an anisotropic structural-aware filter <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_9">E NLS (D) = p r∈A( p) κ pr K p (D( p) -D(r )) 2 , (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where A( p) is a local window (e.g. 11 × 11) in the high-resolution image, κ pr is the weight of the anisotropic structural-aware filter, and K p = r κ pr is normalization constant. κ pr is defined as:</p><formula xml:id="formula_11">κ pr = 1 2 exp(-( p -r ) -1 p ( p -r )) + exp(-( p -r ) -1 r ( p -r )) , p = 1 |A| p ∈A( p) ∇ I ( p )∇ I ( p ) . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Here, ∇ I ( p) = {∇ x I ( p), ∇ y I ( p)} is the x-and y-image gradient vector at p, and I is the high-resolution color image. The term q is defined similarly to p . This anisotropic structural-aware filter defines how likely p and q are on the same structure in the high-resolution RGB image, i.e. if p and r are on the same structure, κ pr will be large. This NLS filter essential allows similar pixel to reinforce each other even if they are not first-order neighbors. To maintain the sparsity of the linear system, we remove neighborhood entries with κ pr &lt; t. A comparison of our approach to illustrate the effectiveness of the NLS regularization is shown in Fig. <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Confidence Weighting</head><p>In the section, we describe our confidence weighting scheme for defining the weights w pq in Eq. ( <ref type="formula" target="#formula_8">6</ref>). The value of w pq defines the spatial coherence of neighborhood pixels. The larger w pq is, the more likely that the two neighborhood pixels having the same depth value. Our confidence weighting is decomposed into four terms based on color similarities (w c ), segmentation (w s ), edge saliency (w e ), and guided bicubic interpolated depth map (w d ).</p><p>The color similarity term is defined in the YUV color space as follows:</p><formula xml:id="formula_13">w c = exp - I ∈Y U V (I( p) -I(q)) 2 2σ 2 I ,<label>(9)</label></formula><p>where σ I controls the relative sensitivity of the different color channels.</p><p>Our second term is defined based on color segmentation using the library provided in <ref type="bibr" target="#b26">[27]</ref> to segment an image into super pixels as shown in Fig. <ref type="figure" target="#fig_4">5(c</ref>). For the neighborhood pixels that are not within the same super pixel, we give a penalty term defined as:</p><formula xml:id="formula_14">w s = 1 if S co ( p) = S co (q) t se otherwise,<label>(10)</label></formula><p>where S co (•) is the segmentation label, t se is the penalty factor with its value between 0 and 1. In our implementation, we empirically set it equals to 0.7.</p><p>Inspired by <ref type="bibr" target="#b1">[2]</ref>, we have also included a weight that depends on the edge saliency response. Different from the color similarity term, the edge saliency responses are detected by a set of Gabor filters with different sizes and orientations. <ref type="foot" target="#foot_1">2</ref> The edge saliency map contains image structures rather than just color differences between neighborhood pixels. We combine the responses of different Gabor filters to form the edge saliency map as shown in Fig. <ref type="figure" target="#fig_4">5(d)</ref>. Our weighting is computed as:</p><formula xml:id="formula_15">w e = 1 s x ( p) 2 + s x (q) 2 + 1 , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where s x (•) is the value of x-axis edge saliency map if p and q are x-axis neighborhoods. The edge saliency enhances smoothness of depth boundary along edge.</p><p>Allowing the depth values to propagate freely with only very sparse data constraint can lead to notable depth bleeding. Here, we introduce the guided depth map to resolve this problem. The guided depth map weighting is similar to the intensity weighting in bilateral filter. Since we do not have a depth sample at each high-resolution pixel location, we use bicubic interpolation to obtain the guided depth map, D g , as shown in Fig. <ref type="figure" target="#fig_4">5(e)</ref>. Similar to the bilateral filter, we define the guided depth map weighting as follow:</p><formula xml:id="formula_17">w d = exp -( (D g ( p) -D g (q)) 2 2σ 2 g ),<label>(12)</label></formula><p>Combining the weight defined from Eq. ( <ref type="formula" target="#formula_13">9</ref>) to Eq. ( <ref type="formula" target="#formula_15">11</ref>) by multiplication, we obtain the weight w pq = w s w c w e w d . Note that except for the edge saliency term, all the weighting defined in this subsection can be applied to the weighting κ pq via multiplication to the NLS regularization term. Figure <ref type="figure">6</ref> illustrates the effects of each weighting term and the combined weighting term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. User Adjustments</head><p>Since the goal is high-quality depth refinement, there may be cases that some depth frames are going to require user touch up, especially if the data is intended for media related applications. Our approach allows easy user corrections by direct manipulation of the weighting term w pq or by adding additional sparse depth sampling for error corrections.</p><p>For the manipulation of the weighting term, we allow the user to draw scribbles along fuzzy image boundaries, or along the boundaries where the image contrast is low. These fuzzy boundaries or low contrast boundaries represent difficult regions for segmentation and edge saliency detection. As a result, they cause depth bleeding in the reconstructed high-resolution depth map as illustrated in Fig. <ref type="figure">7(b)</ref>. Within the scribble areas, we compute an alpha matte based on the work by Wang et al. <ref type="bibr" target="#b27">[28]</ref> for the two different depth layers. An additional weighting term will be added according to the estimated alpha values within the scribble areas. For the two pixels p and q within the scribble areas, if they belong to the same depth layer, they should have the same or similar alpha value. Hence, our additional weighting term for counting the additional depth discontinuity information is defined as:</p><formula xml:id="formula_18">exp -( (α( p) -α(q)) 2 2σ 2 α ), (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where α(•) is the estimated alpha values within the scribble areas. Figure <ref type="figure">7</ref>(d) shows the effect after adding this alpha weighting term. The scribble areas are indicated by the yellow lines in Fig. <ref type="figure">7</ref>(b) and the corresponding alpha matte is shown in Fig. <ref type="figure">7(c</ref>).</p><p>Our second type of user correction allows the user to draw or remove depth samples on the high-resolution depth map directly. When adding a depth sample, the user can simply pick a depth value from the computed depth map and then assign this depth value to locations where depth samples are "missing". After adding the additional depth samples, our algorithm generates the new depth map using the new depth samples as a hard constraint in Equation ( <ref type="formula" target="#formula_4">4</ref>). The second row of Fig. <ref type="figure">7</ref> shows an example of this user correction. Note that for image filtering techniques, such depth sample correction can be more complicated to incorporate since the effect of new depth samples can be filtered by the original depth sample within a large local neighborhood. Removal of depth samples can also cause a hole in the result of image filtering techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation on the Weighting Terms</head><p>Our weighting term, w pq , is a combination of several heuristic weighting terms. Here we provide some insight to the relative effectiveness of each individual weighting term and their combined effect as shown in Fig. <ref type="figure" target="#fig_6">8</ref> (f). Our experiments found that using only the color similar term can still cause propagation errors. The edge cue is more effective in preserving structure along edge boundary, but cannot entirely remove propagation errors. The effect of the segmentation cue is similar to the color cue as the segmentation is also based on color information, but generally produces sharper boundary with piecewise smoothed depth inside each segment than simply using the color cue. The depth cue<ref type="foot" target="#foot_2">3</ref> is good in avoiding propagation bleeding, but it is not effective along the depth boundaries, which do not utilize the co-occurrence of image edges and depth edges. After combining the four different cues together, the combined weighting scheme shows the best results. The results produced with the combined weighting term can effectively utilize the structures in the high-resolution RGB image while it can avoid bleeding by including the depth cue which consists with the low-resolution depth map.</p><p>To further justify the relative effectiveness of our weight terms, we manipulate the weight values w c , w s , w e and w d individually. Since the weight terms are combined by a multiplication, simply multiplying an individual weight term by a constant factor does not affect the relative effectiveness. Therefore, we apply an exponential function f (w, k) = w k . If one of the weights is mapped as w = f (w, k) where 0 &lt; k &lt; 1, the relative importance of this term decreases since w is likely to be 1. In contrast, If k 1, the weight term dominates the overall weight. k = 0 means that the corresponding weight term is not used and k = 1 means no manipulation (equivalent to our empirical choice). Figure <ref type="figure" target="#fig_6">8 (g)</ref> shows the evaluation results on the four weight terms. The graphs reach the maximum accuracy around k = 1, which implies our empirical choice is reasonable. Figure <ref type="figure" target="#fig_6">8</ref> (g) also shows that the choice of t se do not significantly alters the accuracy and the PSNR drops in most cases when k 1. This implies that excessive reliance on a specific weight term is undesirable. For the color cue, when k &gt; 3, the linear system<ref type="foot" target="#foot_3">4</ref> become singular and the solution is invalid.</p><p>Regarding λ s and λ N defined in Eq. ( <ref type="formula" target="#formula_4">4</ref>), we find out that varying ratio between the two values shows subtle changes in PSNR. Instead, we observe that the magnitude of λ s and λ N is significant to the processed depth map accuracy. Figure <ref type="figure" target="#fig_6">8 (e)</ref> shows an example of choosing λ s and λ N for the noisy depth map. When there is high-level of noise, larger lambda is preferable since the optimization is less stick to the input depth. However, if it is too large, the output can be oversmoothed. Throughout this paper, we choose λ s = λ N = 10 for noisy depth input and λ s = λ N = 0.2 for high-fidelity depth input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extension to RGB-D Video</head><p>In this section, we extend our framework to handle RGB-D video depth completion. A major difference between single image depth completion and video depth completion is that video depth completion requires temporal coherence. To achieve temporal coherent depth completion, we substitute our Eq. ( <ref type="formula" target="#formula_6">5</ref>) as</p><formula xml:id="formula_20">E d (D) = p D( p) -G ( p) 2 , (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>where G ( p) is median value of depth from temporal correspondences as illustrated in Fig. <ref type="figure" target="#fig_7">9</ref>. We find the temporal correspondence of depth value by finding dense temporal correspondence across the RGB video. In our implementation, we use the optical flow algorithm by Liu <ref type="bibr" target="#b18">[19]</ref>. We assume that the depth sensor does not move drastically as well as the motion of moving object inside a scene. From the computed dense optical flows, we trace the depth samples in [t -3, t + 3] frames. Since modern depth sensors can record approximately 30 frames per second, the period within [t -3, t + 3] corresponds to 0.2 seconds. After collecting the multiple correspondences of D( p) in [t -3, t + 3] frames, we compute the median depth values and use it as G ( p) in Eq. ( <ref type="formula" target="#formula_20">14</ref>). This gives us a reliable and temporally consistent data term. For the special case such as Kinect, whose depth maps show consistent holes over the frames, we apply our algorithm on each frames independently for hole filling and take median depth value of tracked points over [t -3, t + 3] frames.</p><p>Section V-C demonstrates the effectiveness of our video depth completion extension versus the results of using the original frame-by-frame depth completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Efficient Energy Optimization</head><p>We now describe our approach to optimize Eq. ( <ref type="formula" target="#formula_4">4</ref>). To optimize our quadratic energy function, similar to the work in Diebel et al. <ref type="bibr" target="#b4">[5]</ref>, we can first register the low-resolution depth map with the color image and interpolate the depth values as an initialization (described in Sec. III-C). After that, we apply an iterative convex optimization technique. Specifically, if we take the first derivative w.r.t. D on Eq. ( <ref type="formula" target="#formula_4">4</ref>) and set the derivative equal to zero, i.e. E(D) D = 0, we get:</p><formula xml:id="formula_22">Ad = g, (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>where A is a n × n Laplacian matrix with weight terms, n is number of pixels in RGB domain, d is the desired depth values, and g is observed depth which is conditionally filled with measured depth G. For D( p), the elements of A and g are filled as:</p><formula xml:id="formula_24">A pp = 1 + λ s + λ N if p ∈ G λ s + λ N otherwise,<label>(16)</label></formula><formula xml:id="formula_25">A pq = - λ s w pq W p , (<label>17</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">A pr = - λ N κ pr K p , (<label>18</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">g p = G( p) if p ∈ G 0 otherwise,<label>(19)</label></formula><p>where q ∈ N ( p) and r ∈ A( p) indicate neighborhoods for smoothness term and NLS regularization term respectively, A pq indicates A's element at p-th row and q-th column, and g p indicates p-th element of vector g. In the RGB-D video case, G ( p) is used instead of G( p). To solve Eq. ( <ref type="formula" target="#formula_22">15</ref>), we applied the built-in linear solver in Matlab T M 2009b, or known as a backslash operator '\'. Qualitative comparison of our depth completion result with KinectFusion <ref type="bibr" target="#b11">[12]</ref>. From the 3D mesh of KinectFusion, we generated a depth map at the color image coordinate. Next, we apply our depth completion approach to the raw Kinect depth map. The two depth maps shows quite similar structure and similar depth values in various regions (especially a thin structure of the desk lamp and boundary regions of cups and holes in the books.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND COMPARISONS</head><p>We demonstrate our approach applied to both depth map upsampling and depth map completion. For depth map upsampling, we present quantitative evaluation and real world examples. For depth map completion, we refine the raw Kinect depth which exhibits missing regions (i.e. holes). The system configuration for experiments is 3Ghz CPU, 8GB RAM. The computation time is summarized in Table <ref type="table" target="#tab_1">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Depth Map Upsampling 1) Evaluations Using the Middlebury Stereo Dataset:</head><p>We use synthetic examples for quantitative comparisons with the results from previous approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The depth map from the Middlebury stereo datasets <ref type="bibr" target="#b22">[23]</ref> are used as the ground truth. We downsampled the ground truth depth map by different factors to create the low-resolution depth map. The original color image is used as the high-resolution RGB image. We compare our results with bilinear interpolation, MRF <ref type="bibr" target="#b4">[5]</ref>, bilateral filter <ref type="bibr" target="#b28">[29]</ref>, and a recent work on guided image filter <ref type="bibr" target="#b8">[9]</ref>. Since the previous approaches do not contain a user correction step, the results generated by our method for these synthetic examples are all based on our automatic method in Sec. IV-A and Sec. IV-B for fair comparisons. Table <ref type="table" target="#tab_0">I</ref> summaries the RMSE (root-mean-square error) against the ground truth under different magnification factors for different testing examples. To demonstrate the advantages of our combined weight, w pq = w s w c w e w d , defined in Sec. IV-B, we have also applied our algorithm using each weight independently. As shown in Table <ref type="table" target="#tab_0">I</ref>, our combined weight consistently achieves the lowest RMSE among all the test cases especially for large scale upsampling.</p><p>The qualitative comparison with the results from <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b28">[29]</ref> under 8× magnification factor can be found in Fig. <ref type="figure" target="#fig_8">10</ref>.</p><p>In terms of depth map quality, we found that the MRF method in <ref type="bibr" target="#b4">[5]</ref> produces the most blurred result. This is due to its simple use of neighborhood term, which considers only the image intensity difference as the neighborhood similarity for depth propagation. The results from bilateral filtering in <ref type="bibr" target="#b28">[29]</ref> are comparable to ours with sharp depth discontinuities in some of the test examples. However, since segmentation and edge saliency are not considered, their results can still suffer from depth bleeding highly textured regions. Also, we found that in the real world example in Fig. <ref type="figure" target="#fig_0">1</ref>, the results from <ref type="bibr" target="#b28">[29]</ref> tend to be blurry.</p><p>2) Robustness to Depth Noise: The depth map captured by depth cameras exhibit notable levels of noise. We compare the robustness of our algorithm and the previous algorithms by adding noise. We also compare against the Noise-Aware bilateral filter approach in <ref type="bibr" target="#b2">[3]</ref>. We observe that the noise characteristics in depth camera depends on the distance between the camera and the scene. To simulate this effect, we add a conditional Gaussian noise:</p><formula xml:id="formula_30">p(x, k, σ d ) = k exp -( x 2(1 + σ d ) 2 ), (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>where σ d is a value proportional to the depth value, and k is the magnitude of the Gaussian noise. Although the actual noise distribution of depth camera is more complicated than the Gaussian noise model, many previous depth map upsampling algorithms do not consider the problem of noise in the lowresolution depth map. This experiment therefore attempts an objective comparison on the robustness of different algorithms towards the effect of noisy depth map. The results in term of RMSE are summarized in Table <ref type="table" target="#tab_0">II</ref>. We note that some of our results are worse than results from the previous methods, this is because the previous methods tends to over-smooth upsampled depth map and therefore have higher noise tolerance. Although we can adjust parameters to increase noise tolerance by increasing smoothness weight, we keep the same parameter setting as in Sec. V-A1 to provide a fair comparison.</p><p>3) ToF Depth Upsampling: Figure <ref type="figure" target="#fig_11">13</ref> shows upsampled ToF depth maps which is taken from real scenes. Since the goal of our paper is to obtain high quality depth maps, we include user corrections for the examples in the top and middle row. We show our upsampled depth as well as a novel view rendered by using our depth map. The magnification factors for all these examples are 8×. These real world examples are challenging with complicated boundaries and thin structures. Some of the objects contain almost identical colors but with different depth values. Our approach is successful in distinguishing the various depth layers with sharp boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Map Completion</head><p>This section provides our depth map completion results on Kinect's raw depth data where the depth map and the RGB image are of the same resolution. For all depth map completion results in this section, we do not include any user markup or post-processing. Figure <ref type="figure" target="#fig_9">11</ref> compares our depth map completion results with results obtained using joint bilateral filtering <ref type="bibr" target="#b16">[17]</ref> and colorization <ref type="bibr" target="#b17">[18]</ref>. The scene in the first row is captured by our calibrated Kinect and the second scene is from the NYU RGB-D dataset <ref type="bibr" target="#b23">[24]</ref>. The NYU RGB-D dataset provides RGB images, depth maps and also pre-calculated camera parameters for alignment. In both scenes, filter based result <ref type="bibr" target="#b16">[17]</ref> in Fig. <ref type="figure" target="#fig_9">11 (c</ref>) shows an over-smoothed depth map. Results from colorization <ref type="bibr" target="#b17">[18]</ref> in Fig. <ref type="figure" target="#fig_9">11(d</ref>) are very similar to our results in Fig. <ref type="figure" target="#fig_9">11</ref>(e) as we both use optimization based method for depth map completion. However, upon careful comparisons in the highlighted regions, our method produces sharper depth boundaries and does not over-smooth fine structures.</p><p>Figure <ref type="figure" target="#fig_12">14</ref> shows more depth completion results using the NYU RGB-D dataset. The input depth map from the Kinect has many holes which are indicated as black. Our refined and completed depth maps are shown in Fig. <ref type="figure" target="#fig_12">14(c</ref>), which well align with object boundaries in RGB image. Note that some holes are very big and the available depth samples within these holes are limited.</p><p>Compared to filter based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref>, it is worth noting that our optimization based method does not need a manual filter size adjustment for completing large depth holes.</p><p>We have also evaluated the quality of our depth completion using "ground truth" depth maps captured using KinectFusion <ref type="bibr" target="#b11">[12]</ref>. As previously discussed in Sec. II, KinectFusion integrates raw Kinect depth maps into a regular voxel grid. From this voxel grid we can construct a textured 3D mesh to serve as the ground truth depth maps. Since KinectFusion integrates noisy depth maps which are captured at various view points, compared to the single raw Kinect depth, the fused depth has less holes and less noise. In our evaluation, we applied our algorithm on the single raw Kinect depth with the RGB image guidance. The result in Fig. <ref type="figure" target="#fig_10">12</ref> shows that our depth completion result is visually similar to the KinectFusion depth. Especially in Fig. <ref type="figure" target="#fig_10">12</ref>, we are able to complete thin and metal structure of the desk lamp whose depth measurement is not captured reliably from the raw Kinect depth map. In addition, the boundary regions and holes are also reasonably completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Video Depth Completion</head><p>Finally, we show our video depth completion results in Fig. <ref type="figure" target="#fig_13">15</ref>. In our experiments, we also use the NYU RGB-D dataset <ref type="bibr" target="#b23">[24]</ref> since it is an public data set and it provides synchronized depth-color video pairs in 30 frame rates. As shown in Fig. <ref type="figure" target="#fig_13">15</ref>, the depth map without temporal coherency flickers and has noticeable depth inconsistency. After using our approach described in Sec. IV-E, however, the artifacts are significantly reduced. In addition, the quality of the depth completion results in each individual frame improved.  <ref type="bibr" target="#b23">[24]</ref>. Each rows show sequence of color images, raw Kinect depth, our depth completion without temporal coherency and with temporal coherency, which is described in Sec. IV-E. The white boxes highlight improvements when the temporal coherency is considered; the large holes in raw depth is reliably filled and the depth boundaries become stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION AND SUMMARY</head><p>We have presented a framework to upsample a lowresolution depth from 3D-ToF camera and to repair raw depth maps from a Kinect using an auxiliary RGB image. Our framework is based on a least-square optimization that combines several weighting factors together with nonlocal structure filtering to maintain sharp depth boundaries and to prevent depth bleeding during propagation. Although the definitions of our weighting factors are heuristic, each of them serves different purposes to protect edge discontinuities and fine structures in the repaired depth maps. By combining the different weighting factors together, our approach achieved the best quality results comparing to individual usage of the weighting factors. Our experimental results show that this framework out-performs previous work in terms of both RMSE and visual quality. In addition to the automatic method, we have also discussed how to extend our approach to incorporate user markup. Our user correction method is simple and intuitive and does not require any addition modifications for solving the objective function defined in Sec. IV-A. Lastly, we described how to extend this framework to work on video input by introducing an additional data term to exploit temporal coherency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Low-resolution depth map (enlarged using nearest neighbor upsampling), (b) high-resolution RGB image, (c) result from [29], (d) our result. User scribble areas (blue) and the additional depth sample (red) are highlighted. The dark areas in (c) are the areas without depth samples after registration. Full resolution comparisons are provided in the supplemental materials.</figDesc><graphic coords="2,175.67,162.05,120.83,90.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Our 3D-ToF imaging setup. It uses a 3D-ToF camera which captures images at 176×144 resolution that is synchronized with a 1280×960 resolution RGB camera. (b) The calibration configuration for 3D-ToF imaging setup that uses a planar calibration pattern with holes to allow the 3D-ToF camera to be calibrated. (c) The Kinect imaging setup. The Kinect depth camera which captures depth map at 640×480 resolution that is synchronized with a 1280 × 1024 resolution RGB camera. (d) The calibration configuration for Kinect imaging setup, which utilizes a planar checker board.</figDesc><graphic coords="3,175.67,162.05,120.74,90.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A synthetic example for validating our outlier detection algorithm. (a) A depth map of a synthetic scene. (b) A depth map of the same scene with a translated view point. (c) Detected occlusion map for the view point change. (d) Detected disocclusion map of (b).</figDesc><graphic coords="3,360.35,293.45,75.50,75.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of our result without (a) and with (b) the NLS term. The same weighting scheme proposed in Sec. IV-B is used for both (a) and (b). Although the usage of NLS does not significantly affect the RMS error, it is important in generating high quality depth maps especially along thin structure elements.</figDesc><graphic coords="3,438.59,293.45,75.50,75.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Low-resolution depth map (enlarged using nearest neighbor upsampling). (b) High-resolution RGB image. (c) Color segmentation by<ref type="bibr" target="#b26">[27]</ref>. (d) Edge saliency map. (e) Guided depth map by using bicubic interpolation of (a). (f) Our upsampling result without the guided depth map weighting, depth bleeding occurred in highly textured regions. (g) Our upsampling result with guided depth map weighting. (h) Ground truth. We subsampled the depth value of a dataset from Middlebury to create the synthetic low-resolution depth map. The magnification factor in this example is 5×. The sum of squared difference (SSD) between (f) and (g) comparing to the ground truth are 31.66 and 24.62 respectively. Note that the depth bleeding problem in highly textured regions has been improved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Visualization of different weighting terms and the overall weighting term.</figDesc><graphic coords="5,55.43,187.85,55.34,75.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A synthetic example for self-evaluation of our weighting term. (a)(b) A synthetic image pair consisting of a high-resolution color image and low-resolution depth image. (c) A low-resolution depth map with noise. (d) Our 8× upsampled depth map with the combined weighting term. (e) A zoomed-up images of upsampling results for various parameter configurations. (f) The plot of PSNR accuracy against the results with the combined weighting term and the results with additional weighting term. The combined weighting term consistently produce the best results under different upsampling scales. (g) In 8× upsampling, the four weights are manipulated individually and its corresponding accuracies are displayed in PSNR. Larger k implies giving more reliance to the corresponding weighting term.</figDesc><graphic coords="5,177.23,187.85,55.34,75.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Long range tracking of depth samples. From the dense flow fields u which is acquired from sequential color image pairs, we accumulate u to get correspondences from distant frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Qualitative comparison on Middlebury dataset. (a) MRFs optimzation [5]. (b) Bilateral filtering with subpixel refinement [29]. (c) Our results. The image resolution are enhanced by 8×. Note that we do not include any user correction in these synthetic testing cases. The results are cropped for the visualization, full resolution comparisons are provided in the supplemental materials.</figDesc><graphic coords="6,86.99,281.45,144.02,108.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Depth map completion results. (a) RGB Images, (b) Registered raw depth maps from Kinect, Depth map completion using (c) Joint bilateral filter<ref type="bibr" target="#b16">[17]</ref>, (d) Colorization<ref type="bibr" target="#b17">[18]</ref>, and (e) Our method. Note the high lighted regions.</figDesc><graphic coords="9,50.60,165.41,100.48,108.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Qualitative comparison of our depth completion result with KinectFusion<ref type="bibr" target="#b11">[12]</ref>. From the 3D mesh of KinectFusion, we generated a depth map at the color image coordinate. Next, we apply our depth completion approach to the raw Kinect depth map. The two depth maps shows quite similar structure and similar depth values in various regions (especially a thin structure of the desk lamp and boundary regions of cups and holes in the books.</figDesc><graphic coords="10,74.51,181.13,140.18,105.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Depth map upsampling on ToF-RGB system. (a) Our input, the low-resolution depth maps are shown on the lower left corner (Ratio between the two images are preserved). (b) Our results. User scribble areas (blue) and the additional depth sample (red) were high-lighted. (c) Novel view rendering of our result. Note that no user markup is required in our results in the third row.</figDesc><graphic coords="11,87.95,502.73,143.54,107.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Depth map completion examples using NYU RGB-D dataset [24]. (a) RGB Images. (b) Registered raw depth maps. (c) Our refinement. For these results no user markup is applied.</figDesc><graphic coords="12,71.51,179.93,154.34,117.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Depth map video completion results on NYU RGB-D dataset<ref type="bibr" target="#b23">[24]</ref>. Each rows show sequence of color images, raw Kinect depth, our depth completion without temporal coherency and with temporal coherency, which is described in Sec. IV-E. The white boxes highlight improvements when the temporal coherency is considered; the large holes in raw depth is reliably filled and the depth boundaries become stable.</figDesc><graphic coords="13,158.87,336.05,98.42,91.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISON ON MIDDLEBURY DATASET. THE ERROR IS MEASURED IN RMSE FOR 4 DIFFERENT MAGNIFICATION FACTORS. THE PERFORMANCE OF OUR ALGORITHM IS THE BEST AMONG ALL COMPARED ALGORITHM. NOTE THAT NO USER CORRECTION IS INCLUDED IN THESE SYNTHETIC TESTING EXAMPLES</figDesc><table><row><cell>TABLE II</cell></row><row><cell>RUNNING TIME OF OUR ALGORITHM FOR 8× UPSAMPLING.</cell></row><row><cell>THE UPSAMPLED DEPTH MAP RESOLUTION IS 1376×1088</cell></row><row><cell>FOR SYNTHETIC AND 1280×960 FOR REAL-WORLD</cell></row><row><cell>EXAMPLES. THE ALGORITHM WAS IMPLEMENTED</cell></row><row><cell>USING UNOPTIMIZED MATLAB CODE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARISON</head><label>III</label><figDesc>ON MIDDLEBURY DATASET WITH ADDITIVE NOISE. OUR ALGORITHM ACHIEVES THE LOWEST RMSE IN MOST CASES. NOTE THAT ALL THESE RESULTS ARE GENERATED WITHOUT ANY USER CORRECTION. BETTER PERFORMANCE IS POSSIBLE AFTER INCLUDING USER CORRECTION</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the case of the Kinect, unreliable depth samples were already rejected leaving holes in the captured depth map.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use two scale 7 × 7, and 15 × 15 filter kernels with 8 orientations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The depth cue produces better results than other cues in this synthetic examples because the depth cues have no noise and no missing values. It is blurry because it is from a low resolution depth map.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We optimize overall energy by solving a linear equation. Details are discussed in Sec. IV-F.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Strategic Research and Development Program for Industrial Technology, Korea, under Grant 10031903, and in part by the National Research Foundation of Korea through the Ministry of Science, ICT and Future Planning, Korean Government, under Grant 2010-0028680. The work of M. S. Brown was supported in part by the Science and Engineering Research Council-Agency for Science, Technology and Research and in part by the Public Sector Research Funding under Grant 1121202020. In So Kweon (M'14) received the B.S. and M.S. degrees in mechanical design and production engineering from Seoul National University, Seoul, Korea, in 1981 and 1983, respectively, and the Ph.D. degree in robotics from the Robotics Institute,</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.mesa-imaging.ch/prodview4k.php" />
		<title level="m">SwissRanger SR4000 Data Sheet</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GradientShop: A gradient-domain optimization framework for image and video filtering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A noise-aware filter for real-time depth upsampling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshop Multi-Camera Multi-Modal Sensor Fus</title>
		<meeting>ECCV Workshop Multi-Camera Multi-Modal Sensor Fus</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Noise brush: Interactive high quality image-noise separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An application of Markov random fields to range sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing System (NIPS)</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Upsampling range data in dynamic environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1141" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flash photography enhancement via intrinsic relighting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="673" to="678" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recovering thin structures via nonlocal-means regularization with application to depth from defocus</title>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1133" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint depth and color camera calibration with distortion correction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2058" to="2064" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusion of range and color images for denoising and resolution enhancement with a non-local filter</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huhle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schairer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Straßer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1336" to="1345" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time 3D reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Annu</title>
		<meeting>24th Annu</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A category-level 3D object dataset: Putting the Kinect to work</title>
		<author>
			<persName><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1168" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel 2.5D pattern for extrinsic calibration of ToF and camera fusion system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="page" from="3290" to="3296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Netw</title>
		<meeting>Int. Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1995-12">Nov./Dec. 1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accuracy and resolution of Kinect depth data for indoor mapping applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khoshelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Elberink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1437" to="1454" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond pixels: Exploring new representations and applications for motion analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Elect. Eng. Comput. Sci., Massachusetts Inst. Technol</title>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When can we use kinectfusion for ground truth acquisition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hämmerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Color-Depth Camera Fusion Robot. IROS</title>
		<meeting>Workshop Color-Depth Camera Fusion Robot. IROS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic extrinsic calibration of vision and lidar by maximizing mutual information</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mcbride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
		<idno type="DOI">10.1002/rob.21542/pdf</idno>
		<ptr target="http://onlinelibrary.wiley.com/doi/10.1002/rob.21542/pdf" />
	</analytic>
	<monogr>
		<title level="j">J. Field Robot., Special Issue Calibration Field Robot</title>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High quality depth map upsampling for 3D-ToF cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1623" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense twoframe stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A mutual information approach to automatic calibration of camera and lidar in natural environments</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Austral. Conf. Robot. Autom</title>
		<meeting>Austral. Conf. Robot. Autom</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Properties and applications of shape recipes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">VLFeat: An Open and Portable Library of Computer Vision Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Soft scissors: An interactive tool for realtime high quality matting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial-depth super resolution for range images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fusion of time-of-flight depth and stereo for high accuracy depth maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-sensor super-resolution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE Workshop Appl. Comput. Vis. (WACV)</title>
		<meeting>6th IEEE Workshop Appl. Comput. Vis. (WACV)</meeting>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
			<biblScope unit="page" from="27" to="31" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
