<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Memory Network for Recommendation Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
							<email>tebesu@scu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
							<email>yfang@scu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Santa Clara University</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Bin Shen Google</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Santa Clara University</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collaborative Memory Network for Recommendation Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20651339946530D90C1285FFCCF026D7</idno>
					<idno type="DOI">10.1145/3209978.3209991</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>memory networks</term>
					<term>collaborative filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommendation systems are vital to keeping users engaged and satisfied with personalized recommendations in the age of information explosion. Users expect personalized content in modern E-commerce, entertainment and social media platforms but the effectiveness of recommendations are restricted by existing user-item interactions and model capacity. The ability to leverage higher order reasoning may help alleviate the problem of sparsity. A popular and successful technique, collaborative filtering (CF), establishes the relevance between users and items from past interactions (e.g., clicks, ratings, purchases) by assuming similar users will consume similar items.</p><p>CF can generally be grouped in three categories: memory or neighborhood-based approaches, latent factor models and hybrid models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>. Memory or neighborhood-based methods form recommendations by identifying groups or neighborhoods of similar users or items based on the previous interaction history. The simplicity of these models such as item K nearest neighbor (KNN) have shown success in production systems at Amazon <ref type="bibr" target="#b20">[21]</ref>. Latent factor models such as matrix factorization project each user and item into a common low dimensional space capturing latent relations. Neighborhood methods capture local structure but typically ignore the mass majority of ratings available due to selecting at most K observations from the intersection of feedback between two users or items <ref type="bibr" target="#b16">[17]</ref>. On the other hand, latent factor models capture the overall global structure of the user and item relationships but often ignore the presence of a few strong associations. The following weaknesses between the local neighborhood-based and global latent factor models lead to the development of hybrid models such as SVD++ <ref type="bibr" target="#b16">[17]</ref> and generalizations such as Factorization Machines <ref type="bibr" target="#b23">[24]</ref> which integrate both neighborhood-based approaches and latent factor models to enrich predictive capabilities.</p><p>Recently, deep learning has made massive strides in many research areas obtaining state of the art performance in computer vision <ref type="bibr" target="#b8">[9]</ref>, question answering <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, learning programs <ref type="bibr" target="#b7">[8]</ref>, machine translation <ref type="bibr" target="#b0">[1]</ref> and many other domains. The successful integration of deep learning methods in recommendation systems have demonstrated the noticeable advantages of complex nonlinear transformations over traditional linear models <ref type="bibr" target="#b39">[40]</ref>. However, existing composite architectures incorporate the latent factor model ignoring the integration of neighborhood-based approaches in a nonlinear fashion. Hence, we propose to represent the neighborhood-based component with a Memory Network <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> to capture higher order complex relations between users and items.</p><p>An external memory permits encoding rich feature representations while the neural attention mechanism infers the user specific contribution from the community.</p><p>We propose a unified hybrid model which capitalizes on the recent advances in Memory Networks and neural attention mechanisms for CF with implicit feedback. The memory component allows read and write operations to encode complex user and item relations in the internal memory. An associative addressing scheme acts as a nearest neighborhood model finding semantically similar users based on an adaptive user-item state. The neural attention mechanism places higher weights on specific subsets of users who share similar preferences forming a collective neighborhood summary. Finally, a nonlinear interaction between the local neighborhood summary and the global latent factors 1 derives the ranking score. Stacking multiple memory components allows the model to reason and infer more precise neighborhoods further improving performance.</p><p>Our primary contributions can be summarized as follows:</p><p>• We propose Collaborative Memory Network (CMN) inspired by the success of memory networks to address implicit collaborative filtering. CMN is augmented with an external memory and neural attention mechanism. The associative addressing scheme of the memory module acts as a nearest neighborhood model identifying similar users. The attention mechanism learns an adaptive nonlinear weighting of the user's neighborhood based on the specific user and item. The output module exploits nonlinear interactions between the adaptive neighborhood state jointly with the user and item memories to derive the recommendation. • We reveal the connection between CMN and the two important classes of collaborative filtering models: the latent factor model and neighborhood-based similarity model. Furthermore, we reveal the advantages of the nonlinear integration fusing the two types of models yielding a hybrid model. • Comprehensive experiments on three public datasets demonstrate the effectiveness of CMN against seven competitive baselines. Multiple experimental configurations confirm the added benefits of the memory module 2 . • Qualitative visualizations of the attention weights provide insight into the memory component providing supporting evidence for deeper architectures to capture higher order complex interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Deep Learning in Recommendation Systems</head><p>Recently, a surge of interest in applying deep learning to recommendation systems has emerged. Among the early works, Salakhutdinov et al. <ref type="bibr" target="#b26">[27]</ref> address collaborative filtering by applying a two layer Restricted Boltzmann Machine modeling tabular movie ratings. Autoencoders have been a popular choice of deep learning architecture for recommender systems <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. The autoencoder acts as a nonlinear decomposition of the rating matrix 1 We use the terms user/item latent factors, memories and embeddings interchangeably. 2 Source code available at: http://github.com/tebesu/CollaborativeMemoryNetwork replacing the traditional linear inner product. For example, AutoRec <ref type="bibr" target="#b27">[28]</ref> decomposes the rating matrix with an autoencoder followed by reconstruction to directly predict ratings obtaining competitive results on numerous benchmark datasets. Collaborative denoising autoencoders (CDAE) <ref type="bibr" target="#b36">[37]</ref> address top-n recommendation by integrating a user-specific bias into an autoencoder demonstrating CDAE can be seen as a generalization of many existing collaborative filtering methods. Li et al. <ref type="bibr" target="#b19">[20]</ref> adopt a marginalized denoising autoencoder to diminish the computational costs associated with deep learning. Employing two autoencoders, one for item content and the other for user content bridged with user and item latent factors. AutoSVD++ <ref type="bibr" target="#b40">[41]</ref> extends the original SVD++ model with a contrastive autoencoder to capture auxiliary item information. A hierarchical Bayesian model <ref type="bibr" target="#b32">[33]</ref> bridges matrix factorization with the deepest layer of a stacked denoising autoencoder leveraging item content in the process. Neural Matrix Factorization <ref type="bibr" target="#b10">[11]</ref> address implicit feedback by jointly learning a matrix factorization and a feedforward neural network. The outputs are then concatenated before the final output to produce an interaction between the latent factors and the nonlinear factors. Ebesu and Fang <ref type="bibr" target="#b3">[4]</ref> address the item cold-start problem by tightly coupling a deep neural network (DNN) for item content and pairwise matrix factorization for rating decomposition. The DNN constructs a robust representation of the item content and item latent factor for new items. Cheng et al. <ref type="bibr" target="#b2">[3]</ref> jointly train a logistic regression and a DNN to leverage the generalization aspects of deep learning and specificity of generalized linear models for mobile app recommendations in the Google Play store.</p><p>Convolutional neural networks (CNN) in recommendation systems have been used to capture localized item feature representations of music <ref type="bibr" target="#b30">[31]</ref>, text <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref> and images <ref type="bibr" target="#b39">[40]</ref>. Previous methods represent text as bag-of-words representations, CNN overcomes this limitation by learning weight filters to identify the most prominent phrases within the text. The sequential nature of recurrent neural networks (RNNs) provides desirable properties for timeaware <ref type="bibr" target="#b35">[36]</ref> and session-based recommendation systems <ref type="bibr" target="#b11">[12]</ref>. For example, Recurrent Recommender Networks <ref type="bibr" target="#b35">[36]</ref> capture temporal aspects with a user and item Long Short Term Memory (LSTM) cell coupled with stationary factors to identify movie popularity fluctuations. Jannach and Ludewig <ref type="bibr" target="#b13">[14]</ref> interpolate KNN with a session-based RNN <ref type="bibr" target="#b11">[12]</ref> demonstrating further performance gains. However, the interpolation scheme is a fixed weighting hyperparameter and lacks a nonlinear interaction to capture more complex relations. Wang et al. <ref type="bibr" target="#b33">[34]</ref> unify the generative and discriminative methodologies under the generative adversarial network <ref type="bibr" target="#b6">[7]</ref> framework for web search, item recommendation, and question answering.</p><p>Attention mechanisms have been recently explored in recommender systems. Gong and Zhang <ref type="bibr" target="#b5">[6]</ref> perform hashtag recommendation with a CNN augmented with an attention channel to concentrate on the most informative (trigger) words. However, a hyperparameter must be carefully set to control the threshold of triggering the word to be informative. Huang et al. <ref type="bibr" target="#b12">[13]</ref> tackle the same task with an End-to-End Memory Network <ref type="bibr" target="#b29">[30]</ref> integrating a hierarchical attention mechanism over the user's previous tweets on a word and sentence level. Chen et al. <ref type="bibr" target="#b1">[2]</ref>   <ref type="bibr" target="#b37">[38]</ref> extend Factorization Machines <ref type="bibr" target="#b23">[24]</ref> with an attention mechanism to learn the importance of each pairwise interaction rather than treating them uniformly. Most existing neural attention based methods rely on additional content or context information while our task is to study collaborative filtering. To the best of our knowledge, no prior work has employed the memory network architecture to address implicit feedback in the collaborative filtering setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory Augmented Neural Networks</head><p>We first provide a brief overview of the inner workings of memorybased architectures. Memory augmented neural networks, generally consist of two components: an external memory typically a matrix and a controller which perform operations on the memory (e.g., read, write, and erase). The memory component increases model capacity independent of the controller (typically a neural network) while providing an internal representation of knowledge to track long-term dependencies and perform reasoning. The controller manipulates these memories with either content-based or location-based addressing. Content-based or associative addressing finds a scoring function between the given question (query) and a passage of text, typically the inner product followed by the softmax operation leading to softly reading each memory location <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. Performing a soft read over the memory locations allows the model to maintain differentiation hence can be trained via backpropagation. The latter type of addressing (usually combined with content-based) performs sequential reads or random access <ref type="bibr" target="#b7">[8]</ref>.</p><p>The initial framework proposed by Weston et al. <ref type="bibr" target="#b34">[35]</ref> demonstrated promising results to track long-term dependencies and perform reasoning over synthetic question answering tasks. Sukhbaatar et al. <ref type="bibr" target="#b29">[30]</ref> alleviated the strong levels of supervision required to train the original memory network becoming an End-to-End system. The notion of attention is biologically motivated how humans do not uniformly process all information in a given task but focus on specific subsets of information. Attention mechanisms also provide a level of insight into the deep learning black box by visualizing the attention weights <ref type="bibr" target="#b0">[1]</ref>. Kumar et al. <ref type="bibr" target="#b17">[18]</ref> improve upon the existing architecture by introducing an episodic memory component allowing for multiple passes or consultations of the memory before producing the final answer. The flexibility of the memory network architecture allows it to perform visual question answering <ref type="bibr" target="#b38">[39]</ref> and joint task learning for identifying the sentiment and the relation to target entity <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COLLABORATIVE MEMORY NETWORK</head><p>In this section, we introduce our proposed model Collaborative Memory Network (CMN), see Figure <ref type="figure" target="#fig_0">1a</ref> for a visual depiction of the architecture. At a high level, CMN maintains three memory states: an internal user-specific memory, an item-specific memory, and a collective neighborhood state. The architecture allows for the joint nonlinear interaction of the specialized local structure of neighborhood-based methods and the global structure of latent factor models. The associative addressing scheme acts as a nearest neighbor similarity function that learns to select semantically similar users based on the current item. The neural attention mechanism permits learning an adaptive nonlinear weighting function for the neighbor model, where the most similar users contribute higher weights at the output module. We later extend the model to a deeper architecture by stacking multiple hops in Section 3.4 depicted in Figure <ref type="figure" target="#fig_0">1b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">User Embedding</head><p>The memory component consists of a user memory matrix M ∈ R P ×d and an item memory matrix E ∈ R Q ×d , where P and Q represents the number of users and items respectively and d denotes the size (dimensionality) of each memory cell. Each user u is embedded in a memory slot m u ∈ M storing her specific preferences. Similarly, each item i corresponds to another memory slot e i ∈ E encoding the item's specific attributes. We form a user preference vector q ui where each dimension q uiv is the similarity of the target user u's level of agreement with user v in the neighborhood given item i as:</p><formula xml:id="formula_0">q uiv = m T u m v + e T i m v ∀ v ∈ N (i)<label>(1)</label></formula><p>where N (i) represents the set of all users (neighborhood) who have provided implicit feedback for item i. We would like to point out N (i) could be replaced or combined with R(i) to handle the case of explicit feedback where R(i) denotes the set of all users who provided explicit feedback for item i. The intuition is as follows, the first term computes compatibility between the target user and the users who have rated item i. The second term introduces the level of confidence user v supports the recommendation of item i. Hence, the associative addressing scheme identifies the internal memories with the highest similarity of the target user u with respect to neighborhood of users given the specific item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neighborhood Attention</head><p>The neural attention mechanism learns an adaptive weighting function to focus on a subset of influential users within the neighborhood to derive the ranking score. Traditional neighborhood methods predefine a heuristic weighting function such as Pearson correlation or cosine similarity and require specifying the number of users to consider <ref type="bibr" target="#b25">[26]</ref>. While factorizing the neighborhood partially alleviates this problem, it is still linear in nature <ref type="bibr" target="#b14">[15]</ref>. Instead by learning a weighting function over the entire neighborhood, we no longer need to empirically predefine the weighting function or number of neighbors to consider. Formally, we compute the attention weights for a given user to infer the importance of each user's unique contribution to the neighborhood:</p><formula xml:id="formula_1">p uiv = exp(q uiv ) k ∈N (i ) exp(q uik ) ∀ v ∈ N (i)<label>(2)</label></formula><p>which produces a distribution over the neighborhood. The attention mechanism allows the model to focus on or place higher weights on specific users in the neighborhood while placing less importance on user's who may be less similar. Next we construct the final neighborhood representation by interpolating the external neighborhood memory with the attention weights: </p><formula xml:id="formula_2">o ui = v ∈N (i ) p uiv c v (3)</formula><p>where c v is another embedding vector for user v which is called external memory in the original memory network framework <ref type="bibr" target="#b34">[35]</ref>.</p><p>Denoting the v th column of the embedding matrix C with the same dimensions as M. The external memory allows the storage of long-term information pertaining specifically to each user's role in the neighborhood. In other words, the associative addressing scheme identifies similar users within the neighborhood acting as a key to weight the relevant values stored in the memory matrix C via the attention mechanism. The attention mechanism selectively weights the neighbors according to the specific user and item. The final output o ui represents a weighted sum over the neighborhood composed of the relations between the specific user, item and the neighborhood.</p><p>CMN captures the similarity of users and dynamically assigns the degrees of contribution to the collective neighborhood based on the target item rather than a predefined number of neighbors which may restrict generalization capacity. Furthermore, the attention mechanism reduces the bottleneck of encoding all information into each individual memory slot and allows the joint exploitation of the user and item observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Output Module</head><p>As noted earlier neighborhood models capture the local structure from the rating matrix via the neighbors while latent factor models identify the global structure of the rating matrix <ref type="bibr" target="#b16">[17]</ref>. Hence we consider the collective neighborhood state to capture localized useritem relations and the user and item memories to capture the global user-item interactions. The output module smoothly integrates a nonlinear interaction between the local collective neighborhood state and the global user and item memories. Existing models lack the nonlinear interaction between the two terms potentially limiting the extent of captured relations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41]</ref>. For a given user u and item i the ranking score is given as:</p><formula xml:id="formula_3">rui = v T ϕ U(m u ⊙ e i ) + Wo ui + b (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ⊙ is the elementwise product; W, U ∈ R d×d ; and v, b ∈ R d are parameters to be learned. We first apply the elementwise product between the user and item memories followed by a linear projection with U, subsequently introducing a skip-connection thus reducing the longest path from the output to input. Skip-connections have been shown to encourage the flow of information and ease the learning process <ref type="bibr" target="#b9">[10]</ref>. In this way, the model can better correlate the specific target addresses (user and item memories) with the ranking score to propagate the appropriate error signals. We further motivate this choice by demonstrating its connection to the latent factor model (Section 3.7.1). Similarly, the final neighborhood representation o ui is projected to a latent space with W then combined with the previous term followed by a nonlinear activation function ϕ (•). Empirically we found the rectified linear unit (ReLU) ϕ (x ) = max(0, x ) to work best due to its nonsaturating nature and suitability for sparse data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Our proposed model provides the following advantages. First, consider the case where the amount of feedback for a given user is sparse, we can leverage all users who have rated the item to gain additional insight about the existing user and item relations. Second, the neural attention mechanism adjusts the confidence of each user's contribution to the final ranking score dependent on the specific item. Finally, the nonlinear interaction between the local neighborhood and global latent factors provide a holistic view of the user-item interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multiple Hops</head><p>We now extend our model to handle an arbitrary number of memory layers or hops. Figure <ref type="figure" target="#fig_0">1b</ref> (right) illustrates CMN's architecture with multiple hops. Each hop queries the internal user memory and item memory followed by the attention mechanism to derive the next collective neighborhood state vector. The first hop may introduce the need to acquire additional information. Starting from the second hop, the model begins to take into consideration the collective user neighborhood guiding the search for the representation of the community preferences. Each additional hop repeats this step considering the previous hop's newly acquired information before producing the final neighborhood state. In other words, the model has the chance to look back and reconsider the most similar users to infer more precise neighborhoods. More specifically, multiple memory modules are stacked together by taking the output from the h th hop as input to the (h + 1) t h hop. Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> we apply a nonlinear projection between hops:</p><formula xml:id="formula_5">z h ui = ϕ (W h q h ui + o h ui + b h )<label>(5)</label></formula><p>where W h is a square weight matrix mapping the user preference query q h ui to a latent space coupled with the existing information from the previous hop followed by a nonlinearity. Intuitively, the initial consultation of the memory may introduce the need for additional information to infer more precise neighborhoods. The nonlinear transformation updates the internal state then solicits the user neighborhood:</p><formula xml:id="formula_6">q h+1 uiv = (z h ui ) T m v ∀ v ∈ N (i)<label>(6)</label></formula><p>The newly formed user preference vector then recomputes the compatibility between the target user and the neighborhood followed by the adaptive attention mechanism producing an updated collective neighborhood summary. This process is repeated for each hop yielding an iterative refinement. The output module receives the weighted neighborhood vector from the last (H t h ) hop to produce the final recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Parameter Estimation</head><p>Since our objective is to study implicit feedback which is more pervasive in practice and can be collected automatically (e.g. clicks, likes). In the case of implicit feedback, the rating matrix contains a 1 if the item is observed and 0 otherwise. We opt for the pairwise assumption, where a given user u prefers the observed item i + over unobserved or negative item i -. The traditional pointwise approach assumes the user is not interested in the item i -but in reality may not be aware of the item. We can form triplet preferences (u, i + , i -) since the number of preference triplets is quadratic in nature we uniformly sample a ratio of positive items to negative items which we further investigate in Section 4.7. We leverage the Bayesian Personalized Ranking (BPR) optimization criterion <ref type="bibr" target="#b24">[25]</ref> as our loss function which approximates AUC (area under the ROC curve):</p><formula xml:id="formula_7">L = - (u,i + ,i -) log σ ( rui + -rui -)<label>(7)</label></formula><p>where σ (x ) = 1/ 1 + exp(-x ) is the logistic sigmoid function. It is worth noting we are not restricted to setting σ (x ) as the logistic sigmoid function. Other pairwise probability functions such as the Probit function can be used as in <ref type="bibr" target="#b3">[4]</ref>. Since the entire architecture is differentiable, CMN can be efficiently trained with the backpropagation algorithm. To reduce the number of parameters we perform layerwise weight tying sharing all embedding matrices across hops <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Computational Complexity</head><p>The In practice, prefiltering techniques can be used to limit the number of neighbors to the top-K because not all neighbors may be indicative in contributing to the final prediction <ref type="bibr" target="#b25">[26]</ref>. Since the purpose of our study is to understand the characteristics of CMN, we leave prefiltering techniques to future work. Recommendation can be performed by computing the predicted ranking score (Eqn. 4) for a given user and item with a single pass through the network. The item with the highest value is recommended to the user. The computational complexity for runtime recommendation is the same with that of the single forward pass during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Relation to Existing Models</head><p>CMN consists of components which can be interpreted in terms of the three classes of collaborative filtering methods. We show the connection with the latent factor model and neighborhood-based similarity models, and finally the relation to hybrid models such as SVD++. We conclude the section by drawing parallels between memory networks and CMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.1">Latent Factor Model.</head><p>The latent factor model discovers hidden relations by decomposing the ratings matrix into two lower rank matrices. By omitting the neighborhood term and bias, and further setting U to the identity matrix Eqn. 4 becomes the following:</p><formula xml:id="formula_8">rui = v T ϕ (m u ⊙ e i )<label>(8)</label></formula><p>which leads to a generalized matrix factorization (GMF) model <ref type="bibr" target="#b10">[11]</ref>. Removing the nonlinearlity by setting ϕ (•) to the identity function and constraining v to 1 vector of all ones, we recover matrix factorization. Under our pairwise loss function (Eqn. 7) we recover BPR <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.2">Neighborhood-based Similarity</head><p>Model. The objective of neighborhood-based similarity models are to estimate a user-user 3 similarity matrix S ∈ R P ×P . For each user who rated item i an aggregated similarity score produces the confidence of recommending the item. The general form of neighborhood similarity models are:</p><formula xml:id="formula_9">rui = α v ∈N (i ) S uv<label>(9)</label></formula><p>where α is a normalization term to weight the ranking score. In the simplest case, the normalization term is set to |N (i)| -ρ where ρ is a hyperparameter controlling the level of similarity required to obtain a high score. In KNN, the neighborhood N (i) is restricted to be the weighted combination of the K most similar users and the similarity matrix S is approximated with a heuristically predefined function such as Pearson correlation or cosine similarity. Another approach is to learn the similarity function by approximating S <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>. In our case, the attached memory module from Eqn. 3 acts as the neighborhood similarity matrix. If we designate the attention mechanism as a predefined normalization term the useruser similarity matrix is then factorized as S = CC T . Using the prediction rule from Eqn. 9 the memory module yields a userbased variant of FISM and under the BPR loss function we recover FISMauc <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.3">Hybrid Model.</head><p>We have shown the connection between the components of CMN and the two classes of collaborative filtering models. Hybrid models such as SVD++ <ref type="bibr" target="#b16">[17]</ref> contains two general terms, a user-item latent factor interaction and a neighborhood component. The output module (Eqn. 4) smoothly integrates the latent factors and the similarity or neighborhood terms together leading to a hybrid model.</p><formula xml:id="formula_10">rui = v T ϕ Latent Factors m u ⊙ e i + v ∈N (i )</formula><p>p uiv c v Neighborhood <ref type="bibr" target="#b9">(10)</ref> We remove the projection matrices and bias terms for clarity. We can see the global interaction from the latent factors consist of the user and item memories. The memory module represents the localized neighborhood component and the neighborhood normalization term is replaced with the adaptive attention mechanism pushed inside the summation becoming a user-item specific weighting scheme. Unlike SVD++, our hybrid model allows for complex nonlinear interactions between the two terms to model the diverse tastes of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.4">Memory Networks.</head><p>Traditional memory networks address the task of question answering. A short story or passage of text is provided along with a question for which the answer can be derived by leveraging some form of reasoning. If we pose recommendation as a question answering problem we are asking how likely will this user enjoy the item where the user neighborhood is the story and the output ranking score is the answer. Continuing our analogy, each word in the story acts as a user in the neighborhood providing supporting evidence for the recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We study the effectiveness of our proposed approach on three publicly available datasets. The first dataset is collected from Epinions [22] which provides an online service for users to share product feedback in the form of explicit ratings (1-5) and reviews. We convert the explicit ratings to implicit feedback as a 1 if the user has rated the item and 0 otherwise. The second dataset is citeulike-a<ref type="foot" target="#foot_3">5</ref>  <ref type="bibr" target="#b31">[32]</ref> collected from CiteULike an online service which provides users with a digital catalog to save and share academic papers. User preferences are encoded as 1 if the user has saved the paper (item) in their library. The third dataset from Pinterest<ref type="foot" target="#foot_4">6</ref>  <ref type="bibr" target="#b4">[5]</ref> allows users to save or pin an image (item) to their board indicating a 1 or positive interaction otherwise a 0 and preprocessed according to <ref type="bibr" target="#b10">[11]</ref>. Table <ref type="table" target="#tab_2">1</ref> summarizes the statistics of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We validate the performance of our proposed approach using the leave-one-out evaluation method following the prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref>. Closely following the setup from He et al. <ref type="bibr" target="#b10">[11]</ref>, for each user we randomly hold out one item the user has interacted with and sample 100 unobserved or negative items to form the test set. The remaining positive examples form the training set. If the user has only rated a single item we keep it in the training set to prevent the cold-start setting. We rank the positive item along with the 100 negative items and adopt two common ranking evaluation metrics Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) <ref type="bibr" target="#b25">[26]</ref>. Intuitively, HR measures the presence of the positive item within the top N and NDCG measures the items position in the ranked list and penalizes the score for ranking the item lower in the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and Settings</head><p>We compare our proposed approach against seven competitive baselines representing neighborhood-based; traditional latent factor models; hybrid model and deep learning-based models.</p><p>• KNN <ref type="bibr" target="#b25">[26]</ref> is a neighborhood-based approach computing the cosine item-item similarity to provide recommendations. • Factored Item Similarity Model (FISM) <ref type="bibr" target="#b14">[15]</ref> is a neighborhoodbased approach factorizing the item-item similarity matrix into two low rank matrices optimizing the BPR loss function. • Bayesian Personalized Ranking (BPR) <ref type="bibr" target="#b24">[25]</ref> is a competitive pairwise matrix factorization for implicit feedback. • SVD++ <ref type="bibr" target="#b16">[17]</ref> is a hybrid model combining the neighborhoodbased similarity and the latent factor model. • Generalized Matrix Factorization (GMF) <ref type="bibr" target="#b10">[11]</ref> is a nonlinear generalization of the latent factor model. We use the ReLU activation function and optimize the BPR loss function.</p><p>Epinions citeulike-a Pinterest HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@ • Collaborative Denoising Auto Encoder (CDAE) <ref type="bibr" target="#b36">[37]</ref> is an item-based deep learning model for item ranking with a user specific bias. • Neural Matrix Factorization (NeuMF) <ref type="bibr" target="#b10">[11]</ref> is a composite matrix factorization jointly coupled with a multilayer perceptron model for item ranking. We would like to note that FISM <ref type="bibr" target="#b14">[15]</ref> improves upon Sparse Linear Methods (SLIM) <ref type="bibr" target="#b22">[23]</ref> by factorizing the item-item similarity matrix to handle missing entries hence we do not compare to SLIM. We exclude baselines utilizing additional information for fair comparison since our objective is to study implicit collaborative filtering without content or contextual information.</p><p>All hyperparameters are tuned according to the validation set. The validation set is formed by holding out one interaction per user from the training set <ref type="bibr" target="#b10">[11]</ref>. We perform a grid search over each model's latent factors from {10, 20, 30, 40, 50} and regularization terms {0.1, 0.01, 0.001}. In addition, we varied CDAE's corruption ratio from {0, 0.2, 0.4, 0.6, 0.8, 1.0} and NeuMF's layers from {1, 2, 3}. The number of negative samples is set to 4. Careful initialization of deep neural networks is crucial to avoid saddle points and poor local minima <ref type="bibr" target="#b6">[7]</ref>. Thus, CMN initializes the user (M) and item (E) memory embeddings from a pretrained model according to Eqn. 8. Remaining parameters are initialized according to <ref type="bibr" target="#b8">[9]</ref> which adapts the variance preserving Xavier initialization <ref type="bibr" target="#b6">[7]</ref> for the ReLU activation function. The gradient is clipped if the norm exceeds 5; l 2 weight decay of 0.1 with the exception of the user and item memories; and the mini-batch size is set to 128 for Epinions, citeulikea and 256 for Pinterest. We adopt the RMSProp <ref type="bibr" target="#b6">[7]</ref> optimizer with a learning rate of 0.001 and 0.9 for both decay and momentum. The default number of hops is set to 2 and memory or embedding size d to 40, 50 and 50 for the Epinions, citeulike-a and Pinterest datasets respectively. The effects of these hyperparameters are further explored in Sections 4.5 and 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Comparison</head><p>Table <ref type="table" target="#tab_3">2</ref> lists results of CMN with one, two and three hops along with the baselines for HR and NDCG with cut offs at 5 and 10 on the Epinions, citeulike-a and Pinterest datasets. We denote CMN with one hop as 'CMN-1', two hops with 'CMN-2' and so on. At a highlevel CMN variants obtain the best performance across both HR and NDCG at all cut offs for all datasets. We now provide a detailed breakdown of our results on the Epinions dataset. All baselines with the exception of KNN show competitive performance with each other across all metrics and cut offs. Since CMN shares the same loss function with BPR, FISM and GMF, we can attribute the performance increase to the memory component. The application of a nonlinear transformation does not necessarily help as evident from BPR outperforming its nonlinear counterpart GMF. KNN demonstrated the poorest performance particularly due to the restrictive ability to handle sparse data when only a few neighbors are present and a large number (139k) of items. However, FISM's learned similarity function performs better since it can address missing entries in the item-item similarity matrix. On the other hand, CMN can leverage the global structure of the latent factors encoded in the memory vectors and the additional memory component to infer complex user preferences. Furthermore, CMN's performance gains over CDAE, GMF and NeuMF portray the successful integration of the memory component and attention mechanism over existing nonlinear and deep learning-based methods.</p><p>The denser citeulike-a dataset contains fewer items than the Epinions dataset leading to competitive performance from the itembased KNN method obtaining the strongest baseline NDCG@5. SVD++ outperforms the neighborhood-based FISM and latent factor BPR revealing the effectiveness of combining two approaches into a single hybrid model. The linear decomposition of the item-item similarity matrix in FISM may lack the expressiveness to capture complex preferences as suggested by the nonlinear GMF outperforming the linear BPR. CMN demonstrates improved performance over the fixed neighborhood-based weighting of KNN and the learned linear similarity scheme of FISM indicating the additional nonlinear transformation and adaptive attention mechanism captures more complex semantic relations between users. CMN can be viewed as NeuMF by replacing the memory network component with a multilayer perceptron. CMN outperforming NeuMF further establishes the advantage of the memory network component to identify complex interactions and iteratively update the internal neighborhood state.</p><p>In the Pinterest dataset, CMN with a single hop demonstrates competitive performance to baseline methods but with additional hops performance is further enhanced. SVD++ demonstrates competitive performance but may lack the full expressiveness of nonlinearity found in deep learning-based models to capture latent  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epinions citeulike-a Pinterest</head><p>HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10 HR@5 HR@10 NDCG@5 NDCG@10  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Embedding Size</head><p>We illustrate the effect of varying the size of memory slots or embeddings and the number of hops for HR@10 and NDCG@10 on the Epinions dataset in Figure <ref type="figure" target="#fig_2">2a</ref>. Since HR and NDCG show similar patterns, we focus our analysis on NDCG. The general trend shows a steady improvement as the embedding size increases with the exception of a single hop where an embedding size of 40 shows peak HR@10 performance followed by a degradation potentially due to overfitting. A single hop confines the model's expressiveness to infer and discriminate between the relevant and irrelevant information encoded in each memory cell. With a small embedding size of 20 increasing the number of hops provides negligible benefits but as the embedding size increases multiple hops show significant improvement over a single hop.</p><p>For the citeulike-a dataset, Figure <ref type="figure" target="#fig_2">2b</ref> portrays the best performance of a single hop at an embedding size of 20 followed by a degradation as model capacity increases which is somewhat similar to the Epinions dataset. At three hops and an embedding size of 40 shows an unusual drop in performance potentially from finding a poor local minima due to the nonconvex nature of neural networks. Two and four hops show almost identical performance with at most a deviation of 0.3% from each other on HR and NDCG. In general, two hops demonstrates competitive performance against the three and four hop models across all embedding sizes.</p><p>The Pinterest dataset in Figure <ref type="figure" target="#fig_2">2c</ref> shows a similar trend of gradual performance gains as the embedding size increases but a single hop shows insufficient capacity to model complex user-item interactions. Unlike the results from the previous datasets the performance of a single hop does not degrade as the embedding size increases. The larger dataset may provide some implicit form of regularization to prevent overfitting. Two, three and four hops show similar performance and incremental with larger embedding sizes. Identifying a sufficient number of hops initially takes precedence over the size of the embeddings. With a sufficient number of hops the embedding size can be increased yielding a trade off between computational cost and performance. By introducing additional hops, CMN can better manipulate the memories and internal state to represent more complex nonlinear relations. Consistent with previous results, the addition of more than two hops do not show significant benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effects of Attention and Nonlinearity</head><p>In this section, we seek to further understand the effect of individual components on performance. In Table <ref type="table" target="#tab_5">3</ref>, the results for CMN without attention denoted 'CMN-Attn' uniformly performs worse than with attention hinting at the effectiveness of the attention mechanism. We also experimented with a linear version of CMN where all ReLU activation functions are set to the identity function denoted as 'CMN-Linear'. The linear version with the attention mechanism generally outperforms the nonlinear version without attention. Further illustrating the effectiveness of the attention mechanism. The final variation removes the attention mechanism from the linear version denoted as 'CMN-Linear-Attn' which generally performs worse than the linear version with the attention mechanism. In general, removing the nonlinear transformation and attention mechanism in some variation yield similar performance on the Epinions and Pinterest datasets. In the citeulike-a dataset the linear version with and without the attention mechanism show improvements over the nonlinear variant without the attention mechanism. This seems counter intuitive and may indicate a potential difficulty in finding a good local minima or a vanishing gradient problem which is consistent with the unusual drop in performance reported in Section 4.5. CMN requires a combination from both the attention mechanism and nonlinear transformations to yield the best performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Negative Sampling</head><p>In this section, we study the characteristics of varying the negative samples for CMN reporting HR@10 and NDCG@10. We exclude the results of four hops since the results were consistent with that of three hops. We also omit 1 negative sample since CMN was unable to distinguish between positive and negative samples leading to random performance. Figure <ref type="figure" target="#fig_4">3a</ref> illustrates the performance of CMN varying the negative samples from 2-10 on the Epinions dataset. A single hop shows fluctuations reporting low HR at 5 and 10 negative samples but outperforms the two and three hop versions with 7 negative samples. A single hop uniformly performs worse than the two and three hop counterparts in the citeulike-a dataset presented in Figure <ref type="figure" target="#fig_4">3b</ref>. In Figure <ref type="figure" target="#fig_4">3c</ref>, the results for a single hop on the Pinterest dataset describes a general upward trend where performance improves as the number of negative samples increase. In both the citeulike-a and Pinterest datasets we observe two and three hops show comparable results and more stability to the number of negative samples while outperforming a single hop. Overall, the performance of CMN is fairly stable with respect to the number of negative samples when at least two hops are present. Similar to the previous section on embedding size, we notice having at least two hops reduces the sensitivity to the hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Attention Visualization</head><p>Attention mechanisms allow us to visualize the weights placed on each user in the neighborhood with the hope of providing interpretable recommendations. We plot a heatmap of the weights from Eqn. 2 in Figure <ref type="figure" target="#fig_5">4</ref>. The color scale represents the intensities of the attention weights, where a darker color indicates a higher value and lighter colors indicate a lower value. For ease of reference, we label each column representing a user in the neighborhood starting from 1 which may not necessarily reflect the true user id from the dataset. Furthermore, for each user we provide additional context in the form of user statistics. We denote 11/167 to indicate the user has rated a total of 167 items with 11 items observed in common with the target user in the training set. Since the size of the neighborhood can be large we limit the visualization to top 5 neighbors sorted by the highest aggregated attention values. We would like to point out that in some cases the attention weights can be small and hence not visually distinguishable.</p><p>The attention weights for a random user from the Epinions dataset is portrayed in Figure <ref type="figure" target="#fig_5">4a</ref>. The user has a total of 49 neighbors thus we show only the top 5 neighbors due to space constraints. We can see all the top users attended to have at least a single item in common. The first hop places heavy levels of attention on user 1 and lightly on user 3. At two hops the attention on user 3 increases. Progressing to three hops the attention spread out across four users. Finally, at four hops user 2 and 3 have the highest weight with a balance of the number items in common with the target user and overall number of ratings observed suggesting these users may be the most influential in the recommendation process. As shown in previous sections performance generally increases with additional hops suggesting considering a combination of multiple users may be beneficial.</p><p>Figure <ref type="figure" target="#fig_5">4b</ref> illustrates the attention weights over four hops for a random user from the citeulike-a dataset with a total of 9 neighbors. We observe the first hop places a large amount of weight on a single user which may explain the poorer performance of CMN with a single hop. User 1 has the highest number of observations out of the neighborhood which may be a reasonable choice but it ignores other information that may be present from other users. Examining the weights of the second hop we see the attention is spread out across five users with a higher emphasis on users 1 and 2 who have the most items in common with the target user. Four out of the five users have a common item with the target user providing a strong indicator of the successful integration of the attention mechanism.</p><p>Next, we focus on three hops which removes the attention over user 5 and reduces the intensities on user 4, 2 and 3. In the final hop, attention is returned to user 5 with stronger weights than in hop two. The overall attention levels shift around slightly but focus most heavily on user 2 which makes sense since it has the highest number of commonly rated items. Since user 4 has no items in common with the target user but large attention weights this warranted further investigation. We found user 4 to have at least one item in common with all other users in the neighborhood which may explain the attention placed on user 4 despite no corated items with the target user in the training data. This demonstrates the memory component captures higher level interactions within the neighborhood suggesting some form of transitive reasoning.</p><p>Figure <ref type="figure" target="#fig_5">4c</ref> illustrates the attention weights over four hops for a random user from the Pinterest dataset. Similar to the previous visualization the first hop places heavy weights on a single user followed by a more dispersed weighting in the following hops. In hops two through four, a small amount of attention is placed upon each user which may not be visually distinguishable. Each hop allows CMN to examine the external memory and perhaps through some form of trial and error arrives at identifying the most useful neighbor as user 1 in hop four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduced a novel hybrid architecture unifying the strengths of the latent factor model and neighborhood-based methods inspired by Memory Networks to address collaborative filtering (CF) with implicit feedback. We reveal the connection between components of Collaborative Memory Network (CMN) the two important classes of CF models and draw parallels with the original memory network framework. Comprehensive experiments under multiple configurations demonstrate significant improvements over competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest higher order transitive relations may be present. In future work, we hope to extend CMN to incorporate content and context information, tackle dialogue-based systems, and perform adversarial training <ref type="bibr" target="#b6">[7]</ref>. Adversarial training is particularly attractive since it enables the model learn an implicit similarity metric to characterize the data rather than a surrogate loss function approximating the AUC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Proposed architecture of Collaborative Memory Network (CMN) with a single hop (a) and with multiple hops (b).</figDesc><graphic coords="4,63.00,83.69,486.00,178.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Epinions dataset (b) citeulike-a dataset (c) Pinterest dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental results for CMN varying the embedding size from 20-100 and hops from 1-4.</figDesc><graphic coords="8,54.08,83.69,166.44,61.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Epinions dataset (b) citeulike-a dataset (c) Pinterest dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results for CMN varying the number of negative samples from 2-10 and hops from 1-3.</figDesc><graphic coords="9,54.08,198.10,166.45,55.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(a)Figure 4 :</head><label>4</label><figDesc>Figure 4: Heatmap of the attention weights over four hops. The color scale indicates the intensities of the weights, darker representing a higher weight and lighter a lower weight. Each column represents a user in the neighborhood labeled with the number of items in common with the target user / number of ratings in training set. For example, 11/167 indicates user 1 has 11 items corated with the current user u and has rated a total of 167 items in the training set.</figDesc><graphic coords="9,222.77,198.10,166.45,55.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>computational complexity for a forward pass through CMN for a user is O d |N (i)| + d 2 + d where |N (i)| denotes the size of the neighborhood for item i and d is the embedding size. The first term O d |N (i)| is the cost for computing the user preference vector and the latter terms correspond to the final interactions in the output module. Each additional hop introduces O d |N (i)| + d 2 complexity. During training two forward passes are computed one for the observed positive item and the second for the negative unobserved item. Parameters can be updated via backpropagation with the same complexity. In real-world datasets, |N (i)| is usually slightly larger than or comparable to d, and thus the primary computational complexity is computing O (d |N (i)|).The cost is reasonable since other deep learning methods such as CDAE<ref type="bibr" target="#b36">[37]</ref> compute a forward pass in O Qd where Q is the total number of items. The proposed memory module only computes the similarity with the target user's neighbors (not over all users) and |N (i)| is often less than or comparable to Q. Thus, the training in CMN is quite efficient.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results for different methods on the Epinions, citeulike-a and Pinterest datasets. Best results highlighted in bold. † indicates the improvement over baselines is statistically significant on a paired t-test (p &lt; 0.01).</figDesc><table><row><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>CMN variants without attention (CMN-Attn); linear activation with attention (CMN-Linear); and linear without attention (CMN-Linear-Attn).user-item relations. The larger dataset helps the two deep learning baselines to outperform the non-deep learning-based methods but the hybrid nature of CMN allows the joint nonlinear exploitation of the local neighborhood and global latent factors yielding additional performance gains. Overall, CMN with two hops outperforms CMN with a single hop but supplementing additional hops greater than two did not provide significant advantages.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Equivalently switching users with items yields item-based methods.Session</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>4D: Recommender Systems -Methods SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://www.trustlet.org/downloaded_epinions.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://www.cs.cmu.edu/~chongw/data/citeulike/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://sites.google.com/site/xueatalphabeta/ Session 4D: Recommender Systems -Methods SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with feature-and item-level attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Semantic Personalized Ranking for item cold-start recommendation</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="109" to="131" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning image and user features for recommendation in social networks</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hashtag Recommendation Using Attention-Based Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">Yuyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Shaoqing Ren, and Jian Sun</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Neural Collaborative Filtering</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hashtag Recommendation Using End-To-End Memory Networks with Hierarchical Attention</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">When Recurrent Neural Networks Meet the Neighborhood for Session-Based Recommendation</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fism: factored item similarity models for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional Matrix Factorization for Document Context-Aware Recommendation</title>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinoh</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep Memory Networks for Attitude Identification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In WSDM</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Collaborative Filtering via Marginalized Denoising Auto-encoder</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Amazon.Com Recommendations: Item-to-Item Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Trust-aware Recommender Systems</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Avesani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slim: Sparse linear methods for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BPR : Bayesian Personalized Ranking from Implicit Feedback</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Introduction to recommender systems handbook</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">AutoRec : Autoencoders Meet Collaborative Filtering</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction</title>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Recurrent Recommender Networks. In WSDM</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07435</idno>
		<title level="m">Deep Learning based Recommender System: A Survey and New Perspectives</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">AutoSVD++: An Efficient Hybrid Collaborative Filtering Model via Contractive Auto-encoders</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
