<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning via Semi-Supervised Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<email>jasonw@nec-labs.com</email>
						</author>
						<author>
							<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
							<email>collober@nec-labs.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs America</orgName>
								<address>
									<addrLine>4 Independence Way</addrLine>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country>USA (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IGAR</orgName>
								<orgName type="institution" key="instit2">University of Lausanne</orgName>
								<address>
									<postCode>1015</postCode>
									<settlement>Amphipôle, Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning via Semi-Supervised Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how nonlinear embedding algorithms popular for use with shallow semisupervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Embedding data into a lower dimensional space or the related task of clustering are unsupervised dimensionality reduction techniques that have been intensively studied. Most algorithms are developed with the motivation of producing a useful analysis and visualization tool.</p><p>Recently, the field of semi-supervised learning <ref type="bibr" target="#b7">(Chapelle et al., 2006)</ref>, which has the goal of improving generalization on supervised tasks using unlabeled data, has made use of many of these techniques. For example, researchers have used nonlinear embedding or cluster representations as features for a supervised classifier, with improved results.</p><p>Most of these architectures are disjoint and shallow, by which we mean the unsupervised dimensionality reduction algorithm is trained on unlabeled data separately as a first step, and then its results are fed to a supervised classifier which has a shallow architecture such as a (kernelized) linear model. For example, several methods learn a clustering or a dis-tance measure based on a nonlinear manifold embedding as a first step <ref type="bibr" target="#b8">(Chapelle et al., 2003;</ref><ref type="bibr" target="#b8">Chapelle &amp; Zien, 2005)</ref>. Transductive Support Vector Machines (TSVMs) <ref type="bibr" target="#b20">(Vapnik, 1998)</ref> (which employs a kind of clustering) and LapSVM <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref> (which employs a kind of embedding) are examples of methods that are joint in their use of unlabeled data and labeled data, but their architecture is still shallow.</p><p>Deep architectures seem a natural choice in hard AI tasks which involve several sub-tasks which can be coded into the layers of the architecture. As argued by several researchers <ref type="bibr" target="#b12">(Hinton et al., 2006;</ref><ref type="bibr" target="#b4">Bengio et al., 2007)</ref> semi-supervised learning is also natural in such a setting as otherwise one is not likely to ever have enough labeled data to perform well.</p><p>Several authors have recently proposed methods for using unlabeled data in deep neural network-based architectures. These methods either perform a greedy layer-wise pre-training of weights using unlabeled data alone followed by supervised fine-tuning (which can be compared to the disjoint shallow techniques for semisupervised learning described before), or learn unsupervised encodings at multiple levels of the architecture jointly with a supervised signal. Only considering the latter, the basic setup we advocate is simple:</p><p>1. Choose an unsupervised learning algorithm.</p><p>2. Choose a model with a deep architecture.</p><p>3. The unsupervised learning is plugged into any (or all) layers of the architecture as an auxiliary task.</p><p>4. Train supervised and unsupervised tasks using the same architecture simultaneously.</p><p>The aim is that the unsupervised method will improve accuracy on the task at hand. However, the unsupervised methods so far proposed for deep architectures are in our opinion somewhat complicated and restricted. They include a particular kind of generative model (a restricted Boltzmann machine) <ref type="bibr" target="#b12">(Hinton et al., 2006)</ref>, autoassociators <ref type="bibr" target="#b4">(Bengio et al., 2007)</ref>, and a method of sparse encoding <ref type="bibr" target="#b16">(Ranzato et al., 2007)</ref>. Moreover, in all cases these methods are not compared with, and appear on the surface to be completely different to, algorithms developed by researchers in the field of semi-supervised learning.</p><p>In this article we advocate simpler ways of performing deep learning by leveraging existing ideas from semi-supervised algorithms so far developed in shallow architectures. In particular, we focus on the idea of combining an embedding-based regularizer with a supervised learner to perform semi-supervised learning, such as is used in Laplacian SVMs <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref>. We show that this method can be: (i) generalized to multi-layer networks and trained by stochastic gradient descent; and (ii) is valid in the deep learning framework given above.</p><p>Our experimental evaluation is then split into three parts: (i) stochastic training of semi-supervised multilayered architectures is compared with existing semisupervised approaches on several benchmarks, with positive results; (ii) a demonstration of how to use semi-supervised regularizers in deep architectures by plugging them into any layer of the architecture is shown on the well-known MNIST dataset; and (iii) a case-study is presented using these techniques for deep-learning of semantic role labeling of English sentences.</p><p>The rest of the article is as follows. In Section 2 we describe existing techniques for semi-supervised embedding. In Section 3 we describe how to generalize these techniques to the task of deep learning. Section 4 reviews existing techniques for deep learning, Section 5 gives an experimental comparison between all these approaches, and Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Semi-Supervised Embedding</head><p>A key assumption in many semi-supervised algorithms is the structure assumption<ref type="foot" target="#foot_1">1</ref> : points within the same structure (such as a cluster or a manifold) are likely to have the same label. Given this assumption, the aim is to use unlabeled data to uncover this structure.</p><p>In order to do this many algorithms such as cluster kernels <ref type="bibr" target="#b8">(Chapelle et al., 2003)</ref>, LDS <ref type="bibr" target="#b8">(Chapelle &amp; Zien, 2005)</ref>, label propagation <ref type="bibr" target="#b22">(Zhu &amp; Ghahramani, 2002)</ref> and LapSVM <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref>, to name a few, make use of regularizers that are directly related to unsupervised embedding algorithms. To understand these methods we will first review some relevant approaches to linear and nonlinear embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Embedding Algorithms</head><p>We will focus on a rather general class of embedding algorithms that can be described by the following type of optimization problem: given the data x 1 , . . . , x U find an embedding f (x i ) of each point x i by minimizing</p><formula xml:id="formula_0">U i,j=1 L(f (x i , α), f (x j , α), W ij ) w.r.t. α, subject to Balancing constraint.</formula><p>This type of optimization problem has the following main ingredients:</p><p>• f (x) ∈ R n is the embedding one is trying to learn for a given example x ∈ R d . It is parametrized by α. In many techniques f (x i ) = f i is a lookup table where each example i is assigned an independent vector f i .</p><p>• L is a loss function between pairs of examples.</p><p>• The matrix W of weights W ij specifying the similarity or dissimilarity between examples x i and x j . This is supplied in advance and serves as a kind of label for the loss function.</p><p>• A balancing constraint is often required for certain objective functions so that a trivial solution is not reached.</p><p>Many well known algorithms fit into this framework.</p><p>Multidimensional scaling (MDS) is a classical algorithm that attempts to preserve the distance between points, whilst embedding them in a lower dimensional space, e.g. by using the loss function</p><formula xml:id="formula_1">L(f i , f j , W ij ) = (||f i − f j || − W ij ) 2</formula><p>MDS is equivalent to PCA if the metric is Euclidean <ref type="bibr" target="#b21">(Williams, 2001)</ref>.</p><p>ISOMAP <ref type="bibr" target="#b19">(Tenenbaum et al., 2000)</ref> is a nonlinear embedding technique that attempts to capture manifold structure in the original data. It works by defining a similarity metric that measures distances along the manifold, e.g. W ij is defined by the shortest path on the neighborhood graph. One then uses those distances to embed using conventional MDS.</p><p>Laplacian Eigenmaps <ref type="bibr" target="#b2">(Belkin &amp; Niyogi, 2003)</ref> learn manifold structure by emphasizing the preservation of local distances. One defines the distance metric between the examples by encoding them in the Laplacian L = W − D, where D ii = j W ij is diagonal. Then, the following optimization is used:</p><formula xml:id="formula_2">ij L(f i , f j , W ij ) = ij W ij ||f i − f j || 2 = f Lf (1)</formula><p>subject to the balancing constraint:</p><formula xml:id="formula_3">f Df = I and f D1 = 0.</formula><p>(2)</p><p>Siamese Networks <ref type="bibr" target="#b5">(Bromley et al., 1993)</ref> are also a classical method for nonlinear embedding. Neural networks researchers think of such models as a network with two identical copies of the same function, with the same weights, fed into a "distance measuring" layer to compute whether the two examples are similar or not, given labeled data. In fact, this is exactly the same as the formulation given at the beginning of this Section.</p><p>Several loss functions have been proposed for siamese networks, here we describe a margin-based loss proposed by the authors of <ref type="bibr" target="#b11">(Hadsell et al., 2006)</ref>:</p><formula xml:id="formula_4">L(f i , f j , W ij ) = ||f i − f j || 2 if W ij = 1, max(0, m − ||f i − f j || 2 ) if W ij = 0</formula><p>(3) which encourages similar examples to be close, and dissimilar ones to have a distance of at least m from each other. Note that no balancing constraint is needed with such a choice of loss as the margin constraint inhibits a trivial solution. Compared to using constraints like (2) this is much easier to optimize by gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-Supervised Algorithms</head><p>Several semi-supervised classification algorithms have been proposed which take advantage of the algorithms described in the last section. Here we assume the setting where one is given L + U examples x i , but only the first L have a known label y i .</p><p>Label Propagation <ref type="bibr" target="#b22">(Zhu &amp; Ghahramani, 2002)</ref> adds a Laplacian Eigenmap type regularization to a nearest-neighbor type classifier:</p><formula xml:id="formula_5">min f L i=1 ||f i − y i || 2 + λ L+U i,j=1 W ij ||f i − f j || 2 (4)</formula><p>The algorithm tries to give two examples with large weighted edge W ij the same label. The neighbors of neighbors tend to also get the same label as each other by transitivity, hence the name label propagation.</p><p>LapSVM <ref type="bibr" target="#b3">(Belkin et al., 2006)</ref> uses the Laplacian Eigenmaps type regularizer with an SVM: minimize</p><formula xml:id="formula_6">||w|| 2 + γ L i=1 H(y i f (x i )) + λ L+U i,j=1 W ij ||f (x i ) − f (x j )|| 2</formula><p>(5) where H(x) = max(0, 1 − x) is the hinge loss.</p><p>Other Methods In <ref type="bibr" target="#b8">(Chapelle &amp; Zien, 2005)</ref> a method called graph is suggested which combines a modified version of ISOMAP with an SVM. The authors also suggest to combine modified ISOMAP with TSVMs rather than SVMs, and call it Low Density Separation (LDS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-supervised Embedding for Deep Learning</head><p>We would like to use the ideas developed in semisupervised learning for deep learning. Deep learning consists of learning a model with several layers of nonlinear mapping. In this article we will consider multilayer networks with M layers of hidden units that give a C-dimensional output vector:</p><formula xml:id="formula_7">f i (x) = d j=1 w O,i j h M j (x) + b O,i , i = 1, . . . , C<label>(6)</label></formula><p>where w O are the weights for the output layer, and typically the k th layer is defined as</p><formula xml:id="formula_8">h k i (x) = S j w k,i j h k−1 j (x) + b k,i , k &gt; 1 (7) h 1 i (x) = S j w 1,i j x j + b 1,i<label>(8)</label></formula><p>and S is a non-linear squashing function such as tanh.</p><p>Here, we describe a standard fully connected multilayer network but prior knowledge about a particular problem could lead one to other network designs. For example in sequence and image recognition time delay and convolutional networks (TDNNs and CNNs) <ref type="bibr">(Le-Cun et al., 1998)</ref> have been very successful. In those approaches one introduces layers that apply convolutions on their input which take into account locality information in the data, i.e. they learn features from image patches or windows within a sequence.</p><p>The general method we propose for semi-supervised deep learning is to add a semi-supervised regularizer in deep architectures in one of three different modes, as shown in Figure <ref type="figure" target="#fig_0">1</ref>:</p><formula xml:id="formula_9">OUTPUT OUTPUT INPUT INPUT Embedding Space LAYER 1 LAYER 2 LAYER 3 OUTPUT OUTPUT INPUT INPUT LAYER 1 LAYER 2 LAYER 3</formula><p>Embedding Space (a) Add a semi-supervised loss (regularizer) to the supervised loss on the entire network's output (6):</p><formula xml:id="formula_10">L i=1 (f (x i ), y i ) + λ L+U i,j=1 L(f (x i ), f (x j ), W ij ) (9)</formula><p>This is most similar to the shallow techniques described before, e.g. equation ( <ref type="formula">5</ref>).</p><p>(b) Regularize the k th hidden layer (7) directly:</p><formula xml:id="formula_11">L i=1 (f (x i ), y i ) + λ L+U i,j=1 L(f k (x i ), f k (x j ), W ij ) (10) where f k (x) = (h k 1 (x), . . . , h k N k (x)</formula><p>) is the output of the network up to the k th hidden layer.</p><p>(c) Create an auxiliary network which shares the first k layers of the original network but has a new final set of weights:</p><formula xml:id="formula_12">g i (x) = j w j h k j (x) + b AUX,i<label>(11)</label></formula><p>We train this network to embed unlabeled data simultaneously as we train the original network on labeled data.</p><p>In our experiments we use the loss function (3) for embedding, and the hinge loss</p><formula xml:id="formula_13">(f (x), y) = C c=1 H(y(c)f c (x)),</formula><p>Algorithm 1 EmbedNN Input: labeled data (x i , y i ), i = 1, . . . , L, unlabeled data x i , i = L + 1, . . . , U , set of functions f (•), and embedding functions g k (•), see Figure <ref type="figure" target="#fig_0">1</ref> and equations ( <ref type="formula">9</ref>), ( <ref type="formula">10</ref>) and ( <ref type="formula" target="#formula_12">11</ref>). repeat Pick a random labeled example (x i , y i ) Make a gradient step to optimize (f (x i ), y i ) for each embedding function g k (•) do Pick a random pair of neighbors x i , x j . Make a gradient step for λL(g k (x i ), g k (x j ), 1) Pick a random unlabeled example x n . Make a gradient step for λL(g k (x i ), g k (x n ), 0) end for until stopping criteria is met.</p><p>for labeled examples, where y(c) = 1 if y = c and -1 otherwise. For neighboring points, this is the same regularizer as used in LapSVM and Laplacian Eigenmaps. For non-neighbors, where W ij = 0, this loss "pulls" points apart, thus inhibiting trivial solutions without requiring difficult constraints such as (2). To achieve an embedding without labeled data the latter is necessary or all examples would collapse to a single point in the embedding space. We therefore prefer this regularizer to using (1) alone. Pseudocode of our approach is given in Algorithm 1.</p><p>Labeling unlabeled data as neighbors Training neural networks online using stochastic gradient descent is fast and can scale to millions of examples. A possible bottleneck with our approach is computation of the matrix W , that is, computing which unlabeled examples are neighbors and have value W ij = 1. Embedding algorithms often use k-nearest neighbor for this task, and although many methods for its fast computation do exist, this could still be slower than we would like. One possibility is to approximate it with sampling techniques.</p><p>However, there are also many other ways of collecting neighboring unlabeled data, notably if one is given sequence data such as in audio, video or text problems. For example, one can take images from two consecutive frames of video as a neighboring pair with W ij = 1. Such pairs are likely to have the same label, and are collected cheaply. In Section 5 we apply this kind of idea to text and train a semi-supervised semantic role labeler using an unlabeled set of 631 million words.</p><p>When do we expect this approach to work? One can see our approach as an instance of multi-task learning <ref type="bibr" target="#b6">(Caruana, 1997)</ref> using unsupervised auxiliary tasks. In common with other semi-supervised learning approaches, and indeed other deep learning approaches, we only expect this to work if p(x) useful for the supervised task p(y|x), i.e. if the structure assumption is true. We believe many natural tasks have this property.</p><p>We note that an alternative multi-task learning scheme is presented in <ref type="bibr" target="#b1">(Ando &amp; Zhang, 2005)</ref> and applied to neural networks in <ref type="bibr" target="#b0">(Ahmed et al., 2008)</ref> which instead constructs auxiliary supervised tasks from unlabeled data by constructing tasks with labels y * . This is useful when p(y * |x) is correlated to p(y|x), however an expert must engineer a useful target y * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Existing Approaches to Deep Learning</head><p>Hinton and coworkers ( <ref type="formula">2006</ref>) proposed the Deep Belief Net (DBN) which is a multi-layered network first trained as a generative model with unlabeled data before being subsequently trained in supervised mode. It is based around iteratively training Restricted Boltzmann machines (RBMs) for each layer. An RBM is a two-layer network in which visible, binary stochastic pixels v are connected to hidden binary stochastic feature detectors h. The probability assigned to an example x is:</p><formula xml:id="formula_14">P (x) = h∈H P (x, h) = h∈H e −E(x,h) Z E(x, h) = − i∈pixels w P i v i − j∈features w F j h j − i,j v j h j w ij</formula><p>The idea is to obtain large values for the training examples, and small values elsewhere just as in any maximum likelihood density estimator. This is trained with a procedure called contrastive divergence whereby one pushes up the energy on training data and pushes down the energy on samples generated by the model. The authors used this method to pretrain a deep neighborhood component analysis model (DBN-NCA) and a regularized version that simultaneously trains an autoencoder (DBN-rNCA) <ref type="bibr" target="#b17">(Salakhutdinov &amp; Hinton, 2007)</ref>.</p><p>The authors of <ref type="bibr" target="#b4">(Bengio et al., 2007)</ref> suggested a simpler scheme: define an autoencoder that given an input x tries to encode it in a low dimensional space z = f enc (x), and then decode it again to reproduce it as well as possible, e.g. so that</p><formula xml:id="formula_15">||x − f dec (f enc (x))|| 2</formula><p>is small. (Actually you can also view RBMs in this way, see <ref type="bibr" target="#b16">(Ranzato et al., 2007)</ref>.) The idea is to use an autoencoder as a regularizer which is trained on unlabeled data. If the autoencoder is linear it corresponds to PCA <ref type="bibr" target="#b13">(Japkowicz et al., 2000)</ref> and hence also MDS, making a clear link to the embedding algorithms we discussed in Section 2.1. The authors claim that autoassociators have the advantage "that almost any parametrizations of the layers are possible, as long as the training criterion is continuous in the parameters [...] the class of probabilistic models for which [DBNs] can be applied is currently more limited."</p><p>Finally, recently the authors of <ref type="bibr" target="#b16">(Ranzato et al., 2007)</ref> introduced another method of deep learning which also amounts to a kind of encoder/decoder architecture, called SESM. In this case they choose to learn large, sparse codes as they believe these are good for classification. They choose an encoder f enc (x) = w x + b enc and a decoder with shared weights f dec (z) = wS(z) + b dec . They then optimize the following loss:</p><formula xml:id="formula_16">α e ||z − f enc (x)|| 2 2 + ||x − f dec (z)|| 2 2 + α s h(z) + α r ||w|| 1</formula><p>where the first term makes the output of the encoder close to the code z (which is also learnt), the second term makes the decoder try to reproduce the input, and the third and fourth terms sparsify the codes z and the weights of the encoder and decoder w. α e , α s and α r are all hyperparameters. The training requires an online coordinate descent scheme because both z and w are being optimized.</p><p>We believe all of the methods just described are significantly more complicated than our approach. Our embedding approach can also be seen as an encoder f enc (x) that embeds data into a low dimensional space. However we do not need to decode during training (or indeed at all). Further, if the data is high dimensional and sparse there is a significant speedup from not having to decode.</p><p>Finally, existing approaches advocate greedy layerwise training, followed by a "fine-tuning" step using the supervised signal. The intention is that the unsupervised learning provides a better initialization for supervised learning, and hence a better final local minimum. Our approach does not use a pre-training step, but instead directly optimizes our new objective function. We advocate that it is the new choice of objective that can provide improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>We test our approach on several datasets summarized in Table <ref type="table">1</ref>.</p><p>Small-scale experiments g50c, Text and Uspst are small-scale datasets often used for semi-supervised Table <ref type="table">1</ref>. Datasets used in our experiments. The first three are small scale datasets used in the same experimental setup found in <ref type="bibr" target="#b8">(Chapelle &amp; Zien, 2005;</ref><ref type="bibr" target="#b18">Sindhwani et al., 2005;</ref><ref type="bibr" target="#b9">Collobert et al., 2006)</ref>. The following six datasets are large scale. The Mnist 1h,6h,1k,3k and 60k variants are MNIST with a labeled subset of data, following the experimental setup in <ref type="bibr" target="#b9">(Collobert et al., 2006)</ref>. SRL is a Semantic Role Labeling task <ref type="bibr" target="#b15">(Pradhan et al., 2004)</ref>  learning experiments <ref type="bibr" target="#b8">(Chapelle &amp; Zien, 2005;</ref><ref type="bibr" target="#b18">Sindhwani et al., 2005;</ref><ref type="bibr" target="#b9">Collobert et al., 2006)</ref>. We followed the same experimental setup, averaging results of ten splits of 50 labeled examples where the rest of the data is unlabeled. In these experiments we test the embedding regularizer on the output of a neural network (see equation ( <ref type="formula">9</ref>) and Figure <ref type="figure" target="#fig_0">1</ref>(a)). We define a two-layer neural network (NN) with hu hidden units.</p><p>We define W so that the 10 nearest neighbors of i have W ij = 1, and W ij = 0 otherwise. We train for 50 epochs of stochastic gradient descent and fixed λ = 1, but for the first 5 we optimized the supervised target alone (without the embedding regularizer). This gives two free hyperparameters: the number of hidden units hu = {0, 5, 10, 20, 30, 40, 50} and the learning rate lr = {0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}.</p><p>We report the optimum choices of these values optimized by 5-fold cross validation and by optimizing on the test set in Table <ref type="table">2</ref>. Note the datasets are very small, so cross validation is unreliable. Several methods from the literature optimized their hyperparameters using the test set (those that are not marked with (cv)). Our EmbedNN is competitive with stateof-the-art semi-supervised methods based on SVMs, even outperforming them in some cases.</p><p>MNIST experiments We compare our method in all three different modes (Figure <ref type="figure" target="#fig_0">1</ref>) with conventional semi-supervised learning (TSVM) using the same data split and validation set as in <ref type="bibr" target="#b9">(Collobert et al., 2006)</ref>. We also compare to several deep learning methods: RBMs, SESM and DBN-NCA and DBN-rNCA (however, they are trained on a different data split). In Table <ref type="table">2</ref>. Results on Small-Scale Datasets. We report the best test error over the hyperparameters of our method, EmbedNN, as in the methodology of <ref type="bibr" target="#b8">(Chapelle &amp; Zien, 2005)</ref> as well as the error when optimizing the parameters by cross-validation, EmbedNN (cv) . LDS (cv) and LapSVM (cv)   <ref type="figure" target="#fig_0">1</ref>). A convolutional network (CNN) is also tested in the same way. We compare to SVMs and TSVMs. RBM, SESM, DBN-NCA and DBN-rNCA (marked with ( * )) taken from <ref type="bibr" target="#b16">(Ranzato et al., 2007;</ref><ref type="bibr" target="#b17">Salakhutdinov &amp; Hinton, 2007)</ref>  NN (HUs=100) 2.0 1.9 2.0 2.2 2.3 3.0 Embed ALL NN 1.9 1.5 1.6 1.7 1.8 2.4</p><p>these experiments we consider 2-layer networks (NN) and 6-layer convolutional neural nets (CNN) for embedding. We optimize the parameters of NN ( hu = {50, 100, 150, 200, 400} hidden units and learning rates as before) on the validation set. The CNN architecture is fixed: 5 layers of image patch-type convolutions, followed by a linear layer of 50 hidden units, similar to <ref type="bibr" target="#b14">(LeCun et al., 1998)</ref>. The results given in Table <ref type="table" target="#tab_1">3</ref> show the effectiveness of embedding in all three modes, with both NNs and CNNs.   <ref type="bibr" target="#b15">(Pradhan et al., 2004)</ref> 16.54% SENNA <ref type="bibr" target="#b10">(Collobert &amp; Weston, 2007)</ref> 16  <ref type="bibr" target="#b10">(Collobert &amp; Weston, 2007)</ref> and train a 5-layer convolutional neural network for this task, where the first layer represents the input sentence words as 50dimensional vectors. Unlike <ref type="bibr" target="#b10">(Collobert &amp; Weston, 2007)</ref>, we do not give any prior knowledge to our classifier. In that work words were stemmed and clustered using their parts-of-speech. Our classifier is trained using only the original input words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deeper MNIST experiments</head><p>We attempt to improve this system by, as before, learning an auxiliary embedding task. Our embedding is learnt using unlabeled sentences from the Wikipedia web site, consisting of 631 million words in total using the scheme described in Section 3. The same lookup table of word vectors as in the supervised task is used as input to an 11 word window around a given word, yielding 550 features. Then a linear layer projects these features into a 100 dimensional embedding space. All windows of text from Wikipedia are considered neighbors, and non-neighbors are constructed by replacing the middle word in a sentence window with a random word. Our lookup table indexes the most frequently used 30,000 words, and all other words are assigned index 30,001.</p><p>The results in Table <ref type="table" target="#tab_4">6</ref> indicate a clear improvement when learning an auxiliary embedding. ASSERT <ref type="bibr" target="#b15">(Pradhan et al., 2004</ref>) is an SVM parser-based system with many hand-coded features, and SENNA is a NN which uses part-of-speech information to build its word vectors. In contrast, our system is the only state-of-the-art method that does not use prior knowledge in the form of features derived from parts-of-speech or parse tree data. application will be described in more detail in a forthcoming paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we showed how one can improve supervised learning for deep architectures if one jointly learns an embedding task using unlabeled data. Our results both confirm previous findings and generalize them. Researchers using shallow architectures already showed two ways of using embedding to improve generalization: (i) embedding unlabeled data as a separate pre-processing step (i.e., first layer training) and;</p><p>(ii) using embedding as a regularizer (i.e., at the output layer). More importantly, we generalized these approaches to the case where we train a semi-supervised embedding jointly with a supervised deep multi-layer architecture on any (or all) layers of the network, and showed this can bring real benefits in complex tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Three modes of embedding in deep architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>also use cross-validation. Results on MNIST with 100, 600, 1000 and 3000 labels. A two-layer Neural Network (NN) is compared to an NN with Embedding regularizer (EmbedNN) on the output (O), i th layer (Ii) or auxiliary embedding from the i th layer (Ai) (see Figure</figDesc><table><row><cell></cell><cell>g50c</cell><cell cols="2">Text Uspst</cell></row><row><cell>SVM</cell><cell>8.32</cell><cell>18.86</cell><cell>23.18</cell></row><row><cell>TSVM</cell><cell>5.80</cell><cell>5.71</cell><cell>17.61</cell></row><row><cell>LapSVM (cv)</cell><cell>5.4</cell><cell>10.4</cell><cell>12.7</cell></row><row><cell>LDS (cv)</cell><cell>5.4</cell><cell>5.1</cell><cell>15.8</cell></row><row><cell cols="2">Label propagation 17.30</cell><cell>11.71</cell><cell>21.30</cell></row><row><cell>Graph SVM</cell><cell>8.32</cell><cell>10.48</cell><cell>16.92</cell></row><row><cell>NN</cell><cell>10.62</cell><cell>15.74</cell><cell>25.13</cell></row><row><cell>EmbedNN</cell><cell>5.66</cell><cell>5.82</cell><cell>15.49</cell></row><row><cell>EmbedNN (cv)</cell><cell>6.78</cell><cell>6.19</cell><cell>15.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>are trained on a different data split. Full Mnist60k dataset with deep networks of 2, 6, 8, 10 and 15 layers, using either 50 or 100 hidden units. We classical NN training with Embed ALL NN where we learn an auxiliary embedding on all layers at the same</figDesc><table><row><cell></cell><cell cols="6">Mnst1h Mnst6h Mnst1k Mnst3k</cell></row><row><cell>SVM</cell><cell>23.44</cell><cell>8.85</cell><cell></cell><cell>7.77</cell><cell>4.21</cell></row><row><cell>TSVM</cell><cell>16.81</cell><cell>6.16</cell><cell></cell><cell>5.38</cell><cell>3.45</cell></row><row><cell>RBM ( * )</cell><cell>21.5</cell><cell>-</cell><cell></cell><cell>8.8</cell><cell>-</cell></row><row><cell>SESM ( * )</cell><cell>20.6</cell><cell>-</cell><cell></cell><cell>9.6</cell><cell>-</cell></row><row><cell>DBN-NCA ( * )</cell><cell>-</cell><cell>10.0</cell><cell></cell><cell>-</cell><cell>3.8</cell></row><row><cell cols="2">DBN-rNCA ( * ) -</cell><cell>8.7</cell><cell></cell><cell>-</cell><cell>3.3</cell></row><row><cell>NN</cell><cell>25.81</cell><cell cols="2">11.44</cell><cell>10.70</cell><cell>6.04</cell></row><row><cell>Embed O NN</cell><cell>17.05</cell><cell>5.97</cell><cell></cell><cell>5.73</cell><cell>3.59</cell></row><row><cell>Embed I1 NN</cell><cell>16.86</cell><cell>9.44</cell><cell></cell><cell>8.52</cell><cell>6.02</cell></row><row><cell>Embed A1 NN</cell><cell>17.17</cell><cell>7.56</cell><cell></cell><cell>7.89</cell><cell>4.93</cell></row><row><cell>CNN</cell><cell>22.98</cell><cell>7.68</cell><cell></cell><cell>6.45</cell><cell>3.35</cell></row><row><cell>Embed O CNN</cell><cell>11.73</cell><cell>3.42</cell><cell></cell><cell>3.34</cell><cell>2.28</cell></row><row><cell>Embed I5 CNN</cell><cell>7.75</cell><cell>3.82</cell><cell></cell><cell>2.73</cell><cell>1.83</cell></row><row><cell>Embed A5 CNN</cell><cell>7.87</cell><cell>3.82</cell><cell></cell><cell>2.76</cell><cell>2.07</cell></row><row><cell cols="7">Table 4. Mnist1h dataset with deep networks of 2, 6, 8, 10</cell></row><row><cell cols="7">and 15 layers; each hidden layer has 50 hidden units. We</cell></row><row><cell cols="7">compare classical NN training with EmbedNN where we</cell></row><row><cell cols="7">either learn an embedding at the output layer ( O ) or an</cell></row><row><cell cols="7">auxiliary embedding on all layers at the same time ( ALL ).</cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>15</cell></row><row><cell>NN</cell><cell cols="6">26.0 26.1 27.2 28.3 34.2 47.7</cell></row><row><cell>Embed O NN</cell><cell cols="6">19.7 15.1 15.1 15.0 13.7 11.8</cell></row><row><cell cols="3">Embed ALL NN 18.2 12.6</cell><cell>7.9</cell><cell>8.5</cell><cell>6.3</cell><cell>9.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We then conducted a similar set of experiments but with very deep architectures -up to 15 layers, where each hidden layer has 50 hidden units. Using Mnist1h, we first compare conventional NNs to Embed ALL NN where we learn an auxiliary nonlinear embedding (50 hidden units and a 10 dimensional embedding space) on each layer, as well as Embed O NN where we only embed the outputs. Results are given in Table4. When we increase the number of layers, NNs trained with conventional backpropagation overfit and yield steadily worse test error (although they are easily capable of achieving zero training error). In contrast, Embed ALL NN improves with increasing depth due to the semi-supervised "regularization". Embedding on all layers of the network has made deep learning possible. Embed O NN (embedding on the outputs) also helps, but not as much.</figDesc><table><row><cell>We also conducted some experiments using the full</cell></row><row><cell>MNIST dataset, Mnist60k. Again using deep networks</cell></row><row><cell>of up to 15 layers using either 50 or 100 hidden units</cell></row><row><cell>Embed ALL NN outperforms standard NN. Results are</cell></row><row><cell>given in Table 5. Increasing the number of hidden</cell></row><row><cell>units is likely to improve these results further, e.g. us-</cell></row><row><cell>ing 4 layers and 500 hidden units on each layer, one</cell></row><row><cell>obtains 1.27% using Embed ALL NN.</cell></row></table><note>Semantic Role Labeling The goal of semantic role labeling (SRL) is, given a sentence and a relation of interest, to label each word with one of 16 tags that indicate that word's semantic role with respect to the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>A deep architecture for Semantic Role Labeling with no prior knowledge outperforms state-of-the-art systems ASSERT and SENNA that incorporate knowledge about parts-of-speech and parse trees. A convolutional network (CNN) is improved by learning an auxiliary embedding (Embed A1 CNN) for words represented as 100dimensional vectors using the entire Wikipedia website as unlabeled data.</figDesc><table><row><cell>Method</cell><cell>Test Error</cell></row><row><cell>ASSERT</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>For example the sentence "The cat eats the fish in the pond" is labeled in the following way: "The ARG0 cat ARG0 eats REL the ARG1 fish ARG1 in ARGM −LOC the ARGM −LOC pond ARGM −LOC " where ARG0 and ARG1 effectively indicate the subject and object of the relation "eats" and ARGM-LOC indicates a locational modifier. The PropBank dataset includes around 1 million labeled words from the Wall Street Journal. We follow the experimental setup of</figDesc><table><row><cell></cell><cell>.36%</cell></row><row><cell>CNN [no prior knowledge]</cell><cell>18.40%</cell></row><row><cell>Embed A1 CNN [no prior knowledge]</cell><cell>14.55%</cell></row><row><cell>action of the relation.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Appearing in Proceedings of the 25 th International Conference on MachineLearning, Helsinki, Finland, 2008. Copyright 2008  by the author(s)/owner(s).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">This is often referred to as the cluster assumption or the manifold assumption(Chapelle et al.,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2006" xml:id="foot_2">).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to Sandra Cordero for producing the figures. Frédéric Ratle is funded by the SNF, Grant no. 105211-107862.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularization: a geometric framework for learning from Labeled and Unlabeled Examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<title level="m">Greedy layer-wise training of deep networks. Advances in Neural Information Processing Systems, NIPS 19</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sackinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multitask Learning. Machine Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semisupervised learning. Adaptive computation and machine learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass; USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2003">2003. 2005</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Cluster kernels for semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale transductive svms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1687" to="1712" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast semantic extraction using a novel neural network architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conference (CVPR&apos;06)</title>
				<meeting>Computer Vision and Pattern Recognition Conference (CVPR&apos;06)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comp</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear autoassociation is not equivalent to PCA</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shallow semantic parsing using support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL-2004</title>
				<meeting>HLT/NAACL-2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</title>
				<meeting>Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond the point cloud: from transductive to semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On a connection between kernel PCA and metric multidimensional scaling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS 13</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report CMU-CALD-02-107</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
