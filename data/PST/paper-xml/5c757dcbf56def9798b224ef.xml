<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Junping</forename><surname>Zhong</surname></persName>
							<email>zhongjunping@my.swjtu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Zhigang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhiwei</forename><surname>Han</surname></persName>
							<email>zw.han@my.swjtu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Ye</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
							<email>zhangwenxuan@rails.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Southwest Jiaotong University</orgName>
								<address>
									<postCode>610031</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Infrastructure Inspection Research Institute</orgName>
								<orgName type="department" key="dep2">Academy of Railway Sciences</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A24A9137C46983E0569A22FA2FDD4480</idno>
					<idno type="DOI">10.1109/TIM.2018.2871353</idno>
					<note type="submission">received April 25, 2018; revised July 27, 2018; accepted August 31, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Catenary support devices (CSDs)</term>
					<term>deep convolutional neural network (CNN)</term>
					<term>defect inspection</term>
					<term>high-speed railway</term>
					<term>split pins (SPs)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Split pins (SPs) play an important role in fixing joint components on catenary support devices (CSDs) of highspeed railway. The occurrence of loose and missing defects of SPs could make the structure of CSDs unstable. In this paper, we present a three-stage automatic defect inspection system for SPs mainly based on an improved deep convolutional neural network (CNN), which is called PVANET++. First, SPs are localized by PVANET++ and the Hough transform &amp; Chan-Vese model, and then, three proposed criteria are applied to detect defects of SPs. In PVANET++, a new anchor mechanism is applied to produce suitable candidate boxes for objects, and multiple hidden layer features are combined to construct discriminative hyperfeatures. The performance of PVANET++ and several recent state-of-the-art deep CNNs is compared in a data set that is collected from a 60-km rail line. The results show that our model is superior to others in accuracy, and has a considerable speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A S THE railway network is becoming heavy, the long- term impact and vibration triggered by vehicle operation could make fasteners of catenary support devices (CSDs), especially split pins (SPs), severely loose or missing. SPs play an important role in fixing joint components on CSDs whose function is mainly to maintain the conductor height and stagger of the contact line. The SPs loose or fall off would cause the structure of CSDs unstable, or even may cut off the electric power transmission from the contact line to vehicle. Therefore, it is very essential to find defects in SPs.</p><p>SPs are relatively small and can be divided into two types, i.e., Type A and Type B. Type-A SPs lie on the surface of At present, railway departments inspect defects in SPs by manually reading large amounts of images that are collected by the special inspect vehicle. Multiple high-resolution cameras and illumination compensation devices are mounted on the roof of the inspect vehicle to capture the high-resolution CSDs' image. The sketch map of the CSDs' image collecting along the railway is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The selection of the recognition method is the crucial problem in developing an automatic visual-based inspection in railway <ref type="bibr" target="#b0">[1]</ref>. The visualbased methods for CSDs' defects' inspection usually take two steps. The component localization is implemented first, and then defect detection is followed. An accurate localization is important and difficult to achieve, especially for the tiny SPs, as an appropriate localized box that is provided for subsequent processing should contain the entire object target. For defect detection, as different types of components have different defects, detection methods or criteria are proposed according to the characteristics of defects.</p><p>In the past decade, the local invariant features based on gradient had been widely used, such as scale-invariant feature transform <ref type="bibr" target="#b1">[2]</ref>, histogram of oriented <ref type="bibr" target="#b2">[3]</ref>, and local binary patterns <ref type="bibr" target="#b3">[4]</ref>. Classifiers, such as a support vector machine and the AdaBoost algorithm <ref type="bibr" target="#b4">[5]</ref>, are adopted to realize the localization. The deformable part models <ref type="bibr" target="#b5">[6]</ref> and its variants <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> can slide through the entire image to find regions with a class-specific maximum response based on local invariant features. Most of the existing CSDs' inspection methods <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref> are tried for insulator, clevis, and other components based on local invariant features. However, these methods are still far to satisfy the rail industrial requirement as the artificial features cannot appropriately describe an object and the network usually needs to be trained separately for multiple-class localization.</p><p>A convolutional neural network (CNN) that imitated a biological vision system has shown great power in machine vision tasks. Masci et al. <ref type="bibr" target="#b12">[13]</ref> applied a Max-Pooling CNN for steel defect classification on raw segmented images, which avoided time-consuming preprocessing. Yousefi and Yousefi <ref type="bibr" target="#b13">[14]</ref> investigated the advantage of a Gabor-based CNN in recognition of human actions. Jin et al. <ref type="bibr" target="#b14">[15]</ref> trained a CNN with hinge loss to recognize traffic signs. In <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref>, CNNs have been applied to detect spindle bearing fault and magnetic flux leakage. In recent years, various algorithms based on deep CNNs have made impressive improvements in the benchmark vision competitions <ref type="bibr" target="#b17">[18]</ref>. These methods are mainly divided into two types. One follows the pipeline as the proposal generation, hierarchical structure feature extracting, and classification &amp; regression (C&amp;R). For example, a region-based convolutional neural network (RCNN) <ref type="bibr" target="#b18">[19]</ref> and a fast RCNN <ref type="bibr" target="#b19">[20]</ref>, which are trained in multiple stages and time-consuming, are proposed. Ren et al. <ref type="bibr" target="#b20">[21]</ref> proposed a Faster RCNN, which accelerates proposal generation and realizes training end to end. A 101-layer residual net (ResNet) was proposed in <ref type="bibr" target="#b21">[22]</ref>, which has a high accuracy in the classification task. Dai et al. <ref type="bibr" target="#b22">[23]</ref> adopted a fully convolutional network to accelerate 101-layer ResNet. Kim et al. <ref type="bibr" target="#b23">[24]</ref> presented a deep but lightweight system, named PVANET. The other is based on the regression of default boxes, such as a single-shot multibox detector (SSD) <ref type="bibr" target="#b24">[25]</ref> and you only look once <ref type="bibr" target="#b25">[26]</ref>, and they can achieve the real-time localization in some situations. For railway industry, several deep learning studies have been investigated. In <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref>, deep CNNs were applied on rail track inspection, rail fasteners, and isoelectric line defects' detection. However, there is scarcely any related work that implements CNN for the SPs' defects' inspection.</p><p>PVANET <ref type="bibr" target="#b23">[24]</ref> has discriminative features, which owes to the deep network architecture and the concatenated features that are from medium and high layers. In addition, a concatenated rectified linear unit <ref type="bibr" target="#b30">[31]</ref> block is applied in low layers to reduce computational cost. These characteristics make PVANET accurate and fast in object localization. Based on the PVANET, we propose a network called PVANET++ to realize the localization of SPs. The accuracy can be improved from two aspects as follows.</p><p>1) More discriminative hyperfeatures are constructed in the feature extraction network (FEN) by exploiting natural information from a low layer. 2) A new anchor mechanism that associates with the scales and shapes of CSDs' components is utilized to generate higher quality proposals in the region proposal network (RPN). For defect detection, a direct application of the classification deep CNN on the localized SP areas may be a potential good solution. However, robust classifiers cannot be trained with limited defect samples of CSDs' component. As it is known, the success of the deep CNN in classification tasks is contributed to a large-scale data set. In this paper, we apply the deep CNN and the Hough transform &amp; Chan-Vese model (HT&amp;CVM) to further locate some specific parts of SPs on the localized SP areas, and then three proposed criteria are used to distinguish the states of SPs.</p><p>We summarize the contributions of this paper as follows. 1) A three-stage automatic defect inspection method is proposed for small SPs in high-speed railway. 2) For localization, a more accurate deep learning model is proposed. The proposed PVANET++ achieves the state-of-the-art localization accuracy when compared with several recent competitive deep CNNs. 3) For defects' detection, we proposed three criteria to distinguish the complex states of SPs. 4) The speed evaluation indicates that our three-stage method has a considerable speed. The rest of this paper is organized as follows. Section II presents an overview of our inspection method. Section III describes the localization of SP areas, and illustrates how our deep learning model performs as well as its improvements. Section IV presents the defects' detection criteria for all types of SPs. Section V gives experiments to evaluate the performance of our PVANET++ and several recent most competitive deep CNNs. Performance evaluation of the entire three-stage system is conducted subsequently. Finally, Section VI draws some conclusions and gives some suggestions to further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW OF OUR METHOD</head><p>As SPs are relatively small in the captured 6600 × 4400 pixels image, a few useful features or a little information can be learned if the raw images are directly processed. Therefore, our approach consists of three stages, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We describe them as follows.</p><p>1) Stage One: The first PVANET++ is applied on a 6600× 4400 raw input image to localize joint components (clevis, dt_up, and dt_down). The localized results are marked by blue boxes. They are cropped and sent to the next PVANET++ in stage two. 2) Stage Two: The second PVANET++ is applied to predict pin_areas in the cropped joint component images. The pin_areas are small local regions of joint components that contain SPs, and they can be divided into four classes according to their positions. The localized pin_areas are cropped and sent to the third stage. 3) Stage Three: For Type-A SPs, the cropped pin_area1, pin_area2, and pin_area3 are sent to the third PVANET++ to localize the specific parts head, body, and tail. The definitions of these specific parts will be introduced in Section V. Due to the occlusion issue caused by bolt in pin_area3, two different criteria are adopted to detect Type-A1 and Type-A2 SPs defects, respectively. For Type-B SPs, the rotation of SPs leads to complex SPs states in the 2-D image.</p><p>An HT&amp;CVM block is utilized to localize some specific parts in the cropped pin_area4, and then the third criterion is applied for defects' detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LOCALIZATION OF SP AREAS</head><p>To localize SPs in appropriate areas for defect detection, two PVANETs++ are successively utilized. The first PVANET++ is used to localize three kinds of joint components and the second one is used to further localize SP areas in each joint component image.</p><p>The original PVANET <ref type="bibr" target="#b23">[24]</ref> belongs to the first-type deep CNNs, which are introduced in Section I. It consists of three modules, FEN, region proposal generation (RPG), and C&amp;R. FEN learns discriminative features named hyperfeatures, and then RPG generates proposals [region of interests (ROIs)] by classifying and regressing the initial boxes, which are produced by the anchor mechanism. Finally, the localization is realized by classifying and regressing proposals in C&amp;R. To get a better localization performance in the original PVANET, we focus on obtaining higher quality proposals in RPG and a more discriminative feature representation in an FEN. Thus, some improvements in two aspects are made.</p><p>1) Inspired by HyperNet <ref type="bibr" target="#b31">[32]</ref>, we integrate low-layer features conv2_3 into the hypermaps in FEN, as shallow maps have relative high resolution and contain natural information that may help to make the hyperfeatures more discriminative. 2) In RPG, anchors are important as they provide the initial boxes of proposals. Even though the anchor mechanisms in the Faster RCNN <ref type="bibr" target="#b20">[21]</ref> and the PVANET <ref type="bibr" target="#b23">[24]</ref> have the strong generalization ability, a new anchor mechanism for our CSDs data set may help to improve the proposal quality.</p><p>A. PVANET++ 1) Structure: The basic framework of PVANET++ is illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>. a) Feature Extraction Network: An FEN comprises 16 convolution layers (from conv1_1 to conv5_4) and one pooling layer (pool1_1), and more detailed specifications of FEN's  layers can be found in Table <ref type="table" target="#tab_0">I</ref> and <ref type="bibr" target="#b23">[24]</ref>. The input image is set to 900 × 600 and 300 × 200 at stage one and stage two, respectively. The m × n input image is forwarded through the convolutional layers or the pooling layer, and activation maps are produced sequentially. Due to the hierarchical structure of an FEN, we conjecture that information of interest is distributed not only on the high and middle convolutional layers, but also on the low convolutional layer, as low-layer feature maps have relative high resolution and contain natural information, which may help to describe the object better. Thus, we integrate the shallow feature produced by conv2_3 into the concatenation, and construct the hyperfeatures, which combine the highly semantic features, intermediate but complementary features, and shallow but natural information. b) Region Proposal Generation: This module provides some proposals (ROIs) for C&amp;R. RPN is the main part of RPG, and it operates a spatial 3 × 3 sliding window over the Hyperfeature maps. At each sliding-window location, anchor mechanism is applied to generate initial boxes of proposals, and the box features that corresponding to Hyperfeatures are mapped to a vector. Then, the vector is fed into the softmax layer to estimate the probabilities (object or background), and the regressor layer to predict coordinate offsets to the ground-truth, respectively. We select the top 300 regressed reference boxes on the probability score ranking list as proposals.</p><p>Aspect ratios and scales of anchors provide initial box shapes and sizes for an object, respectively. The anchor mechanism in Faster R-CNN is effective in the benchmark Fig. <ref type="figure">6</ref>. Top: aspect ratios in original PVANET. Bottom: new aspect ratios.</p><p>data set <ref type="bibr" target="#b17">[18]</ref>, in which objects have various shapes and poses. A more abundant aspect rations and scales may help to produce higher quality proposals, but not always, which will be verified in Section V. As shown in Fig. <ref type="figure">6</ref>, the original PVANET anchor ratios are 0.33, 0.5, 0.67, 1.0, 1.5, 2.0, and 3.0, with multiple scales 32, 48, 80, 144, 256, and 512. However, the boxes of the CSDs' components' areas in this paper are neither extreme "slim &amp; tall" nor "short &amp; oblate," and they are likely to be square or distorted to some extent. Thus, the interval between the adjacent ratios is reduced and new aspect ratios as 0.5, 0.57, 0.67, 0.75, 1.0, 1.33, 1.5, 1.75, and 2.0 are presented. For the scales, they are updated to be 32, 48, 80, 112, 144, and 192, which remove two unsuited extreme cases and diversify the scales in the middle range. Actually, the RPN in PVANET++ applies 54 anchors at each sliding-window location. Some proposals that are adjusted from anchors may be highly overlapped with each other, but nonmaximum suppression (NMS) can help to reduce such redundancy. c) Classification and Regression: The category and position of a proposal are predicted in this module. The features of a proposal on 384 channels of hypermaps are pooled into a 6 × 6 × 384 tensor by ROI pooling, and then the tensor passes through a sequence of fully connected layers of "4096-4096-6 -[(k + 1) + 4(k + 1)]" output nodes. "k + 1" outputs are generated by a softmax function, and they have a distribution of probabilities ( p 0 , . . . , p k-1 , p k ), which are the probabilities that a proposal belongs to one category (there are k categories in total) or background. "4(k + 1)" outputs are produced by a smooth function, and they are the predicted box offsets relative to the coordinates of proposal. The category label and offsets that corresponding to the maximum probability are assigned to the test proposal. Then NMS is applied to all predictions to reduce redundancy and generate the final predicted boxes, whose form is given as follows:</p><formula xml:id="formula_0">Predicted box : [P conf , (x 1 , y 1 , x 2 , y 2 )] (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where P conf is the confidence of the predicted category and (x 1 , y 1 , x 2 , y 2 ) are the coordinates of the predicted box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Training Loss Function:</head><p>The PVANET++ is trained end to end by stochastic gradient descent with multitask loss, and the loss function is expressed as follows:</p><formula xml:id="formula_2">Loss = L RPG ({ p i }, {t i }) + L DET ({P j }, {T j })<label>(2)</label></formula><p>which combines the loss in RPG and the detection loss in C&amp;R.</p><p>The loss in RPG is expressed as</p><formula xml:id="formula_3">L RPG ({ p i }, {t i }) = 1 N cls i L cls p i , p * i + λ 1 N reg i p * i L reg t i , t * i (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where i is the index of an anchor in a mini-batch and p i is the predicted probability of anchor i being an object. The ground-truth label p * i is 1 if the anchor is positive, and it is 0 if the anchor is negative. t i and t * i are vectors that represent the parameterized coordinates of the proposal and the parameterized coordinates of the ground-truth box that relate with a positive anchor. λ is set to 10 to balance the two losses. N cls and N reg are the normalize factors.</p><p>The detection loss in C&amp;R is shown below</p><formula xml:id="formula_5">L DET ({P j }, {T j }) = 1 N cls j L cls P j , P * j + λ 1 N reg j P * j L reg T j , T * j (4)</formula><p>in which j is the index of a proposal and P j is the predicted probability of proposal j belonging to one class; P * j and T * j are the ground-truth class label and normalized ground-truth coordinates, respectively. T j is the normalized predicted coordinates; λ is set to 1. N cls and N reg are the normalized factors. Both the L cls and L reg functions used in formulas (3) and ( <ref type="formula">4</ref>) follow the literature <ref type="bibr" target="#b20">[21]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Localization Demos of the First Two Stages</head><p>Localization demos at the first two stages are shown in Fig. <ref type="figure" target="#fig_5">7</ref>(a) and (b), respectively. The performance evaluation scheme on large amounts of images for each stage will be presented in detail in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEFECTS DETECTION</head><p>The defects of SPs can be classified into different forms based on severity. However, except for the missing state, there is no rigid distinction between these forms, since the loosening of SPs is a gradual growth process. We roughly define three states as normal, loose, and missing, and assign some specific parts as head, body, tail, and virtual tail, as shown in Figs. <ref type="figure" target="#fig_6">8</ref> and<ref type="figure" target="#fig_7">9</ref>. For defects' detection, applying a classification deep CNN to the pin_areas may be a potential solution to all types of SPs. However, the number of defective SP samples for different possible modes is small. Overfitting may occur when the classifier is learned with limited samples, since the success of CNNs in image classification tasks is contributed to the large-scale image set. Fortunately, distributions of the specific parts in pin_areas are distinguishable for most states of SPs, and we can still take advantage of the deep CNN to localize the specific parts of Type-A SPs quickly and accurately. For Type-B SPs, as shown in Fig. <ref type="figure" target="#fig_8">10</ref> (left), due to the rotation of SPs, the HT&amp;CVM block is applied to localize SPs parts. After localization, the defects of all types SPs are detected according to the distribution of localized specific parts.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detection for Type-A SPs</head><p>The SPs are obvious in small pin_areas that are generated at stage two. For Type-A1 SPs, as shown in Fig. <ref type="figure" target="#fig_6">8</ref>, the existence conditions of head, body, tail, and virtual tail are different for different states. Combining the existence conditions with the distance characteristic between the head and the body, each state could be distinguished by the criterion 1, as shown in Table <ref type="table" target="#tab_0">II</ref>.</p><p>In Table <ref type="table" target="#tab_0">II</ref>, H, B, T, and VT are short for the localized parts head, body, tail, and virtual tail, respectively. d is the distance between the head and the body, and L is diagonal length of the body, as shown in Fig. <ref type="figure" target="#fig_6">8</ref>. α is an important parameter, and we set it to 1.4 empirically.</p><p>For Type-A2 SPs, as shown in Fig. <ref type="figure" target="#fig_7">9</ref>, the partial occlusion may occur in some situations. Thus, we distinguish each state mode by using criterion 2, as shown in Table <ref type="table" target="#tab_0">III</ref>.</p><p>The presented two criteria are based on the high precision and recall of the PVANET++ model, which would be verified in Fig. <ref type="figure" target="#fig_13">16</ref>. We utilize PVANET++ that is described in Section III as the deep CNN model to realize the specific parts' localization, and the input is set to 160 ×160. The localization demos of Type A are shown in Fig. <ref type="figure" target="#fig_9">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Defects Detection for Type-B SPs</head><p>As shown in Fig. <ref type="figure" target="#fig_8">10</ref>, the Type-B SP may rotate on its axis and around the bolt and some important discriminative information lacked on our 2-D CSD image, which leads to   the complex states. It seems unpractical to present a uniform criterion for such comprehensive defects' detection. In this section, Hough transform (HT) <ref type="bibr" target="#b32">[33]</ref> is used to separate the SPs parts and the bolt part from the foreground, and then the Chan-Vese model <ref type="bibr" target="#b33">[34]</ref> (CVM) is used to obtain the separated parts' areas. Finally, a criterion is applied to detect the suspected missing state based on the results of HT&amp;CVM. The procedures are introduced as follows.</p><p>Step 1: Enhance the image and then set it to negation.</p><p>Step 2: Adopt HT to extract the first 15 maximums in accumulation cells, which are produced in an angle range [-5.5 • , 5 • ]. Cluster those 15 maximums into two groups, as shown in Fig. <ref type="figure" target="#fig_10">12(a)</ref>, and take the average length ρ a and angle θ a in each group as the corresponding approximate vertical straight lines.</p><p>Step 3: The operation is similar to step 2, but the angle range is set to [70 • , 89.5 • ] and only one maximum is extracted for the approximate horizontal-line detection, as shown in Fig. <ref type="figure" target="#fig_10">12(b)</ref>.</p><p>Step 4: Separate the SPs parts and the bolt part from the foreground by the three straight lines that are extracted in steps 2 and 3.</p><p>Step 5: Use CVM to extract separated parts and obtain each area as S1, S2, and S, as shown in Fig. <ref type="figure" target="#fig_10">12(c</ref>), and then, state distinguishing follows criterion 3 in Table <ref type="table" target="#tab_4">IV</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT AND ANALYSIS</head><p>To evaluate the performance of the proposed method, experiment is implemented on a data set, which is collected from a 60-km rail line. We compare our PVANET++ with several recent competitive deep CNNs in localization, and verify the effectiveness of the entire three-stage system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Set Up 1) Data Set:</head><p>The data set used in our system comprises approximately 9210 captured images of 35 540 SPs, which were collected from two 60-km high-speed railway lines by the specific inspection vehicle. We use 4630 images in line 1 for training and 4580 images in line 2 for testing. The image numbers of training and testing set at different stages are shown in Table <ref type="table" target="#tab_4">V</ref>. Specific states of SPs in the train and test set are manually checked and labeled beforehand, and they are shown in Table <ref type="table" target="#tab_5">VI</ref>.</p><p>To build a data set that can be directly used for deep CNN models, we implement a GUI by the MATLAB tool to annotate each image in both training and testing (just required for evaluation) sets. In an annotation, each object is manually assigned a rectangle box, a category tag, and a state tag.</p><p>2) Platform: The experiment environment of all implemented deep CNNs in this paper is as follows: Deep learning framework Caffe <ref type="bibr" target="#b34">[35]</ref>, Linux Ubuntu14.04, Intel Xeon CPU E3-1230 V2 clocked at 3.3 GHz, 12-GB RAM, and GTX1080Ti GPU with 11-GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation of Deep CNNs</head><p>We compare eight deep CNN models: Faster RCNN <ref type="bibr" target="#b20">[21]</ref>, Faster RCNN1, Faster RCNN2, R-FCN ResNet-101 <ref type="bibr" target="#b22">[23]</ref>, SSD <ref type="bibr" target="#b24">[25]</ref>, PVANET <ref type="bibr" target="#b23">[24]</ref>, PVANET+, and PVANET++.   The momentum is set to 0.9, the weight decay is set to 0.0005, and the total number of iterations is 40 000. The followed two stages have the similar set as the first stage except that the total number of iterations is assigned to 30 000, as images are less complex than the first stage. The leaning rate (LR) is initialized to 0.0001, and then turns it to 0.001 after 2000 iterations to avoid nonconvergence in a short time at beginning. At the 10000th, 20000th, and 30000th (for the first stage) iteration, LR is set to be smaller by multiplying 0.5.</p><p>We show the training processes of PVANET++ at stages 1-3 in Figs. <ref type="figure" target="#fig_11">13(a</ref>)-(c), respectively. In Fig. <ref type="figure" target="#fig_11">13(a)</ref>, the train loss decreases at the 10000th and 20000th iterations, but not further decreases at the 30000th iteration. In Fig. <ref type="figure" target="#fig_11">13(b</ref>) and (c), train losses decrease at the 10000th iteration, but do not further decrease at the 20000th iteration. It indicates that even the LR is set smaller, the models cannot learn more and the train losses are converged. Thus, the models are well trained. The training processes of other seven deep CNNs are similar to 2) Testing Index: Predictions generated by the trained model are considered as true positives, false positives, or false negatives based on the predicted class labels and the overlaps with ground-truth bounding boxes. A prediction is correct when the predicted label fits the truth label and the overlap a o between the predicted bounding box B pre and the ground-truth bounding box B gt exceeds L min</p><formula xml:id="formula_6">a 0 &gt; L min , a o = area(B pre ∩ B gt ) area(B pre ∪ B gt ) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The precision and recall are described below</p><formula xml:id="formula_8">Precision = TP TP + FP × 100% (6) Recall = TP TP + FN × 100%. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>The performance of object localization task could be evaluated by the precision-recall <ref type="bibr" target="#b36">[37]</ref> (P-R) curve. The principal quantitative measure will be the average precision (AP) for each class or the mean average precision (mAP) for all classes</p><formula xml:id="formula_10">AP = P(R)d R (8) mAP = 1 N class P(R)d R. (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>To evaluate the speed performance of the deep CNN, we define the average time cost (ATC) as</p><formula xml:id="formula_12">ATC = 3 k=1 T k N k (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where k is the stage index, T k is the total time coat of deep CNN at stage k, and N k is image number of stage k.</p><p>3) Testing Results Comparison and Discussion: In testing, we only accept the predictions that have a high overlap with ground-truth, and thus, the overlap thresholds L min are set to 0.7, 0.74, and 0.78 for three stages, respectively. The P-R curves of test models at all the stages are shown in Figs. <ref type="figure" target="#fig_12">14</ref><ref type="figure" target="#fig_4">15</ref><ref type="figure" target="#fig_13">16</ref>, whose vertical coordinate ranges are set to [0.5 1.01] for clear observing. The AP of a class corresponds to the area under the P-R curve, and it can be calculated by formula <ref type="bibr" target="#b7">(8)</ref>. The mAP for all the classes can be calculated by formula <ref type="bibr" target="#b8">(9)</ref>. The detailed evaluation results are shown in Table <ref type="table" target="#tab_7">VIII</ref>. Result comparison can be conducted from Figs. 14-16 and Table <ref type="table" target="#tab_7">VIII</ref>.</p><p>1) At the stage one, the P-R curves in Fig. <ref type="figure" target="#fig_12">14</ref> show that our PVANET++ and PVANET+ have a similar performance. They perform best in clevis and dt_down, and are slightly worse than SSD in dt_up. Comparing PVANET+ with PVANET, we find that the improvement yielded by our anchor mechanism is obvious, especially in dt_up and dt_down.  2) At later two stages, the PVANET++ performs best on all pin_areas, head, and body. For all localizations of PVANET++ at these two stages, when the recalls achieve 0.95, the corresponding precision also exceeds 0.9 or higher. It indicates that PVANET++ can detect most of our labeled objects and provide reliable outputs for the criteria. The improvement that are yielded by the earlier layer feature conv2_3 can be observed from the comparison between PVANET++ and PVANET+ in Figs. <ref type="figure" target="#fig_4">15</ref> and<ref type="figure" target="#fig_13">16</ref>.</p><p>3) The precision of tail is worse than that of the head and the body. It is mainly caused by the false predictions of the virtual tail. However, we can still distinguish the tail and the virtual tail by their locations to the head and the body. Thus, we can make use of these false predictions for defects distinguishing in criteria 1 and 2, which is illustrated in Section IV-A. 4) Compared with Faster RCNN, Faster RCNN1 has more abundant aspect ratios and scales, but the mAPs of both the models are close to each other at all stages. The Faster RCNN2, which uses our anchor mechanism, just performs better than Faster RCNN and Faster RCNN1 at stage one. Thus, we conjecture that the performance improvement from the anchor mechanism changing is based on a good FEN. 5) As shown in Table VIII, our PVANET++ and PVANET+ get the highest AP for most of the classes, and the PVANET++ has the highest mAP at all three stages. In addition, the ATC of PVANET++ is 0.666 s, which is 0.042 s slower than SSD and only 0.008 s </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation of Entire System</head><p>The results of evaluation in Table <ref type="table" target="#tab_7">VIII</ref> show that our PVANET++ is superior to other compared deep CNNs in accuracy. We use the data set to further evaluate the performance of the entire three-stage system that is based on PVANET++, and the results are shown in Table <ref type="table" target="#tab_0">IX</ref>. The numbers of defect samples in our data set are illustrated in Table <ref type="table" target="#tab_5">VI</ref>, and there are 69 loose and 37 missing samples in the test data set.</p><p>As shown in Table <ref type="table" target="#tab_0">IX</ref>, defect samples at all stages are tracked during evaluation. At stage one, all missing cases are localized, and two joint components with loose SPs are localized unsuccessfully. At stage two, all defect pin_areas are localized. In the third stage, all missing cases are detected, but three loose cases are considered as normal SPs, and it may be caused by the similarity of normal and moderate loose states. The running speeds of different stages and the entire system are also evaluated (the time cost for criteria is negligible). The frames per second of the entire system is 0.43, which is a high speed. The entire system is fast, because most parts of the three-stage system are implemented by PVANET++. The evaluation results show that the proposed system is effective for SPs' defect inspection and also has a considerable speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Deep learning algorithms have shown great capacity on many fields. This paper realizes the effective automatic defect inspection for SPs mainly based on the deep CNN. The proposed PVANET++ is superior to several recent competitive deep CNNs in accuracy, and has a considerable speed. Nevertheless, there are still several issues need to be considered for further advances in this field.</p><p>1) For Type-B SPs, their states are much complex on our 2-D images due to the rotation of SPs. Some latent loose cases cannot be judged. The proposed criterion 3 can detect the suspect missing state, but some normal or loose cases may also be considered as suspect missing cases. Thus, 3-D inspection based on deep learning could be attempted to solve this issue. 2) Finer states between normal and severe loose can be judged. Hence, the railway personnel can take measures for these latent loose cases, not just for severely loose and missing cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SPs and joint components on the CSDs.</figDesc><graphic coords="1,312.59,191.69,249.98,178.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. States of different types of SPs. (a) Pin_areas. (b) Normal. (c) Severely loose. (d) Missing.</figDesc><graphic coords="2,232.43,278.81,58.70,50.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. CSDs image collection along the railway.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Workflow of our approach.</figDesc><graphic coords="3,120.11,75.77,50.06,134.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Framework of PVANET++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Localization demos of the first two stages. (a) Localization at stage one. (b) Localization at stage two.</figDesc><graphic coords="5,316.31,234.17,127.34,69.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Working states of Type-A1 SPs. (a) Normal. (b) Moderate loose. (c) Severely loose. (d) Missing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Working states of Type-A2 SPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Left: rotation of Type-B SPs. Right: line characteristic in approximate horizontal and vertical directions.</figDesc><graphic coords="6,318.95,258.41,65.64,55.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Localization demos of Type A. (a) Specific parts' location of Type A1. (b) Specific parts' location of Type A2.</figDesc><graphic coords="6,322.19,328.13,53.17,53.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Type-B SPs suspect missing detection. (a) Vertical-line detection. (b) Horizontal-line detection. (c) SPs parts' and bolt part extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Training process of PVANET++. (a) Training loss at stage 1. (b) Training loss at stage 2. (c) Training loss at stage 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. P-R curves at stage 1. (a) P-R curve of clevis. (b) P-R curve of dt_up. (c) P-R curve of dt_down.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. P-R curves at stage 3. (a) P-R curve of head. (b) P-R curve of body. (c) P-R curve of tail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SPECIFICATIONS</head><label>I</label><figDesc>OF FEN IN PVANET++</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CRITERION 1</head><label>II1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CRITERION 2</head><label>III2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CRITERION 3</head><label>IV3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V IMAGE</head><label>V</label><figDesc>NUMBERS AT DIFFERENT STAGES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI STATES</head><label>VI</label><figDesc>OF SPS IN TRAIN AND TEST SET 1) Faster RCNN: Faster RCNN is a milestone deep leaning method for object localization. It usually adopts VGG-16 [36] as the FEN, and the anchor mechanism is first proposed in Faster RCNN. PVANET++: A PVANET with our anchor mechanism, and uses the feature from layer conv2_3. It is described in Section III. As the framework of Faster RCNN is similar to PVANET++, Faster RCNN1 and Faster RCNN2 are implemented to investigate the influence of anchor mechanism changing. The anchors in each model are shown in Table VII. IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT</figDesc><table><row><cell>2) Faster RCNN1: A Faster RCNN with the anchor mech-</cell></row><row><cell>anism of PVANET.</cell></row><row><cell>3) Faster RCNN2: A Faster RCNN with our anchor</cell></row><row><cell>mechanism.</cell></row><row><cell>4) R-FCN ResNet-101: R-FCN is a two-stage object local-</cell></row><row><cell>ization method like Faster RCNN, but it removes all</cell></row><row><cell>fully connected layers in ROI-wise subnetwork and</cell></row><row><cell>constructs a fully convolutional structure to accelerate</cell></row><row><cell>training and testing. It adopts a 101-layer ResNet as the</cell></row><row><cell>backbone. The deeper network and fully convolutional</cell></row><row><cell>characteristics make R-FCN ResNet-101 be a competi-</cell></row><row><cell>tive object localization method.</cell></row><row><cell>5) SSD: The framework of SSD composes of two parts,</cell></row><row><cell>a VGG-16 as the base network and seven followed extra</cell></row><row><cell>feature layers. Unlike the other comparison methods that</cell></row><row><cell>use region proposal, SSD directly predicts both the coor-</cell></row><row><cell>dinate offsets and the confidences of all object categories</cell></row><row><cell>for each default box, which is produced at each location</cell></row><row><cell>of the multiscale feature maps of conv3_3, conv4_3,</cell></row><row><cell>conv7, conv8_2, conv9_2, conv10_2, and pool11. The</cell></row><row><cell>multiscale localization guarantees the accuracy to</cell></row><row><cell>some extent. SSD is fast, and the speed improvement of</cell></row><row><cell>SSD mainly comes from eliminating bounding box pro-</cell></row><row><cell>posals and the subsequent pixel or feature resampling.</cell></row><row><cell>6) PVANET: It is also described in Section III.</cell></row><row><cell>7) PVANET+: A PVANET with our anchor mechanism.</cell></row><row><cell>8)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII ANCHOR</head><label>VII</label><figDesc>MECHANISMS CHANGING IN FASTER RCNN 1) Training Process: Training parameters' setting in the implemented deep CNNs is the same. The parameters are set as follows.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII EVALUATION</head><label>VIII</label><figDesc>RESULTS OF THE COMPARED DEEP CNNSTABLE IX EVALUATION RESULTS IN ENTIRE SYSTEM slower than PVANET. It seems that the improvements bring scarcely time cost increase in our system.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant U1734202 and Grant U1434203, in part by the Sichuan Province Youth Science and Technology Innovation Team under Grant 2016TD0012, and in part by China Railway under Grant 2015J008-A.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Junping Zhong (S'18) received the B.S. degree in electronic and information engineering from Southwest Jiaotong University, Chengdu, China, in 2014, where he is currently pursuing the Ph.D. degree in electrical engineering.</p><p>His current research interests include image processing, computer vision, and their applications in railway fault detection.</p><p>Zhigang Liu (M'06-SM'16) received the Ph.D. degree in power system and its automation from Southwest Jiaotong University, Chengdu, China, in 2003.</p><p>He is currently a Full Professor of the School of Electrical Engineering, Southwest Jiaotong University. His research interests include the electrical relationship of EMUs and traction, and detection and assessment pantograph-catenary in high-speed railway.</p><p>Dr. Liu is a fellow of IET. He is an Editor for the IEEE TRANSACTIONS ON INSTRUMENTATION AND MEA-SUREMENT and the IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY.</p><p>Zhiwei Han (M'16) received the Ph.D. degree in power system and its automation from Southwest Jiaotong University of China in 2013. He currently a Lecturer with the School of Electrical Engineering, Southwest Jiaotong University. His current research interests include modern signal processing, computer vision, and their application in railway and electric power system. Ye Han (S'16) received the bachelor's degree in power system and its automation from Southwest Jiaotong University, Chengdu, China, in 2011, where he is currently pursuing the Ph.D. degree in electrical engineering.</p><p>His research interest includes computer vision, machine learning, and their application in railway and electric power system.</p><p>Wenxuan Zhang is currently an Assistant Research Fellow with the Infrastructure Inspection Research Institute, China Academy of Railway Sciences, Beijing, China. His current research interests include data analysis for pantograph and developing inspection equipment for the catenary system in high-speed railway.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rail inspection meets big data: Methods and trends</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">Sep. 2015</date>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An HOG-LBP human detector with partial occlusion handling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Kauai, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010-09">Sep. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection using strongly-supervised deformable part models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="836" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Condition detection of swivel clevis pins in overhead contact system of high-speed railway</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. China Railway Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="65" to="71" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fracture detection of ear pieces in catenary support devices of high-speed railway based on HOG features and two-dimensional Gabor transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. China Railway Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="57" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic recognition for catenary insulators of high-speed railway based on contourlet transform and Chan-Vese model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik-Int. J. Light Electron Opt</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="221" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic fault detection of multiple targets in railway maintenance based on time-scale normalization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="865" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Steel defect classification with max-pooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fricout</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2012.6252468</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf</title>
		<meeting>IEEE Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ABM and CNN application in ventral stream of visual system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yousefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yousefi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSBES.2015.7435920</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Student Symp</title>
		<meeting>IEEE Student Symp</meeting>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with hinge loss trained convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1991">1991-2000, Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Energy-fluctuated multiscale feature learning with deep convnet for intelligent spindle bearing fault diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1926" to="1935" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Injurious or noninjurious defect identification from MFL images in pipeline inspection using convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1883" to="1892" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">Jun./Jul. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">PVANET: deep but lightweight neural networks for real-time object detection</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.08588" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.02325" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">Jun./Jul. 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep multitask learning for railway track inspection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for detection of rail surface defects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Faghih-Roohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hajizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf. Neural Netw</title>
		<meeting>IEEE Int. Joint Conf. Neural Netw<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
			<biblScope unit="page" from="2584" to="2589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic defect detection of fasteners on the catenary support device using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nunez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="269" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A high-precision loose strands diagnosis approach for isoelectric line in high-speed railway</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Informat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1067" to="1077" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated linear units</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Int. Conf. Mach. Learn.</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3276" to="3284" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HyperNet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">Jun./Jul. 2016</date>
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Use of the Hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1972-01">Jan. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1408.5093" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The relationship between recall and precision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fredric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
