<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-05-26">26 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hoang</forename><surname>Nt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tokyo Institute of Technology</orgName>
								<orgName type="institution" key="instit2">RIKEN</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
							<email>takanori.maehara@riken.jp</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tokyo Institute of Technology</orgName>
								<orgName type="institution" key="instit2">RIKEN</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-05-26">26 May 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1905.09550v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks have become one of the most important techniques to solve machine learning problems on graph-structured data. Recent work on vertex classification proposed deep and distributed learning models to achieve high performance and scalability. However, we find that the feature vectors of benchmark datasets are already quite informative for the classification task, and the graph structure only provides a means to denoise the data. In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property. We further investigate their resilience to feature noise and propose some insights on GCN-based graph neural network design. * gihub.com/gear/gfnn Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph neural networks (GNN) belong to a class of neural networks which can learn from graphstructured data. Recently, graph neural networks for vertex classification and graph isomorphism test have achieved excellent results on several benchmark datasets and continuously set new stateof-the-art performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Started with the early success of ChebNet <ref type="bibr" target="#b5">[6]</ref> and GCN <ref type="bibr" target="#b15">[16]</ref> at vertex classification, many variants of GNN have been proposed to solve problems in social networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>, biology <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, chemistry <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, computer vision <ref type="bibr" target="#b20">[21]</ref>, and weakly-supervised learning <ref type="bibr" target="#b8">[9]</ref>.</p><p>In semi-supervised vertex classification, we observe that the parameters of a graph convolutional layer in a Graph Convolutional Network (GCN) <ref type="bibr" target="#b15">[16]</ref> only contribute to overfitting. Similar observations have been reported in both simple architectures such as SGC <ref type="bibr" target="#b27">[28]</ref> and more complex ones such as DGI <ref type="bibr" target="#b26">[27]</ref>. Based on this phenomenon, Wu et al. <ref type="bibr" target="#b27">[28]</ref> proposed to view graph neural networks simply as feature propagation and propose an extremely efficient model with state-of-the-art performance on many benchmark datasets. Kawamoto et al. <ref type="bibr" target="#b13">[14]</ref> made a related theoretical remark on untrained GCN-like GNNs under graph partitioning settings. From these previous studies, a question naturally arises: Why and when do graph neural networks work well for vertex classification? In other words, is there a condition on the vertex feature vectors for graph neural network models to work well even without training? Consequently, can we find realistic counterexamples in which baseline graph neural networks (e.g. SGC or GCN) fail to perform?</p><p>In this study, we provide an answer to the aforementioned questions from the graph signal processing perspective <ref type="bibr" target="#b18">[19]</ref>. Formally, we consider a semi-supervised learning problem on a graph. Given a graph G = (V, E), each vertex i ∈ V has a feature x(i) ∈ X and label y(i) ∈ Y, where X is a d-dimensional Euclidean space R d and Y = R for regression and Y = {1, . . . , c} for classification. The task is to learn a hypothesis to predict the label y(i) from the feature x(i). We then characterize  the graph neural networks solution to this problem and provide insights to the mechanism underlying the most commonly used baseline model GCN <ref type="bibr" target="#b15">[16]</ref>, and its simplified variant SGC <ref type="bibr" target="#b27">[28]</ref>.</p><p>Graph signal processing (GSP) regards data on the vertices as signals and applies signal processing techniques to understand the signal characteristics. By combining signals (feature vectors) and graph structure (adjacency matrix or its transformations), GSP has inspired the development of learning algorithms on graph-structured data <ref type="bibr" target="#b22">[23]</ref>. In a standard signal processing problem, it is common to assume the observations contain some noise and the underlying "true signal" has low-frequency <ref type="bibr" target="#b19">[20]</ref>.</p><p>Here, we pose a similar assumption for our problem.</p><p>Assumption 1. Input features consist of low-frequency true features and noise. The true features have sufficient information for the machine learning task.</p><p>Our first contribution is to verify Assumption 1 on commonly used datasets (Section 3). Figure <ref type="figure">1</ref> shows the performance of 2-layers perceptrons (MLPs) trained on features with different numbers of frequency components. In all benchmark datasets, we see that only a small number of frequency components contribute to learning. Adding more frequency components to the feature vectors only decreases the performance. In turn, the classification accuracy becomes even worse when we add Gaussian noise N (0, σ 2 ) to the features.</p><p>Many recent GNNs were built upon results from graph signal processing. The most common practice is to multiply the (augmented) normalized adjacency matrix I − L with the feature matrix X . The product (I − L)X is understood as features averaging and propagation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref>. In graph signal processing literature, such operation filters signals on the graph without explicitly performing eigendecomposition on the normalized Laplacian matrix, which requires O(n 3 ) time <ref type="bibr" target="#b24">[25]</ref>. Here, we refer to this augmented normalized adjacency matrix and its variants as graph filters and propagation matrices interchangeably.</p><p>Our second contribution shows that multiplying graph signals with propagation matrices corresponds to low-pass filtering (Section 4, esp., Theorem 3). Furthermore, we also show that the matrix product between the observed signal and the low-pass filters is the analytical solution to the true signal optimization problem. In contrast to the recent design principle of graph neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>, our results suggest that the graph convolution layer is simply low-pass filtering. Therefore, learning the parameters of a graph convolution layer is unnecessary. Wu et al. <ref type="bibr">[28,</ref> Theorem 1] also addressed a similar claim by analyzing the spectrum-truncating effect of the augmented normalized adjacency matrix. We extend this result to show that all eigenvalues monotonically shrink, which further explains the implications of the spectrum-truncating effect.</p><p>Based on our theoretical understanding, we propose a new baseline framework named gfNN (graph filter neural network) to empirically analyze the vertex classification problem. gfNN consists of two steps: 1. Filter features by multiplication with graph filter matrices; 2. Learn the vertex labels by a machine learning model. We demonstrate the effectiveness of our framework using a simple realization model in Figure <ref type="figure">2</ref>.</p><p>Our third contribution is the following Theorem:</p><p>Theorem 2 (Informal, see <ref type="bibr">Theorem 7,</ref><ref type="bibr" target="#b7">8)</ref>. Under Assumption 1, the outcomes of SGC, GCN, and gfNN are similar to those of the corresponding NNs using true features.</p><p>Theorem 7 implies that, under Assumption 1, both gfNN and GCN <ref type="bibr" target="#b15">[16]</ref> have similar high performance. Since gfNN does not require multiplications of the adjacency matrix during the learning phase, it is much faster than GCN. In addition, gfNN is also more noise tolerant.</p><p>Finally, we compare our gfNN to the SGC model <ref type="bibr" target="#b27">[28]</ref>. While SGC is also computationally fast and accurate on benchmark datasets, our analysis implies it would fail when the feature input is nonlinearly-separable because the graph convolution part does not contribute to non-linear manifold learning. We created an artificial dataset to empirically demonstrate this claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph Signal Processing</head><p>In this section, we introduce the basic concepts of graph signal processing. We adopt a recent formulation <ref type="bibr" target="#b10">[11]</ref> of graph Fourier transform on irregular graphs.</p><p>Let G = (V, E) be a simple undirected graph, where V = {1, . . . , n} be the set of n ∈ Z vertices and E be the set of edges. <ref type="foot" target="#foot_0">2</ref> Let A = (a ij ) ∈ R n×n be the adjacency matrix of G, D = diag(d(1), . . . , d(n)) ∈ R n×n be the degree matrix of G, where</p><formula xml:id="formula_0">d(i) = j∈V a(i, j) is the degree of vertex i ∈ V. L = D − A ∈ R n×n be the combinatorial Laplacian of G, L = I − D −1/2 AD −1/2</formula><p>be the normalized Laplacian of G, where I ∈ R n×n is the identity matrix, and L rw = I−D −1 A be the random walk Laplacian of G. Also, for γ ∈ R with γ &gt; 0, let Ã = A+γI be the augmented adjacency matrix, which is obtained by adding γ self loops to G, D = D + γI be the corresponding augmented degree matrix, and</p><formula xml:id="formula_1">L = D − Ã = L, L = I − D−1/2 Ã D−1/2 , Lrw = I − D−1 Ã</formula><p>be the corresponding augmented combinatorial, normalized, and random walk Laplacian matrices.</p><p>A vector x ∈ R n defined on the vertices of the graph is called a graph signal. To introduce a graph Fourier transform, we need to define two operations, variation and inner product, on the space of graph signals. Here, we define the variation ∆ : R n → R and the D-inner product by</p><formula xml:id="formula_2">∆(x) := (i,j)∈E (x(i) − x(j)) 2 = x Lx; (x, y) D := i∈V (d(i) + γ)x(i)y(i) = x Dy. (1)</formula><p>We denote by x D := (x, x) D the norm induced by D. Intuitively, the variation ∆ and the inner product (•, •) D specify how to measure the smoothness and importance of the signal, respectively. In particular, our inner product puts more importance on high-degree vertices, where larger γ closes the importance more uniformly. We then consider the generalized eigenvalue problem (variational form): Find u 1 , . . . , u n ∈ R n such that for each i ∈ {1, . . . , n}, u i is a solution to the following optimization problem: minimize ∆(u) subject to (u, u) D = 1, (u, u j ) D = 0, j ∈ {1, . . . , n}.</p><p>(</p><p>The solution u i is called an i-th generalized eigenvector and the corresponding objective value λ i := ∆(u i ) is called the i-th generalized eigenvalue. The generalized eigenvalues and eigenvectors are also the solutions to the following generalized eigenvalue problem (equation form):</p><formula xml:id="formula_4">Lu = λ Du.<label>(3)</label></formula><p>Thus, if (λ, u) is a generalized eigenpair then (λ, D1/2 u) is an eigenpair of L. A generalized eigenvector with a smaller generalized eigenvalue is smoother in terms of the variation ∆. Hence, the generalized eigenvalues are referred to as the frequency of the graph.</p><p>The graph Fourier transform is a basis expansion by the generalized eigenvectors. Let U = [u 1 , . . . , u n ] be the matrix whose columns are the generalized eigenvectors. Then, the graph Fourier transform F : R n → R n is defined by Fx = x := U Dx, and the inverse graph Fourier transform F −1 is defined by F −1 x = U x. Note that these are actually the inverse transforms since</p><formula xml:id="formula_5">FF −1 = U DU = I.</formula><p>The Parseval identity relates the norms of the data and its Fourier transform:</p><formula xml:id="formula_6">x D = x 2 .<label>(4)</label></formula><p>Let h : R → R be an analytic function. The graph filter specified by h is a mapping x → y defined by the relation in the frequency domain: ŷ(λ) = h(λ)x(λ). In the spatial domain, the above equation is equivalent to y = h( Lrw )x. where h( Lrw ) is defined via the Taylor expansion of h; see <ref type="bibr" target="#b12">[13]</ref> for the detail of matrix functions.</p><p>In a machine learning problem on a graph, each vertex i ∈ V has a d-dimensional feature x(i) ∈ R d . We regard the features as d graph signals and define the graph Fourier transform of the features by the graph Fourier transform of each signal. Let X = [x(1); . . . , x(n)] be the feature matrix. Then, the graph Fourier transform is represented by FX = X =: U DX and the inverse transform is</p><formula xml:id="formula_7">F −1 X = U X.</formula><p>We denote X = [x(λ 1 ); . . . ; x(λ n )] as the frequency components of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evidence of Assumption 1</head><p>The results of this paper deeply depend on Assumption 1. Thus, we first verify this assumption in realworld datasets: Cora, Citeseer, and Pubmed <ref type="bibr" target="#b21">[22]</ref>. These are citation networks, in which vertices are scientific papers and edges are citations. We consider the following experimental setting:  The annotated "(best)" horizontal lines show the performance of the best performance using gfNN and a 2-layers MLP trained on the original features.</p><p>In Figure <ref type="figure" target="#fig_2">3</ref>, we incrementally add normalized Laplacian frequency components to reconstruct feature vectors and train a 2-layers MLPs. We see that all three datasets exhibit a low-frequency nature.</p><p>The classification accuracy of a 2-layers MLP tends to peak within the top 20% of the spectrum (green boxes). By adding artificial Gaussian noise, we observe that the performance at low-frequency regions is relatively robust, which implies a strong denoising effect.</p><p>Interestingly, the performance gap between graph neural network and a simple MLP is much larger when high-frequency components do not contain useful information for classification. In the Cora dataset, high-frequency components only decrease classification accuracy. Therefore, our gfNN outperformed a simple MLP. Citeseer and Pubmed have a similar low-frequency nature. However, the performance gaps here are not as large. Since the noise-added performance lines for Citeseer and Pubmed generally behave like the original Cora performance line, we expect the original Citeseer and Pubmed contain much less noise compared with Cora. Therefore, we could expect the graph filter degree would have little effect in Citeseer and Pubmed cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multiplying Adjacency Matrix is Low Pass Filtering</head><p>Computing the low-frequency components is expensive since it requires O(|V| 3 ) time to compute the eigenvalue decomposition of the Laplacian matrix. Thus, a reasonable alternative is to use a low-pass filter. Many papers on graph neural networks iteratively multiply the (augmented) adjacency matrix Ãrw (or Ã) to propagate information. In this section, we see that this operation corresponds to a low-pass filter.</p><p>Multiplying the normalized adjacency matrix corresponds to applying graph filter h(λ) = 1−λ. Since the eigenvalues of the normalized Laplacian lie on the interval [0, 2], this operation resembles a bandstop filter that removes intermediate frequency components. However, since the maximum eigenvalue of the normalized Laplacian is 2 if and only if the graph contains a non-trivial bipartite graph as a connected component <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Lemma 1.7]</ref>. Therefore, for other graphs, multiplying the normalized (non-augmented) adjacency matrix acts as a low-pass filter (i.e., high-frequency components must decrease).</p><p>We can increase the low-pass filtering effect by adding self-loops (i.e., considering the augmented adjacency matrix) since it shrinks the eigenvalues toward zero as follows. <ref type="foot" target="#foot_1">3</ref>Theorem 3. Let λ i (γ) be the i-th smallest generalized eigenvalue of ( D, L) = (D + γI). Then, λ i (γ) is a non-negative number, and monotonically non-increasing in γ ≥ 0. Moreover, λ i (γ) is strictly monotonically decreasing if λ i (0) = 0.</p><p>Note that γ cannot be too large. Otherwise, all the eigenvalues would be concentrated around zero, i.e., all the data would be regarded as "low-frequency"; hence, we cannot extract useful information from the low-frequency components.</p><p>The graph filter h(λ) = 1 − λ can also be derived from a first-order approximation of a Laplacian regularized least squares <ref type="bibr" target="#b2">[3]</ref>. Let us consider the problem of estimating a low-frequency true feature {x(i)} i∈V from the observation {x(i)} i∈V . Then, a natural optimization problem is given by</p><formula xml:id="formula_8">min x i∈V x(i) − x(i) 2 D + ∆( X)<label>(5)</label></formula><p>where</p><formula xml:id="formula_9">∆( X) = d p=1 ∆(x(i) p ). Note that, since ∆( X) = λ∈Λ λ x(λ) 2 2</formula><p>, it is a maximum a posteriori estimation with the prior distribution of x(λ) ∼ N (0, I/λ). The optimal solution to this problem is given by X = (I + Lrw ) −1 X. The corresponding filter is h (λ) = (1 + λ) −1 , and hence h(λ) = 1 − λ is its first-order Taylor approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Bias-Variance Trade-off for Low Pass Filters</head><p>In the rest of this paper, we establish theoretical results under Assumption 1. To be concrete, we pose a more precise assumption as follows. Assumption 4 (Precise Version of the First Part of Assumption 1). Observed features {x(i)} i∈V consists of true features {x(i)} i∈V and noise {z(i)} i∈V . The true features X have frequency at most 0 ≤ 1 and the noise follows a white Gaussian noise, i.e., each entry of the graph Fourier transform of Z independently identically follows a normal distribution N (0, σ 2 ).</p><p>Using this assumption, we can evaluate the effect of the low-pass filter as follows. Lemma 5. Suppose Assumption 4. For any 0 &lt; δ &lt; 1/2, with probability at least 1 − δ, we have</p><formula xml:id="formula_10">X − Ãk rw X D ≤ √ k X D + O log(1/δ)R(2k) E[ Z D ],<label>(6)</label></formula><p>where R(2k) is a probability that a random walk with a random initial vertex returns to the initial vertex after 2k steps.</p><p>The first and second terms of the right-hand side of ( <ref type="formula" target="#formula_10">6</ref>) are the bias term incurred by applying the filter and the variance term incurred from the filtered noise, respectively. Under the assumption, the bias increases a little, say, O( √ ). The variance term decreases in O(1/deg k/2 ) where deg is a typical degree of the graph since R(2k) typically behaves like O(1/deg k ) for small k. <ref type="foot" target="#foot_2">4</ref> Therefore, we can obtain a more accurate estimation of the true data from the noisy observation by multiplying the adjacency matrix if the maximum frequency of X is much smaller than the noise-to-signal ratio Z D / X D . This theorem also suggest a choice of k. By minimizing the right-hand side by k, we obtain: Corollary 6. Suppose that E[ Z D ] ≤ ρ X D for some ρ = O(1). Let k * be defined by k * = O(log(log(1/δ)ρ/ )), and suppose that there exist constants C d and d &gt; 1 such that R(2k) ≤ C d / dk for k ≤ k * . Then, by choosing k = k * , the right-hand side of (6) is Õ( √ ). <ref type="foot" target="#foot_3">5</ref>This concludes that we can obtain an estimation of the true features with accuracy Õ( √ ) by multiplying Ãrw several times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Graph Filter Neural Network</head><p>In the previous section, we see that low-pass filtered features Ãk rw X are accurate estimations of the true features with high probability. In this section, we analyze the performance of this operation.</p><p>Let the multi-layer (two-layer) perceptron be</p><formula xml:id="formula_11">h MLP (X|W 1 , W 2 ) = σ 2 (σ 1 (XW 1 )W 2 ),<label>(7)</label></formula><p>where σ 1 is the entry-wise ReLU function, and σ 2 is the softmax function. Note that both σ 1 and σ 2 are contraction maps, i.e., σ i (X</p><formula xml:id="formula_12">) − σ(Y ) D ≤ X − Y D .</formula><p>Under the second part of Assumption 1, our goal is to obtain a result similar to h( X|W 1 , W 2 ). The simplest approach is to train a multi-layer perceptron h MLP (X|W 1 , W 2 ) with the observed feature. The performance of this approach is evaluated by</p><formula xml:id="formula_13">h MLP ( X|W 1 , W 2 ) − h MLP (X|W 1 , W 2 ) D ≤ Z D ρ(W 1 )ρ(W 2 ),<label>(8)</label></formula><p>where ρ(W ) is the maximum singular value of W . Now, we consider applying graph a filter Ãk rw to the features, then learning with a multi-layer perceptron, i.e., h MLP (h( Lrw )X|W 1 , W 2 ). Using Lemma 5, we obtain the following result. Theorem 7. Suppose Assumption 4 and conditions in Corollary 6. By choosing k as Corollary 6, with probability at least 1 − δ for δ &lt; 1/2,</p><formula xml:id="formula_14">h MLP ( X|W 1 , W 2 ) − h MLP ( Ã2 rw X|W 1 , W 2 ) D = Õ( √ )E[ Z D ]ρ(W 1 )ρ(W 2 ).<label>(9)</label></formula><p>This means that if the maximum frequency of the true data is small, we obtain a solution similar to the optimal one.</p><p>Comparison with GCN Under the same assumption, we can analyze the performance of a twolayers GCN. The GCN is given by</p><formula xml:id="formula_15">h GCN (X|W 1 , W 2 ) = σ 2 ( Ãrw σ 1 ( Ãrw XW 1 )W 2 ). (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Theorem 8. Under the same assumption to Theorem 7, we have</p><formula xml:id="formula_17">h MLP ( X|W 1 , W 2 ) − h GCN (X|W 1 , W 2 ) D ≤ Õ( 4 √ )E[ Z D ]ρ(W 1 )ρ(W 2 ).<label>(11)</label></formula><p>This theorem means that GCN also has a performance similar to MLP for true features. Hence, in the limit of → 0 and Z → 0, all MLP, GCN, and the gfNN have the same performance.</p><p>In practice, gfNN has two advantages over GCN. First, gfNN is faster than GCN since it does not use the graph during the training. Second, GCN has a risk of overfitting to the noise. When the noise is so large that it cannot be reduced by the first-order low-pass filter, Ãrw , the inner weight W 1 is trained using noisy features, which would overfit to the noise. We verify this in Section 7.2.</p><p>Comparison with SGC Recall that a degree-2 SGC is given by</p><formula xml:id="formula_18">h SGC (X|W 1 ) = σ 2 ( Ã2 rw XW 2 ),<label>(12)</label></formula><p>i.e., it is a gfNN with a one-layer perceptron. Thus, by the same discussion, SGC has a performance similar to the perceptron (instead of the MLP) applied to the true feature. This clarifies an issue of SGC: if the true features are non-separable, SGC cannot solve the problem. We verify this issue in experiment E2 (Section 7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>To verify our claims in previous sections, we design two experiments. In experiment E1, we add different levels of white noise to the feature vectors of real-world datasets and report the classification accuracy among baseline models. Experiment E2 studies an artificial dataset which has a complex feature space to demonstrate when simple models like SGC fail to classify.</p><p>There are two types of datasets: Real-worlds data (citation networks, social networks, and biological networks) commonly used for graph neural network benchmarking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> and artificially synthesized random graphs from the two circles dataset. We created the graph structure for the two circles dataset by making each data point into a vertex and connect the 5 closest vertices in Euclidean distance. Table <ref type="table" target="#tab_1">1</ref> gives an overview of each dataset. We compare our results with a few state-of-the-art graph neural networks on benchmark datasets. We train each model using Adam optimizer <ref type="bibr" target="#b14">[15]</ref> (lr=0.2, epochs=50). We use 32 hidden units for the hidden layer of GCN, MLP, and gfNN. Other hyperparameters are set similar to SGC <ref type="bibr" target="#b27">[28]</ref>.</p><p>gfNN We have three graph filters for our simple model: Left Norm ( ), Augumented Normalized Adjacency ( ), and Bilateral ( ).</p><p>SGC <ref type="bibr" target="#b27">[28]</ref> Simple Graph Convolution ( ) simplifies the Graph Convolutional Neural Network model by removing nonlinearity in the neural network and only averaging features.</p><p>GCN <ref type="bibr" target="#b15">[16]</ref> Graph Convolutional Neural Network ($) is the most commonly used baseline.  The noise level is measured by standard deviation of white noise added to the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Denoising Effect of Graph Filters</head><p>For each dataset in Table <ref type="table" target="#tab_1">1</ref>, we introduce a white noise N (0, δ 2 ) into the feature vectors with δ in range (0.01, 0.05). According to the implications of Theorem 8 and Theorem 7, GCN should exhibit a low tolerance to feature noise due to its first-order denoising nature. As the noise level increases, we see in Figure <ref type="figure" target="#fig_3">4</ref> that GCN, Logistic Regression (LR), and MLP tend to overfit more to noise. On the other hand, gfNN and SGC remain comparably noise tolerant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Expressiveness of Graph Filters</head><p>Since graph convolution is simply denoising, we expect it to have little contribution to non-linearity learning. Therefore, we implement a simple 2-D two circles dataset to see whether graph filtering can have the non-linear manifold learning effect. Based on the Euclidean distance between data points, we construct a k-nn graph to use as the underlying graph structure. In this particular experiment, we have each data point connected to the 5 closest neighbors. Figure <ref type="figure">5</ref> demonstrates how SGC is unable to classify a simple 500-data-points set visually.</p><p>Original data SGC 0.485 0.862 0.872</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN gfNN</head><p>Figure <ref type="figure">5</ref>: Decision boundaries on 500 generated data samples following the two circles pattern On the other hand, SGC, GCN, and our model gfNN perform comparably on most benchmark graph datasets. Table <ref type="table" target="#tab_3">2</ref> compares the accuracy (f1-micro for Reddit) of randomized train/test sets. We see that all these graph neural networks have similar performance (some minor improvement might subject to hyperparameters tuning). Therefore, we believe current benchmark datasets for graph neural network have quite "simple" feature spaces.</p><p>In practice, both of our analysis and experimental results suggest that the graph convolutional layer should be thought of simply as a denoising filter. In contrast to the recent design trend involving GCN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> , our results imply that simply stacking GCN layers might only make the model more prone to feature noise while having the same expressiveness as a simple MLP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>There has been little work addressed the limits of the GCN architecture. Kawamoto et al. <ref type="bibr" target="#b13">[14]</ref> took the mean-field approach to analyze a simple GCN model to statistical physics. They concluded that backpropagation improves neither accuracy nor detectability of a GCN-based GNN model. Li et al. <ref type="bibr" target="#b17">[18]</ref> empirically analyzed GCN models with many layers under the limited labeled data setting and stated that GCN would not do well with little labeled data or too many stacked layers. While these results have provided an insightful view for GCN, they have not insufficiently answered the question: When we should we use GNN?.</p><p>Our results indicate that we should use the GNN approach to solve a given problem if Assumption 1 holds. From our perspective, GNNs derived from GCN simply perform noise filtering and learn from denoised data. Based on our analysis, we propose two cases where GCN and SGC might fail to perform: noisy features and nonlinear feature spaces. In turn, we propose a simple approach that works well in both cases.</p><p>Recently GCN-based GNNs have been applied in many important applications such as point-cloud analysis <ref type="bibr" target="#b23">[24]</ref> or weakly-supervised learning <ref type="bibr" target="#b8">[9]</ref>. As the input feature space becomes complex, we advocate revisiting the current GCN-based GNNs design. Instead of viewing a GCN layer as the convolutional layer in computer vision, we need to view it simply as a denoising mechanism. Hence, simply stacking GCN layers only introduces overfitting and complexity to the neural network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>Proof of Theorem 3. Since the generalized eigenvalues of (D + γI, L) are the eigenvalues of a positive semidefinite matrix (D + γI) x Lx x (D + γ 1 I)x</p><p>= λ i (γ 1 ).</p><p>Here, the second inequality follows because x (D + γ 1 )x &lt; x (D + γ 2 )x for all x = 0 Hence, the inequality is strict if x Lx = 0, i.e., λ i (γ 1 ) = 0.</p><p>Lemma 9. If X has a frequency at most then h(L)X </p><p>= max</p><formula xml:id="formula_22">t∈[0, ] h(t) X 2 D .<label>(19)</label></formula><p>Proof of Lemma 5. By substituting X = X + Z, we obtain X − Ãk </p><p>Note that E[ Z 2 D = σ 2 nd, and</p><formula xml:id="formula_24">λ (1 − λ) 2k n = tr( Ã2k rw ) n = R(2k)<label>(25)</label></formula><p>since (i, j) entry of Ã2k rw is the probability that a random walk starting from i ∈ V is on j ∈ V at 2k step, we obtain the lemma.</p><p>Proof of Theorem 7. By the Lipschitz continuity of σ, we have</p><formula xml:id="formula_25">h MLP ( X|W 1 , W 2 ) − h MLP ( Ãk rw X|W 1 , W 2 ) D ≤ X − Ãk rw X D ρ(W 1 )ρ(W 2 ).<label>(26)</label></formula><p>By using Lemma 5, we obtain the result. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Accuracy by frequency components</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average performance of 2-layers MLPs on frequency-limited feature vectors (epochs=20).The annotated "(best)" horizontal lines show the performance of the best performance using gfNN and a 2-layers MLP trained on the original features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Benchmark test accuracy on Cora (left), Citeseer (middle), and Pubmed (right) datasets. The noise level is measured by standard deviation of white noise added to the features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 ( 1</head><label>11</label><figDesc>rw X D ≤ (I − Ãk rw ) X D + Ãk rw Z D .(20)By Lemma 9, the first term is bounded by √ k X D . By the Parseval identity (4), the second term is evaluated byÃk rw Z 2 D = λ∈Λ (1 − λ) 2k ẑ(λ) ,...,d} (1 − λ) 2k ẑ(λ, p) 2 − λ) 2k (ẑ(λ, p) 2 /σ 2 − 1) ≥ 2 t By substituting t = log(1/δ) with δ ≤ 1/2, − λ) 2k ẑ(λ, p) 2 ≥ 5σ 2 nd log(1/δ) λ (1 − λ) 2k n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 10 .XW 1 )W 2 −</head><label>1012</label><figDesc>If X has a frequency at most then there exists Y such that Y has a frequency at most √ and σ(X) − Y 2 We choose Y by truncating the frequency components greater than√ Ãrw σ( Ãrw XW 1 )W 2 D (34) ≤ σ( XW 1 ) − Ãrw σ( XW 1 ) D + Ãrw σ( XW 1 ) − Ãrw σ( Ãrw XW 1 ) D ρ(W 2 ) (35) = Lrw σ( XW 1 ) D + X − Ãrw X D ρ(W 1 ) ρ(W 2 )(36)≤ O( 1/4 ) X D + R(2) Z D ρ(W 1 )ρ(W 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref>. Compute the graph Fourier basis U from L; 2. Add Gaussian noise to the input features: X ← X + N (0, σ</figDesc><table><row><cell></cell><cell></cell><cell>Noise ( ):</cell><cell>0.0 0.01 0.05</cell><cell>Reference:</cell><cell>gfNN (best) MLP (best) MLP (same)</cell></row><row><cell></cell><cell>0.8 0.7</cell><cell></cell><cell>gfNN</cell><cell></cell><cell>gfNN</cell><cell>gfNN MLP</cell></row><row><cell>Accuracy</cell><cell>0.5 0.4 0.6</cell><cell></cell><cell>MLP MLP</cell><cell></cell><cell>MLP MLP</cell><cell>MLP</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell cols="4">Fraction of frequency components 0.0 0.2 0.4 0.6 0.8 0.0 Fraction of frequency components 0.2 0.4 0.6 0.8 0.0 Fraction of frequency components 0.2 0.4 0.6 0.8 cora citeseer pubmed</cell></row></table><note>2 ) for σ = {0, 0.01, 0.05}; 3. Compute the first k-frequency component: Xk = U [: k] D1/2 X ; 4. Reconstruct the features: Xk = D−1/2 U [: k] Xk ; 5. Train and report test accuracy of a 2-layers perceptron on the reconstructed features Xk .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Real-world benchmark datasets and synthetic datasets for vertex classification</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="3">Features Classes Train/Val/Test Nodes</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>1,433</cell><cell>7</cell><cell>140/500/1,000</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>120/500/1,000</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell>60/500/1,000</cell></row><row><cell>Reddit</cell><cell cols="3">231,443 11,606,919 602</cell><cell>41</cell><cell>151,708/23,699/55,334</cell></row><row><cell>PPI</cell><cell>56,944</cell><cell>818,716</cell><cell>50</cell><cell>121</cell><cell>44,906/6,514/5,524</cell></row><row><cell cols="2">Two Circles 4,000</cell><cell>10,000</cell><cell>2</cell><cell>2</cell><cell>80/80/3,840</cell></row><row><cell cols="2">7.1 Neural Networks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average test accuracy on random train/val/test splits(5 times)    </figDesc><table><row><cell>Denoise</cell><cell>Classifier</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Reddit</cell><cell>PPI</cell><cell>Two Circles</cell></row><row><cell>GCN 1st order</cell><cell cols="5">Non-linear 80.0 ± 1.8 69.6 ± 1.1 79.3 ± 1.3 OOM</cell><cell>OOM</cell><cell>83.9 ± 3.1</cell></row><row><cell cols="2">SGC 2nd order Linear</cell><cell cols="6">77.6 ± 2.2 65.6 ± 0.1 78.4 ± 1.1 94.9 ± 1.2 62.0 ± 1.2 54.5 ± 4.4</cell></row><row><cell cols="8">gfNN 2nd order Non-linear 80.9 ± 1.3 69.3 ± 1.1 81.2 ± 1.5 92.4 ± 2.2 62.2 ± 1.5 84.6 ± 2.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1/2 L(D + γI) 1/2 , these are non-negative real numbers. To obtain the shrinking result, we use the Courant-Fisher-Weyl's min-max principle [4, Corollary III. 1.2]: For any 0 ≤ γ 1 &lt; γ 2 ,</figDesc><table><row><cell>λ i (γ 2 ) =</cell><cell>min H:subspace,dim(H)=i</cell><cell>max x∈H,x =0</cell><cell>x Lx x (D + γ 2 I)x</cell><cell>(13)</cell></row><row><cell>≤</cell><cell>min H:subspace,dim(H)=i</cell><cell>max x∈H,x =0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>2 D ≤ max t∈[0, ] h(t) X 2 D .</figDesc><table><row><cell>Proof. By the Parseval identity,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LX 2 D =</cell><cell cols="4">h(λ) x(λ) 2 2</cell><cell>(16)</cell></row><row><cell>λ∈Λ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">≤ max t∈[0, ]</cell><cell>h(t)</cell><cell>λ∈Λ</cell><cell>x(λ) 2 2</cell><cell>(17)</cell></row><row><cell cols="2">= max t∈[0, ]</cell><cell cols="2">h(t) X 2 2</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We only consider unweighted edges but it is easily adopted to positively weighted edges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">The shrinking of the maximum eigenvalue has already been proved in<ref type="bibr" target="#b27">[28,</ref> Theorem 1]. Our theorem is stronger than theirs since we show the "monotone shrinking" of "all" the eigenvalues. In addition, our proof is simpler and shorter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">This exactly holds for a class of locally tree-like graphs<ref type="bibr" target="#b6">[7]</ref>, which includes Erdos-Renyi random graphs and the configuration models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">Õ(•) suppresses logarithmic dependencies.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on riemannian manifolds</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="209" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matrix analysis</title>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Science &amp; Business Media</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Factor models on locally tree-like graphs. The Annals of Probability</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nike</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="4162" to="4213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Irregularity-aware graph fourier transforms</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Girault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="5746" to="5761" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Functions of matrices: theory and computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><surname>Higham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>SIAM</publisher>
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mean-field theory of graph neural networks in graph partitioning</title>
		<author>
			<persName><forename type="first">Tatsuro</forename><surname>Kawamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Obuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4361" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive estimation of a quadratic functional by model selection</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1302" to="1338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph signal processing: Overview, challenges, and applications</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="808" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Theory and application of digital signal processing</title>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Gold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.0053</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning localized generative models for 3d point clouds via graph convolution</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Advanced digital signal processing and noise reduction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><surname>Vaseghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De</surname></persName>
		</author>
		<author>
			<persName><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Christopher Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">How powerful are graph neural networks? International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty Fourth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
				<meeting>the Thirty Fourth Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
