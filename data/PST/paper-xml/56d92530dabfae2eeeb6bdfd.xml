<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Recognition Letters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fernando</forename><surname>Alonso-Fernandez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science</orgName>
								<orgName type="department" key="dep2">Computer and Electrical Engineering</orgName>
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<postBox>Box 823</postBox>
									<postCode>301-18</postCode>
									<settlement>Halmstad</settlement>
									<region>SE</region>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josef</forename><surname>Bigun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science</orgName>
								<orgName type="department" key="dep2">Computer and Electrical Engineering</orgName>
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<postBox>Box 823</postBox>
									<postCode>301-18</postCode>
									<settlement>Halmstad</settlement>
									<region>SE</region>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science</orgName>
								<orgName type="department" key="dep2">Computer and Electrical Engineering</orgName>
								<orgName type="institution">Halmstad University</orgName>
								<address>
									<postBox>Box 823</postBox>
									<postCode>301-18</postCode>
									<settlement>Halmstad</settlement>
									<region>SE</region>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern Recognition Letters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB84324D3F83A29AE1526332CF0AF3D6</idno>
					<idno type="DOI">10.1016/j.patrec.2015.08.026</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Periocular Biometrics Iris Eye Face</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Periocular refers to the facial region in the vicinity of the eye, including eyelids, lashes and eyebrows. While face and irises have been extensively studied, the periocular region has emerged as a promising trait for unconstrained biometrics, following demands for increased robustness of face or iris systems. With a surprisingly high discrimination ability, this region can be easily obtained with existing setups for face and iris, and the requirement of user cooperation can be relaxed, thus facilitating the interaction with biometric systems. It is also available over a wide range of distances even when the iris texture cannot be reliably obtained (low resolution) or under partial face occlusion (close distances). Here, we review the state of the art in periocular biometrics research. A number of aspects are described, including: (i) existing databases, (ii) algorithms for periocular detection and/or segmentation, (iii) features employed for recognition, (iv) identification of the most discriminative regions of the periocular area, (v) comparison with iris and face modalities, (vi) softbiometrics (gender/ethnicity classification), and (vii) impact of gender transformation and plastic surgery on the recognition accuracy. This work is expected to provide an insight of the most relevant issues in periocular biometrics, giving a comprehensive coverage of the existing literature and current state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Periocular biometrics has been shown as one of the most discriminative regions of the face, gaining attention as an independent method for recognition or a complement to face and iris modalities under non-ideal conditions <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b73">74]</ref>. The typical elements of the periocular region are labeled in Fig. <ref type="figure">1</ref>, left. This region can be acquired largely relaxing the acquisition conditions, in contraposition to the more carefully controlled conditions usually needed in face or iris modalities, making it suitable for unconstrained and uncooperative scenarios. Another advantage is that the problem of iris segmentation is automatically avoided, which can be an issue in difficult images <ref type="bibr" target="#b27">[28]</ref>.</p><p>This paper presents a survey of periocular research works found in the literature. We provide a comprehensive framework covering different aspects, from existing databases (Section 2), to algorithms for detection of the periocular region (Section 3), and features for recognition (Section 4). Databases utilized include face and iris databases (since the periocular area appears in such data), as well as newer databases capturing specifically the periocular area. Although initial studies have made use of annotated data, detection and segmentation of the periocular region has become a research target in itself.</p><p>We also provide a taxonomy of the features employed for periocular recognition, which can be divided between those performing a global analysis of the image (extracting properties describing an entire ROI) and those performing local analysis (extracting properties of the neighborhood of a set of sparse selected key points).</p><p>Most recognition algorithms work by applying feature extraction and/or key points detection to a predefined ROI around the eye (Fig. <ref type="bibr">1, right)</ref>. This holistic approach implies that some components not relevant for identity recognition, such as hair or glasses, might be erroneously taken into account <ref type="bibr" target="#b65">[66]</ref>. It can also be the case that a certain feature is not equally discriminative in all parts of the periocular region. Some works have addressed these problems, as presented in Section 5. Since the periocular area appears in face and iris images, comparison and fusion with these modalities has been also proposed, which is the focus of Section 6. Besides personal recognition, a number of other tasks have been also proposed using features extracted from the periocular region. In this direction, Section 7 deals with issues like soft-biometrics (gender/ethnicity classification), and impact of gender transformation and plastic surgery on the recognition accuracy. We finally conclude the paper by highlighting current trends and future directions in periocular biometrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Databases</head><p>Table <ref type="table">1</ref> summarizes the databases used in periocular research. Some sample images are shown in Fig. <ref type="figure">2</ref>. Very few databases have been designed specifically for periocular research, with face and iris databases mostly used for this purpose. The 'best accuracy' shown in Table <ref type="table">1</ref> should be taken as an approximate indication only, since different works may employ different subsets of the database or a different protocol. A general tendency, however, is that facial databases exhibit a better accuracy. These are the most used databases, so each new work builds on top of previous research, resulting in additional improvements. The accuracy with newer periocular databases are only some steps behind, demonstrating the capabilities of the periocular modality even in difficult scenarios, where new research leaps are expected to bring accuracy to even better levels. The following is a short description of each database, highlighting the features not contained in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Facial databases</head><p>M2VTS has video of people counting '0'-'9' in their native language and rotating the head left-right. AR has frontal view with different expressions, illumination, and occlusions (sun glasses, scarf). GTDB: Georgia Tech has frontal/titled faces with cluttered background, four expressions and lightning/scale change. Caltech has frontal pose under with different lighting/expressions/backgrounds. FERET: Facial Recognition Technology has variations of illumination, expression, pose (frontal, left/right), race, glasses, etc. CMU-H: CMU Hyperspectral has videos in the range 450nm-1100 nm, in steps of 10nm. Three halogen lamps surrounding the face was used individually one at a time, and all together (four lightning conditions). FRGC: Face Recognition Grand Challenge has controlled/uncontrolled and 3D images. Controlled images were taken in a studio setting, and uncontrolled images in hallways, atria, or outdoors, with varying lightning and distance. MORPH aging (Album1) has scanned mug-shots taken between 1962 and 1998, with age of the subjects ranging 15-68 years old. The gap between first and last images is from 46 days to 29 years. Images are near-frontal, with many types of illumination and eye occlusions. PUT has partially controlled illumination, uniform background and pose variation. Most images have neutral expression, although a small set has no constraints on pose or expressions. MBGC v2: Multiple Biometric Grand Challenge is organized into 3 challenges: (i) Portal, (ii) Still Face and (iii) Video. Only i and ii have been used in periocular research. Portal data has subjects walking naturally through a portal, acquired simultaneously with NIR and VW video cameras. Therefore, many image perturbations appear. In the NIR sequences, some frames are too dark or too bright since the NIR lights shine only for a short time. Still Face data has high resolution images with controlled/uncontrolled illumination and frontal/non-frontal collected both in a studio environment and in hallways/outdoors. Plastic Surgery has one pre-and one postsurgery image for each person, both frontal, with proper illumination and neutral expression. ND-twins has images of twins under varying lighting (indoor/outdoor), expression (neutral/smile), and pose (frontal/non-frontal). Compass has four manners (neutral, smiling, eyes closed, facial occlusion) at two distances (10 m and 20 m) acquired with a pan-tilt-zoom (PTZ) camera. FG-NET Aging has subjects from multiple race, large variation of lighting, expression, and pose. The age range is 0-69 years, with images taken years apart. CASIA v4 Distance has high-resolution frontal NIR images with neutral expression acquired at ∼3 m. FaceExpressUBI has seven expressions, with location/orientation of the camera and light sources changed between sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Iris databases</head><p>BioSec, CASIA Interval v3 and IIT Delhi v1.0 have NIR images acquired with close-up iris cameras. UBIRIS v2 has VW images acquired between 3-8 m with a digital camera. The 1st session has controlled conditions, and the 2nd session was captured in a real-world setup (natural light, reflections, contrast change, defocus, occlusions, blur and off-angle). MobBIO has VW images from a Tablet PC with two lighting conditions, variable eye orientations and occlusions. Distance to the camera was kept constant. Annotation of the iris databases described, or a subset of them, have been made available <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Periocular databases</head><p>UBIPr was acquired with a digital camera, with distance, illumination, pose and occlusion variability. The distance varies between 4-8 m in steps of 1 m, with resolution from 501 × 401 pixels (8 m) to 1001 × 801 (4 m). FOCS: Face and Ocular Challenge Series has images from NIR videos of subjects walking through a portal (as in MBGC). A large number of images are of very poor quality, with high variations in illumination, out-of-focus blur, sensor noise, specular reflections, partially occluded iris and off-angle. The iris is very small (∼50 pixels wide). IMP: IITD Multispectral Periocular has three spectrums: NIR, VW, and Night Vision. The NIR dataset is created with a close-up iris scanner, the VW dataset with a digital camera at 1.3 m, and the night dataset with a handycam in night mode. CSIP: Cross-Sensor Iris and Periocular has images with four different smarphones. Ten different setups are included by capturing with both frontal/rear cameras and with/without the flash embedded in the device. The resolution of each camera is different, ranging from 640 × 480 to 3264 × 2448. Participants were captured at different sites with artificial, natural and mixed illumination. Noise factors include multiple scales, chromatic distortions, rotation, poor lightning, off-angle, defocus, and iris obstructions (including reflections).</p><p>Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: PATREC [m5G; <ref type="bibr">October 18, 2015;</ref><ref type="bibr">18:58]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Databases used in periocular research. Only public available databases are included. The 'best accuracy' indicates the best performance reported in the literature (Table <ref type="table" target="#tab_2">3</ref>). The availability of ground-truth information is also indicated, either provided with the database, or available elsewhere.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Detection and segmentation of the periocular region</head><p>Initial studies were focused on feature extraction only (with the periocular region manually extracted), but automatic detection and segmentation have increasingly become a research target in itself. Some works have applied a full face detector first such as the Viola-Jones (VJ) detector <ref type="bibr" target="#b83">[84]</ref>, e.g. <ref type="bibr" target="#b55">[56]</ref> or <ref type="bibr" target="#b34">[35]</ref>, but successful extraction of the periocular region in this way relies on an accurate detection of the whole face. Using iris segmentation techniques may not be reliable under challenging conditions either <ref type="bibr" target="#b27">[28]</ref>. On the other hand, eye detection can be a decisive pre-processing task to ensure successful segmentation of the iris texture in difficult images, as in the study by the authors in <ref type="bibr" target="#b27">[28]</ref>. Here, they used correlation filters to detect the eye center over the difficult FOCS database of subjects walking through a portal, achieving a 95% success rate. However, despite this good result in indicating the eye position, accuracy of the iris segmentation algorithms evaluated were between 51% and 90% Correlation filters were also used for eye detection in <ref type="bibr" target="#b44">[45]</ref>, although after applying the VJ face detector.</p><p>Table <ref type="table" target="#tab_1">2</ref> summarizes existing research dealing with the task of locating the eye position directly, without relying on full-face or iris detectors. The authors in <ref type="bibr" target="#b80">[81]</ref> and <ref type="bibr" target="#b82">[83]</ref> used the VJ detector of face sub-parts. The authors in <ref type="bibr" target="#b82">[83]</ref> also experimented with the CMU hyperspectral database, which has images captured simultaneously at multiple wavelengths. Since the eye is centered in all bands, accuracy can be boosted by collective detecting the eye over all bands. The authors in <ref type="bibr" target="#b77">[78]</ref> made use of Gabor features for eye detection and face tracking purposes by performing saccades across the image, whereas the authors in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> proposed the use of symmetry filters tuned to detect circular symmetries. The latter has the advantage of not needing training, and detection is possible with a few 1D convolutions due to separability of the detection filters, built from derivatives of a Gaussian. The authors in <ref type="bibr" target="#b40">[41]</ref> proposed a Local Eyebrow Active Shape Model (LE-ASM) to detect the eyebrow region directly from a given face image, with eyebrow pixels segmented afterwards using graphcut based segmentation. ASMs were also used in <ref type="bibr" target="#b34">[35]</ref> to automatically extract the periocular region, albeit after the application of a VJ fullface detector.</p><p>Recently, the authors in <ref type="bibr" target="#b65">[66]</ref> proposed a method to label seve components of the periocular region (iris, sclera, eyelashes, eyebrows, hair, skin and glasses) by using seven classifiers at the pixel level, with each classifier specialized in one component. Pixel features used for classification included the following texture and shape descriptors: RGB/HSV/YCbCr values, Local Binary Patterns (LBP), entropy and Gabor features. Some works have proposed the extraction of features from the sclera region only, therefore requiring an algorithm to specifically segment this region. For this purpose, the authors in <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b87">88]</ref> used the HSV/YCbCr color spaces. In these works, however, sclera detection is guided by a prior detection of the iris boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recognition using periocular features</head><p>Several feature extraction methods have been proposed for periocular recognition, with a taxonomy shown in Fig. <ref type="figure">3</ref>. Existing features can be classified into: (i) global features, which are extracted from the whole image or region of interest (ROI), and (ii) local features, which are extracted from a set of discrete points, or key points, only. Table <ref type="table" target="#tab_2">3</ref> gives an overview in chronological order of existing works for periocular recognition. The most widely used approaches include Local Binary Patterns (LBP) and, to a lesser extent, Histogram of Oriented Gradients (HOG) and Scale-Invariant Feature Transform (SIFT) key points. Over the course of the years, many other descriptors have been proposed. This section provides a brief description of the features used for periocular recognition (Sections 4.1 and 4.2), followed by a review of the works mentioned in Table <ref type="table" target="#tab_2">3</ref> (Section 4.3), highlighting their most important results or contributions. Due to pages limitation, we will omit references to the original works where features have been presented (unless they are originally proposed for periocular recognition in the mentioned reference). We refer to the references indicated for further information about the presented feature extraction techniques. Some preprocessing steps have been also used to cope with the difficulties found in unconstrained scenarios, such as pose correction by Active Appearance Models (AAM) <ref type="bibr" target="#b32">[33]</ref>, illumination normalization <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref>, correction of deformations due to expression change by Elastic Graph Matching (EGM) <ref type="bibr" target="#b63">[64]</ref>, or color device-specific calibration <ref type="bibr" target="#b71">[72]</ref>. The use of subspace representation methods after feature extraction is also becoming a popular way either to improve performance or reducing the feature set, as mentioned next in this section. There are also periocular studies with human experts. The authors in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> evaluated the ability of (untrained) human observers to compare pairs of periocular images both with VW and NIR illumination, obtaining better results with the VW modality. They also tested three computer experts (LBP, HOG and SIFT), finding that the performance of humans and machines was similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLOBAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Global features</head><p>Global approaches extract properties describing a entire ROI, such as texture, shape or color features. They are typically computed by dividing the image into a grid of patches (Fig. <ref type="figure">1</ref>, right) and extracting features in each patch. A global descriptor is then built by concatenating features from each patch into a single vector. This produces fixed length vectors, with matching between two images simply done by comparing these vectors with some distance measure, which is very time efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Textural-based features</head><p>BGM: Bayesian Graphical Models were used in <ref type="bibr" target="#b11">[12]</ref>. They adapted an iris matcher based on correlation filters applied to nonoverlapping image patches. Patches of gallery and probe images are cross-correlated, and the output used to feed a Bayesian graphical model (BGM) trained to consider non-linear deformations and occlusions between images. BGM were also used in <ref type="bibr" target="#b78">[79]</ref> and <ref type="bibr" target="#b69">[70]</ref>, although called PDM or Probabilistic Deformation Models in these works.</p><p>BSIF: Binarized Statistical Image Features <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref> computes a binary code for each pixel by linearly projecting image patches onto a subspace, whose basis vectors are learnt from natural images using Independent Component Analysis (ICA). Since it is based on natural images, it is expected that BSIF encodes texture features more robustly than other methods that also produce binary codes, such as LBPs.</p><p>CRBM: Convolutional Restricted Boltzman Machines are a convolutional version of the Restricted Boltzman Machines, previously used in handwriting recognition, image classification, and face verification. CRBM, proposed for periocular recognition in <ref type="bibr" target="#b50">[51]</ref>, is a generative stochastic neural network that learn a probability distribution over a set of inputs generated by filters which capture edge orientation and spatial connections between image patches. <ref type="bibr" target="#b31">[32]</ref> expresses data points by a sum of cosine functions oscillating at different frequencies (which in 2D corresponds to horizontal and vertical frequencies). The 2D-DCT is computed in image blocks of size N × N (with N = 3, 5, 7…) and the N 2 coefficients are assigned as featureto the center pixel of the block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCT: Discrete Cosine Transform</head><p>DWT: Discrete Wavelet Transform was used by <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b30">[31]</ref> with respect to the Haar wavelet, which, in 2D, leads to an approximation of image details in three orientations: horizontal, vertical and diagonal.</p><p>Force Field Transform <ref type="bibr" target="#b31">[32]</ref> employs an analogy to gravitational force. Each pixel exerts a 'force' on its neighbors inversely proportional to the distance between them, weighted by the pixel value.</p><p>The net force at one point is the aggregate of the forces exerted by all other 5 × 5 neighbors.</p><p>Gabor filters are texture filters selective in frequency and orientation. A set of different frequencies and orientations are usually employed. For example, the authors in <ref type="bibr" target="#b77">[78]</ref> and <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> employed five frequencies and six orientations equally spaced in the log-polar frequency plane, achieving full coverage of the spectrum. The authors in <ref type="bibr" target="#b31">[32]</ref> employed one frequency and four orientations, the authors in <ref type="bibr" target="#b20">[21]</ref> employed one frequency and one orientation only, and the authors in <ref type="bibr" target="#b30">[31]</ref> employed five frequencies and six orientations. Lastly, the authors in <ref type="bibr" target="#b14">[15]</ref> used two frequencies and eight orientations, with Gabor responses further encoded by LBP operators (below).</p><p>GIST perceptual descriptors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b71">72]</ref> consist of five perceptual dimensions related with scene description, correlated with the secondorder statistics and spatial arrangement of structured image components: naturalness, which quantizes the vertical and horizontal edge distribution; openness, presence or lack of reference points; roughness, size of the largest prominent object; expansion, depth of the space gradient; and ruggedness, which quantizes the contour orientation that deviates from the horizontal.</p><p>HOG: Histogram of Oriented Gradients. In HOG, the gradient orientation and magnitude are computed in each pixel. The histogram of orientations is then built, with each bin accumulating corresponding gradient magnitudes. In PHOG or Pyramid of Histogram of Oriented Gradients, instead of using image patches, HOG is extracted from the whole image. Then, the image is split up several times like a quadtree and all sub-images get their own HOG.</p><p>Please cite this article as: F. Alonso-Fernandez, J. <ref type="bibr">Bigun</ref> JDSR: Joint Dictionary-based Sparse Representation <ref type="bibr" target="#b28">[29]</ref> computes a compact dictionary using a set of training images. A new image is represented as a sparse linear combination of the dictionary elements. A similar approach is SRC, or Sparse Representation Classification <ref type="bibr" target="#b66">[67]</ref>. An image is represented as a sparse linear combination of training images plus sparse errors due to perturbations. Images can be in original raw form or represented in any feature space. The features used included Eigenfaces, Laplacianfaces, Randomfaces, Fisherfaces, and downsampled versions of the raw image. The authors in <ref type="bibr" target="#b66">[67]</ref> also tested BSIF and LBP features.</p><p>Laws masks were used in <ref type="bibr" target="#b31">[32]</ref>. Five 1D masks capturing shapes of level, edge, spot, wave and ripple were employed. In 2D, masks are 1D-convolved in all possible combinations with an image, thus producing 25 local features.</p><p>LBP: Local Binary Patterns were first introduced for texture classification, since they can identify spots, line ends, edges, corners and other patterns. For each pixel p, a 3 × 3 neighborhood is considered. Every neighbor p i (i = 1…8) is assigned a binary value of 1 if p i &gt; p, or 0 otherwise. The binary values are then concatenated into a 8bits binary number, and the decimal equivalent is assigned to characterize the texture at p, leading to 2 8 = 256 possible labels. The LBP values of all pixels within a given patch are then quantized into a 8bin histogram. LBP is one of the most popular periocular matching techniques in the literature (Table <ref type="table" target="#tab_2">3</ref>), with many variants proposed. One is Uniform LBP or ULBP <ref type="bibr" target="#b71">[72]</ref>, used to reduce the length of the feature vector and achieve rotation invariance. A LBP is called uniform if it contains at most two bitwise transitions from 0 to 1 or viceversa. A separate label is used for each uniform pattern, and all the non-uniform patterns are labeled with a single label, yielding to 59 different labels, instead of 256 as the regular LBP. The neighborhood can be also made larger to allow multi-resolution representations of the local texture pattern, leading to a circle of radius R, also called Circular LBP or CLBP <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. To avoid a large number of binary values as R increases, only neighbors separated by certain angular distance may be chosen. In Three-Patch LBP or TPLBP/3PLBP <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, pixel p is compared with the central pixel of two (non-adjacent) patches situated across a circle R. Application of 3PLBP to multiple image scales across a Gaussian pyramid leads to the Hierarchical Three-Patch LBP or H3PLBP <ref type="bibr" target="#b43">[44]</ref>. Further extension to two circles R 1 and R 2 results in Four-Patch LBP or FPLBP <ref type="bibr" target="#b75">[76]</ref>, involving four patches instead of three in the comparison. The use of subspace representation methods applied to LBPs is also very popular to reduce the feature set or improve performance, for example: <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83]</ref>. Other works have also proposed to apply LBP upon other feature extraction itself, for example <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>LMF: Leung-Mallik filters is a set of filters constructed from Gaussian, Gaussian derivatives and Laplacian of Gaussian at different orientations and scales. In the experiments by the authors in <ref type="bibr" target="#b79">[80]</ref>, filter responses from an image training set were clustered by k-means to construct a texton dictionary. The clusters (texton) producing the lowest EER were then used to classify test images.</p><p>LoG: Laplacian of Gaussian filter is an edge detector, used in <ref type="bibr" target="#b31">[32]</ref> for periocular recognition.</p><p>LPQ: Local Phase Quantization <ref type="bibr" target="#b20">[21]</ref> extracts phase statistics of local patches by selective frequency filters in the Fourier domain. The phases of the four low-frequency coefficients are quantized in four bins.</p><p>NGC: Normalized Gradient Correlation <ref type="bibr" target="#b28">[29]</ref> computes in the Fourier domain the normalized correlation between the gradients of two images in pair-wise patches.</p><p>PIGP: Phase Intensive Global Pattern <ref type="bibr" target="#b7">[8]</ref> computes the intensity variation of pixel-neighborhoods with respect to different phases by convolution with a bank of 3 × 3 filters. The filters have 'U' shape when seen in 3D, with different rotations corresponding to the different phases. Four different angles between 0 and 3π /4 in steps of π /4 were considered.</p><p>SRP: Structured Random Projections <ref type="bibr" target="#b53">[54]</ref> encode horizontal and vertical directional features by means of 1D horizontal and vertical binary vectors (projection elements). Such elements have a single group of contiguous '1' values, with the location of '1's' randomly determined. The number k of projection elements and the length l of contiguous '1's' are to be fixed experimentally, with k = 10 and l = 3,6,…150 tested.</p><p>Walsh masks are convolution filters which only contain +1 and -1 values, thus capturing the binary characteristics of an image in terms of contrast. N different 1D-filters of N elements are produced (N = 3, 5, 7…) and combined in all possible pairs, yielding to N 2 2D-filters. Walsh masks were used in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b7">[8]</ref> to compute the Walsh-Hadamard Transform based LBPs (WLBP), which consists of extracting LBPs from the input image after being filtered with Walsh masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Shape-based features</head><p>Eyelids shape descriptors <ref type="bibr" target="#b62">[63]</ref> extract several properties from the polynomial encoding each eyelid, including: accumulated curvature at point i (out of t), defined as i j=1 ∂ 2 y j ∂x 2 / t j=1 ∂ 2 y j ∂x 2 ; shape context, represented by the histogram h i of (x ix j , y iy j ) at each point (x i , y i ), ∀j = i; and the Elliptical Fourier Descriptors (EFD) parameterizing y i coordinates of the eyelids. The author in <ref type="bibr" target="#b62">[63]</ref> also applied LBP to the eyelids region only.</p><p>Eyebrows shape was studied in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b40">[41]</ref>. The authors in <ref type="bibr" target="#b18">[19]</ref> encoded rectangularity, eccentricity, isoperimetric quotient, area percentage of different sub-regions, and critical points (comprising the right/left-most points, the highest point and the centroid). The authors in <ref type="bibr" target="#b40">[41]</ref> proposed the use of shape context histograms encoding the distribution of eyebrow points relative to a given (reference) point, and the Procrustes analysis representing the eyebrow shape asymmetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Color-based features</head><p>LCH: Local Color Histograms from image patches were used in <ref type="bibr" target="#b85">[86]</ref>. They experimented with RGB and HSV spaces and their subspaces, finding that the RG (red-green) color space outperformed the other, with a 4 × 4 histogram giving better results than coarser or finer resolutions. Thus each 4 × 4 histogram provides a 16 element feature vector per patch. LCH were also used in <ref type="bibr" target="#b42">[43]</ref> for gender and ethnicity classification using periocular data (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Local features</head><p>In local approaches, a sparse set of characteristic points (called key points) is detected first. Local features encode properties of the neighborhood around key points only, leading to local key point descriptors. Since the number of detected key points is not necessarily the same in each image, the resulting feature vector may not be of constant length. Therefore, the matching algorithm has to compare each key point of one image against all key points of the other image to find a pair match, thus increasing the computation time. The output from the matching function is typically the number of matched points, although a distance measurement between pairs may also be returned. To achieve scale invariance, key points are usually detected at different scales. Different key point detection algorithms exist, with some of the feature extraction methods of this section also having its own key point extraction method. For example, detection of key points with the SIFT feature extractor relies on a difference of Gaussians (DOG) function in the scale space, whereas detection with SURF is based on the Hessian matrix, but relying on integral images to speed up computations. Newer algorithms such as BRISK and ORB claim to provide an even faster alternative to SIFT or SURF key point extraction methods. The authors in <ref type="bibr" target="#b36">[37]</ref> employ one key point extraction method (SURF), and then compute the SIFT, SURF, BRISK and ORB descriptors from these key points. Other periocular works like Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref> extract key points descriptors at selected sampling points in the center of image patches only, resembling the grid-like analysis of global approaches (Fig. <ref type="figure">1</ref>, right) but using local features. This way, no key point detection is carried out, and the obtained feature vector is of fixed size. The following local descriptors have been proposed in the literature for periocular recognition.</p><p>BRISK: Binary Robust Invariant Scalable Key points descriptor is composed of a binary string by concatenating the results of simple brightness comparison tests. BRISK applies a sampling pattern of N = 60 locations equally spaced on circles concentric with the key point. The origin of the sampling pattern is rotated according to the gradient angle around the key point to achieve rotation invariance. The intensity of all possible short-distance pixel pairs p i and p j of the sampling pattern is then compared, assigning a binary value of 1 if p i &gt; p j , and 0 otherwise. The resulting feature vector at each key point has 512 bits. BRISK is employed for periocular recognition in <ref type="bibr" target="#b36">[37]</ref>.</p><p>ORB: Oriented FAST and Rotated BRIEF is based on the FAST corner detector and the visual descriptor BRIEF (Binary Robust Independent Elementary Features). As in BRISK, BRIEF also uses binary tests between pixels. Pixel pairs are considered from an image patch of size S × S. The original BRIEF deals poorly with rotation, so in ORB it is proposed to steer the descriptor according to the dominant rotation of the key point (obtained from the first order moments). The parameters employed in ORB are S = 31 and a vector length of 256 bits per key point. ORB was used for periocular recognition in <ref type="bibr" target="#b36">[37]</ref>.</p><p>PILP: Phase Intensive Local Pattern was used in <ref type="bibr" target="#b8">[9]</ref>, following the work in <ref type="bibr" target="#b7">[8]</ref> where they presented PIGP (Phase Intensive Global Pattern). PILP uses a similar filter bank than PIGP, but used for key point extraction, rather than for feature encoding. Size of the filters varies from 3 × 3 to 9 × 9, to allow to cope with scale variations. This way, key points are the local extrema among pixels in its own window and windows in its neighboring phases. Feature extraction is then done by computing a gradient orientation histogram in the neighborhood of each keypoint, in a similar way than SIFT descriptor, below. SAFE: Symmetry Assessment by Feature Expansion <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref> describes neighborhoods around key points by projection onto harmonic functions which estimates the presence of various symmetric curve families. The iso-curves of such functions are highly symmetric w.r.t. the key points and the estimated coefficients have well defined geometric interpretations. The detected patterns resemble shapes such as parabolas, circles, spirals, etc. Detection is done in concentric circular bands of different radii around key points, with radii log-equidistantly sampled. Extracted features therefore quantify the presence of pattern families in annular rings around each key point.</p><p>SIFT: Scale Invariant Feature Transformation. Together with LBP, SIFT is the most popular matching technique employed in the literature (Table <ref type="table" target="#tab_2">3</ref>). SIFT encodes local orientation via histograms of gradients around key points. The dominant orientation of a key point is first obtained by the peak of the gradient orientation histogram in a 16 × 16 window. The key point feature vector of dimension 4 × 4 × 8 = 128 is then obtained by computing 8-bin gradient orientation histograms (relative to the dominant orientation to achieve rotation invariance) in 4 × 4 sub-regions around the key point. m-SIFT (modified SIFT) is a SIFT matcher where additional constraints are imposed to the angle and distance of matched key points <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>SURF: Speeded Up Robust Features was aimed at providing a detector and feature extractor faster than SIFT and other local feature algorithms. Feature extraction is done over a 4 × 4 sub-region around the key point (relative to the dominant orientation) using Haar wavelet responses. SURF is employed for periocular recognition in [ <ref type="bibr" target="#b8">[9]</ref>,32,37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Literature review of periocular recognition works</head><p>Periocular recognition started to gain popularity after the studies in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>. Some pioneering works can be traced back to 2002 <ref type="bibr" target="#b77">[78]</ref>, although authors here did not call the local eye area 'periocu-lar'. The approach in <ref type="bibr" target="#b55">[56]</ref> combined global and local features, concretely LBP, HOG and SIFT. Reported performance of such study was fairly good, setting the framework for the use of the periocular modality. Many works have followed this approach as inspiration, with LBPs and their variations being particularly extensive in the literature <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref>. The studies in <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref> used for the first time NIR data (MBGC portal video), although they selected usable frames (higher quality) which mostly are in the earlier part of the video, where scale change is not substantial. The authors in <ref type="bibr" target="#b11">[12]</ref> also presented experiments over NIR portal data from the more difficult FOCS database, but with a different descriptor (BGM). The authors in <ref type="bibr" target="#b43">[44]</ref> also evaluated the impact of covariates such as pose, expression, template aging, glasses and eyelids occlusion. Some works have also employed other features in addition to LBPs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b85">86]</ref>. The authors in <ref type="bibr" target="#b85">[86]</ref> employed LCH (RG color histograms), reporting the best accuracy up to that date with the FRGC database of VW images. The authors in <ref type="bibr" target="#b79">[80]</ref> proposed Leung-Mallik filters (LMF) as texture descriptors over the CASIA v4 Distance database of NIR images. The authors in <ref type="bibr" target="#b36">[37]</ref> evaluated LBP, SIFT, and other local descriptors including SURF, BRISK and ORB over the FERET database. The use of subspace representation methods applied to raw pixels or LBP features is also becoming a popular way either to improve performance or reducing the feature set <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83]</ref>. LBP has been also used in other works analyzing for example the impact of plastic surgery <ref type="bibr" target="#b26">[27]</ref> or gender transformation <ref type="bibr" target="#b44">[45]</ref> on periocular recognition (see <ref type="bibr">Section 7)</ref>.</p><p>Inspired by <ref type="bibr" target="#b56">[57]</ref>, the authors in <ref type="bibr" target="#b31">[32]</ref> extended the experiments with additional global and local features to a significant larger set of the FRGC database with less ideal images (thus the lower accuracy w.r.t. previous studies): WLBP, Laws Masks, DCT, DWT, Force Field transform, SURF, Gabor filters and LoG filters. They later addressed the problem of aging degradation on periocular recognition using the FG-NET database <ref type="bibr" target="#b32">[33]</ref>, reported to be an issue even at small time lapses <ref type="bibr" target="#b55">[56]</ref>. To obtain age invariant features, they first performed preprocessing schemes, such as pose correction by Active Appearance Models (AAM), illumination and periocular region normalization. In a later work, the authors in <ref type="bibr" target="#b34">[35]</ref> also applied WLBPs to study periocular recognition with data from a pan-tilt-zoom (PTZ) camera. As in the study above, they employed different schemes to correct illumination and pose variations.</p><p>The mentioned work in <ref type="bibr" target="#b77">[78]</ref> with Gabor filters served as inspiration to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> to carry out periocular experiments with several iris databases in NIR and VW, as well as a comparison with the iris modality (Section 6). A variation of this algorithm was fused with the SIFT descriptor, obtaining a leading position in the First ICB Competition on Iris Recognition, ICIR2013 <ref type="bibr" target="#b86">[87]</ref>. They later proposed a matcher based on Symmetry Assessment by Feature Expansion (SAFE) descriptors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>, which describes neighborhoods around key-points by estimating the presence of various symmetric curve families. Gabor filters were also used in <ref type="bibr" target="#b20">[21]</ref> in their work presenting Local Phase Quantization (LPQ) as descriptors for periocular recognition. The authors in <ref type="bibr" target="#b30">[31]</ref> also employed Gabor features over four different VW databases, with features reduced by Direct Linear Discriminant Analysis (DLDA) and further classified by a Parzen Probabilistic Neural Network (PPNN).</p><p>The authors in <ref type="bibr" target="#b10">[11]</ref> evaluated CLBP and GIST descriptors. They used the UBIRIS v2 database of uncontrolled VW iris images which includes a number of perturbations intentionally introduced (see Section 2). A number of subsequent works have also made use of UBIRIS v2 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73]</ref>. The authors in <ref type="bibr" target="#b29">[30]</ref> used UBIRIS v2 in their comparison of iris and periocular modalities (Section 6), obtaining better results than <ref type="bibr" target="#b10">[11]</ref> using just LBPs, although over a smaller set of images. The authors in <ref type="bibr" target="#b72">[73]</ref> used LBPs and SIFT as in <ref type="bibr" target="#b56">[57]</ref> in their study combining iris and periocular modalities (Section 6). The authors in <ref type="bibr" target="#b7">[8]</ref> proposed global PIGP features, outperforming the Rank-1 performance of any previous study using UBIRIS v2. They later proposed local PILP features <ref type="bibr" target="#b8">[9]</ref>, reporting the best Rank-1 periocular Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: PATREC [m5G; <ref type="bibr">October 18, 2015;</ref><ref type="bibr">18:58]</ref> performance to date with UBIRIS v2. The author in <ref type="bibr" target="#b62">[63]</ref> studied the fusion of iris and periocular biometrics (Section 6). Periocular features were extracted from the eyelids region only, consisting of the fusion of LBPs and eyelids shape descriptors. In a subsequent study, the authors in <ref type="bibr" target="#b65">[66]</ref> proposed a method to label seven components of the periocular region (see Section 3) with the purpose of demonstrating that regions such as hair or glasses should be avoided since they are unreliable for recognition (Section 5). They also proposed to use the center of mass of the cornea as reference point to define the periocular ROI, rather than the pupil center, which is much more sensitive to changes in gaze. Finally, the authors in <ref type="bibr" target="#b53">[54]</ref> used the first version of UBIRIS in their study presenting directional projections or Structured Random Projections (SRP) as periocular features.</p><p>Other shape features have been also proposed, such as eyebrow shape features, with surprisingly accurate results as a stand-alone trait. Indeed, eyebrows have been used by forensic analysts for years to aid in facial recognition <ref type="bibr" target="#b40">[41]</ref>, suggested to be the most salient and stable features in a face <ref type="bibr" target="#b70">[71]</ref>. The authors in <ref type="bibr" target="#b18">[19]</ref> studied several geometrical shape properties over the MGBC/FRGC databases. They also used the extracted eyebrow features for gender classification (see <ref type="bibr">Section 7)</ref>. The authors in <ref type="bibr" target="#b40">[41]</ref> proposed an eyebrow shape-based identification system, together with a eyebrow segmentation technique (presented in Section 3).</p><p>The authors in <ref type="bibr" target="#b54">[55]</ref> presented the first periocular database in VW range specifically acquired for periocular research (UBIPr). They also proposed to compute the ROI w.r.t. the midpoint of the eye corners (instead of the pupil center), which is less sensitive to gaze variations, leading to a significant improvement (EER from ∼30% to ∼20%). Posterior studies have managed to improve performance over the UBIPr database using a variety of features <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b78">79]</ref>. The UBIPr database is also used in <ref type="bibr" target="#b82">[83]</ref> in their extensive study evaluating data in VW (UBIPr, MBGC), NIR (MBGC) and multi-spectral (CMU-H database) range, with the reported Rank-1 results being the best published performance to date for the four databases employed. A new database of challenging periocular images in VW range (CSIP) was presented recently by <ref type="bibr" target="#b71">[72]</ref>, the first one made public captured with smartphones. The paper proposed a device-specific calibration method to compensate for the chromatic disparity, as result of the variability of camera sensors and lenses used by different mobile phones. They also compared and fused the periocular and iris modalities (Section 6).</p><p>Another database captured specifically for cross-spectral periocular research (IMP) was also recently presented in <ref type="bibr" target="#b75">[76]</ref>, containing data in VW, NIR and night modalities. To match cross-spectral images, they proposed neural networks (NN) to learn the variability caused by different spectrums, with several variations of LBP and HOG tested as features. Cross-spectral recognition was also addressed in <ref type="bibr" target="#b28">[29]</ref> using a proprietary database of NIR and VW images. Finally, the authors in <ref type="bibr" target="#b66">[67]</ref> and <ref type="bibr" target="#b67">[68]</ref> presented a database in VW range acquired with a new type of camera, a Light Field Camera (LFC), which provides multiple images at different focus in a single capture. LFC overcomes one important disadvantage of sensors in VW range, which is guaranteeing a good focused image. Unfortunately, the database has not been made available. Individuals were also acquired with a conventional digital camera, with a superior performance observed with the LFC camera. New periocular features were also presented in the two studies. The authors in <ref type="bibr" target="#b66">[67]</ref> proposed Sparse Representation Classification (SRC), previously used in face recognition. The authors in <ref type="bibr" target="#b67">[68]</ref> proposed Binarized Statistical Image Features (BSIF) for periocular recognition, further utilized as features of the SRC method described. The authors in <ref type="bibr" target="#b66">[67]</ref> and <ref type="bibr" target="#b67">[68]</ref> tested the fusion of iris and periocular modalities as well (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Best regions for periocular recognition</head><p>Most periocular algorithms work in a holistic way, defining a ROI around the eye (usually a rectangle) which is fully used for feature extraction. Such holistic approach implies that some components not relevant for identity recognition, such as hair or glasses, might erroneously bias the process <ref type="bibr" target="#b65">[66]</ref>. It can also be the case that a feature is not equally discriminative in all parts of the periocular region.</p><p>The study in <ref type="bibr" target="#b25">[26]</ref> identified which ocular elements humans find more useful for periocular recognition. With NIR images, eyelashes, tear ducts, eye shape and eyelids, were identified as the most useful, while skin was the less useful. But for VW data, blood vessels and skin were reported more helpful than eye shape and eyelashes. Similar studies have been done with automatic algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b78">79]</ref>, with results in consonance with the study with humans, despite using several machine algorithms based on different features, and different databases. With NIR images, regions around the iris (including the inner tear duct and lower eyelash) were the most useful, while cheek and skin texture were the less important. With VW images, on the other hand, the skin texture surrounding the eye was found very important, with the eyebrow/brow region (when present) also favored in visible range. This is in line with the assumption largely accepted in the literature that the iris texture is more suited to NIR illumination <ref type="bibr" target="#b16">[17]</ref>, whereas the periocular modality is best for VW illumination <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b85">86]</ref>. This seems to be explained by the fact that NIR illumination reveals the details of the iris texture, while the skin reflects most of the light, appearing over-illuminated (see for example 'BioSec' or other NIR iris examples in Fig. <ref type="figure">2</ref>); on the other hand, the skin texture is clearly visible in VW range, but only irises with moderate levels of pigmentation image reasonably well in this range <ref type="bibr" target="#b12">[13]</ref>.</p><p>The authors in <ref type="bibr" target="#b55">[56]</ref> carried out experiments by masking parts of the periocular area over VW images of the FRGC database. They found that inclusion of eyebrows is beneficial for a better identification performance, with differences in Rank-1 of 8-19%, depending on the machine expert. Similarly, they observed that occluding ocular information (iris and sclera) deteriorates the performance, with reductions in Rank-1 accuracy of up to 41%. In the same direction, the authors in <ref type="bibr" target="#b52">[53]</ref> focused on the inclusion of a significant part of the cheek region over VW images of the FERET database, finding that it does not contain significant discriminative information while it increases the image size. Including the eyebrows and the ocular region was also found to be beneficial in this study, corroborating the results in <ref type="bibr" target="#b55">[56]</ref>. Recently, the authors in <ref type="bibr" target="#b65">[66]</ref> proposed a method to label seven components of the periocular region: iris, sclera, eyelashes, eyebrows, hair, skin and glasses. The usefulness of such segmentation is demonstrated by avoiding hair and glasses in the feature encoding and matching stages, obtaining performance improvements by fusion of LBP, HOG and SIFT features <ref type="bibr" target="#b55">[56]</ref> over the UBIRIS v2 database of VW images (EER reduced from 12.8% to 9.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparison and fusion with other modalities</head><p>Periocular biometrics has rapidly evolved to competing with face or iris recognition. The periocular region appears in face or iris images, therefore comparison and/or fusion with these modalities has been also proposed. This section gives an overview of these works, with a summary provided in Table <ref type="table" target="#tab_5">4</ref>. Under difficult conditions, such as acquisition portals <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b84">85]</ref>, distant acquisition <ref type="bibr" target="#b79">[80]</ref>, smartphones <ref type="bibr" target="#b71">[72]</ref>, webcams or digital cameras <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, the periocular modality is shown to be clearly superior to the iris modality, mostly due to the small size of the iris or the use of visible illumination. Visible illumination is predominant in relaxed or uncooperative setups due to the impossibility of using NIR illumination. Iris texture is more suited to the NIR spectrum, since this type of lightning reveals the details of the iris texture <ref type="bibr" target="#b16">[17]</ref>, while the skin reflects most of the light, appearing over-illuminated. On the other hand, the skin texture is clearly visible in VW range, but only irises with moderate levels of pigmentation image reasonably well in this range <ref type="bibr" target="#b12">[13]</ref>. Nevertheless, despite the poor performance shown by the iris in the visible spectrum, fusion with periocular is shown to improve the performance in many Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters ( <ref type="formula">2015</ref> cases as well <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b72">73]</ref>. Similar trends are observed with face. Under difficult conditions, such as blur or downsampling, the periocular modality performs considerably better <ref type="bibr" target="#b48">[49]</ref>. It is also the case of partial face occlusions, where performance of full-face matchers is severely degraded <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Iris modality</head><p>The authors in <ref type="bibr" target="#b84">[85]</ref> evaluated NIR portal videos of the MBGC database. The periocular modality showed considerable superiority, with the performance further improved by the fusion, demonstrating the benefits of fusing periocular and iris information in non-ideal conditions. Authors in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b69">[70]</ref> also used NIR portal data from the FOCS database. Despite using other feature extraction methods, they also concluded that the periocular modality is considerable superior than the iris modality in such difficult data. The authors in <ref type="bibr" target="#b72">[73]</ref> utilized VW images from the UBIRIS v2 database, which has several perturbations deliberately introduced. As with the above studies with NIR data, combining periocular and iris features improved the overall performance over difficult VW data too. The authors in <ref type="bibr" target="#b29">[30]</ref>  a virtual database, with VW periocular data from UBIRIS v2 and NIR iris data from CASIA Interval. Fusion was carried out at the feature level, with vectors from the two modalities pooled together. They also tested a simple mean fusion rule at the score level, which resulted in a smaller performance improvement. The authors in <ref type="bibr" target="#b79">[80]</ref> used at-a-distance images from CASIA v4 Distance database, with a considerable performance improvement w.r.t. the individual modalities. The authors in <ref type="bibr" target="#b66">[67]</ref> used a VW Light Field Camera (LFC), which provides multiple images at different focus in a single capture. Individuals were also acquired with a conventional digital camera. A superior performance with the LFC camera was observed with both modalities, which was reinforced even more with the fusion. The same databases were used in a posterior study in <ref type="bibr" target="#b67">[68]</ref>, obtaining even better performance. The authors in <ref type="bibr" target="#b71">[72]</ref> used their new CSIP database, acquired with 4 different mobile telephones in 10 different setups.</p><p>Using a sensor-specific color correction technique, they achieved a periocular EER cross-sensor performance of 15.5%. Despite the poor performance of Gabor wavelets applied to the iris modality (34.4%), they achieved a 14.5% EER with the fusion of the two modalities. The authors in <ref type="bibr" target="#b4">[5]</ref> evaluated their Gabor-based periocular system and a set of four iris matchers. They used five different databases, three in NIR and two in VW range, observing that performance of the iris matchers was, in general, much better than the periocular matcher with NIR data, and the opposite with VW data. This is in tune with the literature, which indicates that the iris modality is more suited to NIR illumination <ref type="bibr" target="#b16">[17]</ref>, whereas the periocular modality is best for VW illumination <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b85">86]</ref>. With regards to the fusion, despite the poor performance of the iris matchers with VW data, its fusion with the periocular system resulted with important performance improvements. This is remarkable given the adverse acquisition conditions and the small resolution of the VW databases used. They further extended the study with their SAFE matcher <ref type="bibr" target="#b47">[48]</ref>, and a SIFT matcher. Here, the availability of more machine experts allowed to obtain performance improvements through the fusion also with NIR databases, something not observed in their previous studies. The author in <ref type="bibr" target="#b62">[63]</ref> proposed the fusion of a iris matcher based on multi-lobe differential filters (MLDF), with a periocular expert that parameterizes the shape of eyelids, over VW data of FRGC and UBIRIS v2 databases, with an average 20% of EER improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Sclera modality</head><p>Some works have also made use of features from the sclera region. The authors in <ref type="bibr" target="#b53">[54]</ref> proposed to combine periocular and sclera features for identity verification, observing a significant improvement in EER after the fusion using UBIRIS v1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Face modality</head><p>The authors in <ref type="bibr" target="#b77">[78]</ref> presented a face recognition expert based on Gabor filters applied to each facial landmark (eyes and mouth), with a different classifier employed in each landmark. Face authentication was performed by fusion of the three classifier's output. This way, the face expert is really a fusion of two eye (periocular) experts and one mouth expert. The authors in <ref type="bibr" target="#b48">[49]</ref> used LBP on the FRGC database, extracted both from the periocular region and from the full face. Rather than the best accuracy obtained (first sub-row in Table <ref type="table" target="#tab_5">4</ref>), the interest relies on the impact of the input image quality, demonstrating that, at extreme values of blur or down-sampling, periocular recognition performed significantly better than face. On the other hand, both face and periocular under uncontrolled lighting were very poor, indicating that LBPs are not well suited for this scenario. Another study of the effect of non-ideal conditions was also carried out in <ref type="bibr" target="#b55">[56]</ref>. They masked the face region below the nose to simulate partial face occlusion, showing that face performance is severely degraded in the presence of occlusion, whereas the periocular modality is much more ro-before before bust. The authors in <ref type="bibr" target="#b26">[27]</ref> studied the problem of matching face images before and after undergoing plastic surgery. The rank-one recognition performance reported by the fusion of periocular and face matchers (Rank-1: 87.4%) is the highest accuracy observed in the literature with the utilized database, up to the publication of the study. As full face matchers, they used two COTS systems: PittPatt and VeriLook. The authors in <ref type="bibr" target="#b44">[45]</ref> extracted features in different regions of the face (periocular, nose, mouth), and in the full-face to study the impact of face changes due to gender transformation. They found that the periocular region greatly outperformed other face components (nose, mouth) and the full face. They also observed (not reported in Table <ref type="table" target="#tab_5">4</ref>) that their periocular approach outperformed two Commercial Off The Shelf full face Systems (COTS): PittPatt (by 76.83% in Rank-1 accuracy) and Cognetic FaceVACs (by 56.23%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Soft-biometrics, gender transformation and plastic surgery analysis</head><p>Besides the task of personal recognition, a number of other tasks have been also proposed using features from the periocular region, as shown in Table <ref type="table" target="#tab_7">5</ref>. Soft-biometrics refer to the classification of an individual in broad categories such as gender, ethnicity, age, height, weight, hair color, etc. While these cannot be used to uniquely identify a subject, it can reduce the search space or provide additional information to boost the recognition performance. Due to the popularity of facial recognition, face images have been frequently used to obtain both gender and ethnicity information, with high accuracy ( &gt; 96%, for a summary see <ref type="bibr" target="#b42">[43]</ref>). Recently, it has been also suggested that periocular features can be potentially used for soft-biometrics classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>. With accuracies comparable to these obtained by using the entire face, it indicates the effectiveness of the periocular region by itself for soft-biometrics purposes. The authors in <ref type="bibr" target="#b46">[47]</ref> addressed gender classification using a database of 936 low resolution images collected from the web (Flickr), reporting a 85% classification accuracy. The authors in <ref type="bibr" target="#b42">[43]</ref> studied gender and ethnicity classification over the FRGC and MBGC databases, with an accuracy of 89% or higher in both classification tasks. In a previous paper, they also showed that fusion of the soft-biometrics information with texture features from the periocular region can improve the recognition performance <ref type="bibr" target="#b41">[42]</ref>. The authors in <ref type="bibr" target="#b39">[40]</ref> studied the problem of gender classification with images from the FERET database. The reported classification accuracy is of 90%. An interesting study in <ref type="bibr" target="#b18">[19]</ref> made use of shape features from the eyebrow region only, with very good results over the MBGC/FRGC databases comprising both NIR/VW data (96/97% of gender classification rate, respectively).</p><p>Other studies are related with the effect on the recognition performance of plastic surgery or gender transformation, as presented in Section 6.3 (see Fig. <ref type="figure" target="#fig_1">4</ref> as well). The authors in <ref type="bibr" target="#b44">[45]</ref> studied the impact of gender transformation via Hormone Replacement Theory (HRT), which causes changes in the physical appearance of the face and body gradually over the course of the treatment. A database of &gt;1.2 million face images from YouTube videos was built, with data from 38 subjects undergoing HRT over a period of several months to three years, observing that accuracy of the periocular region greatly outperformed other face components (nose, mouth) and the full face. Also, face matchers began to fail after only a few months of HRT treatment. The authors in <ref type="bibr" target="#b26">[27]</ref> studied the matching of face images before and after undergoing plastic surgery. The work proposed a fusion recognition approach that combines face and periocular information, outperforming previous studies where only full-face matchers were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions and future work</head><p>Periocular recognition has emerged as a promising trait for unconstrained biometrics after demands for increased robustness of face or iris systems, showing a surprisingly high discrimination ability <ref type="bibr" target="#b73">[74]</ref>. The fast-growing uptake of face technologies in social networks and smartphones, as well as the widespread use of surveillance cameras, arguably increases the interest of periocular biometrics. The periocular region has shown to be more tolerant to variability in expression, occlusion, and it has more capability of matching partial faces <ref type="bibr" target="#b35">[36]</ref>. It also finds applicability in other areas such as forensics analysis (crime scene images where perpetrators intentionally mask part of their faces). In such situation, identifying a suspect where only the periocular region is visible is one of the toughest real-world challenges in biometrics. Even in this difficult case, the periocular region can aid in the reconstruction of the whole face <ref type="bibr" target="#b33">[34]</ref>.</p><p>This paper reviews the state of the art in periocular biometrics research. Our target is to provide a comprehensive coverage of the existing literature, giving an insight of the most relevant issues and challenges. We start by presenting existing databases utilized in periocular research. Acquisition setups comprise digital cameras, webcams, videocameras, smartphones, or close-up iris sensors. A small number of databases contain video data of subjects walking through an acquisition portal, or in hallways or atria. There are databases for particular problems too, such as aging, plastic surgery effects, gender transformation effects, expression changes, or cross-spectral matching. However, the use of databases acquired with personal devices such as smartphones or tablets is limited, with recognition accuracy still some steps behind <ref type="bibr" target="#b71">[72]</ref>. The same can be said about surveillance cameras <ref type="bibr" target="#b34">[35]</ref>. New sensors are being proposed, such as Light Field Cameras, which capture multiple images at different focus in a single capture <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, guaranteeing to have a good focused image. Since the periocular modality requires less constrained acquisition than other ocular or face modalities, it is likely that the research community will move towards exploring ocular recognition at a distance and on the move in more detail as compared to previous studies <ref type="bibr" target="#b51">[52]</ref>.</p><p>Automatic detection and/of segmentation of the periocular region has been increasingly addressed as well, avoiding the need of segmenting the iris or detecting the full face first (Table <ref type="table" target="#tab_1">2</ref>). Recently, the use of eye corners as reference points to define the periocular ROI has been suggested, instead of the eye center, since eye corners are less sensitive to gaze variations and also appear in closed eyes <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b63">64]</ref>. We further review the features employed for periocular recognition, which comprises the majority of works in the literature. They can be classified into global and local approaches (Fig. <ref type="figure">3</ref>). Some works have also addressed the task of assessing if there are regions of the periocular area more useful than others for recognition purposes. This has been done both by asking to humans <ref type="bibr" target="#b25">[26]</ref> and by using several machine algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b78">79]</ref>, with both humans and machines agreeing in the usefulness of different parts. Automatic segmentation of periocular parts can aid in avoiding those which are non-useful, as well as other elements such as hair or glasses, that can also deteriorate the recognition performance, as shown by The authors in <ref type="bibr" target="#b65">[66]</ref> in the first work which present an algorithm to segment components of the periocular region. Since the periocular area appears in face and iris images, comparison and fusion with these modalities has been also proposed, with a review of related works also given (Table <ref type="table" target="#tab_5">4</ref>). Fusion of multiple modalities using ocular data is a promising path forward that is receiving increasing attention <ref type="bibr" target="#b51">[52]</ref> due to unconstrained environments where switching between available modalities may be necessary <ref type="bibr" target="#b5">[6]</ref>.</p><p>Soft-biometrics is another area where the periocular modality has found applicability, with periocular features showing accuracies comparable to these obtained by using the entire face for the tasks of gender and ethnicity classification (Table <ref type="table" target="#tab_7">5</ref>). The periocular modality is also shown to aid or outperform face matchers in case of undergoing plastic surgery or gender transformation. Another issues that are receiving increasing attention is cross-modality <ref type="bibr" target="#b28">[29]</ref>, cross-spectral <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b75">76]</ref>, hyper-spectral <ref type="bibr" target="#b82">[83]</ref> or cross-sensor <ref type="bibr" target="#b71">[72]</ref> matching. The periocular modality also has the potential to allow ocular recognition at large stand-off distances <ref type="bibr" target="#b14">[15]</ref>, with applications in surveillance. Samples captured with different sensors are to be matched if, for example, people are allowed to use their own smartphones or surveillance cameras, or when new or improved sensors have to co-exist with existing ones (cross-sensor), not to mention if the sensors work in different spectral range (cross-spectral). Iris images are traditionally acquired in NIR spectrum, whereas face images normally are captured with VW sensors. Exchange of biometric information between different law enforcement agencies worldwide also poses similar problems. These are examples of some scenarios where, if biometrics is extensively deployed, data acquired from heterogeneous sources will have to co-exist <ref type="bibr" target="#b5">[6]</ref>. These issues are of high interest in new scenarios arising from the widespread use of biometric technologies and the availability of multiple sensors and vendor solutions. Another important direction therefore is to enable periocular heterogeneous data to work together <ref type="bibr" target="#b51">[52]</ref>.</p><p>Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Left: elements of the periocular region. Right: region of interest around the eye for feature extraction. Image from UBIRIS v2 database.</figDesc><graphic coords="2,32.48,57.44,251.40,90.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Samples of subjects before/after undergoing gender transformation and plastic surgery. Images are from Mahalingam et al., (2004) and Jillela et al., (2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Overview of existing automatic eye/periocular detection and segmentation works. The acronyms of this table are fully defined in the text or in the referenced papers. Column 'task' stands for: D = Detection, S = Segmentation.</figDesc><table><row><cell>Approach</cell><cell>Features</cell><cell>Task</cell><cell>Training</cell><cell>Database</cell><cell>Best accuracy</cell></row><row><cell>[78]</cell><cell>Gabor filters</cell><cell>D</cell><cell>M2VTS (202 VW images)</cell><cell>M2VTS (349 VW images)</cell><cell>99.3% (M2VTS)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>XM2VTS (2388 VW images)</cell><cell>99% (XM2VTS)</cell></row><row><cell>[35]</cell><cell>Active Shape Models (ASM)</cell><cell>D</cell><cell>MBGC (VW images)</cell><cell>Compass (3200 VW images)</cell><cell>n/a</cell></row><row><cell>[81]</cell><cell>Viola-Jones (VJ) detector of</cell><cell>D</cell><cell>n/a</cell><cell>CASIA distance v4 (282 NIR images)</cell><cell>96.4% (NIR)</cell></row><row><cell></cell><cell>face sub-parts (OpenCV)</cell><cell></cell><cell></cell><cell>Yale-B (252 VW images)</cell><cell>99.2% (VW)</cell></row><row><cell>[88]</cell><cell>HSV color space + convex hull</cell><cell>D,S</cell><cell>n/a</cell><cell>UBIRIS v1 (1877 VW images)</cell><cell>n/a</cell></row><row><cell>[28]</cell><cell>Correlation filters</cell><cell>D</cell><cell>1000 eye images</cell><cell>FOCS (404 NIR images)</cell><cell>95%</cell></row><row><cell>[41]</cell><cell>LE-ASM + graph-cut</cell><cell>D,S</cell><cell>MBGC (500 VW images)</cell><cell>MBGC (200 still VW images)</cell><cell>F-measure: 99.4%</cell></row><row><cell>[45]</cell><cell>Correlation filters</cell><cell>D</cell><cell>n/a</cell><cell>HRT (VW images)</cell><cell>n/a</cell></row><row><cell>[54]</cell><cell>HSV color space</cell><cell>D,S</cell><cell>n/a</cell><cell>UBIRIS v1 (1877 VW images)</cell><cell>n/a</cell></row><row><cell>[63]</cell><cell>HSV + YCbCr color spaces</cell><cell>D,S</cell><cell>n/a</cell><cell>UBIRIS v2 /FRGC (2340/4360 VW images)</cell><cell>n/a</cell></row><row><cell>[66]</cell><cell>Texture/shape descriptors</cell><cell>S</cell><cell>UBIRIS v2 (35 VW images)</cell><cell>UBIRIS v2 (200 VW images)</cell><cell>97.5%</cell></row><row><cell>[5]</cell><cell>Symmetry filters</cell><cell>D</cell><cell>NO</cell><cell>6 iris datasets: 4 NIR, 2 VW</cell><cell>96% (NIR)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(6932 NIR images, 3050 VW)</cell><cell>79% (VW)</cell></row><row><cell>[83]</cell><cell>VJ eye-pair + hough</cell><cell>D</cell><cell>n/a</cell><cell>MBGC (VW, NIR), UBIPr (VW)</cell><cell>n/a</cell></row><row><cell></cell><cell>VJ eye-pair + morphology</cell><cell></cell><cell>n/a</cell><cell>CMU-H</cell><cell>n/a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Taxonomy of periocular features. The acronyms are fully defined in the text or in the referenced papers. Overview of existing periocular recognition works in chronological order. The acronyms of this table are fully defined in the text or in the referenced papers.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LOCAL</cell></row><row><cell>BGM BSIF CRBM DCT DWT Force fields Gabor filters</cell><cell>Textural GIST HOG, PHOG JDSR Laws Masks LBP LMF LoG</cell><cell>LPQ NGC PDM PIGP SRC SRP Walsh masks</cell><cell>Shape Eyelids Eyebrows Color LCH</cell><cell>BRISK ORB PILP SAFE SIFT m-SIFT SURF</cell></row><row><cell>Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p>(continued on next page)</p>Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p>ARTICLE IN PRESS</p>JID: PATREC [m5G;</p>October 18, 2015;18:58]    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">(continued)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Best accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>Features evaluated</cell><cell>Test Database</cell><cell>Features</cell><cell># eyes</cell><cell>EER</cell><cell>Rank-1</cell></row><row><cell>[51]</cell><cell>PCA to: CRBM, SIFT, LBP,</cell><cell>UBIPr (10252 VW images)</cell><cell>CRBM-PCA/all</cell><cell>One</cell><cell>10/6.4%</cell><cell>n/a/50.1%</cell></row><row><cell></cell><cell>HOG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[54]</cell><cell>Directional projections</cell><cell>UBIRIS v1 (1877 VW images)</cell><cell>SRP</cell><cell>One</cell><cell>6.52%</cell><cell>n/a</cell></row><row><cell></cell><cell>(SRP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[63]</cell><cell>LBP to eyelids region,</cell><cell>FRGC (4360 VW images)</cell><cell>LBP+EFD</cell><cell>One</cell><cell>&lt;25%</cell><cell>n/a</cell></row><row><cell></cell><cell>eyelids shape (EFD)</cell><cell>UBIRIS v2 (2340 VW images)</cell><cell>LBP+EFD</cell><cell>One</cell><cell>&lt;24%</cell><cell>n/a</cell></row><row><cell>[64]</cell><cell>GC-EGM to:</cell><cell>FaceExpressUBI (90160 VW images)</cell><cell>GC-EGM</cell><cell>One</cell><cell>16%</cell><cell>n/a</cell></row><row><cell></cell><cell>LBP+HOG+SIFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[66]</cell><cell>LBP, HOG, SIFT</cell><cell>UBIRIS v2 (5551 VW images)</cell><cell>LBP+HOG+SIFT</cell><cell>One</cell><cell>9.5%</cell><cell>n/a</cell></row><row><cell>[68]</cell><cell>BSIF</cell><cell>Proprietary, light-field and digital</cell><cell>Light-field: BSIF</cell><cell>One</cell><cell>3.39%</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell>Cameras (420 VW images each)</cell><cell>Digital camera: BSIF</cell><cell>One</cell><cell>3.96%</cell><cell>n/a</cell></row><row><cell>[72]</cell><cell>LBP, HOG, SIFT, ULBP, GIST</cell><cell>CSIP (2004 VW images)</cell><cell>LBP/HOG/SIFT/ULBP/</cell><cell>Bne</cell><cell>30.5/30.8/34.3/25.9/</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GIST/all</cell><cell></cell><cell>16.3/15.5%</cell><cell></cell></row><row><cell>[76]</cell><cell>LBP, HOG, PHOG,</cell><cell>IMP: IITD Multispectral</cell><cell>VW: PHOG+NN</cell><cell>Both</cell><cell>∼8%</cell><cell>n/a</cell></row><row><cell></cell><cell>FPLBP, PHOG+NN</cell><cell>(310 VW, 310 night, 620 NIR images)</cell><cell>Night: PHOG+NN</cell><cell>Both</cell><cell>∼7%</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell>NIR: PHOG+NN</cell><cell>Both</cell><cell>∼3.5%</cell><cell>n/a</cell></row><row><cell>[9]</cell><cell>PILP, SIFT, SURF</cell><cell>Bath (32000 NIR images)</cell><cell>Any feature</cell><cell>One</cell><cell>n/a</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell>CASIA Lamp v3 (16212 NIR images)</cell><cell>PILP/SIFT</cell><cell>One</cell><cell>n/a</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell>UBIRIS v2 (11102 VW images)</cell><cell>PILP</cell><cell>One</cell><cell>n/a</cell><cell>87.62%</cell></row><row><cell></cell><cell></cell><cell>FERET (14126 VW images)</cell><cell>PILP</cell><cell>One</cell><cell>n/a</cell><cell>85.8%</cell></row><row><cell>[83]</cell><cell>Raw pixels, LBP, PCA,</cell><cell>MGBC (NIR portal images)</cell><cell>LBP</cell><cell>One</cell><cell>n/a</cell><cell>99.8%</cell></row><row><cell></cell><cell>LBP+PCA</cell><cell>MGBC (VW portal images)</cell><cell>LBP+PCA</cell><cell>One</cell><cell>n/a</cell><cell>98.5%</cell></row><row><cell></cell><cell></cell><cell>CMU Hyperspectral</cell><cell>PCA</cell><cell>One</cell><cell>n/a</cell><cell>97.2%</cell></row><row><cell></cell><cell></cell><cell>UBIPr (VW images)</cell><cell>LBP</cell><cell>One</cell><cell>n/a</cell><cell>99.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Overview of existing works on comparison and fusion of the periocular modality with other biometric modalities. The acronyms of this table are fully defined in the text or in the referenced papers. Features with best accuracy are those giving the best fusion results. If no fusion results are available, they indicate the best features of each individual modality. The following acronyms are not defined elsewhere: 'w-sum' = 'weighted sum', 'LR ' = 'logistic regression', 'NN ' = 'Neural Networks', 'TERELM ' = 'Total Error Rate Minimization', 'LG' = 'Log-Gabor'.</figDesc><table><row><cell cols="2">JID: PATREC</cell><cell></cell><cell cols="4">ARTICLE IN PRESS</cell><cell></cell><cell cols="3">[m5G;October 18, 2015;18:58]</cell></row><row><cell cols="2">Comparison with the iris modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Best accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Features with best accuracy</cell><cell>Fusion</cell><cell></cell><cell>Periocular</cell><cell></cell><cell>Iris</cell><cell></cell><cell>Fusion</cell><cell></cell></row><row><cell cols="2">Approach Periocular</cell><cell>Iris</cell><cell>Method</cell><cell>Test database</cell><cell cols="6">EER (%) Rank-1 (%) EER (%) Rank-1 (%) EER (%) Rank-1 (%)</cell></row><row><cell>[85]</cell><cell>LBP</cell><cell>Gabor</cell><cell>w-sum</cell><cell>MBGC (1052 NIR portal images)</cell><cell>21</cell><cell>92.5</cell><cell>32</cell><cell>13.81</cell><cell>18</cell><cell>96.5</cell></row><row><cell>[12]</cell><cell>BGM</cell><cell>Gabor</cell><cell>-</cell><cell>FOCS (9581 NIR images)</cell><cell>23.81</cell><cell>94.2</cell><cell>30.8</cell><cell>88.7</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>[30]</cell><cell>LBP</cell><cell>Wavelets</cell><cell>DLDA</cell><cell>UBIRIS v2 (2400 VW images) +</cell><cell>12.94</cell><cell>81.03</cell><cell>12.07</cell><cell>88.79</cell><cell>6.9</cell><cell>96.55</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mean</cell><cell>CASIA Interval (2400 NIR images)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.5</cell><cell>83.62</cell></row><row><cell>[70]</cell><cell>HOG, m-SIFT, PDM</cell><cell>LG</cell><cell>-</cell><cell>FOCS (9581 NIR images)</cell><cell>18.8</cell><cell>n/a</cell><cell>33.1</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>[73]</cell><cell>LBP, SIFT</cell><cell>Wavelets,</cell><cell>LR</cell><cell>UBIRIS v2 (1000 VW images)</cell><cell>31.87</cell><cell>56.4</cell><cell>23.12</cell><cell>41.9</cell><cell>18.48</cell><cell>74.3</cell></row><row><cell></cell><cell></cell><cell>Gabor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[80]</cell><cell>SIFT, LMF</cell><cell>LG</cell><cell>w-sum</cell><cell cols="2">CASIA v4 Distance (2567 NIR images) n/a</cell><cell>∼67</cell><cell>n/a</cell><cell>∼54</cell><cell>n/a</cell><cell>84.5</cell></row><row><cell>[67]</cell><cell>LBP+SRC</cell><cell>LBP+SRC</cell><cell>w-sum</cell><cell>Light-field camera (420 VW images)</cell><cell>12.04</cell><cell>n/a</cell><cell>1.2</cell><cell>n/a</cell><cell>0.81</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Digital camera (420 VW images)</cell><cell>16.21</cell><cell>n/a</cell><cell>8.24</cell><cell>n/a</cell><cell>7.45</cell><cell>n/a</cell></row><row><cell>[63]</cell><cell>LBP + eyelids shape</cell><cell>MLDF</cell><cell>Sum</cell><cell>FRGC (4360 VW images)</cell><cell>&lt;25</cell><cell>n/a</cell><cell>&lt;11</cell><cell>n/a</cell><cell>&lt;8.5</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UBIRIS v2 (2340 VW images)</cell><cell>&lt; 24</cell><cell>n/a</cell><cell>&lt; 11</cell><cell>n/a</cell><cell>&lt; 9</cell><cell>n/a</cell></row><row><cell>[68]</cell><cell>BSIF</cell><cell>BSIF</cell><cell>w-sum</cell><cell>Light-field camera (420 VW images)</cell><cell>3.39</cell><cell>n/a</cell><cell>0.72</cell><cell>n/a</cell><cell>0.61</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Digital camera (420 VW images)</cell><cell>3.96</cell><cell>n/a</cell><cell>3.46</cell><cell>n/a</cell><cell>2.02</cell><cell>n/a</cell></row><row><cell>[72]</cell><cell>LBP+HOG+SIFT+</cell><cell>Gabor</cell><cell>NN</cell><cell>CSIP (2004 VW images)</cell><cell>15.5</cell><cell>n/a</cell><cell>34.4</cell><cell>n/a</cell><cell>14.5</cell><cell>n/a</cell></row><row><cell></cell><cell>ULBP+GIST</cell><cell>filters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[4,5]</cell><cell>Gabor filters</cell><cell>LG</cell><cell>Mean</cell><cell>BioSec (1200 NIR images)</cell><cell>10.56</cell><cell>66</cell><cell>1.12</cell><cell>98</cell><cell>1.96</cell><cell>96</cell></row><row><cell></cell><cell>Gabor filters</cell><cell>LG</cell><cell>Mean</cell><cell>Casia Interval v3 (2655 NIR images)</cell><cell>14.53</cell><cell>n/a</cell><cell>0.67</cell><cell>n/a</cell><cell>2.38</cell><cell>n/a</cell></row><row><cell></cell><cell>Gabor filters</cell><cell>LG</cell><cell>Mean</cell><cell>IIT Delhi v1.0 (2240 NIR images)</cell><cell>2.5</cell><cell>n/a</cell><cell>0.59</cell><cell>n/a</cell><cell>1.2</cell><cell>n/a</cell></row><row><cell></cell><cell>Gabor filters</cell><cell>LG</cell><cell>Mean</cell><cell>MobBIO (800 VW images)</cell><cell>12.32</cell><cell>75</cell><cell>18.81</cell><cell>56</cell><cell>11</cell><cell>77</cell></row><row><cell></cell><cell>Gabor filters</cell><cell>LG</cell><cell>Mean</cell><cell>UBIRIS v2 (2250 VW images)</cell><cell>24.4</cell><cell>n/a</cell><cell>34.94</cell><cell>n/a</cell><cell>22.41</cell><cell>n/a</cell></row><row><cell>[7]</cell><cell>Gabor, SAFE, SIFT</cell><cell>LG, DCT</cell><cell>Mean</cell><cell>BioSec (1200 NIR images)</cell><cell>8.5</cell><cell>n/a</cell><cell>1.12</cell><cell>n/a</cell><cell>0.75</cell><cell>n/a</cell></row><row><cell>[48]</cell><cell>SAFE, SIFT</cell><cell>LG, DCT,</cell><cell>Mean</cell><cell>Casia Interval v3 (2655 NIR images)</cell><cell>7.52</cell><cell>n/a</cell><cell>0.67</cell><cell>n/a</cell><cell>0.51</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell>SIFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SIFT</cell><cell>LG</cell><cell>Mean</cell><cell>IIT Delhi v1.0 (2240 NIR images)</cell><cell>0.8</cell><cell>n/a</cell><cell>0.59</cell><cell>n/a</cell><cell>0.38</cell><cell>n/a</cell></row><row><cell></cell><cell>Gabor, SAFE, SIFT</cell><cell>LG</cell><cell>Mean</cell><cell>MobBIO (800 VW images)</cell><cell>8.73</cell><cell>n/a</cell><cell>18.81</cell><cell>n/a</cell><cell>6.75</cell><cell>n/a</cell></row><row><cell></cell><cell>Gabor, SAFE, SIFT</cell><cell>LG, DCT,</cell><cell>Mean</cell><cell>UBIRIS v2 (2250 VW images)</cell><cell>24.4</cell><cell>n/a</cell><cell>35.61</cell><cell>n/a</cell><cell>15.17</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell>SIFT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Comparison with the sclera modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Best accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Features with best accuracy</cell><cell>Fusion</cell><cell></cell><cell>Periocular</cell><cell></cell><cell>Sclera</cell><cell></cell><cell>Fusion</cell><cell></cell></row><row><cell cols="2">Approach Periocular</cell><cell>Sclera</cell><cell>Method</cell><cell>Test database</cell><cell cols="6">EER (%) Rank-1 (%) EER (%) Rank-1 (%) EER (%) Rank-1 (%)</cell></row><row><cell>[54]</cell><cell>SRP</cell><cell>MLBP</cell><cell cols="2">TERELM UBIRIS v1 (1877 VW images)</cell><cell>6.52</cell><cell>n/a</cell><cell>8.44</cell><cell>n/a</cell><cell>3.26</cell><cell>n/a</cell></row><row><cell cols="2">Comparison with the face modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Best accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Features with best accuracy</cell><cell>Fusion</cell><cell></cell><cell>Periocular</cell><cell></cell><cell>Face</cell><cell></cell><cell>Fusion</cell><cell></cell></row><row><cell cols="2">Approach Periocular</cell><cell>Face</cell><cell>Method</cell><cell>Test database</cell><cell cols="6">EER (%) Rank-1 (%) EER (%) Rank-1 (%) EER (%) Rank-1 (%)</cell></row><row><cell>[78]</cell><cell>Gabor filters</cell><cell>Gabor</cell><cell>w-sum</cell><cell>M2VTS (349 VW images)</cell><cell>0.3</cell><cell>n/a</cell><cell>0.13</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell>filters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[49]</cell><cell>LBP</cell><cell>LBP</cell><cell>-</cell><cell>FRGC (VW images)</cell><cell>n/a</cell><cell>99.5</cell><cell>n/a</cell><cell>99.75</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRGC -blur (kernel = 7 pix, σ = 1.5)</cell><cell>n/a</cell><cell>77.86</cell><cell>n/a</cell><cell>31.09</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRGC -downsampling (40%)</cell><cell>n/a</cell><cell>97.76</cell><cell>n/a</cell><cell>70.40</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRGC -uncontrolled lightning</cell><cell>n/a</cell><cell>11.17</cell><cell>n/a</cell><cell>12.18</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>[56]</cell><cell>HOG, LBP, SIFT</cell><cell>FaceVACS</cell><cell>-</cell><cell>FRGC (1704 VW images)</cell><cell>n/a</cell><cell>87.32</cell><cell>n/a</cell><cell>99.77</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRGC -partial face</cell><cell>n/a</cell><cell>∼84</cell><cell>n/a</cell><cell>39.55</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>[27]</cell><cell>SIFT, LBP</cell><cell>VeriLook,</cell><cell>w-sum</cell><cell>Plastic surgery (1800 VW images)</cell><cell>n/a</cell><cell>63.9</cell><cell>n/a</cell><cell>85.3</cell><cell>n/a</cell><cell>87.4</cell></row><row><cell></cell><cell></cell><cell>PittPatt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[45]</cell><cell>TPLBP</cell><cell>TPLBP</cell><cell>-</cell><cell>HRT (&gt;1.2 mill. VW images)</cell><cell>35.21</cell><cell>57.79</cell><cell>38.60</cell><cell>46.49</cell><cell>n/a</cell><cell>n/a</cell></row></table><note><p>), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>used Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</figDesc><table><row><cell>JID: PATREC</cell><cell>ARTICLE IN PRESS</cell><cell>[m5G;October 18, 2015;18:58]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Overview of existing works on soft-biometrics, gender transformation and plastic surgery analysis using periocular features. The acronyms of this table are fully defined in the text or in the referenced papers. The following acronyms are not defined elsewhere: 'SVM'='Support Vector Machines'. = 39.24%Face: EER = 38.6%, Rank-1 = 46.69% PittPatt: EER = n/a, Rank-1 = 36.99% FaceVACS: EER = n/a, Rank-1 = 29.37%</figDesc><table><row><cell>JID: PATREC</cell><cell></cell><cell cols="2">ARTICLE IN PRESS</cell><cell>[m5G;October 18, 2015;18:58]</cell></row><row><cell>Approach</cell><cell>Purpose</cell><cell>Features</cell><cell>Database</cell><cell>Best accuracy</cell></row><row><cell>[47]</cell><cell>Gender classification</cell><cell>Raw pixels, LBP + LDA-NN/PCA-NN/SVM</cell><cell>Proprietary (936 VW images)</cell><cell>Gender: 85% classification rate</cell></row><row><cell>[19]</cell><cell>Gender classification</cell><cell>Eyebrows shape + MD/LDA/SVM</cell><cell>FRGC (800 VW images)</cell><cell>Gender: 97%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MBGC (922 NIR portal images)</cell><cell>Gender: 96%</cell></row><row><cell>[27]</cell><cell>Impact of</cell><cell>Periocular: SIFT, LBP</cell><cell>Plastic surgery</cell><cell>Rank-1: SIFT = 48.1%, LBP = 45.6%</cell></row><row><cell></cell><cell>plastic surgery</cell><cell>Face: VeriLook (VL), PittPatt (PP)</cell><cell>(1800 VW images)</cell><cell>SIFT+LBP = 63.9%, VL = 73.9%, PP = 81.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>VL+PP = 85.3%, VL+PP+SIFT+LBP = 87.4%</cell></row><row><cell>[40]</cell><cell>Gender classification</cell><cell>ICA + NN</cell><cell>FERET (200 VW images)</cell><cell>Gender: 90% classification rate</cell></row><row><cell>[43]</cell><cell>Gender/ethnicity</cell><cell>LBP/HOG/DCT/LCH + ANN/SVM</cell><cell>FRGC (4232 VW images)</cell><cell>Gender: 97.3%, Ethnicity = 94%</cell></row><row><cell></cell><cell>classification</cell><cell></cell><cell>MBGC (350 NIR portal images)</cell><cell>Gender: 90%, Ethnicity = 89%</cell></row><row><cell>[45]</cell><cell>Impact of gender</cell><cell>Face parts: LBP, TPLBP, HOG</cell><cell>HRT (&gt;1.2 million</cell><cell>Periocular: EER = 35.21%, Rank-1 = 57.79%</cell></row><row><cell></cell><cell>transformation</cell><cell>Face: PittPatt, FaceVACS</cell><cell>VW images)</cell><cell>Nose: EER = 41.82%, Rank-1 = 44.57%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mouth: EER = 43.25%, Rank-1</cell></row></table><note><p>Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters (2015), http://dx.doi.org/10.1016/j.patrec.2015.08.026</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Author F. A.-F. thanks the Swedish Research Council grant 2012-4313 and the Marie-Curie IEF grant 254261 for funding his research. Authors acknowledge the CAISR program of the Swedish Knowledge Foundation and the EU COST Action IC1106.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: PATREC [m5G; October 18, 2015;18:58]   </p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2567 images 2352 × 1728 NIR No No No No No No n/a 67% FaceExpressUBI [10] 2 90,160 images 2056 × 2452 Yes VW No No Yes Yes No No 16% n/a IRIS DATABASES BioSec [20] 2 3200 images 480 × 640 Yes NIR No No No No No No 10.56% 66% CASIA Interval v3 [16] 2 2655 images 280 × 320 Yes NIR No No No No No No 8.45% n/a UBIRIS v2 [65] 2 11,102 images 300 × 400 Yes VW No Yes No Yes No Yes 9.5% 87.62% IIT Delhi v1.0 [39] 1 2240 images 240 × 320 Yes NIR No No No No No No 1.88% n/a MobBIO [75] 1 800 images 200 × 240 Yes VW No</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>JID: PATREC [m5G; <ref type="bibr">October 18, 2015;</ref><ref type="bibr">18:58]</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Genetic-based type ii feature extraction for periocular biometric recognition: less is more</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Woodard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dozier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Glenn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition, ICPR</title>
		<meeting>the International Conference on Pattern Recognition, ICPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="205" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Periocular recognition using retinotopic sampling and gabor decomposition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on What&apos;s in a Face? WIAF, in conjunction with European Conf Computer Vision, ECCV, Springer LNCS-7584</title>
		<meeting>the International Workshop on What&apos;s in a Face? WIAF, in conjunction with European Conf Computer Vision, ECCV, Springer LNCS-7584</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Best regions for periocular recognition with NIR and visible images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing, ICIP</title>
		<meeting>the International Conference on Image Processing, ICIP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eye detection by complex filtering for periocular recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop Biometrics &amp; Forensics</title>
		<meeting>the International Workshop Biometrics &amp; Forensics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Near-infrared and visible-light periocular recognition with gabor features using frequency-adaptive automatic eye detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="89" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quality-based conditional processing in multi-biometrics: application to sensor interoperability</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part A: Syst. Humans</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1168" to="1179" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison and fusion of multiple iris and periocular matchers using near-infrared and visible images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mikaelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop Biometrics and Forensics</title>
		<meeting>the International Workshop Biometrics and Forensics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phase intensive global pattern for periocular recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Majhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual IEEE India Conference INDICON</title>
		<meeting>the Annual IEEE India Conference INDICON</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel phase-intensive local pattern for periocular recognition under visible spectrum</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Majhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybern. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expressions: Discriminability of facial regions and relationship to biometrics recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Computational Intelligence in Biometrics and Identity Management</title>
		<meeting>the IEEE Workshop on Computational Intelligence in Biometrics and Identity Management</meeting>
		<imprint>
			<publisher>CIBIM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Periocular biometrics: When iris recognition fails</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Biometrics: Theory, Applications and Systems</title>
		<meeting>the IEEE Conference on Biometrics: Theory, Applications and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparative evaluation of iris and ocular recognition methods on challenging ocular images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smereka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Biometrics, IJCB</title>
		<meeting>the International Joint Conference on Biometrics, IJCB</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image understanding for iris biometrics: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Visi. Image Und</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="281" to="307" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<idno>07.10.15</idno>
		<ptr target="http://www.vision.caltech.edu/html-files/archive.html" />
		<title level="m">Caltech face database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Matching heterogeneous periocular regions: Short and long standoff distances</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4967" to="4971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<idno>07.10.15</idno>
		<ptr target="http://biometrics.idealtest.org/" />
		<title level="m">CASIA databases</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How iris recognition works</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Metes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>CMU- RI-TR-02-25</idno>
		<title level="m">Hyperspectral Face Database</title>
		<meeting><address><addrLine>Robotics Institute. Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eyebrow shape-based features for biometric recognition and gender classification: a feasibility study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Biometrics, IJCB</title>
		<meeting>the International Joint Conference on Biometrics, IJCB</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BioSec baseline corpus: a multimodal biometric database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Torre-Toledano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1389" to="1392" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust periocular biometrics based on local phase quantisation and gabor transform</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gangwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Congress on Image and Signal Processing</title>
		<meeting>the International Congress on Image and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="714" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://www.anefian.com/research/face_reco.htm" />
		<title level="m">Georgia Tech face database (GTDB)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Demographic estimation from face images: Human vs. machine performance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A ground truth for iris segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hofbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition, ICPR</title>
		<meeting>the International Conference on Pattern Recognition, ICPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identifying useful features for recognition in near-infrared periocular images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics: Theory Applications and Systems</title>
		<meeting>the International Conference on Biometrics: Theory Applications and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human and machine performance on periocular biometrics under near-infrared light and visible light</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Darnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woodard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="588" to="601" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mitigating effects of plastic surgery: Fusing face and ocular biometrics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jillela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics: Theory, Applications and Systems</title>
		<meeting>the International Conference on Biometrics: Theory, Applications and Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Iris Segmentation for Challenging Periocular Images, Handbook of Iris Recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jillela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauca</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="281" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching face against iris images using periocular information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Jillela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person recognition based on fusion of iris and periocular biometrics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gangwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Saquib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Hybrid Intelligent Systems</title>
		<meeting>the International Conference on Hybrid Intelligent Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Periocular recognition based on gabor and parzen pnn</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gangwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Saquib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4977" to="4981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust local binary pattern feature sets for periocular biometric identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abiantun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Biometrics: Theory, Applications and Systems</title>
		<meeting>the IEEE Conference on Biometrics: Theory, Applications and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Investigating age invariant face recognition based on periocular biometrics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Biometrics, IJCB</title>
		<meeting>the International Joint Conference on Biometrics, IJCB</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hallucinating the full face from the periocular region via dimensionally weighted k-svd</title>
		<author>
			<persName><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unconstrained periocular biometric acquisition and recognition using cots ptz camera for uncooperative and non-cooperative subjects</title>
		<author>
			<persName><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Applications of Computer Vision</title>
		<meeting>the IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<publisher>WACV</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Subspace-based discrete transform encoded local binary patterns representations for robust periocular matching on nist face recognition grand challenge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3490" to="3505" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On identification from periocular region utilizing SIFT and SURF</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karaoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Uludag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Signal Processing Conference</title>
		<meeting>the European Signal Processing Conference</meeting>
		<imprint>
			<publisher>EUSIPCO</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1392" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The PUT face database</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Florek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Process. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparison and combination of iris matchers for reliable personal authentication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1016" to="1026" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Periocular gender classification using global ICA features for poor quality images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Majhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Modelling Optimization and Computing</title>
		<meeting>the International Conference on Modelling Optimization and Computing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="945" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A novel eyebrow segmentation and eyebrow shapebased identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Intl Joint Conf Biometrics</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>IJCB</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Soft biometric classification using periocular region features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pundlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics: Theory Applications and Systems</title>
		<meeting>the International Conference on Biometrics: Theory Applications and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Soft biometric classification using local appearance periocular region features</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pundlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3877" to="3885" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">LBP-based periocular recognition on challenging face datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mahalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Image Video Process</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Investigating the periocular-based face recognition across gender transformation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mahalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forens. Secur</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2180" to="2192" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<idno>24</idno>
		<ptr target="http://www2.ece.ohio-state.edu/˜aleix/ARdatabase.html" />
		<title level="m">The AR Face Database</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An exploration of gender identification using only the periocular region</title>
		<author>
			<persName><forename type="first">J</forename><surname>Merkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics: Theory Applications and Systems</title>
		<meeting>the International Conference on Biometrics: Theory Applications and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Periocular recognition by detection of local symmetry patterns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mikaelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop Insight on Eye Biometrics, IEB, in conjunction with the Intl Conf Signal Image Technology and Internet Based Systems</title>
		<meeting>the Workshop Insight on Eye Biometrics, IEB, in conjunction with the Intl Conf Signal Image Technology and Internet Based Systems<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Performance evaluation of local appearance based periocular recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pundlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Biometrics: Theory, Applications, and Systems</title>
		<meeting>the IEEE International Conference on Biometrics: Theory, Applications, and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Personal identification using periocular skin texture</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pundlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Applied Computing (SAC)</title>
		<meeting>the ACM Symposium on Applied Computing (SAC)<address><addrLine>Sierre, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">March 22-26, 2010, 2010</date>
			<biblScope unit="page" from="1496" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Periocular recognition using unsupervised convolutional RBM feature learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition, ICPR</title>
		<meeting>the International Conference on Pattern Recognition, ICPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="399" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ocular biometrics: A survey of modalities and fusion approaches</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On projection-based methods for periocular identity verification</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Industrial Electronics and Applications</title>
		<meeting>the Conference on Industrial Electronics and Applications</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="871" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Combining sclera and periocular features for multi-modal identity verification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Periocular recognition: Analysis of performance degradation factors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Padole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics</title>
		<meeting>the International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="439" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Periocular biometrics in the visible spectrum</title>
		<author>
			<persName><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Jillela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensic Secur</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="96" to="106" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Periocular biometrics in the visible spectrum: A feasibility study</title>
		<author>
			<persName><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Biometrics: Theory, Applications, and Systems</title>
		<meeting>the IEEE International Conference on Biometrics: Theory, Applications, and Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distinguishing identical twins by face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bruegge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pruitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for facerecognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sahibzada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Scallan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics, ICB, LNCS-5558</title>
		<meeting>the International Conference on Biometrics, ICB, LNCS-5558</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The m2vts multimodal face database (release 1.00)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pigeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandendorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Audio-and Video-Based Biometric Person Authentication</title>
		<meeting>the International Conference on Audio-and Video-Based Biometric Person Authentication</meeting>
		<imprint>
			<date type="published" when="1206">1206, 1997</date>
			<biblScope unit="page" from="403" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ocular biometrics by score-level fusion of disparate experts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="5082" to="5093" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Periocular biometrics: constraining the elastic graph matching algorithm to biologically plausible distortions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Briceno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="167" to="175" />
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>IET</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The ubiris.v2: A database of visible wavelength iris images captured on-the-move and at-a-distance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Filipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1529" to="1535" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Segmenting the periocular region using a hierarchical graphical model fed by texture/shape information and geometrical constraints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Biometrics</title>
		<meeting>the International Joint Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Combining iris and periocular recognition using light field camera</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Pattern Recognition</title>
		<meeting>the Asian Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="155" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Binarized statistical features for improved iris and periocular recognition in visible spectrum</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Biometrics and Forensics</title>
		<meeting>the International Workshop on Biometrics and Forensics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">MORPH: a longitudinal image database of normal adult age-progression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Matching highly non-ideal ocular images: An information fusion approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jillela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smereka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics</title>
		<meeting>the International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The role of eyebrows in face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sadr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jarudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="285" to="293" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fusing iris and periocular information for cross-sensor recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grancho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fiadeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A fusion approach to unconstrained iris recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="984" to="990" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Periocular biometrics: An emerging technology for unconstrained scenarios</title>
		<author>
			<persName><forename type="first">G</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Computational Intelligence in Biometrics and Identity Management</title>
		<meeting>the IEEE Workshop on Computational Intelligence in Biometrics and Identity Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="14" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mobbio: a multimodal database captured with a portable handheld device</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Sequeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rebelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<meeting>the International Conference on Computer Vision Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On cross spectral periocular recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Plastic surgery: A new dimension to face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nooreyezdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="441" to="448" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Retinal vision applied to facial features detection and face authentication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Smeraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigün</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="463" to="475" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">What is a good periocular region for recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Smereka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Human identification from at-a-distance images by simultaneously exploiting iris and periocular features</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition, ICPR</title>
		<meeting>the International Conference on Pattern Recognition, ICPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="553" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Combining face with face-part detectors under gaussian assumption</title>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Analysis and Recognition</title>
		<meeting>the International Conference on Image Analysis and Recognition</meeting>
		<imprint>
			<publisher>Springer LNCS-7325</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
	<note>ICIAR</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Periocular biometric recognition using image sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uzair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Applications of Computer Vision</title>
		<meeting>the Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<publisher>WACV</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Periocular region-based person identification in the visible, infrared and hyperspectral imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uzair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="854" to="867" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">On the fusion of periocular and iris biometrics in non-ideal imagery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Woodard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pundlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jillela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR International Conference on Pattern Recognition, ICPR</title>
		<meeting>the IAPR International Conference on Pattern Recognition, ICPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Periocular region appearance cues for biometric identification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Woodard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pundlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition Biometrics Workshop</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition Biometrics Workshop</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The first icb competition on iris recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alonso-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nemesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Biometrics, ICB</title>
		<meeting>the International Conference on Biometrics, ICB</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A new human identification method: Sclera recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delp</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2015.08.026</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patrec.2015.08.026" />
	</analytic>
	<monogr>
		<title level="j">J. Bigun, A survey on periocular biometrics research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="571" to="583" />
			<date type="published" when="2012">2012. 2015</date>
		</imprint>
	</monogr>
	<note>Pattern Recognition Letters</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
