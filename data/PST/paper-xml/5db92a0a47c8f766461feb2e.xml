<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yujiao</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">796B024F144BCDDDE316491D022DE866</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works show that it is possible to train a deep network to determine the geographic location of a ground-level image (e.g., a Google street-view panorama) by matching it against a satellite map covering the wide geographic area of interest. Conventional deep networks, which often cast the problem as a metric embedding task, however, suffer from poor performance in terms of low recall rates. One of the key reasons is the vast differences between the two view modalities, i.e., ground view versus aerial/satellite view. They not only exhibit very different visual appearances, but also have distinctive geometric geometric configurations. Existing deep methods overlook those appearance and geometric differences, and instead use a bruteforce training procedure, leading to inferior performance. In this paper, we develop a new deep network to explicitly address these inherent differences between ground and aerial views. We observe that pixels lying on the same azimuth direction in an aerial image approximately correspond to a vertical image column in the ground view image. Thus, we propose a two-step approach to exploit this prior. The first step is to apply a regular polar transform to warp an aerial image such that its domain is closer to that of a ground-view panorama. Note that polar transform as a pure geometric transformation is agnostic to scene content, hence cannot bring the two domains into full alignment. Then, we add a subsequent spatial-attention mechanism which brings corresponding deep features closer in the embedding space. To improve the robustness of feature representation, we introduce a feature aggregation strategy via learning multiple spatial embeddings. By the above two-step approach, we achieve more discriminative deep representations, facilitating cross-view Geo-localization more accurate. Our experiments on standard benchmark datasets show significant performance boosting, achieving more than doubled recall rate compared with the previous state of the art. Remarkably, the recall rate@top-1 improves from 22.5% in <ref type="bibr" target="#b3">[4]</ref>  (or 40.7% in [10]) to 89.8% on CVUSA benchmark, and from 20.1% [4] to 81.0% on the new CVACT dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image based Geo-localization is referred to the task of determining the location of an image (known as a query image) by comparing it with a large set of Geo-tagged database images. It has important computer vision applications such as for robot navigation, autonomous driving, as well as way-finding in AR/VR applications.</p><p>In this paper, we study ground-to-aerial cross-view image based Geo-localization problem. To be specific, the query image is a normal ground-level image (e.g., a street view image taken by a tourist) whereas the database images are collections of aerial/satellite images covering the same (though 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. wider) geographic region. Cross-view image based localization is a very challenging task because the viewpoints (as well as imaging modality) between ground and aerial images are drastically different; their image visual appearances can also be far apart. As a result, finding feature correspondence between two views (even for a matching pair) can be very challenging. Recently, machine learning techniques (especially deep learning) have been applied to this task, showing promising results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Existing deep neural networks developed for this task often treat the cross-view localization problem as a standard image retrieval task, and are trained to find better image feature embeddings that bring matching image pairs (one from ground view, and one from aerial view) closer while pushing those unmatching pairs far apart. In other words, they cast the problem as a deep metric learning task, and thus learn feature representations purely based on image content (appearance or semantics) without taking into account spatial correspondences between ground and aerial views. To be precise, as seen in Figure <ref type="figure" target="#fig_0">1</ref>(a) and Figure <ref type="figure" target="#fig_0">1</ref>(b), one can easily observe that the locations of objects in an aerial image exhibit a strong spatial relationship with the ones in its corresponding ground image. Furthermore, the relative positions among objects also provide critical clues for the cross-view image matching.</p><p>By exploring such geometric configurations of the scenes, one can significantly reduce the ambiguity of the cross-view image matching problem, and this is the key idea of our paper, which will be described next.</p><p>Unlike conventional approaches, our method focuses on establishing spatial correspondences between these two domains explicitly and then learning feature correspondences from these two coarsely aligned domains. Although deep neural networks are able to learn any functional transformation in theory, explicitly aligning two domains based on geometric correspondences will reduce the burden of the learning process for domain alignment, thus facilitating the network convergence. In our method, we apply polar coordinate transform to aerial images, making it approximately aligned with a ground-view panorama, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(d). After polar transform, we train a Siamese-type network architecture to establish deep feature representation. Since polar transform does not take the scene content into account and the true correspondences between the two different domains are more complex than a simple polar transform, some objects may exhibit distortions. To remedy that, we develop a spatial attention based feature embedding module to extract position-aware features. Precisely, our spatial feature embedding module imposes different attention on different locations and then re-weights features to yield a global descriptor for an input image. In this manner, our method not only retains image content information but also encodes the layout information of object features.</p><p>To achieve robustness of feature representation, we employ a feature aggregation strategy by learning multiple spatial feature embeddings and then aggregating the embedded features. We further employ a triplet loss to establish the feature correspondences between these cross-view images. Our extensive experimental results demonstrate that our method achieves superior Geo-localization performance to the state-of-the-art. Remarkably, the recall rate@top-1 improves from 22.5% in <ref type="bibr" target="#b3">[4]</ref> (or 40.7% in <ref type="bibr" target="#b9">[10]</ref>) to 89.8% on CVUSA benchmark, and from 20.1% <ref type="bibr" target="#b3">[4]</ref> (or 46.9% in <ref type="bibr" target="#b9">[10]</ref>) to 81.0% on the new CVACT dataset.</p><p>Contributions of this paper can be summarized as follows:</p><p>• We propose a new pipeline to address the cross-view Geo-localization problem. We first exploit the geometric correspondences between ground and aerial image domains to align these two domains explicitly by a polar transform, allowing the networks to focus on learning detailed scene-dependent feature correspondences. • We present a spatial-aware attention module to re-weight features in accordance with feature locations. Since our method embeds relative positions between object features into image descriptors, our descriptors are more discriminative. • We conduct extensive experiments which confirm that our proposed method significantly outperforms the state-of-the-art on two standard cross-view benchmark datasets. Our method achieves nearly 4-fold improvement in terms of top-1 recall, compared with the CVM-Net proposed in 2018 <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Due to the drastic appearance and viewpoint changes, it is very difficult to match local handcrafted features between ground and aerial images directly. Several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref> warp ground images into bird-view images and then match the warped images to the aerial ones. Jegou et al. <ref type="bibr" target="#b4">[5]</ref> aggregate the residuals of local features to cluster centroids as image representations, known as VLAD descriptors. The work <ref type="bibr" target="#b13">[14]</ref> aggregates a set of local features into a histogram, known as Bag of words, to attain a global descriptor. The aggregated descriptors are proved to be partially viewpoint and occlusion invariant, and thus facilitating image matching. However, hand-crafted features are still the performance bottleneck of traditional cross-view Geo-localization methods.</p><p>Deep neural networks have demonstrated their powerful image representation ability <ref type="bibr" target="#b11">[12]</ref>. The seminal work <ref type="bibr" target="#b15">[16]</ref> fine-tune AlexNet <ref type="bibr" target="#b6">[7]</ref> on Imagenet <ref type="bibr" target="#b11">[12]</ref> and Places <ref type="bibr" target="#b19">[20]</ref> to extract features for the cross-view matching task. This work also indicates that the better discriminativeness of deep features compared to hand-crafted features. The work <ref type="bibr" target="#b16">[17]</ref> fine-tunes CNNs by minimizing the feature distances between aerial and ground-view images and obtains better localization performance. <ref type="bibr" target="#b14">[15]</ref> employs a triplet CNN architecture to learn feature embedding and achieves significant improvements.</p><p>[4] embeds a NetVLAD layer on top of a VGG backbone network to represent the two-view images more discriminatively. <ref type="bibr" target="#b9">[10]</ref> observes that orientations play a critical role in learning discriminative features. Thus, this method incorporates per-pixel orientation information into a CNN to learn orientation-selective features for the cross-view localization task. However, those deep metric learning based methods do not take the geometric correspondences between ground and aerial images as well as spatial layout information of deep features into account. Besides, it might be difficult for networks to explore both geometric and feature correspondences simultaneously via a metric learning objective. Therefore, we propose to decouple the procedure of constructing geometric and feature correspondences, and let networks learn simple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first introduce the polar transform applied to aerial images for aligning these two cross-view domains, and then we present our spatial-aware position embedding module for descriptor extraction of both ground and aerial images. We employ a Siamese-like two-branch network architecture and our entire pipeline is illustrated in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Polar Transform</head><p>As we observed, pixels lying on the same azimuth direction in an aerial image approximately correspond to a vertical image column in the ground view image. Instead of enforcing neural networks to learn this mapping implicitly, we explicitly transform the aerial images and then roughly eliminate the geometric correspondence gap between these two domains. In doing so, we ease the task of learning multiple correspondences (i.e., geometry and feature representations) and only need to learn a simple feature mapping task, thus significantly facilitating network convergence.</p><p>We apply polar transform to aerial images in order to build more apparent spatial correspondences between aerial and ground images. Specifically, we take the center of each aerial image as the polar origin and the north direction (as it is often available for a satellite image) as angle 0 • in Figure <ref type="figure">2</ref>: Illustration of the pipeline of our proposed method.</p><p>the polar transform. Note that there is no ad hoc pre-centering process for the aerial images, and we do not assume that the location of a query ground-level image corresponds to the center of an aerial image during testing. In fact, small offsets on the polar origin do not affect the appearance of polar-transformed aerial images severely, and the small appearance changes will be reduced by our SPE modules (as illustrated in detail in Section 3.2). On the contrary, when a large offset occurs, the aerial image should be regarded as a negative sample and the polar-transformed aerial image will be significantly different from the ground-truth one. In this manner, the polar transform effectively increases the discriminativeness of our model.</p><p>To facilitate training of our two-branch network, we constrain the size of the transformed aerial images to be the same as the ground ones W g × H g . Note that, the size of the original aerial images is A a × A a . Therefore, the polar transform between the original aerial image points (x s i , y s i ) and the target transformed aerial image points (x t i , y t i ) is defined as:</p><formula xml:id="formula_0">x s i = A a 2 + A a 2 y t i H g sin( 2π W g x t i ) y s i = A a 2 - A a 2 y t i H g cos( 2π W g x t i )<label>(1)</label></formula><p>After polar transform, the objects in the transformed aerial images lie in similar positions to their counterparts in the ground images, as seen in Figure <ref type="figure" target="#fig_0">1</ref>(d). However, the appearance distortions are still obvious in the transformed images because polar transform does not take the depth of the scene content into account. Reducing these distortion artifacts for image descriptor extraction is also desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial-aware Feature Aggregation (SAFA)</head><p>As illustrated in Figure <ref type="figure">2</ref>, we first employ a backbone network, i.e., the first sixteen layers of VGG19 <ref type="bibr" target="#b12">[13]</ref>, to extract features from ground and polar-transformed aerial images. Considering the features from aerial images undergo distortions, we intend to impose an attention mechanism to select salient features while suppressing the features caused by the distortions. Moreover, since spatial layout provides important clues for image matching, we aim to embed spatial configuration into our feature representation as well. Thus, we develop a spatial-aware feature aggregation (SAFA) module to alleviate the distortions in transformed aerial images while embedding the object features into a discriminative global image descriptor for image matching. Our SAFA is built upon the outputs of a Siamese network and learns to encode ground and aerial features individually. The detailed architecture of SAFA is shown in Figure <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-aware Position Embedding Module (SPE):</head><p>Our SPE is designed to encode the relative positions among object features extracted by the CNN backbone network, as well as the important features. In particular, given input feature maps from one branch, our SPE automatically determines an embedding position map from them. Note that, we do not enforce any additional supervision for SPE and it is learned in a self-attention fashion by a metric learning objective. Moreover, although polar transform can significantly reduce the domain gap in terms of geometric configuration, object distortions still exist and cannot be removed by an explicit function. Thus, we employ SPE to select the features from transformed aerial images while reducing the impact of the distortion artifacts in the feature extraction.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> illustrates the workflow of our SPE module. Our SPE first employs a max-pooling operator along feature channels to choose the most distinct object feature, and then adopts a Spatial-aware Importance Generator to generate a position embedding map. In the Spatial-aware Importance Generator, two fully connected layers are used to select features among the prominent ones as well as encode the spatial combinations and feature responses. In this manner, our method can mitigate the impacts of the features from distortions caused by polar transform while represent input images by using salient features. Furthermore, since we choose features based on a certain layout, the encoded features not only represent the emergence of certain objects but also reflect the positions of the objects. Hence, we encode the spatial layout information into feature representation, thus improving the discriminativeness of our descriptors.</p><p>Given the position embedding map P ∈ R H×W , the feature descriptor K = {k c }, c = 1, 2, ..., C, is calculated as:</p><formula xml:id="formula_1">k c = f c , P F ,<label>(2)</label></formula><p>where f c ∈ R H×W represents the input feature map of the SPE module in the c-th channel, ., . denotes the Frobenius inner product of the two inputs, and k c is the embedded feature activation for the c-th channel.</p><p>As seen in Figure <ref type="figure" target="#fig_0">1</ref>, only a certain region achieves high responses in the visualized feature maps. This indicates that our SPE not only localizes the salient features but also encodes the layout information of those features. Note that the SPE module is adopted in both the ground and aerial branches, and our objective forces them to encode correspondent features between these two branches.</p><p>Multiple Position-embedded Feature Aggregation: Motivated by the feature aggregation strategy <ref type="bibr" target="#b7">[8]</ref>, we aim to improve the robustness of our feature representation by aggregating our embedded features. Towards this goal, we employ multiple SPE modules with the same architecture but different weights to generate multiple embedding maps, and then encode input features in accordance with the different generated masks. For instance, some maps focus on the layout of roads while some focus on trees. Therefore, we can explore different spatial layout information in the input images. As illustrated in Figure <ref type="figure">2</ref>, we concatenate the embedded features together as our final image descriptor. Note that, we do not impose any constraint on generating diverse embedding maps but learn embeddings through our metric learning objective. During training, in order to minimize the loss function, our descriptors should be more discriminative. Therefore, the loss function inherently forces our embedding maps to encode different spatial configurations to increase the discriminativeness of our embedded features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objective</head><p>We apply a metric learning objective to learn feature representations for both the ground and aerial image branches. The triplet loss is widely used to train deep networks for image localization and matching tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. The goal of the triplet loss is to make matching pairs closer while pushing unmatching pairs far apart. Similar to <ref type="bibr" target="#b3">[4]</ref>, we employ a weighted soft-margin triplet loss as our objective:</p><formula xml:id="formula_2">L = log(1 + e γ(dpos-dneg) ),<label>(3)</label></formula><p>where d pos and d neg are the 2 distance of matching and unmatching image pairs. γ is a parameter to adjust the gradient of the loss, thus controlling the convergence speed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Training and Testing Datasets: Our experiments are conducted on two standard benchmark datasets: CVUSA <ref type="bibr" target="#b18">[19]</ref> and CVACT <ref type="bibr" target="#b9">[10]</ref>, where ground images are panoramas. CVUSA and CVACT are both cross-view datasets, and each dataset contains 35, 532 ground-and-aerial image pairs for training. CVUSA provides 8, 884 image pairs for testing and CVACT provides the same number of pairs for validation (denoted as CVACT_val). Besides, CVACT also provides 92, 802 crossview image pairs with accurate Geo-tags to evaluate Geo-localization performance (denoted as CVACT_test). CVACT_test is a real geo-localization/retrieval test set where all aerial images within 5 meters to a query ground image are regarded as ground truth correspondences for this query image.</p><p>In other words, for a query ground image, there may exists several corresponding aerial images in the database. Note that in these two datasets the ground and aerial images are captured at different time. Figure <ref type="figure" target="#fig_3">4</ref> presents sampled image pairs from these two datasets.</p><p>Implementation Details: We use the VGG16 model with pretrained weights on Imagenet <ref type="bibr" target="#b2">[3]</ref> as our backbone to extract features from cross-view images, and the output of the last convolutional layer of VGG16 is fed into the proposed SAFA block <ref type="foot" target="#foot_0">1</ref> . The parameters in our proposed SPE module are randomly initialized. Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, we set γ to 10 for the triplet loss. Our network is trained with Adam optimizer <ref type="bibr" target="#b5">[6]</ref>, and the learning rate is set to 10 -5 . Exhaustive mini-batch strategy <ref type="bibr" target="#b14">[15]</ref> is utilized to create triplet images within a batch, and the batch size B s is set to 32. In a mini-batch, there is 1 matching/positive aerial image and B s -1 unmatching/negative aerial images for each ground-view image. Thus, we construct B s (B s -1) triplets in total. Similarly, for each aerial image, there is 1 matching ground-view image and B s -1 unmatching ground-view images, and thus B s (B s -1) triplets are also constructed. Hence, we have 2B s (B s -1) triplets in total within each batch.</p><p>Evaluation Metric: Similar to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>, we use the recall accuracy at top K as our evaluation metric to exam the performance of our model and compare with the state-of-the-art methods. Specifically, given a ground-level query image, it is regarded as "successfully localized" if its ground-truth aerial image is within the nearest top K retrieved images. The percentage of query images which have been correctly localized is reported as r@K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with State-of-the-Art Methods</head><p>We compare our method with two recent state-of-the-art cross-view localization methods: CVM-NET <ref type="bibr" target="#b3">[4]</ref> and Liu et al.'s method <ref type="bibr" target="#b9">[10]</ref>. For fair comparisons, we use the released models or codes provided by the authors. In our method, we apply polar transform to the aerial images and our SAFA outputs 8 spatial-aware embedding maps and then aggregate these embedded features, denoted as Polar_SAFA (M = 8). Note that, the dimension of our descriptors is as the same as that used in CVM-NET. We report recalls at top-1, top-5, top-10, up to top 1%, and the results are listed in Table <ref type="table">1</ref>.</p><p>As indicated by Table <ref type="table">1</ref>, our method significantly outperforms all the state-of-the-art methods. In particular, we almost double the recall at top-1 compared to Liu et al.'s method. The complete recall@K performance is shown in Figure <ref type="figure">5</ref>.</p><p>Table <ref type="table">1</ref>: Comparison with state-of-the-art methods on CVUSA <ref type="bibr" target="#b18">[19]</ref> and CVACT_val dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>CVUSA CVACT_val r@1 r@5 r@10 r@1% r@1 r@5 r@10 r@1% CVM-NET <ref type="bibr" target="#b3">[4]</ref> 22  Figure <ref type="figure">5</ref>: Recall rates on cross-view Geo-localization datasets. This figure demonstrates that our method (i.e., Polar_SAFA(M = 8)) significantly outperforms the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Accurate Geo-localization</head><p>We conduct experiments on the large-scale CVACT_test dataset <ref type="bibr" target="#b9">[10]</ref> to illustrate the effectiveness of our method for accurate city-scale Geo-localization applications. We also compare with the state-of-the-art methods, CVM-NET <ref type="bibr" target="#b3">[4]</ref> and Liu et al.'s method <ref type="bibr" target="#b9">[10]</ref>. The recall performance at top-K is shown in Figure <ref type="figure">5</ref>(c). Our method significantly outperforms the second-best method <ref type="bibr" target="#b9">[10]</ref>, with a relative improvement of 35.6% at top-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization of Learned Spatial Correspondences</head><p>To visualize our generated embedding maps, we employ the method of <ref type="bibr" target="#b17">[18]</ref> to back-propagate the embedding maps to the input ground image as well as the polar-transformed aerial image. As visible in Figure <ref type="figure" target="#fig_5">6</ref>, our SPE is able to encode similar spatial layout as well as feature correspondences between ground and polar-transformed aerial images. Furthermore, different SPE modules can generate different spatial embedding maps. In this way, we can encode multiple spatial layouts into our feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this part, we demonstrate the effectiveness of our proposed polar transform and Spatial-aware Position Embedding (SPE) modules. For the baseline network, we remove the polar transform from our network and replace the SPE module with a global max-pooling operator, which has been widely adopted in image retrieval tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1]</ref>. In this case, spatial correspondences between ground and aerial branches are not used and the baseline network is only trained by our triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Polar Transform:</head><p>To demonstrate the effectiveness of polar transform for the crossview Geo-localization problem, we train our baseline network in two different settings: one takes original cross-view ground and aerial images, marked as VGG_gp, and the other takes ground and polar-transformed aerial images, marked as Polar_VGG_gp. As indicated in Table <ref type="table" target="#tab_1">2</ref>, applying polar transform to aerial images improves the performance greatly on both datasets.</p><p>Moreover, we also investigate the applicability of polar transform to other cross-view Geo-localization models. Liu et al. <ref type="bibr" target="#b9">[10]</ref> needs an additional pixel-wise orientation map for input images and the orientation maps are not available for polar transformed images. Thus, we only conduct experiments on CVM-NET <ref type="bibr" target="#b3">[4]</ref>. As illustrated in Table <ref type="table" target="#tab_1">2</ref>, using the polar-transformed aerial images as input, we even improve the performance of CVM-NET by 27.47% on CVUSA and 14.77% on CVACT at r@1.  CVUSA CVACT_val r@1 r@5 r@10 r@1% r@1 r@5 r@10 r@1% Effects of Spatial-aware Position Embedding: We demonstrate the effectiveness of our proposed Spatial-aware Position Embedding (SPE) module using original cross-view images as inputs. We firstly replace the global max-pooling in VGG_gp with a single SPE module. Since our SPE module explicitly establishes spatial correspondences for cross-view images, it outperforms VGG_gp as indicated in Table <ref type="table" target="#tab_3">3</ref>. Especially, our single SPE model achieves 58.79% on CVUSA and 42.96% on CVACT_val for r@1, and obtains 48% and 33% relative improvements compared with VGG_gp, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Multiple Spatial-aware Position Embeddings:</head><p>To demonstrate the effectiveness of aggregating feature embeddings by using multiple SPE modules, we use different numbers of SPE modules, i.e., 1, 2, 4, and 8, and report the recall rates in Table <ref type="table" target="#tab_3">3</ref>. The results indicates that as M increases, we can obtain better recall performance. Note that, significant improvements ( 10%) for r@1 are obtained when M increases from 1 to 2 and from 2 to 4. However, when M increases from 4 to 8, we only attain slight improvements (&lt;4%). Therefore, we do not increase M to an even larger number. As indicated by Table <ref type="table" target="#tab_3">3</ref>, our method, combining polar transform and multiple SPE modules, achieves the best performance on both datasets. By employing polar transform, we improve the performance over 7%, thus demonstrating the effectiveness of polar transform as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a new deep network to solve the cross-view image based Geo-localization problem. Our network addresses the difficulty caused by significant domain differences between ground-level and aerial-view images by a two-step procedure. The first step approximately brings the two image domains into a rough geometric alignment, and a subsequent spatial-attention mechanism further alleviates content-dependent domain discrepancy. Our key idea is to exploit available problemdependent geometric priors of the task. In contrast to existing methods, we exploit the geometric constraint to coarsely align one domain to the other first. By doing so, we can force our network to focus on learning discriminative features without requiring to minimize the domain gap. Moreover, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVUSA</head><p>CVACT_val r@1 r@5 r@10 r@1% r@1 r@5 r@10 r@1% VGG_gp  we propose a spatial-aware feature aggregation module to not only embed features but also the feature layout information, achieving more discriminative image descriptors. Since the cross-view feature learning process has been decoupled, the domain gap does not affect feature learning. Our method is able to learn more discriminative image descriptors and thus outperforms the state-of-the-art.</p><p>Although our current experiments are conducted on query ground images which are panoramas with known orientation, this restriction can be relaxed under the same network architecture and this is left as our future extension.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of geometric correspondences between ground and aerial images, and visualization of our generated spatial embedding maps.</figDesc><graphic coords="2,111.90,147.62,51.48,51.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Spatial-aware position embedding module.</figDesc><graphic coords="5,146.35,72.00,316.79,57.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ground-to-aerial image pairs sampled from CVUSA<ref type="bibr" target="#b18">[19]</ref> and CVACT<ref type="bibr" target="#b9">[10]</ref>. Each subfigure illustrates a ground image (Left) and an aerial image (right).</figDesc><graphic coords="6,116.26,76.98,118.79,59.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of eight-groups generated spatial embedding maps for ground and polartransformed images. The corresponding ground and polar-transformed aerial images are shown in Figure 1(b) and Figure 1(d). (Best viewed on screen with zoom-in)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of recalls on CVUSA<ref type="bibr" target="#b3">[4]</ref> and CVACT_val<ref type="bibr" target="#b9">[10]</ref> datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.53 50.01 63.19 93.52 20.15 45.00 56.87 87.57 Liu et al.[10] 40.79 66.82 76.36 96.08 46.96 68.28 75.48 92.01 Our polar-SAFA(M=8) 89.84 96.93 98.14 99.64 81.03 92.80 94.84 98.17</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness demonstration of polar transform.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>VGG_gp 39.72 66.91 77.49 96.38 32.22 59.08 69.41 91.85 Polar_VGG_gp 65.74 84.76 89.91 98.30 56.65 79.20 84.98 95.76 CVM-NET [4] 22.53 50.01 63.19 93.52 20.15 45.00 56.87 87.57 Polar_CVM-NET 50.00 77.22 85.13 97.82 34.92 61.74 71.05 91.78</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness demonstration of the proposed SPE modules.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>39.72 66.91 77.49 96.38 32.22 59.08 69.41 91.85 SAFA (M = 1) 58.79 84.19 90.84 99.08 42.96 71.51 80.56 95.48 SAFA (M = 2) 69.33 89.01 93.52 99.31 58.98 82.86 88.46 97.13 SAFA (M = 4) 79.93 93.29 96.15 99.54 74.61 90.02 93.03 98.01 SAFA (M = 8) 81.15 94.23 96.85 99.49 78.28 91.60 93.79 98.15 Polar_SAFA (M = 8) 89.84 96.93 98.14 99.64 81.03 92.80 94.84 98.17</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code of this paper is available at https://github.com/shiyujiao/SAFA.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported in part by China Scholarship Council (201708320417), the Australia Research Council ARC Centre of Excellence for Robotics Vision (CE140100016), ARC-Discovery (DP 190102261) and ARC-LIEF (190100080), and in part by a research gift from Baidu RAL (ApolloScapes-Robotics and Autonomous Driving Lab). The authors gratefully acknowledge the GPU gift donated by NVIDIA Corporation. We thank all anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic cross-view matching</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cvm-net: Cross-view matching network for image-based ground-to-aerial geo-localization</title>
		<author>
			<persName><forename type="first">Sixing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gim</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Hee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lending orientation to neural networks for cross-view geolocalization</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantic image based geolocation given a map</title>
		<author>
			<persName><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00278</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video google: Efficient visual search of videos</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward category-level object recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the location dependence of convolutional neural network features</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3961" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting groundlevel scene layout from aerial imagery</title>
		<author>
			<persName><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
