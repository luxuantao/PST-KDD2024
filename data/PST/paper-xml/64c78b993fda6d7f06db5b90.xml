<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<email>yiming@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3539618.3592052</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>knowledge graph completion</term>
					<term>neural network</term>
					<term>information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of knowledge graph completion (KGC) is of great importance. To achieve scalability when dealing with large-scale knowledge graphs, recent works formulate KGC as a sequence-tosequence process, where the incomplete triplet (input) and the missing entity (output) are both verbalized as text sequences. However, inference with these methods relies solely on the model parameters for implicit reasoning and neglects the use of KG itself, which limits the performance since the model lacks the capacity to memorize a vast number of triplets. To tackle this issue, we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC model, which selects semantically relevant triplets from the KG and uses them as evidence to guide output generation with explicit reasoning. Our method has demonstrated state-of-the-art performance on benchmark datasets Wikidata5M and WikiKG90Mv2, which contain about 5M and 90M entities, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Knowledge representation and reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given an incomplete triplet with head entity ID and relation ID, we first verbalize it to a text sequence using their name labels (Q626490 ? Viva La Vida, P175 ? Performer), then a retriever is used to retrieve relevant information from the KG, followed by the application of a seq2seq model to generate the name label of the missing entity, which is mapped back into an entity ID (Coldplay ? Q45188).</p><p>Most studies in KGC have been focusing on relatively small knowledge graphs. For example, the two most common benchmarks namely FB15k-237 <ref type="bibr" target="#b19">[20]</ref> and WN18RR <ref type="bibr" target="#b6">[7]</ref> contain only 14k and 40k entities, respectively. On the other hand, large KGs in the real world such as Wikidata <ref type="bibr" target="#b22">[23]</ref> and Freebase <ref type="bibr" target="#b0">[1]</ref> contain millions of entities, which present a scalability challenge to many methods. Traditional KGC methods such as TransE <ref type="bibr" target="#b1">[2]</ref> and DistMult <ref type="bibr" target="#b26">[27]</ref> have the learnable embedding parameters for each entity, which means that the number of parameters increases proportionally to the number of entities. Some methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> tried to avoid this by employing pre-trained language models (PLM) to embed entities based on their names and/or descriptions. Those methods, however, still suffer from scalability issues at inference due to the need to traverse all the entities for prediction. This means that the inference computation of those methods is linear in the total number of entities.</p><p>Recent studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref> utilize a sequence-to-sequence (seq2seq) generation methodology for knowledge graph completion, by verbalizing each incomplete triplet as the input text sequence and then applying Transformer <ref type="bibr" target="#b21">[22]</ref>-based models to predict each missing entity as the output sequence. Those methods are advantageous as the model parameters and inference computation are independent of the size of the knowledge graph. And furthermore, the training of such models does not require negative sampling.</p><p>Despite the advantages of existing seq2seq KGC methods, they have one major weakness: That is, their inference only involves implicit reasoning with trained model parameters, neglecting the direct use of the KG itself. In other words, due to the occurrence of catastrophic forgetting during the training process, the models lack the capacity of memorizing all the triplets in large-scale KGs and directly utilizing the triplets for inference. This paper addresses such limitation by proposing a Retrieval-enhanced Seq2seq KG Completion model, namely ReSKGC. It firstly converts the triplets in the entire KG into text passages, secondly uses a free-text retrieval module (such as BM25 <ref type="bibr" target="#b16">[17]</ref>) to find the relevant triplets for each incomplete triplet (the query), and thirdly uses the retrieved triplets to enhance the generation of the missing entity. Figure <ref type="figure" target="#fig_0">1</ref> illustrates such a process, with the example of predicting the performer for the song of Viva La Vida. Given the query, the retrieved triplets such as (Fix You, followed by, Viva La Vida) and (Fix You, Performer, Coldplay), can be used to enhance the likelihood of generating the correct answer.</p><p>Our comparative evaluation shows that the proposed approach can significantly enhance the state-of-the-art performance on two large-scale datasets, Wikidata5M <ref type="bibr" target="#b24">[25]</ref> and WikiKG90Mv2 <ref type="bibr" target="#b7">[8]</ref>, which contain 5M and 90M entities, respectively. Ablation tests were also conducted to analyze the effects of different settings in the retrieval module and for relations with different frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Knowledge Graph Completion (KGC) In a Knowledge Graph KG = (E, R, T ), we denote the set of entities, relations, and triplets as E, R, and T , respectively. A triplet is expressed in the domain E ? R ? E, usually written as (? ? , ?, ? ? ) containing the head entity ? ? ? E, relation ? ? R and tail entity ? ? ? E respectively. The completion task in a KG usually refers to, given an incomplete triplet (? ? , ?, ?) or (?, ?, ? ? ) where its head or tail entity is missing, the model is required to predict the missing entity. Sequence-to-Sequence KGC Given an incomplete triplet, it was first verbalized into a text query ?:</p><formula xml:id="formula_0">? = concat(prefix ? , ?(? ? ), ?(? )) if ? ? is missing concat(prefix ? , ?(? ), ?(? ? )) if ? ? is missing (1)</formula><p>where ?(?) maps the entity or relation into its name label. prefix ? (or prefix ? ) refers to a sequence of tokens that are added to the beginning of a query in order to inform the model which side, whether it be the head or tail entity, is the target for prediction. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, (Q626490, P175, ?) is transformed to Predict tail: Viva La Vida | Performer. Similarly, (?, P175, Q45188) will be transformed to Predict head: Performer | Coldplay. Then the text query will be passed into an encoder-decoder language model such as T5 <ref type="bibr" target="#b15">[16]</ref> to generate the output tokens:</p><formula xml:id="formula_1">? output = Decoder(Encoder(?))<label>(2)</label></formula><p>The aim is to produce the name label of the missing entity. The cross-entropy loss of token prediction is utilized during the training of the generative model. During the process of making predictions, the generated name label will be mapped to an entity ID ?? = ? -1 (? output ). To generate multiple prediction candidates, beam search can be employed, but invalid entity names will be discarded. Constrained decoding methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> can be utilized to ensure the validity of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Instead of directly passing the verbalized incomplete triplet into a seq2seq model for entity generation, we propose ReSKGC, which retrieves relevant information from the knowledge graph to guide the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KG to Text Passages</head><p>To facilitate retrieval based on text semantics of entities and relations, we convert the entire knowledge base into text passages through a process of linearization. To prevent data leakage, we only transform the triplets in the training data. We begin by transforming each triplet into a sentence, accomplished by concatenating the name labels of entities and relations and incorporating a special symbol | to delimit the elements. For example, the triplet (Q1991309, P175, Q45188) will be transformed to the sentence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval</head><p>After converting the knowledge graph into text passages, we proceed to conduct retrieval given an incomplete triplet. To achieve this, we first verbalize the triplet using Equation <ref type="formula">1</ref>. We then employ the widely-used retrieval method BM25 <ref type="bibr" target="#b16">[17]</ref>, which is based on TF-IDF scores of sparse word matches between input queries and passages. We have opted for sparse retrieval as opposed to dense retrieval <ref type="bibr" target="#b9">[10]</ref>, which involves computing passage embeddings and query embeddings using pre-trained language models. The reason for this is that we place value on the generalization and efficiency of sparse representation. Dense retrieval methods typically require additional model training and consume significant amounts of memory in order to save the passage embeddings. Through retrieval, we are able to obtain ? passages [? ? ] ?=1,??? ,? that are potentially relevant to the input question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generation</head><p>Ultimately, we employ a generation module that takes the query and retrieved passage as inputs and produces the desired output entity. One approach is to concatenate all the passages and the query as a single input sequence for the model. However, this can become inefficient when a large number of passages are retrieved due to the quadratic computational complexity in the self-attention mechanism of the Transformer model. To achieve both cross-passage modeling and computation efficiency, we apply Fushion-in-Decoder (FiD) <ref type="bibr" target="#b8">[9]</ref> based on T5 <ref type="bibr" target="#b15">[16]</ref> as the generation module. FiD separately encodes the concatenation of the query and each passage but decodes the output tokens jointly. Specifically, the encoding process for each passage ? ? is as follows:</p><formula xml:id="formula_2">P ? = Encoder(concat(?, ? ? )).<label>(3)</label></formula><p>Next, the token embeddings of all passages outputted from the encoder are concatenated before being sent to the decoder to produce the output tokens ? output :</p><formula xml:id="formula_3">? output = Decoder( [P 1 ; P 2 ; ? ? ? ; P ? ]).<label>(4)</label></formula><p>In this manner, the decoder can generate outputs based on joint modeling of multiple passages. Similar to vanilla seq2seq KGC, we utilize beam search to generate multiple candidates and constrained decoding as <ref type="bibr" target="#b3">[4]</ref> to ensure validity. After generation, we simply map the name back to the corresponding entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Process</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Basic Setting</head><p>We conduct experiments on the two large-scale KGC datasets: Wiki-data5M <ref type="bibr" target="#b24">[25]</ref>, and WikiKG90Mv2 <ref type="bibr" target="#b7">[8]</ref>, where the dataset statistics are shown in Table <ref type="table" target="#tab_2">1</ref>. We follow the conventional train/valid/test split setting according to the original paper of each dataset. For the training process, we use Adam <ref type="bibr" target="#b11">[12]</ref> as the optimizer and set the learning rate as 0.0001. The number of training steps is 30,000 for all the datasets with the same batch size of 16. For Wikidata5M, we retrieve 10 passages per query and sampled 100k queries for training. For WikiKG90Mv2, we retrieve 20 passages and sampled 200k queries. During inference, constrained decoding is utilized for the Wikidata5M dataset, while it is not used for WikiKG90Mv2 due to the dataset's extensive number of entities, making the construction of a prefix tree excessively memory and time-consuming. For the generation module, we use T5-small and T5-base <ref type="bibr" target="#b15">[16]</ref> with numbers of parameters 60M and 220M, respectively. All experiments are carried out on NVIDIA 2080-Ti GPUs.</p><p>For the evaluation metric, we use the filtered setting <ref type="bibr" target="#b1">[2]</ref> for computing mean reciprocal rank (MRR) and hits at 1, 3, and 10 (H@1, H@3, and H@10). Higher MRR and hits scores indicate better performance. For Wikidata5M, the metrics are computed for head entity and tail entity prediction separately and then averaged. For WikiKG90Mv2, we follow the original paper to only compute metrics for tail entity prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>As demonstrated in Table <ref type="table">2</ref>, ReSKGC attains state-of-the-art results on Wikidata5M, exhibiting a significant enhancement of MRR by 10.6% as compared to the most superior baseline technique. The first section of baselines comprises conventional KGC approaches, which demonstrate satisfactory performance but have a considerably larger number of parameters than the second section, comprising PLM-based techniques. It is noteworthy that SimKGC employs PLM to create embeddings for each entity. Despite having a relatively small number of parameters, it still necessitates substantial memory usage to store all entity embeddings, which is avoided by our proposed methodology. It is also noteworthy that our method outperforms the baselines significantly on Hits@1, with an improvement of 19.1%, but it performs slightly worse than the best baseline SimKGC on Hits@10. We posit that the generation-based approach utilized by our model may produce less diverse predicted answers in contrast to the matching-based approach. Thus, having more predictions through the matching approach can result in better improvement of answer coverage. Table <ref type="table" target="#tab_3">3</ref> presents results on the extremely large-scale dataset WikiKG90Mv2<ref type="foot" target="#foot_0">1</ref> , which contains 90 million entities. Our proposed method surpasses the current state-of-the-art technique<ref type="foot" target="#foot_1">2</ref> by 13.2%, while utilizing only 1% of the parameters. Moreover, our method outperforms the seq2seq KGC baseline KGT5, even when both models have an equivalent number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we aim to answer the following essential questions for a more comprehensive understanding of our proposed method.</p><p>Q1. How does the number of retrieved passages affect the model performance?</p><p>First, we demonstrate the efficacy of retrieval through the variation in the number of passages retrieved, ranging from 0 to 20, where 0 implies the application of vanilla seq2seq KGC without retrieval. Figure <ref type="figure" target="#fig_1">2</ref> indicates that retrieval yields considerable advantages on both datasets. Notably, the retrieval of 10 passages can increase the MRR by 16.1% and 89.2% for the respective datasets when compared to non-retrieval. Furthermore, we note that enhancing the number of passages from 10 to 20 produces minimal performance enhancement, indicating that the information presented in later passages is overshadowed by noise. Therefore, an additional Table <ref type="table">2</ref>: KG completion results on Wikidata5M. The best result in each column is marked in bold. The second best is marked in * . ? results are from the best pre-trained models made available by Graphvite <ref type="bibr" target="#b32">[33]</ref>. ? ? results are from <ref type="bibr" target="#b12">[13]</ref>. Other baseline results are from the corresponding papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MRR H@1 H@3 H@10 #Params TransE <ref type="bibr">[</ref>  increase in the number of retrieved passages to the generation module will not lead to a significant improvement in performance. Q2. How does the sampling of training queries affect the final performance? As mentioned in Section 3.4, we randomly sample a subset of triplets as training queries. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the performance of the model with varying numbers of triplets used as training queries.</p><p>Our results indicate that for Wikidata5M, a mere 100K triplets are sufficient to achieve good performance, which represents less than 0.5% of the total number of triplets. For WikiKG90Mv2, 200K triplets are sufficient, representing only 0.03% of the total triplets. These findings suggest that a retrieval-enhanced model can effectively learn patterns from a considerably smaller amount of training data.</p><p>Q3. What's the effect of retrieval on relations with different frequencies?</p><p>In a knowledge graph, relations manifest in variable quantities of triplets. To classify relations by their frequency, we have grouped them into four categories based on the number of occurrences: less than 10 4 , 10 4 to 10 5 , 10 5 to 10 6 , and more than 10 6 . Figure <ref type="figure" target="#fig_2">3</ref> illustrates that the incorporation of retrieval yields more pronounced gains for relations that occur less frequently. An illustrative example can be observed in the relations that occur less than 10 4 times. In this scenario, the inclusion of retrieval results in a 59% MRR increment. In contrast, for relations that appear in more than 10 6 triplets, the MRR increment is merely 2% when utilizing retrieval. This finding is logical, as the generation module may lack adequate training data to comprehend relations that are less commonly observed. Therefore, retrieval-enhanced explicit reasoning can facilitate performance improvement in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper proposes a Retrieval-enhanced Seq2seq KG Completion model, namely ReSKGC, which converts the triplets in the entire KG into text passages and uses a free-text retrieval module to find the relevant triplets for each incomplete triplet. The retrieved triplets are then used to enhance the generation of the missing entity. Comparative evaluations on two large-scale datasets, Wikidata5M and WikiKG90Mv2, demonstrate that the proposed approach significantly outperforms the state-of-the-art models by 10.6% and 13.2% in terms of mean reciprocal rank. Additionally, we conducted ablation studies to explore the effects of the number of retrieved passages, training data, and relation frequencies. Our approach shows a promising direction for large-scale KG completion, where further improvements can be achieved by research on improving the retrieval modules and generation modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FixFigure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed framework ReSKGC.Given an incomplete triplet with head entity ID and relation ID, we first verbalize it to a text sequence using their name labels (Q626490 ? Viva La Vida, P175 ? Performer), then a retriever is used to retrieve relevant information from the KG, followed by the application of a seq2seq model to generate the name label of the missing entity, which is mapped back into an entity ID (Coldplay ? Q45188).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance of ReSKGC (base) over the validation sets based on different numbers of retrieval passages and sampled training queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The performance of ReSKGC (base) on the Wiki-data5M test set triplets, categorized according to their relation frequencies. ? represents the number of test triplets within that particular range of relation frequencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>training of ReSKGC only involves optimizing the generation module, since the retrieval component (BM25) is unsupervised and does not require any model training. Similar to vanilla seq2seq KGC methods, we employ the cross-entropy loss of token prediction for training. However, there are two crucial considerations: Removing query triplet from retrieved passages: It is important to note that all of the training triplets are included within the linearized passages as mentioned in Section 3.1. Therefore, throughout the training process, it is necessary to ensure that the retrieved passages do not include the query triplet itself, as failure to do so would render the training task insignificant. As a result, we simply remove the linearized triplet from the retrieved passages if it is present in them.</figDesc><table /><note><p>Sampling query triplets: Large-scale KGs may contain hundreds of millions of triplets, making it excessively costly to enumerate them all during training. In this study, we address this issue by randomly sampling a subset of triplets to generate training queries to optimize the model. We demonstrate in Section 4.3 that such sampling does not impair performance while keeping training highly efficient. It should be noted that we still utilize all the training triplets to construct the passages as mentioned in Section 3.1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Entities #Relations #Triplets</cell></row><row><cell>Wikidata5M</cell><cell>4.8M</cell><cell>828</cell><cell>21M</cell></row><row><cell>WikiKG90Mv2</cell><cell>91M</cell><cell>1,387</cell><cell>601M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>KG completion results on WikiKG90Mv2 (validation set). The best result is marked in bold. The second best is marked in * . All the baseline results are taken from the official leaderboard of<ref type="bibr" target="#b7">[8]</ref> except that ? results are from<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table><row><cell>2]  ?</cell><cell>0.253</cell><cell cols="2">0.170</cell><cell>0.311</cell><cell>0.392</cell><cell>2.4B</cell></row><row><cell>DistMult [27]  ?</cell><cell>0.253</cell><cell cols="2">0.209</cell><cell>0.278</cell><cell>0.334</cell><cell>2.4B</cell></row><row><cell>SimplE [11]  ?</cell><cell>0.296</cell><cell cols="2">0.252</cell><cell>0.317</cell><cell>0.377</cell><cell>2.4B</cell></row><row><cell>RotatE [19]  ?</cell><cell>0.290</cell><cell cols="2">0.234</cell><cell>0.322</cell><cell>0.390</cell><cell>2.4B</cell></row><row><cell>QuatE [32]  ?</cell><cell>0.276</cell><cell cols="2">0.227</cell><cell>0.301</cell><cell>0.359</cell><cell>2.4B</cell></row><row><cell cols="2">ComplEx [21]  ? ? 0.308</cell><cell cols="2">0.255</cell><cell>-</cell><cell>0.398</cell><cell>614M</cell></row><row><cell>KEPLER [25]</cell><cell>0.210</cell><cell cols="2">0.173</cell><cell>0.224</cell><cell>0.277</cell><cell>125M</cell></row><row><cell>MLMLM [5]</cell><cell>0.223</cell><cell cols="2">0.201</cell><cell>0.232</cell><cell>0.264</cell><cell>355M</cell></row><row><cell>KGT5 [18]</cell><cell>0.300</cell><cell cols="2">0.267</cell><cell>0.318</cell><cell>0.365</cell><cell>60M</cell></row><row><cell>SimKGC [24]</cell><cell>0.358</cell><cell cols="2">0.313</cell><cell cols="2">0.376 0.441</cell><cell>220M</cell></row><row><cell cols="6">ReSKGC (small) 0.363  *  0.334  *  0.386  *  0.416</cell><cell>60M</cell></row><row><cell>ReSKGC (base)</cell><cell cols="5">0.396 0.373 0.413 0.437  *</cell><cell>220M</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MRR #Params</cell></row><row><cell cols="5">TransE-Shallow-PIE [3] 0.234  *</cell><cell>18.2B</cell></row><row><cell cols="2">TransE-Concat [8]</cell><cell></cell><cell></cell><cell>0.206</cell><cell>18.2B</cell></row><row><cell cols="3">ComplEx-Concat [8]</cell><cell></cell><cell>0.205</cell><cell>18.2B</cell></row><row><cell cols="3">ComplEx-MPNet [8]</cell><cell></cell><cell>0.126</cell><cell>307K</cell></row><row><cell cols="2">ComplEx [21]</cell><cell></cell><cell></cell><cell>0.115</cell><cell>18.2B</cell></row><row><cell cols="2">TransE-MPNet [8]</cell><cell></cell><cell></cell><cell>0.113</cell><cell>307K</cell></row><row><cell cols="2">TransE [2]</cell><cell></cell><cell></cell><cell>0.110</cell><cell>18.2B</cell></row><row><cell cols="2">KGT5 [18]  ?</cell><cell></cell><cell></cell><cell>0.221</cell><cell>60M</cell></row><row><cell cols="2">ReSKGC (small)</cell><cell></cell><cell></cell><cell>0.230</cell><cell>60M</cell></row><row><cell cols="2">ReSKGC (base)</cell><cell></cell><cell></cell><cell>0.265</cell><cell>220M</cell></row><row><cell>0.15 0.20 0.25 0.30 0.35 0.40 MRR</cell><cell cols="2">Wikidata5M WikiKG90Mv2</cell><cell cols="2">0.24 0.26 0.28 0.30 0.32 0.38 0.36 0.34 MRR</cell><cell cols="2">Wikidata5M WikiKG90Mv2</cell></row><row><cell cols="2">01 5 #Passages 10</cell><cell>20</cell><cell></cell><cell cols="3">2050 100 200 #Training Queries (k) 400</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We only show results obtained from the validation set since the test set is not publicly available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We conduct a fair comparison by solely considering a single model, without any ensemble methods.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/hash/1cecc" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States, Christo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
	<note>a77928ca8133fa24680a88d2f9-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">PIE: a parameter and inference efficient solution for large scale knowledge graph embedding reasoning</title>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2204.13957</idno>
		<ptr target="https://arxiv.org/abs/2204.13957" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Knowledge Is Flat: A Seq2Seq Generative Framework for Various Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Yan</forename><surname>Lam</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2209.07299</idno>
		<ptr target="https://arxiv.org/abs/2209.07299" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MLMLM: Link Prediction with Mean Likelihood Masked Language Model</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Clouatre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Trempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amal</forename><surname>Zouaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.378</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.findings-acl.378" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4321" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2010.00904</idno>
		<ptr target="https://arxiv.org/abs/2010.00904" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17366" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2103.09430</idno>
		<ptr target="https://arxiv.org/abs/2103.09430" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.eacl-main.74" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.550" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SimplE Embedding for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Montr?al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Canada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/b" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Nicol?</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
	<note>2ab001909a8a6f04b51920306046ce5-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel training of knowledge graph embedding models: a comparison of techniques</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="633" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1223</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2395" to="2405" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning visual models using a knowledge graph as a trainer</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Monka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lavdim</forename><surname>Halilaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achim</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web-ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-24">2021. October 24-28, 2021</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sequence-tosequence knowledge graph completion and question answering</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2203.10321</idno>
		<ptr target="https://arxiv.org/abs/2203.10321" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkgEQnRqYQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-4007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. 2016. June 19-24, 2016</date>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
	<note>JMLR.org, 2071-2080</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">SimKGC: Simple contrastive knowledge graph completion with pre-trained language models</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2203.02167</idno>
		<ptr target="https://arxiv.org/abs/2203.02167" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">KEPLER: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3209982</idno>
		<ptr target="https://doi.org/10.1145/3209978.3209982" />
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</editor>
		<meeting><address><addrLine>Ann Arbor, MI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-07-08">2018. July 08-12, 2018</date>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
	<note>SIGIR 2018</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6575" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph contrastive learning for recommendation</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1434" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno>ArXiv preprint abs/1909.03193</idno>
		<ptr target="https://arxiv.org/abs/1909.03193" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Hanbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno>ArXiv preprint abs/2210.00063</idno>
		<ptr target="https://arxiv.org/abs/2210.00063" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jaket: Joint pre-training of knowledge graph and language understanding</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11630" to="11638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quaternion Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019, NeurIPS 2019, December 8-14, 2019</date>
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
	<note>d961e9f236177d65d21100592edb0769-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GraphVite: A High-Performance CPU-GPU Hybrid System for Node Embedding</title>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313508</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313508" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<editor>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amin</forename><surname>Mantrach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leila</forename><surname>Zia</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. 2019. May 13-17, 2019</date>
			<biblScope unit="page" from="2494" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
