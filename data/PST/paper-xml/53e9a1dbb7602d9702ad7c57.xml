<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Shared-Weight Networks with Applications to Automatic Target Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yonggwan</forename><surname>Won</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri-Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columbia</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri-Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columbia</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Artificial Intelligence Section</orgName>
								<orgName type="department" key="dep2">Electronics and Telecommunications Research Institute</orgName>
								<address>
									<addrLine>Yoosung-Ku, Kajung-Dong 161</addrLine>
									<settlement>Daejon-City</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><forename type="middle">C</forename><surname>Coffield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri-Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columbia</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering and Computer Science</orgName>
								<orgName type="institution">University of Missouri-Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<settlement>Columbia</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphological Shared-Weight Networks with Applications to Automatic Target Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB02C95F6C9C6B428A68A67143BA2571</idno>
					<note type="submission">received September 17, 1995; revised April 19, 1996 and May 4, 1997.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Morphological operations</term>
					<term>neural networks</term>
					<term>object detection</term>
					<term>pattern recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A shared-weight neural network based on mathematical morphology is introduced. The feature extraction process is learned by interaction with the classification process. Feature extraction is performed using gray-scale hit-miss transforms that are independent of gray-level shifts. The morphological sharedweight neural network (MSNN) is applied to automatic target recognition (ATR). Two sets of images of outdoor scenes are considered. The first set consists of two subsets of infrared images of tracked vehicles. The goal in this set is to reject the background and to detect tracked vehicles. The second set consists of visible images of cars in a parking lot. The goal in this set is to detect the Chevrolet Blazers with various degrees of occlusion. A training method that is effective in reducing false alarms and a target aim point selection algorithm are introduced. The MSNN is compared to the standard shared-weight neural network. The MSNN trains relatively quickly and exhibits better generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D ESIGNING effective feature extraction algorithms is difficult <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. A heterogeneous neural network that learns feature extraction and classification simultaneously is attractive. Shared-weight neural networks (SSNN's) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> are heterogeneous networks that perform classification based on local features. They consist of two network stages: the first extracts features using linear convolution, the second performs classification.</p><p>We showed that the SSNN outperformed MACE filters <ref type="bibr" target="#b4">[5]</ref> for several target recognition applications in <ref type="bibr" target="#b5">[6]</ref>. We used a segmentation-free approach in which a neural network scanned an entire image producing a value at each point indicating confidence that a target is present. By contrast, the standard approach to target recognition is to first isolate regions of interest and then try to classify each region <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The segmentation-free approach has also been applied to target recognition with two-dimensional (2-D) feedforward networks <ref type="bibr" target="#b8">[9]</ref>, with feedforward networks using the MACE prefilter <ref type="bibr" target="#b9">[10]</ref>, to machine-printed and handwritten character recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, to texture discrimination <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and to blood vessel detection in angiogram images <ref type="bibr" target="#b12">[13]</ref>.</p><p>The operations of mathematical morphology are nonlinear, translation invariant transformations. Information obtained by morphological operations is highly dependent upon structuring elements, which are equivalent to masks in linear convolution. Several researchers have investigated the problem of systematic design of structuring elements for pattern recognition and image processing applications using statistical, genetic, and neural-network methods <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this paper, we describe a method for systematic design of structuring elements using a modification of the SSNN, called morphological shared weight networks (MSNN's), in which the feature extraction uses morphological instead of linear operations. In the next section, we describe the sharedweight neural-network model. In Section III, we describe a gray-level hit-miss transform used as the activation function for the feature extraction operations. We then provide the MSNN learning rules for the MSNN and present experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SHARED-WEIGHT NEURAL NETWORK</head><p>A SSNN is composed of two cascaded subnetworks called stages: a feature extraction stage followed by a classification stage. The feature extraction stage usually has a twodimensional array as input and local connections. The feature extraction layers perform linear convolution of their inputs with kernels defined by the local connections. The nodes of the last feature extraction layer are inputs to the classification stage, which is an ordinary fully connected feedforward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MORPHOLOGICAL OPERATIONS</head><p>Mathematical morphology involves translation-invariant, nonlinear convolution type operations based on fundamental operations of erosion and dilation. Erosion and dilation are operations that involve probing the image with sets, called structuring elements. Morphological processing is used to derive shape information from the image. This information is dependent on the structuring elements. The structuring elements are analogous to masks in linear filtering.</p><p>Morphological operations are well described in the literature <ref type="bibr" target="#b22">[23]</ref>. We provide a brief introduction. Let be a subset ofdimensional Euclidean space, and let be a real The gray-scale erosion of a function by a structuring element is defined by This operation measures the minimum difference between and for and is equivalent to measuring the maximum amount that the structuring element x can be translated vertically and still lie beneath It indirectly measures how well the shape represented by fits under but the output value is not invariant to shifts because if is a number, then</p><p>The gray-scale dilation of a function by a structuring element is defined by The gray-scale dilation is the dual of the erosion and indirectly measures how well the structuring element fits above in the same sense that erosion measures the fit below . An operation which measures both the fit above and below is the gray-scale hit-miss transform, which is defined as This definition is motivated by the umbra transform <ref type="bibr" target="#b23">[24]</ref>. The hit-miss transform measures how the shape fits under the function and how the shape fits above . Higher values indicate good fits. If then there exists a set of vertical shifts for which both shapes and closely fit the function . If the structuring elements match the shape of an image over a region I, the hit-miss transform will produce a peak where the shape occurs as described in the following.</p><p>Theorem: Let be a compact region and let be a piecewise continuous function. Let and The structuring elements and are defined to have exactly the same shape as Let be the set of all functions from to and define by Then, is maximal. Thus, the hit-miss transform constructed from will generate a maximal output value when is the input. This theorem is proved in <ref type="bibr" target="#b23">[24]</ref>.</p><p>The hit-miss transform is invariant to shifts in gray-scale: if , then,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MORPHOLOGICAL SHARED-WEIGHT NEURAL NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NodeOperations</head><p>The structure of the MSNN is the same as the SSNN. The nodes in the feature extraction stage perform the gray-scale hit-miss transform. Here we define the notation for the MSNN Output of the node x. Hit(Miss) structuring element weight associating node with node x.</p><p>The net input for the hit (erosion) and miss (dilation) operations are defined as hit net miss net Finally, net net There are no biases and no sigmoid functions at these nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Rule</head><p>We provide the learning rules for the feature extraction weights of an MSNN. The derivations, based on gradient descent, and implementation details are provided in <ref type="bibr" target="#b23">[24]</ref> These derivatives will not exist if there is a tie for the and the . If the inputs are real numbers, then the probability of a tie is zero. In our implementation, we arbitrarily break ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Set Description</head><p>Two data sets, the tracked (TV) and occluded vehicle, were used. Most data are described elsewhere <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. We describe them briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracked Vehicle Image Data</head><p>The TV data set has of two subsets. One contains 35, 256 256 8-b infrared images of a tracked vehicle in a natural background This subset contains horizontal revolutions of the tracked vehicle about every 10 at a fixed aspect angle. It is used to measure generalization to intermediate views. We evaluate this capability by training with views separated by 20 , the TV35 training set, and testing with different views separated by 20 , the TV35 test set. Results from many correlation filter approaches using these data are reported in <ref type="bibr" target="#b24">[25]</ref>. Results from a multilayer perceptron combined with a MACE preprocessor are also available <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The SSNN outperformed MACE filters on these data <ref type="bibr" target="#b5">[6]</ref>. The other subset, the TV test set, consists of 256 256 8-b infrared images with tracked vehicles in different backgrounds and at a variety of ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Occluded Vehicle Image Data</head><p>These data were collected with a camera mounted on a roof focused on a parking lot. A Chevrolet Blazer, called the training Blazer, was driven around the parking lot and recorded on video tape. The training set consists of 36, 256 256 images that contain the training Blazer. The training Blazer is not occluded in any training image. We used 21 images for testing. They include the training Blazer undergoing occlusion ranging from no occlusion to complete occlusion. They also contain other Blazers and similar vehicles. Detailed descriptions of all images are given in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Procedure</head><p>There is no standardized method for training a segmentation-free network. The network must reject an enormous number of backgrounds. We developed a method, called "dynamic random selection," which is effective at reducing false alarms based on the one described in <ref type="bibr" target="#b5">[6]</ref>.</p><p>The training begins with randomly selected background and target subimages. The size of the subimages is the maximum size target that should be detected. The pattern-sum-squared (PSS) error is measured at each epoch for each subimage. If it is low, a new subimage is randomly selected to replace the old one. For target subimages, the algorithm selects a subimage with center in a small window centered at the target center. The background subimages are selected outside of a window which contains the entire target. This method can be described as follows:</p><p>Read input image scenes along with the center positions of the targets in the scene; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Target Aimpoint Selection Algorithms</head><p>During training, the MSNN and SSNN take a (sub)image as input, and produce one output value per class. For testing, the network scans an entire input scene and generates an output image, the detection plane. The values of the detection plane are proportional to the outputs from the target class node. This scanning is efficient because of the local, translation invariant operations.</p><p>ATR systems require a point, called the target-aim-point (TAP), at which to aim. We measured the network performance by the location of the TAP. We devised a TAP selection algorithm that identifies one or more potential target locations using the detection plane as input. The TAP selection algorithm consists of thresholding followed by a modified ultimate erosion (TMUE). The detection image plane is thresholded and repeatedly eroded with a 3 3 structuring element. After a finite number of iterations, the eroded image will be empty. The eroded image that is the last nonempty image is defined to be the result of the modified ultimate erosion. This algorithm uses both the intensity and spatial size of a peak in the detection plane as an indicator of a target. It can detect multiple   targets if two peaks have high intensity and approximately the same spatial size. The output of the TMUE algorithm may consist of more than one pixel for a single target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tracked Vehicle Detection</head><p>We present results for a network with a single feature extraction layer with two feature maps and a single hidden layer with three hidden units for the classification stage. The input subimages were 50 80, and the sampling rate for the feature extraction layer was two. The size of the hit-miss structuring elements was 5 5. All weights were initialized with random numbers in [ 0.5, 0.5]. The learning rate was 0.05 and the momentum was 0.8. The other training parameters are shown in Table <ref type="table">I</ref>. With these training parameters, the network trained for 1000 epochs.</p><p>For comparison, we trained an SSNN with the same configuration. The SSNN has more parameters because the feature map nodes have biases whereas the MSNN nodes do not. We also trained a SSNN without biases but it performed poorly.</p><p>The SSNN took more training to achieve similar performance, so we used 4000 for MaxEpoch and "0.05 until 2000 epochs and 0.03 afterward" for StopErr. Thus, the SSNN training time was much longer than for the MSNN.</p><p>We first investigated threshold values for the TMUE. A wide range of thresholds, from 50 to 250, produced 100% detection with no false alarms on the TV35 training set. We chose a value of 230. In Fig. <ref type="figure" target="#fig_1">1</ref>, we show the some TV35 testing images with TAP's obtained using the TMUE. The TAP's were dilated with a 3 3 structuring element for better visualization.</p><p>The MSNN produced single TAP regions and all the TAP's were on the vehicles. This performance could not be obtained using the SSNN. Some typical testing examples obtained using the SSNN are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. The SSNN produced multiple TAP regions and TAP region or regions on the edge of the target. Both methods achieved 100% detection with no false alarms.</p><p>We demonstrated the background rejection capability of the MSNN using the TV test set with a MSNN that was trained  To demonstrate invariance to gray-level shifts, we applied the SSNN and MSNN to an original and a gray-level shifted image. The results are in Fig. <ref type="figure" target="#fig_4">4</ref>. The first column contains the original and shifted images; the second and third contain the SSNN and MSNN detection planes, respectively. The SSNN   cannot detect the vehicle in the shifted image but the MSNN is unaffected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Occluded Vehicle Detection</head><p>We present results of a comparison between an MSNN and an SSNN and then results showing the effects of varying some of the parameters of the MSNN architecture.</p><p>We trained an MSNN with the same configuration used for tracked vehicle detection. The training parameters were the same except that we used 36 training images and 1500 for MaxEpoch. A threshold of 180 was used for the TMUE algorithm. We trained an SSNN that had the same configuration as the MSNN for 3234 epochs to reach the same stopping error.</p><p>It took many more epochs for the SSNN to reach the same level of error than the MSNN. It actually takes much longer to scan with the SSNN because the biases make the computation translation variant. We have not optimized the computation so we do not report a time here.</p><p>The comparison results on the 21 testing images are shown in Table <ref type="table">II</ref>. We record the number of training Blazers detected (T), the number of false alarms (F), and the number of nontraining Blazers detected (N). The training Blazer is in a different position in each test image. There are two nontraining Blazers. They do not move so we only count whether each was detected so the maximum number of nontraining Blazers that can be detected is two.</p><p>We also broke the results out by occlusion level. We categorized the test images by level of occlusion: None (N), Very Small (VS), Small (S), Medium (M), and High (H). The numbers of images in each category are 4, 6, 6, 3, and 2 respectively. The results are shown in Table <ref type="table">III</ref>.</p><p>Typical outputs from the MSNN are shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The aimpoints are superimposed on the images in white. In Fig. <ref type="figure" target="#fig_5">5</ref>(a), the training Blazer is detected backing into a   by the boundary of the image. The scanning window must fit inside the image for an aimpoint to be found. In Fig. <ref type="figure" target="#fig_5">5</ref>(b), the nontraining Blazer is close enough to the boundary that the TAP is smaller. In Fig. <ref type="figure" target="#fig_5">5(c</ref>), the dark vehicle just above and to the right of the training Blazer is a nontraining Blazer but is not detected. There is also a Jeep in the image just above the nontraining Blazer which looks very similar to a Blazer but which is not detected.</p><p>Fig. <ref type="figure" target="#fig_7">6</ref> shows an example of a false alarm. Fig. <ref type="figure" target="#fig_7">6</ref>(a) shows the input image with TAP's. Fig. <ref type="figure" target="#fig_7">6</ref>(b) shows a threedimensional graph of the output detection plane before TAP selection. There is a significant output on the training Blazer but the output on the false alarm was slightly stronger. This indicates that a more sophisticated TAP selection algorithm may be useful.</p><p>We also ran experiments in which we varied the sizes of the structuring elements and the undersampling rate. The results are shown in Tables IV and V (In Tables IV-VI, we count the total number of nontraining Blazers detected). The networks are not very sensitive to structuring element size, except with respect to detecting nontraining Blazers. It is more sensitive to the undersampling rate. Reducing the undersampling rate speeds up computation.</p><p>We also performed an experiment to evaluate the sensitivity of the network to size variation. We tested the networks on several variations of the test set. Each variation consisted of resizing the images in the test set. Specifically, for each value , we created a new test set Let and denote the height and width of the images in the standard test set. The images in are obtained by resampling the images in the standard test set to have height and width . Thus, the test sets represent a variation in target sizes ranging from half the original size to 20% larger than the original size. There is also significant variation in the sizes of the Blazers in the original test set because the test set includes images of the Blazers at different ranges. The results of this experiment are shown in Table <ref type="table" target="#tab_6">VI</ref>.</p><p>To appreciate the range of sizes under consideration, Fig. <ref type="figure">7</ref> shows the image in Fig. <ref type="figure" target="#fig_5">5</ref>(a) as it looks in and . The network is capable of detecting Blazers in a range of sizes but is also adversely affected by size. The number of false alarms generally increases as the size goes up or down. The number of detections generally remains constant or increases as the size goes up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>The MSNN trained relatively quickly and exhibited better generalization than the SSNN. Both performed well at detecting occluded targets even though they were trained on nonoccluded targets. The MSNN is invariant to shifts in grayscale. All experiments with the shared-weight neural networks were conducted with raw input image scenes. We believe that one may obtain different results using preprocessing such as edge-maps. However, this requires judgment and effort to devise and evaluate preprocessing methods. Further work could include additional analysis of sensitivity of the network to variations in range and improving the TAP selection algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>in the top feature extraction layer and net net for the nodes in other feature extraction layers. The functions net and net are only piecewise differentiable. When the derivatives exist, they are given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some testing tracked vehicle images with MSNN TAP's superimposed.</figDesc><graphic coords="4,103.74,137.93,393.00,198.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Some testing tracked vehicle images with SSNN TAP's superimposed.</figDesc><graphic coords="4,104.88,375.56,391.00,100.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The MSNN has good background rejection and size tolerance capabilities. (a) Tracked vehicles in a variety of backgrounds. (b) Closing sequence.</figDesc><graphic coords="5,112.86,171.51,375.00,187.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The MSNN is invariant to gray-level shift whereas the SSNN is not.</figDesc><graphic coords="5,163.92,414.63,273.00,181.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sample outputs from the MSNN. The aimpoints are shown as bright white pixels.</figDesc><graphic coords="6,73.44,314.89,454.00,152.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>The training time per epoch is similar for both networks. We used a DEC Alpha3000 with 96 Mbytes of RAM running at 133 Mhz for our experiments. The training time averages about 2.5 s per epoch. It takes approximately 5 s to scan a 256 256 image with the MSNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of a false alarm.</figDesc><graphic coords="7,82.74,152.48,435.00,212.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I TRAINING PARAMETERS</head><label>IPARAMETERS</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II TESTING RESULTS TABLE III NUMBER</head><label>IIRESULTSIII</label><figDesc>OF TRAINING BLAZERS DETECTED BY OCCLUSION LEVEL</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF VARYING STRUCTURING ELEMENT SIZES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF VARYING UNDERSAMPLING RATE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>OF VARYING SIZES OF INPUT IMAGES</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank H.-R. Ryoo for his assistance in running many of the experiments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by the Armament Directorate, Wright Laboratories, Eglin Air Force Base and by the University of Missouri Research Board. The experiments with the tracked vehicles were not supported by Eglin. Y. Won was with the</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning texture discrimination masks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Neural Networks</title>
		<meeting>IEEE Conf. Neural Networks<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
			<biblScope unit="page" from="4374" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural networks and pattern recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence: Imitating Life</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Zurada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Ii</surname></persName>
		</editor>
		<editor>
			<persName><surname>Robinson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="194" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Minimum average correlation energy filters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Casasent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3633" to="3640" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation free shared weight networks for automatic vehicle detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Miramonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coffield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1457" to="1473" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Artificial neural networks for automatic target recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Daniell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kemsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tackett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baraghimian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2521" to="2531" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Survey of neural-network technology for automatic target recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neural-network filter to detect small targets in high clutter backgrounds</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Shirvaikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="252" to="257" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MACE Prefiltering for neural-network-based automatic target recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miramonti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf Neural Networks</title>
		<meeting>IEEE Conf Neural Networks<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<biblScope unit="page" from="4006" to="4011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A segmentation-free neural-network classifier for machine-printed numeric fields</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Forester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ganzberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th United States Postal Service Advanced Technol. Conf</title>
		<meeting>5th United States Postal Service Advanced Technol. Conf<address><addrLine>Washington, D.C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating neural networks for real-world applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Soulie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence: Imitating Life</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Zurada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Ii</surname></persName>
		</editor>
		<editor>
			<persName><surname>Robinson</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="396" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Backpropagation network and its configuration for blood vessel detection in angiograms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nekovei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Morphological filter design with genetic algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ehrhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Image Algebra and Morphological Image Processing</title>
		<meeting>SPIE Conf. Image Algebra and Morphological Image essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal mean-absolute hit-or-miss filters</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Loce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="827" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image algebra networks for pattern classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khabou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Image Algebra and Morphological Image Processing V</title>
		<meeting>SPIE Conf. Image Algebra and Morphological Image essing V<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic generation of morphological template features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gillies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Image Algebra And Morphological Image Processing</title>
		<meeting>SPIE Conf. Image Algebra And Morphological Image essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-07">July 1990</date>
			<biblScope unit="page" from="252" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimization of soft morphological filters by genetic algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuosmanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koskinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Image Algebra and Morphological Image Processing</title>
		<meeting>SPIE Conf. Image Algebra and Morphological Image essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-08">Aug. 1994</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facilitation of optimal morphological filter design via structuring element libraries and design constraints</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Loce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1008" to="1025" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic generation of morphological sequences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zmuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tamburino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rizki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Image Algebra and Morphological Image Processing</title>
		<meeting>SPIE Conf. Image Algebra and Morphological Image essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient descent techniques for feature detection template generation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Pont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Gader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Image Algebra and Morphological Image Processing II</title>
		<meeting>SPIE Image Algebra and Morphological Image essing II<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised training of structuring elements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. Image Algebra and Morphological Image Processing</title>
		<meeting>SPIE Conf. Image Algebra and Morphological Image essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
			<biblScope unit="page" from="188" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An Introduction to Morphological Image Processing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>SPIE</publisher>
			<pubPlace>Bellingham, WA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nonlinear correlation filter and morphology neural networks for image pattern and automatic target recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Won</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Missouri-Columbia, dissertation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synthetic discriminant functions using relaxed constraints</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Automat. Object Location IV</title>
		<meeting>SPIE Automat. Object Location IV<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-04">Apr. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MACE prefilter networks for automatic target recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Automat. Object Recognition IV</title>
		<meeting>SPIE Automat. Object Recognition IV<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
