<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Online Computation Offloading in Wireless Powered Mobile-Edge Computing Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE, Suzhi Bi, Senior Member, IEEE</roleName><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<email>lianghuang@zjut.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
							<email>yjzhang@ie.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Bi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Information Engineering</orgName>
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<postCode>310058</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Information Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<postCode>2013</postCode>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Information Engineer-ing</orgName>
								<orgName type="institution">Zhejiang University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Online Computation Offloading in Wireless Powered Mobile-Edge Computing Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F87BC6599D737A97E3A9ECCCECE9E66D</idno>
					<idno type="DOI">10.1109/TMC.2019.2928811</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mobile-edge computing</term>
					<term>wireless power transfer</term>
					<term>reinforcement learning</term>
					<term>resource allocation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wireless powered mobile-edge computing (MEC) has recently emerged as a promising paradigm to enhance the data processing capability of low-power networks, such as wireless sensor networks and internet of things (IoT). In this paper, we consider a wireless powered MEC network that adopts a binary offloading policy, so that each computation task of wireless devices (WDs) is either executed locally or fully offloaded to an MEC server. Our goal is to acquire an online algorithm that optimally adapts task offloading decisions and wireless resource allocations to the time-varying wireless channel conditions. This requires quickly solving hard combinatorial optimization problems within the channel coherence time, which is hardly achievable with conventional numerical optimization methods. To tackle this problem, we propose a Deep Reinforcement learning-based Online Offloading (DROO) framework that implements a deep neural network as a scalable solution that learns the binary offloading decisions from the experience. It eliminates the need of solving combinatorial optimization problems, and thus greatly reduces the computational complexity especially in large-size networks. To further reduce the complexity, we propose an adaptive procedure that automatically adjusts the parameters of the DROO algorithm on the fly. Numerical results show that the proposed algorithm can achieve near-optimal performance while significantly decreasing the computation time by more than an order of magnitude compared with existing optimization methods. For example, the CPU execution latency of DROO is less than 0.1 second in a 30-user network, making real-time and optimal offloading truly viable even in a fast fading environment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D UE to the small form factor and stringent production cost constraint, modern Internet of Things (IoT) devices are often limited in battery lifetime and computing power. Thanks to the recent advance in wireless power transfer (WPT) technology, the batteries of wireless devices (WDs) can be continuously charged over the air without the need of battery replacement <ref type="bibr" target="#b0">[1]</ref>. Meanwhile, the device computing power can be effectively enhanced by the recent development of mobile-edge computing (MEC) technology <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. With MEC, the WDs can offload computationally intensive tasks to nearby edge servers to reduce computation latency and energy consumption <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>The newly emerged wireless powered MEC combines the advantages of the two aforementioned technologies, and thus holds significant promise to solve the two fundamental performance limitations for IoT devices <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In this paper, we consider a wireless powered MEC system as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where the access point (AP) is responsible for both transferring RF (radio frequency) energy to and receiving computation offloading from the WDs. In particular, the WDs follow a binary task offloading policy <ref type="bibr" target="#b7">[8]</ref>, where a task is either computed locally or offloaded to the MEC server for remote computing. The system setup may correspond to a typical outdoor IoT network, where each energy-harvesting wireless sensor computes a non-partitionable simple sensing task with the assistance of an MEC server.</p><p>In a wireless fading environment, the time-varying wireless channel condition largely impacts the optimal offloading decision of a wireless powered MEC system <ref type="bibr" target="#b8">[9]</ref>. In a multi-user scenario, a major challenge is the joint optimization of individual computing mode (i.e., offloading or local computing) and wireless resource allocation (e.g., the transmission air time divided between WPT and offloading). Such problems are generally formulated as mixed integer programming (MIP) problems due to the existence of binary offloading variables. To tackle the MIP problems, branch-and-bound algorithms <ref type="bibr" target="#b9">[10]</ref> and dynamic programming <ref type="bibr" target="#b10">[11]</ref> have been adopted, however, with prohibitively high computational complexity, especially for large-scale MEC networks. To reduce the computational complexity, heuristic local search <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref> and convex relaxation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> methods are proposed. However, both of them require considerable number of iterations to reach a satisfying local optimum. Hence, they are not suitable for making real-time offloading decisions in fast fading channels, as the optimization problem needs to be re-solved once the channel fading has varied significantly.</p><p>In this paper, we consider a wireless powered MEC network with one AP and multiple WDs as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where each WD follows a binary offloading policy.</p><p>In particular, we aim to jointly optimize the individual WD's task offloading decisions, transmission time allocation between WPT and task offloading, and time allocation among multiple WDs according to the time-varying wireless channels. Towards this end, we propose a deep reinforcement learning-based online offloading (DROO) framework to maximize the weighted sum of the computation rates of all the WDs, i.e., the number of processed bits within a unit time. Compared with the existing integer programming and learning-based methods, we have the following novel contributions:</p><p>1) The proposed DROO framework learns from the past offloading experiences under various wireless fading conditions, and automatically improves its action generating policy. As such, it completely removes the need of solving complex MIP problems, and thus, the computational complexity does not explode with the network size. 2) Unlike many existing deep learning methods that optimize all system parameters at the same time resulting infeasible solutions, DROO decomposes the original optimization problem into an offloading decision sub-problem and a resource allocation subproblem, such that all physical constraints are guaranteed. It works for continuous state spaces and does not require the discretization of channel gains, thus, avoiding the curse of dimensionality problem. 3) To efficiently generate offloading actions, we devise a novel order-preserving action generation method. Specifically, it only needs to select from few candidate actions each time, thus is computationally feasible and efficient in large-size networks with high-dimensional action space. Meanwhile, it also provides high diversity in the generated actions and leads to better convergence performance than conventional action generation techniques. 4) We further develop an adaptive procedure that automatically adjusts the parameters of the DROO algorithm on the fly. Specifically, it gradually decreases the number of convex resource allocation sub-problems to be solved in a time frame. This effectively reduces the computational complexity without compromising the solution quality.</p><p>We evaluate the proposed DROO framework under extensive numerical studies. Our results show that on average the DROO algorithm achieves over 99.5% of the computation rate of the existing near-optimal benchmark method <ref type="bibr" target="#b6">[7]</ref>. Compared to the Linear Relaxation (LR) algorithm <ref type="bibr" target="#b12">[13]</ref>, it significantly reduces the CPU execution latency by more than an order of magnitude, e.g., from 0.81 second to 0.059 second in a 30-user network. This makes realtime and optimal design truly viable in wireless powered MEC networks even in a fast fading environment. The complete source code implementing DROO is available at https://github.com/revenol/DROO. The remainder of this paper is organized as follows. In Section 2, a review of related works in literature is presented. In Section 3, we describe the system model and problem formulation. We introduce the detailed designs of the DROO algorithm in Section 4. Numerical results are presented in Section 5. Finally, the paper is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There are many related works that jointly model the computing mode decision problem and resource allocation problem in MEC networks as the MIP problems. For instance, <ref type="bibr" target="#b6">[7]</ref> proposed a coordinate descent (CD) method that searches along one variable dimension at a time. <ref type="bibr" target="#b11">[12]</ref> studies a similar heuristic search method for multi-server MEC networks, which iteratively adjusts binary offloading decisions. Another widely adopted heuristic is through convex relaxation, e.g., by relaxing integer variables to be continuous between 0 and 1 <ref type="bibr" target="#b12">[13]</ref> or by approximating the binary constraints with quadratic constraints <ref type="bibr" target="#b13">[14]</ref>. Nonetheless, on one hand, the solution quality of the reduced-complexity heuristics is not guaranteed. On the other hand, both searchbased and convex relaxation methods require considerable number of iterations to reach a satisfying local optimum and are inapplicable for fast fading channels.</p><p>Our work is inspired by recent advantages of deep reinforcement learning in handling reinforcement learning problems with large state spaces <ref type="bibr" target="#b14">[15]</ref> and action spaces <ref type="bibr" target="#b15">[16]</ref>. In particular, it relies on deep neural networks (DNNs) <ref type="bibr" target="#b16">[17]</ref> to learn from the training data samples, and eventually produces the optimal mapping from the state space to the action space. There exists limited work on deep reinforcement learning-based offloading for MEC networks <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b21">[22]</ref>. By taking advantage of parallel computing, <ref type="bibr" target="#b18">[19]</ref> proposed a distributed deep learning-based offloading (DDLO) algorithm for MEC networks. For an energy-harvesting MEC networks, <ref type="bibr" target="#b19">[20]</ref> proposed a deep Q-network (DQN) based offloading policy to optimize the computational performance. Under the similar network setup, <ref type="bibr" target="#b20">[21]</ref> studied an online computation offloading policy based on DQN under random task arrivals. However, both DQN-based works take discretized channel gains as the input state vector, and thus suffer from the curse of dimensionality and slow convergence when high channel quantization accuracy is required. Besides, because of its exhaustive search nature in selecting the action in each iteration, DQN is not suitable for handling problems with high-dimensional action spaces <ref type="bibr" target="#b22">[23]</ref>. In our problem, there are a total of 2 N offloading decisions (actions) to choose from, where DQN is evidently inapplicable even for a small N , e.g., N = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Model</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we consider a wireless powered MEC network consisting of an AP and N fixed WDs, denoted as a set N = {1, 2, . . . , N }, where each device has a single antenna. In practice, this may correspond to a static sensor network or a low-power IoT system. The AP has stable power supply and can broadcast RF energy to the WDs. Each WD has a rechargeable battery that can store the harvested energy to power the operations of the device. Suppose that the AP has higher computational capability than the WDs, so that the WDs may offload their computing tasks to the AP. Specifically, we suppose that WPT and communication (computation offloading) are performed in the same frequency band. Accordingly, a time-division-multiplexing (TDD) circuit is implemented at each device to avoid mutual interference between WPT and communication.</p><p>The system time is divided into consecutive time frames of equal lengths T , which is set smaller than the channel coherence time, e.g., in the scale of several seconds <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> in a static IoT environment. At each tagged time, both the amount of energy that a WD harvests from the AP and the communication speed between them are related to the wireless channel gain. Let h i denote the wireless channel gain between the AP and the i-th WD at a tagged time frame. The channel is assumed to be reciprocal in the downlink and uplink,<ref type="foot" target="#foot_0">1</ref> and remain unchanged within each time frame, but may vary across different frames. At the beginning of a time frame, aT amount of time is used for WPT, a ∈ [0, 1], where the AP broadcasts RF energy for the WDs to harvest. Specifically, the i-th WD harvests E i = µP h i aT amount of energy, where µ ∈ (0, 1) denotes the energy harvesting efficiency and P denotes the AP transmit power <ref type="bibr" target="#b0">[1]</ref>. With the harvested energy, each WD needs to accomplish a prioritized computing task before the end of a time frame. A unique weight w i is assigned to the i-th WD. The greater the weight w i , the more computation rate is allocated to the i-th WD. In this paper, we consider a binary offloading policy, such that the task is either computed locally at the WD (such as WD2 in Fig. <ref type="figure" target="#fig_0">1</ref>) or offloaded to the AP (such as WD1 and WD3 in Fig. <ref type="figure" target="#fig_0">1</ref>). Let x i ∈ {0, 1} be an indicator variable, where x i = 1 denotes that the i-th user's computation task is offloaded to the AP, and x i = 0 denotes that the task is computed locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Computing Mode</head><p>A WD in the local computing mode can harvest energy and compute its task simultaneously <ref type="bibr" target="#b5">[6]</ref>. Let f i denote the processor's computing speed (cycles per second) and 0 ≤ t i ≤ T denote the computation time. Then, the amount of processed bits by the WD is f i t i /φ, where φ &gt; 0 denotes the number of cycles needed to process one bit of task data. Meanwhile, the energy consumption of the WD due to the computing is constrained by</p><formula xml:id="formula_0">k i f 3 i t i ≤ E i ,</formula><p>where k i denotes the computation energy efficiency coefficient <ref type="bibr" target="#b12">[13]</ref>. It can be shown that to process the maximum amount of data within T under the energy constraint, a WD should exhaust the harvested energy and compute throughout the time frame, i.e., t * i = T and accordingly f * i = Ei kiT 1 3 . Thus, the local computation rate (in bits per second) is</p><formula xml:id="formula_1">r * L,i (a) = f * i t * i φT = η 1 h i k i 1 3 a 1 3 ,<label>(1)</label></formula><p>where η 1 (µP )</p><formula xml:id="formula_2">1 3</formula><p>/φ is a fixed parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Edge Computing Mode</head><p>Due to the TDD constraint, a WD in the offloading mode can only offload its task to the AP after harvesting energy. We denote τ i T as the offloading time of the i-th WD,</p><formula xml:id="formula_3">τ i ∈ [0, 1].</formula><p>Here, we assume that the computing speed and the transmit power of the AP is much larger than the size-and energy-constrained WDs, e.g., by more than three orders of magnitude <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Besides, the computation feedback to be downloaded to the WD is much shorter than the data offloaded to the edge server. Accordingly, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we safely neglect the time spent on task computation and downloading by the AP, such that each time frame is only occupied by WPT and task offloading, i.e.,</p><formula xml:id="formula_4">N i=1 τ i + a ≤ 1.<label>(2)</label></formula><p>To maximize the computation rate, an offloading WD exhausts its harvested energy on task offloading, i.e., P * i = Ei τiT . Accordingly, the computation rate equals to its data offloading capacity, i.e.,</p><formula xml:id="formula_5">r * O,i (a, τ i ) = Bτ i v u log 2 1 + µP ah 2 i τ i N 0 ,<label>(3)</label></formula><p>where B denotes the communication bandwidth and N 0 denotes the receiver noise power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Problem Formulation</head><p>Among all the system parameters in (1) and (3), we assume that only the wireless channel gains h = {h i |i ∈ N } are time-varying in the considered period, while the others (e.g., w i 's and k i 's) are fixed parameters. Accordingly, the weighted sum computation rate of the wireless powered MEC network in a tagged time frame is denoted as</p><formula xml:id="formula_6">Q (h, x, τ , a) N i=1 w i (1 -x i )r * L,i (a) + x i r * O,i (a, τ i ) ,</formula><p>where x = {x i |i ∈ N } and τ = {τ i |i ∈ N }. τ , a Fig. <ref type="figure">2</ref>. The two-level optimization structure of solving (P1).</p><p>For each time frame with channel realization h, we are interested in maximizing the weighted sum computation rate:</p><formula xml:id="formula_7">(P 1) : Q * (h) = maximize x,τ ,a Q (h, x, τ , a) (4a) subject to N i=1 τ i + a ≤ 1, (4b) a ≥ 0, τ i ≥ 0, ∀i ∈ N , (4c) x i ∈ {0, 1}.<label>(4d)</label></formula><p>We can easily infer that τ i = 0 if x i = 0, i.e., when the i-th WD is in the local computing mode. Problem (P1) is a mixed integer programming nonconvex problem, which is hard to solve. However, once x is given, (P1) reduces to a convex problem as follows.</p><formula xml:id="formula_8">(P 2) : Q * (h, x) = maximize τ ,a Q (h, x, τ , a) subject to N i=1 τ i + a ≤ 1, a ≥ 0, τ i ≥ 0, ∀i ∈ N .</formula><p>Accordingly, problem (P1) can be decomposed into two subproblems, namely, offloading decision and resource allocation (P2), as shown in Fig. <ref type="figure">2:</ref> • Offloading Decision: One needs to search among the 2 N possible offloading decisions to find an optimal or a satisfying sub-optimal offloading decision x. For instance, meta-heuristic search algorithms are proposed in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b11">[12]</ref> to optimize the offloading decisions. However, due to the exponentially large search space, it takes a long time for the algorithms to converge.</p><p>• Resource Allocation: The optimal time allocation {a * , τ * } of the convex problem (P2) can be efficiently solved, e.g., using a one-dimensional bi-section search over the dual variable associated with the time allocation constraint in O(N ) complexity <ref type="bibr" target="#b6">[7]</ref>.</p><p>The major difficulty of solving (P1) lies in the offloading decision problem. Traditional optimization algorithms require iteratively adjusting the offloading decisions towards the optimum <ref type="bibr" target="#b10">[11]</ref>, which is fundamentally infeasible for real-time system optimization under fast fading channel. To tackle the complexity issue, we propose a novel deep reinforcement learning-based online offloading (DROO) algorithm that can achieve a millisecond order of computational time in solving the offloading decision problem.</p><p>Before leaving this section, it is worth mentioning the advantages of applying deep reinforcement learning over The wireless channel gain between the i-th WD and the AP a</p><p>The fraction of time that the AP broadcasts RF energy for the WDs to harvest E i</p><p>The amount of energy harvested by the i-th WD P</p><p>The AP transmit power when broadcasts RF energy µ</p><p>The energy harvesting efficiency w i</p><p>The weight assigned to the i-th WD x i An offloading indicator for the i-th WD f i</p><p>The processor's computing speed of the i-th WD φ</p><p>The number of cycles needed to process one bit of task data t i</p><p>The computation time of the i-th WD k i</p><p>The computation energy efficiency coefficient τ i</p><p>The fraction of time allocated to the i-th WD for task offloading B</p><p>The communication bandwidth N 0</p><p>The receiver noise power h</p><p>The vector representation of wireless channel gains</p><formula xml:id="formula_9">{h i |i ∈ N } x</formula><p>The vector representation of offloading indicators</p><formula xml:id="formula_10">{x i |i ∈ N } τ</formula><p>The vector representation of</p><formula xml:id="formula_11">{τ i |i ∈ N } Q(•)</formula><p>The weighted sum computation rate function π</p><p>Offloading policy function θ</p><p>The parameters of the DNN xt Relaxed computation offloading action K</p><p>The number of quantized binary offloading actions g K</p><p>The quantization function</p><formula xml:id="formula_12">L(•)</formula><p>The training loss function of the DNN δ</p><p>The training interval of the DNN ∆</p><p>The updating interval for K supervised learning-based deep neural network (DNN) approaches (such as in <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b27">[28]</ref>) in dynamic wireless applications. Other than the fact that deep reinforcement learning does not need manually labeled training samples (e.g., the (h, x) pairs in this paper) as DNN, it is much more robust to the change of user channel distributions. For instance, the DNN needs to be completely retrained once some WDs change their locations significantly or are suddenly turned off. In contrast, the adopted deep reinforcement learning method can automatically update its offloading decision policy upon such channel distribution changes without manual involvement. Those important notations used throughout this paper are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE DROO ALGORITHM</head><p>We aim to devise an offloading policy function π that quickly generates an optimal offloading action x * ∈ {0, 1} N of (P1) once the channel realization h ∈ R N &gt;0 is revealed at the beginning of each time frame. The policy is denoted as</p><formula xml:id="formula_13">π : h → x * .<label>(5)</label></formula><p>The proposed DROO algorithm gradually learns such policy function π from the experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithm Overview</head><p>The structure of the DROO algorithm is illustrated in Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offloading Action Generation</head><p>Fig. <ref type="figure">3</ref>. The schematics of the proposed DROO algorithm.</p><p>generation and offloading policy update. The generation of the offloading action relies on the use of a DNN, which is characterized by its embedded parameters θ, e.g., the weights that connect the hidden neurons. In the t-th time frame, the DNN takes the channel gain h t as the input, and outputs a relaxed offloading action xt (each entry is relaxed to continuous between 0 and 1) based on its current offloading policy π θt , parameterized by θ t . The relaxed action is then quantized into K binary offloading actions, among which one best action x * t is selected based on the achievable computation rate as in (P2). The corresponding {x * t , a * t , τ * t } is output as the solution for h t , which guarantees that all the physical constrains listed in (4b)-(4d) are satisfied. The network takes the offloading action x * t , receives a reward Q * (h t , x * t ), and adds the newly obtained state-action pair (h t , x * t ) to the replay memory. Subsequently, in the policy update stage of the t-th time frame, a batch of training samples are drawn from the memory to train the DNN, which accordingly updates its parameter from θ t to θ t+1 (and equivalently the offloading policy π θt+1 ). The new offloading policy π θt+1 is used in the next time frame to generate offloading decision x * t+1 according to the new channel h t+1 observed. Such iterations repeat thereafter as new channel realizations are observed, and the policy π θt of the DNN is gradually improved. The descriptions of the two stages are detailed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Offloading Action Generation</head><p>Suppose that we observe the channel gain realization h t in the t-th time frame, where t = 1, 2, • • • . The parameters of the DNN θ t are randomly initialized following a zero-mean normal distribution when t = 1. The DNN first outputs a relaxed computation offloading action xt , represented by a parameterized function xt = f θt (h t ), where</p><formula xml:id="formula_14">xt = {x t,i |x t,i ∈ [0, 1], i = 1, • • • , N }<label>(6)</label></formula><p>and xt,i denotes the i-th entry of xt .</p><p>The well-known universal approximation theorem claims that one hidden layer with enough hidden neurons suffices to approximate any continuous mapping f if a proper activation function is applied at the neurons, e.g., sigmoid, ReLu, and tanh functions <ref type="bibr" target="#b28">[29]</ref>. Here, we use ReLU as the activation function in the hidden layers, where the output y and input v of a neuron are related by y = max{v, 0}. In the output layer, we use a sigmoid activation function, i.e., y = 1/ (1 + e -v ), such that the relaxed offloading action satisfies xt,i ∈ (0, 1).</p><p>Then, we quantize xt to obtain K binary offloading actions, where K is a design parameter. The quantization function, g K , is defined as</p><formula xml:id="formula_15">g K : xt → {x k | x k ∈ {0, 1} N , k = 1, • • • , K}.<label>(7)</label></formula><p>In general, K can be any integer within [1, 2 N ] (N is the number of WDs), where a larger K results in better solution quality and higher computational complexity, and vice versa. To balance the performance and complexity, we propose an order-preserving quantization method, where the value of K could be set from 1 to (N + 1). The basic idea is to preserve the ordering during quantization. That is, for each quantized action</p><formula xml:id="formula_16">x k , x k,i ≥ x k,j should hold if xt,i ≥ xt,j for all i, j ∈ {1, • • • , N }.</formula><p>Specifically, for a given 1 ≤ K ≤ N + 1, the set of K quantized actions {x k } is generated from the relaxed action xt as follows:</p><p>1) The first binary offloading decision x 1 is obtained as</p><formula xml:id="formula_17">x 1,i = 1 xt,i &gt; 0.5, 0 xt,i ≤ 0.5,<label>(8)</label></formula><formula xml:id="formula_18">for i = 1, • • • , N . 2)</formula><p>To generate the remaining K -1 actions, we first order the entries of xt with respective to their dis- tances to 0.5, denoted by |x t,(1</p><formula xml:id="formula_19">) -0.5| ≤ |x t,(2) - 0.5| ≤ • • • ≤ |x t,(i) -0.5| • • • ≤ |x t,(N ) -0.5|, where xt,(i) is the i-th order statistic of xt . Then, the k- th offloading decision x k , where k = 2, • • • , K, is calculated based on xt,(k-1) as x k,i =          1 xt,i &gt; xt,(k-1) , 1 xt,i = xt,(k-1) and xt,(k-1) ≤ 0.5, 0 xt,i = xt,(k-1) and xt,(k-1) &gt; 0.5, 0 xt,i &lt; xt,(k-1) ,<label>(9)</label></formula><formula xml:id="formula_20">for i = 1, • • • , N .</formula><p>Because there are in total N order statistic of xt , while each can be used to generate one quantized action from (9), the above order-preserving quantization method in ( <ref type="formula" target="#formula_17">8</ref>) and ( <ref type="formula" target="#formula_19">9</ref>) generates at most (N + 1) quantized actions, i.e., K ≤ N + 1. In general, setting a large K (e.g., K = N ) leads to better computation rate performance at the cost of higher complexity. However, as we will show later in Section 4.4, it is not only inefficient but also unnecessary to generate a large number of quantized actions in each time frame. Instead, setting a small K (even close to 1) suffices to achieve good computation rate performance and low complexity after sufficiently long training period.</p><p>We use an example to illustrate the above orderpreserving quantization method. Suppose that xt = [0.2, 0.4, 0.7, 0.9] and K = 4. The corresponding order statistics of xt are xt,(1) = 0.4, xt,(2) = 0.7, xt,(3) = 0.2, and xt,(4) = 0.9. Therefore, the 4 offloading actions generated from the above quantization method are</p><formula xml:id="formula_21">x 1 = [0, 0, 1, 1], x 2 = [0, 1, 1, 1], x 3 = [0, 0, 0, 1], and x 4 = [1, 1, 1, 1].</formula><p>In comparison, when the conventional KNN method is used, the obtained actions are</p><formula xml:id="formula_22">x 1 = [0, 0, 1, 1], x 2 = [0, 1, 1, 1], x 3 = [0, 0, 0, 1], and x 4 = [0, 1, 0, 1].</formula><p>Compared to the KNN method where the quantized solutions are closely placed around x, the offloading actions produced by the order-preserving quantization method are separated by a larger distance. Intuitively, this creates higher diversity in the candidate action set, thus increasing the chance of finding a local maximum around xt . In Section 5.1, we show that the proposed order-preserving quantization method achieves better convergence performance than KNN method.</p><p>Recall that each candidate action x k can achieve Q * (h t , x k ) computation rate by solving (P2). Therefore, the best offloading action x * t at the t-th time frame is chosen as</p><formula xml:id="formula_23">x * t = arg max xi∈{x k } Q * (h t , x i ).<label>(10)</label></formula><p>Note that the K-times evaluation of Q * (h t , x k ) can be processed in parallel to speed up the computation of <ref type="bibr" target="#b9">(10)</ref>.</p><p>Then, the network outputs the offloading action x * t along with its corresponding optimal resource allocation (τ * t , a * t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Offloading Policy Update</head><p>The offloading solution obtained in <ref type="bibr" target="#b9">(10)</ref> will be used to update the offloading policy of the DNN. Specifically, we maintain an initially empty memory of limited capacity. At the t-th time frame, a new training data sample (h t , x * t ) is added to the memory. When the memory is full, the newly generated data sample replaces the oldest one.</p><p>We use the experience replay technique <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b29">[30]</ref> to train the DNN using the stored data samples. In the t-th time frame, we randomly select a batch of training data samples {(h τ , x * τ ) | τ ∈ T t } from the memory, characterized by a set of time indices T t . The parameters θ t of the DNN are updated by applying the Adam algorithm <ref type="bibr" target="#b30">[31]</ref> to reduce the averaged cross-entropy loss, as</p><formula xml:id="formula_24">L(θt) = - 1 |Tt| τ ∈T t (x * τ ) log f θ t (hτ ) + (1 -x * τ ) log 1 -f θ t (hτ ) ,</formula><p>where |T t | denotes the size of T t , the superscript denotes the transpose operator, and the log function denotes the element-wise logarithm operation of a vector. The detailed update procedure of the Adam algorithm is omitted here for brevity. In practice, we train the DNN every δ time frames after collecting sufficient number of new data samples. The experience replay technique used in our framework has several advantages. First, the batch update has a reduced complexity than using the entire set of data samples. Second, the reuse of historical data reduces the variance of θ t during the iterative update. Third, the random sampling fastens the convergence by reducing the correlation in the training samples.</p><p>Overall, the DNN iteratively learns from the best stateaction pairs (h t , x * t )'s and generates better offloading decisions output as the time progresses. Meanwhile, with the finite memory space constraint, the DNN only learns from the most recent data samples generated by the most recent (and more refined) offloading policies. This closed-loop reinforcement learning mechanism constantly improves its offloading policy until convergence. We provide the pseudocode of the DROO algorithm in Algorithm 1.  of solving hard MIP problems, and thus has the potential to significantly reduce the complexity. The major computational complexity of the DROO algorithm comes from solving (P2) K times in each time frame to select the best offloading action. Evidently, a larger K (e.g., K = N ) in general leads to a better offloading decision in each time frame and accordingly a better offloading policy in the long term. Therefore, there exists a fundamental performancecomplexity tradeoff in setting the value of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1:</head><p>In this subsection, we propose an adaptive procedure to automatically adjust the number of quantized actions generated by the order-preserving quantization method. We argue that using a large and fixed K is not only computationally inefficient but also unnecessary in terms of computation rate performance. To see this, consider a wireless powered MEC network with N = 10 WDs. We apply the DROO algorithm with a fixed K = 10 and plot in Fig. <ref type="figure" target="#fig_1">4</ref> the index of the best action x * t calculated from (10) over time, denoted as k * t . For instance, k * t = 2 indicates that the best action in the t-th time frame is ranked the second among the K ordered quantized actions. In the figure, the curve is plotted as the 50-time-frames rolling average of k * t and the light shadow region is the upper and lower bounds of k * t in the past 50 time frames. Apparently, most of the selected indices k * t are no larger than 5 when t ≥ 5000. This indicates that those generated offloading actions x k with k &gt; 5 are redundant. In other words, we can gradually reduce K during the learning process to speed up the algorithm without compromising the performance.</p><p>Inspired by the results in Fig. <ref type="figure" target="#fig_1">4</ref>, we propose an adaptive method for setting K. We denote K t as the number of binary offloading actions generated by the quantization function at the t-th time frame. We set K 1 = N initially and update K t every ∆ time frames, where ∆ is referred to as the updating interval for K. Upon an update time frame, K t is set as 1 plus the largest k * t observed in the past ∆ time frames. The reason for the additional 1 is to allow K t to increase during the iterations. Mathematically, K t is calculated as</p><formula xml:id="formula_25">K t =      N, t = 1, min max k * t-1 , • • • , k * t-∆ + 1, N , t mod ∆ = 0, K t-1 ,</formula><p>otherwise, for t ≥ 1. For an extreme case with ∆ = 1, K t updates in each time frame. Meanwhile, when ∆ → ∞, K t never updates such that it is equivalent to setting a constant K = N . In Section 5.2, we numerically show that setting a proper ∆ can effectively speed up the learning process without compromising the computation rate performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NUMERICAL RESULTS</head><p>In this section, we use simulations to evaluate the performance of the proposed DROO algorithm. In all simulations, we use the parameters of Powercast TX91501-3W with P = 3 Watts for the energy transmitter at the AP, and those of P2110 Powerharvester for the energy receiver at each WD. 2 The energy harvesting efficiency µ = 0.51.</p><p>The distance from the i-th WD to the AP, denoted by d i , is uniformly distributed in the range of (2.5, 5. </p><formula xml:id="formula_26">t = [h t 1 , h t 2 , • • • , h t N ]</formula><p>, is generated from a Rayleigh fading channel model as h t i = hi α t i . Here α t i is the independent random channel fading factor following an exponential distribution with unit mean. Without loss of generality, the channel gains are assumed to remain the same within one time frame and vary independently from one time frame to another. We assume equal computing efficiency k i = 10 -26 , i = 1, • • • , N , and φ = 100 for all the WDs <ref type="bibr" target="#b31">[32]</ref>. The data offloading bandwidth B = 2 MHz, receiver noise power N 0 = 10 -10 , and v u = 1.1. Without loss of generality, we set T = 1 and the w i = 1 if i is an odd number and w i = 1.5 otherwise. All the simulations are performed on a desktop with an Intel Core i5-4570 3.2 GHz CPU and 12 GB memory. We simply consider a fully connected DNN consisting of one input layer, two hidden layers, and one output layer in the proposed DROO algorithm, where the first and second hidden layers have 120 and 80 hidden neurons, respectively. Note that the DNN can be replaced by other structures with different number of hidden layers and neurons, or even other types of neural networks to fit the specific learning problem, such as convolutional neural network (CNN) or recurrent neural network (RNN) <ref type="bibr" target="#b32">[33]</ref>. In this paper, we find that a simple two-layer perceptron suffices to achieve satisfactory convergence performance, while better convergence performance is expected by further optimizing the DNN parameters. We implement the DROO algorithm in Python with TensorFlow  learning rate for Adam optimizer as 0.01. The source code is available at https://github.com/revenol/DROO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Convergence Performance</head><p>We first consider a wireless powered MEC network with N = 10 WDs. Here, we define the normalized computation rate</p><formula xml:id="formula_27">Q(h, x) ∈ [0, 1], as Q(h, x) = Q * (h, x) max x ∈{0,1} N Q * (h, x ) ,<label>(11)</label></formula><p>where the optimal solution in the denominator is obtained by enumerating all the 2 N offloading actions. In Fig. <ref type="figure" target="#fig_3">5</ref>, we plot the training loss L(θ t ) of the DNN and the normalized computation rate Q. Here, we set a fixed K = N . In the figure below, the blue curve denotes the moving average of Q over the last 50 time frames, and the light blue shadow denotes the maximum and minimum of Q in the last 50 frames. We see that the moving average Q of DROO gradually converges to the optimal solution when t is large. Specifically, the achieved average Q exceeds 0.98 at an early stage when t &gt; 400 and the variance gradually decreases to zero as t becomes larger, e.g., when t &gt; 3, 000. Meanwhile, in the figure above, the training loss L(θ t ) gradually decreases and stabilizes at around 0.04, whose fluctuation is mainly due to the random sampling of training data.</p><p>In Fig. <ref type="figure" target="#fig_4">6</ref>, we evaluate DROO for MEC networks with alternating-weight WDs. We evaluate the worst case by alternating the weights of all WDs between 1 and 1.5 at the same time, specifically, at t = 6, 000 and t = 8, 000. The training loss sharply increases after the weights alternated and gradually decreases and stabilizes after training for 1,000 time frames, which means that DROO automatically updates its offloading decision policy and converges to the new optimal solution. Meanwhile, as shown in Fig. <ref type="figure" target="#fig_4">6</ref>, the minimum of Q is greater than 0.95 and the moving average of Q is always greater than 0.99 for t &gt; 6, 000.</p><p>In Fig. <ref type="figure">7</ref>, we evaluate the ability of DROO in supporting WDs' temporarily critical computation demand. Suppose  that W D 1 and W D 2 have a temporary surge of commutation demands. We double W D 2 's weight from 1.5 to 3 at time frame t = 4, 000, triple W D 1 's weight from 1 to 3 at t = 6, 000, and reset both of their weights to the original values at t = 8, 000. In the top sub-figure in Fig. <ref type="figure">7</ref>, we plot the relative computation rates for both WDs, where each WD's computation rate is normalized against that achieved under the optimal offloading actions with their original weights. In the first 3,000 time frames, DROO gradually converges and the corresponding relative computation rates for both WDs are lower than the baseline at most of the time frames. During time frames 4, 000 &lt; t &lt; 8, 000, W D 2 's weight is doubled. Its computation rate significantly improves over the baseline, where at some time frames the improvement can be as high as 2 to 3 times of the baseline. Similar rate improvement is also observed for W D 1 when its weight is tripled between 6, 000 &lt; t &lt; 8, 000. In addition, their computation rates gradually converge to the baseline when their weights are reset to the original value after t = 8, 000. On average, W D 1 and W D 2 have experienced 26% and 12% higher computation rate, respectively, during their periods with increased weights. In the bottom subfigure in Fig. <ref type="figure">7</ref>, we plot the normalized computation rate performance of DROO, which shows that the algorithm can quickly adapt itself to the temporary demand variation of users. The results in Fig. <ref type="figure">7</ref> have verified the ability of the propose DROO framework in supporting temporarily critical service quality requirements. In Fig. <ref type="figure">8</ref>, we evaluate DROO for MEC networks where WDs can be occasionally turned off/on. After DROO converges, we randomly turn off on one WD at each time frame t = 6, 000, 6, 500, 7, 000, 7, 500, and then turn them on at time frames t = 8, 000, 8, 500, 9, 000. At time frame t = 9, 500, we randomly turn off two WDs, resulting an MEC network with 8 acitve WDs. Since the number of neurons in the input layer of DNN is fixed as N = 10, we set the input channel gains h for the inactive WDs as 0 to exclude them from the resource allocation optimization with respect to (P2). We numerically tudy the performance of this modified DROO in Fig. <ref type="figure">8</ref>. Note that, when evaluating the normalized computation rate Q via equation ( <ref type="formula" target="#formula_27">11</ref>  the denominator is re-computed when one WD is turned off/on. For example, when there are 8 active WDs in the MEC network, the denominator is obtained by enumerating all the 2 8 offloading actions. As shown in Fig. <ref type="figure">8</ref>, the training loss L(θ t ) increases little after WDs are turned off/on, and the moving average of the resulting Q is always greater than 0.99. In Fig. <ref type="figure">9</ref>, we further study the effect of different algorithm parameters on the convergence performance of DROO, including different memory sizes, batch sizes, training intervals, and learning rates. In Fig. <ref type="figure" target="#fig_4">6</ref>(a), a small memory (=128) causes larger fluctuations on the convergence performance, while a large memory (=2048) requires more training data to converge to optimal, as Q = 1. In the following simulations, we choose the memory size as 1024. For each training procedure, we randomly sample a batch of data samples from the memory to improve the DNN. Hence, the batch size must be no more than the memory size 1024. As shown in Fig. <ref type="figure" target="#fig_4">6</ref>(b), a small batch size (=32) does not take advantage of all training data stored in the memory, while a large batch size (=1024) frequently uses the "old" training data and degrades the convergence performance. Furthermore, a large batch size consumes more time for training. As a trade-off between convergence speed and computation time, we set the training batch size |T | = 128 in the following simulations. In Fig. <ref type="figure" target="#fig_4">6</ref>(c), we investigate the convergence of DROO under different training intervals δ. DROO converges faster with shorter training interval, and thus more frequent policy update. However, numerical results show that it is unnecessary to train and update the DNN too frequently. Hence, we set the training interval δ = 10 to speed up the convergence of DROO. In Fig. <ref type="figure" target="#fig_4">6</ref>(d), we study the impact of the learning rate in Adam optimizer <ref type="bibr" target="#b30">[31]</ref> to the convergence performance. We notice that either a too small or a too large learning rate causes the algorithm to converge to a local optimum. In the following simulations, we set the learning rate as 0.01.</p><p>In Fig. <ref type="figure" target="#fig_0">10</ref>, we compare the performance of two quantization methods: the proposed order-preserving quantization and the conventional KNN quantization method under different K. In particular, we plot the the moving average of Q over a window of 200 time frames. When K = N , both methods converge to the optimal offloading actions, i.e., the moving average of Q approaches 1. However, they both achieve suboptimal offloading actions when K is small. For instance, when K = 2, the order-preserving quantization method and KNN both only converge to around 0.95. Nonetheless, we can observe that when K ≥ 2, the order-preserving quantization method converges faster than the KNN method. Intuitively, this is because the orderpreserving quantization method offers a larger diversity in the candidate actions than the KNN method. Therefore, the training of DNN requires exploring fewer offloading actions before convergence. Notice that the DROO algorithm does not converge for both quantization methods when K = 1. This is because the DNN cannot improve its offloading policy when action selection is absent.</p><p>The simulation results in this subsection show that the proposed DROO framework can quickly converge to the optimal offloading policy, especially when the proposed order-preserving action quantization method is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of Updating Intervals ∆</head><p>In Fig. <ref type="figure" target="#fig_7">11</ref>, we further study the impact of the updating interval of K (i.e., ∆) on the convergence property. Here, we use the adaptive setting method of K in Section 4.4 and plot the moving average of Q over a window of 200 time frames. We see that the DROO algorithm converges to the optimal solution only when setting a sufficiently large ∆, e.g., ∆ ≥ 16. Meanwhile, we also plot in Fig. <ref type="figure" target="#fig_0">12</ref> the moving average of K t under different ∆. We see that K t increases with ∆ when t is large. This indicates that setting a larger ∆ will lead to higher computational complexity, i.e., requires computing (P2) more times in a time frame. Therefore, a performance-complexity tradeoff exists in setting ∆.</p><p>To properly choose an updating interval ∆, we plot in Fig. <ref type="figure" target="#fig_0">13</ref> the tradeoff between the total CPU execution latency of 10000 channel realizations and the moving average of Q in the last time frame. On one hand, we see that the average of Q quickly increases from 0.96 to close to 1 when ∆ ≤ 16, while the improvement becomes marginal afterwards when we further increase ∆. On the other hand, the CPU execution latency increases monotonically with ∆.</p><p>To balance between performance and complexity, we set ∆ = 32 for DROO algorithm in the following simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Computation Rate Performance</head><p>Regarding to the weighted sum computation rate performance, we compare our DROO algorithm with three representative benchmarks:</p><p>• Coordinate Descent (CD) algorithm <ref type="bibr" target="#b6">[7]</ref>. The CD algorithm iteratively swaps in each round the computing mode of the WD that leads to the largest computation rate improvement. That is, from x i = 0 to x i = 1, or vice versa. The iteration stops when the computation performance cannot be further improved by the computing mode swapping. The CD method is shown to achieve near-optimal performance under different N .</p><p>• Linear Relaxation (LR) algorithm <ref type="bibr" target="#b12">[13]</ref>.  Then the optimization problem (P1) with this relaxed constraint is convex with respect to {x i } and can be solved using the CVXPY convex optimization toolbox. 3 Once xi is obtained, the binary offloading decision x i is determined as follows</p><formula xml:id="formula_28">x i = 1, when r * O,i (a, τ i ) ≥ r * L,i (a), 0, otherwise.<label>(12)</label></formula><p>• Local Computing. All N WDs only perform local computation, i.e., setting</p><formula xml:id="formula_29">x i = 0, i = 1, • • • , N in (P2).</formula><p>• Edge Computing. All N WDs offload their tasks to the AP, i.e., setting</p><formula xml:id="formula_30">x i = 1, i = 1, • • • , N in (P2).</formula><p>In Fig. <ref type="figure" target="#fig_1">14</ref>, we first compare the computation rate performance achieved by different offloading algorithms under varying number of WDs, N . Before the evaluation, DROO has been trained with 24, 000 independent wireless channel realizations, and its offloading policy has converged. This is reasonable since we are more interested in the long-term operation performance <ref type="bibr" target="#b33">[34]</ref> for field deployment. Each point in the figure is the average performance of 6, 000 independent wireless channel realizations. We see that DROO achieves similar near-optimal performance with the CD method, and significantly outperforms the Edge Computing and Local Computing algorithms. In Fig. <ref type="figure" target="#fig_3">15</ref>, we further compare the performance of DROO and LR algorithms. For better exposition, we plot the normalized computation rate Q achievable by DROO and LR. Specifically, we enumerate all 2 N possible offloading actions as in <ref type="bibr" target="#b10">(11)</ref> when N = 10.</p><p>For N = 20 and 30, it is computationally prohibitive to enumerate all the possible actions. In this case, Q is obtained by normalizing the computation rate achievable by DROO (or LR) against that of CD method. We then plot both the median and the confidence intervals of Q over 6000 independent channel realizations. We see that the median of DROO is always close-to-1 for different number of users, and the confidence intervals are mostly above 0.99. Some normalized computation rate Q of DROO is greater than 1, since DROO generates greater computation rate than CD at some time frame. In comparison, the median of the LR algorithm is always less than 1. The results in Fig. <ref type="figure" target="#fig_1">14</ref> and Fig. <ref type="figure" target="#fig_3">15</ref> show that the proposed DROO method can achieve near-optimal computation rate performance under different network placements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Execution Latency</head><p>At last, we evaluate the execution latency of the DROO algorithm. The computational complexity of DROO algorithm greatly depends on the complexity in solving the resource allocation sub-problem (P2). For fair comparison, we use the same bi-section search method as the CD algorithm in <ref type="bibr" target="#b6">[7]</ref>. The CD method is reported to achieve an O(N 3 ) complexity. For the DROO algorithm, we consider both using a fixed K = N and an adaptive K as in Section 4.4. Note that the execution latency for DROO listed in  <ref type="table" target="#tab_9">2</ref> that an adaptive K can effectively reduce the CPU execution latency than a fixed K = N . Besides, DROO with an adaptive K requires much shorter CPU execution latency than the CD algorithm and the LR algorithm. In particular, it generates an offloading action in less than 0.1 second when N = 30, while CD and LR take 65 times and 14 times longer CPU execution latency, respectively. Overall, DROO achieves similar rate performance as the near-optimal CD algorithm but requires substantially less CPU execution latency than the heuristic LR algorithm. The wireless-powered MEC network considered in this paper may correspond to a static IoT network with both the transmitter and receivers are fixed in locations. Measurement experiments <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> show that the channel coherence time, during which we deem the channel invariant, ranges from 1 to 10 seconds, and is typically no less than 2 seconds. The time frame duration is set smaller than the coherence time. Without loss of generality, let us assume that the time frame is 2 seconds. Taking the MEC network with N = 30 as an example, the total execution latency of DROO is 0.059 second, accounting for 3% of the time frame, which is an acceptable overhead for field deployment. In fact, DROO can be further improved by only generating offloading actions at the beginning of the time frame and then training DNN during the remaining time frame in parallel with energy transfer, task offloading and computation. In comparison, the execution of LR algorithm consumes 40% of the time frame, and the CD algorithm even requires longer execution time than the time frame, which are evidently unacceptable in practical implementation. Therefore, DROO makes real-time offloading and resource allocation truly viable for wireless powered MEC networks in fading environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have proposed a deep reinforcement learning-based online offloading algorithm, DROO, to maximize the weighted sum computation rate in wireless powered MEC networks with binary computation offloading. The algorithm learns from the past offloading experiences to improve its offloading action generated by a DNN via reinforcement learning. An order-preserving quantization and an adaptive parameter setting method are devised to achieve fast algorithm convergence. Compared to the conventional optimization methods, the proposed DROO algorithm completely removes the need of solving hard mixed integer programming problems. Simulation results show that DROO achieves similar near-optimal performance as existing benchmark methods but reduces the CPU execution latency by more than an order of magnitude, making realtime system optimization truly viable for wireless powered MEC networks in fading environment.</p><p>Despite that the resource allocation subproblem is solved under a specific wireless powered network setup, the proposed DROO framework is applicable for computation offloading in general MEC networks. A major challenge, however, is that the mobility of the WDs would cause DROO harder to converge.</p><p>As a concluding remark, we expect that the proposed framework can also be extended to solve MIP problems for various applications in wireless communications and networks that involve in coupled integer decision and continuous resource allocation problems, e.g., mode selection in D2D communications, user-to-base-station association in cellular systems, routing in wireless sensor networks, and caching placement in wireless networks. The proposed DROO framework is applicable as long as the resource allocation subproblems can be efficiently solved to evaluate the quality of the given integer decision variables. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An example of the considered wireless powered MEC network and system time allocation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The index k * t of the best offloading actions x * t for DROO algorithm when the number of WDs is N = 10 and K = N . The detailed simulation setups are presented in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2) meters, i = 1, • • • , N . Due to the page limit, the exact values of d i 's are omitted. The average channel gain hi follows the free- space path loss model hi = A d 3•10 8 4πfcdi de , where A d = 4.11 denotes the antenna gain, f c = 915 MHz denotes the carrier frequency, and d e = 2.8 denotes the path loss exponent. The time-varying wireless channel gain of the N WDs at time frame t, denoted by h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Normalized computation rates and training losses for DROO algorithm under fading channels when N = 10 and K = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Normalized computation rates and training losses for DROO algorithm with alternating-weight WDs when N = 10 and K = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Computation rates for DROO algorithm with temporarily new weights when N = 10 and K = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Moving average of Q under different algorithm parameters when N = 10: (a) memory size ; (b) training batch size; (c) training interval; (d) learning rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Moving average of Q for DROO algorithm with different updating interval ∆ for setting an adaptive K. Here, we set N = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Dynamics of Kt under different updating interval ∆ when N = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Notations used throughout the paper</figDesc><table><row><cell cols="2">Notation Description</cell></row><row><cell>N</cell><cell>The number of WDs</cell></row></table><note><p><p>T</p>The length of a time frame i Index of the i-th WD h i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>12 end 13 end 4.4 Adaptive Setting of</head><label></label><figDesc>An online DROO algorithm to solve the offloading decision problem. Initialize the DNN with random parameters θ 1 and empty memory; 2 Set iteration number M and the training interval δ; 3 for t = 1, 2, . . . , M do Generate a relaxed offloading action xt = f θt (h t ); Quantize xt into K binary actions {x k } = g K (x t ); (h t , x k ) for all {x k } by solving (P2); This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2019.2928811, IEEE Transactions on Mobile Computing</figDesc><table><row><cell></cell><cell>input : Wireless channel gain h t at each time frame t,</cell></row><row><cell></cell><cell>the number of quantized actions K</cell></row><row><cell></cell><cell>output: Offloading action x  *  t , and the corresponding</cell></row><row><cell></cell><cell>optimal resource allocation for each time</cell></row><row><cell></cell><cell>frame t;</cell></row><row><cell>1 5</cell><cell></cell></row><row><cell cols="2">7 Q  9 Select the best action x  *  t = arg max {x k } if t mod δ = 0 then</cell></row><row><cell>10</cell><cell>Uniformly sample a batch of data set</cell></row><row><cell>11</cell><cell>{(h τ , x  *  τ ) | τ ∈ T t } from the memory; Train the DNN with {(h τ , x  *  τ ) | τ ∈ T t } and</cell></row><row><cell></cell><cell>update θ t using the Adam algorithm;</cell></row></table><note><p>4 6 Compute Q * * (h t , x k ); 8 Update the memory by adding (h t , x * t ); K Compared to the conventional optimization algorithms, the DROO algorithm has the advantage in removing the need 1536-1233 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1.0 and set training interval δ = 10, training batch size |T | = 128, memory size as 1024, and This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2019.2928811, IEEE Transactions on Mobile Computing</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">t ) Training Loss L(θ π</cell><cell>0.1 0.2 0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell><cell>5000</cell><cell>6000</cell><cell>7000</cell><cell>8000</cell><cell>9000</cell><cell>10000</cell></row><row><cell>Q</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Normalized Computation Rate</cell><cell cols="2">0.8 0.85 0.9 0.95 1</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell cols="3">Time Frame t 4000 5000 6000</cell><cell>7000</cell><cell>8000</cell><cell>9000</cell><cell>10000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2. See</cell><cell>detailed</cell><cell>product</cell><cell>specifications</cell><cell>at</cell><cell>http://</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>www.powercastco.com.</cell></row></table><note><p>1536-1233 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>), 1536-1233 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2019.2928811, IEEE Transactions on Mobile Computing</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9</cell></row><row><cell></cell><cell>Relative Computation Rate</cell><cell>1 2 3 5 6 4</cell><cell>WD WD</cell><cell>2 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell><cell>5000</cell><cell></cell><cell>6000</cell><cell>7000</cell><cell>8000</cell><cell>9000</cell><cell>10000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Double</cell><cell cols="2">Triple</cell><cell></cell><cell cols="2">Reset both</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WD 2 's weight</cell><cell cols="3">WD 1 's weight</cell><cell cols="2">WDs' weights</cell></row><row><cell>Normalized Computation Rate Q</cell><cell cols="2">0.8 0.85 0.9 1</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell><cell cols="2">Time Frame t 5000</cell><cell>6000</cell><cell>7000</cell><cell>8000</cell><cell>9000</cell><cell>10000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2019.2928811, IEEE Transactions on Mobile Computing</figDesc><table /><note><p><p>The binary offloading decision variable x i conditioned on (4d) is relaxed to a real number between 0 and 1, as xi ∈ [0, 1].</p>1536-1233 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2</head><label>2</label><figDesc>is averaged over 30,000 independent wireless channel realizations including both offloading action generation and DNN training. Overall, the training of DNN contributes only a small proportion of CPU execution latency, which is much smaller than that of the bi-section search algorithm DRLOO LR Fig. 15. Boxplot of the normalized computation rate Q for DROO and LR algorithms under different number of WDs. The central mark (in red) indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. for resource allocation. Taking DROO with K = 10 as an example, it uses 0.034 second to generate an offloading action and uses 0.002 second to train the DNN in each time frame. Here training DNN is efficient. During each offloading policy update, only a small batch of training data samples, |T | = 128, are used to train a two-hiddenlayer DNN with only 200 hidden neurons in total via backpropagation. We see from Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 2</head><label>2</label><figDesc>= N ) (Adaptive K with ∆ = 32)</figDesc><table><row><cell></cell><cell cols="2">Comparisons of CPU execution latency</cell><cell></cell><cell></cell></row><row><cell cols="2"># of WDs (Fixed K 10 DROO 3.6e-2s</cell><cell>DROO 1.2e-2s</cell><cell cols="2">CD 2.0e-1s 2.4e-1s LR</cell></row><row><cell>20</cell><cell>1.3e-1s</cell><cell>3.0e-2s</cell><cell>1.3s</cell><cell>5.3e-1s</cell></row><row><cell>30</cell><cell>3.1e-1s</cell><cell>5.9e-2s</cell><cell>3.8s</cell><cell>8.1e-1s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The channel reciprocity assumption is made to simplify the notations of channel state. However, the results of this paper can be easily extended to the case with unequal uplink and downlink channels.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>This work is supported in part by the National Natural Science Foundation of China (Project 61871271), the Zhejiang Provincial Natural Science Foundation of China (Project LY19F020033), the Guangdong Province Pearl River Scholar Funding Scheme 2018, the Department of Education of Guangdong Province (Project 2017KTSCX163), the Foundation of Shenzhen City (Project JCYJ20170818101824392), and the Science and Technology Innovation Commission of Shenzhen (Project 827/000212), and General Research Funding (Project number 14209414, 14208107) from the Research Grants Council of Hong Kong.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>M'16) received the B.Eng. degree in communications engineering from Zhejiang University, Hangzhou, China, in 2009, and the Ph.D. degree in information engineering from</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Suzhi Bi (S'10-M'14-SM <ref type="bibr">'19)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wireless powered communication: Opportunities and challenges</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="117" to="125" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fog and IoT: An overview of research opportunities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="854" to="864" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic computation offloading for mobile-edge computing with energy harvesting devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3590" to="3605" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Energy-efficient resource allocation for mobile-edge computation offloading</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1397" to="1411" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient multi-user computation offloading for mobile-edge cloud computing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2795" to="2808" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint offloading and computing optimization in wireless powered mobile-edge computing systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1784" to="1797" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computation rate maximization for wireless powered mobile-edge computing with binary computation offloading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4177" to="4190" />
			<date type="published" when="2018-06">Jun. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on mobile edge computing: The communication perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surveys Tuts</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2322" to="2358" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Energy efficient mobile cloud computing powered by wireless energy transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A branch and bound algorithm for feature subset selection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="917" to="922" />
			<date type="published" when="1977-09">Sep. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dynamic programming and optimal control</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Athena Scientific Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Joint task offloading and resource allocation for multi-server mobile-edge computing networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pompili</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00704</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Energy-efficient dynamic offloading and resource scheduling in mobile cloud computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2016-04">Apr. 2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Offloading in mobile edge computing: Task allocation and computational frequency scaling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3571" to="3584" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning in large discrete action spaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coppin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07679</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Softwaredefined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed Deep Learning-based Offloading for Mobile Edge Computing Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11036-018-1177-x</idno>
	</analytic>
	<monogr>
		<title level="j">Mobile Netw. Appl</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning-based computation offloading for IoT devices with energy harvesting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Veh. Technol</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1930" to="1941" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance optimization in mobile-edge computing via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning-based joint task offloading and bandwidth allocation for multi-user mobile edge computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Communications and Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Measurement, characterization and modeling of indoor 800/900 MHz radio channels for digital communications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bultitude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5" to="12" />
			<date type="published" when="1987-06">Jun. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Doppler spread measurements of indoor radio channel</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pahlavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="109" />
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Characterizing the spectral properties and time variation of the in-vehicle wireless communication channel</title>
		<author>
			<persName><forename type="first">S</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rigelsford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2390" to="2399" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to optimize: Training deep neural networks for wireless resource management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE SPAWC</title>
		<meeting>IEEE SPAWC</meeting>
		<imprint>
			<date type="published" when="2017-07">Jul. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Power of deep learning for channel estimation and signal detection in OFDM systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="117" />
			<date type="published" when="2018-02">Feb 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Machine learning: an algorithmic perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marsland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mobile-edge computing: Partial computation offloading using dynamic voltage scaling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4268" to="4282" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">Reinforcement learning: An introduction</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
