<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVERSARIAL ATTACKS ON GRAPH NEURAL NETWORKS VIA META LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">Z</forename><surname>Ügner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Stephan</forename><forename type="middle">G</forename><surname>Ünnemann</surname></persName>
							<email>guennemann@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADVERSARIAL ATTACKS ON GRAPH NEURAL NETWORKS VIA META LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphs are a powerful representation that can model diverse data from virtually any domain, such as biology (protein interaction networks), chemistry (molecules), or social networks (Facebook). Not surprisingly, machine learning on graph data has a longstanding history, with tasks ranging from node classification, over community detection, to generative modeling.</p><p>In this paper, we study node classification, which is an instance of semi-supervised classification: given a single (attributed) network and a subset of nodes whose class labels are known (e.g., the topic of a paper in a citation graph), the goal is to infer the classes of the unlabeled nodes. While there exist many classical approaches to node classification <ref type="bibr" target="#b15">(London &amp; Getoor, 2014;</ref><ref type="bibr" target="#b8">Chapelle et al., 2006)</ref>, recently deep learning on graphs has gained much attention <ref type="bibr" target="#b17">(Monti et al., 2017;</ref><ref type="bibr" target="#b4">Bojchevski &amp; Günnemann, 2018a;</ref><ref type="bibr" target="#b2">Battaglia et al., 2018;</ref><ref type="bibr" target="#b21">Perozzi et al., 2014;</ref><ref type="bibr">Bojchevski et al., 2018;</ref><ref type="bibr" target="#b14">Klicpera et al., 2019)</ref>. Specifically, graph convolutional approaches <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b22">Pham et al., 2017)</ref> have improved the state of the art in node classification.</p><p>However, recent works have also shown that such approaches are vulnerable to adversarial attacks both at test time (evasion) as well as training time (poisoning attacks) <ref type="bibr" target="#b28">(Zügner et al., 2018;</ref><ref type="bibr" target="#b10">Dai et al., 2018)</ref>. A core strength of models using graph convolution -exploiting the information in a node's neighborhood to improve classification -is also a major vulnerability: because of these propagation effects, an attacker can change a single node's prediction without even changing any of its attributes or edges. This is because the foundational assumption that all samples are independent of each other does not hold for node classification. Network effects such as homophily <ref type="bibr" target="#b15">(London &amp; Getoor, 2014)</ref> support the classification, while on the other hand they enable indirect adversarial attacks.</p><p>So far, all existing attacks on node classification models are targeted, that is, aim to provoke misclassification of a specific single node, e.g. a person in a social network. In this work, we propose the first algorithm for poisoning attacks that is able to compromise the global node classification performance of a model. We show that even under restrictive attack settings and without access to the target classifier, our attacks can render it near-useless for use in production (i.e., on test data).</p><p>Our approach is based on the principle of meta learning, which has traditionally been used for hyperparameter optimization <ref type="bibr" target="#b3">(Bengio, 2000)</ref>, or, more recently, few-shot learning <ref type="bibr" target="#b11">(Finn et al., 2017)</ref>. In essence, we turn the gradient-based optimization procedure of deep learning models upside down and treat the input data -the graph at hand -as a hyperparameter to learn.</p><p>Adversarial attacks on machine learning models have been studied both in the machine learning and security community and for many different model types <ref type="bibr">(Mei &amp; Zhu, 2015)</ref>. It is important to distinguish attacks from outliers; while the latter naturally occur in graphs <ref type="bibr">(Bojchevski &amp; Günnemann, 2018)</ref>, adversarial examples are deliberately created with the goal to mislead machine learning models and often designed to be unnoticeable. Deep neural networks are highly sensitive to these small adversarial perturbations to the data <ref type="bibr" target="#b25">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b12">Goodfellow et al., 2015)</ref>. The vast majority of attacks and defenses assume the data instances to be independent and continuous. This assumption clearly does not hold for node classification and many other tasks on graphs.</p><p>Works on adversarial attacks for graph learning tasks are generally sparse. <ref type="bibr" target="#b9">Chen et al. (2017)</ref> have measured the changes in the resulting graph clustering when injecting noise to a bi-partite graph that represent DNS queries. However, their focus is not on generating attacks in a principled way. <ref type="bibr" target="#b27">Torkamani &amp; Lowd (2013)</ref> consider adversarial noise in the node features in order to improve robustness of collective classification via associative Markov networks.</p><p>Only recently researchers have started to study adversarial attacks on deep learning for graphs. <ref type="bibr" target="#b10">Dai et al. (2018)</ref> consider test-time (i.e., evasion) attacks on graph classification (i.e., classification of graphs themselves) and node classification. However, they do not consider poisoning (i.e., trainingtime) attacks or evaluate transferability of their attacks, and restrict the attacks to edge deletions only. Moreover, they focus on targeted attacks, i.e. attacks designed to change the prediction of a single node. <ref type="bibr" target="#b28">Zügner et al. (2018)</ref> consider both test-time and training-time attacks on node classification models. They circumvent explicitly tackling the bilevel optimization problem underlying poisoning attacks by performing their attacks based on a (static) surrogate model and evaluating their impact by training a classifier on the data modified by their algorithm. In contrast to <ref type="bibr" target="#b10">Dai et al. (2018)</ref>, their attacks can both insert and remove edges, as well as modify node attributes in the form of binary vectors. Again, their algorithm is suited only to targeted attacks on single nodes; the problem of training-time attacks on the overall performance of node classification models remains unexplored. <ref type="bibr" target="#b5">Bojchevski &amp; Günnemann (2018b)</ref> propose poisoning attacks on a different task: unsupervised node representation learning (or node embeddings). They exploit perturbation theory to maximize the loss obtained after training DeepWalk. In this work, we focus on semi-supervised learning.</p><p>Meta-learning <ref type="bibr" target="#b26">(Thrun &amp; Pratt, 1998;</ref><ref type="bibr" target="#b19">Naik &amp; Mammone, 1992)</ref>, or learning to learn, is the task of optimizing the learning algorithm itself; e.g., by optimizing the hyperparameters <ref type="bibr" target="#b3">Bengio (2000)</ref>, learning to update the parameters of a neural network <ref type="bibr" target="#b23">(Schmidhuber, 1992;</ref><ref type="bibr">Bengio et al., 1992)</ref>, or the activation function of a model <ref type="bibr" target="#b1">(Agostinelli et al., 2014)</ref>. Gradient-based hyperparameter optimization works by differentiating the training phase of a model to obtain the gradients w.r.t. the hyperparameters to optimize.</p><p>The key idea of this work is to use meta-learning for the opposite: modifying the training data to worsen the performance after training (i.e., training-time or poisoning attacks). <ref type="bibr" target="#b18">Muñoz-González et al. (2017)</ref> demonstrate that meta learning can indeed be used to create training-time attacks on simple, linear classification models. On continuous data, they report little success when attacking deep neural networks, and on discrete datasets, they do not consider deep learning models or problems with more than two classes. Like most works on adversarial attacks, they assume the data instances to be independent. In this work, for the first time, we propose an algorithm for global attacks on (deep) node classification models at training time. In contrast to <ref type="bibr" target="#b28">Zügner et al. (2018)</ref>, we explicitly tackle the bilevel optimization problem of poisoning attacks using meta learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>We consider the task of (semi-supervised) node classification. Given a single (attributed) graph and a set of labeled nodes, the goal is to infer the class labels of the unlabeled nodes. Formally, let G = (A, X) be an attributed graph with adjacency matrix A ∈ {0, 1} N ×N and node attribute matrix X ∈ R N ×D , where N is the number of nodes and D the dimension of the node feature vectors. W.l.o.g., we assume the node IDs to be V = {1, . . . , N }.</p><p>Given the set of labeled nodes V L ⊆ V, where nodes are assigned exactly one class in C = {c 1 , c 2 , ..., c K }, the goal is to learn a function f θ , which maps each node v ∈ V to exactly one of the K classes in C (or in a probabilistic formulation: to the K-simplex). Note that this is an instance of transductive learning, since all test samples (i.e., the unlabeled nodes) as well as their attributes and edges (but not their class labels!) are known and used during training <ref type="bibr" target="#b8">(Chapelle et al., 2006)</ref>. The parameters θ of the function f θ are generally learned by minimizing a loss function L train (e.g. cross-entropy) on the labeled training nodes:</p><formula xml:id="formula_0">θ * = arg min θ L train (f θ (G)),<label>(1)</label></formula><p>where we overload the notation of f θ to indicate that we feed in the whole graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ATTACK MODEL</head><p>Adversarial attacks are small deliberate perturbations of data samples in order to achieve the outcome desired by the attacker when applied to the machine learning model at hand. The attacker is constrained in the knowledge they have about the data and the model they attack, as well as the adversarial perturbations they can perform.</p><p>Attacker's goal. In our work, the attacker's goal is to increase the misclassification rate (i.e., one minus the accuracy) of a node classification algorithm achieved after training on the data (i.e., graph) modified by our algorithm. In contrast to <ref type="bibr" target="#b28">Zügner et al. (2018)</ref> and <ref type="bibr" target="#b10">Dai et al. (2018)</ref>, our algorithm is designed for global attacks reducing the overall classification performance of a model. That is, the goal is to have the test samples classified as any class different from the true class.</p><p>Attacker's knowledge. The attacker can have different levels of knowledge about the training data, i.e. the graph G, the target machine learning model M, and the trained model parameters θ. In our work, we focus on limited-knowledge attacks where the attacker has no knowledge about the classification model and its trained weights, but the same knowledge about the data as the classifier.</p><p>In other words, the attacker can observe all nodes' attributes, the graph structure, as well as the labels of the subset V L and uses a surrogate model to modify the data. Besides assuming knowledge about the full data, we also perform experiments where only a subset of the data is given. Afterwards, this modified data is used to train deep neural networks to degrade their performance.</p><p>Attacker's capability. In order to be effective and remain undiscovered, adversarial attacks should be unnoticeable. To account for this, we largely follow <ref type="bibr" target="#b28">Zügner et al. (2018)</ref>'s attacker capabilities. First, we impose a budget constraint ∆ on the attacks, i.e. limit the number of changes A − Â 0 ≤ ∆ (here we have 2∆ since we assume the graph to be symmetric). Furthermore, we make sure that no node becomes disconnected (i.e. a singleton) during the attack. One of the most fundamental properties of a graph is its degree distribution. Any significant changes to it are very likely to be noticed; to prevent such large changes to the degree distribution, we employ <ref type="bibr" target="#b28">Zügner et al. (2018)</ref>'s unnoticeability constraint on the degree distribution. Essentially, it ensures that the graph's degree distribution can only marginally be modified by the attacker. The authors also derive an efficient way to check for violations of the constraint so that it adds only minimal computational overhead to the attacks. While in this work we focus on changing the graph structure only, our algorithm can easily be modified to change the node features as well. We summarize all these constraints and denote the set of admissible perturbations on the data as Φ(G), where G is the graph at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OVERALL GOAL</head><p>Poisoning attacks can be mathematically formulated as a bilevel optimization problem:</p><formula xml:id="formula_1">min Ĝ∈Φ(G) L atk (f θ * ( Ĝ)) s.t. θ * = arg min θ L train (f θ ( Ĝ)). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>L atk is the loss function the attacker aims to optimize. In our case of global and unspecific (regarding the type of misclassification) attacks, the attacker tries to decrease the generalization performance of the model on the unlabeled nodes. Since the test data's labels are not available, we cannot directly optimize this loss. One way to approach this is to maximize the loss on the labeled (training) nodes L train , arguing that if a model has a high training error, it is very likely to also generalize poorly (the opposite is not true; when overfitting on the training data, a high generalization loss can correspond to a low training loss). Thus, our first attack option is to choose L atk = −L train .</p><p>Recall that semi-supervised node classification is an instance of transductive learning: all data samples (i.e., nodes) and their attributes are known at training time (but not all labels!). We can use this insight to obtain a second variant of L atk . The attacker can learn a model on the labeled data to estimate the labels ĈU of the unlabeled nodes V U = V\V L . The attacker can now perform selflearning, i.e. use these predicted labels and compute the loss of a model on the unlabeled nodes, yielding our second option L atk = −L self where L self = L(V U , ĈU ). Note that, at all times, only the labels of the labeled nodes are used for training; L self is only used to estimate the generalization loss after training. In our experimental evaluation, we compare both versions of L atk outlined above.</p><p>Importantly, notice the bilevel nature of the problem formulation in Eq. ( <ref type="formula" target="#formula_1">2</ref>): the attacker aims to maximize the classification loss achieved after optimizing the model parameters on the modified (poisoned) graph Ĝ. Optimizing such a bilevel problem is highly challenging by itself. Even worse, in our graph setting the data and the action space of the attacker are discrete: the graph structure is A = {0, 1} N ×N , and the possible actions are edge insertions and deletions. This makes the problem even more difficult in two ways. First, the action space is vast; given a budget of ∆ perturbations, the number of possible attacks is, ignoring symmetry, N 2 ∆ and thus in O(N 2∆ ); exhaustive search is clearly infeasible. Second, a discrete data domain means that we cannot use gradient-based methods such as gradient descent to make small (real-valued) updates on the data to optimize a loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPH STRUCTURE POISONING</head><formula xml:id="formula_3">VIA META-LEARNING 4.1 POISONING VIA META-GRADIENTS</formula><p>In this work, we tackle the bilevel problem described in Eq. ( <ref type="formula" target="#formula_1">2</ref>) using meta-gradients, which have traditionally been used in meta-learning. The field of meta-learning (or learning to learn) tries to make the process of learning machine learning models more time and/or data efficient, e.g. by finding suitable hyperparameter configurations <ref type="bibr" target="#b3">(Bengio, 2000)</ref> or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning <ref type="bibr" target="#b11">(Finn et al., 2017)</ref>.</p><p>Meta-gradients (e.g., gradients w.r.t. hyperparameters) are obtained by backpropagating through the learning phase of a differentiable model (typically a neural network). The core idea behind our adversarial attack algorithm is to treat the graph structure matrix as a hyperparameter and compute the gradient of the attacker's loss after training with respect to it:</p><formula xml:id="formula_4">∇ meta G := ∇ G L atk (f θ * (G)) s.t. θ * = opt θ (L train (f θ (G))),<label>(3)</label></formula><p>where opt(•) is a differentiable optimization procedure (e.g. gradient descent and its stochastic variants) and L train the training loss. Notice the similarity of the meta-gradient to the bi-level formulation in Eq. ( <ref type="formula" target="#formula_1">2</ref>); the meta-gradient indicates how the attacker loss L atk after training will change for small perturbations on the data, which is exactly what a poisoning attacker needs to know.</p><p>As an illustration, consider an example where we instantiate opt with vanilla gradient descent with learning rate α starting from some intial parameters θ 0 θ t+1 = θ t − α∇ θt L train (f θt (G)) (4) The attacker's loss after training for T steps is L atk (f θ T (G)). The meta-gradient can be expressed by unrolling the training procedure:</p><formula xml:id="formula_5">∇ meta G = ∇ G L atk (f θ T (G)) = ∇ f L atk (f θ T (G)) • [∇ G f θ T (G) + ∇ θ T f θ T (G) • ∇ G θ T ] , where (5) ∇ G θ t+1 = ∇ G θ t − α∇ G ∇ θt L train (f θt (G))</formula><p>Note that the parameters θ t itself depend on the graph G (see Eq. 4); they are not fixed. Thus, the derivative w.r.t. the graph has to be taken into account, chaining back until θ 0 . Given this, the attacker can use the meta-gradient to perform a meta update M on the data to minimize L atk :</p><formula xml:id="formula_6">G (k+1) ← M (G (k) ) (6)</formula><p>The final poisoned data G (∆) is obtained after performing ∆ meta updates. A straightforward way to instantiate M is (meta) gradient descent with some step size β:</p><formula xml:id="formula_7">M (G) = G − β∇ G L atk (f θ T (G))).</formula><p>It has to be noted that such a gradient-based update rule is neither possible nor well-suited for problems with discrete data (such as graphs). Due to the discreteness, the gradients are not defined. Thus, in our approach we simply relax the data's discreteness condition. However, we still perform discrete updates (actions) since the above simple gradient update would lead to dense (and continuous) adjacency matrices; not desired and not efficient to handle. Thus, in the following section, we propose a greedy approach to preserve the data's sparsity and discreteness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GREEDY POISONING ATTACKS VIA META GRADIENTS</head><p>We assume that the attacker does not have access to the target classifier's parameters, outputs, or even knowledge about its architecture; the attacker thus uses a surrogate model to perform the poisoning attacks. Afterwards the poisoned data is used to train deep learning models for node classification (e.g. a GCN) to evaluate the performance degradation due to the attack. We use the same surrogate model as <ref type="bibr" target="#b28">Zügner et al. (2018)</ref>, which is a linearized two-layer graph convolutional network:</p><formula xml:id="formula_8">f θ (A, X) = softmax( Â2 XW ),<label>(7)</label></formula><p>where Â = D −1/2 ÃD −1/2 , Ã = A + I, A is the adjacency matrix, X are the node features, D the diagonal matrix of the node degrees, and θ = {W } the set of learnable parameters. In contrast to <ref type="bibr" target="#b28">Zügner et al. (2018)</ref> we do not linearize the output (softmax) layer.</p><p>Note that we only perform changes to the graph structure A, hence we treat the node attributes X as a constant during our attacks. For clarity, we replace G with A in the meta gradient formulation.</p><p>We define a score function S : V × V → R that assigns each possible action a numerical value indicating its (estimated) impact on the attacker objective L atk . Given the meta-gradient for a node pair (u, v), we define S(u, v) = ∇ meta auv • (−2 • a uv + 1) where a uv is the entry at position (u, v) in the adjacency matrix A. We essentially flip the sign of the meta-gradients for connected node pairs as this yields the gradient for a change in the negative direction (i.e., removing the edge).</p><p>We greedily pick the perturbation e = (u , v ) with the highest score one at a time e = arg max</p><formula xml:id="formula_9">e=(u,v): M (A,e)∈Φ(G) S(u, v),<label>(8)</label></formula><p>where M (A, e) ∈ Φ(G) ensures that we only perform changes compliant with our attack constraints (e.g., unnoticeability). The meta update function M (A, e) inserts the edge e = (i, j) by setting a ij = 1 if nodes (i, j) are currently not connected and otherwise deletes the edge by setting a ij = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">APPROXIMATING META-GRADIENTS</head><p>Computing the meta gradients is expensive both from a computational and a memory point-of-view.</p><p>To alleviate this issue, <ref type="bibr" target="#b11">Finn et al. (2017)</ref> propose a first-order approximation, leading to</p><formula xml:id="formula_10">∇ meta A = ∇ A L atk (f θ T (A)) ≈ ∇ A L atk (f θT (A)) = ∇ f L atk (f θT (A)) • ∇ A f θT (A).<label>(9)</label></formula><p>We denote by θt the parameters at time t independent of the data A (and θt−1 ), i.e. ∇ A θt = 0; the gradient is thus not propagated through θt . This corresponds to taking the gradient of the attack loss L atk w.r.t. the data, after training the model for T steps. We compare against this baseline in our experiments; as also done in <ref type="bibr" target="#b28">Zügner et al. (2018)</ref>. However, unlike the meta-gradient, this approximation completely disregards the training dynamics. <ref type="bibr" target="#b20">Nichol &amp; Schulman (2018)</ref> propose a heuristic of the meta gradient in which they update the initial weights θ 0 on a line towards the local optimum θ T to achieve faster convergence in a multi-task learning setting:</p><formula xml:id="formula_11">∇ meta θ0 ≈ T t=1 ∇ θt L train (f θt (A; X)).</formula><p>Again, they assume θt to be independent of θt−1 . While there is no direct connection to the formulation of the meta gradient in Eq. ( <ref type="formula">5</ref>), there is an intuition behind it: the heuristic meta gradient is the direction, in which, on average, we have observed the strongest increase in the training loss during the training procedure. The authors' experimental evaluation further indicates that this heuristic achieves similar results as the meta gradient while being much more efficient to compute (see Appendix C for a discussion on complexity).</p><p>Adapted to our adversarial attack setting on graphs, we get</p><formula xml:id="formula_12">∇ meta A ≈ T t=1 ∇ A L train (f θt (A; X)).</formula><p>We can view this as a heuristic of the meta gradient when L atk = −L train . Likewise, again taking the transductive learning setting into account, we can use self-learning to estimate the loss on the unlabeled nodes, replacing L train by L self . Indeed, we combine these two views</p><formula xml:id="formula_13">∇ meta A ≈ Σ T t=1 λ∇ A L train (f θt (A; X)) + (1 − λ)∇ A L self (f θt (A; X)),<label>(10)</label></formula><p>where λ can be used to weight the two objectives. This approximation has a much smaller memory footprint than the exact meta gradient since we don't have to store the whole training trajectory θ1 , . . . , θT in memory; additionally, there there are no second-order derivatives to be computed. A summary of our algorithm can be found in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Setup. We evaluate our approach on the well-known CITESEER <ref type="bibr" target="#b24">(Sen et al., 2008)</ref>, CORA-ML <ref type="bibr" target="#b16">(McCallum et al., 2000)</ref>, and POLBLOGS (Adamic &amp; Glance, 2005) datasets; an overview is given in Table <ref type="table" target="#tab_5">6</ref>. We split the datasets into labeled (10%) and unlabeled (90%) nodes. The labels of the unlabeled nodes are never visible to the attacker or the classifiers and are only used to evaluate the generalization performance of the models. Our code is available at https://www.kdd.in. tum.de/gnn-meta-attack.</p><p>We evaluate the transferability of adversarial attacks by training deep node classification models on the modified (poisoned) data. For this purpose, we use Graph Convolutional Networks (GCN) (Kipf &amp; Welling, 2017) and Column Networks (CLN) <ref type="bibr" target="#b22">(Pham et al., 2017)</ref>. Both are models utilizing the message passing framework (a.k.a. graph convolution) and trained in a semi-supervised way.</p><p>We further evaluate the node classification performance achieved by training a standard logistic regression model on the node embeddings learned by DeepWalk <ref type="bibr" target="#b21">(Perozzi et al., 2014)</ref>. DeepWalk itself is trained in an unsupervised way and without node attributes or graph convolutions; thus, this is arguably an even more difficult transfer task.</p><p>We repeat all of our attacks on five different splits of labeled/unlabeled nodes and train all target classifiers ten times per attack (using the split that was used to create the attack). In our tables, the uncertainty indicates 95 % confidence intervals of the mean obtained via bootstrapping. For our meta-gradient approaches, we compute the meta-gradient ∇ meta A L atk (f θ T (A; X)) by using gradient descent with momentum for 100 iterations. We refer to our meta-gradient approach with self-training as Meta-Self and to the variant without self-training as Meta-Train. Similarly, we refer to our approximations as A-Meta-Self (with λ=0), A-Meta-Train (λ=1), and A-Meta-Both (λ=0.5).</p><p>Comparing meta-gradient heuristics. First, we analyze the different meta gradient heuristics described in Section 4.3. The results can be seen in Table <ref type="table" target="#tab_0">1</ref>. All principles successfully increase the misclassification rate (i.e., 1 − accuracy on unlabeled nodes) obtained on the test data, compared to the results obtained with the unperturbed graph. Since A-Meta-Self consistently shows a weaker performance than A-Meta-Both, we do not further consider A-Meta-Self in the following.</p><p>Comparison with competing methods. We compare our meta-gradient approach as well as its approximations with various baselines and Nettack <ref type="bibr" target="#b28">(Zügner et al., 2018)</ref>. DICE ('delete internally, connect externally') is a baseline where, for each perturbation, we randomly choose whether to insert or remove an edge. Edges are only removed between nodes from the same class, and only inserted between nodes from different classes. This baseline has all true class labels (train and test) available and thus more knowledge than all competing methods. First-order refers to the approximation proposed by <ref type="bibr" target="#b11">Finn et al. (2017)</ref>, i.e. ignoring all second-order derivatives. Note that Nettack is not designed for global attacks. In order to be able to compare to them, for each perturbation we ran-  domly select one target node from the unlabeled nodes and attack it using Nettack while considering all nodes in the network. In this case, its time and memory complexity is O(N 3 ) and thus it was not feasible to run it on any but the sparsest dataset. Meta w/ Oracle corresponds to our meta-gradient approach when supplied with all true class labels on the test data -this only serves as a reference point since it cannot be carried out in real scenarios where the test nodes' labels are unknown. For all methods, we enforce the unnoticeability constraint introduced by Zügner et al. ( <ref type="formula">2018</ref>), which ensures that the graph's degree distribution changes only slightly. In Appendix D we show that the unnoticeability constraint does not significantly limit the impact of our attacks.</p><p>In Table <ref type="table" target="#tab_1">2</ref> we see the misclassification rates (i.e., 1 -accuracy on unlabeled nodes) achieved by changing 5% of E LCC edges according to the different methods (larger is better, except for the average rank). That is, each method is allowed to modify 5% of E LCC , i.e. the number of edges present in the graph before the attack. We present similar tables for 1% and 10% changes in Appendix F. Our meta-gradient with self-training (Meta-Self) produces the strongest drop in performance across all models and datasets as indicated by the average rank. Changing only 5% of the edges leads to a relative increase of up to 48% in the misclassification rate of GCN on CORA-ML.</p><p>Remarkably, our memory efficient meta-gradient approximations lead to strong increases in misclassifications as well. They outperform both baselines and are in many cases even on par with the more expensive meta-gradient. In Appendix F, Table <ref type="table" target="#tab_10">11</ref> we also show that using only T = 10 training iterations of the surrogate models for computing the meta gradient (or its approximations) can significantly hurt the performance across models and datasets. Moreover, in Table <ref type="table" target="#tab_7">8</ref> in Appendix F we show that our heuristic is successful at attacking a dataset with roughly 20K nodes.</p><p>While the focus of our work is poisoning attacks by modifying the graph structure, our method can be applied to node feature attacks as well. In Appendix E we show a proof of concept that our attacks are also effective when attacking by perturbing both node features and the graph structure.</p><p>In Fig. <ref type="figure">1</ref> we see the drop in classification performance of GCN on CORA-ML for increasing numbers of edge insertions/deletions (similar plots for the remaining datasets and models are provided in Appendix F). Meta-Self is even able to reduce the classification accuracy below 50%. Fig. <ref type="figure" target="#fig_0">2</ref> shows the classification accuracy of GCN and CLN as well as a baseline operating on the node attributes only, i.e. ignoring the graph. Not surprisingly, deep models achieve higher accuracy than the baseline when trained on the clean CITESEER graph -exploiting the network information improves classification. However, by only perturbing 5% of the edges, we obtain the opposite: GCN and CLN perform worse than the baseline -the graph structure now hurts classification.</p><p>Impact of graph structure and trained weights. Another interesting property of our attacks can be seen in Table <ref type="table" target="#tab_2">3</ref>, where W and Ŵ correspond to the weights trained on the clean CORA-ML network A and a version Â poisoned by our algorithm (here with even 25 % modified edges), respectively.</p><p>Note that the classification accuracy barely changes when modifying the underlying network for a given set of trained weights; even when applying the clean weights W on the highly corrupted Â, the performance drops only marginally. Likewise, even the clean graph A only leads to a low accuracy when using it with the weights Ŵ . This result emphasizes the importance of the training procedure for the performance of graph models and shows that our poisoning attack works by derailing the training procedure from the start, i.e. leading to 'bad' weights.  In Fig. <ref type="figure" target="#fig_1">3</ref> we compare edges inserted by our meta-gradient approach to the edges originally present in the CORA-ML network. Fig. <ref type="figure" target="#fig_1">3</ref> (a) shows the shortest path lengths between nodes pairs before being connected by adversarially inserted edges vs. shortest path lengths between all nodes in the original graph. In Fig. <ref type="figure" target="#fig_1">3</ref> (b) we compare the edge betweenness centrality (C E ) of adversarially inserted edges to the centrality of edges present in the original graph. In (c) we see the node degree distributions of the original graph and the node degrees of the nodes that are picked for adversarial edges. For all three measures no clear distinction can be made. There is a slight tendency for the algorithm to connect nodes that have higher-than-average shortest paths and low degrees, though.</p><p>As we can see in Table <ref type="table" target="#tab_4">5</ref>, roughly 80% of our meta attack's perturbations are edge insertions (INS).</p><p>As expected by the homophily assumption, in most cases edges inserted connect nodes from different classes and edges deleted connect same-class nodes. However, as the comparison with the DICE baseline shows, this by itself can also not explain the destructive performance of the meta-gradient.</p><p>Limited knowledge about the graph structure. In the experiments described above, the attacker has full knowledge about the graph structure and all node attributes (as typical in a transductive setting). We also tested our algorithm on a sub-graph of CORA-ML and CITESEER. That is, we select the 10% labeled nodes and randomly select neighbors of these until we have a subgraph with number of nodes n = 0.3N . We run our attacks on this small subgraph, and afterwards plug in the perturbations into the original graphs to train GCN and CLN as before. Table <ref type="table" target="#tab_3">4</ref> summarizes the results: Even in this highly restricted setting, our attacks consistently increase misclassification rate across datasets and models, highlighting the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose an algorithm for training-time adversarial attacks on (attributed) graphs, focusing on the task of node classification. We use meta-gradients to solve the bilevel optimization problem underlying the challenging class of poisoning adversarial attacks. Our experiments show that attacks created using our meta-gradient approach consistently lead to a strong decrease in classification performance of graph convolutional models and even transfer to unsupervised models. Remarkably, even small perturbations to a graph based on our approach can lead to graph neural networks performing worse than a baseline ignoring all relational information. We further propose approximations of the metagradients that are less expensive to compute and, in many cases, have a similarly destructive impact on the training of node classification models. While we are able to show small statistical differences of adversarial and 'normal' edges, it is still an open question what makes the edges inserted/removed by our algorithm so destructive, which could then be used to detect or defend against attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET STATISTICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPLEXITY ANALYSIS</head><p>In our attack we handle both edge insertions and deletions, i.e. each element in the adjacency matrix A ∈ {0, 1} N ×N can be changed. This means that without further optimization, the (approximate) meta gradient for each node pair has to be computed, leading to a baseline memory and computational complexity of O(N 2 ). For the meta gradient computation we additionally have to store the entire weight trajectory during training, adding O(T • |θ|) to the memory cost, where T is the number of inner training steps and |θ| the number of weights. Thus, memory complexity of our meta gradient attack is O(N 2 + T • |θ|). The second-order derivatives at each step T in the meta gradient formulation can be computed in O(N 2 ) using Hessian-vector products, leading to a computational complexity of O(T • N 2 ).</p><p>For the meta gradient heuristics, the computational complexity is similar since we have to evaluate the gradient w.r.t. the adjacency matrix at every training step. However, the training trajectory of the weights does not have to be kept in memory, yielding a memory complexity of O(N 2 ). This is highly beneficial, as memory (especially on GPUs) is limited.</p><p>The computational and memory complexity of our adversarial attacks implies that (as-is) it can be executed for graphs with roughly 20K nodes using a commodity GPU. The complexity, however, can be drastically reduced by pre-filtering the elements in the adjacency matrix for which the (meta) gradient needs to be computed, since only a fraction of entries in the adjacency matrix are promising candidate perturbations. We leave such performance optimization for future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D UNNOTICEABILITY CONSTRAINT</head><p>In all our experiments, we enforce the unnoticeability constraint on the degree distribution proposed by <ref type="bibr" target="#b28">(Zügner et al., 2018)</ref>. In Fig. <ref type="figure" target="#fig_2">4</ref> we show that this constraint does not significantly limit the destructive performance of our attacks. Thus we conclude that these constraints should always be enforced, since they improve unnoticeability while at the same time our attacks remain effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ATTACKS WITH CHANGES TO THE NODE FEATURES</head><p>While the focus of our work is poisoning attacks by modifying the graph structure, our method can be applied to node feature attacks as well. The most straightforward case is when the node features are binary, since then we can use the same greedy algorithm as for the graph structure (ignoring the degree distribution constraints). Among the datasets we evaluated, CITESEER has binary node features, hence in Table <ref type="table" target="#tab_6">7</ref> we display the results when attacking both node features and graph structure (while the total number of perturbations stays the same). We can observe that the impact of the combined attacks is slightly lower than the structure-only attack. We attribute this to the fact that we assign the same cost to structure and feature changes, but arguably we expect a structure perturbation to have a stronger effect on performance than a feature perturbation. Future work can provide a framework where structure and feature changes impose a different cost on the attacker. When the node features are continuous, there also needs to be some tuning of the meta step size and considerations whether multiple features per instance can be changed in a single step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ADDITIONAL RESULTS</head><p>In this section we present additional results of our experiments. In Table <ref type="table" target="#tab_7">8</ref> we see that our heuristic is successful at attacking PUBMED, a dataset with roughly 20K nodes. Tables <ref type="table" target="#tab_9">9 and 10</ref> show misclassification rates with 1% and 10% perturbed edges, respectively. Meta with Oracle 16.2 ± 0.3 18.2 ± 0.2 20.5 ± 0.3 30.1 ± 0.5 29.5 ± 0.5 34.8 ± 0.3 13.6 ± 1.1 10.5 ± 1.2 6.0 ± 0.7 3.5 * Did not finish within three days on CORA-ML and POLBLOGS A-Meta-Train 28.1 ± 1.1 23.6 ± 0.4 33.6 ± 0.7 34.3 ± 1.1 31.3 ± 0.6 32.1 ± 0.5 12.8 ± 1.6 18.2 ± 2.6 6.9 ± 0.2 4.9 A-Meta-Both 24.6 ± 1.0 20.0 ± 0.3 34.8 ± 0.6 29.1 ± 0.5 29.2 ± 0.4 33.6 ± 0.4 22.7 ± 0.7 22.3 ± 0.9 26.3 ± 1.0 4.8</p><p>Meta-Train 37.3 ± 1.4 24.9 ± 0.5 34.4 ± 1.6 31.8 ± 1.0 29.9 ± 0.7 36.0 ± 0.2 28.7 ± 3.6 32.9 ± 1.6 73.7 ± 3.9 2.6 Meta-Self 34.5 ± 0.9 22.9 ± 0.6 37.0 ± 1.0 38.6 ± 1.0 35.3 ± 0.7 36.0 ± 1.2 26.1 ± 0.6 23.5 ± 0.9 60.7 ± 2.7 2.8</p><p>Meta with Oracle 34.8 ± 1.5 25.2 ± 0.4 44.0 ± 0.9 40.1 ± 1.2 37.2 ± 0.9 37.2 ± 0.6 28.9 ± 0.4 25.8 ± 0.9 67.1 ± 2.4 1.4 * Did not finish within three days for any dataset. 3 20.3 ± 0.9 28.5 ± 0.8 28.3 ± 0.8 34.8 ± 1.3 6.4 ± 0.5 7.6 ± 0.5 5.3 ± 0.5 3.0 A-Meta-Both 21.6 ± 0.6 18.9 ± 0.3 27.8 ± 0.2 31.6 ± 0.4 30.3 ± 0.6 40.7 ± 0.4 17.8 ± 1.9 13.9 ± 1.4 11.0 ± 0.5 1.8 Meta-Self 29.7 ± 2.2 20.1 ± 0.4 31.5 ± 1.2 29.9 ± 0.7 32.7 ± 0.8 45.6 ± 0.7 17.4 ± 0.8 14.6 ± 1.2 16.8 ± 1.9 1.2 1 5 10 15 25</p><p>Edges changed (%)  Edges changed (%) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Change in accuracy of GCN on CORA-ML for increasing number of perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of adversarially inserted edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Change in accuracy of GCN on CITESEER with and without enforcing unnoticeability constraints (singleton nodes are never admissible). Meta-Self-U corresponds to not enforcing the unnoticeability constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Change in accuracy of CLN on CORA-ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 11 :</head><label>11</label><figDesc>Figure 6: Change in accuracy of Deepwalk on CORA-ML.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Change in accuracy of Deepwalk on POLBLOGS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Misclassification rate (in %) for different meta-gradient heuristics with 5% perturbed edges.</figDesc><table><row><cell></cell><cell cols="2">CORA-ML</cell><cell cols="2">CITESEER</cell></row><row><cell></cell><cell>GCN</cell><cell>CLN</cell><cell>GCN</cell><cell>CLN</cell></row><row><cell>Clean</cell><cell cols="4">16.6 ± 0.3 17.3 ± 0.3 28.5 ± 1.0 28.3 ± 0.8</cell></row><row><cell cols="5">A-Meta-Train 21.2 ± 0.9 20.3 ± 0.3 31.8 ± 0.8 29.8 ± 0.5</cell></row><row><cell>A-Meta-Self</cell><cell cols="4">21.8 ± 0.7 18.9 ± 0.3 28.6 ± 0.4 28.5 ± 0.4</cell></row><row><cell cols="5">A-Meta-Both 22.5 ± 0.6 19.2 ± 0.3 28.9 ± 0.4 28.8 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Misclassification rate (in %) with 5% perturbed edges.</figDesc><table><row><cell></cell><cell></cell><cell>CORA</cell><cell></cell><cell></cell><cell>CITESEER</cell><cell></cell><cell></cell><cell>POLBLOGS</cell><cell></cell><cell>Avg.</cell></row><row><cell>Attack</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell cols="2">DeepWalk rank</cell></row><row><cell>Clean</cell><cell>16.6 ± 0.3</cell><cell>17.3 ± 0.3</cell><cell>20.3 ± 1.0</cell><cell>28.5 ± 0.9</cell><cell>28.3 ± 0.9</cell><cell>34.8 ± 1.4</cell><cell>6.4 ± 0.6</cell><cell>7.6 ± 0.5</cell><cell>5.3 ± 0.5</cell><cell>7.4</cell></row><row><cell>DICE</cell><cell>18.0 ± 0.4</cell><cell>18.0 ± 0.2</cell><cell>22.8 ± 0.3</cell><cell>28.9 ± 0.3</cell><cell>29.1 ± 0.3</cell><cell>39.1 ± 0.4</cell><cell>11.2 ± 1.1</cell><cell>11.2 ± 0.8</cell><cell>10.2 ± 0.6</cell><cell>5.0</cell></row><row><cell>First-order</cell><cell>17.2 ± 0.3</cell><cell>17.6 ± 0.2</cell><cell>20.7 ± 0.2</cell><cell>28.3 ± 0.3</cell><cell>28.4 ± 0.3</cell><cell>34.0 ± 0.3</cell><cell>7.8 ± 0.9</cell><cell>7.6 ± 0.5</cell><cell>7.9 ± 0.6</cell><cell>7.1</cell></row><row><cell>Nettack  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.9 ± 0.3</cell><cell cols="2">30.2 ± 0.4 41.2 ± 0.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>A-Meta-Train</cell><cell>21.8 ± 0.9</cell><cell>20.5 ± 0.3</cell><cell>25.0 ± 0.6</cell><cell>31.9 ± 0.7</cell><cell>30.1 ± 0.5</cell><cell>32.7 ± 0.5</cell><cell>11.9 ± 2.8</cell><cell>12.9 ± 2.5</cell><cell>5.8 ± 0.2</cell><cell>4.7</cell></row><row><cell>A-Meta-Both</cell><cell>20.7 ± 0.4</cell><cell cols="3">19.0 ± 0.3 28.5 ± 0.5 28.6 ± 0.4</cell><cell>28.7 ± 0.4</cell><cell>34.4 ± 0.4</cell><cell>19.8 ± 0.8</cell><cell>16.5 ± 1.3</cell><cell>21.5 ± 1.9</cell><cell>4.3</cell></row><row><cell>Meta-Train</cell><cell cols="3">22.0 ± 1.2 21.7 ± 0.4 26.1 ± 0.6</cell><cell>30.3 ± 1.0</cell><cell>29.0 ± 0.6</cell><cell>36.0 ± 0.2</cell><cell cols="3">16.3 ± 2.9 18.7 ± 2.3 14.5 ± 4.2</cell><cell>3.2</cell></row><row><cell>Meta-Self</cell><cell cols="2">24.5 ± 1.0 20.3 ± 0.4</cell><cell cols="7">28.1 ± 0.6 34.6 ± 0.7 32.2 ± 0.6 34.6 ± 0.7 22.5 ± 0.8 17.9 ± 1.7 59.0 ± 3.0</cell><cell>2.3</cell></row><row><cell cols="2">Meta w/ Oracle 21.0 ± 0.5</cell><cell>21.6 ± 0.3</cell><cell>27.8 ± 0.7</cell><cell>34.2 ± 0.9</cell><cell>32.9 ± 0.6</cell><cell>36.1 ± 0.7</cell><cell>25.6 ± 1.9</cell><cell>19.1 ± 1.4</cell><cell>52.3 ± 2.8</cell><cell>2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* Did not finish within three days on CORA-ML and POLBLOGS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of clean/ corrupted graph and weights.</figDesc><table><row><cell>W</cell><cell>Ŵ</cell></row><row><cell cols="2">A 0.85 0.52</cell></row><row><cell cols="2">Â 0.83 0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Poisoning results with limited knowledge about the graph (i.e. on a subgraph) after 10% changes.</figDesc><table><row><cell></cell><cell cols="2">CORA-ML</cell><cell cols="2">CITESEER</cell></row><row><cell></cell><cell>GCN</cell><cell>CLN</cell><cell>GCN</cell><cell>CLN</cell></row><row><cell>Clean</cell><cell cols="4">16.6 ± 0.3 17.3 ± 0.3 28.5 ± 0.8 28.3 ± 0.8</cell></row><row><cell cols="5">A-Meta-Sub 21.4 ± 0.7 22.4 ± 0.4 30.9 ± 0.7 31.4 ± 0.7</cell></row><row><cell>Meta-Sub</cell><cell cols="4">21.2 ± 0.6 20.8 ± 0.3 28.7 ± 0.3 31.4 ± 0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Analysis of attacks. An interesting question to ask is why the adversarial changes created by our meta-gradient approach are so destructive, and what patterns they follow. If we can find out what makes an edge insertion or deletion a strong adversarial change, we can circumvent expensive metagradient computations or even use this knowledge to detect adversarial attacks.</figDesc><table><row><cell cols="3">: Share (in %) of edge</cell></row><row><cell cols="3">deletions (DEL) and inser-</cell></row><row><cell cols="3">tions (INS) by Meta-Self on</cell></row><row><cell cols="2">CORA-ML.</cell><cell></cell></row><row><cell></cell><cell cols="2">c i =c j c i =c j</cell></row><row><cell>DEL</cell><cell>15.3</cell><cell>3.9</cell></row><row><cell>INS</cell><cell>9.4</cell><cell>71.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics.In Table6we see the characteristics of the datasets used in this work. Results for PUBMED can be found in Table8in Appendix F.</figDesc><table><row><cell>Dataset</cell><cell cols="2">N LCC E LCC</cell><cell>D</cell><cell>K</cell></row><row><cell>CORA-ML</cell><cell>2,810</cell><cell cols="3">7,981 2,879 7</cell></row><row><cell>CITESEER</cell><cell>2,110</cell><cell cols="3">3,757 3,703 6</cell></row><row><cell>POLBLOGS</cell><cell>1,222</cell><cell>16,714</cell><cell>-</cell><cell>2</cell></row><row><cell>PUBMED</cell><cell cols="2">19,717 44,324</cell><cell>500</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Misclassification rate (in %) with 10% perturbations in edges / node features. For Meta-Self with features, at each step the perturbation (edge or feature) is selected that has the highest meta gradient score.</figDesc><table><row><cell></cell><cell cols="2">CITESEER</cell></row><row><cell></cell><cell>GCN</cell><cell>CLN</cell></row><row><cell>Clean</cell><cell cols="2">28.5 ± 0.9 28.3 ± 0.8</cell></row><row><cell cols="3">Meta-Self with features 37.2 ± 1.1 34.2 ± 0.7</cell></row><row><cell>Meta-Self</cell><cell cols="2">38.6 ± 1.0 35.3 ± 0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Table 11 displays results when training the surrogate model for T = 10 iterations to obtain the (meta) gradients. Finally, Figures 5 through 12 show how the respective models' classification accuracies change for different attack methods and datasets. Misclassification rate (in %) with 5% perturbed edges onPUBMED Sen et al. (2008)  when training the surrogate model for T = 30 iterations to compute the approximate meta gradients.</figDesc><table><row><cell></cell><cell></cell><cell>PUBMED</cell></row><row><cell></cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell></row><row><cell>Clean</cell><cell cols="3">13.8 ± 0.3 15.9 ± 0.5 21.8 ± 0.1</cell></row><row><cell>DICE</cell><cell cols="3">15.3 ± 0.1 16.6 ± 0.4 25.1 ± 0.1</cell></row><row><cell cols="4">A-Meta-Self 16.4 ± 0.2 16.4 ± 0.4 27.4 ± 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Misclassification rate (in %) with 1% perturbed edges. ± 0.4 17.6 ± 0.2 21.6 ± 0.3 28.5 ± 0.4 28.3 ± 0.5 34.6 ± 0.3 13.7 ± 1.6 10.4 ± 1.2 7.5 ± 0.6 3.6 Meta-Train 16.2 ± 0.3 18.0 ± 0.3 20.6 ± 0.4 28.3 ± 0.5 28.1 ± 0.6 35.3 ± 0.3 9.4 ± 1.1 10.3 ± 1.7 7.3 ± 2.5 4.8 Meta-Self 17.0 ± 0.4 17.9 ± 0.2 21.1 ± 0.3 29.2 ± 0.5 29.0 ± 0.4 35.2 ± 0.3 11.4 ± 0.4 10.7 ± 1.7 6.6 ± 1.4 2.7</figDesc><table><row><cell></cell><cell></cell><cell>CORA</cell><cell></cell><cell></cell><cell>CITESEER</cell><cell></cell><cell></cell><cell>POLBLOGS</cell><cell></cell><cell>Avg.</cell></row><row><cell>Attack</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell cols="2">DeepWalk rank</cell></row><row><cell>Clean</cell><cell cols="6">16.6 ± 0.3 17.3 ± 0.3 20.3 ± 0.9 28.5 ± 0.8 28.3 ± 0.8 34.8 ± 1.3</cell><cell>6.4 ± 0.5</cell><cell>7.6 ± 0.5</cell><cell>5.3 ± 0.5</cell><cell>6.3</cell></row><row><cell>DICE</cell><cell cols="6">16.6 ± 0.3 17.6 ± 0.2 20.1 ± 0.2 28.4 ± 0.3 28.4 ± 0.3 35.9 ± 0.3</cell><cell>7.7 ± 0.9</cell><cell>8.5 ± 0.6</cell><cell>7.4 ± 1.0</cell><cell>4.8</cell></row><row><cell>First-order</cell><cell cols="6">16.6 ± 0.3 17.3 ± 0.1 20.2 ± 0.2 28.2 ± 0.3 28.3 ± 0.4 34.9 ± 0.4</cell><cell>7.0 ± 0.8</cell><cell>7.7 ± 0.5</cell><cell>8.5 ± 1.6</cell><cell>5.7</cell></row><row><cell>Nettack  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">29.0 ± 0.4 28.6 ± 0.4 36.4 ± 0.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>A-Meta-Train</cell><cell cols="6">16.3 ± 0.4 18.2 ± 0.2 20.6 ± 0.3 29.1 ± 0.5 28.6 ± 0.5 34.7 ± 0.5</cell><cell cols="2">8.9 ± 2.9 10.2 ± 1.9</cell><cell>5.0 ± 0.2</cell><cell>4.7</cell></row><row><cell>A-Meta-Both</cell><cell>17.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Misclassification rate (in %) with 10% perturbed edges.</figDesc><table><row><cell></cell><cell></cell><cell>CORA</cell><cell></cell><cell></cell><cell>CITESEER</cell><cell></cell><cell></cell><cell>POLBLOGS</cell><cell></cell><cell>Avg.</cell></row><row><cell>Attack</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell cols="2">DeepWalk rank</cell></row><row><cell>Clean</cell><cell cols="6">16.6 ± 0.3 17.3 ± 0.3 20.3 ± 1.0 28.5 ± 0.8 28.3 ± 0.8 34.8 ± 1.3</cell><cell>6.4 ± 0.5</cell><cell>7.6 ± 0.5</cell><cell>5.3 ± 0.5</cell><cell>7.5</cell></row><row><cell>DICE</cell><cell cols="9">19.5 ± 0.5 19.1 ± 0.2 26.2 ± 0.3 29.7 ± 0.3 29.9 ± 0.3 41.2 ± 0.3 14.4 ± 0.8 14.3 ± 0.6 12.9 ± 0.3</cell><cell>4.9</cell></row><row><cell>First-order</cell><cell cols="6">17.6 ± 0.5 17.9 ± 0.2 21.5 ± 0.2 28.2 ± 0.3 28.7 ± 0.4 32.4 ± 0.4</cell><cell>7.7 ± 0.6</cell><cell>7.6 ± 0.3</cell><cell>8.2 ± 0.6</cell><cell>7.1</cell></row><row><cell>Nettack  *</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Misclassification rate (in %) with 5% perturbed edges and only training the surrogate model for T = 10 iterations to obtain the (meta-) gradients.</figDesc><table><row><cell></cell><cell></cell><cell>CORA</cell><cell></cell><cell></cell><cell>CITESEER</cell><cell></cell><cell></cell><cell>POLBLOGS</cell><cell>Avg.</cell></row><row><cell>Attack</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk</cell><cell>GCN</cell><cell>CLN</cell><cell>DeepWalk rank</cell></row><row><cell>Clean</cell><cell cols="2">16.6 ± 0.3 17.3 ± 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This research was supported by the German Research Foundation, grant GU 1409/2-1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ALGORITHM</head><p>Algorithm 1: Poisoning attack on graph neural networks with meta gradients and self-training  θ ← train surrogate model on the input graph using known labels CL; ĈU ← predict labels of unlabeled nodes using θ; Â ← A; while Â − A 0 &lt; 2∆ do randomly initialize θ0; </p><p>; // Flip gradient sign of node pairs with edge e ← maximum entry (u, v) in S that fulfills constraints Φ(G); Â ← insert or remove edge e to/from Â; Ĝ ← ( Â, X); return : Ĝ</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The political blogosphere and the 2004 US election: divided they blog</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on Link discovery</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning activation functions to improve deep neural networks</title>
		<author>
			<persName><forename type="first">Forest</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6830</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
	</analytic>
	<monogr>
		<title level="m">Preprints Conf. Optimality in Artificial and Biological Neural Networks</title>
				<imprint>
			<date type="published" when="1992">2018. 1992</date>
			<biblScope unit="page" from="6" to="8" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Texas</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based optimization of hyperparameters</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1889" to="1900" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01093</idno>
		<title level="m">Adversarial attacks on node embeddings</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian robust attributed graph clustering: Joint learning of partial anomalies and group structure</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2738" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NetGAN: Generating graphs via random walks</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning. Adaptive Computation and Machine Learning series</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Manos Antonakakis, and Nikolaos Vasiloglou. Practical attacks against graph-based clustering</title>
		<author>
			<persName><forename type="first">Yizheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacin</forename><surname>Nadji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Kountouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Monrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Perdisci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09056</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective classification of network data</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Classification: Algorithms and Applications</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">399</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using machine teaching to identify optimal training-set attacks on machine learners</title>
		<author>
			<persName><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shike Mei and Xiaojin Zhu</title>
				<imprint>
			<date type="published" when="2000">2000. 2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2871" to="2877" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards poisoning of deep learning algorithms with back-gradient optimization</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Muñoz-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambra</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Paudice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasin</forename><surname>Wongrassamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emil</forename><forename type="middle">C</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM Workshop on Artificial Intelligence and Security</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meta-neural networks that learn by learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Devang</surname></persName>
		</author>
		<author>
			<persName><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
				<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2485" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Google</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to learn: Introduction and overview</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convex adversarial collective classification</title>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torkamani</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="642" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
