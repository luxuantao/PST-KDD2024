<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-05-15">15 May 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Vishwanath</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
							<email>vishwanath.sindagi@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Recent Advances in CNN-based Single Image Crowd Counting and Density Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-05-15">15 May 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">47AD7135886F99DEB5DA40C93D583F38</idno>
					<idno type="DOI">10.1016/j.patrec.2017.07.007</idno>
					<note type="submission">Received date: 14 April 2017 Revised date: 5 July 2017 Accepted date: 16 July 2017 Received 1 May 2013 Received in final form 10 May 2013 Accepted 13 May 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pattern Recognition Letters Crowd counting Density estimation Crowd analysis Pattern Recognition Letters</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating count and density maps from crowd images has a wide range of applications such as video surveillance, traffic monitoring, public safety and urban planning. In addition, techniques developed for crowd counting can be applied to related tasks in other fields of study such as cell microscopy, vehicle counting and environmental survey. The task of crowd counting and density map estimation is riddled with many challenges such as occlusions, non-uniform density, intra-scene and inter-scene variations in scale and perspective. Nevertheless, over the last few years, crowd count analysis has evolved from earlier methods that are often limited to small variations in crowd density and scales to the current state-of-the-art methods that have developed the ability to perform successfully on a wide range of scenarios. The success of crowd counting methods in the recent years can be largely attributed to deep learning and publications of challenging datasets. In this paper, we provide a comprehensive survey of recent Convolutional Neural Network (CNN) based approaches that have demonstrated significant improvements over earlier methods that rely largely on hand-crafted representations. First, we briefly review the pioneering methods that use hand-crafted representations and then we delve in detail into the deep learning-based approaches and recently published datasets. Furthermore, we discuss the merits and drawbacks of existing CNN-based approaches and identify promising avenues of research in this rapidly evolving field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• Survey on CNN-based approaches for crowd counting and density estimation.</p><p>• Discussion on recent hand-crafted representations-based methods.</p><p>• Recently datasets that pose various challenges are discussed.</p><p>• Detailed analysis and comparison of results of CNN-based and traditional methods.</p><p>• Discussion on future directions and trends for further progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowd counting aims to count the number of people in a crowded scene where as density estimation aims to map an input crowd image to it's corresponding density map which indicates the number of people per pixel present in the image (as illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>) and the two problems have been jointly addressed by researchers. The problem of crowd counting and density estimation is of paramount importance and it is essential for building higher level cognitive abilities in crowded scenarios such as crowd monitoring <ref type="bibr" target="#b14">[15]</ref> and scene understanding <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b114">115]</ref>. Crowd analysis has attracted significant attention from researchers in the recent past due to a variety of reasons. Exponential growth in the world population and the resulting urbanization has led to an increased number of activities such as sporting events, political rallies, public demonstrations etc. (shown in Fig. <ref type="figure" target="#fig_2">2</ref>), thereby resulting in more frequent crowd gatherings in the recent years. In such scenarios, it is essential to analyze crowd behavior for better management, safety and security.</p><p>Like any other computer vision problem, crowd analysis comes with many challenges such as occlusions, high clutter, non-uniform distribution of people, non-uniform illumination, intra-scene and inter-scene variations in appearance, scale and perspective making the problem extremely difficult. Some of these challenges are illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. The complexity of the problem together with the wide range of applications for crowd analysis has led to an increased focus by researchers in the recent past.</p><p>Crowd analysis is an inherently inter-disciplinary research topic with researchers from different communities (such as sociology <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b9">10]</ref>, psychology <ref type="bibr" target="#b4">[5]</ref>, physics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref>, biology <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b109">110]</ref>, computer vision and public safety) have addressed  the issue from different viewpoints. Crowd analysis has a variety of critical applications of inter-disciplinarian nature: Safety monitoring: The widespread usage of video surveillance cameras for security and safety purposes in places such as sports stadiums, tourist spots, shopping malls and airports has enabled easier monitoring of crowd in such scenarios. However, traditional surveillance algorithms may break down as they are unable to process high density crowds due to limitations in their design. In such scenarios, we can leverage the results of algorithms specially designed for crowd analysis related tasks such as behavior analysis <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b47">48]</ref>, congestion analysis <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b39">40]</ref>, anomaly detection <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b13">14]</ref> and event detection <ref type="bibr" target="#b7">[8]</ref>. Disaster management: Many scenarios involving crowd gatherings such as sports events, music concerts, public demonstrations and political rallies face the risk of crowd related disasters such as stampedes which can be life threatening. In such cases, crowd analysis can be used as an effective tool for early overcrowding detection and appropriate management of crowd, hence, eventual aversion of any disaster <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Design of public spaces: Crowd analysis on existing public spots such as airport terminals, train stations, shopping malls and other public buildings <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b89">90]</ref> can reveal important design shortcomings from crowd safety and convenience point of view. These studies can be used for design of public spaces that are optimized for better safety and crowd movement <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b1">2]</ref>. Intelligence gathering and analysis: Crowd counting techniques can be used to gather intelligence for further analysis and inference. For instance, in retail sector, crowd counting can be used to gauge people's interest in a product in a store and this information can be used for appropriate product placement <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b66">67]</ref>. Similarly, crowd counting can be used to measure queue lengths to optimize staff numbers at different times of the day. Furthermore, crowd counting can be used to analyze pedestrian flow at signals at different times of the day and this information can be used for optimizing signal-wait times <ref type="bibr" target="#b8">[9]</ref>. Virtual environments: Crowd analysis methods can be used to understand the underlying phenomenon thereby enabling us to establish mathematical models that can provide accurate simulations. These mathematical models can be further used for simulation of crowd phenomena for various applications such as computer games, inserting visual effects in film scenes and designing evacuation plans <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b73">74]</ref>. Forensic search: Crowd analysis can be used to search for sus- pects and victims in events such as bombing, shooting or accidents in large gatherings. Traditional face detection and recognition algorithms can be speeded up using crowd analysis techniques which are more adept at handling such scenarios <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>These variety of applications has motivated researchers across various fields to develop sophisticated methods for crowd analysis and related tasks such as counting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>, density estimation <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b10">11]</ref>, segmentation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b26">27]</ref>, behaviour analysis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b102">103]</ref>, tracking <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b115">116]</ref>, scene understanding <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b114">115]</ref> and anomaly detection <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b55">56]</ref>. Among these, crowd counting and density estimation are a set of fundamental tasks and they form basic building blocks for various other applications discussed earlier. Additionally, methods developed for crowd counting can be easily extended to counting tasks in other fields such as cell microscopy <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">20]</ref>, vehicle counting <ref type="bibr" target="#b69">[70]</ref>, environmental survey <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b104">105]</ref>, etc.</p><p>Over the last few years, researchers have attempted to address the issue of crowd counting and density estimation using a variety of approaches such as detection-based counting, clustering-based counting and regression-based counting <ref type="bibr" target="#b60">[61]</ref>. The initial work on regression-based methods mainly use handcrafted features and the more recent works use Convolutional Neural Network (CNN) based approaches. The CNN-based approaches have demonstrated significant improvements over previous hand-crafted feature-based methods, thus, motivating more researchers to explore CNN-based approaches further for related crowd analysis problems. In this paper, we review various single image crowd counting and density estimation methods with a specific focus on recent CNN-based approaches.</p><p>Researchers have attempted to provide a comprehensive survey and evaluation of existing techniques for various aspects of <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b116">117]</ref>. Zhan et al. <ref type="bibr" target="#b104">[105]</ref> and Junior et al. <ref type="bibr" target="#b43">[44]</ref> were among the first ones to study and review existing methods for general crowd analysis. Li et al. <ref type="bibr" target="#b54">[55]</ref> surveyed different methods for crowded scene analysis tasks such as crowd motion pattern learning, crowd behavior, activity analysis and anomaly detection in crowds. More recently, Zitouni et al. <ref type="bibr" target="#b116">[117]</ref> evaluated existing methods across different research disciplines by inferring key statistical evidence from existing literature and provided suggestions towards the general aspects of techniques rather than any specific algorithm. While these works focussed on the general aspects of crowd analysis, researchers have studied in detail crowd counting and density estimation methods specifically <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b78">79]</ref>. Loy et al. <ref type="bibr" target="#b60">[61]</ref> provided a detailed description and comparison of video imagerybased crowd counting and evaluation of different methods using the same protocol. They also analyzed each processing module to identify potential bottlenecks to provide new directions for further research. In another work, Ryan et al. <ref type="bibr" target="#b78">[79]</ref> presented an evaluation of regression-based methods for crowd counting across multiple datasets and provided a detailed analysis of performance of various hand-crafted features. Recently, Saleh et al. <ref type="bibr" target="#b80">[81]</ref> surveyed two main approaches which are direct approach (i.e., object based target detection) and indirect approach (e.g. pixel-based, texture-based, and corner points based analysis).</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T 4 crowd analysis</formula><p>Though existing surveys analyze various methods for crowd analysis and counting, they however cover only traditional methods that use hand-crafted features and do not take into account the recent advancements driven primarily by CNN-based approaches <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b87">88]</ref> and creation of new challenging crowd datasets <ref type="bibr" target="#b105">[106,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b110">111]</ref>. While CNN-based approaches have achieved drastically lower error rates, the creation of new datasets has enabled learning of more generalized models. To keep up with the rapidly advancing research in crowd counting, we believe it is necessary to analyze these methods in detail in order to understand the trends. Hence, in this paper, we provide a survey of recent state-ofthe-art CNN-based approaches for crowd counting and density estimation for single images.</p><p>Rest of the paper is organized as follows: Section 2 briefly reviews the traditional crowd counting and density estimation approaches with an emphasis on the most recent methods. This is followed by a detailed survey on CNN-based methods along with a discussion on their merits and drawbacks in Section 3. In Section 5, recently published challenging datasets for crowd counting are discussed in detail along with results of the stateof-the-art methods. We discuss several promising avenues for achieving further progress in Section 6. Finally, concluding remarks are made in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Review of traditional approaches</head><p>Various approaches have been proposed to tackle the problem of crowd counting in images <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b110">111]</ref> and videos <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b20">21]</ref>. Loy et al. <ref type="bibr" target="#b60">[61]</ref> broadly classified traditional crowd counting methods based on the approach into the following categories: (1) Detection-based approaches,</p><p>(2) Regression-based approaches, and (3) Density estimationbased approaches.</p><p>Since the focus of this work is on CNN-based approaches, in this section, we briefly review the detection and regressionbased approaches using hand-crafted features for the sake of completeness. In addition, we present a review of the recent traditional methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b101">102]</ref> that have not been analyzed in earlier surveys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Detection-based approaches</head><p>Most of the initial research was focussed on detection style framework, where a sliding window detector is used to detect people in the scene <ref type="bibr" target="#b25">[26]</ref> and this information is used to count the number of people <ref type="bibr" target="#b53">[54]</ref>. Detection is usually performed either in the monolithic style or parts-based detection. Monolithic detection approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b27">28]</ref> typically are traditional pedestrian detection methods which train a classifier using features (such as Haar wavelets <ref type="bibr" target="#b94">[95]</ref>, histogram oriented gradients <ref type="bibr" target="#b24">[25]</ref>, edgelet <ref type="bibr" target="#b99">[100]</ref> and shapelet <ref type="bibr" target="#b79">[80]</ref>) extracted from a full body. Various learning approaches such as Support Vector Machines, boosting <ref type="bibr" target="#b95">[96]</ref> and random forest <ref type="bibr" target="#b33">[34]</ref> have been used with varying degree of success. Though successful in low density crowd scenes, these methods are adversely affected by the presence of high density crowds. Researchers have attempted to address this issue by adopting part-based detection methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b100">101]</ref>, where one constructs boosted classifiers for specific body parts such as the head and shoulder to estimate the people counts in a designated area <ref type="bibr" target="#b53">[54]</ref>. In another approach using shape learning, Zhao et al. <ref type="bibr" target="#b111">[112]</ref> modelled humans using 3D shapes composed of ellipsoids, and employed a stochastic process to estimate the number and shape configuration that best explains a given foreground mask in a scene. Ge and Collins <ref type="bibr" target="#b34">[35]</ref> further extended the idea by using flexible and practical shape models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Regression-based approaches</head><p>Though parts-based and shape-based detectors were used to mitigate the issues of occlusion, these methods were not successful in the presence of extremely dense crowds and high background clutter. To overcome these issues, researchers attempted to count by regression where they learn a mapping between features extracted from local image patches to their counts <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b19">20]</ref>. By counting using regression, these methods avoid dependency on learning detectors which is a relatively complex task. These methods have two major components: low-level feature extraction and regression modelling. A variety of features such as foreground features, edge features, texture and gradient features have been used for encoding low-level information. Foreground features are extracted from foreground segments in a video using standard background subtraction techniques. Blob-based holistic features such as area, perimeter, perimeter-area ration, etc. have demonstrated encouraging results <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b77">78]</ref>. While these methods capture global properties of the scene, local features such as edges and texture/gradient features such as local binary pattern (LBP), histogram oriented gradients (HOG), gray level co-occurrence matrices (GLCM) have been used to further improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Once these global and local features are extracted, different regression techniques such as linear regression <ref type="bibr" target="#b70">[71]</ref>, piecewise linear regression <ref type="bibr" target="#b14">[15]</ref>, ridge regression <ref type="bibr" target="#b19">[20]</ref>, Gaussian process regression and neural network <ref type="bibr" target="#b63">[64]</ref> are used to learn a mapping from low-level feature to the crowd count.</p><p>In a recent approach, Idrees et al. <ref type="bibr" target="#b40">[41]</ref> identified that no single feature or detection method is reliable enough to provide sufficient information for accurate counting in the presence of high density crowds due to various reasons such as low resolution, severe occlusion, foreshortening and perspective. Additionally, they observed that there exists a spatial relationship that can be used to constrain the count estimates in neighboring local regions. With these observations in mind, they proposed to extract features using different methods that capture different information. By treating densely packed crowds of individuals as irregular and non-homogeneous texture, they employed Fourier analysis along with head detections and SIFT interestpoint based counting in local neighborhoods. The count estimates from this localized multi-scale analysis are then aggregated subject to global consistency constraints. The three sources, i.e., Fourier, interest points and head detection are then combined with their respective confidences and counts at localized patches are computed independently. These local counts are then globally constrained in a multi-scale Markov Random Field (MRF) framework to get an estimate of count for the entire image. The authors also introduced an annotated dataset (UCF CC 50) of 50 images containing 64000 humans.</p><p>Chen et al. <ref type="bibr" target="#b18">[19]</ref> introduced a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. Considering that the challenges of inconsistent features along with sparse and imbalanced (encountered during learning a regression function) are related, cumulative attribute-based representation for learning a regression model is proposed. Specifically, features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space. The method is based on the notion of discriminative attributes used for addressing sparse training data. This method is inherently capable of handling imbalanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Density estimation-based approaches</head><p>While the earlier methods were successful in addressing the issues of occlusion and clutter, most of them ignored important spatial information as they were regressing on the global count. In contrast, Lempitsky et al. <ref type="bibr" target="#b51">[52]</ref> proposed to learn a linear mapping between local patch features and corresponding object density maps, thereby incorporating spatial information in the learning process. In doing so, they avoided the hard task of learning to detect and localize individual object instances by introducing a new approach of estimating image density whose integral over any region in the density map gives the count of objects within that region. The problem of learning density maps is formulated as a minimization of a regularized risk quadratic cost function. A new loss function appropriate for learning density maps is introduced. The entire problem is posed as a convex optimization task which they solve using cutting-plane optimization.</p><p>Observing that it is difficult to learn a linear mapping, Pham et al. <ref type="bibr" target="#b74">[75]</ref> proposed to learn a non-linear mapping between lo-cal patch features and density maps. They used random forest regression from multiple image patches to vote for densities of multiple target objects to learn a non-linear mapping. In addition, they tackled the problem of large variation in appearance and shape between crowded image patches and non-crowded ones by proposing a crowdedness prior and they trained two different forests corresponding to this prior. Furthermore, they were able to successfully speed up the estimation process for real-time performance by proposing an effective forest reduction that uses permutation of decision trees. Apart from achieving real-time performance, another advantage of their method is that it requires relatively less memory to build and store the forest.</p><p>Similar to the above approach, Wang and Zou <ref type="bibr" target="#b98">[99]</ref> identified that though existing methods are effective, they were inefficient from computational complexity point of view. To this effect, they proposed a fast method for density estimation based on subspace learning. Instead of learning a mapping between dense features and their corresponding density maps, they learned to compute the embedding of each subspace formed by image patches. Essentially, they exploited the relationship between images and their corresponding density maps in the respective feature spaces. The feature space of image patches are clustered and examples of each subspace are collected to learn its embedding. Their assumption that local image patches and their corresponding density maps share similar local geometry enables them to learn locally linear embedding using which the density map of an image patch can be estimated by preserving the geometry. Since, implementing locally linear embedding (LLE) is time-consuming, they divided the feature spaces of image patches and their counterpart density maps into subspaces, and computed the embedding of each subspace formed by image patches. The density map of input patch is then estimated by simple classification and mapping with the corresponding embedding matrix.</p><p>In a more recent approach, Xu and Qiu <ref type="bibr" target="#b101">[102]</ref> observed that the existing crowd density estimation methods used a smaller set of features thereby limiting their ability to perform better. Inspired by the ability of high-dimensional features in other domains such as face recognition, they proposed to boost the performances of crowd density estimation by using a much extensive and richer set of features. However, since the regression techniques used by earlier methods (based on Gaussian process regression or Ridge regression) are computationally complex and are unable to process very high-dimensional features, they used random forest as the regression model whose tree structure is intrinsically fast and scalable. Unlike traditional approaches to random forest construction, they embedded random projection in the tree nodes to combat the curse of dimensionality and to introduce randomness in the tree construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN-based methods</head><p>The success of CNNs in numerous computer vision tasks has inspired researchers to exploit their abilities for learning nonlinear functions from crowd images to their corresponding density maps or corresponding counts. A variety of CNN-based methods have been proposed in the literature. We broadly categorize these methods based on property of the networks and training approach as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Based on the property of the networks, we classify the approaches into the following categories:</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>• Basic CNNs: Approaches that involve basic CNN layers in their networks fall into this category. These methods are amongst initial deep learning approaches for crowd counting and density estimation. • Scale-aware models: The basic CNN-based approaches evolved into more sophisticated models that were robust to variations in scale. This robustness is achieved through different techniques such as multi-column or multi-resolution architectures. • Context-aware models: Another set of approaches attempted to incorporate local and global contextual information present in the image into the CNN framework for achieving lower estimation errors. • Multi-task frameworks: Motivated by the success of multi-task learning for various computer vision tasks, various approaches have been developed to combine crowd counting and estimation along with other tasks such as foreground-background subtraction and crowd velocity estimation.</p><p>In an yet another categorization, we classify the CNN-based approaches based on the inference methodology into the following two categories:</p><p>• Patch-based inference: In this approach, the CNNs are trained using patches cropped from the input images. Different methods use different crop sizes. During the prediction phase, a sliding window is run over the test image and predictions are obtained for each window and finally aggregated to obtain total count in the image. • Whole image-based inference: Methods in this category perform a whole-image based inference. These methods avoid computationally expensive sliding windows.</p><p>Table <ref type="table" target="#tab_0">1</ref> presents a categorization of various CNN-based crowd counting methods based on their network property and inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Survey of CNN-based methods</head><p>In this section, we review various CNN-based crowd counting and density estimation methods along with their merits and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Network property Inference process Fu et al. <ref type="bibr" target="#b32">[33]</ref> Basic Patch-based Wang et al. <ref type="bibr" target="#b97">[98]</ref> Basic Patch-based Zhang et al. <ref type="bibr" target="#b106">[107]</ref> Multi-task Patch-based Boominathan et al. <ref type="bibr" target="#b10">[11]</ref> Scale-aware Patch-based Zhang et al. <ref type="bibr" target="#b110">[111]</ref> Scale-aware Whole image-based Walach and Wolf <ref type="bibr" target="#b96">[97]</ref> Basic Patch-based Onoro et al. <ref type="bibr" target="#b69">[70]</ref> Scale-aware Patch-based Shang et al. <ref type="bibr" target="#b84">[85]</ref> Context-aware Whole image-based Sheng et al. <ref type="bibr" target="#b88">[89]</ref> Context-aware Whole image-based Kumagai et al. <ref type="bibr" target="#b49">[50]</ref> Scale-aware Patch-based Marsden et al. <ref type="bibr" target="#b64">[65]</ref> Scale-aware Whole image-based Mundhenk et al. <ref type="bibr" target="#b68">[69]</ref> Basic Patch-based Artetta et al. <ref type="bibr" target="#b3">[4]</ref> Multi-task Patch-based Zhao et al. <ref type="bibr" target="#b112">[113]</ref> Multi-task Patch-based Sindagi et al. <ref type="bibr" target="#b91">[92]</ref> Multi-task Whole image-based Sam et al. <ref type="bibr" target="#b81">[82]</ref> Scale-aware Patch-based Kang et al. <ref type="bibr" target="#b112">[113]</ref> Basic Patch-based drawbacks.</p><p>Wang et al. <ref type="bibr" target="#b97">[98]</ref> and Fu et al. <ref type="bibr" target="#b32">[33]</ref> were among the first ones to apply CNNs for the task of crowd density estimation. Wang et al. proposed an end-to-end deep CNN regression model for counting people from images in extremely dense crowds. They adopted AlexNet network <ref type="bibr" target="#b48">[49]</ref> in their architecture where the final fully connected layer of 4096 neurons is replaced with a single neuron layer for predicting the count. Besides, in order to reduce false responses background like buildings and trees in the images, training data is augmented with additional negative samples whose ground truth count is set as zero. In a different approach, Fu et al. proposed to classify the image into one of the five classes: very high density, high density, medium density, low density and very low density instead of estimating density maps. Multi-stage ConvNet from the works of Sermanet et al. <ref type="bibr" target="#b83">[84]</ref> was adopted for better shift, scale and distortion invariance. In addition, they used a cascade of two classifiers to achieve boosting in which the first one specifically samples misclassified images whereas the second one reclassifies rejected samples.</p><p>Zhang et al. <ref type="bibr" target="#b106">[107]</ref> analyzed existing methods to identify that their performance reduces drastically when applied to a new scene that is different from the training dataset. To overcome this issue, they proposed to learn a mapping from images to crowd counts and to adapt this mapping to new target scenes for cross-scene counting. To achieve this, they first learned their network by alternatively training on two objective functions: crowd count and density estimation which are related objectives. By alternatively optimizing over these objective functions one is able to obtain better local optima. In order to adapt this network to a new scene, the network is fine-tuned using training samples that are similar to the target scene. It is important to note that the network is adapted to new target scenes without any extra label information. The overview of their approach is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. Also, in contrast to earlier methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>that use the sum of Gaussian kernels centered on the locations of objects, a new method for generating ground truth density map is proposed that incorporates perspective information. In doing so, the network is able to perform perspective normalization thereby achieving robustness to scale and perspective variations. Additionally, they introduced a new dataset for the purpose of evaluating cross-scene crowd counting. The network is evaluated for cross-scene crowd counting as well as single scene crowd counting and superior results are demonstrated for both scenarios. Inspired by the success of cross-scene crowd counting <ref type="bibr" target="#b106">[107]</ref>, Walach and Wolf <ref type="bibr" target="#b96">[97]</ref> performed layered boosting and selective sampling. Layered boosting involves iteratively adding CNN layers to the model such that every new layer is trained to estimate the residual error of the earlier prediction. For instance, after the first CNN layer is trained, the second CNN layer is trained on the difference between the estimation and ground truth. This layered boosting approach is based on the notion of Gradient Boosting Machines (GBM) <ref type="bibr" target="#b31">[32]</ref> which are a subset of powerful ensemble techniques. An overview of their boosting approach is presented in Fig. <ref type="figure" target="#fig_5">5</ref>. The other contribution made by the authors is the use of sample selection algorithm to improve the training process by reducing the effect of low quality samples such as trivial samples or outliers. According to the authors, the samples that are correctly classified early on are trivial samples. Presenting such samples for training even after the networks have learned to classify them tends to introduce bias in the network for such samples, thereby affecting its generalization performance. Another source of training inefficiency is the presence of outliers such as mislabeled samples. Apart from affecting the network's performance, these samples increase the training time. To overcome this issue, such samples are eliminated out of the training process for a number of epochs. The authors demonstrated that their method reduces the count estimation error by 20% to 30% over existing stateof-the-art methods at that time on different datasets.</p><p>In contrast to the above methods that use patch-based training, Shang et al. <ref type="bibr" target="#b84">[85]</ref> proposed an end-to-end count estimation method using CNNs (Fig. <ref type="figure" target="#fig_6">6</ref>). Instead of dividing the image into patches, their method takes the entire image as input and directly outputs the final crowd count. As a result, computations on overlapping regions are shared by combining multiple  In an effort to capture semantic information in the image, Boominathan et al. <ref type="bibr" target="#b10">[11]</ref> combined deep and shallow fully convolutional networks to predict the density map for a given crowd image. The combination of two networks enables one to build a model robust to non-uniform scaling of crowd and variations in perspective. Furthermore, an extensive augmentation of the training dataset is performed in two ways. Patches from the multi-scale image representation are sampled to make the system robust to scale variations. Fig. <ref type="figure" target="#fig_7">7</ref> shows overview of this method.</p><p>In another approach, Zhang et al. <ref type="bibr" target="#b110">[111]</ref> proposed a multicolumn based architecture (MCNN) for images with arbitrary crowd density and arbitrary perspective. Inspired by the success of multi-column networks for image recognition <ref type="bibr" target="#b23">[24]</ref>, the proposed method ensures robustness to large variation in object scales by constructing a network that comprises of three columns corresponding to filters with receptive fields of different sizes (large, medium, small) as shown in Fig. <ref type="figure" target="#fig_8">8</ref>. These different columns are designed to cater to different object scales present in the images. Additionally, a new method for generating ground truth crowd density maps is proposed. In contrast to existing methods that either use sum of Gaussian kernels with a fixed variance or perspective maps, Zhang et al. proposed to take into account perspective distortion by estimating spread parameter of the Gaussian kernel based on the size of the head of each person within the image. However, it is impractical to estimate head sizes and their underlying relationship with density maps. Instead they used an important property observed in high density crowd images that the head size is related to distance between the centers of two neighboring persons. The spread parameter for each person is data-adaptively determined based on its average distance to its neighbors. Note that the ground truth density maps created using this technique incorporate distortion information without the use of perspective maps. Finally, considering that existing crowd counting datasets do not cater to all the challenging situations encountered in real world scenarios, a new ShanghaiTech crowd datasets is constructed. This new dataset includes 1198 images with about 330,000 annotated heads. Similar to the above approach, Onoro and Sastre <ref type="bibr" target="#b69">[70]</ref> developed a scale aware counting model called Hydra CNN that is able to estimate object densities in a variety of crowded sce-narios without any explicit geometric information of the scene. First, a deep fully-convolutional neural network (which they call as Counting CNN) with six convolutional layers is employed. Motivated by the observation of earlier work <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b60">61]</ref> that incorporating perspective information for geometric correction of the input features results in better accuracy, geometric information is incorporated into the Counting CNN (CCNN). To this effect, they developed Hydra CNN that learns a multiscale non-linear regression model. As shown in Fig. <ref type="figure" target="#fig_9">9</ref> the network consists of 3 heads and a body with each head learning features for a particular scale. Each head of the Hydra-CNN is constructed using the CCNN model whose outputs are concatenated and fed to the body. The body consists of a set of two fully-connected layers followed by a rectified linear unit (ReLu), a dropout layer and a final fully connected layer to estimate the object density map. While the different heads extract image descriptors at different scales, the body learns a highdimensional representation that fuses the multi-scale information provided by the heads. This network design of Hydra CNN is inspired by the work of Li et al. <ref type="bibr" target="#b52">[53]</ref>. Finally, the network is trained with pyramid of image patches extracted at multiple scales. The authors demonstrated through their experiments that the Hydra CNN is able to perform successfully in scenarios and datasets with significant variations in the scene. Instead of training all regressors of a multi-column network <ref type="bibr" target="#b110">[111]</ref> on all the input patches, Sam et al. <ref type="bibr" target="#b81">[82]</ref> argue that better performance is obtained by training regressors with a particular set of training patches by leveraging variation of crowd density within an image. To this end, they proposed a switching CNN that cleverly selects an optimal regressor suited for a particular input patch. As shown in Fig. <ref type="figure" target="#fig_10">10</ref>, the proposed network consists of multiple independent regressors similar to multi-column network <ref type="bibr" target="#b110">[111]</ref> with different receptive fields and a switch classifier. The switch classifier is trained to select the optimal regressor for a particular input patch. Independent CNN crowd density regressors are trained on patches sampled from a grid in a given crowd scene. The switch classifier and the independent regressors are alternatively trained. The authors describe multiple stages of training their network. First, the independent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>regressors are pretrained on image patches to minimize the Euclidean distance between the estimated density map and ground truth. This is followed by a differential training stage where, the count error is factored in to improve the counting performance by back-propagating a regressor with the minimum count error for a given training patch. After training the multiple regressors, a switch classifier based on VGG-16 architecture <ref type="bibr" target="#b90">[91]</ref> is trained to select an optimal regressor for accurate counting. Finally, the switch classifier and CNN regressors are co-adapted in the coupled training stage. While the above methods concentrated on incorporating scale information in the network, Sheng et al. in <ref type="bibr" target="#b88">[89]</ref> proposed to integrate semantic information by learning localityaware feature sets. Noting that earlier methods that use handcrafted features ignored key semantic and spatial information, the authors proposed a new image representation which incorporates semantic attributes as well as spatial cues to improve the discriminative power of feature representations. They defined semantic attributes at the pixel level and learned semantic feature maps via deep CNN. The spatial information in the image is encoded using locality-aware features in the semantic attribute feature map space. The locality-aware features (LAF) are built on the idea of spatial pyramids on neighboring patches thereby encoding spatial context and local information. The local descriptors from adjacent cells are then encoded into image representations using weighted VLAD encoding method.</p><p>Similar to <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b69">70]</ref>, Kumagai et al. <ref type="bibr" target="#b49">[50]</ref>, based on the observation that a single predictor is insufficient to appropriately predict the count in the presence of large appearance changes, proposed a Mixture of CNNs (MoCNN) that are specialized to a different scene appearances. As shown in Fig. <ref type="figure" target="#fig_11">11</ref>, the architecture consists of a mixture of expert CNNs and a gating CNN that adaptively selects the appropriate CNN among the experts according to the appearance of the input image. For prediction, the expert CNNs predict crowd count in the image while the gating CNN predicts appropriate probabilities for each of the expert CNNs. These probabilities are further used as weighting factors to compute the weighted average of the counts predicted by all the expert CNNs. Motivated by the success of scale aware models <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b69">70]</ref>, Marsden et al. <ref type="bibr" target="#b64">[65]</ref> proposed to incorporate scale into the models with much less number of model parameters. Observing that the earlier scale aware models <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b69">70]</ref> are difficult to optimize and are computationally complex, Marsden et al. <ref type="bibr" target="#b64">[65]</ref> proposed a single column fully convolutional network where the scale information is incorporated into the model using a simple yet effective multi-scale averaging step during prediction without any increase in the model parameters. The method addresses the issues of scale and perspective changes by feeding multiple scales of test image into the network during prediction phase. The crowd count is estimated for each scale and the final count is obtained by taking an average of all the estimates. Additionally, a new training set augmentation scheme is developed to reduce redundancy among the training samples. In contrast to the earlier methods that use randomly cropped patches with high degree of overlap, the training set in this work is constructed using the four image quadrants as well as their horizontal flips ensuring no overlap. This technique avoids potential overfit when the network is continuously exposed to the same set of pixels during training, thereby improving the generalization performance of the network. In addition, the generalization performance of the proposed method is studied by measuring cross dataset performance.</p><p>Inspired by the superior results achieved by simultaneous learning of related tasks <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b103">104]</ref>, Sindagi et al. <ref type="bibr" target="#b91">[92]</ref> and Marsden et al. <ref type="bibr" target="#b65">[66]</ref> explored multi-task learning to boost individual task performance. Marsden et al. <ref type="bibr" target="#b65">[66]</ref> proposed a Resnet-18 <ref type="bibr" target="#b36">[37]</ref> based architecture for simultaneous crowd counting, violent behaviour detection and crowd density level classification. The network consists of initial 5 convolutional layers of Resnet18 including batch normalisation layers and skip connections form the primary module. The convolutional layers are followed by a set of task specific layers. Finally, sum of all the losses corresponding to different tasks is minimized. Additionally, the authors constructed a new 100 image dataset specifi- cally designed for multi-task learning of crowd count and behaviour. In a different approach, Sindagi et al. <ref type="bibr" target="#b91">[92]</ref> proposed a cascaded CNN architecture to incorporate learning of a highlevel prior to boost the density estimation performance. Inspired by <ref type="bibr" target="#b17">[18]</ref>, the proposed network simultaneously learns to classify the crowd count into various density levels and estimate density map (as shown in Fig. <ref type="figure" target="#fig_13">13</ref>). Classifying crowd count into various levels is equivalent to coarsely estimating the total count in the image thereby incorporating a high-level prior into the density estimation network. This enables the layers in the network to learn globally relevant discriminative features. Additionally, in contrast to most recent work, they make use of transposed convolutional layers to generate high resolution density maps. In a recent work, Kang et al. <ref type="bibr" target="#b44">[45]</ref> explored maps generated by density estimation methods for the purpose of various crowd analysis tasks such as counting, detection and tracking. They performed a detailed analysis of the effect of using full-resolution density maps on the performance of these tasks. They demonstrated through their experiments that full resolution density maps improved the performance of localization tasks such as detection and tracking. Two different approaches are considered for generating full-resolution maps. In the first approach, a sliding window based CNN regressor is used for pixel-wise density prediction. In the second approach, Fully Convolutional Networks <ref type="bibr" target="#b59">[60]</ref> along with skip connections are used to learning a non-linear mapping between input image and the corresponding density map.</p><p>In a slightly different application context of counting, Mundhenk et al. <ref type="bibr" target="#b68">[69]</ref> and Arteta et al. <ref type="bibr" target="#b3">[4]</ref> proposed to count different types of objects such as cars and penguins respectively. Mundhenk et al. <ref type="bibr" target="#b68">[69]</ref> addressed the problem of automated counting of automobiles from satellite/aerial platforms. Their primary contribution is the creation of a large diverse set of cars from overhead images. Along with the large dataset, they present a deep CNN-based network to recognize the number of cars in patches. The network is trained in a classification setting where the output of the network is a class that is indicative of the number of objects in the input image. Also, they incorporated contextual information by including additional regions around the cars in the training patches. Three different networks based on AlexNet <ref type="bibr" target="#b48">[49]</ref>, GoogLeNet <ref type="bibr" target="#b92">[93]</ref> and ResNet <ref type="bibr" target="#b36">[37]</ref> with Inception are evaluated. For a different application of counting penguins in images, Arteta et al. <ref type="bibr" target="#b3">[4]</ref> proposed a deep multi-task architecture for accurate counting even in the presence of labeling errors. The network is trained in a multi-task setting where, the tasks of foreground-background subtraction and uncertainty estimation along with counting are jointly learned. The authors demonstrated that the joint learning especially helps in learning a counting model that is robust to labeling errors. Additionally, they exploited scale variations and count variability across the annotations to incorporate scale information of the object and prediction of annotation difficulty respectively into the model. The network was evaluated on a newly created Penguin dataset.</p><p>Zhao et al. addressed a higher level cognitive task of counting people that cross a line in <ref type="bibr" target="#b112">[113]</ref>. Though the task is a videobased application, it comprises of a CNN-based model that is trained with pixel-level supervision maps similar to single image crowd density estimation methods, making it a relevant approach to be included in this article. Their method consists of a two-phase training scheme (as shown in Fig. <ref type="figure" target="#fig_4">14</ref>) that decomposes original counting problem into two sub-problems: estimating crowd density map and crowd velocity map where the two tasks share the initial set of layers enabling them to learn more effectively. The estimated crowd density and crowd velocity maps are then multiplied element-wise to generate the crowd counting maps. Additionally, they contributed a largescale dataset for evaluating crossing-line crowd counting algorithms, which includes 5 different scenes, 3,100 annotated frames and 5,900 annotated pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>With a variety of methods discussed in Section 3, we analyze various advantages and disadvantages of the broad approaches followed by these methods in this section.</p><p>Zhang et al. <ref type="bibr" target="#b106">[107]</ref> were among the first ones to address the problem of adapting models to new unlabelled datasets using a simple and effective method based on finding similar patches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T Fig. <ref type="figure" target="#fig_4">14</ref>: Overview of the method proposed by Zhao et al. <ref type="bibr" target="#b112">[113]</ref> for counting people crossing a line. across datasets. However, their method is heavily dependent on accurate perspective maps which may not be necessarily available for all the datasets. Additionally, the use of 72×72 sized patches for training and evaluation ignores global context which is necessary for accurate estimation of count. Walach et al. <ref type="bibr" target="#b96">[97]</ref> successfully addressed training inefficiencies in earlier methods using a layered boosting approach and a simple sample selection method. However, similar to Zhang et al. <ref type="bibr" target="#b106">[107]</ref>, their method involves patch-based training and evaluation resulting in loss of global context information along with inefficiency during evaluation due to the use of a sliding window approach. Additionally, these methods tend to ignore scale variance among the dataset assuming that their models will implicitly learn the invariance.</p><p>In an effort to explicitly model scale invariance, several methods involving combination of networks were proposed ( <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b10">11]</ref>). While these methods demonstrated significant improvements in the performance using multiple column networks and a combination of deep and shallow networks, the invariance achieved is limited by the number of columns present in the network and receptive field sizes which are chosen based on the scales present in the dataset. Additionally, these methods do not explicitly model global context information which is crucial for a task such as crowd counting. In a different approach, Marsden et al. <ref type="bibr" target="#b64">[65]</ref> attempt to address the scale issue by performing a multi-scale averaging during the prediction phase. While being simple and effective, it results in an inefficient inference stage. Additionally, these methods do not explicitly encode global context present in an image which can be crucial for improving the count performance. To this end, few approaches model local and global context <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b84">85]</ref> by considering key spatial and semantic information present in the image.</p><p>In an entirely different approach, few methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b91">92]</ref> take advantage of multi-task learning and incorporate high-level priors into the network. For instance, Sindagi et al. <ref type="bibr" target="#b91">[92]</ref> simultaneously learn density estimation and a high-level prior in the form of crowd count classification. While they demonstrated high performance gain by learning an additional task of crowd density level classification, the number of density levels is dataset dependent and it needs to be carefully chosen based on the density levels present in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets and results</head><p>A variety of datasets have been created over the last few years driving researchers to create models with better generalization abilities. While the earlier datasets usually contain low density crowd images, the most recent ones focus on high density crowd thus posing numerous challenges such as scale variations, clutter and severe occlusion. The creation of these large scale datasets has motivated recent approaches to develop methods that cater to such challenges. In this section, we review five key datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b110">111]</ref> followed by a discussion on the results of CNN-based approaches and recent traditional methods that were not included in the earlier surveys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>UCSD dataset: The UCSD dataset <ref type="bibr" target="#b14">[15]</ref> was among the first datasets to be created for counting people. The dataset was collected from a video camera at a pedestrian walkway. The dataset consists of 2000 frames of size 238×158 from a video sequence along with ground truth annotations of each pedestrian in every fifth frame. For the rest of the frames, linear interpolation is used to create the annotations. A region-ofinterest is also provided to ignore unnecessary moving objects such as trees. The dataset contains a total of 49,885 pedestrian instances and it is split into training and test set. While the training set contains frames with indices 600 to 1399, the test set contains the remaining 1200 images. This dataset has relatively low density crowd with an average of around 15 people in a frame and since the dataset was collected from a single location, there is no variation in the scene perspective across images.</p><p>Mall dataset: Considering little variation in the scene type in the UCSD dataset, Chen et al. in <ref type="bibr" target="#b19">[20]</ref> collected a new Mall dataset with diverse illumination conditions and crowd densities. The dataset was collected using a surveillance camera installed in a shopping mall. Along with having various density levels, it also has different activity patterns (static and moving crowds). Additionally, the scene contained in the dataset has severe perspective distortion resulting in large variations in size and appearance of objects. The dataset also presents the challenge of severe occlusions caused by the scene objects, e.g.stall, indoor plants along the walking path. The video sequence in the dataset consists of 2000 frames of size 320×240 with 6000 instances of labelled pedestrians. The first 800 frames are used for training and the remaining 1200 frames are used for evaluation. In comparison to the UCSD dataset, the Mall dataset has relatively higher crowd density images. However, both the datasets do not have any variation in the scene perspective across images since they are a part of a single continuous video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF CC 50 dataset:</head><p>The UCF CC 50 <ref type="bibr" target="#b40">[41]</ref> is the first truly challenging dataset constructed to include a wide range of densities and diverse scenes with varying perspective distortion. The dataset was created from publicly available web images. In order to capture diversity in the scene types, the authors collected images with different tags such as concerts, protests, stadiums and marathons. It contains a total of 50 images    <ref type="table" target="#tab_1">2</ref>. It can be observed that the UCSD and the Mall dataset have relatively low density images and typically focus on single scene type. In contrast, the other datasets have significant variations in the density levels along with different perspectives across images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discussion on results</head><p>Results of the recent traditional approaches along with CNNbased methods are tabulated in Table <ref type="table" target="#tab_2">3</ref>. The count estimation errors are reported directly from the respective original works. The following standard metrics are used to compare different methods:</p><formula xml:id="formula_2">MAE = 1 N N i=1 |y i -y i |,<label>(1</label></formula><p>)</p><formula xml:id="formula_3">MS E = 1 N N i=1 |y i -y i | 2 , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where MAE is mean absolute error, MSE is mean squared error, N is the number of test samples, y i is the ground truth count and y i is the estimated count corresponding to the i th sample. We make the following observations regarding the results:</p><p>• In general, CNN-based methods outperform the traditional approaches across all datasets. • While the CNN-based methods are especially effective in large density crowds with a diverse scene conditions, the traditional approaches suffer from high error rates in such scenarios. • Among the CNN-based methods, most performance improvement is achieved by scale-aware and context-aware models. It can be observed from Table <ref type="table" target="#tab_2">3</ref> that a reduction in count error is largely driven by the increase in the complexity of CNN models (due to addition of context and scale information). • While the multi-column CNN architecture <ref type="bibr" target="#b110">[111]</ref> achieves the state-of-the-art results on 3 datasets: UCSD, World-Expo '10 and ShanghaiTech, the CNN-boosting approach by <ref type="bibr" target="#b96">[97]</ref> achieves the best results on the Mall dataset. The best results on the UCF CC 50 dataset are achieved by joint local and global count approach <ref type="bibr" target="#b84">[85]</ref> and Hydra-CNN <ref type="bibr" target="#b69">[70]</ref>. • The work in <ref type="bibr" target="#b96">[97]</ref> suggests that layered boosting can achieve performances that are comparable to scale aware models. • The improvements obtained by selective sampling in <ref type="bibr" target="#b97">[98]</ref> and <ref type="bibr" target="#b96">[97]</ref> suggests that it helps to obtain unbiased performance. • Whole image-based methods such as Zhang et al. <ref type="bibr" target="#b110">[111]</ref> and Shang et al. <ref type="bibr" target="#b84">[85]</ref> are less computationally complex from the prediction point of view and they have proved to achieve better results over patch-based techniques. • Finally, techniques such as layered boosting and selective sampling <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b98">99]</ref> not only improve the estimation error but also reduce the training time significantly. It can be observed that though the method is able to accurate estimation of crowd count, the estimated density maps are of poor quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future research directions</head><p>Based on the analysis of various methods and results from Section 3 and 5 and the trend of other developments in computer vision, we believe that CNN-based deeper architectures will dominate further research in the field of crowd counting and density estimation. We make the following observations regarding future trends in research on crowd counting:  1. Given the requirement of large datasets for training deep networks, collection of large scale datasets (especially for extremely dense crowds) is essential. Though many datasets exist currently, only one of them (The UCF CC 50 <ref type="bibr" target="#b40">[41]</ref>) caters to large density crowds. However, the size of the dataset is too small for training deeper networks. Though Shanghai Tech <ref type="bibr" target="#b110">[111]</ref>) attempts to capture large density crowds, the number of images per density level is non-uniform with a large number of images available for low density levels and very few samples for high density levels (as shown in Fig. <ref type="figure" target="#fig_17">17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Considering the difficulty of training deep networks for</head><p>new scenes, it would be important to explore how to leverage from models trained on existing sources. Most of the existing methods retrain their models on a new scene and it is impractical to do so in real world scenarios as it would be expensive to obtain annotations for every new scene. Zhang et al. <ref type="bibr" target="#b106">[107]</ref> attempted to address this issue by performing a data driven training without the need of labelled data for new scenes. In an another approach, Liu et al. <ref type="bibr" target="#b58">[59]</ref> considered the problem of transfer learning for crowd counting. A model adaptation technique for Gaussian process counting model was introduced. Considering the source model as a prior and the target dataset as a set of observations, the components are combined into a predictive distribution that captures information in both the source and target datasets. However, the idea of transfer learning or domain adaptation <ref type="bibr" target="#b72">[73]</ref> for crowd scenes is relatively unexplored and is a nascent area of research. have certain disadvantages <ref type="bibr" target="#b42">[43]</ref>. Regressing on downsampled density maps using Euclidean loss results in low quality density maps. Fig. <ref type="figure" target="#fig_16">16</ref> demonstrates the results obtained using the state-of-the-art method <ref type="bibr" target="#b110">[111]</ref>. It can be observed that though accurate count estimates are obtained, the quality of the density maps is poor. As a result, these poor quality maps adversely affect other higher level cognition tasks which depend on them. Recent work on style-transfer <ref type="bibr" target="#b107">[108]</ref>, image de-raining <ref type="bibr" target="#b108">[109]</ref> and imageto-image translation <ref type="bibr" target="#b41">[42]</ref> have demonstrated promising results from the use of additional loss functions such as adversarial loss and perceptual loss. In principle, density estimation can be considered as an image-to-image translation problem and it would be interesting to see the effect of these recent loss functions. Generating high quality density maps along with low count estimation error would be another important issue to be addressed in the future. 5. Finally, considering advancements by scale-aware <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b69">70]</ref> and context-aware models <ref type="bibr" target="#b84">[85]</ref>, we believe designing networks to incorporate additional contextual and scale information will enable further progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This article presented an overview of recent advances in CNN-based methods for crowd counting and density estimation. In particular, we summarized various methods for crowd counting into traditional approaches (that use hand-crafted features) and CNN-based approaches. The CNN-based approaches are further categorized based on the training process and the network property. Obviously all the literature on crowd counting cannot be covered, hence, we have chosen a representative subset of the latest approaches for a detailed analysis and review. We also reviewed the results demonstrated by various traditional and CNN-based approaches to conclude that CNNbased methods are more adept at handling large density crowds with variations in object scales and scene perspective. Additionally, we observed that incorporating scale and contextual information in the CNN-based methods drastically improves the estimation error. Finally, we identified some of the most compelling challenges and issues that confront research in crowd counting and density estimation using computer vision and machine leaning approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of density map estimation. (a) Input image (b) Corresponding density map with count.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of various crowded scenes and the associated challenges. (a) Parade (b) Musical concert (c) Public demonstration (d) Sports stadium. High clutter, overlapping of subjects, variation in scale and perspective can be observed across images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Categorization of existing CNN-based approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Overview of cross scene crowd counting proposed by Zhang et al. [107].</figDesc><graphic coords="9,30.33,191.41,251.15,129.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Overview of learning to count using boosting by Walach and Wolf [97].</figDesc><graphic coords="9,337.10,60.55,175.81,184.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Overview of the end-to-end counting method proposed by Shang et al.<ref type="bibr" target="#b84">[85]</ref>. GoogLeNet is used to compute high-dimensional features which are further decoded into local counts using LSTM units.</figDesc><graphic coords="9,299.42,491.21,251.15,61.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Overview of counting method proposed by Boominathan et al.<ref type="bibr" target="#b10">[11]</ref>. A deep network is used in combination with a shallow network to address scale variations across images.</figDesc><graphic coords="10,30.33,60.56,251.15,75.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Overview of single image crowd counting via multi-column network by Zhang et al. [111].</figDesc><graphic coords="10,30.33,536.05,251.16,134.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Overview of Hydra-CNN by Onoro et al. [70].</figDesc><graphic coords="10,299.42,443.72,251.15,96.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Overview of Switching CNN by Sam et al. [82].</figDesc><graphic coords="11,30.33,193.66,251.14,223.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Overview of MoC (Mixture of CNN) for crowd counting by Kumagai et al. [50].</figDesc><graphic coords="11,324.54,123.54,200.91,148.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Overview of Fully Convolutional Network for crowd counting by Marsden et al. [65].</figDesc><graphic coords="12,55.44,60.55,200.93,191.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Overview of Cascaded Multi-task CNN by Sindagi et al. [92].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Sample images from various datasets. (a) UCSD [15] (b) Mall [20] (c) UCF CC 50 [41] (d) WorldExpo '10 [107] (e) Shanghai Tech Part A [111] (f) SHanghai Tech Part B [111]. It can be observed that in the case of UCSD and Mall dataset , the images come from the same video sequence providing no variation in perspective across images.</figDesc><graphic coords="14,30.33,185.34,84.79,324.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Results of Zhang et al. [111] on ShanghaiTech dataset. (a) Input image(b) Ground-truth density map (c) Estimated density maps.It can be observed that though the method is able to accurate estimation of crowd count, the estimated density maps are of poor quality.</figDesc><graphic coords="15,299.42,491.46,82.87,62.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 :</head><label>17</label><figDesc>Fig. 17: Distribution of crowd counts in ShanghaiTech dataset. It can be observed that the dataset is highly imbalanced.</figDesc><graphic coords="16,32.85,429.16,112.99,72.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>3 .</head><label>3</label><figDesc>Most crowd counting and density estimation methods have been designed for and evaluated either only on single images or videos. Combining the techniques developed separately for these methods is a non-trivial task. Development of low-latency methods that can operate in real-time for counting people in crowds from videos is another interesting problem to be addressed in future. 4. Another key issue ignored by earlier research is that the quality of estimated crowd density maps. Many existing CNN-based approaches have a number of max-pooling layers in their networks compelling them to regress on down-sampled density maps. Also, most methods optimize over traditional Euclidean loss which is known to A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Categorization of existing CNN-based approaches.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of various datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="6">No. of images Resolution Min Ave Max Total count</cell></row><row><cell>UCSD [15]</cell><cell>2000</cell><cell>158x238</cell><cell>11</cell><cell>25</cell><cell>46</cell><cell>49,885</cell></row><row><cell>Mall [20]</cell><cell>2000</cell><cell>320x240</cell><cell>13</cell><cell>-</cell><cell>53</cell><cell>62,325</cell></row><row><cell>UCF CC 50 [41]</cell><cell>50</cell><cell>Varied</cell><cell>94</cell><cell cols="2">1279 4543</cell><cell>63,974</cell></row><row><cell>WorldExpo '10 [106, 107]</cell><cell>3980</cell><cell>576x720</cell><cell>1</cell><cell>50</cell><cell>253</cell><cell>199,923</cell></row><row><cell>ShanghaiTech Part A [111]</cell><cell>482</cell><cell>Varied</cell><cell>33</cell><cell cols="2">501 3139</cell><cell>241,677</cell></row><row><cell>ShanghaiTech Part B [111]</cell><cell>716</cell><cell>768x1024</cell><cell>9</cell><cell>123</cell><cell>578</cell><cell>88,488</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of results on various datasets. The CNN-based approaches provide significant improvements over traditional approaches that rely on hand-crafted representations. Further, among the CNN-based methods, scale aware and context aware approaches tend to achieve lower count error.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">UCSD</cell><cell cols="2">Mall</cell><cell>UCF CC 50</cell><cell>WorldExpo '10</cell><cell>Shanghai Tech-A</cell><cell>Shanghai Tech-B</cell></row><row><cell>Approach type</cell><cell>Method</cell><cell cols="7">MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE</cell></row><row><cell></cell><cell>Multi-source multi-scale Idrees et al. [41]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>468.0 590.3</cell><cell></cell></row><row><cell>Traditional approaches</cell><cell>Cumulative Attributes Chen et al. [19] Density learning Lempitsky et al. [52] Count forest Pham et al. [75] Exemplar density Wang et al. [99]</cell><cell>2.07 1.7 1.61 1.98</cell><cell>6.86 4.40 1.82</cell><cell cols="2">3.43 17.07 2.5 10.0 2.74 2.10</cell><cell>493.4 487.1</cell><cell></cell></row><row><cell></cell><cell>Random projection forest Xu et al. [102]</cell><cell>1.90</cell><cell>6.01</cell><cell>3.22</cell><cell>15.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cross-scene Zhang et al. [107]</cell><cell>1.60</cell><cell>3.31</cell><cell></cell><cell></cell><cell>467.0 498.5</cell><cell>12.9</cell><cell>181.8 277.7</cell><cell>32.0</cell><cell>49.8</cell></row><row><cell></cell><cell>Deep + shallow Boominathan et al. [11]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>452.5</cell><cell></cell></row><row><cell></cell><cell>M-CNN Zhang et al. [111]</cell><cell>1.07</cell><cell>1.35</cell><cell></cell><cell></cell><cell>377.6 509.1</cell><cell>11.6</cell><cell>110.2 173.2</cell><cell>26.4</cell><cell>41.3</cell></row><row><cell>CNN-based approaches</cell><cell>CNN-boosting Walach and Wolf [97] Hydra-CNN Onoro et al. [70] Joint local &amp; global count Shang et al. [85] MoCNN Kumagai et al. [50] FCN Marsden et al. [65]</cell><cell>1.10</cell><cell></cell><cell>2.01 2.75</cell><cell cols="2">364.4 333.7 425.2 270.3 13.4 361.7 493.3 338.6 424.5</cell><cell>11.7</cell><cell>126.5 173.5 23.76 33.12</cell></row><row><cell></cell><cell>CNN-pixel Kang et al. [45]</cell><cell>1.12</cell><cell>2.06</cell><cell></cell><cell></cell><cell>406.2 404.0</cell><cell>13.4</cell></row><row><cell></cell><cell>Weighted V-LAD Sheng et al. [89]</cell><cell>2.86</cell><cell>13.0</cell><cell>2.41</cell><cell>9.12</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cascaded-MTL Sindagi et al. [92]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>322.8 341.4</cell><cell></cell><cell>101.3 152.4</cell><cell>20.0</cell><cell>31.1</cell></row><row><cell></cell><cell>Switching-CNN Sam et al. [82]</cell><cell>1.62</cell><cell>2.10</cell><cell></cell><cell></cell><cell>318.1 439.2</cell><cell>9.4</cell><cell>90.4</cell><cell>135.0 21.6</cell><cell>33.4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by US Office of Naval Research (ONR) Grant YIP N00014-16-1-3134.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling framework for optimal evacuation of large-scale crowded pedestrian facilities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelghany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abdelghany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mahmassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alhalabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="1105" to="1118" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowd management and urban design: New scientific approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Al-Kodmany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Urban Design International</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="282" to="295" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Crowd simulation modeling applied to emergency and evacuation simulations using multiagent systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Rosseti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Coelho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.4692</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The not-so-lonely crowd: Friendship groups in collective behavior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aveni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociometry</title>
		<imprint>
			<biblScope unit="page" from="96" to="99" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards an integrated approach to crowd analysis and crowd synthesis: A case study and first results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gorrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vizzari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="16" to="29" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The effectiveness of face detection algorithms in unconstrained crowd scenes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1020" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion pattern extraction and event detection for automatic visual surveillance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benabbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ihaddadene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page">163682</biblScope>
			<date type="published" when="2010">2010. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">System and method for videobased detection of drive-offs and walk-offs in vehicular and pedestrian queues</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Loce</surname></persName>
		</author>
		<idno>App. 14/279</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">652</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Collective behavior. New outline of the principles of sociology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Blumer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="166" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crowdnet: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Loreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical physics of social dynamics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">591</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social network model for crowd anomaly detection and localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Al Aghbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="266" to="281" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Counting people with low-level features and bayesian regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2160" to="2177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cascaded convolutional neural network for age estimation of unconstrained faces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on BTAS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Change Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person count localization in videos from noisy foreground and detections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1364" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human group action by layered model with multiple cues</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="124" to="135" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Waiting time in emergency evacuation of crowded public transport terminals</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Safety Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="844" to="857" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast crowd segmentation using shape indexing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zoghlami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision, IEEE</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2179" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance evaluation of crowd image analysis using the pets2009 dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for counting fish in fisheries surveillance video</title>
		<author>
			<persName><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Needle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference Workshop</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast crowd density estimation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hough forests for object detection, tracking, and action recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2188" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="2913" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mure: fast agent based crowd simulation for vfx and animation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arumugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kanyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lorenzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM SIG-GRAPH 2016 Talks, ACM</publisher>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.1038/229381a0</idno>
	</analytic>
	<monogr>
		<title level="j">The Statistics of Crowd Fluids</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="381" to="383" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dense crowd counting from still images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Congestion detection of pedestrians using the velocity entropy: A case study of love parade 2010 disaster</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="200" to="209" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-source multiscale counting in extremely dense crowd images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Crowd analysis using computer vision techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C S J</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Musse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="66" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10118</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Fully convolutional neural networks for crowd segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4464</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A case study on unconstrained facial recognition using the boston marathon bombings suspects</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Klontz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Michigan State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey on behavior analysis in video surveillance for homeland security applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Imagery Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>AIPR&apos;08. 37th IEEE</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumagai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09393</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Crowded scene analysis: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="367" to="386" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimation of number of people in crowded scenes using perspective transformation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">X</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="645" to="654" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Video analytics for retail business process monitoring</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Venetianer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Chosak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">975</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bayesian model adaptation for crowd counts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4175" to="4183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Crowd counting and profiling: Methodology and evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Simulation and Visual Analysis of Crowds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="347" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A study of pedestrian group behaviors in crowd evacuation based on an extended floor field cellular automaton model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the efficacy of texture analysis for crowd monitoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. SIBGRAPI&apos;98. International Symposium on</title>
		<meeting>SIBGRAPI&apos;98. International Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
	<note>Computer Graphics, Image Processing, and Vision</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Fully convolutional crowd counting on highly congested scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguiness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00220</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Resnetcrowd: A residual deep learning architecture for crowd counting, violent behaviour detection and crowd density level classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10698</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Busyness detection and notification method and system</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mongeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Loce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shreve</surname></persName>
		</author>
		<idno>App. 14/625</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">960</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The walking behaviour of pedestrian social groups and its impact on crowd dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moussaïd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Theraulaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2010">2010. 10047</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A mrf-based approach for real-time subway monitoring</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001. 2001. 2001</date>
			<biblScope unit="page">1034</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2001. Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Complexity, pattern, and evolutionary trade-offs in animal aggregation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Edelstein-Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="99" to="101" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Task-based crowd simulation for heterogeneous architectures, in: Innovative Research and Applications in Next-Generation High Performance Computing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rudomin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IGI Global</publisher>
			<biblScope unit="page" from="194" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2423" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DICTA&apos;09</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An evaluation of crowd counting methods, features and regression models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Detecting pedestrians by learning shapelet features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sabzmeydani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Recent survey on crowd density estimation and counting for visual surveillance</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Suandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ibrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Crowd behavior recognition for video surveillance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Concepts for Intelligent Vision Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="970" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Convolutional neural networks applied to house numbers digit classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3288" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">End-to-end crowd counting via joint learning local and global count</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1215" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Scene-independent group profiling in crowd</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Change Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2219" to="2226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Slicing convolutional neural network for crowd video understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5620" to="5628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Crowd counting via weighted vlad on dense attribute feature maps</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Crowd psychology and engineering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Sime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Safety science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting, in: Advanced Video and Signal Based Surveillance (AVSS)</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Pedestrian detection via classification on riemannian manifolds</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1299" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Fast visual object counting via example-based density estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="3653" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="247" to="266" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Crowd density estimation based on rich features and random projection forest</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">L0 regularized stationarytime estimation for crowd analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">iprivacy: image privacy protection by identifying sensitive objects via deep multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1005" to="1016" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Crowd analysis: a survey. Machine Vision and Applications 19</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Monekosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="345" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">2016a. Datadriven crowd understanding: A baseline for a large-scale crowd dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1048" to="1061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Multi-style generative network for real-time transfer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Collective motion and density fluctuations in bacterial colonies</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Swinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="13626" to="13630" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Segmentation and tracking of multiple humans in crowded environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1198" to="1211" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Crossing-line crowd counting with two-phase deep neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="712" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Learning collective crowd behaviors with dynamic pedestrian-agents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="50" to="68" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="2871" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Crowd tracking with dynamic evolution of group structures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Advances and trends in visual crowd analysis: A systematic survey and evaluation of crowd modelling techniques</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Al-Mualla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
