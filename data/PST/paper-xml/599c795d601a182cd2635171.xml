<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Smart Mining for Deep Metric Learning *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-27">27 Jul 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>Harwood</surname></persName>
							<email>ben.harwood@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
							<email>vijay.kumar@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Drummond</surname></persName>
							<email>tom.drummond@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Smart Mining for Deep Metric Learning *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-27">27 Jul 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1704.01285v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The development of deep metric learning models for the estimation of effective feature embedding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref> is at the core of many recently proposed computer vision methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. The main advantage of such models lies in their ability to automatically learn metric spaces, where samples from similar classes tend to be close together, while samples from different classes are more likely to be far from each other. The main scenario envisaged for such an approach involves * Vijay Kumar B G and Ben Harwood contributed equally to this work the presence of an extremely large number of classes (more than 10 5 classes) and low number of samples per class (in [10<ref type="foot" target="#foot_0">1</ref> , 10 2 ]), where the implementation of traditional classifiers becomes challenging <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Arguably, the most explored deep learning model that can estimate feature embedding is based on triplet networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, which are an extension of the siamese network <ref type="bibr" target="#b0">[1]</ref>. Triplet networks are composed of three identical convolutional neural networks (ConvNets) that are trained using triplets of samples: an anchor sample, a positive sample of the same class as the anchor, and a negative sample of a different class. The training procedure is based on a loss function that penalises large relative distances between the anchor and positive samples and small relative distances between the anchor and negative samples. Therefore, this training procedure relies on triplets that contain hard positive cases (anchor and positive samples that are far apart) and hard negative cases (anchor and negative samples that are close together). In other words, these hard samples will form triplets that produce gradients with sufficiently large magnitude. Assuming that a training set has N samples, then the set of triplets has complexity size O(N 3 ), which means that its formation is infeasible even for datasets of modest sizes (e.g., N = 10 5 ). This issue has lead to the implementation of importance sampling techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref> that stochastically under-samples the set of triplets. Here, their success relies on using enough samples to guarantee that a certain fraction of the hard positives and negatives are available for training 1 . Given the high complexity involved in finding hard positive and negative samples, another training procedure has been developed in order to guarantee training samples with large gradient magnitudes: the incorporation of loss functions that take into account the global structure of the embedding space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>In this paper, we propose a novel deep metric learning approach that combines a global <ref type="bibr" target="#b9">[10]</ref> and a triplet loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> computed using training samples acquired from a smart sampling method that has low computational com- plexity <ref type="bibr" target="#b4">[5]</ref> and can find effective training samples that produce gradients with large magnitude (see Fig. <ref type="figure" target="#fig_0">1</ref>). Essentially, our smart sampling method circumvents the importance sampling issue mentioned above, enabling our model to be robustly trained with more effective hard negatives and positives, and without the need for a stochastic undersampling of the training set. Additionally, we propose a novel adaptive controller that accelerates learning by monitoring training performance, estimating its own internal parameters and then automatically adjusting the smart sampling hyper-parameters. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review recent approaches for selecting hard positives and negatives for training triplet and siamese networks, methods that explore the global structure of the embedding space, and the approximate nearest neighbour search that forms the basis of our proposed method. As pointed out by Shrivastava et al. <ref type="bibr" target="#b16">[17]</ref>, hard negative and positive mining is a relabeling of the problem of bootstrappping <ref type="bibr" target="#b21">[22]</ref>, where the idea is to start the training of the embedding model with triplets containing positives and negatives that appear to be well separated, and gradually introduce more challenging positive and negative samples as we progress with the training. One of the major issues associated with this approach is on how to introduce such challenging samples -in particular: 1) how to effectively and efficiently sample the training set to select effective training samples, particularly considering that there are N 3 triplets from a training set containing N samples, and 2) what is the definition of challenging positive and negative samples.</p><p>Wang et al. <ref type="bibr" target="#b25">[26]</ref> described a way to build triplets based on a manual annotation of sample relevance. Using such relevance, the idea is to use importance sampling to build triplets, but this approach is limited by the fact that it needs those manual annotations. More recently proposed approaches rely on image label, such as the siamese network that gradually introduces hardest possible positive and negative samples <ref type="bibr" target="#b17">[18]</ref>. This is achieved by randomly sampling the training set for pairs of anchor and positive samples, and sorting these pairs in descending order with respect to the distance between the two samples in the embedding space. A similar approach is applied for pairs of anchor and negative samples, but the sorting is in ascending order. Then, the training pairs are formed by the top pairs in both lists. We use this sampling scheme as the hard mining baseline. Han et al. <ref type="bibr" target="#b3">[4]</ref> introduced an efficient reservoir sampling method to select positive and negative samples, but they do not apply any type of importance sampling to select challenging samples. In FaceNet, Schroff et al. <ref type="bibr" target="#b15">[16]</ref> introduced a triplet training approach, where pairs of anchor and positive samples are randomly selected, and pairs of anchor and negative samples are selected from a subset of the training set (i.e., the mini-batch in regular deep learning model training) using a criterion that selects "semi-hard" negatives: pairs of anchor and negative samples are selected if they are close, but at least farther than the anchor-positive pair. This semihard negative sampling improves the robustness of training by avoiding overfitting outliers in the training set. An efficient computation of the full matrix of pairwise distances of a subset of the training set (i.e., the mini-batch) allows Song et al. <ref type="bibr" target="#b20">[21]</ref> to design of a new loss function that integrates all positive and negative samples to form a lifted structured embedding. However, differently from our work, the lifted structured embedding only works for the mini-batch instead of the full training set.</p><p>The aforementioned issues present in the training of triplet models has motivated the development of approaches that explore the global structure of the embedding space. Kumar et al. <ref type="bibr" target="#b9">[10]</ref> proposed a global loss function that uses first and second order statistics to allow for robust training of triplet networks in an approach that improves the training robustness, but still relies on stochastic sampling of positive and negative samples. Ustinova and Lempitsky <ref type="bibr" target="#b23">[24]</ref> presented a loss function that minimises the integral of the product between the distribution of negative similarities and the cumulative density function of the positive similarities. Similarly, Song et al. <ref type="bibr" target="#b14">[15]</ref> introduced a loss function that optimises a global clustering quality metric (NMI). As shown by Kumar et al. <ref type="bibr" target="#b9">[10]</ref>, it appears that a combination of local and global losses can produce the most effective embedding spaces, so we believe that the last two approaches mentioned above <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref> still have room for improvement, but that improvement depends on more effective hard negative and positive sampling approaches.</p><p>In seeking a more effective approach to find hard triplets, we observe that hard negative mining (and to a lesser extent hard positive mining) can be framed as an instance of the well studied approximate nearest neighbour (ANN) search problem. In particular, when mining for negatives we are primarily interested in avoiding the computational cost of exhaustively searching through the entire training set. Fortunately, ANN search methods are able to trade-off a small decrease in nearest neighbour recall for large gains in computational efficiency. In the context of hard negative mining, a small set of nearest neighbours in the current embedding can be guaranteed to contain samples from at least two difference classes (due to training with very few samples per class). A FANNG (Fast Approximate Nearest Neighbour Graph) <ref type="bibr" target="#b4">[5]</ref> is a graph based index that can find these neighbourhoods quickly and with a very high rate of recall. Additionally, FANNGs are built in the full embedding space which allows the triplet selection to reuse exact distances that have been computed during the ANN search. FANNG provides state-of-the-art performance at high recall rates while adding only a single tuning parameter for the indexing quality and another for the ANN search quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We first describe the architecture of a triplet network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> and the loss function used in its training. Then, we describe the sampling method applied in the training process. Assume that the training set is represented by T = {(x i , y i )} N i=1 , with x i ∈ R n×n and y i ∈ {1, ..., C}. The feature embedding is denoted by f (x, θ f ), where f : R n ×R k → R m , with θ f ∈ R k representing the network pa-rameters (weight matrices, bias vectors, and normalisation parameters). The triplet network comprises three identical deep convolutional neural networks (ConvNet) containing L layers, each defined by:</p><formula xml:id="formula_0">f (x, θ f ) = f out • r L • h L • f L • ... • r 1 • h 1 • f 1 (x), (1)</formula><p>where θ f is defined above, f l (.) denotes the linear transforms, h l (.) represents a normalisation function, and r l (.) denotes a non-linear activation function (e.g., ReLU <ref type="bibr" target="#b13">[14]</ref>). Also in <ref type="bibr" target="#b0">(1)</ref>, note that f l = [f l,1 , ..., f l,n l ] represents an array of n l pre-activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet Networks</head><p>The triplet network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> is trained with an input composed of an anchor point x i (from class y i ), another point from the same class x + i = x j (with i = j and y i = y j ), and a point from a different class x − i = x k ((with k = i and y i = y k ). The loss function for each triplet is defined by:</p><formula xml:id="formula_1">J t (x i , x + i , x − i , θ f ) = max 0, 1 − f (1) (x i , θ f ) − f (3) (x − i , θ f ) 2 f (1) (x i , θ f ) − f (2) (x + i , θ f ) 2 + m ,<label>(2)</label></formula><p>where m is the margin, x + i and x i belong to the same class, x − i and x i are from different classes, and f (1) (.), f (2) (.) and f (3) (.) are constrained to be the same network parameterized by θ f .</p><p>The training of the triplet network can be made more robust with the introduction of a loss that explores the global structure of the embedding <ref type="bibr" target="#b9">[10]</ref>. In particular, the triplet loss in (2) can be extended with a global loss that assumes that the distribution of distances between anchor and positive samples and anchor and negative samples follow a Gaussian distribution. This global loss aims to: 1) minimise the variance of the two distributions, 2) minimise the mean value of the distances between anchor and positive samples, and 3) maximise the mean value of the distances between anchor and negative samples, as follows:</p><formula xml:id="formula_2">J g ({x i } N i=1 ,{x + i } N i=1 , {x − i } N i=1 , θ f ) = (σ 2+ + σ 2− ) + λ max 0, µ + − µ − + t ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">µ + = N i=1 d + i /N, µ − = N i=1 d − i /N , σ 2+ = N i=1 (d + i − µ + ) 2 /N, σ 2− = N i=1 (d − i − µ − ) 2</formula><p>/N , with µ + and σ 2+ denoting the mean and variance of the matching pair distance distribution, µ − and σ 2− representing the mean and variance of the non-matching pair distance distribution,</p><formula xml:id="formula_4">d + i = f (1) (xi,θ f )−f (2) (x + i ,θ f ) 2 2 4 , d − i = f (1) (xi,θ f )−f (3) (x − i ,θ f ) 2 2 4</formula><p>, λ is a term that balances the importance of each term, t is the margin between the mean of the matching and non-matching distance distributions and N is the size of the training set. Note in (3), that we assume a triplet network (i.e., f (1) (.), f (2) (.) and f (3) (.) are the same network), where the squared Euclidean distances of the matching and non-matching pairs of the i th triplet are constrained to be 0 ≤ d + i , d − i ≤ 1 because of the division by 4, and the normalisation layer enforces that the norm of the embedding is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Smart Mining</head><p>As discussed in Sec. 2, semi-hard mining has proved an effective method for training triplet networks <ref type="bibr" target="#b15">[16]</ref> with the primary aim of finding sets of triplets that will continue to progress the training of the network. Naively, this can be achieved by selecting triplets that provide the greatest violation of the triplet constraint. For instance, given an anchor x i , the hardest positive is defined as</p><formula xml:id="formula_5">x + i = arg max (xj ,yj )∈T ,xj =xi,yj =yi f (1) (x i , θ f ) − f (2) (x j , θ f ) 2 2 ,<label>(4)</label></formula><p>and the hardest negative as</p><formula xml:id="formula_6">x − i = arg min (xj ,yj )∈T ,xj =xi,yj =yi f (1) (x i , θ f ) − f (3) (x j , θ f ) 2 2 .</formula><p>(5) In order to avoid the costly arg max over the entire training set, semi-hard mining is instead commonly performed over the stochastic subset of samples used in each minibatch <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref>. This method has the additional advantage of avoiding repeated attempts at learning from the hardest triplets that may never improve from epoch to epoch.</p><p>We define a novel off-line mining strategy that consists of first finding a set of approximate nearest neighbours S ⊂ T . Then, for all triplets with anchor x i the set of neighbours S i is used to determine appropriate positive and negative samples. To avoid mining poorly structured regions of the embedding, we limit our selection of negative samples to only include negatives where there is at least one positive sample that is closer to the anchor than the negative is. Positive samples are then chosen to guarantee a non-zero response from the loss function <ref type="bibr" target="#b1">(2)</ref>.</p><p>More formally, we define a smart negative as any negative sample x − i ∈ S i such that</p><formula xml:id="formula_7">f (1) (x i , θ f )−f (3) (x − i , θ f ) 2 2 &gt; κ • f (1) (x i , θ f ) − f (2) (x +N N i , θ f ) 2 2 ,<label>(6)</label></formula><p>where κ is a global tuning variable and x +N N i is the closest positive to x i (note that this is not the positive used to form the triplet). The relationship between the exclusion boundary, the anchor, positive samples and negative samples can be seen in Figure <ref type="figure" target="#fig_1">2</ref>. Mining outside the region defined by the distance between an anchor x i and the closest positive sample x +N N i ties the selection of negatives to how tightly the class y i is currently clustered in the embedding space. Additionally, the global parameter κ provides a tunable scaling factor for the radius of these hyper-spherical exclusion boundaries centred on each anchor. Experimentally we have found that the best results are achieved by beginning training with a larger value for κ and then gradually relaxing this constraint throughout the training. This allows previously excluded negatives to be selected for training during later epochs either because the positive neighbours have formed a tighter neighbourhood or the global exclusion value has been sufficiently reduced. The practical details of implementing this mining scheme are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Implementing Smart Mining with FANNG</head><p>At the beginning of each training epoch we perform a full forward pass of the training set T to generate the current feature embedding f (x, θ f ). The indexing graph used in FANNG <ref type="bibr" target="#b4">[5]</ref> is then constructed using the traverse-add algorithm (Alg. 4 in <ref type="bibr" target="#b4">[5]</ref>) with the embedding of each element of T forming a vertex in the graph. At each vertex, a list of outbound edges connect to un-occluded neighbours in a way that approximates the local surface structures of a lower dimensional manifold. Experimental results show that the order of these edge lists remains low (between 15-30 edges) and is independent of the size of the data set and the extrinsic dimensionality of the embedding space. The newly formed traversable graph enables the computationally efficient collection of the approximate nearest neighbour set S.</p><p>As described in <ref type="bibr" target="#b4">[5]</ref>, the traverse-add algorithm can be repeatedly applied until a specified percentage success rate is reached. Once our target build percentage of 98% is achieved, our approach diverges from the original building process for FANNGs. Rather than applying the backtrack search (Alg. 3 in <ref type="bibr" target="#b4">[5]</ref>) to further refine the graph, we instead use the same backtrack search algorithm to immediately generate the approximate nearest neighbour set S. Since the graph vertices provide a complete index of the training samples, we can compute each neighbour list S i by passing the vertex f (x i , θ f ) to the backtracking search algorithm as both the query vector and starting point for the search. Because the collection of these neighbour lists does not modify the indexing graph, the searches can be performed in parallel. Each query returns a pre-specified number of nearest neighbours sorted in ascending order by distance from the query vertex, as well as the distances themselves. The size of the neighbour lists is selected to guarantee that both positive and negative samples will always be seen in the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Triplet Construction</head><p>Once S is computed, the class label information y is used to separate the neighbours into several lists. We perform a single iterative pass over each neighbour list S i while maintaining a count of samples from class y i and a count of all samples from outside that class. Once the first positive sample has been found, the exclusion boundary is computed. Then any future negative samples that satisfy (6) are added to the list of valid negatives. Each subsequent positive sample is added to the list of valid positives along with the current number of valid negatives. With this information we can ensure that a positive sample is not put into a triplet with a later negative sample that is further from the anchor. Lastly, to construct each mined triplet in the current epoch, we take the first unused negative from the list of valid negatives associated with the triplets anchor as well as the first valid positive that is also valid for the chosen negative. Random triplets are used in rare cases where there are no valid negatives. If there are no valid positive samples associated with the chosen negative, then a positive sample is uniformly selected from the set T \N i . Algorithm 1 illustrates this triplet selection process in pseudo-code. It is important to note that while each negative is used no more than once for a given anchor during any given epoch, positive samples can be used multiple times with the same anchor. However, the unique negatives will always ensure that no triplets are repeated. In general, our method will select softer negative and positive samples ahead of harder options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Runtime Complexity</head><p>A naive hard mining algorithm that selects O(N ) triplets will have a worst case complexity of O(N  The smart mining algorithm requires the construction of a nearest neighbour index. Exhaustive index construction is O(N 3 ) due to the sorting of all N 2 pairwise distances. However, we can guarentee a worst case complexity of O(N 2 ) by instead building an approximation of the index. Using this index to find negatives up to the closest positive sample for each anchor can be performed with worst case complexity O(N 2 ) regardless of class distribution. Given that O(N 2 ) is the best case complexity for the naive hard mining approach above, we can conclude that our method is computationally more efficient. ). For comparitive purposes, we note that larger minibatches (i.e. smaller M) tend to reduce training error <ref type="bibr" target="#b17">[18]</ref> up until performance begins to be limited by the naive use of argmax. Even so, as M → 1 the semi-hard mining complexity approaches O(N 2 ) and the information available in each minibatch also approches that of both the naive and smart mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Automatic Parameter Selection</head><p>Up to this point, running our mining scheme requires hand tuning for the hyper-parameter κ. We propose a more robust solution that closes the loop on the triplet mining and training losses. At the beginning of each epoch, we would like to estimate what value of κ will produce triplets of a suitable difficulty for the current network. One such goal could be to ensure that the error from the training set is consistent with the current error of the validation set. We estimate κ with a simple linear model</p><formula xml:id="formula_8">κ = αe + β,<label>(7)</label></formula><p>that finds the least-squares solution for internal parameters α and β from a vector of recent training errors e and their associated κ. Once we have computed the internal parameters, we can obtain the estimated value As training progresses and the embedding improves, it is expected that both the training and validation error will decrease. Targeting a low training error will guarantee that most of the next epoch is spent on triplets that will not make a significant impact on the training. So instead, we can deliberately separate the training and validation errors so that the training error is kept high, while the validation error continues to decrease. To achieve this, we replace the use of the current validation error with a constant value that represents our target training error. Experimental results have shown that a target of between 50% and 75% training error is capable of producing more accurate embeddings in far fewer epochs. To maintain a high training error, it is best to use batches that are 50% to 100% mined triplets.</p><formula xml:id="formula_9">κ = αe t + β,<label>(8)</label></formula><p>A comparison of hand tuned and adaptive parameter selection can be seen in figure <ref type="figure" target="#fig_5">3</ref>. Training error gives an indication of the fraction of each batch that is producing a nonzero gradient and so can continue to shape the embedding space. The validation error is produced by evaluating the embedding with a reserved set of samples not used for training and is used as an inverse measure of the current quality of the embedding. Since the adaptive method is able to select harder triplets, while avoiding triplets that are so hard that the embedding structure could be damaged, we see that it can produce a higher quality embedding. Additionally, the steeper decent of the adaptive validation indicates that these results can be reached while also using fewer training epochs. In practice when using GPU accelerated code, our triplet selection accounts for less than 1% of the total epoch runtime (the majority of the cost being the forward and backward propagation of the selected triplets). As such, the ability to produce high quality embeddings while converging in comparatively fewer epochs will greatly reduce the overall training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>For the experiments, we follow the protocol used in previous papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15]</ref>, which uses unseen classes from the CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref> datasets in order to assess the clustering quality and k nearest neighbour retrieval <ref type="bibr" target="#b7">[8]</ref>. Our proposed method combining triplet and global losses using FANNG <ref type="bibr" target="#b4">[5]</ref> with and without automated hyper-parameter selection (i.e., the adaptive controller) is compared with the following state-of-the-art deep metric learning approaches: (1) triplet learning with semihard negative mining <ref type="bibr" target="#b15">[16]</ref> (with and without FANNG <ref type="bibr" target="#b4">[5]</ref>), (2) lifted structured embedding <ref type="bibr" target="#b20">[21]</ref>, (3) N-pairs metric loss <ref type="bibr" target="#b19">[20]</ref>, (4) clustering <ref type="bibr" target="#b14">[15]</ref>, and (5) triplet combined with global loss <ref type="bibr" target="#b9">[10]</ref>. For the approaches (1), ( <ref type="formula" target="#formula_1">2</ref>), ( <ref type="formula" target="#formula_2">3</ref>) and ( <ref type="formula" target="#formula_5">4</ref>) above, we report the results from Song et al.'s paper <ref type="bibr" target="#b14">[15]</ref>. For the remaining approaches (i.e. our proposed method, and ( <ref type="formula">5</ref>) ), we use the same training and test set split described in <ref type="bibr" target="#b20">[21]</ref> across all datasets. Specifically, the means CUB200-2011 <ref type="bibr" target="#b24">[25]</ref> has 11, 788 images of 200 bird species, from which we take the first 100 species for training and use the remaining 100 species for testing. Cars196 <ref type="bibr" target="#b8">[9]</ref> has 16, 185 images from 196 car models, from which we take the first 98 classes for training and use the remaining 98 for testing. For all our experiments, we initialize the network with pre-trained GoogLeNet <ref type="bibr" target="#b22">[23]</ref> weights and randomly initialize the final fully connected layer similar to <ref type="bibr" target="#b20">[21]</ref>. We set the embedding size to 64 <ref type="bibr" target="#b20">[21]</ref> and the learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type="bibr" target="#b20">[21]</ref>.</p><p>For the experiments using triplet combined with global loss <ref type="bibr" target="#b9">[10]</ref> and for our proposed approach, we let the training procedure run for a maximum of 20 epochs or until convergence (if fewer epochs were required). During the first two epochs, triplet mining was completely disabled to allow for batches comprised of only random triplets. Similar to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, we set the margin for the triplet and global loss to 0.2 and 0.01 respectively. We start experiment with an initial learning rate of 0.1 and gradually decrease it by a factor of 2 after every 3 epochs. We use a weight decay of 0.0005 for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Results</head><p>Here we report quantitative results based on the normalised mutual information (NMI) <ref type="bibr" target="#b10">[11]</ref> score defined by the ratio of mutual information and product of entropies for two clustering assignments -this measures the label agreement between these two clustering assignments (ignoring the permutations). We also report the k nearest neighbour performance using the Recall@K metric <ref type="bibr" target="#b14">[15]</ref>.</p><p>Tables <ref type="table">1 and 2</ref> show the NMI and k nearest neighbour performance with the Recall@K metric results defined above comparing our method to the state of the art for the datasets CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>. From these tables, we can first see that Triplet + FANNG significantly improves upon the Semi-hard <ref type="bibr" target="#b15">[16]</ref> results with respect to all measures, and showing that the smart mining process using FANNGs is more effective than the more commonly used stochastic under-sampling of the training set. The combination of Triplet + FANNG + Global shows gains with respect to all measures, when compared to Triplet + Global and Triplet + FANNG, demonstrating the importance of both the smart mining process and the use of a global loss. The final model Triplet + FANNG + Global + Adaptive shows competitive results with respect to all measures as well as a much faster convergence rate (see Fig. <ref type="figure" target="#fig_6">4</ref>). For instance, for the CUB-200-2011 dataset <ref type="bibr" target="#b24">[25]</ref>, Triplet + FANNG + Global + Adaptive converges in just Table <ref type="table">1</ref>. Clustering and recall performance on CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref>. Our proposals are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>NMI R@1 R@2 R@4 R@8</p><p>Semi-hard <ref type="bibr" target="#b15">[16]</ref> 55   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>Figures <ref type="figure" target="#fig_8">5 and 6</ref> show triplets for visual inspection. The first column of each figure contains randomly selected anchor points from the training set. Each row then contains the positive and negative sample images that complete each of the triplets. For each of the mined triplets, the negative sample is guaranteed to be a shorter distance from the anchor when compared to the positive sample in the embedding being mined. As per our smart mining algorithm, each of the mined positives is the closest possible positive to the anchor, while still maintaining distance relationships. These properties can be clearly seen when the mined triplets are compared to a randomly generated triplet. The anchors of the mined triplets appear to have stronger similarities with the negative samples, while the random triplet anchors are much closer in appearance to the positive samples. While the mined positive samples are dissimilar from the anchors, in many cases it appears that they are still sharing more features with the anchor than the random positives are sharing with the same anchor. By presenting difficult (but not impossible) triplets more often, our mined triplets enable faster learning of the embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>From the results in Tables 1-2, we see that Triplet + FANNG + Global + Adaptive significantly outperforms the current state of the art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> in terms of clustering and recall performances. Furthermore, it is worth noting that Triplet + FANNG performs substantially better than its counterpart Semi-hard <ref type="bibr" target="#b15">[16]</ref> with respect to the clustering and recall performances, thus highlighting the importance of the smart mining process. Comparing Triple + FANNG + Global and Triple + FANNG, we can conclude that the global loss is indeed an important component in improving the clustering and recall performance of the embedding. Finally, Triplet + FANNG + Global + Adaptive and Triplet + FANNG + Global show almost equally strong results, but the former has a significantly faster training process.</p><p>In this paper, we proposed a novel triplet-based deep metric learning approach that combines a global structure loss with a triplet loss. We rely on a smart mining process to train our approach, which allows an effective selection of training samples at a low computational cost. Furthermore, we also extend this smart mining with an adaptive controller that automatically selects its hyper-parameters throughout training. By searching the entire training set, we pay a high upfront cost, but make good use of the extra available information to ultimately improve the convergence rate of the training process without compromising on the quality of the embedding. Using CUB-200-2011 <ref type="bibr" target="#b24">[25]</ref> and Cars196 <ref type="bibr" target="#b8">[9]</ref>, we show that our proposed method achieves fast and more accurate training of triplet ConvNets than other competing mining methods. Our approach sets new state-of-the-art deep metric learning results for these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Effect of Parameters on the Embedding</head><p>In this section, we evaluate the performance of our proposed smart mining method with respect to various parameter settings. Note that in all our experiments (including the ones in the paper), we initialize the network with pretrained GoogleNet weights <ref type="bibr" target="#b22">[23]</ref> and randomly initialize the final fully connected layer similar to <ref type="bibr" target="#b20">[21]</ref> . The learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Effect of Scaling Parameter κ on the Embedding</head><p>We define smart triplets as those that satisfy Eq. 6, where κ is a global scaling factor that decides the radius of the hyper-spherical exclusion boundary centred around the anchor. In this sub-section, we show the effect of κ on the feature embedding.  <ref type="figure" target="#fig_9">7</ref> shows that the performance degrades for smaller values of κ. This is due to hard triplets generated by the mining algorithm. For large values of κ, there are fewer smart triplets returned by the approximate nearest neighbor search, so random triplets are used instead. In the latter case, the behavior of the method tends to be similar to that of the Triplet + Global.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Effect of the Percentage of Mined Triplets for Training</head><p>Figure <ref type="figure">8</ref> shows the effect of varying the percentage of mined triplets for training on the CUB-200-2011 dataset. We train Triplet + FANNG + Adaptive networks for 20 epochs using a target training error of 0.5 and with the percentage of mined triplets varying from 10% to 60% in 10% increments. For these experiments the global loss has been disabled so that the training error is a result of only the triplet losses. At the lower percentages, there are insufficient mined triplets to properly control the training error and accelerate the training. From 40% mined triplet and beyond, there are enough mined triplets to allow for control the training error and so the performance begins to saturate at this level. As such, we find that a percentage of anywhere between 50% to 100% mined triplets is sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Visualizing Embedding using t-SNE</head><p>Fig. <ref type="figure">9</ref> shows the Barnes-Hut t-SNE visualisation of the learned embedding space obtained by mapping the CUB-200-2011 test image features to a two-dimensional space. Although, there is no overlap between the train and test classes, the images from the test classes are clustered well. . R@1 vs κ (top left), R@2 vs κ (top right), R@4vs κ (bottom left), R@8 vs κ (bottom right)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Sample Mined Triplets using FANNG</head><p>The images in Figure <ref type="figure" target="#fig_0">10</ref> are triplets from randomly selected anchor points while training Triplet + FANNG + Adaptive on the CUB-200-2011 dataset. Similar to the experiments in Section 6.2, we are interested in showing only the learnning resulting from the triplet mining and as such global loss is disabled. At epochs 4, 8, 12 and 16 the first triplet formed for each of the chosen anchor points was recorded. Beginning with the epoch 4 images, visual inspection shows that the mined negative samples share distinct visual traits with the anchor image and hence they are already much harder than random negatives. Beyond epoch 4, the mined negatives continue to become more difficult as the embedding is refined. In particular, many of the negative images at Epoch 16 could easily be mistaken as coming from the same class as the anchor image. The appearence of the positive samples is largly constrained by the negatives, since our method always selects the softest positive that is also still harder that the chosen negative. This selec-tion process can be seen in the way each positive-negative pair share many distinctive visual traits such that they are roughly the same distance from the anchor point. However, in some cases the negative and positive samples could be in very different directions from the anchor, and so visually judging the similar level of difficulty is much more difficult across different regions of the embedding.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our proposed deep metric learning model that combines a triplet and a global loss and uses a smart sampling procedure that is capable of quickly searching the entire training set to select effective training samples. The hyper-parameters of the smart sampling are automatically estimated by the proposed adaptive controller with the goal of accelerating the training process.</figDesc><graphic url="image-1.png" coords="2,74.86,72.00,445.48,172.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A simplified 2-dimensional projection of the neighbours for anchor xi, here Si contains two positive and four negative samples. Distance d(x, y) is squared Euclidean. a) The current κ and clustering of class yi specify an exclusion boundary containing all negatives samples, as such none are currently deemed suitable for training. b) On a subsequent epoch a smaller κ and tighter class clustering now yeilds a negative sample outside the exclusion boundary. This negative and the positive sample further outside the exclusion boundary are used to complete a triplet that is guaranteed to violate the triplet constraint.</figDesc><graphic url="image-2.png" coords="4,344.30,72.00,165.38,72.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>22 for 25 t</head><label>2225</label><figDesc>] ← x i , neg[0], random positive / ∈ pos each positive pos[k] do 23 if neg[0] / ∈ validRange(pos[k]) then 24 continue [j] ← x i , neg[0], pos[k] As C → N , this complexity reduces to a best case of O(N 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 )</head><label>2</label><figDesc>For semi-hard mining, such as<ref type="bibr" target="#b15">[16]</ref>, algorithmic complexity is reduced by limiting triplet selection to a brute force search within each minibatch. Given an epoch with M minibatches, the argmax for each anchor results in a total complexity of O(M N M , or simply O( N 2 M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>by providing the current target error e t . The model is initialised at the beginning of the third training epoch with an initial estimate of the internal parameters. At the beginning of the triplet mining on each subsequent epoch, the training results from the previous epoch are used to update the model. The inclusion of as little as 2% mined triplets per batch is enough to control the training losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A comparison of training performance using hand tuned and adaptive selection of κ. Training and validation error is shown for the first 20 epochs while training on CUB-200-2011 [25].</figDesc><graphic url="image-3.png" coords="6,344.30,72.00,165.37,130.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A comparison of the convergence rate of our different methods using Recall@1 on CUB-200-2011 dataset (left) and Cars196 (right).</figDesc><graphic url="image-4.png" coords="7,313.35,470.65,111.03,88.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. a) Triplets mined from the CUB-200-2011 [25] training set using FANNG [5]. b) Random triplets constructed with the same anchor.</figDesc><graphic url="image-6.png" coords="8,79.64,72.00,177.19,239.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. a) Triplets mined from the Cars196 [9] training set using FANNG [5]. b) Random triplets constructed with the same anchor.</figDesc><graphic url="image-7.png" coords="8,338.39,72.00,177.19,239.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7</head><label>7</label><figDesc>Figure 7. R@1 vs κ (top left), R@2 vs κ (top right), R@4vs κ (bottom left), R@8 vs κ (bottom right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .Figure 9 .Figure 10 .</head><label>8910</label><figDesc>Figure 8. Training error vs epoch (top left), NMI vs epoch (top right), R@1vs epoch (bottom left), R@8 vs epoch (bottom right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-8.png" coords="13,50.11,193.94,495.00,371.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="14,87.24,72.00,420.75,315.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="14,87.24,391.14,420.75,315.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3 ) on any given epoch. Assuming that the samples are equally distributed Algorithm 1: Triplet Selection Input: Training samples X, Nearest neighbours S, Class labels y, Scale parameter κ Output: Mined triplets T 1 for each sorted neighbour list s i do</figDesc><table><row><cell>2</cell><cell>neg ← empty list of negatives</cell></row><row><cell>3</cell><cell>pos ← empty list of positives/valid negative range</cell></row><row><cell>4</cell><cell>for each neighbour s i [j] of sample x i do</cell></row><row><cell>5</cell><cell>if isEmpty(pos) then</cell></row><row><cell>6</cell><cell>if class(s i [j]) = y i then</cell></row><row><cell>7</cell><cell>continue</cell></row><row><cell>8</cell><cell>bound ← κ • distance(x i , s i [j])</cell></row><row><cell>9</cell><cell>pos.add(s i [j], ∅)</cell></row><row><cell>10</cell><cell>continue</cell></row><row><cell>11</cell><cell>if distance(x i , s i [j]) &lt; bound then</cell></row><row><cell>12</cell><cell>continue</cell></row><row><cell>13</cell><cell>if class(s i [j]) = y i then</cell></row><row><cell>14</cell><cell>neg.add(s i [j])</cell></row><row><cell>15</cell><cell>if class(s i [j]) = y i then</cell></row><row><cell>16</cell><cell>pos.add(s i [j], clone(neg))</cell></row></table><note>17 for each triplet t[j] with anchor x i do 18 if isEmpty(neg) then 19 t[j] ← random triplet</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Semi-hard<ref type="bibr" target="#b15">[16]</ref> 53.35 51.54 63.78 73.52 82.41 Lifted Structure [21] 56.88 52.98 65.70 76.01 84.27 N-pairs [20] 57.79 53.90 66.76 77.75 86.35 Triplet + Global [10] 58.20 61.41 72.51 81.75 88.39 Clustering [15] 59.04 58.11 70.64 80.27 87.81 Triplet + FANNG 58.24 56.11 68.34 77.99 85.92</figDesc><table><row><cell></cell><cell>.38 42.59 55.03 66.44 77.23</cell></row><row><cell cols="2">Lifted Structure [21] 56.50 43.57 56.55 68.59 79.63</cell></row><row><cell>N-pairs [20]</cell><cell>57.24 45.37 58.41 69.51 79.49</cell></row><row><cell cols="2">Triplet + Global [10] 58.61 49.04 60.97 72.33 81.85</cell></row><row><cell>Clustering [15]</cell><cell>59.23 48.18 61.44 71.83 81.92</cell></row><row><cell>Triplet + FANNG</cell><cell>58.10 45.90 57.65 69.63 79.83</cell></row><row><cell>Triplet + FANNG + Global</cell><cell>60.09 49.44 61.60 73.09 82.85</cell></row><row><cell>Triplet + FANNG + Global + Adaptive</cell><cell>59.90 49.78 62.34 74.05 83.31</cell></row><row><cell cols="2">Table 2. Clustering and recall performance on Cars196 [9]. Our</cell></row><row><cell>proposals are highlighted</cell><cell></cell></row><row><cell>Method</cell><cell>NMI R@1 R@2 R@4 R@8</cell></row><row><cell>Triplet + FANNG + Global</cell><cell>59.70 64.20 75.22 83.24 88.94</cell></row><row><cell>Triplet + FANNG + Global + Adaptive</cell><cell>59.50 64.65 76.20 84.23 90.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To this end, we run the experiments on CUB-200-2011 dataset for different initial values of κ ∈ {1, 4, 16, 64}. We use Triplet + FANNG + Global as the loss function and report the recall values at 1, 2, 4 and 8 at the end of 20 th epoch. Fig.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We have not found a formal study that describes the number of samples used for training versus the fraction of hard positive/negatives.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>This research was supported by the Australian Research Council through the Cen-tre of Excellence in Robotic Vision, CE140100016, and through Laureate Fellowship FL130100102 to IDR. We would like thank Guosheng Lin and Chunhua Shen for their insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple instance metric learning from automatically labeled bags of faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="634" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fanng: Fast approximate nearest neighbour graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2016. 2, 3, 4, 5, 6, 8</date>
			<biblScope unit="page" from="5713" to="5722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.07737" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6622</idno>
		<title level="m">Deep metric learning using triplet network</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
				<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2016. 1, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge university press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Descriptor learning for omnidirectional image matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Migliore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Registration and Recognition in Images and Videos</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.07464" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
				<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2008">July 2017. 1, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2015. 1, 2, 3, 4, 6, 7, 8</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2015. 1, 2, 4, 6</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class npair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2016. 1, 2, 6</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning and example selection for object and pattern detection</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.05720" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast training of triplet-based deep binary embedding networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
