<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance-Driven Hand-Drawn Animation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ian</forename><surname>Buck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Jacobs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Allison</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Seims</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University † University of Washington ‡ Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Performance-Driven Hand-Drawn Animation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F7EE6DC54D62CE95E04A7F02FBEA539</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms</term>
					<term>I.6.3 [Computer Graphics]: Methodology and Techniques-Interaction Techniques</term>
					<term>I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Tracking Animation, Non-photorealistic rendering, Image morphing, Face tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method for generating performance-driven, "hand-drawn" animation in real-time. Given an annotated set of hand-drawn faces for various expressions, our algorithm performs multi-way morphs to generate real-time animation that mimics the expressions of a user. Our system consists of a vision-based tracking component and a rendering component. Together, they form an animation system that can be used in a variety of applications, including teleconferencing, multi-user virtual worlds, compressed instructional videos, and consumer-oriented animation kits. This paper describes our algorithms in detail and illustrates the potential for this work in a teleconferencing application. Experience with our implementation suggests that there are several advantages to our hand-drawn characters over other alternatives: (1) flexibility of animation style; (2) increased compression of expression information; and (3) masking of errors made by the face tracking system that are distracting in photorealistic animations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The proliferation of video cameras as standard PC peripherals expands the opportunities for synergy between computer vision and computer graphics. Many of the potential applications involve users driving graphical avatars using vision-based techniques that track facial movement. Standard video teleconferencing, for example, could be modified to display graphically generated faces instead of displaying the camera image as is. Anonymous chat rooms could be enhanced by avatars whose expressions are controlled by the participants in real time. Similarly, users could drive avatars in virtual worlds or gaming environments.</p><p>In this paper, we present an example of such a vision-driven application-a novel method for automatic animation of nonphotorealistic (NPR) faces from example images. We assume we are given, as input, a set of drawings for a given character with various facial expressions, e.g., 6 different mouths, 4 pairs of eyes, and 1 overall head (Figure <ref type="figure" target="#fig_0">1</ref>). To perform one-time training of the system for a specific user, we take sample footage of the subject and manually establish correspondences between the hand-drawn elements and similar expressions on the subject's face, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. During execution, vision algorithms track the sender's expressions, which are distilled to a few parameters (we use ten 8-bit integers, or 80 bits, per frame) and sent to the renderer. Then, using various data interpolation and warping techniques, the renderer sythesizes the animated character from the appropriate pieces of artwork. Careful engineering of the components allows the system to run in real time on off-the-shelf PCs.</p><p>NPR animation of faces offers several advantages over attempts to work with photorealistic images.</p><p>First, because an illustrated character represents an abstraction of the real person, we as viewers do not expect a faithful replica of the speaker. Use of abstraction invites our imaginations to fill in the details <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b55">54]</ref>, as confirmed by our ability to watch hand-drawn cartoons without difficulty. Representational inaccuracies, lower frame rates, and lower temporal coherence-all of which might be unacceptable in realistic video-are perfectly acceptable with NPR animation.</p><p>Second, relaxing the constraints of realism allows us to significantly compress the information contained in an image. Our implementation, for example, requires only 10 parameters per frame to be transmitted from the face tracker to the renderer. These 10 parameters are already enough to generate convincing animations, but even increasing the number of parameters to 100 would not make significant demands on bandwidth for any network application. (The Facial Action Coding System, for example, includes only 68 parameters <ref type="bibr" target="#b0">[1]</ref>.) When combined with speech technology, such significant compression holds great promise for UI agents, instructional videos, and the like. For example, an immense amount of "talking head" video can be stored in the form of facial animation parameters plus plain ASCII text and synthesized into an NPR facial animation and voice track on the fly.</p><p>Another benefit of animated characters is that they allow the user to mask his or her true appearance, while permitting expressions and other visual cues to be perceived. This can have different value based on the application. In teleconferencing and MUD applications, it offers privacy. In a game engine, a player's facial expressions could be mapped onto the video game characters, thus enhancing the fantasy of playing the character.</p><p>Finally, an animated figure has an engaging quality that is often more fun than a live video clip. With the flexibility to render in many artistic styles and media, users can choose from a wider range of emotional contexts and appearances than with photorealistic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Graphical avatars driven by vision have a rich, varied history. We discuss some of this history by examining work with synthetic facial models, both 2D and 3D, photorealistic and otherwise.</p><p>In photorealistic 2D models, image blending or morphing is used to render face images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b56">55]</ref>. A typical example of this approach is a teleconferencing system <ref type="bibr" target="#b56">[55]</ref> that stores a set of image samples of a person's face on both the transmitting and receiving computers. A face matching algorithm determines which faces among the stored samples look most like the input face, and a blend of these faces is displayed on the receiving side. The GeniMator system <ref type="bibr" target="#b53">[52]</ref> also uses motion capture data to drive nonphotorealistic renderings, although the system is targeted more toward full-body rather than facial animation. Their system isn't targeted toward facial animation, however.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. NPAR 2000 Annecy France Copyright ACM 2000 1-58113-277-8/00/6... $5.00 To drive 3D models, geometric model parameters must be recovered. In particular, facial features are tracked in the incoming images and then used to drive the movements of the 3D models, which are rendered with well-established graphics techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b52">51]</ref>.</p><p>Although facial features can be tracked by motion capture techniques to produce performance-driven animation systems <ref type="bibr" target="#b60">[59,</ref><ref type="bibr" target="#b43">42]</ref>, systems requiring special markers or devices are unlikely to be adopted by the casual user. Using vision-based techniques, facial features can be tracked non-invasively. Trackers can be based on deformable patches <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b14">14]</ref>, edge or feature detectors <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b54">53]</ref>, and/or 3D models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b46">45]</ref>. Face tracking is currently an active area of research: robust, full-featured, real-time face tracking remains elusive. In this paper, we use a simple, color-based feature tracker (Section 2) that runs in real time.</p><p>On the rendering side, 3D facial animation is one of the most actively studied areas of computer graphics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b63">61]</ref>. Generally, these techniques store a 3D mesh model of the face on which texture is overlaid. Movement of the mesh vertices, sometimes accompanied by texture changes, creates variations in expression. Truly photorealistic animation of faces, however, remains an unsolved challenge that we avoid altogether using NPR animations.</p><p>Non-photorealistic techniques come in several flavors. Haeberli <ref type="bibr" target="#b22">[22]</ref> introduced the idea of using a reference image's pixel values to create interesting NPR effects. This idea has led to some beautiful imitations of artistic styles, such as pen and ink <ref type="bibr" target="#b51">[50]</ref> and watercolor <ref type="bibr" target="#b15">[15]</ref>. However, such algorithms are computationally expensive, limited to a particular artistic effect, and lack frame-to-frame coherence-a quality that is essential for animation. A totally different style of non-photorealistic rendering is to use cartoon characters, such as in Comic Chat <ref type="bibr" target="#b25">[25]</ref>. This is most similar in spirit to our work, in that it uses combinations of hand-drawn artwork. However, we are rendering sequences of moving images, whereas the Comic Chat work concentrated on the layout of static scenes.</p><p>Inspired by work that shows how a wide range of faces (including variations in gender and age) can be synthesized by morphing among a small set of images <ref type="bibr" target="#b50">[49]</ref>, we rely on the Beier and Neely morph <ref type="bibr" target="#b3">[4]</ref>, as extended by Lee et al. <ref type="bibr" target="#b29">[29]</ref> to blend multiple input images. Because we use morphing, we avoid several disadvantages of other NPR techniques: rendering performance is dependent only on the morphing process and independent of the artistic style of the drawings. Morphing can also avoid some frame-to-frame jitter caused by a stochastic rendering process. Lastly, morphing can be applied to drawing styles of any visual complexity.</p><p>Ultimately, our technique is akin to image-based rendering (IBR) approaches for non-photorealistic rendering applications. Litwinowicz and Williams <ref type="bibr" target="#b33">[33]</ref> explored an image-based approach to NPR animation using rotoscoped lines over an image to warp it into new positions for each frame. Wood et al. <ref type="bibr" target="#b61">[60]</ref> used hand-drawn artwork combined with a mobile camera to design moving background scenery for cel animation. Corrêa et al. <ref type="bibr" target="#b13">[13]</ref> warped hand-drawn artwork wrapped over 3D models to attach complex textures to handdrawn foreground characters in cel animation. Our work extends these ideas to 2D hand-drawn character animation, where we morph among many images to capture the full texture variation that can be seen across different facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Approach</head><p>To construct a face, our system requires an initial set of hand-drawn images to blend together. An artist draws this set, divided into mouth, eyes, and background head images, all of which can be warped and blended independently. All drawings are annotated with morph control lines <ref type="bibr" target="#b3">[4]</ref>. The control lines mark certain facial features, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, allowing the rendering process to morph artwork together without ghosting.</p><p>The hand-drawn images should span the range of visually distinguishable expressions and lip poses. In previous research on realistic facial rendering, only nineteen mouth images were found to be necessary for lip reading <ref type="bibr" target="#b40">[39]</ref>, and five eye images for a believable eye blink <ref type="bibr" target="#b16">[16]</ref>. Since our goal is not to produce animations of the quality necessary for lip reading, we are able to use far fewer than this ideal number of images. Additionally, the morphing process generates many of the in-between images that would normally have to be drawn by hand. We found that just six mouth and four eye images are enough to get adequate results in a teleconferencing environment.</p><p>A training step requires manually associating each eye and mouth expression from the set of artwork with an equivalent expression from a video frame (Figure <ref type="figure" target="#fig_1">2</ref>). This correspondence allows the system to discern which tracked measurements of the real person's face (as described in the next section) best match each hand-drawn image.</p><p>Training is required only once for a given user and set of artwork. After this initialization, a person's facial features are tracked in real time. For a teleconferencing application, these features are transmitted to a receiving computer. They are then used to compute a good blend of artwork to reconstruct a synthetic face.</p><p>As an additional feature, both our tracker and renderer are constructed to work with MPEG-4 Face Animation Parameters (FAPs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref>. Therefore, our renderer could be used, with minimal changes, as a non-photorealistic renderer for MPEG-4 streams.</p><p>Section 2 describes our tracker in more detail. Section 3 describes our renderer. Section 4 describes some resulting animations. Section 5 concludes with some areas for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tracking</head><p>An ideal tracking system would accurately track all of the various deformations of the face in real-time, and allow us to pick a fixed set of parameters that would drive the renderer. Excellent face trackers exist (see Toyama <ref type="bibr" target="#b58">[57]</ref> for a brief survey), but none are yet perfect. For the time being, we choose a passive, vision-based implementation that runs in real time and is non-intrusive (i.e., it does not require the user to wear special devices, cosmetics, or markers).</p><p>Our particular implementation takes a frame of video and extracts ten scalar quantities, each encoded as an 8-bit integer:</p><p>• The x and y values of the midpoint of the line segment connecting the two pupils (2).</p><p>• The angle of with respect to the horizontal axis <ref type="bibr" target="#b0">(1)</ref>.</p><p>• The distance between the upper and lower eyelids of each eye (2). • The height of each eyebrow relative to the pupil (2).</p><p>• The distance between the left and right corners of the mouth (1).</p><p>• The height of the upper and lower lips, relative to the mouth center (2).</p><p>The first 3 scalars represent the head pose, while the middle 4 are used for eyes, and the final 3 for the mouth. A frame of video indicating the features tracked by our system is shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>The tracking algorithms for all features rely on color information, which is relatively inexpensive to compute and somewhat resistant to illumination variations.</p><p>First, the tracker tags all pixels within a region of interest according to the output of a color-based pupil classifier. Then, for each tagged pixel, a correlation-based template match <ref type="bibr" target="#b19">[19]</ref> is performed against a previously stored picture of the user's eye (if no pixels are tagged by the classifier, the immediate neighborhood of the previous frame's pupil location is used). If the highest score from the correlation matcher falls below an empirically determined threshold, the eye is assumed to be closed, in which case we use the previous pupil location.</p><p>Once the pupils are found, we search for the eyebrows. For all points above the pupil (within a certain range) we perform a 1D template match <ref type="bibr" target="#b9">[9]</ref> against a stored cross section of the eyebrow, and choose the location with the highest correlation.</p><p>To find the mouth, the tracker first tags each pixel within a selected region according to a lip color classifier. Next, simple imageprocessing operations are applied to this set of tagged pixels to remove noise and eliminate stray pixels, and then the largest 4connected blob is found. Finally, a many-sided polygon is fitted to this blob, using a technique similar to Toyama's radial-spanning blob technique <ref type="bibr" target="#b57">[56]</ref>. The mouth measurements are taken from this fitting polygon.</p><p>Once the pupils, eyebrows, and mouth have been detected, most of the 10 scalar values are easily determined. The distance between the upper and lower eyelids is computed simply by searching for the first pixel above and below the pupil that has a luminance below some threshold.</p><p>The performance of the tracker is reasonable for driving the renderer in real time. There are some remaining problems, however.</p><p>The system requires a manual, per-user initialization to determine eye, eyebrow, and lip colors. The tracker is limited to a range between 0.5 and 1.5 meters from the camera, and to an approximately 30 degree rotation from an upright, frontally oriented face (about all 3 axes). The tracker assumes reasonable, fixed illumination. From time to time, it mistracks, requiring a combination of user movement and manual reinitialization to correct. And finally, some subjects do not exhibit sufficient color contrast between skin and lip color for the tracker to work. All of these problems need to be handled for a more robust system, but many of these issues remain open problems in the vision-based face tracking community. We anticipate that future research will alleviate these difficulties. For the time being, our tracker is sufficient to drive the renderer in a stably illuminated office environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rendering</head><p>The rendering stage runs after the tracking stage (possibly on a separate machine, depending on the application). It takes the 10 tracked parameters as input and renders a synthetic animated character that mimics the user's expressions. The renderer can be divided into two components: expression mapping, which determines which pieces of artwork should be used in creating the final animated character for a given frame, and in what proportions; and warping, which combines the various pieces of artwork together using feathered masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expression mapping</head><p>The problem of determining the best blend of artwork needed to mimic a given expression is really one of scattered data interpolation <ref type="bibr" target="#b41">[40]</ref>. For now, let us consider how expression mapping is done for the mouth; the eyes are handled similarly.</p><p>Suppose we have n pieces of artwork for the mouth in different expressions M 1, . . . , Mn. The training data provides an association between each mouth expression Mi and a k-dimensional training point mi, whose components are the values of the k tracked parameters associated with that mouth. (Recall that for the sample art set shown in Figure <ref type="figure" target="#fig_0">1</ref>, n = 6, and in our implementation, k = 3 for the mouths and k = 4 for the eyes.)</p><p>Our problem is: Given some new set m of tracked parameters for the mouth, find a set of weights α1, . . . , αn such that i αi = 1 and mi αimi is minimized (or at least small). We can then use these weights αi to create the new expression by morphing together an appropriately weighted combination of the original artwork, as described in Section 3.2.</p><p>Our solution to this scattered data interpolation problem must be fast, yet accurate enough to faithfully reproduce the speaker's expression. Furthermore, for our animation to be smooth, two points m and m that lie close to each other should produce expressions that are similar. On the other hand, there is a tradeoff between accuracy and visual clarity: the more pieces of artwork we blend together, the blurrier the imagery in the resulting animation. A straightforward approach to this problem is to compute a kdimensional Delaunay triangulation <ref type="bibr" target="#b20">[20]</ref> among the training points mi. Then, for a given new point m, locate that point in the triangulation and use the barycentric coordinates of the simplex it lies in as the morphing weights. These weights could then be applied to the drawings corresponding to the vertices of that simplex, as in the polymorph method described by Lee et al. <ref type="bibr" target="#b29">[29]</ref>. This approach has the advantage that it provides smooth transitions between nearby expressions. However, if the number of tracked parameters k is large, the resulting morph may be blurry.</p><p>Our solution is to use the same Delaunay triangulation approach, but in a lower-dimensional space. We use a principal component analysis (PCA) <ref type="bibr" target="#b7">[7]</ref> to choose the j largest eigenvectors that span the k-dimensional space created by the training points. We project the training set into this j-dimensional space, as well as the query point m. In practice, we use j = 2, which appears to provide a good tradeoff between expression accuracy and image clarity. Thus, we merely locate the projection of m in its 2-D triangulation (Figure <ref type="figure" target="#fig_4">5</ref>) and use the barycentric coordinates of the triangle vertices as morph weights for the three corresponding drawings (Figure <ref type="figure" target="#fig_5">6</ref>). Projected points m falling outside of any Delaunay triangle are mapped to a point on the convex hull of the training points and associated with the Delaunay triangle that abuts that region of the convex hull. In our experience, if the initial correspondences are properly set up, most points projecting outside of the convex hull lie close enough to the hull that this method works well.</p><p>In our implementation, we find the PCA of the training set by doing a singular value decomposition of the matrix of training points <ref type="bibr" target="#b48">[47]</ref>. The two dominant eigenvectors, as well as the resulting Delaunay triangulation, are saved to map feature points at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Morphing</head><p>To draw the face, we first render the warped versions of the eye and mouth regions of the face. Next, the head image, which contains "soft" alpha values in the eye and mouth areas to provide feathered masking, is placed on top of the rendered eyes and mouth. More than one such head image can be loaded, and the program will cycle through them at each frame. For the animations in the style of Bill Plympton, we use two different overall heads to achieve a shimmering quality. Finally, we apply a translation and rotation to the image in order to get the head tilt.</p><p>We describe here the process of creating a single, new mouth based on an expression mapping. (To create the eyes, we use an identical procedure.) Recall that the morphing module receives weights cor- responding to the barycentric coordinates of the projected tracked parameters in a triangle whose corners correspond to three original mouth drawings in the training set. To create a new mouth, we use three-way Beier-Neely morphing <ref type="bibr" target="#b3">[4]</ref>, as generalized by Lee et al. <ref type="bibr" target="#b29">[29]</ref> for polymorphs-morphs between more than two source images.</p><p>Consider the triangle formed by the three mouths. To create a new intermediate mouth, we first warp the mouths at the corners, yielding three mouths whose features align. Next we composite the three warped mouths using alpha blending to render the new mouth image. The blending weights are given by the aforementioned barycentric coordinates.</p><p>We employ two strategies for accelerating the morph.</p><p>First, we sample the warps over the vertices of a 30 × 30 quadmesh, represented as triangle strips, and use texture mapping hardware to render triangles rather than computing the warp at every pixel. Since many common PC graphics boards now come with texture mapping and alpha blending acceleration, we use this hardware to our advantage.</p><p>Second, the actual 3-way warp function is not evaluated at each mesh vertex. Instead, we approximate the correct warp function by summing together two 2-way warps. Consider the 3-way warp function W ABC (x, y, αB, αC), which returns a vector indicating how pixel (x, y) in image A moves when warping it toward image B by a fraction αB and toward image C by a fraction αC. Now, consider the 2-way warp WAB(x, y, α), returning a vector indicating where the pixel at location (x, y) in image A moves when warping it α of the way toward image B. Our approximation of the 3-way warp is WABC(x, y, αB, αC) ≈ WAB(x, y, αB) + WAC(x, y, αC). The warp weights αB and αC used are simply the barycentric coordinates corresponding to the points A and B as given above. To make evaluation of the approximate warps fast, we precompute the 2-way warps at a number of discrete values of α and interpolate between these stored functions.</p><p>A more accurate method would be to sample the actual warp function at various points inside the triangle and interpolate these precomputed functions, but this would require sampling a 2-D function rather than a 1D function. In our work, we did not notice much difference between our approximation and the real warp, so we did not explore this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation and observations</head><p>We implemented our algorithms on a 450MHz Pentium III processor, equipped with a high-end PC graphics card. Table <ref type="table" target="#tab_0">1</ref> shows average running times rounded to the nearest millisecond for the various stages in our process. The slowest component is the facial feature tracking. Nonetheless, we can track and render simultaneously on the same computer at 25 fps. Since our video capture board can only grab frames at 15 fps, this leaves CPU cycles to spare.</p><p>Because the tracker produces 10 8-bit integers per frame, our current  bandwidth requirements are 2400 baud for 30 frames per second. At 10 fps, the bandwidth requirement drops to a miserly 800 baud.</p><p>Taking advantage of temporal coherence in the tracked features is likely to yield even greater compression.</p><p>Five different styles of imagery were used to demonstrate the flexibility of the rendering scheme. Of these, four were generated from hand-drawn artwork in the manner described in Section 3 (Monster, Blue, Wavy, and Straight). These are illustrated in Figure <ref type="figure">7</ref>. The last style (Photoreal), which appears on the video only, was created from actual video of one of the subjects herself based on images acquired prior to run time.</p><p>We tried rendering at 10, 15, and 30 frames per second. The animation at 30 fps was done off-line, using a previously digitized video stream, and rendered according to the methods described in Section 3. The animations at lower speeds take averages of tracked parameters collected at 30Hz over 3 (or 2) frames and render new images at 10 (or 15) fps.</p><p>Sample frames from the conversations can be seen in Figure <ref type="figure">7</ref>. The characters shown exhibit a variety of expressions and a variety of mouth, eye, and head poses.</p><p>After using the system, we have made the following observations:</p><p>• While the output is engaging, the rendering does not achieve the quality of hand-drawn animation. This is not surprising, since our frames are generated automatically, without run-time input from a professional animator. It suggests that our technique in its current form is best suited for applications in which professional quality animation is not the goal.</p><p>• The animations rendered at 30 fps appeared jittery and anxious, whereas animations rendered at 10 and 15 fps reduced the visible jitter considerably. While this is partly due to noise in the tracking process, it may also be that the quality of animations are sometimes improved at a lower frame rate <ref type="bibr" target="#b55">[54]</ref>.</p><p>• In spite of the jitter, the 30 fps animation reproduces visual speech articulations more clearly, undoubtedly because of the high speed of mouth movement during speech.</p><p>• Apparent eye-contact is made with all of the characters. This is an advantage over standard video teleconferencing in which lack of eye contact is often cited as a major drawback.</p><p>• The four hand-drawn animations appear more compelling than the Photoreal style.</p><p>This last point is interesting. Although the Photoreal animation more faithfully mimics a real human face, the hand-drawn animations are nevertheless perceived as more compelling. In particular, the mouth movements in the Photoreal style appear erratic and affected. Frequently, artifacts in the morphing process become apparent as one mouth appears to dissolve, rather than morph, into another. In contrast, although the hand-drawn animation exhibits the same technical problems, it appears more natural. We hypothesize that as observers, we are more forgiving of cartoon characters, whose abstraction and imperfection we readily accept; on the other hand, we expect absolute fidelity of photorealistic video and notice even minor departures from reality.</p><p>Finally, we discovered that different observers liked different animation styles. This highlights the flexibility of our algorithm for animation-a library of various hand-drawn faces (requiring only a few drawings per face) would allow users to pick and choose the style according to their preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and future work</head><p>We have demonstrated how a small set of hand-drawn artwork, in conjunction with a small amount of facial tracking data, can be used to create a real-time performance-driven animation system in which animations effectively mimic the expressions and facial actions of a human speaker. Our system works in real time using a combination of a fast feature tracker and a fast novel morphing technique that paints the appropriate eyes and mouth onto a head. One component of this work is a novel face expression interpolation algorithm that projects tracking data onto a two-dimensional subspace, and then uses a Delaunay triangulation to find the three nearest expressions and to compute their blending weights.</p><p>Our system is one of the first to apply image-based rendering techniques to a collection of hand-drawn artwork to produce facial animations. Our framework has several advantages. It can accommodate a variety of artistic styles and media, limited only by the possible styles of the input artwork. It accommodates a variety of animation styles, e.g., different frame rates and different amounts of flicker or blending. It is able to compress facial expression information to a handful of integers per frame. Finally, by relying on hand-drawn animations, it avoids the primary difficulty with photorealistic avatars, namely, that the rendered animations appear unnatural in some way.</p><p>There are several ways in which we plan to improve and extend our existing system. First, we plan to add the capability to both track and render additional parameters. Changes in head pose are essential for transmitting gestures such as nodding and shaking of the head. Facial creases and wrinkles will add to the expressivity of the renderings. It would also be interesting to explore using different animation styles at run-time based on the expressive content of the frame being rendered. For example, when the speaker has highly raised eyebrows indicating an extreme emotional state, random jitter and reddish shifts in color might be introduced to the rendering process to convey an additional intensity.</p><p>By shifting the focus of image-based rendering away from photorealistic reproduction towards the goal of expressive animation, we have opened up a wide range of new expressive possibilities. For example, we could use avatars without any faces per se, e.g., a scene in which weather reflects the speaker's expression. A cloudless sunny sky could correspond to a smile, while a dark overcast or stormy sky could reflect a frown.</p><p>We believe that the combination of real-time feature tracking, performance-driven animation, and non-photorealistic rendering can form the basis for a range of exciting applications. We imagine teleconferencing applications and multi-user virtual worlds in which users can put on graphical masks that transmit their expressions without revealing identity. Another possibility is for video games, in which players could puppeteer the expressions that appear on their characters. A final application might be home animation kits, where children could shoot, edit, and replay animations of their favorite cartoon characters, driven by their own faces in real time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 A</head><label>1</label><figDesc>Figure 1 A full set of hand-drawn images used by our system.</figDesc><graphic coords="2,57.02,54.00,234.01,197.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Sample correspondences between hand-drawn eyes and mouths and real-face equivalents. Two different users with two different corresponding sets of artwork are shown.</figDesc><graphic coords="2,320.98,54.00,233.99,206.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 This image shows both eye and mouth regions with feathered masks. The purple lines are the control lines used to guide the morphing of these images.</figDesc><graphic coords="3,57.02,54.00,234.00,264.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 A</head><label>4</label><figDesc>Figure4A frame of video tracked by our system. The ten scalar quantities sent to the renderer are easily derived from the features detected by our tracker.</figDesc><graphic coords="3,320.97,54.00,234.01,303.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Figure 5 This image shows a sample Delaunay triangulation for mouth interpolations. Note the interpolated mouth inside one of the triangles.</figDesc><graphic coords="4,320.98,54.00,234.00,156.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure<ref type="bibr" target="#b5">6</ref> The three mouths on the left are warped and then blended to make the mouth on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Running times (450MHz Pentium III) for various stages of the pipeline.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MPEG-4 facial animation technology: Survey, implementation, and results</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Abrantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="290" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accurate automatic frame fitting for semantic-based moving image coding using a facial code-book</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Antoszczyszyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D modeling and tracking of human lip motions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Computer Vision</title>
		<meeting>Int&apos;l Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature based image metamorphosis</title>
		<author>
			<persName><forename type="first">Thaddeus</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH 92 Conference Proceedings</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1992-08">August 1992</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Controlling facial expressions and body movements in the computer-generated animated short</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lachapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH 85 Advanced Computer Animation seminar notes. ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1985-07">July 1985</date>
		</imprint>
	</monogr>
	<note>Tony De Peltrie</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Example based image analysis and synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Memo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1431-11">1431. November 1993</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;94)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Active Contours: The Application of Techniques from Graphics, Vision, Control Theory and Statistics to Visual Tracking of Shapes in Motion</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 99 Conference Proceedings</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voice puppetry</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 99 Conference Proceedings</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of facial image sequences in model-based image coding</title>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kiyoharu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsuyoshi</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><surname>Takebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="257" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Texture mapping for cel animation</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Wagner Toledo Corrêa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">E</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1998-07">July 1998</date>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigen-points: control-point location using principal component analyses</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="122" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer-generated watercolor</title>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Seims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Fleischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Animation using image samples</title>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing Images of Faces</title>
		<meeting>essing Images of Faces<address><addrLine>Norwood, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Ablex Publishing Corp</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="179" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling, tracking and interactive animation of faces and heads using input from video</title>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Animation Conference</title>
		<imprint>
			<date type="published" when="1996-06">June 1996</date>
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facial analysis and synthesis using image-based models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ezzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Automatic Faces and Gesture Recognition</title>
		<meeting>the Second International Conference on Automatic Faces and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computer vision for interactive computer graphics</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">N</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">D</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Yerazunis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuo</forename><surname>Kyuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasunari</forename><surname>Miyake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken Ichi</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="1998-06">May. June. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handbook of Discrete and Computational Geometry</title>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph O'</forename><surname>Rourke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>CRC Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making faces</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Malvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1998-07">July 1998</date>
			<biblScope unit="page" from="55" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paint by numbers: Abstract image representations</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Haeberli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;90 Proceedings)</title>
		<editor>
			<persName><forename type="first">Forest</forename><surname>Baskett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parametrized structure from motion for 3D adaptive feedback tracking of faces</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Patt. Recog</title>
		<meeting>Computer Vision and Patt. Recog</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Snakes, active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comic chat</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kurlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Skelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">Holly</forename><surname>Rushmeier</surname></persName>
		</editor>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unified approach to coding and interpretting faces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Conference on Computer Vision</title>
		<meeting>5th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Synthetic and hybrid imaging in the HUMANOID and VIDAS projects</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lavagetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Pandzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="663" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time analysissynthesis and intelligibility of talking faces</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guard-Marigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International conference on Speech Synthesis</title>
		<imprint>
			<date type="published" when="1994-09">September 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polymorph: Morphing among multiple images</title>
		<author>
			<persName><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Wolberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Yong</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1998-01">January 1998</date>
			<biblScope unit="page" from="58" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Realistic modeling for facial animation</title>
		<author>
			<persName><forename type="first">Yuencheng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1995-08">August 1995</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3-D motion estimation in model-based facial image coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roivainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="545" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Example-based character drawing</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Librande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-08">August 1992</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Animating images with drawings</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Litwinowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH 94 Conference Proceedings</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1994-08">August 1994</date>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic recognition of human facial expressions</title>
		<author>
			<persName><forename type="first">Katsuhiro</forename><surname>Matsuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chil-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saburo</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="352" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding Comics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mccloud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kitcken Sink Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An automatic system for model-based coding of faces</title>
		<author>
			<persName><forename type="first">Baback</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Data Compression Conference</title>
		<imprint>
			<date type="published" when="1995-03">March 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Committee draft of ISO/IEC 14496-2, information technology -coding of audiovisual objects: Video. Annex C contains Face object decoding tables and definitions</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Coding of moving pictures and audio</title>
		<idno>ISO/IEC JTC1/SC29/WG11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">International Organisation for Standardisation</title>
		<author>
			<orgName type="collaboration">N2459</orgName>
		</author>
		<ptr target="http://www.cselt.stet.it/mpeg/standards/mpeg-4/mpeg-4.htm" />
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graphics aid the deaf</title>
		<author>
			<persName><forename type="first">Ware</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="102" />
			<date type="published" when="1982-03">March 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scattered data modeling</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Nielson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="60" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Frederic</forename><forename type="middle">I</forename><surname>Parke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Facial Animation. A K Peters</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Wellesley, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial animation by spatial mapping</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">C</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Litwinowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ned</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Animation 91</title>
		<editor>
			<persName><forename type="first">Nadia</forename><surname>Magnenat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thalmann</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</editor>
		<meeting><address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust face feature analysis for automatic speechreading and character animation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Petajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Autom. Face and Gesture Recog</title>
		<meeting>Int&apos;l Conf. on Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="357" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Synthesizing realistic facial expressions from photographs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings. ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Resynthesizing facial animation through 3D model-based tracking</title>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Conference on Computer Vision (ICCV &apos;99)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Animating facial expressions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics</title>
		<imprint>
			<biblScope unit="page">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saul</forename><forename type="middle">A</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">P</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName><surname>Flannery</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C: The Art of Scientific Computing</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards a real-time model-based-coding system</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mersereau</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Image and Multidimensional Signal Processing</title>
		<meeting>Workshop on Image and Multidimensional Signal essing</meeting>
		<imprint>
			<date type="published" when="1996-03">March 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Manipulating facial appearance through shape and color</title>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Perrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="70" to="76" />
			<date type="published" when="1995-09">September. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Orientable textures for image-based pen-and-ink illustration</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Salisbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings, Annual Conference Series</title>
		<imprint>
			<publisher>ACM SIGGRAPH, Addison Wesley</publisher>
			<date type="published" when="1997-08">August 1997</date>
			<biblScope unit="page" from="401" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time facial analysis and synthesis chain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Viaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geldreich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Autom. Face and Gesture Recog</title>
		<meeting>Int&apos;l Conf. on Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="86" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Genimator: Applies human motion onto linedrawn cartoon characters</title>
		<author>
			<persName><forename type="first">Orjan</forename><surname>Standberg</surname></persName>
		</author>
		<ptr target="http://home5.swipnet.se/˜w-56588/GeniMator.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of facial image sequences using physical and anatomical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<biblScope unit="page" from="569" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The Illusion of Life: Disney Animation</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ollie</forename><surname>Johnston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Hyperion Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards an example-based image compression architecture for video-conferencing</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Toelg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI Memo 1494, CBCL Paper 100</title>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Radial spanning for fast blob detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Information Sciences Proceedings</title>
		<meeting><address><addrLine>Research Triangle Park, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="484" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Prolegomena for robust face tracking</title>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
		</author>
		<idno>MSR- TR-98-65</idno>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A muscle model for animating three-dimensional facial expression</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;87 Proceedings)</title>
		<editor>
			<persName><forename type="first">Maureen</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1987-07">July 1987</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Performance-driven facial animation</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;90 Proceedings)</title>
		<editor>
			<persName><forename type="first">Forest</forename><surname>Baskett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990-08">August 1990</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multiperspective panoramas for cel animation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Thayer</surname></persName>
		</author>
		<author>
			<persName><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings, Annual Conference Series</title>
		<editor>
			<persName><forename type="first">Turner</forename><surname>Whitted</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Acm</forename><surname>Siggraph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Addison</forename><surname>Wesley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tracking subspace representations of face images</title>
		<author>
			<persName><forename type="first">Jung</forename><surname>Hsi</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="1994-04">April 1994</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="389" to="392" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
