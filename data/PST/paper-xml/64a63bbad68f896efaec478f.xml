<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All in One: Multi-Task Prompting for Graph Neural Networks</title>
				<funder ref="#_6gnrPyS">
					<orgName type="full">Research Grant Council of the Hong Kong Special Administrative Region, China</orgName>
				</funder>
				<funder ref="#_AU9VjtP">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_S5m2hGf">
					<orgName type="full">Open Research Projects of Zhejiang Lab</orgName>
				</funder>
				<funder ref="#_uXeceh2 #_SfMCXYb #_2xzBKfk #_8gg3avF #_GE9uRdH #_HwumqUr">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_fNvYGn2">
					<orgName type="full">Guangzhou-HKUST(GZ) Joint Funding Scheme</orgName>
				</funder>
				<funder ref="#_kTy4uvd">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_b9fKqv5">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-07-04">4 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
							<email>xiangguosun@cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
							<email>hcheng@se.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
							<email>bliu@seu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jihong</forename><surname>Guan</surname></persName>
							<email>jhguan@tongji.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management, and Shun Hing Institute of Advanced Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Engineering and Engineering Management, and Shun Hing Institute of Advanced Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Data Science and Analytics Thrust</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Southeast University Purple Mountain Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">All in One: Multi-Task Prompting for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-04">4 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3580305.3599256</idno>
					<idno type="arXiv">arXiv:2307.01504v1[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pre-training</term>
					<term>prompt tuning</term>
					<term>graph neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, "pre-training and fine-tuning" has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a "negative transfer" to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pretrained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have been widely applied to various applications such as social computing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref> , anomaly detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> , and network analysis <ref type="bibr" target="#b3">[4]</ref>. Beyond exploring various exquisite GNN structures, recent years have witnessed a new research trend on how to train a graph model for dedicated problems.</p><p>Traditional supervised learning methods on graphs heavily rely on graph labels, which are not always sufficient in the real world. Another shortcoming is the over-fitting problem when the testing data is out-of-distribution <ref type="bibr" target="#b23">[24]</ref>. To solve these challenges, many studies turn to "pre-training and fine-tuning" <ref type="bibr" target="#b12">[13]</ref>, which means pre-training a graph model with easily accessible data, and then transferring the graph knowledge to a new domain or task via tuning the last layer of the pre-trained model. Although much progress has been achieved on pre-training strategies <ref type="bibr" target="#b8">[9]</ref>, there still exists a huge gap between these pretexts and multiple downstream tasks. For example, a typical pretext for the pre-training graph is binary edge prediction. Usually, this pre-training strategy makes connected nodes closer in a latent representation space. However, many downstream tasks are not limited to edge-level tasks but also include node-level tasks (e.g., node multi-class classification) or graph-level tasks (e.g., graph classification). If we transfer the above pre-trained model to multi-class node classification, it may require us to carefully search the results in higher dimensional parameter space for the additional classes of node labels. This tuning may even break down (a.k.a negative transfer <ref type="bibr" target="#b32">[33]</ref>) when connected nodes have different labels. Tuning this pre-trained model to graphlevel tasks is neither efficient because we have to pay huge efforts to learn an appropriate function mapping node embedding to the whole graph representation. A promising solution to the above problems is to extend "pretraining and fine-tuning" to "pre-training, prompting, and finetuning". Prompt learning is a very attractive idea derived from natural language processing (NLP) and has shown notable effectiveness in generalizing pre-trained language models to a wide range of language applications <ref type="bibr" target="#b19">[20]</ref>. Specifically, a language prompt refers to a piece of text appended to the rear of an input text. For example, a sentiment task like "KDD2023 will witness many high-quality papers. I feel so [MASK]" can be easily transferred to a word prediction task via a preset prompt ("I feel so [MASK]"). It is highly expected that the language model may predict "[MASK]" as "excited" rather than "upset" without further optimizing parameters for the new sentiment task because this model has already been pre-trained via the pretext of masked words prediction and contains some useful knowledge to answer this question. By this means, some downstream objectives can be naturally aligned with the pre-training target. Inspired by the success of the language prompt, we hope to introduce the same idea to graphs. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, prompt tuning in the graph domain is to seek some light-weighted prompt, keep the pre-training model frozen, and use the prompt to reformulate downstream tasks in line with the pre-training task. In this way, the pre-trained model can be easily applied to downstream applications with highly efficient fine-tuning or even without any fine-tuning. This is particularly useful when the downstream task is a few-shot setting.</p><p>However, designing the graph prompt is more intractable than language prompts. First, classic language prompts are usually some preset phrases or learnable vectors attached at the end of input texts. As shown in Figure <ref type="figure" target="#fig_2">2</ref>, we only need to consider the content for the language prompt, whereas the graph prompt not only requires the prompt "content" but also needs to know how to organize these prompt tokens and how to insert the prompt into the original graph, both of which are undefined problems.</p><p>Second, there is a huge difficulty in reconciling downstream problems to the pre-training task. In the NLP area, we usually pretrain a language model via masked prediction and then transfer it to various applications like question answering <ref type="bibr" target="#b21">[22]</ref>, sentiment classification <ref type="bibr" target="#b16">[17]</ref>. The underlying support <ref type="bibr" target="#b20">[21]</ref> is that these language tasks usually share a large overlapping task sub-space, making a masked language task easily transferred to other applications. However, how much does the same observation exist (if truly exists) in graph learning? It is crucial but difficult to decide on an appropriate pre-training task and reformulate downstream tasks to improve ``KDD2023 will witness many high-quality papers. I feel so <ref type="bibr">[excited]</ref> ''  the capability of model generalization. Currently, we only find very few works <ref type="bibr" target="#b26">[27]</ref> studying the graph prompt issue. However, it can only deal with a single-type task (e.g., node classification) using a specific pretext (e.g., edge prediction), which is far from addressing the multi-task setting with different-level tasks. Last but not least, learning a reliable prompt usually needs huge manpower and is more sensitive to prompt initialization in the multi-task setting <ref type="bibr" target="#b17">[18]</ref>. Although there are some works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> in the NLP area trying to initialize the prompt via hand-crafted content or some discrete features, these methods are task-bounded, which is not sufficient when we confront a new task. This problem may be even worse in our multi-task graph area since graph features vary a lot in different domains and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>prompt answer input</head><p>Presented work. To further fill the gap between graph pretraining and downstream tasks, we introduce the prompt method from NLP to graphs under the multi-task background. Specifically, to address the first challenge, we propose to unify the format of the language prompt and graph prompt in one way so that we can smoothly transfer the prompt idea from NLP to graphs, then we design the graph prompt from prompt tokens, token structures, and prompt inserting patterns. To address the second challenge, we first study the task subspace in graphs and then propose to reformulate node-level and edge-level tasks to graph-level tasks by induced graphs from original graphs. To address the third challenge, we introduce the meta-learning technique over multiple tasks to learn better prompts. We carefully evaluate our method with other approaches and the experimental results extensively demonstrate our advantages.</p><p>Contributions:</p><p>? We unify the prompt format in the language area and graph area, and further propose an effective graph prompt for multitask settings (section 3.3). ? We propose an effective way to reformulate node-level and edge-level tasks to graph-level tasks, which can further match many pre-training pretexts (section 3.2). ? We introduce the meta-learning technique to our graph prompting study so that we can learn a reliable prompt for improving the multi-task performance (section 3.4). ? We carefully analyze why our method works (section <ref type="bibr">3.5)</ref> and confirm the effectiveness of our method via extensive experiments (section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Graph Neural Networks. Graph neural networks (GNNs) have presented powerful expressiveness in many graph-based applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> . The nature of most GNNs is to capture the underlying message-passing patterns for graph representation. To this end, there are many effective neural network structures proposed such as graph attention network (GAT) <ref type="bibr" target="#b31">[32]</ref>, graph convolution network (GCN) <ref type="bibr" target="#b33">[34]</ref>, Graph Transformer <ref type="bibr" target="#b24">[25]</ref>. Recent works also consider how to make graph learning more adaptive when data annotation is insufficient or how to transfer the model to a new domain, which triggered many graph pre-training studies instead of traditional supervised learning.</p><p>Graph Pre-training. Graph pre-training <ref type="bibr" target="#b12">[13]</ref> aims to learn some general knowledge for the graph model with easily accessible information to reduce the annotation costs of new tasks. Some effective pre-training strategies include node-level comparison like GCA <ref type="bibr" target="#b39">[40]</ref>, edge-level pretext like edge prediction <ref type="bibr" target="#b12">[13]</ref>, and graph-level contrastive learning such as GraphCL <ref type="bibr" target="#b35">[36]</ref> and SimGRACE <ref type="bibr" target="#b34">[35]</ref>.</p><p>In particular, GraphCL minimizes the distance between a pair of graph-level representations for the same graph with different augmentations whereas SimGRACE tries to perturb the graph model parameter spaces and narrow down the gap between different perturbations for the same graph. These graph-level strategies perform more effectively in graph knowledge learning <ref type="bibr" target="#b10">[11]</ref> and are the default strategies of this paper.</p><p>Prompt Learning &amp; Motivations. Intuitively, the above graphlevel pre-training strategies have some intrinsic similarities with the language-masked prediction task: aligning two graph views generated by node/edge/feature mask or other perturbations is very similar to predicting some vacant "blanks" on graphs. That inspires us to further consider: why can't we use a similar format prompt for graphs to improve the generalization of graph neural networks? Instead of fine-tuning a pre-trained model with an adaptive task head, prompt learning aims to reformulate input data to fit the pretext <ref type="bibr" target="#b6">[7]</ref>. Many effective prompt methods are firstly proposed in the NLP area, including some hand-crafted prompts like GPT-3 <ref type="bibr" target="#b2">[3]</ref>, discrete prompts like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, and trainable prompts in the continuous spaces like <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>. Despite significant results achieved, prompt-based methods have been rarely introduced in graph domains yet. We only find very few works like GPPT <ref type="bibr" target="#b26">[27]</ref>, trying to design prompts for graphs. Unfortunately, most of them are very limited and are far from sufficient to meet the multi-task demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-TASK PROMPTING ON GRAPHS 3.1 Overview of Our Framework</head><p>Objective: In this paper, we aim to learn a prompt graph that can be inserted into the original graph, through which we wish to further bridge the gap between a graph pre-training strategy and multiple downstream tasks, and further relieve the difficulties of transferring prior knowledge to different domains.</p><p>Overview: To achieve our goal, we propose a novel multi-task prompting framework for graph models. First, we unify various graph tasks in the same format and reformulate these downstream tasks as graph-level tasks. Second, with the unified graph-level instances, we further narrow down the gap among multiple tasks by a novel prompt graph with learnable tokens, inner structures, and  adaptive inserting patterns. Third, we build a meta-learning process to learn more adaptive graph prompts for multi-task settings. Next, we elaborate on the main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reformulating Downstream Tasks</head><p>3.2.1 Why Reformulate Downstream Tasks. The success of the traditional "pre-training and fine-tuning" framework in the NLP area largely lies in the fact that the pre-training task and downstream tasks share some common intrinsic task subspace, making the pretraining knowledge transferable to other downstream tasks (Figure <ref type="figure" target="#fig_3">3a</ref>). However, things are a little complicated in the graph domain since graph-related tasks are far from similar. As shown in Figure <ref type="figure" target="#fig_3">3b</ref>, it is far-fetched to treat the edge-level task and the node-level task as the same one because node-level operations and edge-level operations are far more different <ref type="bibr" target="#b26">[27]</ref>. This gap limits the performance of pre-training models and might even cause negative transfer <ref type="bibr" target="#b12">[13]</ref>.</p><p>The same problem also happens in our "pre-training, prompting, and fine-tuning" framework since we aim to learn a graph prompt for multiple tasks, which means we need to further narrow down the gap between these tasks by reformulating different graph tasks in a more general form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Why</head><p>Reformulate to the Graph Level. With the above motivation, we revisit the potential task space on the graph and find their hierarchical relation as shown in Figure <ref type="figure" target="#fig_3">3b</ref>. Intuitively, many node-level operations such as "changing node features", "delete/add a node", or edge-level operations such as "add/delete an edge", can be treated as some basic operations at the graph level. For example, "delete a subgraph" can be treated as "delete nodes and edges". Compared with node-level and edge-level tasks, graph-level tasks are more general and contain the largest overlapping task sub-spaces for knowledge transfer, which has been adopted as the mainstream task in many graph pre-training models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. This observation further inspires us to reformulate downstream tasks to look like the graph-level task and then leverage our prompting model to match graph-level pre-training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">How to Reformulate Downstream</head><p>Tasks. Specifically, we reformulate node-level and edge-level tasks to graph-level tasks by building induced graphs for nodes and edges, respectively. As shown in Figure <ref type="figure" target="#fig_4">4a</ref>, an induced graph for a target node means its local area in the network within ? distance, which is also known as its ?-ego network. This subgraph preserves the node's local structure by neighboring node connections and its semantic context by neighboring node features, which is the main scope of most graph neural encoders. When we treat the target node's label as this induced graph label, we can easily translate the node classification problem into graph classification; Similarly, we present an induced graph for a pair of nodes in Figure <ref type="figure" target="#fig_4">4b</ref>. Here, the pair of nodes can be treated as a positive edge if there is an edge connecting them, or a negative edge if not. This subgraph can be easily built by extending this node pair to their ? distance neighbors. We can reformulate the edgelevel task by assigning the graph label with the edge label of the target node pair. Note that for unweighted graphs, the ? distance is equal to ?-hop length; for weighted graphs, the ? distance refers to the shortest path distance, where the induced graph can be easily found by many efficient algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. (3) inserting pattern, which presents how to add the prompt to the input data. In the NLP area, the prompt is usually added in the front or the back end of the input sentences by default. However, in the graph area, there are no explicit positions like a sentence to joint graph prompt, making the graph prompting more difficult. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Prompt Tokens. Let a graph instance be</head><formula xml:id="formula_0">G = (V, E) where V = {? 1 , ? 2 , ? ? ? , ? ? }</formula><formula xml:id="formula_1">A = | P | -1 ? ?=1 ? =?+1 {? ? ? }</formula><p>where ? ? ? is a tunable parameter indicating how possible the token ? ? and the token ? ? should be connected; (2) the second way is to use the dot product of each prompt token pair and prune them according to the dot value. In this case, (? ? , ? ? ) ? S iff ? (p ? ?p ? ) &lt; ? where ? (?) is a sigmoid function and ? is a pre-defined threshold;</p><p>(3) the third way is to treat the tokens as independent and then we have S = ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Inserting Patterns.</head><p>Let ? be the inserting function that indicates how to add the prompt graph G ? to the input graph G, then the manipulated graph can be denoted as G ? = ? (G, G ? ). We can define the inserting pattern as the dot product between prompt tokens and input graph nodes, and then use a tailored connection like x? = x ? + | P | ?=1 ? ?? p ? where ? ?? is a weighted value to prune unnecessary connections:</p><formula xml:id="formula_2">? ?? = ? (p ? ? x ? ? ), if ? (p ? ? x ? ? ) &gt; ? 0, otherwise<label>(1)</label></formula><p>As an alternative and special case, we can also use a more simplified way to get x? = x ? + | P | ?=1 p ? . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-task Prompting via Meta Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Applying</head><p>Meta-learning to Graph Prompting. Let ? be prompt parameters, ? * be the fixed parameters of the pre-trained graph backbone, and ? be the tasker's parameters. We use ? ?,? |? * to denote the pipeline with prompt graph (? ), pre-trained model (? * , fixed), and downstream tasker (?). Let L D (? ) be the task loss with pipline ? on data D. Then for each task ? ? , the corresponding parameters can be updated as follows:</p><formula xml:id="formula_3">? ? ? = ? ? -1 ? -?? ? ? -1 ? L D ? ? ? ? ? ? -1 ? ,? ? -1 ? |? * ? ? ? = ? ? -1 ? -?? ? ? -1 ? L D ? ? ? ? ? ? -1 ? ,? ? -1 ? |? * (2)</formula><p>where the initialization is set as: ? 0 ? = ? , and ? 0 ? = ?. The goal of this section is to learn effective initialization settings (?, ?) for meta prompting tasks, which can be achieved by minimizing the meta loss on various tasks:</p><formula xml:id="formula_4">? * , ? * = arg min ?,? ?? ? ? ? T L D ? ? ? ? ? ? ,? ? |? * (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where T is the task set. According to the chain rule, we use the second-order gradient to update ? (or ?) based on query data:</p><formula xml:id="formula_6">? ?? -? ? ? ?????? ? =? -? ? ?? ? ? ? T ? ? L D ? ? ? ? ? ? ,? ? |? * =? -? ? ?? ? ? ? T ? ? ? L D ? ? ? ? ? ? ,? ? |? * ? ? ? (? ? ) =? -? ? ?? ? ? ? T ? ? ? L D ? ? ? ? ? ? ,? ? |? * ? I-?H ? L D ? ? ? ? ? ? ,? ? |? *<label>(4)</label></formula><p>where H ? (L) is the Hessian matrix with (H ? (L)) ? ? = ? 2 L/?? ? ? ? ; and ? can be updated in the same way. Kindly note that in the prompt learning area, the task head is also known as the answering function, which connects the prompt to the answers for downstream tasks to be reformulated. The answering function can be both tunable or hand-craft templates. In section 3.5, we also propose a very simple but effective hand-crafted prompt answering template without any tunable task head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Overall Learning Process.</head><p>To improve the learning stability, we organize these tasks as multi-task episodes where each episode contains batch tasks including node classification ("?" for short), edge classification ("?" for short), and graph classification ("?" for short). Let</p><formula xml:id="formula_7">E ? = (T E ? , L E ? , S E ? , Q E ? ) be a multi-task episode. We de- fine task batch T E ? = {T (?) E ? , T (?) E ? , T (? ) E ? } where each subset T (?) E ? = {? ?1 , ? ? ? , ? ?? ? }; loss function sets L E ? = {L (?) , L (?) , L (? ) }, sup- porting data S E ? = {S (?) E ? , S (?) E ? , S (? ) E ? } where each subset S (?) E ? = {D ? ? ?1 , ? ? ? , D ? ? ??? }, and query data Q E ? = {Q (?) E ? , Q (?) E ? , Q (? ) E ? }<label>where</label></formula><formula xml:id="formula_8">S (?) E ? = {D ? ? ?1 , ? ? ? , D ? ? ??? }.</formula><p>Then the multi-task prompting is presented in Algorithm 1. We treat each node/edge/graph class as a binary classification task so that they can share the same task head. Note that our method can also deal with other tasks beyond classification with only a few adaptations (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Why It Works?</head><p>3.5.1 Connection to Existing Work. A prior study on graph prompt is proposed by <ref type="bibr" target="#b26">[27]</ref>, namely GPPT. They use edge prediction as a pre-training pretext and reformulate node classification to the pretext by designing labeled tokens added to the original graph. The compound graph will be sent to the pre-trained model again to predict the link connecting each node to the label tokens. Their work somehow is a special case of our method when our prompt graph only contains isolated tokens, each of which corresponds to a node category. However, there are at least three notable differences:</p><p>(1) GPPT is not flexible to manipulate original graphs; (2) GPPT is only applicable for node classification; and (3) GPPT only supports edge prediction task as the pretext but is not compatible with more advanced graph-level pre-training strategies such as GraphCL <ref type="bibr" target="#b35">[36]</ref>, UGRAPHEMB <ref type="bibr" target="#b1">[2]</ref>, SimGRACE <ref type="bibr" target="#b34">[35]</ref> etc. We further discuss these issues w.r.t. flexibility, efficiency, and compatibility as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Flexibility.</head><p>The nature of prompting is to manipulate the input data to match the pretext. Therefore, the flexibility of data operations is the bottleneck of prompting performance. Let ? be any graph-level transformation such as "changing node features", "adding or removing edges/subgraphs" etc., and ? * be the frozen pre-trained graph model. For any graph G with adjacency matrix A and node feature matrix X, Fang et al. <ref type="bibr" target="#b5">[6]</ref> have proved that we can always learn an appropriate prompt token ? * making the following equation stand:</p><formula xml:id="formula_9">? * A, X + ? * = ? * (?(A, X)) + ? ??<label>(5)</label></formula><p>This means we can learn an appropriate token applied to the original graph to imitate any graph manipulation. Here ? ?? denotes the error bound between the manipulated graph and the prompting graph w.r.t. their representations from the pre-trained graph model. This error bound is related to some non-linear layers of the model (unchangeable) and the quality of the learned prompt (changeable), which is promising to be further narrowed down by a more advanced prompt scheme. In this paper, we extend the standalone token to a prompt graph that has multiple prompt tokens with learnable inner structures. Unlike the indiscriminate inserting in Equation ( <ref type="formula" target="#formula_9">5</ref>) ("X + ? * " means the prompt token should be added to every node of the original graph), the inserting pattern of our proposed prompt graph is highly customized. Let ? (G, G ? ) denote the inserting pattern defined in section 3.3; G is the original graph, and G ? is the prompt graph, then we can learn an optimal prompt graph G * ? to extend Equation ( <ref type="formula" target="#formula_9">5</ref>) as follows:</p><formula xml:id="formula_10">? * ? (G, G * ? ) = ? * (g(A, X)) + ? * ??<label>(6)</label></formula><p>By efficient tuning, the new error bound ? * ?? can be further reduced. In section 4.6, we empirically demonstrate that ? * ?? can be significantly smaller than ? ?? via efficient training. That means our method supports more flexible transformations on graphs to match various pre-training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Efficiency.</head><p>Assume an input graph has ? nodes and ? edges and the prompt graph has ? tokens with ? edges. Let the graph model contain ? layers and the maximum dimension of one layer be ?. The parameter complexity of the prompt graph is only ? (??). In contrast, some typical graph models (e.g., GAT <ref type="bibr" target="#b31">[32]</ref>) usually contain ? (??? 2 + ???) parameters to generate node embedding and additional ? (??) parameters to obtain the whole graph representation (? is the multi-head number). The parameters may be even larger in other graph neural networks (e.g., graph transformer <ref type="bibr" target="#b36">[37]</ref>). In our prompt learning framework, we only need to tune the prompt with the pre-trained graph model frozen, making the training process converge faster than traditional transfer tuning.</p><p>For the time complexity, a typical graph model (e.g., GCN <ref type="bibr" target="#b33">[34]</ref>) usually needs ? (??? 2 +???+??) time to generate node embedding via message passing and then obtain the whole graph representation (e.g., ? (??) for summation pooling). By inserting the prompt into the original graph, the total time is ? (?(?+? )? 2 +?(?+?)?+(?+? )?). Compared with the original time, the additional time cost is only ? (??? 2 +??? +??) where ? ? ?, ? ? ? , ? ? ?.</p><p>Besides the efficient parameter and time cost, our work is also memory friendly. Taking node classification as an example, the memory cost of a graph model largely includes parameters, graph features, and graph structure information. As previously discussed, our method only needs to cache the prompt parameters, which are far smaller than the original graph model. For the graph features and structures, traditional methods usually need to feed the whole graph into a graph model, which needs huge memory to cache these contents. However, we only need to feed an induced graph to the model for each node, the size of which is usually far smaller than the original graph. Notice that in many real-world applications, we are often interested in only a few parts of the total nodes, which means our method can stop timely if there is no more node to be predicted and we do not need to propagate messages on the whole graph either. This is particularly helpful for large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Compatibility.</head><p>Unlike GPPT, which can only use binary edge prediction as a pretext, and is only applicable for node classification as downstream tasks, our framework can support node-level, edge-level, and graph-level downstream tasks, and adopt various graph-level pretexts with only a few steps of tuning. Besides, when transferring the model to different tasks, traditional approaches usually need to additionally tune a task head. In contrast, our method focuses on the input data manipulation and it relies less on the downstream tasks. This means we have a larger tolerance for the task head. For example, in section 4.3, we study the transferability from other domains or tasks but we only adapt our prompt, leaving the source task head unchanged. We can even select some specific pretext and customize the details of our prompt without any tuned task head. Here we present a case that does not need to tune a task head and we evaluate its feasibility in section 4.4.</p><p>Prompt without Task Head Tuning: Pretext: GraphCL <ref type="bibr" target="#b35">[36]</ref>, a graph contrastive learning task that tries to maximize the agreement between a pair of views from the same graph. Downstream Tasks: node/edge/graph classification. Prompt Answer: node classification. Assume there are ? categories for the nodes. We design the prompt graph with ? sub-graphs (a.k.a sub-prompts) where each sub-graph has ? tokens. Each sub-graph corresponds to one node category. Then we can generate ? graph views for all input graphs. We classify the target node with label ? (? = 1, 2, ? ? ? , ?) if the ?-th graph view is closest to the induced graph. It is similar to edge/graph classification.</p><p>Interestingly, by shrinking the prompt graph as isolate tokens aligned with node classes and replacing the induced graphs with the whole graph, our prompt format can degenerate to GPPT, which means we can also leverage edge-level pretext for node classification. Since this format is exactly the same as GPPT, we will not discuss it anymore. Instead, we directly compare GPPT on node classification with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we extensively evaluate our method with other approaches on node-level, edge-level, and graph-level tasks of graphs. In particular, we wish to answer the following research questions: Q1: How effective is our method under the few-shot learning background for multiple graph tasks? Q2: How adaptable is our method when transferred to other domains or tasks? Q3: How do the main components of our method impact the performance? Q4: How efficient is our model compared with traditional approaches? Q5: How powerful is our method when we manipulate graphs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>4.1.1 Datasets. : We compare our methods with other approaches on five public datasets including Cora <ref type="bibr" target="#b33">[34]</ref>, CiteSeer <ref type="bibr" target="#b33">[34]</ref>, Reddit <ref type="bibr" target="#b7">[8]</ref>, Amazon <ref type="bibr" target="#b22">[23]</ref>, and Pubmed <ref type="bibr" target="#b33">[34]</ref>. Detailed statistics are presented in Table <ref type="table" target="#tab_3">1</ref> where the last column refers to the number of node classes. To conduct edge-level and graph-level tasks, we sample edges and subgraphs from the original data where the label of an edge is decided by its two endpoints and the subgraph label follows the majority of the subgraph nodes. For example, if nodes have 3 different classes, say ? 1 , ? 2 , ? 3 , then edge-level tasks contain at least 6 categories (? 1 , ? 2 , ? 3 , ? 1 ? 2 , ? 1 ? 3 , ? 2 ? 3 ). We also evaluate additional graph classification and link prediction on more specialized datasets where the graph label and the link label are inborn and not related to any node (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Approaches. Compared approaches are from three categories:</head><p>(1) Supervised methods: these methods directly train a GNN model on a specific task and then directly infer the result. We here take three famous GNN models including GAT <ref type="bibr" target="#b31">[32]</ref>, GCN <ref type="bibr" target="#b33">[34]</ref>, and Graph Transformer <ref type="bibr" target="#b24">[25]</ref> (short as GT). These GNN models are also included as the backbones of our prompt methods.</p><p>(2) Pre-training with fine-tuning: These methods first pre-train a GNN model in a self-supervised way such as GraphCL <ref type="bibr" target="#b35">[36]</ref> and SimGRACE <ref type="bibr" target="#b34">[35]</ref>, then the pre-trained model will be fine-tuned for a new downstream task.</p><p>(3) Prompt methods: With a pre-trained model frozen and a learnable prompt graph, our prompt method aims to change the input graph and reformulate the downstream task to fit the pre-training strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementations.</head><p>We set the number of graph neural layers as 2 with a hidden dimension of 100. To study the transferability across different graph data, we use SVD (Singular Value Decomposition) to reduce the initial features to 100 dimensions. The token number of our prompt graph is set as 10. We also discuss the impact of token numbers in section 4.4 where we change the token number from 1 to 20. We use the Adam optimizer for all approaches. The learning rate is set as 0.001 for most datasets. In the meta-learning stage, we split all the node-level, edge-level, and graph-level tasks randomly in 1:1 for meta-training and meta-testing. Reported results are averaged on all tested tasks. More implementation details are shown in Appendix A, in which we also analyze the performance on more datasets and more kinds of tasks such as regression, link prediction, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Task Performance with Few-shot Learning Settings (RQ1)</head><p>We compared our prompt-based methods with other mainstream training schemes on node-level, edge-level, and graph-level tasks under the few-shot setting. We repeat the evaluation 5 times and report the average results in Table <ref type="table" target="#tab_5">2</ref>, Table <ref type="table" target="#tab_5">12</ref> (Appendix A), and Table <ref type="table" target="#tab_6">13</ref> (Appendix A). From the results, we can observe that most supervised methods are very hard to achieve better performance compared with pre-train methods and prompt methods. This is because the empirical annotations required by supervised frameworks in the few-shot setting are very limited, leading to poor performance. In contrast, pre-training approaches contain more prior knowledge, making the graph model rely less on data labels. However, to achieve better results on a specific task, we usually need to carefully select an appropriate pre-training approach and carefully tune the model to match the target task, but this huge effort is not ensured to be applicable to other tasks. The gap between pre-training strategies and downstream tasks is still very large, making the graph model very hard to transfer knowledge on multi-task settings (we further discuss the transferability in section 4.3.) Compared with pre-training approaches, our solutions further improve the compatibility of graph models. The reported improvements range from 1.10% to 8.81% on node-level tasks, 1.28% to 12.26% on edge-level tasks, and 0.14% to 10.77% on graph-level tasks. In particular, we also compared our node-level performance with the previously mentioned node-level prompt model GPPT in Table <ref type="table" target="#tab_5">2</ref>. Kindly note that our experiment settings are totally different from GPPT. In GPPT, they study the few-shot problem by masking 30% or 50% data labels. However, in our paper, we propose a more challenging problem: how does the model perform if we further reduce the label data? So in our experiment, each class only has 100 labeled samples. This different setting makes our labeled ratio approximately only 25% on Cora, 18% on CiteSeer, 1.7% on Reddit, 7.3% on Amazon, and 1.5% on Pubmed, which are far less than the reported GPPT (50% labeled).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transferability Analysis (RQ2)</head><p>To evaluate the transferability, we compared our method with the hard transfer method and the fine-tuning method. Here the hard transfer method means we seek the source task model which has the same task head as the target task and then we directly conduct the model inference on the new task. The fine-tune method means we load the source task model and then tune the task head for the new task. We evaluate the transferability from two perspectives: (1) how effectively is the model transferred to different tasks within the same domain? and (2) how effectively is the model transferred to different domains?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Transferability to Different Level Tasks.</head><p>Here we pre-train the graph neural network on Amazon, then conduct the model on two source tasks (graph level and node level), and further evaluate the performance on the target task (edge level). For simplicity, both source tasks and the target task are built as binary classifications with 1 : 1 positive and negative samples (we randomly select a class as the positive label and sample negatives from the rest). We report the results in Table <ref type="table" target="#tab_6">3</ref>, from which we have two observations: First, our prompt method significantly outperforms the other approaches and the prediction results make sense. In contrast, the problem of the hard transfer method is that the source model sometimes can not well decide on the target tasks because the target classes may be far away from the source classes. This may even cause negative transfer results (results that are lower than random guess). In most cases, the fine-tuning method can output meaningful results with a few steps of tuning but it can still encounter a negative transfer problem. Second, the graph-level task has better adaptability than the node-level task for the edge-level target, which is in line with our previous intuition presented in Figure <ref type="figure" target="#fig_3">3</ref> (section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Transferability to Different Domains.</head><p>We also conduct the model on Amazon and PubMed as source domains, then load the model states from these source domains and report the performance on the target domain (Cora). Since different datasets have various input feature dimensions, we here use SVD to unify input features from all domains as 100 dimensions. Results are shown in Table <ref type="table" target="#tab_7">4</ref>, from which we can find that the good transferability of our prompt also exists when we deal with different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study (RQ3)</head><p>In this section, we compare our complete framework with four variants: "w/o meta" is the prompt method without meta-learning   any across links between prompt tokens and the input graphs. We report the performance in Figure <ref type="figure" target="#fig_5">5</ref>, from which we can find the meta-learning and token structure all contribute significantly to the final results. In particular, the inserting pattern between a prompt graph and the input graph plays a very crucial role in the final performance. As previously discussed, the purpose of the promptbased method is to relieve the difficulty of traditional "pre-train, fine-tuning" by filling the gap between the pre-training model and the task head. This means the prompt graph is proposed to further improve the fine-tuning performance. This is particularly important when we transfer the model across different tasks/domains, which proposes harder demand for the task head. As suggested in Figure <ref type="figure" target="#fig_5">5</ref>, even when we totally remove the tunable task head, the "w/o h" variant can still perform very competitively, which suggests the powerful capability of bridging upstream and downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency Analysis (RQ4)</head><p>Figure <ref type="figure" target="#fig_6">6</ref> presents the impact of increasing token number on the model performance, from which we can find that most tasks can reach satisfactory performance with very limited tokens, making the complexity of the prompt graph very small. The limited token numbers make our tunable parameter space far smaller than traditional methods, which can be seen in Table <ref type="table" target="#tab_8">5</ref>. This means our method can be efficiently trained with a few steps of tuning. As shown in Figure <ref type="figure" target="#fig_7">7</ref>, the prompt-based method converges faster than traditional pre-train and supervised methods, which further suggests the efficiency advantages of our method.   As discussed in section 3.5.2, the flexibility of data transformation is the bottleneck of prompt-based methods. Here we manipulate several graphs by dropping nodes, dropping edges, and masking features, then we calculate the error bound mentioned in Equation 5 and 6. We compare the original error with the naive prompt mentioned in Equation <ref type="formula" target="#formula_9">5</ref>, and our prompt graph with 3, 5, and 10 tokens. As shown in Table <ref type="table" target="#tab_10">6</ref>, our designed prompt graph significantly reduces the error between the original graph and the manipulated graph. This means our method is more powerful to stimulate various graph transformations and can further support significant improvement for downstream tasks. This capability can also be observed in the graph visualization from two approaches. As shown in Figure <ref type="figure" target="#fig_9">8</ref>, the graph representations from a pre-trained model present lower resolution to node classes compared with our prompted graph.</p><formula xml:id="formula_11">GAT ? 155K ? 382K ? 75K ? 88K ? 61K 95.4? GCN ? 154K ? 381K ? 75K ? 88K ? 61K 95.4? GT ? 615K ? 1.52M ? 286K ? 349K ? 241K 98.8? prompt ? 7K ? 19K ? 3K ? 4K ? 3K -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we study the multi-task problem of graph prompts with few-shot settings. We propose a novel method to reformulate different-level tasks to unified ones and further design an effective prompt graph with a meta-learning technique. We extensively evaluate the performance of our method. Experiments demonstrate the effectiveness of our framework.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In this section, we supplement more experiments to evaluate the effectiveness of our framework further. The source code is publicly available at https://anonymous.4open.science/r/mpg Additional Datasets Besides the datasets mentioned in the main experiments of our paper, we here supplement more datasets in Table <ref type="table" target="#tab_13">7</ref> to further evaluate the effectiveness of our framework. Specifically, ENZYMES and ProteinsFull are two molecule/protein datasets that are used in our additional graph-level classification tasks. Movielens and QM9 are used to evaluate the performance of our method on edge-level and graph-level regression, respectively.</p><p>In particular, Movielens contains user's rating scores to the movies, each edge in which has a score value ranging from 0 to 5. QM9 is a molecule graph dataset where each graph has 19 regression targets, which are treated as graph-level multi-output regression. Person-alityCafe and Facebook datasets are used to test the performance of link prediction, both of which are social networks where edges denote the following/quoting relations.</p><p>Multi-label v.s. Multi-class Classification In the main experiments, we treat the classification task as a multi-label problem. Here we present the experimental results under a multi-class setting. As reported in Table <ref type="table" target="#tab_14">8</ref>, our prompt-based method still outperforms the rest methods.</p><p>Additional Graph-level Classification Here, we evaluate the graph-level classification performance where the graph label is not impacted by nodes' attributes. As shown in Table <ref type="table" target="#tab_11">9</ref>, our method is more effective in the multi-class graph classification, especially in the few-shot setting.</p><p>Edge/Graph-level Regression Beyond classification tasks, our method can also support to improve graph models on regression tasks. Here, we evaluate the regression performance of both graphlevel (QM9) and edge-level (MovieLens) datasets by MAE (mean absolute error) and MSE (mean squared error). We only feed 100shot edge induced graphs for the model and the results are shown in Table <ref type="table" target="#tab_12">10</ref>, from which we can observe that our prompt-based methods outperform traditional approaches.</p><p>Link Prediction Beyond edge classification, link prediction is also a widely studied problem in the graph learning area. Here, the edges are split into three parts: (1) 80% of the edges are for message passing only. (2) 10% of the rest edges as the supervision training set. and (3) the rest edges as the testing set. For each edge in the training set and the testing set, we treat these edges as positive samples and sample non-adjacent nodes as negative samples. We generate the edge-induced graph for these node pairs according to the first part edges. The graph label is assigned as positive if the node pairs have a positive edge and vice versa. To further evaluate our method's potential in the extremely limited setting, we only sample 100 positive edges from the training set to train our model. In the testing stage, each positive edge has 100 negative edges. We evaluate the performance by MRR (mean reciprocal rank), and Hit Ratio@ 1, 5, 10. Results from Table <ref type="table" target="#tab_15">11</ref> demonstrate that the performance of our prompt-based method still keeps the best in most cases. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fine-tuning, Pre-training, and Prompting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our graph prompt inspired by the language prompt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Task space in NLP and graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Induced graphs for nodes and edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effectiveness of main components</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of token numbers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Training losses with epochs. Mean values and 65% confidence intervals by 5 repeats with different seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of graph representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>denotes the set of prompt tokens and |P | is the number of tokens. Each token ? ? ? P can be represented by a token vector p ? ? R 1?? with the same size of node features in the input graph; Note that in practice, we usually have |P | ? ? and |P | ? ? ? where ? ? is the size of the hidden layer in the pre-trained graph model. With these token vectors, the input graph can be reformulated by adding the ?-th token to graph node ? ? (e.g., x? = x ? + p ? ). Then, we replace the input features with the prompted features and send them to the pre-trained model for further processing.3.3.3 TokenStructures. S = {(? ? , ? ? )|? ? , ? ? ? P} is the token structure denoted by pair-wise relations among tokens. Unlike the NLP prompt, the token structure in the prompt graph is usually implicit. To solve this problem, we propose three methods to design the prompt token structures: (1) the first way is to learn tunable parameters:</figDesc><table /><note><p><p>is the node set containing ? nodes; each node has a feature vector denoted by x ? ? R 1?? for node ? ? ; E = {(? ? , ? ? )|? ? , ? ? ? V} is the edge set where each edge connects a pair of nodes in V. With the previous discussion, we here present our prompt graph as G ? = (P, S) where P = {? 1 , ? 2</p>, ? ? ? , ? | P | }</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>3.4.1 Constructing Meta Prompting Tasks. Let ? ? be the ?-th task with supporting data D ? ? ? and query data D ? ? ? contain labeled graphs; for the node classification task, we generate an induced graph for each node as mentioned in section 3.2.3, align the graph label with the target node label, and treat this graph as a member in D ? ? ? or D</figDesc><table><row><cell>? ? ? ; Specifically, for the</cell></row><row><cell>graph classification task, D ? ? ? and D</cell></row><row><cell>? ? ? ;</cell></row><row><cell>for the edge classification task, we first generate edge induced</cell></row><row><cell>graphs for training and testing and the edge label is up to its two</cell></row><row><cell>endpoints.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Overall Learning Process Input: Overall pipeline ? ?,? |? * with prompt parameter ? , pre-trained model with frozen parameter ? * , and task head parameterized by ?; Multi-task episodesE = {E 1 , ? ? ? , E ? }; Output: Optimal pipeline ? ? * ,? * |? * ? ? E where E ? = (T E ? , L E ? , S E ? , Q E ? ) 4 for ? ?? ? T E ? , ? = ?, ?, ? do 5 ? ? ?? , ? ? ?? ? ?, ? 6 ? ? ?? ? ? ? ?? -?? ? ? ?? L ? ? ? ?? ,? ? ?? |? * 7 ? ? ?? ? ? ? ?? -?? ? ? ?? L ?? |? ?? ? T E ? , ? = ?,?, ? } 10 end 11 return ? ? * ,? * |? *</figDesc><table><row><cell cols="2">1 Initialize ? and ?</cell></row><row><cell cols="2">2 while not done do</cell></row><row><cell></cell><cell>// inner adaptation</cell></row><row><cell></cell><cell>(?)</cell></row><row><cell></cell><cell>D ? ? ??</cell></row><row><cell></cell><cell>(?) D ? ? ??</cell><cell>? ? ? ?? ,? ? ?? |?  *</cell></row><row><cell>8</cell><cell>end</cell></row><row><cell></cell><cell>// outer meta update</cell></row></table><note><p>3 Sample E 9 Update ?, ? by Equation (4) on Q E ? = {D ? ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell cols="2">Dataset #Nodes</cell><cell>#Edges</cell><cell cols="2">#Features #Labels</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>CiteSeer</cell><cell>3,327</cell><cell>9,104</cell><cell>3,703</cell><cell>6</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 23,213,838</cell><cell>602</cell><cell>41</cell></row><row><cell cols="2">Amazon 13,752</cell><cell>491,722</cell><cell>767</cell><cell>10</cell></row><row><cell cols="2">Pubmed 19,717</cell><cell>88,648</cell><cell>500</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Node-level performance (%) with 100-shot setting. IMP (%): the average improvement of prompt over the rest.</figDesc><table><row><cell>Training schemes</cell><cell>Methods</cell><cell cols="9">Cora Acc F1 AUC Acc F1 AUC Acc F1 AUC Acc F1 AUC Acc F1 AUC CiteSeer Reddit Amazon Pubmed</cell></row><row><cell></cell><cell>GAT</cell><cell cols="9">74.45 73.21 82.97 83.00 83.20 89.33 55.64 62.03 65.38 79.00 73.42 97.81 75.00 77.56 79.72</cell></row><row><cell>supervised</cell><cell>GCN</cell><cell cols="9">77.55 77.45 83.71 88.00 81.79 94.79 54.38 52.47 56.82 95.36 93.99 96.23 53.64 66.67 69.89</cell></row><row><cell></cell><cell>GT</cell><cell cols="9">74.25 75.21 82.04 86.33 85.62 90.13 61.50 61.38 65.56 85.50 86.01 93.01 51.50 67.34 71.91</cell></row><row><cell></cell><cell>GraphCL+GAT</cell><cell cols="9">76.05 76.78 81.96 87.64 88.40 89.93 57.37 66.42 67.43 78.67 72.26 95.65 76.03 77.05 80.02</cell></row><row><cell>pre-train + fine-tune</cell><cell>GraphCL+GCN GraphCL+GT SimGRACE+GAT SimGRACE+GCN</cell><cell cols="9">78.75 79.13 84.90 87.49 89.36 90.25 55.00 65.52 74.65 96.00 95.92 98.33 69.37 70.00 74.74 73.80 74.12 82.77 88.50 88.92 91.25 63.50 66.06 68.04 94.39 93.62 96.97 75.00 78.45 75.05 76.85 77.48 83.37 90.50 91.00 91.56 56.59 65.47 67.77 84.50 84.73 89.69 72.50 68.21 81.97 77.20 76.39 83.13 83.50 84.21 93.22 58.00 55.81 56.93 95.00 94.50 98.03 77.50 75.71 87.53</cell></row><row><cell></cell><cell>SimGRACE+GT</cell><cell cols="9">77.40 78.11 82.95 87.50 87.05 91.85 66.00 69.95 70.03 79.00 73.42 97.58 70.50 73.30 74.22</cell></row><row><cell></cell><cell>GraphCL+GAT</cell><cell cols="9">76.50 77.26 82.99 88.00 90.52 91.82 57.84 67.02 75.33 80.01 75.62 97.96 77.50 78.26 83.02</cell></row><row><cell></cell><cell>GraphCL+GCN</cell><cell cols="9">79.20 79.62 85.29 88.50 91.59 91.43 56.00 68.57 78.82 96.50 96.37 98.70 72.50 72.64 79.57</cell></row><row><cell>prompt</cell><cell>GraphCL+GT SimGRACE+GAT</cell><cell cols="9">75.00 76.00 83.36 91.00 91.00 93.29 65.50 66.08 68.86 95.50 95.43 97.56 76.50 79.11 76.00 76.95 78.51 83.55 93.00 93.14 92.44 57.63 66.64 69.43 95.50 95.43 97.56 73.00 74.04 81.89</cell></row><row><cell></cell><cell>SimGRACE+GCN</cell><cell cols="9">77.85 76.57 83.79 90.00 89.47 94.87 59.50 55.97 59.46 95.00 95.24 98.42 78.00 78.22 87.66</cell></row><row><cell></cell><cell>SimGRACE+GT</cell><cell cols="9">78.75 79.53 85.03 91.00 91.26 95.62 69.50 71.43 70.75 86.00 83.72 98.24 73.00 73.79 76.64</cell></row><row><cell></cell><cell>IMP (%)</cell><cell cols="9">1.47 1.94 1.10 3.81 5.25 2.05 3.97 5.04 6.98 4.49 5.84 2.24 8.81 4.55 4.62</cell></row><row><cell cols="3">Reported Acc of GPPT (Label Ratio 50%) 77.16 -</cell><cell>-</cell><cell>65.81 -</cell><cell>-</cell><cell>92.13 -</cell><cell>-</cell><cell>86.80 -</cell><cell>-</cell><cell>72.23 -</cell><cell>-</cell></row><row><cell cols="2">appr. Label Ratio of our 100-shot setting</cell><cell>? 25%</cell><cell></cell><cell>? 18%</cell><cell></cell><cell>? 1.7%</cell><cell></cell><cell>? 7.3%</cell><cell></cell><cell>? 1.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Transferability (%) on Amazon from different level tasks spaces. Source tasks: graph-level tasks and node-level tasks. Target task: edge-level tasks.</figDesc><table><row><cell cols="5">Source task Methods Accuracy F1-score AUC score</cell></row><row><cell></cell><cell>hard</cell><cell>51.50</cell><cell>65.96</cell><cell>40.34</cell></row><row><cell>graph level</cell><cell cols="2">fine-tune 62.50</cell><cell>70.59</cell><cell>53.91</cell></row><row><cell></cell><cell>prompt</cell><cell>70.50</cell><cell>71.22</cell><cell>74.02</cell></row><row><cell></cell><cell>hard</cell><cell>40.50</cell><cell>11.85</cell><cell>29.48</cell></row><row><cell>node level</cell><cell cols="2">fine-tune 46.00</cell><cell>54.24</cell><cell>37.26</cell></row><row><cell></cell><cell>prompt</cell><cell>59.50</cell><cell>68.73</cell><cell>55.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Transferability (%) from different domains. Source domains: Amazon and PubMed. Target domain: Cora</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>full</cell><cell cols="2">w/o meta</cell><cell>w/o h</cell><cell>w/o token structure</cell><cell>w/o inserting</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.00</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Acc</cell><cell>F1</cell><cell>AUC</cell><cell>Acc</cell><cell>F1</cell><cell>AUC</cell><cell>Acc</cell><cell>F1</cell><cell>AUC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>node-level</cell><cell></cell><cell>edge-level</cell><cell>graph-level</cell></row><row><cell cols="2">Source Domains</cell><cell>Amazon</cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tasks</cell><cell cols="6">hard fine-tune prompt hard fine-tune prompt</cell><cell></cell><cell></cell></row><row><cell>node level</cell><cell>Acc F1 AUC 17.56 26.9 13.11</cell><cell>64.14 77.59 88.79</cell><cell>65.07 80.23 92.59</cell><cell>55.62 66.33 82.34</cell><cell>57.93 70.00 83.34</cell><cell>62.07 76.60 88.46</cell><cell></cell><cell></cell></row><row><cell>edge level</cell><cell>Acc 17.00 F1 10.51 AUC 4.26</cell><cell>77.00 81.58 94.27</cell><cell>82.00 84.62 96.19</cell><cell>10.00 2.17 6.15</cell><cell>90.50 89.73 93.89</cell><cell>96.50 91.80 94.70</cell><cell></cell><cell></cell></row><row><cell>graph level</cell><cell>Acc 46.00 F1 62.76 AUC 54.23</cell><cell>87.50 89.11 86.33</cell><cell>88.00 88.12 94.99</cell><cell>50.00 10.00 90.85</cell><cell>91.00 93.90 91.47</cell><cell>95.50 95.60 98.47</cell><cell></cell><cell></cell></row><row><cell cols="7">step; "w/o h" is our method without task head tuning, which is</cell><cell></cell><cell></cell></row><row><cell cols="7">previously introduced in section 3.5.4; "w/o token structure" is</cell><cell></cell><cell></cell></row><row><cell cols="7">the prompt where all the tokens are treated as isolated without</cell><cell></cell><cell></cell></row><row><cell cols="7">any inner connection; and "w/o inserting" is the prompt without</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Tunable parameters comparison. RED (%): average reduction of the prompt method to others.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>CiteSeer Reddit Amazon Pubmed RED (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Error bound discussed by section 3.5.2 RED (%): average reduction of each method to the original error.</figDesc><table><row><cell>Prompt Solutions</cell><cell cols="2">Token Number</cell><cell>Drop Nodes</cell><cell>Drop Edges</cell><cell>Mask Features</cell><cell>RED (%)</cell></row><row><cell>Original Error (without prompt)</cell><cell></cell><cell>0</cell><cell cols="2">0.9917 2.6330</cell><cell>6.8209</cell><cell>-</cell></row><row><cell>Naive Prompt (Equation 5)</cell><cell></cell><cell>1</cell><cell cols="2">0.8710 0.5241</cell><cell>2.0835</cell><cell>66.70?</cell></row><row><cell>Our Prompt Graph</cell><cell></cell><cell>3</cell><cell cols="2">0.0875 0.2337</cell><cell>0.6542</cell><cell>90.66?</cell></row><row><cell cols="2">(with token, structure,</cell><cell>5</cell><cell cols="2">0.0685 0.1513</cell><cell>0.4372</cell><cell>93.71?</cell></row><row><cell cols="2">and inserting patterns)</cell><cell>10</cell><cell cols="2">0.0859 0.1144</cell><cell>0.2600</cell><cell>95.59?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Additional graph-level classification.</figDesc><table><row><cell>Methods</cell><cell cols="4">ProteinsFull (100 shots) ENZYMES (50 shots) Acc (%) Macro F1 (%) Acc (%) Macro F1 (%)</cell></row><row><cell>Supervised</cell><cell>66.64</cell><cell>65.03</cell><cell>31.33</cell><cell>30.25</cell></row><row><cell>Pre-train + Fine-tune</cell><cell>66.50</cell><cell>66.43</cell><cell>34.67</cell><cell>33.94</cell></row><row><cell>Prompt</cell><cell>70.50</cell><cell>70.17</cell><cell>35.00</cell><cell>34.92</cell></row><row><cell>Prompt w/o h</cell><cell>68.50</cell><cell>68.50</cell><cell>36.67</cell><cell>34.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Graph/edge-level regression with few-shot settings.</figDesc><table><row><cell>Tasks</cell><cell cols="2">Graph Regression</cell><cell cols="2">Edge Regression</cell></row><row><cell>Datasets</cell><cell cols="4">QM9 (100 shots) MovieLens (100 shots)</cell></row><row><cell>Methods</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell>Supervised</cell><cell>0.3006</cell><cell>0.1327</cell><cell>0.2285</cell><cell>0.0895</cell></row><row><cell cols="2">Pre-train + Fine-tune 0.1539</cell><cell>0.0351</cell><cell>0.2171</cell><cell>0.0774</cell></row><row><cell>Prompt</cell><cell>0.1384</cell><cell>0.0295</cell><cell>0.1949</cell><cell>0.0620</cell></row><row><cell>Prompt w/o h</cell><cell>0.1424</cell><cell>0.0341</cell><cell>0.2120</cell><cell>0.0744</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Statistics of Additional Datasets</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Nodes #Edges #Features #Labels #Graphs</cell></row><row><cell>ENZYMES</cell><cell>19,580</cell><cell>74,564</cell><cell>21</cell><cell>6</cell><cell>600</cell></row><row><cell>ProteinsFull</cell><cell>43,471</cell><cell>162,088</cell><cell>32</cell><cell>2</cell><cell>1,113</cell></row><row><cell>Movielens</cell><cell>10,352</cell><cell>100,836</cell><cell>100</cell><cell>-</cell><cell>1</cell></row><row><cell>QM9</cell><cell cols="2">2,333,625 4,823,498</cell><cell>16</cell><cell>-</cell><cell>129,433</cell></row><row><cell cols="3">PersonalityCafe 100,340 3,788,032</cell><cell>100</cell><cell>0</cell><cell>1</cell></row><row><cell>Facebook</cell><cell>4,039</cell><cell>88,234</cell><cell>1,283</cell><cell>0</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Multi-class node classification (100-shots)</figDesc><table><row><cell>Methods</cell><cell cols="4">Cora Acc (%) Macro F1 (%) Acc (%) Macro F1 (%) CiteSeer</cell></row><row><cell>Supervised</cell><cell>74.11</cell><cell>73.26</cell><cell>77.33</cell><cell>77.64</cell></row><row><cell>Pre-train and Fine-tune</cell><cell>77.97</cell><cell>77.63</cell><cell>79.67</cell><cell>79.83</cell></row><row><cell>Prompt</cell><cell>80.12</cell><cell>79.75</cell><cell>80.50</cell><cell>80.65</cell></row><row><cell>Prompt w/o h</cell><cell>78.55</cell><cell>78.18</cell><cell>80.00</cell><cell>80.05</cell></row><row><cell>Reported Acc of GPPT (Label Ratio 50%)</cell><cell>77.16</cell><cell>-</cell><cell>65.81</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Evaluation on link prediction (100-shot settings)</figDesc><table><row><cell>Datasets</cell><cell></cell><cell cols="2">PersonalityCafe</cell><cell></cell><cell></cell><cell cols="2">Facebook</cell><cell></cell></row><row><cell>Methods</cell><cell cols="8">MRR Hit@1 Hit@5 Hit@10 MRR Hit@1 Hit@5 Hit@10</cell></row><row><cell>Supervised</cell><cell>0.18</cell><cell>0.04</cell><cell>0.24</cell><cell>0.56</cell><cell>0.13</cell><cell>0.06</cell><cell>0.17</cell><cell>0.35</cell></row><row><cell>Pre-train + Fine-tune</cell><cell>0.13</cell><cell>0.05</cell><cell>0.12</cell><cell>0.34</cell><cell>0.10</cell><cell>0.02</cell><cell>0.16</cell><cell>0.33</cell></row><row><cell>Prompt</cell><cell>0.20</cell><cell>0.07</cell><cell>0.32</cell><cell>0.60</cell><cell>0.19</cell><cell>0.10</cell><cell>0.23</cell><cell>0.39</cell></row><row><cell cols="2">Prompt w/o h 0.20</cell><cell>0.06</cell><cell>0.30</cell><cell>0.50</cell><cell>0.15</cell><cell>0.09</cell><cell>0.15</cell><cell>0.33</cell></row><row><cell>Label Ratio</cell><cell></cell><cell cols="3">? 0.003% (training) ? 80%(message passing)</cell><cell cols="4">? 0.1% (training) ? 80%(message passing)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research is supported in part by project #<rs type="grantNumber">MMT-p2-23</rs> of the <rs type="affiliation">Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong</rs>, by grants from the <rs type="funder">Research Grant Council of the Hong Kong Special Administrative Region, China</rs> (No. <rs type="grantNumber">CUHK 14217622</rs>), <rs type="funder">NSFC</rs> (No. <rs type="grantNumber">61972087</rs>, No. <rs type="grantNumber">62206067</rs>, No. <rs type="grantNumber">U1936205</rs>, No. <rs type="grantNumber">62172300</rs>, No. <rs type="grantNumber">62202336</rs>), <rs type="funder">Guangzhou-HKUST(GZ) Joint Funding Scheme</rs> (No. <rs type="grantNumber">2023A03J0673</rs>), <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2022YFB3104300</rs>, No. <rs type="grantNumber">2021YFC3300300</rs>), the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> (No. <rs type="grantNumber">ZD-21-202101</rs>), and <rs type="funder">Open Research Projects of Zhejiang Lab</rs> (No. <rs type="grantNumber">2021KH0AB04</rs>). The first author, <rs type="person">Dr. Xiangguo Sun</rs>, in particular, wants to thank his parents for their kind support during his tough period.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6gnrPyS">
					<idno type="grant-number">MMT-p2-23</idno>
				</org>
				<org type="funding" xml:id="_b9fKqv5">
					<idno type="grant-number">CUHK 14217622</idno>
				</org>
				<org type="funding" xml:id="_uXeceh2">
					<idno type="grant-number">61972087</idno>
				</org>
				<org type="funding" xml:id="_SfMCXYb">
					<idno type="grant-number">62206067</idno>
				</org>
				<org type="funding" xml:id="_2xzBKfk">
					<idno type="grant-number">U1936205</idno>
				</org>
				<org type="funding" xml:id="_8gg3avF">
					<idno type="grant-number">62172300</idno>
				</org>
				<org type="funding" xml:id="_fNvYGn2">
					<idno type="grant-number">62202336</idno>
				</org>
				<org type="funding" xml:id="_AU9VjtP">
					<idno type="grant-number">2023A03J0673</idno>
				</org>
				<org type="funding" xml:id="_GE9uRdH">
					<idno type="grant-number">2022YFB3104300</idno>
				</org>
				<org type="funding" xml:id="_kTy4uvd">
					<idno type="grant-number">2021YFC3300300</idno>
				</org>
				<org type="funding" xml:id="_S5m2hGf">
					<idno type="grant-number">ZD-21-202101</idno>
				</org>
				<org type="funding" xml:id="_HwumqUr">
					<idno type="grant-number">2021KH0AB04</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient top-k shortest-path distance queries on large networks by pruned landmark labeling</title>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nozomi</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised inductive graph-level representation learning via graph-graph proximity</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><surname>Marinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1988">2019. 1988-1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-level graph convolutional networks for crossplatform anchor link prediction</title>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Musial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1503" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning</title>
		<author>
			<persName><forename type="first">Junru</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2741" to="2751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Prompt Tuning for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Taoran</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15240</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making Pre-trained Language Models Better Few-shot Learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Multi-Strategy based Pre-Training Method for Cold-Start Recommendation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GraphMAE: Self-Supervised Masked Graph Autoencoders</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="594" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Strategies For Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wiener Graph Deconvolutional Network Improves Graph Self-Supervised Learning</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Jiashun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fugee</forename><surname>Tsung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Power of Scale for Parameter-Efficient Prompt Tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting path failure in time-evolving graphs</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujia</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2149" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayley</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elior</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Pouran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilana</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01243</idno>
		<title level="m">Recent advances in natural language processing via large pre-trained language models: A survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring low-dimensional intrinsic task subspace via prompt tuning</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07867</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Getting closer to AI complete question answering: A set of prerequisite real tasks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8722" to="8731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13624</idno>
		<title level="m">Towards out-of-distribution generalization: A survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semisupervised classification</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GPPT: Graph pre-training and prompt tuning to generalize graph neural networks</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1717" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised Hypergraph Representation Learning for Sociological Analysis</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-level hyperedge distillation for social linking prediction on sparsely observed networks</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2934" to="2945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure Learning Via Meta-Hyperedge for Dynamic Rumor Detection</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking Graph Neural Networks for Anomaly Detection</title>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Afec: Active forgetting of negative transfer in continual learning</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingtian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22379" to="22391" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">J. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Factual Probing Is [MASK]: Learning vs. Learning to Recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient single-source shortest path and distance queries on large graphs</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Diwen Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="998" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
