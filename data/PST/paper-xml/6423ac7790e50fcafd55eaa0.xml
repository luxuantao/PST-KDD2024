<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-28">28 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Vladislav</forename><surname>Lialin</surname></persName>
							<email>vlialin@cs.uml.edu</email>
						</author>
						<author>
							<persName><forename type="first">Vijeta</forename><surname>Deshpande</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UMass Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UMass Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UMass</orgName>
								<address>
									<settlement>Lowell Alexa AI</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-28">28 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.15647v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a systematic overview and comparison of parameter-efficient finetuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rich Sutton, The Bitter Lesson</head><p>In October 2018, BERT Large <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> with 350 million parameters was the biggest Transformer model <ref type="bibr" target="#b58">(Vaswani et al., 2017</ref>) ever trained. At the time, contemporary hardware struggled to fine-tune this model. The section "Out-of-memory issues" on BERT's GitHub<ref type="foot" target="#foot_0">1</ref> specifies the maximum batch size for BERT Large given 12Gb of GPU RAM and 512 tokens as zero. Four years in, publically available models grew up to 176 billion parameters <ref type="bibr">(Scao et al., 2022;</ref><ref type="bibr" target="#b66">Zhang et al., 2022;</ref><ref type="bibr" target="#b64">Zeng et al., 2022)</ref>, by a factor of 500. Published literature includes models up to 1 trillion parameters <ref type="bibr">(Chowdhery et al., 2022;</ref><ref type="bibr" target="#b53">Shoeybi et al., 2019;</ref><ref type="bibr" target="#b17">Fedus et al., 2021)</ref>. However, single-GPU RAM increased less than 10 times (to 80Gb) due to the high cost of HBM memory. Model size scales almost two orders of magnitude quicker than computational resources making fine-tuning the largest models to downstream tasks infeasible for most and impractical for everyone.</p><p>In-context learning <ref type="bibr" target="#b46">(Radford et al., 2019)</ref> thus became the new normal, the standard way to pass downstream task training data to billion-scale language models. However, the limited context size of Transformers artificially limits the training set size to just a few examples, typically less than 100. This constraint, coupled with the absence of in-context learning performance guarantees even on the training data, presents a challenge. Additionally, expanding the context size leads to a quadratic increase in inference costs. Even though language models perform exceptionally well <ref type="bibr" target="#b5">(Brown et al., 2020)</ref> in a few-shot scenario, "get more data" is still the most reliable way to improve on any given task<ref type="foot" target="#foot_1">2</ref> . Thus, we, as a community of researchers and engineers, need efficient ways to train on downstream task data.</p><p>Parameter-efficient fine-tuning, which we denote as PEFT, aims to resolve this problem by only training a small set of parameters which might be a subset of the existing model parameters or a set of newly added parameters. These methods differ in parameter efficiency, memory efficiency, training speed, final quality of the model, and additional inference costs (if any). In the last few years, more than a hundred of PEFT papers have been published, with several studies <ref type="bibr" target="#b14">(Ding et al., 2022)</ref> providing a good overview of the most popular methods, such as Adapters <ref type="bibr" target="#b27">(Houlsby et al., 2019)</ref>, BitFit <ref type="bibr" target="#b4">(Ben-Zaken et al., 2021)</ref>, LoRa <ref type="bibr" target="#b28">(Hu et al., 2021)</ref>, <ref type="bibr">Compacter (Karimi Mahabadi et al., 2021)</ref>, and Soft Prompts <ref type="bibr" target="#b38">(Liu et al., 2021;</ref><ref type="bibr" target="#b36">Li and Liang, 2021)</ref>. Recently, <ref type="bibr" target="#b43">Pfeiffer et al. (2023)</ref> presented a survey on modular deep learning overviewing similar methods from the perspective of modularity and multi-task inference.</p><p>This survey presents a systematic overview, comparison, and taxonomy of 30 parameterefficient fine-tuning methods with 20 methods discussed in-depth, covering over 40 papers published from February 2019 to February 2023. We highlight the current unresolved challenges in PEFT, including the limited theoretical understanding, the gap between PEFT and fine-tuning performance, and reporting issues. In conclusion, we suggest several avenues for improvement, such as developing standardized PEFT benchmarks, investigating novel reparameterization techniques with superior parameter-to-rank ratios, conducting in-depth studies on hyperparameters and interpretability, and drawing inspiration from ondevice (edge) machine learning research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Transformer</head><p>Many of the parameter-efficient fine-tuning techniques discussed in this survey can be applied to all neural networks, but some are specifically designed to take advantage of the Transformer architecture <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>. Given that Transformers are the largest neural networks ever trained, these methods are particularly valuable. Thus, we present a brief overview of the Transformer to provide context for these techniques.</p><p>The core building block of the Transformer architecture consists of multi-head attention (MHA) followed by a fully-connected network (FFN), as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Both attention and fullyconnected layers incorporate residual connections <ref type="bibr" target="#b24">(He et al., 2016)</ref> and Layer Normalization <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> to improve trainability.</p><p>The heart of the Transformer is attention operation. Following the NamedTensor notation <ref type="bibr" target="#b8">(Chiang et al., 2021)</ref>, it can be described as 3 Taxonomy of PEFT: a birds-eye view PEFT methods can be classified in multiple ways. They may be differentiated by their underlying approach or conceptual framework: does the method introduce new parameters to the model, or does it fine-tune a small subset of existing parameters? Alternatively, they may be categorized according to their primary objective: does the method aim to minimize memory footprint or only storage efficiency? In this section, we begin by presenting a taxonomy based on the former. We depict this taxonomy and 30 PEFT methods in Figure <ref type="figure" target="#fig_1">2</ref>. Sections 3.1-3.4 give a brief taxonomy overview. Then, based on our taxonomy classification, we describe 20 PEFT methods in detail, accompanied by the pseudocode in Sections 6 -11.</p><formula xml:id="formula_0">Att : R key ? R seq?key ? R seq?val ? R val Att(Q, K, V ) = ? ? softmax seq Q key K |key| ? ? seq V Q(x) = x ? W Q + b k , K(x) = x ? W K + b q , V (x) = x ? W V + b v , W Q , W K ? R input?key , W V ? R input?val b Q , b K ? R key , b V ? R val</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Additive methods</head><p>The main idea behind additive methods is augmenting the existing pre-trained model with extra parameters or layers and training only the newly added parameters. As of now, this is the largest and widely explored category of PEFT methods. Within this category, two large subcategories have emerged: Adapter-like methods and soft prompts.</p><p>Adapters Adapters <ref type="bibr" target="#b27">(Houlsby et al., 2019)</ref> are a type of additive parameter-efficient fine-tuning method that involves introducing small fullyconnected networks after Transformer sub-layers. The idea has been widely adopted <ref type="bibr">(Pfeiffer et al., 2020b)</ref> 3 , and multiple variations of Adapters have been proposed. These variations include modifying the placement of adapters <ref type="bibr">(He et al., 2022a;</ref><ref type="bibr" target="#b67">Zhu et al., 2021)</ref>, pruning <ref type="bibr">(He et al., 2022b)</ref>, and using reparametrization to reduce the number of trainable parameters <ref type="bibr" target="#b29">(Karimi Mahabadi et al., 2021;</ref><ref type="bibr" target="#b16">Edalati et al., 2022)</ref>.</p><p>Soft Prompts Language model prompting <ref type="bibr" target="#b46">(Radford et al., 2019)</ref> aims to control the behavior of a language model by modifying the input text, which typically consists of a task description accompanied by a few in-context examples. However, these methods are difficult to optimize and are inherently limited in the number of training examples by the maximum model input length. To address these drawbacks, the concept of "soft" prompts was introduced (Liu et al.,   3 https://github.com/adapter-hub/ adapter-transformers 2021; <ref type="bibr" target="#b33">Lester et al., 2021;</ref><ref type="bibr" target="#b36">Li and Liang, 2021)</ref>, where a part of the model's input embeddings is fine-tuned via gradient descent. This pivots the problem of finding prompts in a discrete space to a continuous optimization problem. Soft prompts can be trained for the input layer only <ref type="bibr" target="#b38">(Liu et al., 2021;</ref><ref type="bibr" target="#b33">Lester et al., 2021)</ref> or for all layers <ref type="bibr" target="#b36">(Li and Liang, 2021)</ref>. Recent advancements explore how soft prompts could be pre-trained or prompts for different tasks utilized to reduce the computation required for fine-tuning a soft prompt for a new task <ref type="bibr" target="#b59">(Vu et al., 2021;</ref><ref type="bibr" target="#b22">Hambardzumyan et al., 2021;</ref><ref type="bibr" target="#b54">Su et al., 2021;</ref><ref type="bibr" target="#b45">Qin et al., 2021)</ref>.</p><p>Other additive approaches Additive methods are a diverse category of parameter-efficient finetuning techniques that extends beyond adapters and soft prompts. For example, LeTS <ref type="bibr" target="#b18">(Fu et al., 2021)</ref>, LST <ref type="bibr" target="#b55">(Sung et al., 2022)</ref>, and (IA) 3 <ref type="bibr" target="#b37">(Liu et al., 2022)</ref> introduce novel ways to add parameters that improve adapters or soft prompts in terms of memory, computation, or accuracy.</p><p>Why add parameters? Although these methods introduce additional parameters to the network, they achieve significant training time and memory efficiency improvements by reducing the size of the gradients and the optimizer states. Note that in the case of Adam (Kingma and <ref type="bibr" target="#b31">Ba, 2015)</ref>, for every byte of trainable parameter, one extra byte is needed for its gradient, and two more bytes are needed to store the optimizer state: the first and second moments of the gradient. In practice, depending on the setup, training a model requires 12-20 times more GPU memory than the model weights. By saving memory on optimizer states, gradients, and allowing frozen model parameters to be quantized <ref type="bibr">(Dettmers et al., 2022)</ref>, additive PEFT methods enable the fine-tuning of much larger networks or the use of larger microbatch sizes. Which improves training throughput on GPUs. Moreover, optimizing fewer parameters in distributed setups drastically reduces communication volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Selective methods</head><p>Arguably the earliest example of selective PEFT is fine-tuning only a few top layers of a network <ref type="bibr" target="#b15">(Donahue et al., 2014)</ref>. Modern approaches are usually based on the type of the layer <ref type="bibr" target="#b19">(Gheini et al., 2021)</ref> or the internal structure, such as tuning only model biases <ref type="bibr" target="#b4">(Ben-Zaken et al., 2021)</ref> or only particular rows <ref type="bibr" target="#b60">(Vucetic et al., 2022)</ref>.</p><p>An extreme version of selective methods is sparse update methods which can completely ignore the structure of the model, and select parameters individually <ref type="bibr" target="#b56">(Sung et al., 2021;</ref><ref type="bibr" target="#b1">Ansell et al., 2022;</ref><ref type="bibr" target="#b20">Guo et al., 2020)</ref>. However, sparse parameter updates present multiple engineering and efficiency challenges, some of which have been tackled in recent research on parameter reconfiguration <ref type="bibr" target="#b60">(Vucetic et al., 2022</ref>) (Section 9.3) and NxM sparsity <ref type="bibr" target="#b26">(Holmes et al., 2021)</ref>. Nevertheless, unrestricted unstructured sparsity is still impractical on contemporary hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reparametrization-based methods</head><p>Reparametrization-based parameter-efficient finetuning methods leverage low-rank representations to minimize the number of trainable parameters. The notion that neural networks have lowdimensional representations has been widely explored in both empirical and theoretical analysis of deep learning <ref type="bibr" target="#b39">(Maddox et al., 2020;</ref><ref type="bibr" target="#b35">Li et al., 2018;</ref><ref type="bibr" target="#b2">Arora et al., 2018;</ref><ref type="bibr" target="#b40">Malladi et al., 2022)</ref>. <ref type="bibr" target="#b0">Aghajanyan et al. (2020)</ref> have demonstrated that fine-tuning can be performed effectively in low-rank subspaces. Further, they showed that the size of the subspace that needs adaption is smaller for bigger models or the models pre-trained for longer. Their approach, referred to as Intrinsic SAID (Section 10.1), employs the Fastfood trans-form <ref type="bibr" target="#b32">(Le et al., 2013)</ref> to reparametrize the update to neural network parameters.</p><p>However, perhaps the most well-known reparametrization-based method is Low-Rank Adaptation or LoRa <ref type="bibr" target="#b28">(Hu et al., 2021)</ref>, which employs a simple low-rank matrix decomposition to parametrize the weight update ?W = W down W up . This approach is straightforward to implement and has been evaluated on models with up to 175 billion parameters. We provide a detailed discussion of this method in Section 10.2. More recent works <ref type="bibr" target="#b29">(Karimi Mahabadi et al., 2021;</ref><ref type="bibr" target="#b16">Edalati et al., 2022)</ref> have also explored the use of Kronecker product reparametrization (?W = A ? B), which yields a more favorable tradeoff between rank and parameter count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hybrid methods</head><p>A number of methods have emerged that combine ideas from multiple categories of PEFT <ref type="bibr">(He et al., 2022a,b;</ref><ref type="bibr" target="#b41">Mao et al., 2021;</ref><ref type="bibr" target="#b29">Karimi Mahabadi et al., 2021)</ref>. For instance, MAM Adapter (Section 11.2) incorporates both Adapters and Prompt tuning. UniPELT (Section 11.3) adds LoRa to the mixture. Compacter and KronA B res reparametrize the adapters to reduce their parameter count (Sections 11.4 and 10.3). Finally, S4 (Section 11.5) is a result of an automated algorithm search that combines all PEFT classes to maximize accuracy at 0.5% of extra parameter count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparing PEFT methods</head><p>Parameter efficiency encompasses multiple aspects, including storage, memory, computation, and performance. However, achieving parameter efficiency alone does not necessarily lead to reduced RAM usage. For example, DiffPruning (Section 9.2) entails training a binary mask with the same number of parameters as the model. Consequently, this method can be only considered storage-efficient, while still incurring considerable RAM and time costs during fine-tuning.</p><p>To compare PEFT methods, we keep five dimensions of comparison in mind: storage efficiency, memory efficiency, computation efficiency, accuracy, and inference overhead. We observe that while they aren't completely independent from one another, improvements along one of the axes do not necessarily translate into improvements along others (Table <ref type="table" target="#tab_0">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Overview of PEFT Approaches</head><p>In the next sections, we dive into the details of several parameter-efficient fine-tuning approaches.</p><p>We will describe the distinctions and tradeoffs between them in terms of the axes outlined in Section 4. We bold a one-sentence summary of each method to simplify skimming through them.</p><p>In the method description, we also indicate whether it has been applied to models with fewer than 1 billion, fewer than 20 billion, or more than 20 billion parameters. The model size summary can be found in Table <ref type="table">2</ref>. We stick to the parameter counts indication where possible because the words "small" and "large" change their meaning too quickly. Finally, we provide a brief pseudo-PyTorch implementation of the most important part of the algorithm where it's feasible. 5 Depends on sparse operations hardware support. 6 Depends on the weight pruning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additive methods: Adapters</head><p>We start our dive into PEFT methods with one of the largest sub-families of methods that are built on the idea of adding fully-connected networks between model layers, i.e., adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Adapters</head><p>The idea of Adapters was initially developed for multi-domain image classification <ref type="bibr" target="#b47">(Rebuffi et al., 2017</ref><ref type="bibr" target="#b48">(Rebuffi et al., , 2018) )</ref> and consisted in adding domainspecific layers between neural network modules. <ref type="bibr" target="#b27">Houlsby et al. (2019)</ref> adapt this idea to NLP. They propose to add fully-connected networks after attention and FFN layers in Transformer. Unlike the transformer FFN block, Adapters usually have a smaller hidden dimension than the input. Adapters have demonstrated impressive parameter efficiency at the time, showing that it is possible to achieve performance competitive to full fine-tuning by tuning less than 4% of the total model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method % Trainable parameters % Changed parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluated on &lt;1B &lt;20B &gt;20B</head><p>Adapters <ref type="bibr" target="#b27">(Houlsby et al., 2019)</ref> 0.1 -6 0.1 -6 yes yes yes AdaMix <ref type="bibr" target="#b62">(Wang et al., 2022)</ref> 0.1 -0.2 0.1 -0.2 yes no no SparseAdapter <ref type="bibr">(He et al., 2022b)</ref> 2.0 2.0 yes no no BitFit <ref type="bibr" target="#b4">(Ben-Zaken et al., 2021)</ref> 0.05 -0.1 0.05 -0.1 yes yes yes DiffPruning <ref type="bibr" target="#b20">(Guo et al., 2020)</ref> 200 0.5 yes no no Fish-Mask <ref type="bibr" target="#b56">(Sung et al., 2021)</ref> 0.01 -0.5 0.01 -0.5 yes yes no Prompt Tuning <ref type="bibr" target="#b33">(Lester et al., 2021)</ref> 0.1 0.1 yes yes yes Prefix-Tuning <ref type="bibr" target="#b36">(Li and Liang, 2021)</ref> 0.1 -4.0 0.1 -4.0 yes yes yes IPT <ref type="bibr" target="#b45">(Qin et al., 2021)</ref> 56.0 56.0 yes</p><p>Table <ref type="table">2</ref>: What model sizes PEFTmethods have been evaluated on and their typical amount of trainable parameters used in the papers. By trainable parameter count we specifically mean the number parameters that are updated by a gradient optimization algorithm, not the delta between the original and final model which we denote "changed parameters". For reprametrization-based method we report parameters before and after reparametrization. Estimating updated parameter count for S4 is complicated as the model uses different methods at different layers. We report the range at which the methods were evaluated in the published literature.</p><p>A schematic adapter implementation:</p><formula xml:id="formula_1">def transformer_block_with_adapter(x): residual = x x = SelfAttention(x) x = FFN(x) # adapter x = LN(x + residual) residual = x x = FFN(x) # transformer FFN x = FFN(x) # adapter x = LN(x + residual) return x</formula><p>Pfeiffer et al. (2020a) found that inserting the adapter only after the self-attention layer (after normalization) achieves similar performance as using two adapters per transformer block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">AdaMix</head><p>AdaMix <ref type="bibr" target="#b62">(Wang et al., 2022)</ref> improves the performance of adapters by utilizing multiple adapters in a mixture-of-experts (MoE) fashion <ref type="bibr" target="#b52">(Shazeer et al., 2017)</ref>. Meaning, that each adapter layer is a set of layers (experts), and for each forward pass only a small set of experts is activated. In contrast to a regular MoE, which selects and weights multiple experts using a routing network AdaMix randomly selects a single expert for each forward pass. This minimizes computational costs and, according to <ref type="bibr" target="#b62">Wang et al. (2022)</ref>, doesn't degrade performance. Another difference from a regular MoE layer is that up and down projections of the adapter are selected independently. After training, the adapter weights are averaged across the experts which makes inference more memoryefficient. To stabilize training, the authors propose consistency regularization which minimizes symmetrized KL between two models' forwards with different sets of experts selected.</p><p>Although AdaMix achieves better performance than regular adapters with the same inference cost, it can use more memory during training. <ref type="bibr" target="#b62">Wang et al. (2022)</ref> show that AdaMix can use much smaller adapter hidden states, which amortizes trainable parameter overhead over the number of experts (?4-8). However, the consistency regularization technique increases computational requirements and memory consumption, as it needs to keep two versions of the hidden states and gradients over two model forward passes with different experts. Prompting language models has demonstrated remarkable performance in zero-and few-shot scenarios <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr">Schick and Sch?tze, 2021)</ref>. However, optimizing discrete natural language prompts or using in-context learning is impractical when there are many training examples.</p><p>To overcome this challenge, the concept of "soft" or "continuous" prompts was proposed <ref type="bibr" target="#b36">(Li and Liang, 2021;</ref><ref type="bibr" target="#b33">Lester et al., 2021;</ref><ref type="bibr" target="#b38">Liu et al., 2021)</ref> that converts the discrete optimization problem of finding the best "hard" prompt to a continuous one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Prompt Tuning</head><p>Prompt tuning <ref type="bibr" target="#b33">(Lester et al., 2021)</ref> proposes to prepend the model input embeddings with a trainable tensor P ? R l?h . This tensor is commonly referred to as "soft prompt" and it is optimized directly through gradient descent.</p><p>def soft_prompted_model(input_ids):</p><formula xml:id="formula_2">x = Embed(input_ids) x = concat([soft_prompt, x], dim=seq) return model(x)</formula><p>Ablation studies by <ref type="bibr" target="#b54">Su et al. (2021)</ref> over prompt length from 1 to 150 tokens and model size from 10M to 11B parameters reveal that prompt tuning is more parameter efficient the larger the model. For instance, prompt tuning of T5-11B achieves the same SuperGLUE <ref type="bibr" target="#b61">(Wang et al., 2019)</ref> performance with either 5 or 150 soft prompt length.</p><p>Furthermore, efficiency grows faster than the model size: T5-large performance saturates at prompt length 20 or 20K trainable parameters (0.002%), and T5-XL performance saturates prompt length 5, also 20K trainable parameters (0.0002%). However, prompt tuning only becomes comparable with full fine-tuning at the 10B model scale. Also, increasing the length of the input by 20-100 tokens can significantly increase computation, given the quadratic complexity of the transformer. Overall, soft prompts are incredibly parameter-efficient at the cost of inference overhead and more applicable to larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Prefix-Tuning</head><p>Li and Liang (2021) independently develop the idea of soft prompts with a distinctive flavor: instead of adding a soft prompt to the model input, trainable parameters are prepended to the hidden states of all layers. The same prefix P ? ? R l?h is prepended to all of the hidden states.</p><p>They observe that directly optimizing the soft prompt leads to instabilities during training. Instead, soft prompts are parametrized through a feed-forward network P ? = FFN( P? ). During training, P? and the parameters of the FFN are optimized. After, only P ? is needed for inference, and the FFN can be discarded.</p><p>Pseudocode for a single layer:</p><formula xml:id="formula_3">def transformer_block_for_prefix_tuning(x): soft_prompt = FFN(soft_prompt) x = concat([soft_prompt, x], dim=seq) return transformer_block(x)</formula><p>Note that the approach is very similar to Prompt-tuning (Section 7.1), but the soft prompts are added in each layer.</p><p>In their experiments, Li and Liang (2021) apply BART <ref type="bibr" target="#b34">(Lewis et al., 2019)</ref> model (&lt;1B) to different generation tasks and show a performance close to the full fine-tuning by training only 0.1% parameters. Soft prompt lengths used in the study vary from 10 to 200 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Intrinsic Prompt Tuning (IPT)</head><p>Prompt tuning methods, despite being parameter efficient, present practical problems such as slow convergence and a need to store all of the taskspecific parameters. A few studies <ref type="bibr" target="#b54">(Su et al., 2021;</ref><ref type="bibr" target="#b59">Vu et al., 2021)</ref> proposed to pre-train soft prompts to improve performance and convergence speed. However, these methods do not provide solutions to reduce the number of parameters per task. <ref type="bibr" target="#b45">Qin et al. (2021)</ref> hypothesize that the hdimensional space used to define soft prompt parameters contains an "intrinsic task subspace" that can differentiate between various tasks.</p><p>IPT method works in three steps. First, given a set of training tasks, their soft prompts are learned in a regular way (Section 7.1). Then, these prompts are used to train an autoencoder that compresses their dimensionality. After this, the encoder part is discarded, and only the input to the autoencoder decoder is trained on new tasks. In summary, IPT uses an autoencoder to (de)compress the soft prompt. Even though the IPT framework reduces the number of parameters for the unseen tasks, this reduction comes at the price of training the autoencoder. The authors conduct experiments with the BART-base model and a prompt length of 100. The resulting autoencoder, which is implemented<ref type="foot" target="#foot_2">4</ref> as a fully-connected network that accepts a one-dimensional tensor of size 768 ? 100 reaches approximately 78 million parameters. I.e., over 56% of total parameters in the BART-base model. Hence, significantly more efficient ways of prompt autoencoding are required to make IPT practically applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Additive Methods: Other Approaches</head><p>Several of the additive PEFT methods do not follow the idea of either adapters or soft prompts and propose to augment a pre-trained network in an original way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Ladder-Side Tuning (LST)</head><p>Ladder-Side Tuning <ref type="bibr" target="#b55">(Sung et al., 2022)</ref> trains a small transformer network on the side of the pre-trained network. This side network combines the hidden states of the pre-trained backbone network with its own hidden states.</p><p>This way, the side network only uses the pretrained model as a feature extractor, and backpropagation must only be computed through the side network saving on both memory and compute during training. The authors also use multiple tricks no improve the performance and parameter efficiency of LST. Namely, initializing the side network from the pre-trained model parameters using structural pruning and using twice (or 4x) fewer layers in the side network than in the backbone network.</p><p>The pseudocode: Where h_pt is the output of the corresponding layer of the pre-trained network, and alpha is an input-independent trainable scalar gate.</p><p>LST demonstrated a three-fold RAM reduction in fine-tuning T5-Base compared to full finetuning and a two-fold RAM usage reduction compared to LoRa (Section 10.2) with a small degradation in accuracy and outperforms these methods when controlling for RAM usage. <ref type="bibr">et al. (2022)</ref> propose a new parameter-efficient method to multi-task fine-tune T-few. (IA) 3 learns new parameters l v , l k , l f f which rescale key, value, and hidden FFN activations. Specifically,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">(IA) 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Liu</head><formula xml:id="formula_4">def transformer_block_with_ia3(x): residual = x x = ia3_self_attention(x) x = LN(x + residual) residual = x x = x @ W_1 # FFN in x = l_ff * gelu(x) # (IA)3 scaling x = x @ W_2 # FFN out x = LN(x + residual) return x def ia3_self_attention(x): k, q, v = x @ W_k, x @ W_q, x @ W_v k = l_k * k v = l_v * v return softmax(q @ k.T) @ V</formula><p>Training only these three vectors l v , l k , l f f for each transformer block leads to high parameter efficiency. For T0-3B, it only updates about 0.02% of model parameters and outperforms other methods, including Compacter (Section 11.4) with similar parameter count and LoRa (Section 10.2) with 16 times more trainable parameters. Unlike adapters-tuned models, (IA) 3 -tuned models exhibit minimal overhead. Vectors l v and l k can be integrated into the corresponding linear layers, and the only overhead comes from l f f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Selective Methods</head><p>Selective methods fine-tune a subset of the existing parameters of the model. It could be a layer depth-based selection, layer type-based lection, or even individual parameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">BitFit</head><p>Ben-Zaken et al. ( <ref type="formula">2021</ref>) propose to only fine-tune the biases of the network. That is, for every linear or convolutional layer, the weight matrix W is left as is, and only the bias vector b is optimized. BitFit only updates about 0.05% of the model parameters. The original paper demonstrated that the method achieves similar performance to full fine-tuning or better performance in low and medium-data scenarios in BERT models (&lt;1B parameters). Further research evaluated the method on larger networks such as T0-3B <ref type="bibr" target="#b50">(Sanh et al., 2022;</ref><ref type="bibr" target="#b37">Liu et al., 2022)</ref> or GPT-3 <ref type="bibr" target="#b28">(Hu et al., 2021)</ref>. At this scale, BitFit significantly underperforms full fine-tuning, and other PEFT approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">DiffPruning</head><p>DiffPruning <ref type="bibr" target="#b20">(Guo et al., 2020)</ref> aims to achieve parameter efficiency by learning a sparse update of a neural network's weights. The method introduces a learnable binary mask on the weights, denoted by ? = z ? ?W , where ? represents the Hadamard product. This parameter mask is learned during model fine-tuning as part of the regularization objective, which is a differentiable approximation to the L 0 norm of the update vector ?.</p><p>DiffPruning has achieved comparable performance to full fine-tuning while modifying only 0.5% of the model parameters in smaller-scale (&lt;1B) scenarios. This makes it a useful method for multi-task deployment for edge (mobile) scenarios where storage is limited. However, this method requires more memory than traditional fine-tuning, as it involves optimizing all parameters during training in addition to the learnable binary mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Freeze and Reconfigure (FAR)</head><p>FAR <ref type="bibr" target="#b60">(Vucetic et al., 2022)</ref> selects columns of parameter matrices to prune and reconfigures linear layers into trainable and frozen. The method operates in two stages. In the first stage, the most important rows of parameter matrices are identified for updating. This process is similar to structured pruning and can use any pruning method. In their paper, the authors fine-tune the model on a percentage of the data and select the top-r rows based on the L 1 distance between the fine-tuned and original models.</p><p>In the second stage, the network is reconfigured by splitting each parameter</p><formula xml:id="formula_5">W ? R in?h into a trainable component W t ? R in?h and a frozen component W f ? R in?(h-h )</formula><p>, where h is the desired number of trainable parameters. The matrix multiplications with W t and W f are computed independently, and the results are concatenated. A similar operation is performed on biases.</p><p>Pseudocode implementation is rather simple</p><formula xml:id="formula_6">def far_layer(x): h1 = x @ W_t h2 = x @ W_f return concat([h1, h2], dim=-1)</formula><p>While this approach creates additional compute overhead during training, it provides great flexibility in terms of parameter selection on modern hardware using standard frameworks like Py-Torch. After training, the parameters can be reconfigured back removing any inference overhead.</p><p>The original paper focused on edge scenarios and used DistilBERT (66M) for their experiments. FAR was only applied to feed-forward layers, as these make up the majority of the DistilBERT parameters. The authors showed that FAR achieved similar performance to fine-tuning on five GLUE tasks and SQuAD 2.0 <ref type="bibr">(Rajpurkar et al., 2018)</ref> while updating 6% of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">FishMask</head><p>FishMask <ref type="bibr" target="#b56">(Sung et al., 2021</ref>) is a sparse finetuning method that selects top-p parameters of the model based on their Fisher information. Fisher information is estimated in a common way through a diagonal approximation</p><formula xml:id="formula_7">F? = 1 N N i=1 E y?p ? (y|x i ) (? ? log p ? (y|x i )) 2</formula><p>Thus, the method requires computing gradients for all parameters on (several batches of) the data.</p><p>However, after the highest-Fisher parameters are selected, only they need to be optimized.</p><p>Pseudocode to compute the masks: The method generally performs on-par with adapters but sub-par to LoRa and (IA) 3 (Sections 10.2 and 8.2). It has been evaluated on BERT (&lt;1B) and T0-3B models. However, FishMask is computationally intensive and inefficient on contemporary deep learning hardware due to the lack of support for sparse operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Reparametrization-based methods</head><p>These methods use the idea of reparametrizing the weights of the network using a low-rank transformation. This decreases the trainable parameter count while still allowing the method to work with high-dimensional matrices, such as the pre-trained parameters of the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Intrinsic</head><p>In their work, <ref type="bibr" target="#b0">Aghajanyan et al. (2020)</ref> investigate the intrinsic dimensionality of fine-tuning and demonstrate that this process can be performed in a low-rank subspace. Specifically, they use the Fastfood transform to reparametrize the update to the model weights. Fastfood is a compute-efficient dimensionality expansion transform F : R d ? R D that can be done in O(D log d) time and O(D) memory.</p><p>Their results indicate that larger models require changes in a lower-rank subspace compared to smaller models to achieve the same fine-tuning performance. This observation motivates both scaling large models and parameter-efficient finetuning. It is important to note that, unlike methods that select a particular subset of parameters for fine-tuning, Intrinsic SAID updates all model parameters in a low-rank manner, i.e., ? = ? 0 + F (? d ), where ? 0 ? R D denotes the pre-trained model parameters and ? d ? R d denotes the parameters to be optimized. Therefore, while the number of optimizable parameters is low, the O(D) memory complexity of Fastfood and the update to all of the model's parameters make Intrinsic SAID impractical for fine-tuning large networks. For more details on Fastfood, we refer the reader to the original paper by <ref type="bibr" target="#b32">Le et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">LoRa</head><p>LoRa <ref type="bibr" target="#b28">(Hu et al., 2021)</ref> takes inspiration from In-trinsicSAID and proposes a simpler way to perform low-rank fine-tuning. Parameter update for a weight matrix in LoRa is decomposed into a product of two low-rank matricies</p><formula xml:id="formula_8">?W = W A W B W A ? R in?r , W B ? R r?out (1)</formula><p>All pre-trained model parameters are kept frozen, and only W A and W B matrices are trainable. The scaling factor is constant and typically equals 1 r . After training, they can be integrated into the original W by just adding the matrix W A W B to the original matrix W .</p><p>Pseudocode is very simple:</p><p>def lora_linear(x): h = x @ W # regular linear h += x @ W_A @ W_B # low-rank update return scale * h</p><p>In Transformers, LoRa is typically used for W K and W V projection matrices in multi-head attention modules. The method outperforms BitFit and Adapters and has been evaluated on the models up to 175B parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">KronA</head><p>KronA <ref type="bibr" target="#b16">(Edalati et al., 2022)</ref> replaces matrix factorization ?W = W A W B in LoRa (Section 10.2) with a matrix factorization through a Kronecker product ?W = W A ? W B . This yields a better rank per parameters tradeoff because the Kronecker product keeps the rank of the original matrices being multiplied. Or, in other words, <ref type="bibr" target="#b16">Edalati et al. (2022)</ref> propose to use efficient Kronecker product-vector product operation x(A?B) which avoids representing ?W explicitly and leads to significant speedups Krona pseudocode:</p><formula xml:id="formula_9">rank(A ? B) = rank A ? rank B. Additionally,</formula><p>def krona_linear(x): x = x @ W # regular linear x += kronecker_vector_prod(x, W_A, W_B) return scale * x # same as x @ kronecker_product(A, B) def kronecker_vector_prod(x, A, B):</p><formula xml:id="formula_10">x = x.reshape(A.shape[1], B.shape[1]) x = A.T @ x @ B return x.reshape(-1)</formula><p>KronA B res is another method presented in <ref type="bibr" target="#b16">Edalati et al. (2022)</ref>. It is a parallel adapter with Kronecker product-parametrization of the weights and a residual connection.</p><p>On GLUE, KronA methods perform on-par or better than adapters (Section 6.1), LoRa (Section 10.2), and Compacter (Section 11.4) at the same trainable parameter count 0.07% while being significantly faster than adapters or Compacter at inference time. The method was evaluated only on small (&lt;1B) models.</p><p>Background: What is a Kronecker product? Kronecker product is a tensor operation defined as</p><formula xml:id="formula_11">A ? B : R n?m ? R k?l ? R nk?ml A ? B = ? ? ? a 1,1 B ? ? ? a 1,n B . . . . . . . . . a m,1 B ? ? ? a m,n B ? ? ?<label>(2)</label></formula><p>It can be easily implemented 5 in PyTorch using the command torch.einsum def batched_kronecker_product(a, b): bs, i, j = a.shape bs, k, m = b.shape res = einsum("bij,bkm-&gt;bikjm", a, b) return res.view(bs, i * k, j * m)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Hybrid Approaches</head><p>Hybrid methods for parameter-efficient finetuning combine different techniques and strategies to achieve better performance while reducing the computational costs associated with finetuning large neural networks. These methods can be viewed as a synergistic blend of multiple approaches. The resulting hybrid methods can leverage the strengths of each individual technique, while mitigating their weaknesses, leading to improved performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">SparseAdapter</head><p>He <ref type="bibr">et al. (2022b)</ref> propose Large-Sparse strategy to train adapter layers. In this strategy, they use a large hidden dimension for the added module and prune around 40% of the values at initialization. Large-Sparse consistently outperforms its non-sparse counterpart with the same trainable parameter count. However, training and inference costs can be higher depending on hardware support for sparse tensors and operations. It is also worth noting that computing the pruning mask for 5 Source: github.com/rabeehk/compacter this method may require obtaining gradients for all newly added parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">MAM Adapters</head><p>In their study, <ref type="bibr">He et al. (2022a)</ref> conducted a thorough investigation of adapter placement and soft prompts. They concluded that scaled parallel adapters outperform sequentially-placed adapters and that placing an adapter in parallel to FFN outperforms multi-head attention-parallel adapters. <ref type="foot" target="#foot_3">6</ref> They also notice that soft prompts can efficiently modify attentions by only changing 0.1% of the parameters and propose to "mix-and-match" (MAM) these ideas. Their final model, MAM Adapter is a combination of scaled parallel adapter for FFN layer and soft prompt.</p><formula xml:id="formula_12">def transformer_block_mam(x): x = concat([x, soft_prompt], dim=seq) residual = x x = SelfAttention(x) x = LN(x + residual) x_a = FFN(x) # parallel adapter x_a = scale * x_a x = LN(x + x_adapter) return x</formula><p>MAM method outperforms BitFit and PromptTuning by a large margin and consistently outperforms LoRa (Section 10.2), Adapters (Section 6.1), and Prefix Tuning (Section 7.2) with 200 soft prompt length at 7% extra parameters. The experiments were concluded on &lt;1B models. It's worth noting that parallel adapters were independently studied by <ref type="bibr" target="#b67">Zhu et al. (2021)</ref> in the domain of machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">UniPELT</head><p>UniPELT <ref type="bibr" target="#b41">(Mao et al., 2021</ref>) is a gated combination of LoRa, Prefix-tuning, and Adapters. More specifically, LoRa reparametrization is used for W Q and W V attention matrices, prefix-tuning is applied to keys and values of each layer, and adapters are added after the feed-forward layer of the transformer block. For each of the modules, gating is implemented as a linear layer projecting the module input into a dimension of size one, sigmoid activation, and averaging the resulting vector over the sequence length. Trainable parameters include LoRa matrices W A , W B , prompt tuning parameters P q , P k , adapter parameters, and gating function weights.</p><p>Next, we present a schematic implementation of UniPELT. We omit multiple attention heads for simplicity.</p><p>def transformer_block_with_unipelt(x): residual = x x = unipelt_self_attention(x) x = LN(x + residual) residual = x x = FFN(x) adapter_gate = gate(x) x = adapter_gate * FFN(x) x = LN(x + residual) return x def unipelt_self_attention(x): k, q, v = x @ W_k, x @ W_q, x @ W_v # lora for queries and values lora_gate = gate(x) q += lora_gate * W_qA @ W_aB v += lora_gate * W_vA @ W_vB # prefix tuning pt_gate = gate(x) q_prefix = pt_gate * P_q k_prefix = pt_gate * P_k return softmax(q @ k.T) @ V def gate(x): x = Linear(x) x = sigmoid(x) return mean(x, dim=seq)</p><p>UniPELT demonstrates significant improvements over individual LoRa, Adapters, and Prefix Tuning approaches in low-data scenarios with only 100 examples. In higher data scenarios, UniPELT performs on par or better than these approaches. <ref type="bibr" target="#b41">Mao et al. (2021)</ref> reports 1.3% trainable model parameters using UniPELT on BERT (&lt;1B parameters) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Compacter</head><p>Compacter <ref type="bibr" target="#b29">(Karimi Mahabadi et al., 2021)</ref> utilizes Kronecker product, low-rank matrices, and parameter sharing across layers to produce adapter weights. Each parameter W in an adapter is equal to a sum of Kronecker products</p><formula xml:id="formula_13">? = n i=0 A i ? B i ? ? R k?d , A i ? R n?n , B i ? R k n ? d n .</formula><p>(3) A linear layer x ? + b with this parametrization is called parametrized hypercomplex multiplication (PHM) layer <ref type="bibr" target="#b65">(Zhang et al., 2021)</ref>. Compacter takes this idea futher, parametrizing B i similar to LoRa (Section 10. Note that all A and B are 3D tensors with the first dimension equal to n, the number of Kronecker products in the PHM layer.</p><p>Compacter comes in two flavors: two adapters per transformer block or a single adapter after a feedforward layer (Compacter++). With only 0.05% additional parameters Compacter++ performs on par or better than adapters with 0.8% additional parameters. The model has been evaluated on T5 Base (&lt;1B) and T0-3B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.5">S4</head><p>Chen et al. ( <ref type="formula">2023</ref>) conduct an extensive exploration of various combinations of parameterefficient fine-tuning techniques. Their search space includes dividing consecutive layers into four uneven groups, allocating varying amounts of trainable parameters to each layer, determining which groups to fine-tune, and deciding which PEFT methods to apply to each group.</p><p>Their proposed method, S4, divides layers into four groups (G 1,2,3,4 ) using a "spindle" pattern: more layers are allocated to the middle groups and fewer to the top and bottom groups. All groups are trainable, with trainable parameters uniformly allocated across the layers (not groups). Different combinations of PEFT methods are applied to different groups. Specifically,</p><formula xml:id="formula_14">G 1 : A, L G 3 : A, P, B G 2 : A, P G 4 : P, B, L<label>(4)</label></formula><p>where A stands for Adapters (Section 6.1), P for Prefix-Tuning (Section 7.2), B for BitFit (Section 9.1), and L for LoRa (Section 10.2).</p><p>The search experiments were conducted on T5base and the GLUE dataset at 0.5% trainable parameters. The S4 method was then applied to T5-3B, RoBERTa, and XL-Net, consistently outperforming individual BitFit, Prefix Tuning, LoRa, and Adapters across different architectures, model sizes, and tasks.</p><p>12 Reporting and comparison issues Survey papers tend to discuss reporting issues, and unfortunately, this one is not an exception. We identified several challenges and inconsistencies that warrant discussion. These challenges make it difficult to draw direct comparisons between methods and evaluate their true performance.</p><p>Inconsistent Parameter Counts One of the primary challenges stems from the difference in the way researchers report parameter counts. These inconsistencies are not a result of dishonesty but arise from the inherent complexity of the problem. Generally, parameter counts can be categorized into three types: the number of trainable parameters, the number of changed parameters between the original and fine-tuned models, and the rank of the difference between the original and fine-tuned models.</p><p>These distinctions can have significant implications. For example, IntrinsicSAID (Section 10.1) learns a low-rank (?100-1000) transformation of model parameters. However, it changes all of the model's parameters. DiffPruning (Section 9.2) learns an update of 0.5% parameters, but it actually trains 200% parameters: fine-tuning the model and learning the binary mask. For reparameterization-based methods (Sections 10.2, 10.3, 11.4), memory requirements may vary depending on the implementation design choices.</p><p>Of the three types, the number of trainable parameters is the most reliable predictor of memory efficiency. However, it is still imperfect. For instance, Ladder-side Tuning (Section 8.1) uses a separate side-network with more parameters than LoRa or BitFit, but it requires less RAM, since backpropagation is not computed through the main network.</p><p>Model Size Another challenge arises from the variation in model sizes used in the evaluation of PEFT methods. Several studies <ref type="bibr" target="#b0">(Aghajanyan et al., 2020;</ref><ref type="bibr" target="#b28">Hu et al., 2021)</ref> have demonstrated that larger models require fewer parameters to be updated during fine-tuning, both in terms of percentage and when the model is large enough, sometimes even in absolute terms <ref type="bibr" target="#b36">(Li and Liang, 2021)</ref>. Thus, model size must be considered when comparing PEFT methods, not just the ratio of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lack of Standard Benchmarks and Metrics</head><p>The absence of standard benchmarks and metrics further complicates comparisons. New methods are often evaluated on different model/dataset combinations, making it challenging to draw meaningful conclusions.</p><p>We would like to highlight the papers that report a variety of metrics on standard datasets simplifying comparison to other methods. For example, KronA <ref type="bibr" target="#b16">(Edalati et al., 2022</ref>) evaluated T5base on the GLUE benchmark and reported accuracy, training time, and inference time while maintaining the same number of trainable parameters. UniPELT <ref type="bibr" target="#b41">(Mao et al., 2021)</ref> assessed BERT on the GLUE benchmark and reported accuracy, training time, and inference latency, although it used different parameter counts for various methods. LST <ref type="bibr" target="#b55">(Sung et al., 2022</ref>) evaluated different T5 sizes on the GLUE benchmark, reporting metrics such as accuracy, training time, the number of updated parameters, and memory usage. MAM <ref type="bibr">(He et al., 2022a)</ref> applied multiple models to the XSUM benchmark and reported accuracy across a range of trainable parameters, although memory comparisons were not provided.</p><p>However, even these papers lack full comparability due to differences in their evaluation settings, such as varying parameter counts or the absence of certain metrics like memory comparisons. These inconsistencies highlight the need for a standardized benchmark and unified metrics to facilitate more accurate comparisons and evaluations of PEFT methods.</p><p>Issues with Published Implementations Another issue encountered is the state of published implementations. Many codebases are simply copies of the Transformers library <ref type="bibr" target="#b63">(Wolf et al., 2020)</ref> or other repositories with only minor modifications. These copies often do not use git forks, making it difficult to identify the differences unless they are highlighted in the README file.</p><p>Furthermore, even when differences are easy to find, the code is frequently not reusable. Users are often required to install a modified version of the Transformers library, which conflicts with the most recent version and lacks documentation or any examples of how to reuse the method outside of the existing codebase.</p><p>Despite these challenges, there are some methods with reusable implementations worth highlighting, such as LoRa<ref type="foot" target="#foot_4">7</ref> and Compacter<ref type="foot" target="#foot_5">8</ref> . These implementations stand out for their userfriendliness and adaptability, providing a solid foundation for further research and development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Best Practices</head><p>To address different issues identified in the parameter-efficient fine-tuning literature, we put forward the following best practices for future research:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit reporting of parameter count type:</head><p>We encourage authors to clearly specify the parameter count being reported in their papers or, ideally, report all three types of parameter count: trainable, changed, and rank. This will improve understanding and allow for more accurate comparisons between methods.</p><p>Evaluate with different model sizes: It is important to assess their methods using different model sizes, as this can provide a more comprehensive understanding of each method's strengths and limitations. This is particularly important considering that even recent papers often focus solely on BERT.</p><p>Comparisons to similar methods: In addition to comparing their methods with popular approaches (e.g., LoRa, BitFit, Adapters) we should also analyze their methods alongside other techniques that share conceptual and architectural resemblances. In our review, we often came across methods that were based on very similar ideas and designs but were never directly compared. Undertaking such comparisons will offer a more comprehensive understanding of a method's performance and its relative strengths in relation to existing techniques.</p><p>Standardized PEFT benchmarks and competitions: We propose the development of standardized PEFT benchmarks and competitions, which would require participants to compete under the same conditions and facilitate direct comparisons of results. These benchmarks should provide standardized data and models at different scales to evaluate training efficiency.</p><p>To assess training time and memory efficiency, competitions can offer a centralized server or specify a comprehensive server configuration that out-lines the CPU type, amount of memory, and GPU type and quantity. Ideally, this could take the form of an instance template to one of the major cloud providers. GPU memory consumption should be evaluated in a standardized way.</p><p>Emphasize code clarity and minimal implementations: As a community, we need to prioritize code that is easy to understand and features simple, reusable implementations. In some cases, such implementations provide additional insight to the paper and could be written in a concise manner. This proposal is in the interest of individual researchers as well, as easy-to-reuse methods may become more popular and, consequently, more cited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">Discussion</head><p>The growing accessibility of large language models <ref type="bibr" target="#b66">(Zhang et al., 2022;</ref><ref type="bibr" target="#b64">Zeng et al., 2022;</ref><ref type="bibr" target="#b30">Khrushchev et al., 2022;</ref><ref type="bibr">Touvron et al., 2023)</ref> and the democratization of their inference through low-bit quantization <ref type="bibr">(Dettmers et al., 2022;</ref><ref type="bibr">Dettmers and Zettlemoyer, 2022)</ref> has enabled the research community to study, experiment, and tackle new tasks with relatively modest compute budgets. Parameter-efficient fine-tuning is the next step that will allow us not just to inference, but to modify these models.</p><p>Among the developed methods, some have demonstrated their practicality at scale (Table <ref type="table">2</ref>), such as Adapters (Section 6.1), Prompt Tuning (Section 7.1), LoRa (Section 10.2), and (IA) 3 (Section 8.2). However, in practice, matching the performance of full fine-tuning remains a challenge. One of the reasons is high sensitivity to hyperparameters, with optimal hyperparameters often significantly deviating from those used in full fine-tuning due to the varying number of trainable parameters. For instance, the optimal learning rate for parameter-efficient fine-tuning is generally much higher than that for full fine-tuning. The research community should promote in-depth investigations into the impact of hyperparameters on these methods and finding reasonable defaults, as parameter-efficient fine-tuning or large models can be noticeably costly at 20-100B scale. Additionally, efforts should be directed towards developing methods that minimize hyperparameter sensitivity, such as pre-training new parameters <ref type="bibr" target="#b59">(Vu et al., 2021;</ref><ref type="bibr" target="#b54">Su et al., 2021)</ref>.</p><p>Examining the taxonomy of methods and the progress made thus far, it is evident that low-rank reparameterization has been remarkably successful in enhancing parameter efficiency. LoRa-style (Section 10.2) and Kronecker-product (Sections 11.4 and 10.3) reparameterizations both decrease the number of trainable parameters while requiring minimal extra computation. A possible future direction of finding new PEFT models is exploring different reparametrization techniques with favorable trainable parameter count vs. rank ratio. Another possible direction of improvement is utilizing what we know about how transformer models process texts <ref type="bibr" target="#b49">(Rogers et al., 2020)</ref>. Most of the PEFT methods work uniformly for the model, while we know that models process input differently at different layers. Utilizing this knowledge or building systems that have an adaptive number of parameters per layer could further improve parameter efficiency and accuracy.</p><p>In many respects, our current situation resembles the challenges from edge machine learning: we consistently face constraints in memory, computation, and even energy consumption. Techniques like quantization and pruning <ref type="bibr" target="#b21">(Gupta et al., 2015;</ref><ref type="bibr">LeCun et al., 1989)</ref> widely used in edge machine learning, now benefit large language models. As we move forward, it is not only plausible but also likely that more ideas could be exchanged between these two areas. Cross-disciplinary collaboration could further exchange ideas, accelerating innovation and progress in parameter-efficient fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Basic Transformer block</figDesc><graphic url="image-1.png" coords="2,345.24,71.50,143.66,165.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Parameter-efficient fine-tuning methods taxonomy. We identify three main classes of methods: Addition-based, Selection-based, and Reparametrization-based. Within additive methods, we distinguish two large included groups: Adapter-like methods and Soft prompts.</figDesc><graphic url="image-6.png" coords="3,72.00,62.81,453.52,231.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>def transformer_block_with_adamix(x): residual = x x = SelfAttention(x) x = LN(x + residual) residual = x x = FFN(x) # adamix starts here x = random_choice(experts_up)(x) x = nonlinearity(x) x = random_choice(experts_down)(x) x = LN(x + residual) return x def consistency_regularization(x): logits1 = transformer_adamix(x) # second pass uses different experts logits2 = transformer_adamix(x) r = symmetrized_KL(logits1, logits2) return r 7 Additive Methods: Soft Prompts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.reshape(prompt_len, hidden) x = concat([P, x], dim=seq) return model(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>params = (p for n, p in model.named_parameters() if "bias" in n) optimizer = Optimizer(params)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2) B i = B down i B up i where all matricies are of rank at most r. Matrices A i are shared across all adapter layers for further parameter efficiency. The corresponding layer is named Low-rank PHM or LPHM. Compacter layer pseudocode:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="5">Type Storage Memory Backprop Inference overhead</cell></row><row><cell>Adapters (Houlsby et al., 2019)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN</cell></row><row><cell>AdaMix (Wang et al., 2022)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN</cell></row><row><cell>SparseAdapter (He et al., 2022b)</cell><cell>AS</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN</cell></row><row><cell>Cross-Attn tuning (Gheini et al., 2021)</cell><cell>S</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>BitFit (Ben-Zaken et al., 2021)</cell><cell>S</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>DiffPruning (Guo et al., 2020)</cell><cell>S</cell><cell>yes</cell><cell>no</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>Fish-Mask (Sung et al., 2021)</cell><cell>S</cell><cell>yes</cell><cell>maybe 5</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>LT-SFT (Ansell et al., 2022)</cell><cell>S</cell><cell>yes</cell><cell>maybe 5</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>Prompt Tuning (Lester et al., 2021)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra input</cell></row><row><cell>Prefix-Tuning (Li and Liang, 2021)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra input</cell></row><row><cell>Spot (Vu et al., 2021)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra input</cell></row><row><cell>IPT (Qin et al., 2021)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN and input</cell></row><row><cell>MAM Adapter (He et al., 2022a)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN and input</cell></row><row><cell>Parallel Adapter (He et al., 2022a)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN</cell></row><row><cell>Intrinsinc SAID (Aghajanyan et al., 2020)</cell><cell>R</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>LoRa (Hu et al., 2021)</cell><cell>R</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>UniPELT (Mao et al., 2021)</cell><cell>AR</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN and input</cell></row><row><cell>Compacter (Karimi Mahabadi et al., 2021)</cell><cell>AR</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN</cell></row><row><cell>PHM Adapter (Karimi Mahabadi et al., 2021)</cell><cell>AR</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN</cell></row><row><cell>KronA (Edalati et al., 2022)</cell><cell>R</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>KronA B res (Edalati et al., 2022) (IA) 3 (Liu et al., 2022)</cell><cell>AR A</cell><cell>yes yes</cell><cell>yes yes</cell><cell>no no</cell><cell>Extra linear layer Extra gating</cell></row><row><cell>Attention Fusion (Cao et al., 2022)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>Extra decoder</cell></row><row><cell>LeTS (Fu et al., 2021)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>Extra FFN</cell></row><row><cell>Ladder Side-Tuning (Sung et al., 2022)</cell><cell>A</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>Extra decoder</cell></row><row><cell>FAR (Vucetic et al., 2022)</cell><cell>S</cell><cell>yes</cell><cell>maybe 6</cell><cell>no</cell><cell>No overhead</cell></row><row><cell>S4-model (Chen et al., 2023)</cell><cell>ARS</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>Extra FFN and input</cell></row></table><note><p>Comparing PEFT methods across storage efficiency, memory efficiency, and computational efficiency in terms of reducing backpropagation costs and having inference overhead. Method types: A -additive, S -selective, R -reparametrization-based.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>github.com/google-research/bert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A Recipe for Training Neural Networks, A. Karpathy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>github.com/thunlp/Intrinsic-Prompt-Tuning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>While at first this might look contradicting the finding ofPfeiffer et al. (2020a), it actually supports it because the FFNparallel adapter modifies the outputs of attention, just like the MHA-sequential adapter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>github.com/microsoft/LoRA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>github.com/rabeehk/compacter</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="person">Daniel H Garrette</rs>, <rs type="person">Deepak R. Tunuguntla</rs>, <rs type="person">Ehud Reiter</rs>, <rs type="person">Ekaterina Taktasheva</rs>, <rs type="person">Ekaterina Voloshina</rs>, <rs type="person">Eli Bogdanov</rs>, <rs type="person">Genta Indra Winata</rs>, <rs type="person">Hailey Schoelkopf</rs>, <rs type="person">Jan-Christoph Kalo</rs>, <rs type="person">Jekaterina Novikova</rs>, <rs type="person">Jessica Zosa Forde</rs>, <rs type="person">Jordan Clive</rs>, <rs type="person">Jungo Kasai</rs>, <rs type="person">Ken Kawamura</rs>, <rs type="person">Liam Hazan</rs>, <rs type="person">Marine Carpuat</rs>, <rs type="person">Miruna Clinciu</rs>, <rs type="person">Najoung Kim</rs>, <rs type="person">Newton Cheng</rs>, <rs type="person">Oleg Serikov</rs>, <rs type="person">Omer Antverg</rs>, <rs type="person">Oskar van der Wal</rs>, <rs type="person">Rui Zhang</rs>, <rs type="person">Ruochen Zhang</rs>, <rs type="person">Sebastian Gehrmann</rs>, <rs type="person">S. Osher Pais</rs>, <rs type="person">Tatiana Shavrina</rs>, <rs type="person">Thomas Scialom</rs>, <rs type="person">Tian Yun</rs>, <rs type="person">Tomasz Limisiewicz</rs>, <rs type="person">Verena Rieser</rs>, <rs type="person">Vitaly Protasov</rs>, <rs type="person">Vladislav Mikhailov</rs>, <rs type="person">Yada Pruksachatkun</rs>, <rs type="person">Yonatan Belinkov</rs>, <rs type="person">Zachary Bamberger</rs>, <rs type="person">Zdenvek Kasner</rs>, <rs type="person">Alice Rueda</rs>, <rs type="person">Amanda Pestana</rs>, <rs type="person">Amir Feizpour</rs>, <rs type="person">Ammar Khan</rs>, <rs type="person">Amy Faranak</rs>, <rs type="person">Ananda Santa Rosa Santos</rs>, <rs type="person">Anthony Hevia</rs>, <rs type="person">Antigona Unldreaj</rs>, <rs type="person">Arash Aghagol</rs>, <rs type="person">Arezoo Abdollahi</rs>, <rs type="person">Aycha Tammour</rs>, <rs type="person">Azadeh HajiHosseini</rs>, <rs type="person">Bahareh Behroozi</rs>, <rs type="person">Benjamin Olusola Ajibade</rs>, <rs type="person">Bharat Kumar Saxena</rs>, <rs type="person">Carlos Mu?oz Ferrandis</rs>, <rs type="person">Danish Contractor</rs>, <rs type="person">David M. Lansky</rs>, <rs type="person">Davis David</rs>, <rs type="person">Douwe Kiela</rs>, <rs type="person">Duong Anh Nguyen</rs>, <rs type="person">Edward Tan</rs>, <rs type="person">Emily Baylor</rs>, <rs type="person">Ezinwanne Ozoani</rs>, <rs type="person">Fatim T Mirza</rs>, <rs type="person">Frankline Ononiwu</rs>, <rs type="person">Habib Rezanejad</rs>, <rs type="person">H.A. Jones</rs>, <rs type="person">Indrani Bhattacharya</rs>, <rs type="person">Irene Solaiman</rs>, <rs type="person">Irina Sedenko</rs>, <rs type="person">Isar Nejadgholi</rs>, <rs type="person">Jan Passmore</rs>, <rs type="person">Joshua Seltzer</rs>, <rs type="person">Julio Bonis Sanz</rs>, <rs type="person">Karen Fort</rs>, <rs type="person">L?via Macedo Dutra</rs>, <rs type="person">Mairon Samagaio</rs>, <rs type="person">Maraim Elbadri</rs>, <rs type="person">Margot Mieskes</rs>, <rs type="person">Marissa Gerchick</rs>, <rs type="person">Martha Akinlolu</rs>, <rs type="person">Michael McKenna</rs>, <rs type="person">Mike Qiu</rs>, <rs type="person">M. K. K. Ghauri</rs>, <rs type="person">Mykola Burynok</rs>, <rs type="person">Nafis Abrar</rs>, <rs type="person">Nazneen Rajani</rs>, <rs type="person">Nour Elkott</rs>, <rs type="person">Nourhan Fahmy</rs>, <rs type="person">Olanrewaju Modupe Samuel</rs>, <rs type="person">Ran An</rs>, <rs type="person">R. P. Kromann</rs>, <rs type="person">Ryan Hao</rs>, <rs type="person">Samira Alizadeh</rs>, <rs type="person">Sarmad Shubber</rs>, <rs type="person">Silas L. Wang</rs>, <rs type="person">Sourav Roy</rs>, <rs type="person">Sylvain Viguier</rs>, <rs type="person">Thanh-Cong Le</rs>, <rs type="person">Tobi Oyebade</rs>, <rs type="person">Trieu Nguyen Hai Le</rs>, <rs type="person">Yoyo Yang</rs>, <rs type="person">Zachary Kyle Nguyen</rs>, <rs type="person">Abhinav Ramesh Kashyap</rs>, <rs type="person">Alfredo Palasciano</rs>, <rs type="person">Alison Callahan</rs>, <rs type="person">Anima Shukla</rs>, <rs type="person">Antonio Miranda-Escalada</rs>, <rs type="person">Ayush Kumar Singh</rs>, <rs type="person">Benjamin Beilharz</rs>, <rs type="person">Bo Wang</rs>, <rs type="person">Caio Matheus Fonseca de Brito</rs>, <rs type="person">Chenxi Zhou</rs>, <rs type="person">Chirag Jain</rs>, <rs type="person">Chuxin Xu</rs>, <rs type="person">Cl?mentine Fourrier</rs>, <rs type="person">Daniel Le</rs>'on Perin'an, <rs type="person">Daniel Molano</rs>, <rs type="person">Dian Yu</rs>, <rs type="person">Enrique Manjavacas</rs>, <rs type="person">Fabio Barth</rs>, <rs type="person">Florian Fuhrimann</rs>, <rs type="person">Gabriel Altay</rs>, <rs type="person">Giyaseddin Bayrak</rs>, <rs type="person">Gully A. Burns</rs>, <rs type="person">Helena U. Vrabec</rs>, <rs type="person">Iman I.B. Bello</rs>, <rs type="person">Isha Dash</rs>, <rs type="person">Ji Soo</rs></p></div>
<div><head>A Acknowledgements</head><p>We would like to thank <rs type="person">Vladimir Kluenkov</rs> and <rs type="person">Victoria Maltseva</rs> for their help with Figure 2.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Composable sparse finetuning for cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ansell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1778" to="1796" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ArXiv, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben-Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention fusion: a light yet efficient late fusion mechanism for task adaptation in nlu</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Parameter-efficient fine-tuning design spaces</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv, abs/2301.01821</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Named tensor notation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.13196</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><surname>Reif</surname></persName>
		</author>
		<imprint>
			<pubPlace>Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi ; Oleksandr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.02311</idno>
		<editor>David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz</editor>
		<imprint>
			<pubPlace>Erica Moreira, Re; Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
	</analytic>
	<monogr>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The case for 4-bit precision: k-bit inference scaling laws</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.09720</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.06904</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Confer-ence on Machine Learning</title>
		<meeting>the 31st International Confer-ence on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Krona: Parameter efficient tuning with kronecker adapter</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Edalati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><forename type="middle">S</forename><surname>Tahaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.10650</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.03961</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learnto-share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3469" to="3479" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-attention is all you need: Adapting pretrained transformers for machine translation</title>
		<author>
			<persName><forename type="first">Mozhdeh</forename><surname>Gheini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021-05">May. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritish</forename><surname>Narayanan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Warp: Word-level adversarial reprogramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021-05">May. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2022a. Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2022b. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters</title>
		<author>
			<persName><forename type="first">Shwai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daize</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2184" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nxmtransformer: Semistructured sparsification for natural language understanding via admm</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1818" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1022" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khrushchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Zinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">YaLM</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fastfood: Approximate kernel expansions in loglinear time</title>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam?s</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
		<idno>ArXiv, abs/1408.3060</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1989">2013. 1989</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Optimal brain damage</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename></persName>
		</author>
		<idno>ArXiv, abs/2104.08691</idno>
		<imprint>
			<date type="published" when="2021">Constant. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Few-shot parameterefficient fine-tuning is better and cheaper than in-context learning</title>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.05638</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gpt understands, too</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.10385</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Wesley</forename><forename type="middle">J</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02139</idno>
		<title level="m">Rethinking parameter counting: Effective dimensionality revisted</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.05643</idno>
		<title level="m">A kernel-based view of language model finetuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unipelt: A unified framework for parameter-efficient language model tuning</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.07577</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Kyunghyun Cho, and Iryna Gurevych. 2020a. Adapterfusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Modular deep learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ponti</surname></persName>
		</author>
		<idno>ArXiv, abs/2302.11529</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adapterhub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exploring universal intrinsic task subspace via prompt tuning</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners. Pranav Rajpurkar, Robin Jia, and Percy Liang</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018">2019. 2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Know what you don&apos;t know: Unanswerable questions for squad</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A primer in BERTology: What we know about how BERT works</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00349</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Trishala</forename><surname>Neeraj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abheesht</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Santilli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</editor>
		<editor>
			<persName><surname>Scao</surname></persName>
		</editor>
		<meeting><address><addrLine>Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth-Jane</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ili'c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagn'e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franccois</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gall?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">Rose</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Laurenccon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Simhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Soroa Etxabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Alfassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><forename type="middle">Kreisberg</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Nitzav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">C</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Klamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Alexander Van Strien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">G</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efrat</forename><surname>Ponferrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Levkovizh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Bar Natan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?rard</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germ?n</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giada</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Pistilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Benyamina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idris</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Abdulmumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itziar</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorg</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><forename type="middle">L</forename><surname>Frohberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Tobing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimbo</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Tanguy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maraim</forename><surname>Romero Mu?oz</surname></persName>
		</author>
		<author>
			<persName><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Mar'ia Grandury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Vsavsko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">Chien</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ali Jauhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Ghaleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nora</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurulaqilla</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ona</forename><surname>Espejel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>De Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priscilla</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Amuok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rheza</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Harliman</surname></persName>
		</author>
		<author>
			<persName><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Roberto L'opez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salomey</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Osei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamsuddeen</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Hassan Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somaieh</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Nikpoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Silberberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sydney</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Zink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Timponi Torrent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilina</forename><surname>Danchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Nikoulina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Violette</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vrinda</forename><surname>Lepercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeerak</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Talat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglei</forename><surname>Heinzerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">Y</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maged</forename><forename type="middle">S</forename><surname>Al-Shaibani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><forename type="middle">V</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srulik</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hatim</forename><surname>Bourfoune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myriam</forename><surname>Peyrounette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Martin ; Yashasvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhee</forename><surname>Chao Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongli</forename><surname>Xao Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
		<title level="m">Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Canalli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rosaline</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruisi</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samuele</forename><surname>Garda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Shlok</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shubhanshu</forename><surname>Deshmukh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sid</forename><surname>Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Kiblawi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sinee</forename><surname>Ott</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Srishti</forename><surname>Sang-Aroonsiri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sushil</forename><surname>Schweter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Pratap Bharati</surname></persName>
		</editor>
		<editor>
			<persName><surname>Laud</surname></persName>
		</editor>
		<meeting><address><addrLine>Zhiqing Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts; Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall&apos;ee; Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner</addrLine></address></meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
		<respStmt>
			<orgName>Th&apos;eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,</orgName>
		</respStmt>
	</monogr>
	<note>Hyung Won Chung, Jaesung Tae, Jason Phang. and Thomas Wolf. 2022. Bloom: A 176b-parameter openaccess multilingual language model. ArXiv, abs/2211.05100v2. Timo Schick and Hinrich Sch?tze. 2021. It&apos;s not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixtureof-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename><surname>Krzysztof Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multibillion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On transferability of prompt tuning for natural language understanding</title>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan-Zi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv, abs/2111.06719</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Lst: Ladder side-tuning for parameter and memory efficient transfer learning</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno>ArXiv, abs/2206.06522</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Training neural networks with fixed sparse masks</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24193" to="24205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spot: Better frozen model adaptation through soft prompt transfer</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cer</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient fine-tuning of bert models on the edge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Vucetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Tayaranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Ziaeefard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1838" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Adamix: Mixture-ofadapter for parameter-efficient tuning of large language models</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.12410</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<title level="m">Transformers: State-of-the-Art Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Glm-130b: An open bilingual pretrained model</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Opt: Open pre-trained transformer language models</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Counterinterference adapter for multilingual machine translation</title>
		<author>
			<persName><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.240</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2812" to="2823" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
