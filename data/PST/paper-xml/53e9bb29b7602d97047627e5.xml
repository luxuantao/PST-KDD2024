<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward an efficient and scalable feature selection approach for internet traffic classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adil</forename><surname>Fahad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zahir</forename><surname>Tari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ibrahim</forename><surname>Khalil</surname></persName>
							<email>ibrahim.khalil@rmit.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ibrahim</forename><surname>Habib</surname></persName>
							<email>habib@ccny.cuny.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">City University of New York</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hussein</forename><surname>Alnuweiri</surname></persName>
							<email>hussein.alnuweiri@qatar.tamu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Electrical &amp; Computer Engineering Program</orgName>
								<orgName type="institution">Texas A&amp; M University at Qatar</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Information Technology</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<postCode>3001</postCode>
									<settlement>Melbourne, Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward an efficient and scalable feature selection approach for internet traffic classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BF1C4BD541CC704452CC280A3418A8D1</idno>
					<idno type="DOI">10.1016/j.comnet.2013.04.005</idno>
					<note type="submission">Received 20 April 2012 Received in revised form 4 March 2013 Accepted 7 April 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Feature selection Metrics Traffic classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is significant interest in the network management and industrial security community about the need to identify the ''best'' and most relevant features for network traffic in order to properly characterize user behaviour and predict future traffic. The ability to eliminate redundant features is an important Machine Learning (ML) task because it helps to identify the best features in order to improve the classification accuracy as well as to reduce the computational complexity related to the construction of the classifier. In practice, feature selection (FS) techniques can be used as a preprocessing step to eliminate irrelevant features and as a knowledge discovery tool to reveal the ''best'' features in many soft computing applications. In this paper, we investigate the advantages and disadvantages of such FS techniques with new proposed metrics (namely goodness, stability and similarity). We continue our efforts toward developing an integrated FS technique that is built on the key strengths of existing FS techniques. A novel way is proposed to identify efficiently and accurately the ''best'' features by first combining the results of some well-known FS techniques to find consistent features, and then use the proposed concept of support to select a smallest set of features and cover data optimality. The empirical study over ten high-dimensional network traffic data sets demonstrates significant gain in accuracy and improved run-time performance of a classifier compared to individual results produced by some well-known FS techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Network traffic classification has attracted a lot of interest in various areas, including Supervisory Control and Data Acquisition (SCADA) (industrial network) security monitoring, Internet user accounting, Quality of Service, and user behaviour. Classification-based techniques <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b32">34]</ref> rely on a set of ''good'' features (that can provide a better class separability) in order to develop accurate and realistic traffic models. Identification of good features for classification is a challenging task because: (i) this requires expert knowledge of the domain to understand which features are important, (ii) data sets may contain redundant and irrelevant features which greatly reduces the accuracy of the classification process and (iii) the efficiency of the classifiers (e.g., based on Machine Learning techniques) is reduced when analysing a large number of features. Indeed, a number of studies (e.g. <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b24">26]</ref>) have shown that irrelevant/redundant features can degrade the predictive accuracy and intelligibility of the classification model, maximise training and testing processing time of the classification model, and increase storage requirements. This paper addresses such issues and proposes a new technique that identifies a small set of ''good'' features that can increase the accuracy and efficiency of network traffic classification.</p><p>Previous classification approaches that used the basic information from IP headers and payload (such as the packet content) for classification did not work well. IP headers contained a few features (such as IP addresses, port numbers, and protocols) cannot accurately distinguish between applications. Payload-based techniques relied on deep inspection of packet content which resulted in significant processing and memory constraints on the bandwidth management tool. Recent approaches <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b25">27]</ref> address the above limitations by (i) avoiding deep packet inspection by creating additional new features from Transport Layer Statistics (TLS), e.g., statistical information in features of the traffic such as packet length and packet arrival time (see Section 5.1), and (ii) applying machine learning techniques to learn from the data. Even though these approaches provide a promising alternative, they suffer from the presence of a large number of irrelevant/ redundant TLS-based features. To improve such approaches, we need to properly eliminate redundant features and identify the most relevant features (which we refer to as best features).</p><p>Existing machine learning (ML) based approaches <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b48">50]</ref> focus on achievable classification accuracy through the use of various ML techniques such as classification and clustering; however, they suffer from irrelevant and redundant features. On the other hand, feature selection techniques <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b24">26]</ref> can be used for identifying the best features by eliminating irrelevant features. Feature selection techniques can be divided into two main categories: the wrapper method and the filter method <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b24">26]</ref>. The former method <ref type="bibr" target="#b24">[26]</ref> employs an existing ML technique (e.g. Support Vector Machine (SVM) <ref type="bibr" target="#b47">[49]</ref> and Bayesian Neural Network (BNN) <ref type="bibr" target="#b1">[3]</ref>) as a classifier and uses the classifier's accuracy as the evaluation measure to select the best possible features. Such a method tends to be not only computationally expensive but also inherits bias toward the predetermined learning algorithm. The latter method <ref type="bibr" target="#b3">[5]</ref> relies on the natural characteristics of the data (e.g. correlation) and does not require a predetermined mining algorithm to select feature subsets. As a result, this method does not inherit the bias <ref type="bibr" target="#b28">[30]</ref> of any mining algorithm and it is also computationally effective. However, different filter-based methods use different evaluation criterion (e.g. information-based measure, dependence-based measure, consistency-based measure and distance-based measure). Therefore, one of the key challenges (in selecting a filter method) is to define appropriated metrics that can be used to properly compare existing FS techniques to classify traffic. This paper proposes new metrics, called goodness (to measure the quality of the generated feature set by each FS technique), stability (to measure the sensitivity of a FS technique under variations to the training traffic data) and similarity (to measure the diversity and disagreement between FS techniques), and these three metrics enable us to compare and understand the inner instruments of each FS technique and the common differences between them. As shown in the proposed experiments (in Section 5), Fig. <ref type="figure">1</ref>. The process of network traffic classification consists of four parts: (1) Traffic Data Repository (from/to which traffic data are retrieved and stored), (2) Data Pre-processing (for traffic flow feature selection), (3) Classification Engine (which comprises of various types of classification methods), and (4) Dispersion Graph (for traffic visualisation) <ref type="bibr" target="#b25">[27]</ref>.</p><p>each FS technique has its own advantages and no single technique performs equally well on all three metrics.</p><p>The other key challenge (for traffic classification) is to preserve the maximum number of relevant features for traffic classification. It is found that classification accuracy is related to the number of relevant features used in the classification process. However, different FS techniques choose different sets of relevant features. Even worse, they do not always choose the same number of relevant features. This is problematic for the following reasons: (i) different feature selection techniques may yield feature subsets that can be considered local optima in the space of feature subsets; (ii) the representative power of particular feature selection techniques may constrain its search space such that the optimal subset cannot be reached; and (iii) a ''combined'' approach can give a better approximation to the optimal subset or ranking of features (which is often not applicable with a single FS technique). In addition to the new metrics, the second contribution of this paper is an algorithm that combines the benefits of several well-known feature selection techniques, which is inspired by similar work in sensor fusion <ref type="bibr" target="#b39">[41]</ref>, classifier combination <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref>, and clustering ensemble algorithms <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b50">52]</ref>. To the best of our knowledge and research in existing literature, this is the first ensemble-based technique used for feature selection. In this approach, a feature selection technique is considered as a domain expert. Features that were supported by many domain experts are considered important in contributing to high classification accuracy and more efficient computation. Our proposed approach presents an efficient way of selecting the ''best'' features for network traffic classification by introducing the concept of support as an optimality criterion to keep the size of the feature set small. Fig. <ref type="figure">1</ref> provides an overview of the implementation of the proposed approach in practice.</p><p>The proposed metrics and approach are evaluated using four publicly available benchmark traffic data sets. Our extensive experiments show that the proposed approach indeed provides a robust and meaningful way of identifying ''best'' features for traffic classification by exploiting the advantages of each FS technique (see Section 7.3 for details). Fig. <ref type="figure">1</ref> provides an overview of the implementation of the FS techniques and ML algorithms for traffic classification in practice.</p><p>This paper is organised as follows. Section 3 describes the steps of the general feature selection process. Section 4 introduces the concept of the three new metrics. Section 5 describes the experimental methodology, including benchmark traffic data sets. Section 6 presents our initial investigation based on the proposed metrics, followed by a discussion of the experimental results. Section 7 presents our proposed Local Optimization Approach. Section 2 discusses related work. Section 8 presents the conclusion and discussions on future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This section will survey some of the well known techniques that are relevant to network traffic characterisation. Previously, well-known port numbers have been used to identify Internet traffic <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b19">21]</ref>. Such an approach was successful because traditional applications used fixed port numbers; however <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b18">20]</ref> show that the current generations of P2P applications try to hide their traffic by using dynamic port numbers. Consequently, applications whose port numbers are unknown cannot be identified in advance.</p><p>Another approach relies on the inspection of packet contents <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b14">16]</ref>, and analyses the packets' payload content to check if they contain signatures of well-known or anomalous applications. Features are extracted from the traffic data, and later compared to well-known signatures of applications provided by human experts. This works well for Internet traffic; however, several studies <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b31">33]</ref> have shown that this approach has a number of drawbacks and limitations. Firstly, it cannot identify a new or unknown attack for which signatures are not available, so there is a need to maintain an up-to-date list of signatures. This is a problem because new applications and attacks emerge everyday, hence it is not practical and sometimes impossible to keep up with the latest signatures. Secondly, deep packet inspection is a difficult task as this requires significant processing time and memory. Finally, if the application uses encryption, this approach no longer works <ref type="bibr" target="#b9">[11]</ref>. Extensive surveys for the issues of traditional traffic classification can be found in <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b26">28]</ref>. A promising approach that has recently attracted some attention is based on TLS (Transport Layer Statistics) data and efficient data mining algorithms <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b32">34]</ref>. This assumes that applications typically send data in some sort of pattern, which can be used as a means of classification of connections by different traffic classes. To extract such patterns only TCP/IP headers are needed to observe flow statistics such as mean packet size, flow length and total number of packets (a full description of these features can be found in <ref type="bibr" target="#b33">[35]</ref>). This allows the classification techniques to rely on sufficient information.</p><p>A number of other research studies have applied Machine Learning (ML) algorithms on the TLS data to address the problem of network traffic analysis. ML algorithms can be divided into two categories: supervised learning (e.g. <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b48">50]</ref>) and unsupervised learning (e.g. <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b8">10]</ref>). However, the quality of network traffic data (TLS) can degrade the performance of these ML techniques <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b25">27]</ref>. This is because the TLS data can contain irrelevant or redundant features. Therefore, the question related to the selection of best features still remains a real problem for many models. FS techniques act as a preprocessing step to improve the efficiency as well as the accuracy of ML algorithms by identifying a small subset of highly predictive features out of a large number of features in the data set that are possibly irrelevant or redundant. The use and the implementation of FS techniques (both filter and wrapper) for network traffic data (TLS) have drawn significant attention over the past few years <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b41">43]</ref>. Much of these studies focus on improving the accuracy classification task by discarding the relevant and/or redundant features from the traffic data. However, these studies neglected some issues such as similarity, goodness and stability of these FS techniques when classifying TCP/IP traffic-flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The feature selection techniques used for benchmarking</head><p>There is a wide variety of FS techniques in the literature. However, with high-dimensional network traffic data, neither the wrapper method nor complex search algorithms are applicable. In this paper, we resorted to the use of filter methods <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b7">9]</ref>, since they do not rely on the use of any data mining algorithm. They are simpler to implement and have fewer parameters to be tuned. However, filter methods are designed with different evaluation criteria and a given method is often tailored to a specific domain, which therefore may not work well on other domains. To identify the best features for network traffic, we have analysed some well-known FS techniques, each being the best one for a specific criterion. At the end, we selected six (6) FS techniques, which cover the following evaluation criteria: information, dependence, consistency, distance, and transformation. These are: Information Gain (IG) <ref type="bibr" target="#b4">[6]</ref> and Gain Ratio (GR) <ref type="bibr" target="#b16">[18]</ref>(for information-based criteria), Principal Component Analysis (PCA) <ref type="bibr" target="#b17">[19]</ref> (for transformation-based criteria), Correlation-based Feature Selection (CBF) <ref type="bibr" target="#b46">[48]</ref> (for dependence-based criteria), Chisquare, <ref type="bibr" target="#b38">[40]</ref> (for statistical criteria), and Consistency-based Search (CBC) <ref type="bibr" target="#b7">[9]</ref> (for consistency-based criteria).</p><p>Before describing the specifics of each of the six FS techniques, let us first look at their common characteristics. All these techniques (as shown in Fig. <ref type="figure">2</ref>) share a similar process to select a best subset <ref type="bibr" target="#b6">[8]</ref>, in which the selection process of the best subset (by each FS technique) has four steps which include: subset generation, subset evaluation, stopping criterion, and final subset validation. Consequently, a feature is selected if additional information is gained when it is added to the previously selected feature set, and discarded in the opposite case since the information obtained is already contained (redundant) in the previous set.</p><p>Here are the specifics of the six selected FS techniques.</p><p>Information Gain <ref type="bibr" target="#b16">[18]</ref>: This is one of the approaches used for decision tree construction the ID3<ref type="foot" target="#foot_0">1</ref> (Iterative Dichotomiser 3 algorithm) <ref type="bibr" target="#b34">[36]</ref>. It measures the number of bits of information provided in class prediction by knowing the value of features <ref type="bibr" target="#b40">[42]</ref>. The feature with the highest value of information gain is considered as the splitting point, while a feature with the minimum value reflects the impurity in data partitions. Information gain is defined as the difference between the original information (which is based on the proportion of the class) and the new information (which is obtained after partitioning). A variety of FS techniques based on information criterion have been proposed including Mutual Information and Term Strength. However, Yang <ref type="bibr" target="#b45">[47]</ref> reported that Information Gain performed much better on their multi-class benchmarks due to its ability to aggressively reduce non-informative features, therefore Information Gain has been chosen in our approach as a generalised form for the information-based criterion.</p><p>Gain Ratio <ref type="bibr" target="#b16">[18]</ref>: This approach incorporates ''split information'' of the feature into information gain measure. Gain ratio attempts to overcome the bias of information gain toward the feature with a large number of distinct values by applying normalisation to information gain using a split information measure (which represents the potential information generated by splitting the training data set into partitions). The features are ranked based on the value of gain ratio. Therefore, the ratio becomes unstable if the value of splitting point reaches zero. In general, gain ratio is an information theoretic measure that selects features with an average-or-better gain and its advantage over Information Gain is that it does not consider features with a large number of distinct values. Principal Component Analysis (PCA) <ref type="bibr" target="#b17">[19]</ref>: This approach searches for K n-based vectors used to represent the original traffic data. Such data is projected onto a much smaller space. PCA combines the essence of attributes by creating a small set of variables. The input data are a linear combination of principal components, and explain the entire changes with several components. The purpose is to make an effective explanation through dimension reduction using linear equations.</p><p>Although the p components are required to reproduce the total system variability, often much of this variability can be accounted for by a small number, say k, of the principal components. If so, there is almost as much information in the k components as there is in the original p variables. The k principal components can then replace the initial p variables, and the original data set, consisting of n measurements on p variables, is reduced to one consisting of n measurements on k principal components. PCA and Linear Discriminant Fig. <ref type="figure">2</ref>. Feature selection process <ref type="bibr" target="#b28">[30]</ref>.</p><p>Analysis (LDA) approaches transform the data in the high-dimensional space to a space of fewer dimensions, and they are considered to be the only two feature selection techniques available for this criterion <ref type="bibr" target="#b2">[4]</ref>. However, Yan et al. <ref type="bibr" target="#b43">[45]</ref> report that LDA suffers from two intrinsic problems: (i) singularity of within-class scatter matrices and (ii) limited available projection directions. Therefore, we have chosen PCA (in our approach to represent the transformation-based criterion) since it outperforms LDA <ref type="bibr" target="#b49">[51]</ref>.</p><p>Correlation-based Feature Selection (CBF) <ref type="bibr" target="#b46">[48]</ref>: CBF is a widely used filtering algorithm. For a given traffic data set, the algorithm tries to find an optimal subset which is best related to the predicted class and does not contain any redundant features. Two aspects are noteworthy: feature class correlation and feature-feature correlation. The former indicates how much a feature is correlated to a specific class while the latter represents the correlation between two features. Fayyad and Irani <ref type="bibr" target="#b12">[14]</ref> used an information theory method to discretise numeric features and then used symmetrical uncertainty to measure feature-feature correlation where there is no notion of one feature being a class <ref type="bibr" target="#b46">[48]</ref>. The advantage of such a method is that it is fast, and can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis <ref type="bibr" target="#b46">[48]</ref>.</p><p>Chi-square <ref type="bibr" target="#b38">[40]</ref>: This approach uses a discretisation technique based on a statistic measure and evaluates flows individually with respect to the classes. It measures the association between the class and input feature F. The range of continuous valued features needs to be discretised into intervals. A numeric feature is initially stored by placing each observed value into its own interval. The next step, Chi-square X 2 measurement determines whether the relative frequencies of the classes in adjacent intervals are similar enough to justify merging. The merging process is controlled by a predetermined threshold, which is determined through attempting to maintain the validity of the original data. <ref type="bibr" target="#b45">[47]</ref> reported that Chi-square performs well due to its ability to potentially perform as a feature selection and discretise numeric and ordinal features at the same time. In other words, it works as a combined discretisation and feature selection technique <ref type="bibr" target="#b38">[40]</ref>.</p><p>Consistency-based Search (CBC) <ref type="bibr" target="#b7">[9]</ref>: This technique uses a consistency measure that does not attempt to maximise the class separability but tries to retain the discriminating power of data defined by original features. In other words, using this measure, feature selection is formalised as finding the smallest set of features that can identify flows of a class as consistently as the complete feature set. Therefore, the consistency measure is capable of handling irrelevant features in the original space reflected as a percentage of inconsistencies. For instance, if two instances of the pattern represent different classes, then that pattern is considered to be inconsistent. The consistency measure can help remove both redundant and irrelevant features. This type of evaluation measure is characteristically different from other measures because of its heavy reliance on the training data set and use of Min-Features bias in selecting a subset of features <ref type="bibr" target="#b7">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed new metrics</head><p>Section 3 described six well-known FS approaches covering various criteria. However, one of the major problem is the lacks of metrics to properly compare such techniques so to reveal the best features in network traffic. Here, we propose three new metrics to address such a problem: Goodness refers to how well a generated subset can classify the traffic flows accurately. Stability refers to the property of selecting the same set of features irrespective of variations in the traffic data that have been collected over a period of time. Similarity compares the behaviour of multiple FS techniques on the same data, and also evaluates how different criteria differ in generating an optimal set for a given data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluating goodness</head><p>The aim here is to evaluate the accuracy of the final output of the selected FS techniques (described in Section 3). In practice, a straightforward way is to measure the results directly using prior knowledge about the data. In network traffic data, however, we often do not have such prior knowledge. Therefore, we need to rely on some indirect methods <ref type="bibr" target="#b6">[8]</ref> (e.g. error rate), such as the one that monitors the change in classification performance caused by the change of features. On extremely imbalanced data set, the error rates cannot provide the information on minority class (e.g. Attack), thus the Goodness Rate (GR) is used as a performance metric. For a selected feature subset, we simply conduct the before-and-after experiment to compare the Goodness Rate (GR) of the classifier learned on the full set of features and that learned on the final selected subsets.</p><p>The goal is to explain how to evaluate the goodness of the final set, since varying an independent measure (denoted as M i ) will produce different sets of features. The following steps are used to validate (the goodness of) the output set generated by an independent measure: Apply a Naive Bayes classifier to the data with only the optimal subset produced by independent measure M i . Naive Bayes is chosen because it does not require feature weighting <ref type="bibr" target="#b13">[15]</ref>; therefore, its performance depends solely on the number of features selected. Moreover, Naive Bayes has been shown to work better than more complex methods <ref type="bibr" target="#b42">[44]</ref>. Hence, we emphasise the advantages of using the simplest of the computational methods to ensure the process is tractable in time. Validate the goodness of the results using the fitness function, which is defined as follows:</p><formula xml:id="formula_0">GoodnessðS i Þ ¼ 1 Y X Y i¼1 N tp i N i<label>ð1Þ</label></formula><p>where Y is the number of classes in the data set, N tp i denotes the number of true positive of each class, and N i is the total number of instances for class i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluating stability</head><p>The aim here is to measure the stability of the selected FS techniques, motivated by the need to provide network experts with quantified evidence that the selected features are relatively robust to variations in the traffic data. In practice, network operators tend to have less confidence in FS techniques that produce a different set of features on data sets taken over a period of time. Therefore, a candidate set of features that not only yields high prediction but also has a relatively stability is preferable <ref type="bibr" target="#b36">[38]</ref>. Let S = {S 1 , S 2 , . . . , S jDj } be a collection of feature subsets obtained by running a single FS technique t 2 T, each time with the same configuration, on different traffic data sets (say D, where jDj is the total number of data sets used). Let X be a subset representing all the features that occur anywhere in S:</p><formula xml:id="formula_1">X ¼ ff i jf i 2 S; F f i &gt; 0g ¼ [ jDj i¼1 S i ; X -0<label>ð2Þ</label></formula><p>where F f i is the frequency of the feature f i . In a situation when the confidence of a feature needs to be measured, then the following formula is used:</p><formula xml:id="formula_2">stabðf i Þ ¼ F f i À 1 jDj À 1<label>ð3Þ</label></formula><p>where F f i is the frequency of feature f i 2 X in the collection S, and jDj denotes the total number of generated subsets. Thus, all confidence values are normalised between [0, 1]. The measure of stability of the feature f i 2 X in collection S takes the following properties:</p><p>stab (f i ) = 0: f i does not appear anywhere in the observed subsets. stab (f i ) = 1: f i appears in each subsets of the system.</p><p>To evaluate the average confidence of all features in the collection S, we need to extend Eq. ( <ref type="formula" target="#formula_2">3</ref>). Let N be the total number of frequencies of any feature f i that appears in collection S. N will then be</p><formula xml:id="formula_3">N ¼ X i2X F i ¼ X jDj i¼1 jS i j; fN 2 IN; N P ng ð<label>4Þ</label></formula><p>Therefore, the stability over all features f i 2 X in collection S is defined as</p><formula xml:id="formula_4">stabðSÞ ¼ X f i 2X F f i N Â F f i À 1 jDj À 1<label>ð5Þ</label></formula><formula xml:id="formula_5">F f i</formula><p>N represents the relative frequency of the features f i 2 X in a subset. If stab (S) value is close to 1, this indicates that all subsets are identical, in particular, only if N = jDj Â jXj.</p><p>In contrast, suppose stab (S) value is close to 0 (if N = jXj), then this implies a low level of stability to in overall subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluating similarity</head><p>The stability measure can only evaluate the stability of an FS technique on different traffic data sets <ref type="bibr" target="#b36">[38]</ref>. However, it is important to compare the behaviour of different FS techniques on the same traffic data set, or evaluate how, for a given data set, the candidate FS techniques differ in the preference of particular features. Therefore, we propose a similarity measure to allow a comparison of multiple FS techniques results. This will, in turn, enable the evaluation of how an optimal subset generated (using one criterion) can differ according to another criterion.</p><p>Given F f i is the number of frequency of features f i in a collection S, the computation of the desirable properties of the proposed similarity measure is done as follows. If the value of Sim (jTj) is close to 1, this will indicate high similarity; and any value close to 0 will indicate low similarity. Similarity is defined as follows:</p><formula xml:id="formula_6">SimðTÞ ¼ X f i 2X F f i N Â F f i À 1 jTj À 1<label>ð6Þ</label></formula><p>where jTj denotes the total number of feature selection techniques that have been applied on a single data set.</p><p>Let jDj be the number of used traffic data sets. The similarity between two candidate FS techniques, say t 1 and t 2 , (across different data sets), is defined as</p><formula xml:id="formula_7">Simðt 1 ; t 2 Þ ¼ 1 À 1 2 X F t 1 f i N t 1 À F t 2 f i N t 2<label>ð7Þ</label></formula><p>where F t 1 f i denotes the number of occurrences (frequencies) of feature f i in t 1 and F t 2 f i is the frequency of the same feature in t 2 .</p><p>Both Sim (T) and Sim(t 1 , t 2 ) take value form [0, 1], with 0 indicating that there is no similarity between the candidates FS techniques' outputs, and 1 indicating that such techniques are generating identical subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental methodology</head><p>The main focus of this section is to demonstrate the benefits of the proposed metrics as an evaluation framework (to find an appropriate FS technique that improves the performance of the classification process). In what follows, we describe the network traffic trace data collected over different periods of time. We also show the performance results of the different FS techniques for the network traffic classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The data sets</head><p>We compare the candidate FS techniques on labelled Internet traffic data <ref type="bibr" target="#b33">[35]</ref>. The TCP traffic flows in such data have been manually classified and collected by monitoring a high-performance network <ref type="bibr" target="#b30">[32]</ref>. We limit ourselves to the available traffic data. This data consists of ten data sets of flows taken from two days of network activity. Each data set consists of flows (objects), and each flow is described by a set of features and its membership class. Each set covers randomly the same length of time throughout the 24-h period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Data collection</head><p>Publicly available labelled traffic data sets are very rare due to security and privacy concerns <ref type="bibr" target="#b29">[31]</ref>. The traffic data sets collected by the high-performance network monitor (described in <ref type="bibr" target="#b30">[32]</ref>) are one of the largest publicly available network traffic traces that have been used in our experiment. These data sets are based on traces captured using its loss-limited, full-payload capture to disk where timestamps with resolution of better than 35 ns are provided. The data was taken for several different periods in time from one site on the Internet. This site is a research facility which hosts up to 1000 users connected to the Internet via a full-duplex Gigabit Ethernet link. Full-duplex traffic on this connection was monitored for each traffic set. The site hosts several biology-related facilities, collectively referred to as the Genome Campus (Cambridge Lab).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Traffic categories</head><p>Classes of traffic are common groups of applications. Some approaches have simpler definitions of classes (e.g., Normal versus Attack), but others have more complex definitions (e.g. the classification of specific applications) <ref type="bibr" target="#b35">[37]</ref>. We have used the class descriptions provided in <ref type="bibr" target="#b33">[35]</ref>, which can be used as the basis for evaluating the candidate feature selection techniques to identify important features for traffic classification. Table <ref type="table" target="#tab_0">1</ref> shows the classes for the corresponding applications. The complete description can be found in <ref type="bibr" target="#b33">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Flow features</head><p>Each flow is characterized by a set of unique features that correspond to a specific class. Such features allow for discrimination between various traffic classes. Table <ref type="table" target="#tab_1">2</ref> provides a few examples drawn from the 248 per-flow features that are available from the data set. A full description of these features can be found in <ref type="bibr" target="#b33">[35]</ref>. Our aim is to identify the best features that are independent of a particular network-configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Classification flows</head><p>The application of a classification scheme requires the features of the objects to be classified. By using these features, the classifier allocates an object (flow) to a specific class. In this data set, the object classified is a TCP/IP traf-fic-flow, which is represented as a flow of single or multiple packets between a given pair of hosts. The flow is defined by n-tuple consisting of the IP addresses of the pair of hosts and the TCP port numbers used by the server and client. In this work, we are limited to the training and testing sets available (10 data sets), which consist only of TCP and semantically complete TCP connections. Semantically complete flows are flow events for which a complete connection set-up and tear-down was observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental setup</head><p>To evaluate the effectiveness of the selected FS techniques (to compute the ''best'' features for network traffic), we have used various sizes of data sets. Table <ref type="table" target="#tab_2">3</ref> shows information on the structure of data sets for evaluation. It is clear from the numbers that there are a different number of flows in each data set. This is due to a higher density of traffic during each block of 28 min. In these sets, each record is composed of a set of features and a class label of the flow. The type of the features is either continuous or discrete. The former is quantitative and the latter is on a qualitative scale.</p><p>The goodness of each FS technique is evaluated by applying K-fold-cross validation on each traffic data set. In this process, the data set is divided into K subsets. Each time, one of the K subsets is used for testing while the remaining K À 1 subsets form the training set. Performance statistics are calculated across all K trails. In these experiments, the value of K is set to 10, since this was suggested by Kohavi <ref type="bibr" target="#b23">[25]</ref> as the best empirical number for accuracy estimation and model selection. Therefore, we also expect that this K value will provide a good indication of how well the classifier performs and classifies unseen data based on all features (of the original data sets). Fig. <ref type="figure">3</ref> shows the different steps involved in evaluating the goodness of the candidate FS techniques. The first step provides the FS techniques with the required information (i.e. IP traffic generated by different applications). In practice, such techniques identify the smallest set of features that can be used to differentiate be-  tween applications. In the second step, the filtered traffic data is used to train a supervised machine learning algorithm (e.g. Naive Bayes) and to create the classifier model. This process of using cross validation to generate goodness results is repeated for each data set. The statistics of goodness are accumulated for all ten data sets. In this paper, the implementation of these FS techniques were performed using version 3.7.7 of the WEKA software suite (the readers are referred to <ref type="bibr" target="#b15">[17]</ref> for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>The six selected FS techniques are compared using the new metrics (see Section 4). For a given selected subset, an experiment is conducted using a Naive Bayes classifier to compare the goodness of the optimal subsets generated by different FS techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">The results</head><p>Here we discuss the various experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1.">Classification of the traffic based on all the features</head><p>Naive Bayes classifier is used to characterize the network traffic flows using the original full data set without applying any FS technique. From the results shown in Fig. <ref type="figure">4</ref>, the classifier achieved goodness on average of 67.94%, which means that on average only, 67.94% of flows have been correctly classified into their target classes using all the features. As expected, this result is not satisfactory because of the presence of irrelevant and redundant features in the data sets. However, it can also be seen in Fig. <ref type="figure">4</ref> that the model trained on some data sets (e.g. data sets #3, #4, #5 and #6) outperforms the remaining sets. This suggests that there is a low similarity between the corresponding flows of the data sets. The results also suggest good class separability, and this is why there is a significant increase in the number of correctly classified flows. Therefore, in the remainder of this paper, we will use different FS techniques to discard the irrelevant and redundant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Evaluation of ''goodness''</head><p>Both Table <ref type="table" target="#tab_3">4</ref> and Figs. 5 and 6 compare the classification Goodness Rate (GR) of the six FS techniques on the ten data sets. For the classification task, the applications of network traffic are expected to be correctly classified. Therefore, an FS technique with high GR is desired. Note that the sequence CBF, Chi-square, InfoGain, PCA, CBC and GainRatio roughly order the FS techniques according to increasing GR. This ordering is notable in Figs. <ref type="figure">5</ref> and<ref type="figure">6</ref> and Table <ref type="table" target="#tab_3">4</ref> on the achieved criterion values. From the results, the FS techniques achieve a higher classification of goodness in comparison with the outcomes of using a full features set. Overall, the goodness rate (GR) of the classification model has been substantially improved (mainly by removing these irrelevant and redundant features from network traffic data), except for GainRatio. The average GR using GainRatio is 61.73%, which is much lower than for Naive Bayes with all features. This indicates that the optimal sub- set selected by GainRatio may include some features that provide poor class separability. As a result, such features would reduce the accuracy of the (Naive Bayes) classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3.">Temporal variation of FS goodness</head><p>Fig. <ref type="figure">6</ref> shows that most FS techniques (for exception GainRatio) enable the classification scheme to perform better than the base case (i.e., complete feature set). It is also shown in Table <ref type="table" target="#tab_3">4</ref> and Fig. <ref type="figure">6</ref> that no FS performs well on all data sets. Fig. <ref type="figure">5</ref> shows a comparison of the performance of six widely used FS techniques on 10 different data sets. Overall, CBF has the best performance on all of the data sets, except for the data set #9. Chi-square achieves the best performance on the data sets #2, #3 and #7, but has the worst performance on data sets #1, #5 and #8. Information gain peaked on data sets #3, #7 and #8, but has the worst performance on data sets #4, #5 and #10. PCA achieves the best performance on data set #5 and #6, but has the worst performance on the data sets #1, #2, #3, #4 and #7. CBC achieves the best performance on the data sets #2, #5 and #6, but has the worst performance on the other data sets (i.e. #1, #4, #7, #9 and #10). Gain Ratio peaked on the data sets #3, #7 and #8, but performed significantly worse than the other techniques on data sets #1, #2, #9 and #10. We therefore conclude that we cannot rely on a single technique, and this is our main reason for developing a hybrid approach to identify a reliable (best) set of features that help classifiers to perform well on all data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4.">Evaluating stability of candidate feature selections</head><p>Fig. <ref type="figure" target="#fig_3">7</ref> shows the stability results obtained from each FS technique. Firstly, it can be seen that the clear winner is InfoGain, as this achieves the highest stability value of   0.87 for the ten traffic data sets under consideration. Secondly, Chi-square appears to have a better stability result than GainRatio, CBF and CBC respectively with a value of 0.70. Interestingly, GainRatio has a better stability score with 0.60. Notably better (higher) values in terms of method stability can be observed amongst InfoGain, Chi-square and GainRatio, with InfoGain being the most different from the other two methods. However, these three methods yield more stable results than CBF and CBC, which can be explained by the fact that they provide feature preference in a global respective. Finally, the stability of CBF and CBC is quite similar in terms of stability evaluation, but they achieved the worst stability scores with 0.42 and 0.39 respectively. The main reason for this is that the features provided by CBF and CBC focus on the top ranked or selected subsets. Consequently, they account poorly for feature inter-dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5.">Evaluating similarity of candidate feature selections</head><p>Table <ref type="table" target="#tab_4">5</ref> shows the similarity results obtained by performing n runs of feature selection techniques. This information given by the proposed similarity measure reveals the behaviour of n FS techniques on the same data It can be seen from the figure that there is low similarity between feature subsets (produced by InfoGain, Chi-square, CBF, CBC, GainRatio) on each traffic data set, with similarity values between 0.24 and 0.30. As suggested in Section 3, each FS technique produces an optimal set considerably different from those produced by other techniques. This observation makes sense, since an optimal subset selected using one criterion may not be the optimal subset when using another criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Discussion</head><p>As can been seen from the previous section, the results are not conclusive for any single FS technique. As such, the more FS techniques available, the more challenging it is to find a suitable one which will identify the best features for network traffic data.</p><p>This section provides a simple tabular approach to categorise the different FS techniques based on the proposed metrics. This way of comparing can serve two purposes: (i) grouping FS techniques with similar characteristics as well providing a way to compare such techniques on the same framework; and (ii) providing an intermediate step toward building an integrated FS technique to choose the best set of features for network traffic data. We categorise the normalised values of the proposed evaluation metric (EM) into three categories: low, medium and high using the following criteria:</p><formula xml:id="formula_8">0 Low if 0 6 EM 6 r 1 r &lt; EM &lt; HÀðMÀrÞ 2 2 High if HÀðMÀrÞ 2 6 EM 6 1 8 &gt; &lt; &gt; :<label>ð8Þ</label></formula><p>where M and H denote medium and high respectively. The value of r is set according to the various experimental results (presented in Section 6). In particular, the value of r is set to 0.60 for evaluating goodness as this is the lowest goodness rate among the candidate FS feature selection techniques. The value of r is set to 0.4 to evaluate stability as this is lowest score for stability among the candidate feature selection techniques. The same applies to similarity as this value indicates most selected features were supported by less than two techniques. Table <ref type="table">6</ref> summarises the values for each technique with regard to goodness, stability and similarity. We use these comparisons to help illustrate the appropriateness of each FS technique using Eq. ( <ref type="formula" target="#formula_8">8</ref>).</p><p>(1) For the goodness metric, prevailing ordering can be recognised among FS techniques: all FS techniques have an average value, except for CBF, whose value depends on the good quality of its output compared to the other techniques. This suggests that CBF is recommended for cases when FS techniques fail to produce good quality output. (2) In terms of stability, all FS techniques are almost unstable, with the exception of Information Gain and Chi-square. (3) From Table <ref type="table">6</ref>, one can notice constant low similarity between values yielded by the candidate FS techniques on the same data set. This suggests that a large number of features are consistently excluded while the rest appear in the selected subsets with low similarity. Also, it suggests that an optimal subset selected using one criterion is almost not optimal according to another criterion <ref type="bibr" target="#b28">[30]</ref>.</p><p>Based on the developed categorisation approach, it can be seen that in most cases, there is no visible ''winner''  among the FS techniques. As a result, there is no FS technique that satisfies all evaluation criteria. Hence, we cannot rely on a single feature selection technique to select the best set of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">The Local Optimisation Approach (LOA)</head><p>The previous section showed that any single FS technique cannot perform well on all data sets, and that different FS techniques generally produce different results. Given a traffic data set, and without any a priori knowledge, the problem still remains about the selection of an FS technique that will perform the best. Therefore, instead of choosing a particular FS technique, we have looked at ''combining'' five FS techniques so to ''aggregate'' their benefits in selecting the best features. This new approach is called Local Optimisation Approach (LOA). The reader may later notice that we have excluded Principle Component Analysis (PCA), as this technique transforms the original features to new ones to produce the best approximation of the original features. Therefore, this does not strictly fall into the category of feature selection. Fig. <ref type="figure">8</ref> depicts the overall idea of LOA: given a data set, local optimisation aims to select the most reliable subset of features based on feature subsets selected by FS techniques. As different FS techniques produce different subsets of features, we introduce the concept of support to indicate the importance of a specific feature. The idea behind this concept support is that the judgement of a group is superior to that of individuals. The underlying assumption is that an important feature for traffic analysis is very likely to be supported by most FS techniques.</p><p>Definition 1 (Support). Let F = {f i j1 6 i 6 m} be a set of features in a given data set, and T = {t j j1 6 j n} be a set of existing FS techniques. We then use a matrix A to record the occurrence of features for different techniques, where a i,j are binary values indicating whether the feature f i is selected by a technique t j (1 for selected, 0 for not selected). Therefore, the support of feature f i 2 F is defined as follows:</p><formula xml:id="formula_9">supportðf i Þ ¼ P n j¼1 a i;j jTj<label>ð9Þ</label></formula><p>where jTj is the number of techniques that have been applied.</p><p>The following steps are taken by LOA to identify the most reliable subset of features to be used for a particular training set.</p><p>Apply the five FS techniques on a training data set and keep all the selected features in an initial pool. As different FS techniques use different ways to generate feature subsets, finding salient features is often hard. Therefore, to make the best of the different techniques, LOA applies the five FS techniques on a training set to generate an initial pool of five sets of features. Features that have not been selected by any of the five FS techniques are discarded. Calculate the frequency of the observed features. Let S best be the set of selected features, where S best = {f 1 , -. . . , f n }. Then, the frequency of f i is defined by F (f i ) :¼ m i , where m i is the number of times F has the value f i . Order the features value of F based on their occurrences (frequency). Let have</p><formula xml:id="formula_10">f 1 ; f 2 ; . . . ; f n such that Fð f 1 Þ P Fð f 2 Þ P Á Á Á Fð f n Þ.</formula><p>Using Eq. ( <ref type="formula" target="#formula_9">9</ref>), calculate the support of the features in the S best by counting the number of times they are selected by FS techniques divided by the cardinality of T, where 0 &lt; F (f i ) 6 jTj. This is based on our assumption that the importance of a feature is indicated by the number of occurrences in the ''optimal'' feature subsets generated by the different FS techniques we applied. We hypothesise that a larger count of occurrences implies more distinct and reliable features. Examine the degree of support in the observed features. To do so, we apply an arbitrary threshold to retain only the top N features whose supports are above the threshold. The features in the S best have been selected by at least one of the five FS techniques; but to retrieve an optimal set of features, a threshold must be set to keep only those features that are sufficiently distinct and reliable. For instance, if a feature selected by at least three out of five FS techniques is considered reliable enough, then we apply a threshold of supp P 0.60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The proposed algorithm</head><p>Algorithm 1 summarises the various steps of LOA to compute the most informative features in a single data set, which we denote as DATA. This algorithm is divided into two parts. In the first part (lines 1-17), the algorithm extracts an initial pool of five sets of features by applying the five FS techniques, and returns the corresponding generated feature subsets. In particular, the search starts with an initial set of features, say S 0 , then this set S 0 is evaluated by an independent measurement, say t i . Evaluate each newly generated subset S i using t i . Compare the current subset, S i , with the Fig. <ref type="figure">8</ref>. The LOA approach.</p><p>previous optimal set S optimal . As a result, if the current set is better than the previous set, then it is considered as the current optimal subset. Iterate the search until a sufficiently optimal set, S optimal , is found based on the independent measurement t i . Output S optimal as the optimal set. Then add the optimal set,S optimal , of technique t i to the initial pool set S Sel . The second part of the algorithm (lines 18-30) measures the support of each feature value in S freq and includes those whose support exceeds the threshold into the set of reliable features, S best . Finally, the selected features are significant features that contain indispensable information about the original features. The algorithm would need an O(k), (where k is the number of FS techniques) for identifying a pool of features, followed by O(g) for the operation of calculating the support of features and selecting the most supportive set. Thus, the algorithm has a total complexity of O(k + g) for choosing the final set of features (see Section 7.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Local Optimisation algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">An illustrative example</head><p>A simple example is given below to illustrate the use of the proposed LOA approach to select the best possible features. Table <ref type="table" target="#tab_7">9</ref> (a) represents a unity of the sets of rows which correspond to the various FS techniques T (where t i -2 T) and the columns represent the features themselves (where f i 2 F). The binary values of Fig. <ref type="figure" target="#fig_4">9</ref> (a) indicates whether a feature is selected or not by the corresponding FS technique t i , where 1 stands for selected, and 0 for not selected. For instance, FS technique t 1 selects features {f 1 , f 2 ,f 4 , f 7 , f 10 }. In the last row of Fig. <ref type="figure" target="#fig_4">9</ref> (a), the frequency of a feature that has been selected is calculated by counting the number of times we observe f i taking the binary value (1). Fig. <ref type="figure" target="#fig_4">9</ref> (b) shows a list of the features sorted by frequency. Then the support of a feature is calculated using Eq. <ref type="bibr" target="#b7">(9)</ref>. A predetermined threshold is applied to retrieve the best features. For example, if the predefined threshold of support(f i ) P 0.60 is applied, then the features {f 7 , f 1 , f 4 , f 10 } are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Result and analysis</head><p>The aim here is to evaluate the performance of the proposed LOA algorithm. We first compare the performance of LOA against the five FS techniques (see Section 3). Then, we evaluate the effect of various parameter settings on the performance of LOA. For each FS technique, Naive Bayes is used to evaluate the classification goodness rate on selected features, as we have no prior knowledge about the most reliable features for Internet traffic data. Table <ref type="table" target="#tab_5">7</ref> summarises the Goodness Rate (GR) of LOA on 10 data sets using the Naive Bayes algorithm. As can be seen from Table <ref type="table" target="#tab_5">7</ref>, LOA performs well and was stable on all data sets. From the results shown in Fig. <ref type="figure">10</ref>, we observe that the LOA achieves an average goodness of 95.97%. Given the average goodness shown in Fig. <ref type="figure">10a</ref>, it can be seen that we were able to achieve higher GR in comparison with the remaining FS techniques. The experimental results shown in Fig. <ref type="figure">10c</ref> clearly demonstrate the performance of LOA across all the data sets. Notice, the GR on the data sets number #9 and #10 are not as good for either LOA or any other FS technique. The reason is that the HTTP class in these two data sets include 2600 records, which are related to a different class HTTPS. However, LOA has the best GR among all the techniques on #9 and #10 data sets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Choice of parameters</head><p>As discussed in the previous section, the main parameter in LOA is the support threshold, which we refer to as b, used for selecting the best feature set. The performance of LOA critically depends on the value of b. Thus, the choice of the parameter b not only affects the training and the testing time of classification, but also influences the goodness of the classification model. As discussed previously in Section 7, the choice of b is a trade-off between lower GR and higher processing requirements due to the increased number of features in the selected feature set. In this section, we investigate the effects of this parameter setting on the performance of LOA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1.">Runtime performance</head><p>We apply the proposed LOA approach to samples of traffic selected randomly from the 10 data sets, and the final result set is fed to the machine learning classifier (Naive Bayes) to generate and test the classifier model. We then measure the execution time required by Naive Bayes for different threshold settings.</p><p>Fig. <ref type="figure">11</ref> and Table <ref type="table" target="#tab_6">8</ref> show the run-time performance of the classifier with the threshold varying between 0.2 and 1.0. For various threshold settings, the test was repeated ten times to give the average execution time and the GR. As predicted earlier, the complexity of the classifier is linear with respect to the number of input features. Furthermore, LOA shows a significant reduction in computation time for the classifier when compared to using the full set of features.</p><p>Fig. <ref type="figure">11c</ref> shows how classification goodness evolves with the value of the threshold, along with run-time performance. It can be shown that the maximum value of GR for the parameter b is achieved when the support is set to 0.60. This suggests that the goodness would not increase if b were to be achieved. On the other hand, it can be seen that the high runtime performance is achieved when b set to 1.0 and that the higher parameter of support decreases the accuracy. Therefore, we have found that 0.8 &gt; b &gt; 0.4 provides the appropriate and stable region for the trade-off between increasing the goodness rate and lowering the processing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Impact of FS techniques on runtime</head><p>One of the key motivations for using FS techniques is to reduce the amount of time required by any ML algorithm (e.g Naive Bayes) to build the model and evaluate new coming flows. This is particularly important because the model building phase is computationally time consuming. Therefore, we use the output of the LOA and the other candidate FS techniques to measure the execution time required by Naive Bayes to build the classification model on a Core dual 2.2-GHz Intel processor machine with 2 Gbytes of RAM.</p><p>Fig. <ref type="figure" target="#fig_6">12a</ref> shows normalized build time for Niave Bayes when using the output of the LOA approach in comparison to the candidate FS techniques. The data set is comprises network traffic <ref type="bibr" target="#b30">[32]</ref> from all days of week, and the number of data in the data set was varied between 1000 and 10,000 (the sized of the data set is ultimately limited by the amount of memory since Naive Bayes needs to load the entire training data into the memory before building the model). For each of the feature sets, the test was repeated ten times to give the average execution time and to achieve greater confidence in the results. It can be seen that LOA shows a significant reduction of times in comparison to InfoGain, GainRatio and Chi-square. Note also that there is substantially smaller variance in computational performance for Naive Bayes when using LOA in comparison to CBF and CBC.  thousands of simultaneous network flows. The results show that we can successfully reduce the computation time if our selected feature subset is used in comparison to InfoGain, GainRatio and Chi-square. However, it is obvious that there is a smaller variance in computational performance for Naive Bayes when using LOA in comparison to CBF and CBC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6.">Comparing FS techniques computational performance</head><p>In this subsection, we compare the execution time of LOA against the candidate FS techniques to generate optimal features. For the analysis, the performance of each technique was tested with traffic samples varying from approximately 1000 to 10,000 traffic records, and all operations were performed on a Toshiba Satellite with Intel Pentium Core dual 2.2 GHz processor and 2 Gbytes of RAM. Fig. <ref type="figure">13</ref> shows the time needed by InfoGain, GainRatio, Chi-square and CBF techniques is quite low. This is because these techniques use a sequential search which is fast in producing results as the order of the search space is usually O(m((n 2 À n)/2)) or less (where m is the number of instances and n is the initial number of features). It is also notable that the cost of CBC is very high compared to the other FS techniques, as it requires O(mpn p ) (where p is the number of relevant features). On the other hand, the LOA execution time was significantly higher than other FS techniques, this is because LOA relies on all FS techniques to generate the initial feature set. A promising future research direction would be to reduce the execution time of LOA by using parallel computing such as multi-cores CPU or Graphics Processing Units (GPUs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7.">Summary of results with different data sets</head><p>In addition to the ten data sets collected by high-performance network monitor <ref type="bibr" target="#b30">[32]</ref> (discussed in Section 5.1), the capabilities of the proposed LOA has been further assessed against the baseline FS techniques with two of the recent and most widely used data sets. The first one is wide2009 [2] data set, where its flows are categorized into six classes: P2P, DNS, FTP, WWW, CHAT and MAIL. The second data set is KDD99 <ref type="bibr" target="#b0">[1]</ref>, which is the most widely used data set for evaluation of anomaly detection methods. KDD99 data set consists of 60,000 single connection vectors and labeled as either normal or an attack.</p><p>Table <ref type="table" target="#tab_7">9</ref> gives an overview of the data sets used along with their associated information.</p><p>Fig. <ref type="figure">14a</ref> compares the performance of the purposed LOA approach and the baseline FS techniques by considering the three metrics goodness, stability and similarity. The values of these three metrics are computed as explained in Section 6.2.</p><p>It can be seen from Fig. <ref type="figure">14</ref> that the LOA approach has an advantage over the other related FS techniques. First, the features obtained by LOA helps the Naive Bayes classifier to achieve a higher goodness rate in comparison with the remaining FS techniques on all the four data sets. Second, the LOA approach preserves the maximum number of relevant features for traffic classification by considering only highly supported features. In general,  the experimental results shown in Fig. <ref type="figure">14</ref> indicate that the proposed LOA approach satisfies the three metrics to some extent (in comparison to the related approaches). However, it can be seen from Fig. <ref type="figure">14c</ref> that the LOA approach still suffers from the stability issue on traffic data of the high-performance network monitor. This is due to the high variations in these data sets, since these data sets are collected from different periods of times and from different locations. In the future, we will work on developing a new approach to address the sensitivity of the baselines methods and the LOA approach to variations in the traffic data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Identifying the best and most robust (in terms of similarity) features from large data sets of Internet traffic is of critical importance in light of the emergence of new and distributed applications. This paper made three contributions with regard to the problem of computing the best features (in network traffic). We introduced three new metrics, namely goodness, similarityand stability. The primary purpose of these metrics is to gain a deeper understanding of the properties of the FS techniques as well as to compare the quality of their outputs (selected subsets). The experimental results have shown that no existing FS technique performs well on all the three metrics. Motivated by this, we proposed a method that exploits the advantages of individual FS techniques to obtain an optimal feature set that is better than any individual set. We also showed how to select the best subset based on the concept of support to extract the optimal set. The proposed LOA (Local Optimisation Approach) technique was analysed in light of the optimality criteria. Results obtained on real network traffic data illustrates the ability of LOA to identify the best features for traffic classification. As expected, the joint contributions of the five well-known feature selection techniques had a compensatory effect. Experimental results also showed that LOA performs significantly better than an individual technique.</p><p>Integrated FS approaches are computationally more expensive than the single run; however, as demonstrated empirically, once computed, they provide increased robustness and performance, being able to identify the best features for traffic classification, not adequately handled by these techniques. We have identified the need for developing an adaptive threshold instead of the fixed threshold beta used in LOA. Future work will further reduce such computational requirements by parallelizing the proposed approaches in distributed systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Final subset validation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Goodness of CBC and GainRatio</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Classification of the traffic based on features of the candidate FS techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparing feature selection stability on traffic data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Procedure of local optimization approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Evaluation LOA against the selected FS techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b10">12</ref>. Evaluation LOA against the selected FS techniques (a value 1 represents the lowest build and classification time).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 .Fig. 13 .</head><label>1413</label><figDesc>Fig. 14. Comparing the performance of FS techniques on two more traffic data sets, namely: wide2009 [2] and KDD99 [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,55.28,389.84,439.61,266.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>An example of network applications.</figDesc><table><row><cell>Classification</cell><cell>Application</cell></row><row><cell>BULK</cell><cell>FTP</cell></row><row><cell>DATABASE</cell><cell>Postgres, Sqlnet Oracle, Ingres</cell></row><row><cell>INTERACTIVE</cell><cell>SSH, klogin, rlogin, telnet</cell></row><row><cell>MAIL</cell><cell>imap, pop2, SMTP</cell></row><row><cell>SERVICES</cell><cell>X11, DNS, ident, ldap, NTP</cell></row><row><cell>WWW</cell><cell>http</cell></row><row><cell>P2P</cell><cell>KazaA, Bittorrent, GnuTella</cell></row><row><cell>ATTACK</cell><cell>Internet worm and virus attacks</cell></row><row><cell>GAMES</cell><cell>Microsoft Direct Play</cell></row><row><cell>MULTIMEDIA</cell><cell>Windows Media player, Real</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>An example of features used as input for traffic classification<ref type="bibr" target="#b51">[53]</ref>.</figDesc><table><row><cell>Features</cell></row><row><cell>Flow metrics (duration, total packets)</cell></row><row><cell>Packet inter arrival time (mean, variance)</cell></row><row><cell>Size of TCP/IP control fields</cell></row><row><cell>Total packets (in each direction of flow)</cell></row><row><cell>Payload size (mean, variance)</cell></row><row><cell>Effective bandwidth based</cell></row><row><cell>Fourier-transform of packet</cell></row><row><cell>TCP specific values derived from</cell></row><row><cell>tcptrace (e.g. total of pushed packets)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Flow statistics (percentages of flows) according to applications.</figDesc><table><row><cell>Total flows</cell><cell>WWW</cell><cell>MAIL</cell><cell>BULK</cell><cell>SERV</cell><cell>DB</cell></row><row><cell>378101</cell><cell>86.77%</cell><cell>7.56%</cell><cell>3.05%</cell><cell>0.56%</cell><cell>0.70%</cell></row><row><cell></cell><cell>INT</cell><cell>P2P</cell><cell cols="3">ATTACK MMEDIA GAMES</cell></row><row><cell></cell><cell>0.03%</cell><cell>0.55%</cell><cell>0.47%</cell><cell>0.28%</cell><cell>0.02%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>The goodness rate of FS techniques on the ten data sets.</figDesc><table><row><cell>FS techniques</cell><cell>D 1</cell><cell>D 2</cell><cell>D 3</cell><cell>D 4</cell><cell>D 5</cell><cell>D 6</cell><cell>D 7</cell><cell>D 8</cell><cell>D 9</cell><cell>D 10</cell></row><row><cell>CBF (%)</cell><cell>96.98</cell><cell>94.60</cell><cell>93.65</cell><cell>95.47</cell><cell>94.91</cell><cell>87.29</cell><cell>94.60</cell><cell>92.69</cell><cell>44.19</cell><cell>93.8</cell></row><row><cell>InfoGain (%)</cell><cell>87.78</cell><cell>88.96</cell><cell>95.95</cell><cell>83.06</cell><cell>50.36</cell><cell>86.75</cell><cell>94.99</cell><cell>89.70</cell><cell>87.71</cell><cell>48.40</cell></row><row><cell>Chi-square (%)</cell><cell>67.36</cell><cell>92.68</cell><cell>95.94</cell><cell>85.55</cell><cell>48.12</cell><cell>84.92</cell><cell>95.51</cell><cell>76.50</cell><cell>90.99</cell><cell>89.32</cell></row><row><cell>PCA (%)</cell><cell>78.89</cell><cell>65.23</cell><cell>79.57</cell><cell>82.41</cell><cell>90.76</cell><cell>90.57</cell><cell>71.38</cell><cell>81.99</cell><cell>84.35</cell><cell>86.93</cell></row><row><cell>CBC (%)</cell><cell>21.58</cell><cell>93.58</cell><cell>76.92</cell><cell>27.79</cell><cell>96.40</cell><cell>92.80</cell><cell>67.06</cell><cell>93.38</cell><cell>42.57</cell><cell>69.02</cell></row><row><cell>Original (%)</cell><cell>57.89</cell><cell>61.70</cell><cell>84.45</cell><cell>74.51</cell><cell>79.29</cell><cell>90.07</cell><cell>51.86</cell><cell>58.35</cell><cell>67.12</cell><cell>54.20</cell></row><row><cell>GainRatio (%)</cell><cell>9.48</cell><cell>12.28</cell><cell>96.27</cell><cell>89.72</cell><cell>88.11</cell><cell>88.64</cell><cell>94.52</cell><cell>93.91</cell><cell>29.77</cell><cell>14.63</cell></row><row><cell>Goodness Rate (%)</cell><cell>0 20 40 60 80 100</cell><cell></cell><cell cols="2">CBF InfoGain</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">D1 D2 D3 D4 D5 D6 D7 D8 D9 D10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Traffic Data Sets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Comparing feature selection similarity on traffic data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Evaluation of FS techniques on the categorisation framework.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FS tech</cell><cell>Goodness</cell><cell>Stability</cell><cell>Similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CBF</cell><cell>High</cell><cell>Low</cell><cell>Low</cell></row><row><cell>Data sets Similarity</cell><cell>D 1 0.28</cell><cell>D 2 0.27</cell><cell>D 3 0.29</cell><cell>D 4 0.26</cell><cell>D 5 0.29</cell><cell>InfoGain Chi-square PCA</cell><cell>Medium Medium Medium</cell><cell>High High Low</cell><cell>Low Low Low</cell></row><row><cell>Data sets</cell><cell>D 6</cell><cell>D 7</cell><cell>D 8</cell><cell>D 9</cell><cell>D 10</cell><cell>CBC</cell><cell>Medium</cell><cell>Low</cell><cell>Low</cell></row><row><cell>Similarity</cell><cell>0.27</cell><cell>0.28</cell><cell>0.25</cell><cell>0.24</cell><cell>0.27</cell><cell>GainRatio</cell><cell>Low</cell><cell>Medium</cell><cell>Low</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>The GR of LOA approach on 10 different data sets.</figDesc><table><row><cell>Data set</cell><cell>D 1</cell><cell>D 2</cell><cell>D 3</cell><cell>D 4</cell><cell>D 5</cell></row><row><cell>Goodness rate</cell><cell>97.51</cell><cell>95.94</cell><cell>97.89</cell><cell>96.03</cell><cell>97.48</cell></row><row><cell>Data set</cell><cell>D 6</cell><cell>D 7</cell><cell>D 8</cell><cell>D 9</cell><cell>D 10</cell></row><row><cell>Goodness rate</cell><cell>97.09</cell><cell>96.05</cell><cell>97.32</cell><cell>90.51</cell><cell>93.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Influence of different setting of support threshold.</figDesc><table><row><cell>Supp. threshold</cell><cell>Goodness</cell><cell></cell><cell>Set size</cell><cell cols="2">Run time (s)</cell></row><row><cell></cell><cell>GD</cell><cell>Stdev</cell><cell></cell><cell>RT</cell><cell>TT</cell></row><row><cell>0.2</cell><cell>70.39</cell><cell>21.13</cell><cell>25</cell><cell>3.57</cell><cell>50.54</cell></row><row><cell>0.4</cell><cell>87.04</cell><cell>7.60</cell><cell>14</cell><cell>2.16</cell><cell>30.12</cell></row><row><cell>0.6</cell><cell>95.97</cell><cell>4.11</cell><cell>6</cell><cell>0.53</cell><cell>7.59</cell></row><row><cell>0.8</cell><cell>78.95</cell><cell>39.00</cell><cell>3</cell><cell>0.17</cell><cell>2.47</cell></row><row><cell>1</cell><cell>77.86</cell><cell>32.00</cell><cell>1</cell><cell>0.15</cell><cell>2.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9</head><label>9</label><figDesc>Summary of the data sets used for performance evaluation.</figDesc><table><row><cell>Data Sets</cell><cell cols="4"># Instances # Features # Classes</cell></row><row><cell cols="3">High-performance network [32] 377,526</cell><cell>248</cell><cell>13</cell></row><row><cell>wide2009 [2]</cell><cell></cell><cell>20,000</cell><cell>20</cell><cell>6</cell></row><row><cell>KDD99 [1]</cell><cell></cell><cell>60,000</cell><cell>41</cell><cell>5</cell></row><row><cell>FS Tech</cell><cell cols="4">Goodness Stability Similarity</cell></row><row><cell>LOA</cell><cell>High</cell><cell>Low</cell><cell></cell><cell>High</cell></row><row><cell>CBF</cell><cell>High</cell><cell>Low</cell><cell></cell><cell>Low</cell></row><row><cell>InfoGain</cell><cell>Medium</cell><cell>High</cell><cell></cell><cell>Low</cell></row><row><cell cols="2">Chi-square Medium</cell><cell>High</cell><cell></cell><cell>Low</cell></row><row><cell>CBC</cell><cell>Medium</cell><cell>Low</cell><cell></cell><cell>Low</cell></row><row><cell>GainRatio</cell><cell>Low</cell><cell cols="2">Medium</cell><cell>Low</cell></row><row><cell cols="5">(a) On high-performance network data set</cell></row><row><cell>FS Tech</cell><cell cols="4">Goodness Stability Similarity</cell></row><row><cell>LOA</cell><cell>High</cell><cell>High</cell><cell></cell><cell>High</cell></row><row><cell>CBF</cell><cell>High</cell><cell cols="2">Medium</cell><cell>Medium</cell></row><row><cell>InfoGain</cell><cell>High</cell><cell>High</cell><cell></cell><cell>Medium</cell></row><row><cell>Chi-square</cell><cell>High</cell><cell>High</cell><cell></cell><cell>Medium</cell></row><row><cell>CBC</cell><cell>High</cell><cell cols="2">Medium</cell><cell>Medium</cell></row><row><cell>GainRatio</cell><cell>High</cell><cell cols="2">Medium</cell><cell>Medium</cell></row><row><cell cols="4">(b) On wide2009 data set</cell></row><row><cell>FS Tech</cell><cell cols="4">Goodness Stability Similarity</cell></row><row><cell>LOA</cell><cell>High</cell><cell>High</cell><cell></cell><cell>High</cell></row><row><cell>CBF</cell><cell>High</cell><cell cols="2">Medium</cell><cell>Medium</cell></row><row><cell>InfoGain</cell><cell>High</cell><cell>High</cell><cell></cell><cell>Medium</cell></row><row><cell>Chi-square</cell><cell>High</cell><cell cols="2">Medium</cell><cell>Medium</cell></row><row><cell>CBC</cell><cell>High</cell><cell>Low</cell><cell></cell><cell>Medium</cell></row><row><cell>GainRatio</cell><cell>High</cell><cell cols="2">Medium</cell><cell>Medium</cell></row><row><cell cols="5">(c) On DARPA (KDD99) data set</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It uses the information theory to determine the most informative attributes and to have tree with minimal branching.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Please cite this article in press as: A. Fahad et al., Toward an efficient and scalable feature selection approach for internet traffic classification, Comput. Netw. (2013), http://dx.doi.org/10.1016/j.comnet.2013.04.005</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Abdun Mahmood for providing insightful comments and helpful feedback on the draft of this paper. This work has been partially supported by ARC Linkage Grants LP100100404 and LP110100602. Both of the grants relate to the detection of attacks in SCADA environments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information Technology, RMIT University, Melbourne. His current research has a special focus on machine learning, data mining, the performance of Web servers and SOAP-based systems, SCADA system security, and Web services, including protocol verification and service matching. He has organized more than 12 international conferences as either a general co-chair or a PC co-chair. He regularly publishes in reputable journals, such as ACM and IEEE Transactions. He is a senior member of the IEEE. From 1996 to 1998, he also represented the University of British Columbia, Vancouver, BC, Canada, at the ATM Forum. From 2000 to 2006, he served as a Canadian delegate to the ISO/IEC JTC1/SC29 Standards Committee (MPEG-4 Multimedia Delivery), where he worked within the MPEG-4 standardization JTC1-SC29WG11 and the Ad-Hoc group involved in the development of the reference software IM1 AHG. Dr. Alnuweiri has a long record of industrial collaborations with several major companies worldwide. He is also an inventor, and holds three US patents, and one International patent. He has authored or co-authored over 150 refereed journal and conference papers in various areas of computer and communications research. In particular, his research interests include mobile Internet technologies, multimedia communications, wireless protocols, routing and information dissemination algorithms for opportunistic networking, and quality-of-service provisioning and resource allocation in wireless networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ibrahim</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DARPA Intrusion Detection Evaluation Data Set</title>
		<ptr target="&lt;http://www.ll.mit.edu/IST/ideval/data/1998/&gt;" />
		<imprint>
			<date type="published" when="1998">1998. 1999</date>
		</imprint>
		<respStmt>
			<orgName>Mit Lincoln Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian neural networks for internet traffic classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Auld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="239" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selection of relevant features and examples in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="245" to="271" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiley</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Issues and future directions in traffic classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dainotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pescapè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Claffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network, IEEE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="40" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature selection for classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consistency-based search in feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1-</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Traffic classification using clustering algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Erman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 SIGCOMM Workshop on Mining Network Data</title>
		<meeting>the 2006 SIGCOMM Workshop on Mining Network Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Offline/ realtime traffic classification using semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Erman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9-12</biblScope>
			<biblScope unit="page" from="1194" to="1213" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying and discriminating between web and peer-to-peer traffic in the network core</title>
		<author>
			<persName><forename type="first">J</forename><surname>Erman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, ACM</title>
		<meeting>the 16th International Conference on World Wide Web, ACM</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="883" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatically inferring patterns of resource consumption in network traffic</title>
		<author>
			<persName><forename type="first">C</forename><surname>Estan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-interval discretization of continuousvalued attributes for classification learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature selection strategy in text classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ACAS: automated construction of application signatures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Spatscheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM SIGCOMM Workshop on Mining Network Data</title>
		<meeting>the 2005 ACM SIGCOMM Workshop on Mining Network Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="197" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>It</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transport layer identification of P2P traffic</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement</title>
		<meeting>the 4th ACM SIGCOMM Conference on Internet Measurement</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="121" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLINC: multilevel traffic classification in the dark</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Papagiannaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Internet traffic classification demystified: myths, caveats, and the best practices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Claffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fomenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM CoNEXT Conference</title>
		<meeting>the 2008 ACM CoNEXT Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On combining classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble classifiers for steganalysis of digital media</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="444" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study of cross-validation and bootstrap for accuracy estimation and model selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1137" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Netramark: a network traffic classification benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of network flow applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName><surname>Gunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chi2: feature selection and discretization of numeric attributes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Setiono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Tools with Artificial Intelligence</title>
		<meeting>the Seventh International Conference on Tools with Artificial Intelligence</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An analysis of the 1999 DARPA/lincoln laboratory evaluation data for network anomaly detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Intrusion Detection</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="220" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Architecture of a network monitor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kreibich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Passive &amp; Active Measurement Workshop 2003 (PAM2003)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Toward the accurate identification of network applications, Passive and Active Network Measurement</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Papagiannaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="41" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Internet traffic classification using bayesian analysis techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zuev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM SIGMETRICS International Conference on Measurement and Modelling of Computer systems</title>
		<meeting>the 2005 ACM SIGMETRICS International Conference on Measurement and Modelling of Computer systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Discriminators for Use in Flow-Based Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zuev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Astute: detecting a different class of traffic anomalies</title>
		<author>
			<persName><forename type="first">F</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="278" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating stability and comparing output of feature selectors that optimize feature subset cardinality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Somol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1921" to="1939" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Machine learning algorithms for accurate flow-based network traffic classification: evaluation and comparison</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="451" to="467" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An extended chi2 algorithm for discretization of real value attributes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="441" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cluster-oriented ensemble classifier: impact of multicluster characterization on ensemble classifier learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="605" to="618" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">HykGene: a hybrid approach for selecting marker genes for phenotype classification using microarray gene expression data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1530</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A preliminary performance comparison of five machine learning algorithms for practical IP traffic flow classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Armitage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5" to="16" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multilinear discriminant analysis for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="212" to="220" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal data clustering via weighted clustering ensemble with different representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="307" to="320" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning-International Workshop Then Conference</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data: a fast correlation-based filter solution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning-International Workshop Then Conference</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">856</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An svm-based machine learning method for accurate internet traffic classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems Frontiers</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="156" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Internet traffic classification by aggregating correlated naive bayes predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="15" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning semi-riemannian metrics for semisupervised feature extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="600" to="611" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ensemble clustering for internet security applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1784" to="1796" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">He received his M.S. degree (with high distinction) in 2008 from RMIT University, Melbourne, Australia, and he is currently a Ph.D. candidate in the Department of Computer Science and Information Technology at the University of RMIT. He joined the University of Albaha as a lecturer in 2009 and took a leave of absence in 2010 for his Ph.D. studies. His research interests are in the areas of wireless sensor networks, mobile networks, and ad hoc networks with emphasis on data mining, statistical analysis/modelling and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zuev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Adil Fahad received his B.S. degree in Computer Science from King Abdul Aziz University</title>
		<imprint>
			<biblScope unit="page" from="321" to="324" />
			<date type="published" when="2003">2005. 2003</date>
			<pubPlace>Jeddah, Saudi Arabia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science and</orgName>
		</respStmt>
	</monogr>
	<note>Zahir Tari received the honors degree in operational research from the Universite des Sciences et de la Technologie Houari Boumediene (USTHB), Algiers, Algeria, the masters degree in operational research from the University of Grenoble I, France, and the Ph. D. degree in artificial intelligence from the University of Grenoble II, France. He is the head of the Distributed Systems and Networking Discipline</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
